{"id": "2508.15043", "pdf": "https://arxiv.org/pdf/2508.15043.pdf", "abs": "https://arxiv.org/abs/2508.15043", "title": "LitForager: Exploring Multimodal Literature Foraging Strategies in Immersive Sensemaking", "authors": ["Haoyang Yang", "Elliott H. Faa", "Weijian Liu", "Shunan Guo", "Duen Horng Chau", "Yalong Yang"], "categories": ["cs.HC"], "comment": "11 pages, 10 figures, Accepted to IEEE ISMAR 2025 (TVCG)", "summary": "Exploring and comprehending relevant academic literature is a vital yet\nchallenging task for researchers, especially given the rapid expansion in\nresearch publications. This task fundamentally involves sensemaking -\ninterpreting complex, scattered information sources to build understanding.\nWhile emerging immersive analytics tools have shown cognitive benefits like\nenhanced spatial memory and reduced mental load, they predominantly focus on\ninformation synthesis (e.g., organizing known documents). In contrast, the\nequally important information foraging phase - discovering and gathering\nrelevant literature - remains underexplored within immersive environments,\nhindering a complete sensemaking workflow. To bridge this gap, we introduce\nLitForager, an interactive literature exploration tool designed to facilitate\ninformation foraging of research literature within an immersive sensemaking\nworkflow using network-based visualizations and multimodal interactions.\nDeveloped with WebXR and informed by a formative study with researchers,\nLitForager supports exploration guidance, spatial organization, and seamless\ntransition through a 3D literature network. An observational user study with 15\nresearchers demonstrated LitForager's effectiveness in supporting fluid\nforaging strategies and spatial sensemaking through its multimodal interface."}
{"id": "2508.15045", "pdf": "https://arxiv.org/pdf/2508.15045.pdf", "abs": "https://arxiv.org/abs/2508.15045", "title": "Understanding Accessibility Needs of Blind Authors on CMS-Based Websites", "authors": ["Guillermo Vera-Amaro", "José Rafael Rojano-Cáceres"], "categories": ["cs.HC", "H.5.2; H.5.4"], "comment": "15 pages, 5 figures, presented in Ibero-American Conference on\n  Human-Computer Interaction 2025", "summary": "This paper addresses the limited attention given to blind users as content\ncreators in Content Management Systems (CMS), a gap that remains under-explored\nin web accessibility research. For blind authors, effective interaction with\nCMS platforms requires more than technical compliance; it demands interfaces\ndesigned with semantic clarity, predictable navigation, and meaningful feedback\nfor screen reader users. This study investigates the accessibility barriers\nblind users face when performing key tasks, such as page creation, menu\nediting, and image publishing, using CMS platforms. A two-fold evaluation was\nconducted using automated tools and manual usability testing with three blind\nand one sighted participant, complemented by expert analysis based on the\nBarrier Walkthrough method. Results showed that block-based interfaces were\nparticularly challenging, often marked as accessible by automated tools but\nresulting in critical usability issues during manual evaluation. The use of a\ntext-based editor, the integration of AI-generated image descriptions, and\ntraining aligned with screen reader workflows, significantly improved usability\nand autonomy. These findings underscore the limitations of automated\nassessments and highlight the importance of user-centered design practices.\nEnhancing CMS accessibility requires consistent navigation structures, reduced\nreliance on visual interaction patterns, and the integration of AI tools that\nsupport blind content authors throughout the content creation process."}
{"id": "2508.15146", "pdf": "https://arxiv.org/pdf/2508.15146.pdf", "abs": "https://arxiv.org/abs/2508.15146", "title": "QueryGenie: Making LLM-Based Database Querying Transparent and Controllable", "authors": ["Longfei Chen", "Shenghan Gao", "Shiwei Wang", "Ken Lin", "Yun Wang", "Quan Li"], "categories": ["cs.HC"], "comment": "Accepted by The 38th Annual ACM Symposium on User Interface Software\n  and Technology (UIST Adjunct '25), September 28-October 1, 2025, Busan,\n  Republic of Korea", "summary": "Conversational user interfaces powered by large language models (LLMs) have\nsignificantly lowered the technical barriers to database querying. However,\nexisting tools still encounter several challenges, such as misinterpretation of\nuser intent, generation of hallucinated content, and the absence of effective\nmechanisms for human feedback-all of which undermine their reliability and\npractical utility. To address these issues and promote a more transparent and\ncontrollable querying experience, we proposed QueryGenie, an interactive system\nthat enables users to monitor, understand, and guide the LLM-driven query\ngeneration process. Through incremental reasoning, real-time validation, and\nresponsive interaction mechanisms, users can iteratively refine query logic and\nensure alignment with their intent."}
{"id": "2508.15148", "pdf": "https://arxiv.org/pdf/2508.15148.pdf", "abs": "https://arxiv.org/abs/2508.15148", "title": "ReviseMate: Exploring Contextual Support for Digesting STEM Paper Reviews", "authors": ["Yuansong Xu", "Shuhao Zhang", "Yijie Fan", "Shaohan Shi", "Zhenhui Peng", "Quan Li"], "categories": ["cs.HC"], "comment": "Appear in Proc. ACM Hum.-Comput. Interact., Vol. 9, No. 7, Article\n  CSCW321. Publication date: November 2025", "summary": "Effectively assimilating and integrating reviewer feedback is crucial for\nresearchers seeking to refine their papers and handle potential rebuttal phases\nin academic venues. However, traditional review digestion processes present\nchallenges such as time consumption, reading fatigue, and the requisite for\ncomprehensive analytical skills. Prior research on review analysis often\nprovides theoretical guidance with limited targeted support. Additionally,\ngeneral text comprehension tools overlook the intricate nature of\ncomprehensively understanding reviews and lack contextual assistance. To bridge\nthis gap, we formulated research questions to explore the authors' concerns and\nmethods for enhancing comprehension during the review digestion phase. Through\ninterviews and the creation of storyboards, we developed ReviseMate, an\ninteractive system designed to address the identified challenges. A controlled\nuser study (N=31) demonstrated the superiority of ReviseMate over baseline\nmethods, with positive feedback regarding user interaction. Subsequent field\ndeployment (N=6) further validated the effectiveness of ReviseMate in\nreal-world review digestion scenarios. These findings underscore the potential\nof interactive tools to significantly enhance the assimilation and integration\nof reviewer feedback during the manuscript review process."}
{"id": "2508.14904", "pdf": "https://arxiv.org/pdf/2508.14904.pdf", "abs": "https://arxiv.org/abs/2508.14904", "title": "Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training", "authors": ["Jianfeng Si", "Lin Sun", "Zhewen Tan", "Xiangzheng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages,5 figures,4 tables", "summary": "Current methods for content safety in Large Language Models (LLMs), such as\nSupervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback\n(RLHF), often rely on multi-stage training pipelines and lack fine-grained,\npost-deployment controllability. To address these limitations, we propose a\nunified co-training framework that efficiently integrates multiple safety\nbehaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and\nrejective (refusal-oriented/conservative) within a single SFT stage. Notably,\neach behavior is dynamically activated via a simple system-level instruction,\nor magic token, enabling stealthy and efficient behavioral switching at\ninference time. This flexibility supports diverse deployment scenarios, such as\npositive for safe user interaction, negative for internal red-teaming, and\nrejective for context-aware refusals triggered by upstream moderation signals.\nThis co-training strategy induces a distinct Safety Alignment Margin in the\noutput space, characterized by well-separated response distributions\ncorresponding to each safety mode. The existence of this margin provides\nempirical evidence for the model's safety robustness and enables unprecedented\nfine-grained control. Experiments show that our method matches the safety\nalignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1\n(671B) in safety performance, while significantly reducing both training\ncomplexity and deployment costs. This work presents a scalable, efficient, and\nhighly controllable solution for LLM content safety."}
{"id": "2508.15152", "pdf": "https://arxiv.org/pdf/2508.15152.pdf", "abs": "https://arxiv.org/abs/2508.15152", "title": "Evaluating an Immersive Analytics Application at an Enterprise Business Intelligence Customer Conference", "authors": ["Matthew Brehmer", "Ginger Gloystein", "Bailiang Zhou", "Abby Gray", "Sruthi Pillai", "Ben Medina", "Vidya Setlur"], "categories": ["cs.HC"], "comment": "To appear at the Human Factors in Immersive Analytics (HFIA) Workshop\n  at IEEE VIS 2025", "summary": "We reflect on an evaluation of an immersive analytics application (Tableau\nfor visionOS) conducted at a large enterprise business intelligence (BI)\nconference. Conducting a study in such a context offered an opportunistic\nsetting to gather diverse feedback. However, this setting also highlighted the\nchallenge of evaluating usability while also assessing potential utility, as\nfeedback straddled between the novelty of the experience and the practicality\nof the application in participants' analytical workflows. This formative\nevaluation with 22 participants allowed us to gather insights with respect to\nthe usability of Tableau for visionOS, along with broader perspectives on the\npotential for head-mounted displays (HMDs) to promote new ways to engage with\nBI data. Our experience suggests a need for new evaluation considerations that\nintegrate qualitative and quantitative measures and account for unique\ninteraction patterns with 3D representations and interfaces accessible via an\nHMD. Overall, we contribute an enterprise perspective on evaluation\nmethodologies for immersive analytics."}
{"id": "2508.14909", "pdf": "https://arxiv.org/pdf/2508.14909.pdf", "abs": "https://arxiv.org/abs/2508.14909", "title": "Preliminary Ranking of WMT25 General Machine Translation Systems", "authors": ["Tom Kocmi", "Eleftherios Avramidis", "Rachel Bawden", "Ondřej Bojar", "Konstantin Dranch", "Anton Dvorkovich", "Sergey Dukanov", "Natalia Fedorova", "Mark Fishel", "Markus Freitag", "Thamme Gowda", "Roman Grundkiewicz", "Barry Haddow", "Marzena Karpinska", "Philipp Koehn", "Howard Lakougna", "Jessica Lundin", "Kenton Murray", "Masaaki Nagata", "Stefano Perrella", "Lorenzo Proietti", "Martin Popel", "Maja Popović", "Parker Riley", "Mariya Shmatova", "Steinþór Steingrímsson", "Lisa Yankovskaya", "Vilém Zouhar"], "categories": ["cs.CL"], "comment": null, "summary": "We present the preliminary ranking of the WMT25 General Machine Translation\nShared Task, in which MT systems have been evaluated using automatic metrics.\nAs this ranking is based on automatic evaluations, it may be biased in favor of\nsystems that employ re-ranking techniques, such as Quality Estimation\nre-ranking or Minimum Bayes Risk decoding. The official WMT25 ranking will be\nbased on human evaluation, which is more reliable and will supersede the\nautomatic ranking.\n  The purpose of this report is not to present the final findings of the\nGeneral MT task, but rather to share preliminary results with task\nparticipants, which may be useful when preparing their system submission\npapers."}
{"id": "2508.15227", "pdf": "https://arxiv.org/pdf/2508.15227.pdf", "abs": "https://arxiv.org/abs/2508.15227", "title": "GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design", "authors": ["Wen-Fan Wang", "Ting-Ying Lee", "Chien-Ting Lu", "Che-Wei Hsu", "Nil Ponsa Campany", "Yu Chen", "Mike Y. Chen", "Bing-Yu Chen"], "categories": ["cs.HC", "cs.AI", "H.5.2"], "comment": "Accepted ACM Symposium on User Interface Software and Technology\n  (UIST '25)", "summary": "Environment designers in the entertainment industry create imaginative 2D and\n3D scenes for games, films, and television, requiring both fine-grained control\nof specific details and consistent global coherence. Designers have\nincreasingly integrated generative AI into their workflows, often relying on\nlarge language models (LLMs) to expand user prompts for text-to-image\ngeneration, then iteratively refining those prompts and applying inpainting.\nHowever, our formative study with 10 designers surfaced two key challenges: (1)\nthe lengthy LLM-generated prompts make it difficult to understand and isolate\nthe keywords that must be revised for specific visual elements; and (2) while\ninpainting supports localized edits, it can struggle with global consistency\nand correctness. Based on these insights, we present GenTune, an approach that\nenhances human--AI collaboration by clarifying how AI-generated prompts map to\nimage content. Our GenTune system lets designers select any element in a\ngenerated image, trace it back to the corresponding prompt labels, and revise\nthose labels to guide precise yet globally consistent image refinement. In a\nsummative study with 20 designers, GenTune significantly improved prompt--image\ncomprehension, refinement quality, and efficiency, and overall satisfaction\n(all $p < .01$) compared to current practice. A follow-up field study with two\nstudios further demonstrated its effectiveness in real-world settings."}
{"id": "2508.14913", "pdf": "https://arxiv.org/pdf/2508.14913.pdf", "abs": "https://arxiv.org/abs/2508.14913", "title": "Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages", "authors": ["Israel Abebe Azime", "Tadesse Destaw Belay", "Dietrich Klakow", "Philipp Slusallek", "Anshuman Chhabra"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant capabilities in\nsolving mathematical problems expressed in natural language. However,\nmultilingual and culturally-grounded mathematical reasoning in low-resource\nlanguages lags behind English due to the scarcity of socio-cultural task\ndatasets that reflect accurate native entities such as person names,\norganization names, and currencies. Existing multilingual benchmarks are\npredominantly produced via translation and typically retain English-centric\nentities, owing to the high cost associated with human annotater-based\nlocalization. Moreover, automated localization tools are limited, and hence,\ntruly localized datasets remain scarce. To bridge this gap, we introduce a\nframework for LLM-driven cultural localization of math word problems that\nautomatically constructs datasets with native names, organizations, and\ncurrencies from existing sources. We find that translated benchmarks can\nobscure true multilingual math ability under appropriate socio-cultural\ncontexts. Through extensive experiments, we also show that our framework can\nhelp mitigate English-centric entity bias and improves robustness when native\nentities are introduced across various languages."}
{"id": "2508.15249", "pdf": "https://arxiv.org/pdf/2508.15249.pdf", "abs": "https://arxiv.org/abs/2508.15249", "title": "Visualization on Smart Wristbands: Results from an In-situ Design Workshop with Four Scenarios", "authors": ["Alaul Islam", "Fairouz Grioui", "Raimund Dachselt", "Petra Isenberg"], "categories": ["cs.HC"], "comment": "12 pages, 8 figures", "summary": "We present the results of an in-situ ideation workshop for designing data\nvisualizations on smart wristbands that can show data around the entire wrist\nof a wearer. Wristbands pose interesting challenges because the visibility of\ndifferent areas of the band depends on the wearer's arm posture. We focused on\nfour usage scenarios that lead to different postures: office work, leisurely\nwalks, cycling, and driving. As the technology for smart wristbands is not yet\ncommercially available, we conducted a paper-based ideation exercise that\nshowed how spatial layout and visualization design on smart wristbands may need\nto vary depending on the types of data items of interest and arm postures.\nParticipants expressed a strong preference for responsive visualization designs\nthat could adapt to the movement of wearers' arms. Supplemental material from\nthe study is available here: https://osf.io/4hrca/."}
{"id": "2508.14951", "pdf": "https://arxiv.org/pdf/2508.14951.pdf", "abs": "https://arxiv.org/abs/2508.14951", "title": "Improving LLMs for Machine Translation Using Synthetic Preference Data", "authors": ["Dario Vajda", "Domen Vreš", "Marko Robnik-Šikonja"], "categories": ["cs.CL"], "comment": "Paper with individual presentation at LUHME workshop at ECAI 2025", "summary": "Large language models have emerged as effective machine translation systems.\nIn this paper, we explore how a general instruction-tuned large language model\ncan be improved for machine translation using relatively few easily produced\ndata resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct\nmodel using Direct Preference Optimization (DPO) training on a programmatically\ncurated and enhanced subset of a public dataset. As DPO requires pairs of\nquality-ranked instances, we generated its training dataset by translating\nEnglish Wikipedia articles using two LLMs, GaMS-9B-Instruct and\nEuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics\ncoupled with automatic evaluation metrics such as COMET. The evaluation shows\nthat our fine-tuned model outperforms both models involved in the dataset\ngeneration. In comparison to the baseline models, the fine-tuned model achieved\na COMET score gain of around 0.04 and 0.02, respectively, on translating\nWikipedia articles. It also more consistently avoids language and formatting\nerrors."}
{"id": "2508.15258", "pdf": "https://arxiv.org/pdf/2508.15258.pdf", "abs": "https://arxiv.org/abs/2508.15258", "title": "Spatio-Temporal Mixed and Augmented Reality Experience Description for Interactive Playback", "authors": ["Dooyoung Kim", "Woontack Woo"], "categories": ["cs.HC"], "comment": "4 pages, 2 figures, Accepted in the IEEE ISMAR 2025 XRStand Workshop", "summary": "We propose the Spatio-Temporal Mixed and Augmented Reality Experience\nDescription (MAR-ED), a novel framework to standardize the representation of\npast events for interactive and adaptive playback in a user's present physical\nspace. While current spatial media technologies have primarily focused on\ncapturing or replaying content as static assets, often disconnected from the\nviewer's environment or offering limited interactivity, the means to describe\nan experience's underlying semantic and interactive structure remains\nunderexplored. We propose a descriptive framework called MAR-ED based on three\ncore primitives: 1) Event Primitives for semantic scene graph representation,\n2) Keyframe Primitives for efficient and meaningful data access, and 3)\nPlayback Primitives for user-driven adaptive interactive playback of recorded\nMAR experience. The proposed flowchart of the three-stage process of the\nproposed MAR-ED framework transforms a recorded experience into a unique\nadaptive MAR experience during playback, where its spatio-temporal structure\ndynamically conforms to a new environment and its narrative can be altered by\nlive user input. Drawing on this framework, personal digital memories and\nrecorded events can evolve beyond passive 2D/3D videos into immersive,\nspatially-integrated group experiences, opening new paradigms for training,\ncultural heritage, and interactive storytelling without requiring complex,\nper-user adaptive rendering."}
{"id": "2508.14982", "pdf": "https://arxiv.org/pdf/2508.14982.pdf", "abs": "https://arxiv.org/abs/2508.14982", "title": "Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems", "authors": ["Qianli Wang", "Tatiana Anikina", "Nils Feldhus", "Simon Ostermann", "Fedor Splitt", "Jiaao Li", "Yoana Tsoneva", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 Findings, camera-ready version", "summary": "Conversational explainable artificial intelligence (ConvXAI) systems based on\nlarge language models (LLMs) have garnered considerable attention for their\nability to enhance user comprehension through dialogue-based explanations.\nCurrent ConvXAI systems often are based on intent recognition to accurately\nidentify the user's desired intention and map it to an explainability method.\nWhile such methods offer great precision and reliability in discerning users'\nunderlying intentions for English, a significant challenge in the scarcity of\ntraining data persists, which impedes multilingual generalization. Besides, the\nsupport for free-form custom inputs, which are user-defined data distinct from\npre-configured dataset instances, remains largely limited. To bridge these\ngaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQL\ndataset spanning five typologically diverse languages, including one\nlow-resource language. Subsequently, we propose a new parsing approach aimed at\nenhancing multilingual parsing performance, and evaluate three LLMs on\nMultiCoXQL using various parsing strategies. Furthermore, we present Compass, a\nnew multilingual dataset designed for custom input extraction in ConvXAI\nsystems, encompassing 11 intents across the same five languages as MultiCoXQL.\nWe conduct monolingual, cross-lingual, and multilingual evaluations on Compass,\nemploying three LLMs of varying sizes alongside BERT-type models."}
{"id": "2508.15716", "pdf": "https://arxiv.org/pdf/2508.15716.pdf", "abs": "https://arxiv.org/abs/2508.15716", "title": "Foundation Models for Cross-Domain EEG Analysis Application: A Survey", "authors": ["Hongqi Li", "Yitong Chen", "Yujuan Wang", "Weihang Ni", "Haodong Zhang"], "categories": ["cs.HC", "cs.AI"], "comment": "Submitted to IEEE Journals", "summary": "Electroencephalography (EEG) analysis stands at the forefront of neuroscience\nand artificial intelligence research, where foundation models are reshaping the\ntraditional EEG analysis paradigm by leveraging their powerful representational\ncapacity and cross-modal generalization. However, the rapid proliferation of\nthese techniques has led to a fragmented research landscape, characterized by\ndiverse model roles, inconsistent architectures, and a lack of systematic\ncategorization. To bridge this gap, this study presents the first comprehensive\nmodality-oriented taxonomy for foundation models in EEG analysis,\nsystematically organizing research advances based on output modalities of the\nnative EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal\nframeworks. We rigorously analyze each category's research ideas, theoretical\nfoundations, and architectural innovations, while highlighting open challenges\nsuch as model interpretability, cross-domain generalization, and real-world\napplicability in EEG-based systems. By unifying this dispersed field, our work\nnot only provides a reference framework for future methodology development but\naccelerates the translation of EEG foundation models into scalable,\ninterpretable, and online actionable solutions."}
{"id": "2508.15044", "pdf": "https://arxiv.org/pdf/2508.15044.pdf", "abs": "https://arxiv.org/abs/2508.15044", "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner", "authors": ["Bolian Li", "Yanran Wu", "Xinyu Luo", "Ruqi Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency."}
{"id": "2508.15727", "pdf": "https://arxiv.org/pdf/2508.15727.pdf", "abs": "https://arxiv.org/abs/2508.15727", "title": "Demystifying Reward Design in Reinforcement Learning for Upper Extremity Interaction: Practical Guidelines for Biomechanical Simulations in HCI", "authors": ["Hannah Selder", "Florian Fischer", "Per Ola Kristensson", "Arthur Fleig"], "categories": ["cs.HC", "H.5.2; F.m"], "comment": "17 pages, 14 figures, 1 table, ACM UIST 2025", "summary": "Designing effective reward functions is critical for reinforcement\nlearning-based biomechanical simulations, yet HCI researchers and practitioners\noften waste (computation) time with unintuitive trial-and-error tuning. This\npaper demystifies reward function design by systematically analyzing the impact\nof effort minimization, task completion bonuses, and target proximity\nincentives on typical HCI tasks such as pointing, tracking, and choice\nreaction. We show that proximity incentives are essential for guiding movement,\nwhile completion bonuses ensure task success. Effort terms, though optional,\nhelp refine motion regularity when appropriately scaled. We perform an\nextensive analysis of how sensitive task success and completion time depend on\nthe weights of these three reward components. From these results we derive\npractical guidelines to create plausible biomechanical simulations without the\nneed for reinforcement learning expertise, which we then validate on remote\ncontrol and keyboard typing tasks. This paper advances simulation-based\ninteraction design and evaluation in HCI by improving the efficiency and\napplicability of biomechanical user modeling for real-world interface\ndevelopment."}
{"id": "2508.15085", "pdf": "https://arxiv.org/pdf/2508.15085.pdf", "abs": "https://arxiv.org/abs/2508.15085", "title": "LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text", "authors": ["MohamamdJavad Ardestani", "Ehsan Kamalloo", "Davood Rafiei"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "LongRecall. The completeness of machine-generated text, ensuring that it\ncaptures all relevant information, is crucial in domains such as medicine and\nlaw and in tasks like list-based question answering (QA), where omissions can\nhave serious consequences. However, existing recall metrics often depend on\nlexical overlap, leading to errors with unsubstantiated entities and\nparaphrased answers, while LLM-as-a-Judge methods with long holistic prompts\ncapture broader semantics but remain prone to misalignment and hallucinations\nwithout structured verification. We introduce LongRecall, a general three-stage\nrecall evaluation framework that decomposes answers into self-contained facts,\nsuccessively narrows plausible candidate matches through lexical and semantic\nfiltering, and verifies their alignment through structured entailment checks.\nThis design reduces false positives and false negatives while accommodating\ndiverse phrasings and contextual variations, serving as a foundational building\nblock for systematic recall assessment. We evaluate LongRecall on three\nchallenging long-form QA benchmarks using both human annotations and LLM-based\njudges, demonstrating substantial improvements in recall accuracy over strong\nlexical and LLM-as-a-Judge baselines."}
{"id": "2508.15752", "pdf": "https://arxiv.org/pdf/2508.15752.pdf", "abs": "https://arxiv.org/abs/2508.15752", "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards Geospatial AI Agents for Visual Inquiries", "authors": ["Jon E. Froehlich", "Jared Hwang", "Zeyu Wang", "John S. O'Meara", "Xia Su", "William Huang", "Yang Zhang", "Alex Fiannaca", "Philip Nelson", "Shaun Kane"], "categories": ["cs.HC", "cs.AI", "cs.CV", "H.5; I.2"], "comment": "Accepted to the ICCV'25 Workshop \"Vision Foundation Models and\n  Generative AI for Accessibility: Challenges and Opportunities\"", "summary": "Interactive digital maps have revolutionized how people travel and learn\nabout the world; however, they rely on pre-existing structured data in GIS\ndatabases (e.g., road networks, POI indices), limiting their ability to address\ngeo-visual questions related to what the world looks like. We introduce our\nvision for Geo-Visual Agents--multimodal AI agents capable of understanding and\nresponding to nuanced visual-spatial inquiries about the world by analyzing\nlarge-scale repositories of geospatial images, including streetscapes (e.g.,\nGoogle Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial\nimagery (e.g., satellite photos) combined with traditional GIS data sources. We\ndefine our vision, describe sensing and interaction approaches, provide three\nexemplars, and enumerate key challenges and opportunities for future work."}
{"id": "2508.15090", "pdf": "https://arxiv.org/pdf/2508.15090.pdf", "abs": "https://arxiv.org/abs/2508.15090", "title": "Mapping the Course for Prompt-based Structured Prediction", "authors": ["Matt Pauk", "Maria Leonor Pacheco"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have been shown to be useful for a variety of language tasks, without\nrequiring task-specific fine-tuning. However, these models often struggle with\nhallucinations and complex reasoning problems due to their autoregressive\nnature. We propose to address some of these issues, specifically in the area of\nstructured prediction, by combining LLMs with combinatorial inference in an\nattempt to marry the predictive power of LLMs with the structural consistency\nprovided by inference methods. We perform exhaustive experiments in an effort\nto understand which prompting strategies can effectively estimate LLM\nconfidence values for use with symbolic inference, and show that, regardless of\nthe prompting strategy, the addition of symbolic inference on top of prompting\nalone leads to more consistent and accurate predictions. Additionally, we show\nthat calibration and fine-tuning using structured prediction objectives leads\nto increased performance for challenging tasks, showing that structured\nlearning is still valuable in the era of LLMs."}
{"id": "2508.14920", "pdf": "https://arxiv.org/pdf/2508.14920.pdf", "abs": "https://arxiv.org/abs/2508.14920", "title": "Human Feedback Driven Dynamic Speech Emotion Recognition", "authors": ["Ilya Fedorov", "Dmitry Korobchenko"], "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "comment": null, "summary": "This work proposes to explore a new area of dynamic speech emotion\nrecognition. Unlike traditional methods, we assume that each audio track is\nassociated with a sequence of emotions active at different moments in time. The\nstudy particularly focuses on the animation of emotional 3D avatars. We propose\na multi-stage method that includes the training of a classical speech emotion\nrecognition model, synthetic generation of emotional sequences, and further\nmodel improvement based on human feedback. Additionally, we introduce a novel\napproach to modeling emotional mixtures based on the Dirichlet distribution.\nThe models are evaluated based on ground-truth emotions extracted from a\ndataset of 3D facial animations. We compare our models against the sliding\nwindow approach. Our experimental results show the effectiveness of\nDirichlet-based approach in modeling emotional mixtures. Incorporating human\nfeedback further improves the model quality while providing a simplified\nannotation procedure."}
{"id": "2508.15096", "pdf": "https://arxiv.org/pdf/2508.15096.pdf", "abs": "https://arxiv.org/abs/2508.15096", "title": "Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset", "authors": ["Rabeeh Karimi Mahabadi", "Sanjeev Satheesh", "Shrimai Prabhumoye", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Pretraining large language models (LLMs) on high-quality, structured data\nsuch as mathematics and code substantially enhances reasoning capabilities.\nHowever, existing math-focused datasets built from Common Crawl suffer from\ndegraded quality due to brittle extraction heuristics, lossy HTML-to-text\nconversion, and the failure to reliably preserve mathematical structure. In\nthis work, we introduce Nemotron-CC-Math, a large-scale, high-quality\nmathematical corpus constructed from Common Crawl using a novel,\ndomain-agnostic pipeline specifically designed for robust scientific text\nextraction.\n  Unlike previous efforts, our pipeline recovers math across various formats\n(e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx\nand a targeted LLM-based cleaning stage. This approach preserves the structural\nintegrity of equations and code blocks while removing boilerplate,\nstandardizing notation into LaTeX representation, and correcting\ninconsistencies.\n  We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+\n(133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably,\nNemotron-CC-Math-4+ not only surpasses all prior open math datasets-including\nMegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens\nthan FineMath-4+, which was previously the highest-quality math pretraining\ndataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to\n+12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines,\nwhile also improving general-domain performance on MMLU and MMLU-Stem.\n  We present the first pipeline to reliably extract scientific\ncontent--including math--from noisy web-scale data, yielding measurable gains\nin math, code, and general reasoning, and setting a new state of the art among\nopen math pretraining corpora. To support open-source efforts, we release our\ncode and datasets."}
{"id": "2508.14996", "pdf": "https://arxiv.org/pdf/2508.14996.pdf", "abs": "https://arxiv.org/abs/2508.14996", "title": "\\textit{adder-viz}: Real-Time Visualization Software for Transcoding Event Video", "authors": ["Andrew C. Freeman", "Luke Reinkensmeyer"], "categories": ["cs.MM", "cs.CV", "cs.HC", "eess.IV"], "comment": "Accepted to the Open-Source Track at ACM Multimedia 2025", "summary": "Recent years have brought about a surge in neuromorphic ``event'' video\nresearch, primarily targeting computer vision applications. Event video eschews\nvideo frames in favor of asynchronous, per-pixel intensity samples. While much\nwork has focused on a handful of representations for specific event cameras,\nthese representations have shown limitations in flexibility, speed, and\ncompressibility. We previously proposed the unified AD{\\Delta}ER representation\nto address these concerns. This paper introduces numerous improvements to the\n\\textit{adder-viz} software for visualizing real-time event transcode processes\nand applications in-the-loop. The MIT-licensed software is available from a\ncentralized repository at\n\\href{https://github.com/ac-freeman/adder-codec-rs}{https://github.com/ac-freeman/adder-codec-rs}."}
{"id": "2508.15139", "pdf": "https://arxiv.org/pdf/2508.15139.pdf", "abs": "https://arxiv.org/abs/2508.15139", "title": "Identifying and Answering Questions with False Assumptions: An Interpretable Approach", "authors": ["Zijie Wang", "Eduardo Blanco"], "categories": ["cs.CL"], "comment": "To appear at EMNLP 2025 Main conference", "summary": "People often ask questions with false assumptions, a type of question that\ndoes not have regular answers. Answering such questions require first\nidentifying the false assumptions. Large Language Models (LLMs) often generate\nmisleading answers because of hallucinations. In this paper, we focus on\nidentifying and answering questions with false assumptions in several domains.\nWe first investigate to reduce the problem to fact verification. Then, we\npresent an approach leveraging external evidence to mitigate hallucinations.\nExperiments with five LLMs demonstrate that (1) incorporating retrieved\nevidence is beneficial and (2) generating and validating atomic assumptions\nyields more improvements and provides an interpretable answer by specifying the\nfalse assumptions."}
{"id": "2508.15680", "pdf": "https://arxiv.org/pdf/2508.15680.pdf", "abs": "https://arxiv.org/abs/2508.15680", "title": "Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle", "authors": ["Mark Cote", "Susana Aires"], "categories": ["cs.AI", "cs.HC", "I.2.6; I.2.11; K.4.1; K.6.0"], "comment": "15 pages, 3 figures, Presented at IAIL 2025 - Imagining the AI\n  Landscape after the AI Act, 4th International Workshop on Imagining the AI\n  Landscape After the AI Act, The fourth International Conference on Hybrid\n  Human-Artificial Intelligence", "summary": "This paper argues that a techno-philosophical reading of the EU AI Act\nprovides insight into the long-term dynamics of data in AI systems,\nspecifically, how the lifecycle from ingestion to deployment generates\nrecursive value chains that challenge existing frameworks for Responsible AI.\nWe introduce a conceptual tool to frame the AI pipeline, spanning data,\ntraining regimes, architectures, feature stores, and transfer learning. Using\ncross-disciplinary methods, we develop a technically grounded and\nphilosophically coherent analysis of regulatory blind spots. Our central claim\nis that what remains absent from policymaking is an account of the dynamic of\nbecoming that underpins both the technical operation and economic logic of AI.\nTo address this, we advance a formal reading of AI inspired by Simondonian\nphilosophy of technology, reworking his concept of individuation to model the\nAI lifecycle, including the pre-individual milieu, individuation, and\nindividuated AI. To translate these ideas, we introduce futurity: the\nself-reinforcing lifecycle of AI, where more data enhances performance, deepens\npersonalisation, and expands application domains. Futurity highlights the\nrecursively generative, non-rivalrous nature of data, underpinned by\ninfrastructures like feature stores that enable feedback, adaptation, and\ntemporal recursion. Our intervention foregrounds escalating power asymmetries,\nparticularly the tech oligarchy whose infrastructures of capture, training, and\ndeployment concentrate value and decision-making. We argue that effective\nregulation must address these infrastructural and temporal dynamics, and\npropose measures including lifecycle audits, temporal traceability, feedback\naccountability, recursion transparency, and a right to contest recursive reuse."}
{"id": "2508.15164", "pdf": "https://arxiv.org/pdf/2508.15164.pdf", "abs": "https://arxiv.org/abs/2508.15164", "title": "ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following", "authors": ["Seungmin Han", "Haeun Kwon", "Ji-jun Park", "Taeyang Yoon"], "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advancements in Large Language Models (LLMs) and Large\nVision-Language Models (LVLMs), current models still face substantial\nchallenges in handling complex, multi-turn, and visually-grounded tasks that\ndemand deep reasoning, sustained contextual understanding, entity tracking, and\nmulti-step instruction following. Existing benchmarks often fall short in\ncapturing the dynamism and intricacies of real-world multi-modal interactions,\nleading to issues such as context loss and visual hallucinations. To address\nthese limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning\nBenchmark), a novel dataset comprising 300 meticulously designed complex\nmulti-turn dialogue scenarios, each averaging 5-7 turns and evaluated across\nsix core dimensions including visual entity tracking and reasoning depth.\nFurthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic\nframework that enhances existing LVLMs with advanced reasoning and instruction\nfollowing capabilities through an iterative\n\"memory-perception-planning-execution\" cycle, requiring no extensive\nre-training of the underlying models. Our extensive experiments on MMDR-Bench\ndemonstrate that CoLVLM Agent consistently achieves superior performance,\nattaining an average human evaluation score of 4.03, notably surpassing\nstate-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro\n(3.85). The framework exhibits significant advantages in reasoning depth,\ninstruction adherence, and error suppression, and maintains robust performance\nover extended dialogue turns, validating the effectiveness of its modular\ndesign and iterative approach for complex multi-modal interactions."}
{"id": "2501.04905", "pdf": "https://arxiv.org/pdf/2501.04905.pdf", "abs": "https://arxiv.org/abs/2501.04905", "title": "Balancing Exploration and Cybersickness: Investigating Curiosity-Driven Behavior in Virtual Environments", "authors": ["Tangyao Li", "Yuyang Wang"], "categories": ["cs.HC"], "comment": "12 pages, 8 figures, submitted to the IEEE", "summary": "During virtual navigation, users exhibit varied interaction and navigation\nbehaviors influenced by several factors. Existing theories and models have been\ndeveloped to explain and predict these diverse patterns. While users often\nexperience uncomfortable sensations, such as cybersickness, during virtual\nreality (VR) use, they do not always make optimal decisions to mitigate these\neffects. Although methods like reinforcement learning have been used to model\ndecision-making processes, they typically rely on random selection to simulate\nactions, failing to capture the complexities of real navigation behavior. In\nthis study, we propose curiosity as a key factor driving irrational\ndecision-making, suggesting that users continuously balance exploration and\ncybersickness according to the free energy principle during virtual navigation.\nOur findings show that VR users generally adopt conservative strategies when\nnavigating, with most participants displaying negative curiosity across trials.\nHowever, curiosity levels tend to rise when the virtual environment changes,\nillustrating the dynamic interplay between exploration and discomfort. This\nstudy provides a quantitative approach to decoding curiosity-driven behavior\nduring virtual navigation, offering insights into how users balance exploration\nand the avoidance of cybersickness. Future research will further refine this\nmodel by incorporating additional psychological and environmental factors to\nimprove the accuracy of navigation pattern predictions."}
{"id": "2508.15190", "pdf": "https://arxiv.org/pdf/2508.15190.pdf", "abs": "https://arxiv.org/abs/2508.15190", "title": "SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling", "authors": ["Dong Liu", "Yanxuan Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization plays a critical role in language modeling, yet existing\napproaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on\nfrequency statistics, ignoring the underlying semantic structure of text. This\nleads to over-tokenization of semantically redundant spans and underutilization\nof contextual coherence, particularly in long-context scenarios. In this work,\nwe propose \\textbf{SemToken}, a semantic-aware tokenization framework that\njointly reduces token redundancy and improves computation efficiency. SemToken\nfirst extracts contextual semantic embeddings via lightweight encoders and\nperforms local semantic clustering to merge semantically equivalent tokens.\nThen, it allocates heterogeneous token granularity based on semantic density,\nallowing finer-grained tokenization in content-rich regions and coarser\ncompression in repetitive or low-entropy spans. SemToken can be seamlessly\nintegrated with modern language models and attention acceleration methods.\nExperiments on long-context language modeling benchmarks such as WikiText-103\nand LongBench show that SemToken achieves up to $2.4\\times$ reduction in token\ncount and $1.9\\times$ speedup, with negligible or no degradation in perplexity\nand downstream accuracy. Our findings suggest that semantic structure offers a\npromising new axis for optimizing tokenization and computation in large\nlanguage models."}
{"id": "2507.19898", "pdf": "https://arxiv.org/pdf/2507.19898.pdf", "abs": "https://arxiv.org/abs/2507.19898", "title": "TS-Insight: Visualizing Thompson Sampling for Verification and XAI", "authors": ["Parsa Vares", "Éloi Durant", "Jun Pang", "Nicolas Médoc", "Mohammad Ghoniem"], "categories": ["cs.HC", "cs.AI", "cs.LG", "stat.ML", "I.2.6; H.5.2"], "comment": "Accepted as a poster at IEEE VIS 2025 (\"TS-Insight: Visual\n  Fingerprinting of Multi-Armed Bandits\"). Open-source tool available at\n  https://github.com/LIST-LUXEMBOURG/ts-insight", "summary": "Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit\nalgorithms used to balance exploration and exploitation strategies in active\nlearning. Yet, their probabilistic nature often turns them into a \"black box\",\nhindering debugging and trust. We introduce TS-Insight, a visual analytics tool\nexplicitly designed to shed light on the internal decision mechanisms of\nThompson Sampling-based algorithms, for model developers. It comprises multiple\nplots, tracing for each arm the evolving posteriors, evidence counts, and\nsampling outcomes, enabling the verification, diagnosis, and explainability of\nexploration/exploitation dynamics. This tool aims at fostering trust and\nfacilitating effective debugging and deployment in complex binary\ndecision-making scenarios especially in sensitive domains requiring\ninterpretable decision-making."}
{"id": "2508.15202", "pdf": "https://arxiv.org/pdf/2508.15202.pdf", "abs": "https://arxiv.org/abs/2508.15202", "title": "Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models", "authors": ["Yuanchen Zhou", "Shuo Jiang", "Jie Zhu", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising framework for\nsupervising intermediate reasoning in large language models (LLMs), yet\nexisting PRMs are primarily trained on general or Science, Technology,\nEngineering, and Mathematics (STEM) domains and fall short in domain-specific\ncontexts such as finance, where reasoning is more structured, symbolic, and\nsensitive to factual and regulatory correctness. We introduce \\textbf{Fin-PRM},\na domain-specialized, trajectory-aware PRM tailored to evaluate intermediate\nreasoning steps in financial tasks. Fin-PRM integrates step-level and\ntrajectory-level reward supervision, enabling fine-grained evaluation of\nreasoning traces aligned with financial logic. We apply Fin-PRM in both offline\nand online reward learning settings, supporting three key applications: (i)\nselecting high-quality reasoning trajectories for distillation-based supervised\nfine-tuning, (ii) providing dense process-level rewards for reinforcement\nlearning, and (iii) guiding reward-informed Best-of-N inference at test time.\nExperimental results on financial reasoning benchmarks, including CFLUE and\nFinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs\nand strong domain baselines in trajectory selection quality. Downstream models\ntrained with Fin-PRM yield substantial improvements with baselines, with gains\nof 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in\ntest-time performance. These findings highlight the value of domain-specialized\nreward modeling for aligning LLMs with expert-level financial reasoning. Our\nproject resources will be available at https://github.com/aliyun/qwen-dianjin."}
{"id": "2508.12504", "pdf": "https://arxiv.org/pdf/2508.12504.pdf", "abs": "https://arxiv.org/abs/2508.12504", "title": "Organization Matters: A Qualitative Study of Organizational Dynamics in Red Teaming Practices for Generative AI", "authors": ["Bixuan Ren", "EunJeong Cheon", "Jianghui Li"], "categories": ["cs.HC"], "comment": null, "summary": "The rapid integration of generative artificial intelligence (GenAI) across\ndiverse fields underscores the critical need for red teaming efforts to\nproactively identify and mitigate associated risks. While previous research\nprimarily addresses technical aspects, this paper highlights organizational\nfactors that hinder the effectiveness of red teaming in real-world settings.\nThrough qualitative analysis of 17 semi-structured interviews with red teamers\nfrom various organizations, we uncover challenges such as the marginalization\nof vulnerable red teamers, the invisibility of nuanced AI risks to vulnerable\nusers until post-deployment, and a lack of user-centered red teaming\napproaches. These issues often arise from underlying organizational dynamics,\nincluding organizational resistance, organizational inertia, and organizational\nmediocracy. To mitigate these dynamics, we discuss the implications of user\nresearch for red teaming and the importance of embedding red teaming throughout\nthe entire development cycle of GenAI systems."}
{"id": "2508.15212", "pdf": "https://arxiv.org/pdf/2508.15212.pdf", "abs": "https://arxiv.org/abs/2508.15212", "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning", "authors": ["Huanxuan Liao", "Yixing Xu", "Shizhu He", "Guanchen Li", "Xuanwu Yin", "Dong Li", "Emad Barsoum", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."}
{"id": "2508.13509", "pdf": "https://arxiv.org/pdf/2508.13509.pdf", "abs": "https://arxiv.org/abs/2508.13509", "title": "koboshi: A Base That Animates Everyday Objects", "authors": ["Yuta Sugiura"], "categories": ["cs.HC"], "comment": null, "summary": "We propose a base-shaped robot named \"koboshi\" that moves everyday objects.\nThis koboshi has a spherical surface in contact with the floor, and by moving a\nweight inside using built-in motors, it can rock up and down, and side to side.\nBy placing everyday items on this koboshi, users can impart new movement to\notherwise static objects. The koboshi is equipped with sensors to measure its\nposture, enabling interaction with users. Additionally, it has communication\ncapabilities, allowing multiple units to communicate with each other."}
{"id": "2508.15213", "pdf": "https://arxiv.org/pdf/2508.15213.pdf", "abs": "https://arxiv.org/abs/2508.15213", "title": "Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering", "authors": ["Bolei He", "Xinran He", "Run Shao", "Shanfu Shu", "Xianwei Xue", "Mingquan Cheng", "Haifeng Li", "Zhenhua Ling"], "categories": ["cs.CL"], "comment": "EMNLP2025 Findings", "summary": "Large Language Models (LLMs) perform well in general QA but often struggle in\ndomain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces\nexternal knowledge but suffers from hallucinations and latency due to noisy\nretrievals. Continued pretraining internalizes domain knowledge but is costly\nand lacks cross-domain flexibility. We attribute this challenge to the\nlong-tail distribution of domain knowledge, which leaves partial yet useful\ninternal knowledge underutilized. We further argue that knowledge acquisition\nshould be progressive, mirroring human learning: first understanding concepts,\nthen applying them to complex reasoning. To address this, we propose Selct2Know\n(S2K), a cost-effective framework that internalizes domain knowledge through an\ninternal-external knowledge self-selection strategy and selective supervised\nfine-tuning. We also introduce a structured reasoning data generation pipeline\nand integrate GRPO to enhance reasoning ability. Experiments on medical, legal,\nand financial QA benchmarks show that S2K consistently outperforms existing\nmethods and matches domain-pretrained LLMs with significantly lower cost."}
{"id": "2404.18021", "pdf": "https://arxiv.org/pdf/2404.18021.pdf", "abs": "https://arxiv.org/abs/2404.18021", "title": "CRISPR-GPT for Agentic Automation of Gene-editing Experiments", "authors": ["Yuanhao Qu", "Kaixuan Huang", "Ming Yin", "Kanghong Zhan", "Dyllan Liu", "Di Yin", "Henry C. Cousins", "William A. Johnson", "Xiaotong Wang", "Mihir Shah", "Russ B. Altman", "Denny Zhou", "Mengdi Wang", "Le Cong"], "categories": ["cs.AI", "cs.CL", "cs.HC", "q-bio.QM"], "comment": "Accepted to Nature Biomedical Engineering", "summary": "The introduction of genome engineering technology has transformed biomedical\nresearch, making it possible to make precise changes to genetic information.\nHowever, creating an efficient gene-editing system requires a deep\nunderstanding of CRISPR technology, and the complex experimental systems under\ninvestigation. While Large Language Models (LLMs) have shown promise in various\ntasks, they often lack specific knowledge and struggle to accurately solve\nbiological design problems. In this work, we introduce CRISPR-GPT, an LLM agent\naugmented with domain knowledge and external tools to automate and enhance the\ndesign process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages\nthe reasoning ability of LLMs to facilitate the process of selecting CRISPR\nsystems, designing guide RNAs, recommending cellular delivery methods, drafting\nprotocols, and designing validation experiments to confirm editing outcomes. We\nshowcase the potential of CRISPR-GPT for assisting non-expert researchers with\ngene-editing experiments from scratch and validate the agent's effectiveness in\na real-world use case. Furthermore, we explore the ethical and regulatory\nconsiderations associated with automated gene-editing design, highlighting the\nneed for responsible and transparent use of these tools. Our work aims to\nbridge the gap between beginner biological researchers and CRISPR genome\nengineering techniques, and demonstrate the potential of LLM agents in\nfacilitating complex biological discovery tasks. The published version of this\ndraft is available at https://www.nature.com/articles/s41551-025-01463-z."}
{"id": "2508.15214", "pdf": "https://arxiv.org/pdf/2508.15214.pdf", "abs": "https://arxiv.org/abs/2508.15214", "title": "Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall", "authors": ["Sijia Cui", "Aiyao He", "Shuai Xu", "Hongming Zhang", "Yanna Wang", "Qingyang Zhang", "Yajing Wang", "Bo Xu"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025", "summary": "Function calling enables large language models (LLMs) to interact with\nexternal systems by leveraging tools and APIs. When faced with multi-step tool\nusage, LLMs still struggle with tool selection, parameter generation, and\ntool-chain planning. Existing methods typically rely on manually designing\ntask-specific demonstrations, or retrieving from a curated library. These\napproaches demand substantial expert effort and prompt engineering becomes\nincreasingly complex and inefficient as tool diversity and task difficulty\nscale. To address these challenges, we propose a self-guided method, Stepwise\nExperience Recall (SEER), which performs fine-grained, stepwise retrieval from\na continually updated experience pool. Instead of relying on static or manually\ncurated library, SEER incrementally augments the experience pool with past\nsuccessful trajectories, enabling continuous expansion of the pool and improved\nmodel performance over time. Evaluated on the ToolQA benchmark, SEER achieves\nan average improvement of 6.1\\% on easy and 4.7\\% on hard questions. We further\ntest SEER on $\\tau$-bench, which includes two real-world domains. Powered by\nQwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains\nof 7.44\\% and 23.38\\%, respectively."}
{"id": "2506.15928", "pdf": "https://arxiv.org/pdf/2506.15928.pdf", "abs": "https://arxiv.org/abs/2506.15928", "title": "Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues", "authors": ["Myke C. Cohen", "Zhe Su", "Hsien-Te Kao", "Daniel Nguyen", "Spencer Lynch", "Maarten Sap", "Svitlana Volkova"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Presented at the KDD 2025 Workshop on Evaluation and Trustworthiness\n  of Agentic and Generative AI Models under the title \"Evaluating the\n  LLM-simulated Impacts of Big Five Personality Traits and AI Capabilities on\n  Social Negotiations\"\n  (https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/assets/papers/Submission%2036.pdf)", "summary": "This paper presents an evaluation framework for agentic AI systems in\nmission-critical negotiation contexts, addressing the need for AI agents that\ncan adapt to diverse human operators and stakeholders. Using Sotopia as a\nsimulation testbed, we present two experiments that systematically evaluated\nhow personality traits and AI agent characteristics influence LLM-simulated\nsocial negotiation outcomes--a capability essential for a variety of\napplications involving cross-team coordination and civil-military interactions.\nExperiment 1 employs causal discovery methods to measure how personality traits\nimpact price bargaining negotiations, through which we found that Agreeableness\nand Extraversion significantly affect believability, goal achievement, and\nknowledge acquisition outcomes. Sociocognitive lexical measures extracted from\nteam communications detected fine-grained differences in agents' empathic\ncommunication, moral foundations, and opinion patterns, providing actionable\ninsights for agentic AI systems that must operate reliably in high-stakes\noperational scenarios. Experiment 2 evaluates human-AI job negotiations by\nmanipulating both simulated human personality and AI system characteristics,\nspecifically transparency, competence, adaptability, demonstrating how AI agent\ntrustworthiness impact mission effectiveness. These findings establish a\nrepeatable evaluation methodology for experimenting with AI agent reliability\nacross diverse operator personalities and human-agent team dynamics, directly\nsupporting operational requirements for reliable AI systems. Our work advances\nthe evaluation of agentic AI workflows by moving beyond standard performance\nmetrics to incorporate social dynamics essential for mission success in complex\noperations."}
{"id": "2508.15218", "pdf": "https://arxiv.org/pdf/2508.15218.pdf", "abs": "https://arxiv.org/abs/2508.15218", "title": "Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?", "authors": ["Momoka Furuhashi", "Kouta Nakayama", "Takashi Kodama", "Saku Sugawara"], "categories": ["cs.CL"], "comment": "Accepted to the EMNLP 2025 Main Conference", "summary": "Automatic evaluation of generative tasks using large language models faces\nchallenges due to ambiguous criteria. Although automatic checklist generation\nis a potentially promising approach, its usefulness remains underexplored. We\ninvestigate whether checklists should be used for all questions or selectively,\ngenerate them using six methods, evaluate their effectiveness across eight\nmodel sizes, and identify checklist items that correlate with human\nevaluations. Through experiments on pairwise comparison and direct scoring\ntasks, we find that selective checklist use tends to improve evaluation\nperformance in pairwise settings, while its benefits are less consistent in\ndirect scoring. Our analysis also shows that even checklist items with low\ncorrelation to human scores often reflect human-written criteria, indicating\npotential inconsistencies in human evaluation. These findings highlight the\nneed to more clearly define objective evaluation criteria to guide both human\nand automatic evaluations. \\footnote{Our code is available\nat~https://github.com/momo0817/checklist-effectiveness-study"}
{"id": "2508.14119", "pdf": "https://arxiv.org/pdf/2508.14119.pdf", "abs": "https://arxiv.org/abs/2508.14119", "title": "Documenting Deployment with Fabric: A Repository of Real-World AI Governance", "authors": ["Mackenzie Jorgensen", "Kendall Brogle", "Katherine M. Collins", "Lujain Ibrahim", "Arina Shah", "Petra Ivanovic", "Noah Broestl", "Gabriel Piles", "Paul Dongha", "Hatim Abdulhussein", "Adrian Weller", "Jillian Powers", "Umang Bhatt"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "AIES 2025", "summary": "Artificial intelligence (AI) is increasingly integrated into society, from\nfinancial services and traffic management to creative writing. Academic\nliterature on the deployment of AI has mostly focused on the risks and harms\nthat result from the use of AI. We introduce Fabric, a publicly available\nrepository of deployed AI use cases to outline their governance mechanisms.\nThrough semi-structured interviews with practitioners, we collect an initial\nset of 20 AI use cases. In addition, we co-design diagrams of the AI workflow\nwith the practitioners. We discuss the oversight mechanisms and guardrails used\nin practice to safeguard AI use. The Fabric repository includes visual diagrams\nof AI use cases and descriptions of the deployed systems. Using the repository,\nwe surface gaps in governance and find common patterns in human oversight of\ndeployed AI systems. We intend for Fabric to serve as an extendable, evolving\ntool for researchers to study the effectiveness of AI governance."}
{"id": "2508.15229", "pdf": "https://arxiv.org/pdf/2508.15229.pdf", "abs": "https://arxiv.org/abs/2508.15229", "title": "VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models", "authors": ["Hanling Zhang", "Yayu Zhou", "Tongcheng Fang", "Zhihang Yuan", "Guohao Dai", "Yu Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Small Language Models (SLMs) provide computational advantages in\nresource-constrained environments, yet memory limitations remain a critical\nbottleneck for edge device deployment. A substantial portion of SLMs' memory\nfootprint stems from vocabulary-related components, particularly embeddings and\nlanguage modeling (LM) heads, due to large vocabulary sizes. Existing static\nvocabulary pruning, while reducing memory usage, suffers from rigid,\none-size-fits-all designs that cause information loss from the prefill stage\nand a lack of flexibility. In this work, we identify two key principles\nunderlying the vocabulary reduction challenge: the lexical locality principle,\nthe observation that only a small subset of tokens is required during any\nsingle inference, and the asymmetry in computational characteristics between\nvocabulary-related components of SLM. Based on these insights, we introduce\nVocabTailor, a novel decoupled dynamic vocabulary selection framework that\naddresses memory constraints through offloading embedding and implements a\nhybrid static-dynamic vocabulary selection strategy for LM Head, enabling\non-demand loading of vocabulary components. Comprehensive experiments across\ndiverse downstream tasks demonstrate that VocabTailor achieves a reduction of\nup to 99% in the memory usage of vocabulary-related components with minimal or\nno degradation in task performance, substantially outperforming existing static\nvocabulary pruning."}
{"id": "2508.15239", "pdf": "https://arxiv.org/pdf/2508.15239.pdf", "abs": "https://arxiv.org/abs/2508.15239", "title": "WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai", "authors": ["Peerat Limkonchotiwat", "Pume Tuchinda", "Lalita Lowphansirikul", "Surapon Nonesung", "Panuthep Tasawong", "Alham Fikri Aji", "Can Udomcharoenchaikit", "Sarana Nutanong"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 (Main). Model and Dataset:\n  https://huggingface.co/collections/airesearch/wangchan-thai-instruction-6835722a30b98e01598984fd", "summary": "Large language models excel at instruction-following in English, but their\nperformance in low-resource languages like Thai remains underexplored. Existing\nbenchmarks often rely on translations, missing cultural and domain-specific\nnuances needed for real-world use. We present WangchanThaiInstruct, a\nhuman-authored Thai dataset for evaluation and instruction tuning, covering\nfour professional domains and seven task types. Created through a multi-stage\nquality control process with annotators, domain experts, and AI researchers,\nWangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing\nperformance gaps on culturally and professionally specific tasks, and (2) an\ninstruction tuning study with ablations isolating the effect of native\nsupervision. Models fine-tuned on WangchanThaiInstruct outperform those using\ntranslated data in both in-domain and out-of-domain benchmarks. These findings\nunderscore the need for culturally and professionally grounded instruction data\nto improve LLM alignment in low-resource, linguistically diverse settings."}
{"id": "2508.15244", "pdf": "https://arxiv.org/pdf/2508.15244.pdf", "abs": "https://arxiv.org/abs/2508.15244", "title": "UniCoM: A Universal Code-Switching Speech Generator", "authors": ["Sangmin Lee", "Woojin Chung", "Seyun Um", "Hong-Goo Kang"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Code-switching (CS), the alternation between two or more languages within a\nsingle speaker's utterances, is common in real-world conversations and poses\nsignificant challenges for multilingual speech technology. However, systems\ncapable of handling this phenomenon remain underexplored, primarily due to the\nscarcity of suitable datasets. To resolve this issue, we propose Universal\nCode-Mixer (UniCoM), a novel pipeline for generating high-quality, natural CS\nsamples without altering sentence semantics. Our approach utilizes an algorithm\nwe call Substituting WORDs with Synonyms (SWORDS), which generates CS speech by\nreplacing selected words with their translations while considering their parts\nof speech. Using UniCoM, we construct Code-Switching FLEURS (CS-FLEURS), a\nmultilingual CS corpus designed for automatic speech recognition (ASR) and\nspeech-to-text translation (S2TT). Experimental results show that CS-FLEURS\nachieves high intelligibility and naturalness, performing comparably to\nexisting datasets on both objective and subjective metrics. We expect our\napproach to advance CS speech technology and enable more inclusive multilingual\nsystems."}
{"id": "2508.15250", "pdf": "https://arxiv.org/pdf/2508.15250.pdf", "abs": "https://arxiv.org/abs/2508.15250", "title": "EMNLP: Educator-role Moral and Normative Large Language Models Profiling", "authors": ["Yilin Jiang", "Mingzi Zhang", "Sheng Jin", "Zengyi Yu", "Xiangjie Kong", "Binghao Tu"], "categories": ["cs.CL", "I.2.7"], "comment": "24pages, 12 figures, Accepted by EMNLP Main Confrence", "summary": "Simulating Professions (SP) enables Large Language Models (LLMs) to emulate\nprofessional roles. However, comprehensive psychological and ethical evaluation\nin these contexts remains lacking. This paper introduces EMNLP, an\nEducator-role Moral and Normative LLMs Profiling framework for personality\nprofiling, moral development stage measurement, and ethical risk under soft\nprompt injection. EMNLP extends existing scales and constructs 88\nteacher-specific moral dilemmas, enabling profession-oriented comparison with\nhuman teachers. A targeted soft prompt injection set evaluates compliance and\nvulnerability in teacher SP. Experiments on 12 LLMs show teacher-role LLMs\nexhibit more idealized and polarized personalities than human teachers, excel\nin abstract moral reasoning, but struggle with emotionally complex situations.\nModels with stronger reasoning are more vulnerable to harmful prompt injection,\nrevealing a paradox between capability and safety. The model temperature and\nother hyperparameters have limited influence except in some risk behaviors.\nThis paper presents the first benchmark to assess ethical and psychological\nalignment of teacher-role LLMs for educational AI. Resources are available at\nhttps://e-m-n-l-p.github.io/."}
{"id": "2508.15253", "pdf": "https://arxiv.org/pdf/2508.15253.pdf", "abs": "https://arxiv.org/abs/2508.15253", "title": "Conflict-Aware Soft Prompting for Retrieval-Augmented Generation", "authors": ["Eunseong Choi", "June Park", "Hyeri Lee", "Jongwuk Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025; 14 pages; 5 figures, 11 tables", "summary": "Retrieval-augmented generation (RAG) enhances the capabilities of large\nlanguage models (LLMs) by incorporating external knowledge into their input\nprompts. However, when the retrieved context contradicts the LLM's parametric\nknowledge, it often fails to resolve the conflict between incorrect external\ncontext and correct parametric knowledge, known as context-memory conflict. To\ntackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation\n(CARE), consisting of a context assessor and a base LLM. The context assessor\nencodes compact memory token embeddings from raw context tokens. Through\ngrounded/adversarial soft prompting, the context assessor is trained to discern\nunreliable context and capture a guidance signal that directs reasoning toward\nthe more reliable knowledge source. Extensive experiments show that CARE\neffectively mitigates context-memory conflicts, leading to an average\nperformance gain of 5.0\\% on QA and fact-checking benchmarks, establishing a\npromising direction for trustworthy and adaptive RAG systems."}
{"id": "2508.15274", "pdf": "https://arxiv.org/pdf/2508.15274.pdf", "abs": "https://arxiv.org/abs/2508.15274", "title": "TComQA: Extracting Temporal Commonsense from Text", "authors": ["Lekshmi R Nair", "Arun Sankar", "Koninika Pal"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Understanding events necessitates grasping their temporal context, which is\noften not explicitly stated in natural language. For example, it is not a\ntrivial task for a machine to infer that a museum tour may last for a few\nhours, but can not take months. Recent studies indicate that even advanced\nlarge language models (LLMs) struggle in generating text that require reasoning\nwith temporal commonsense due to its infrequent explicit mention in text.\nTherefore, automatically mining temporal commonsense for events enables the\ncreation of robust language models. In this work, we investigate the capacity\nof LLMs to extract temporal commonsense from text and evaluate multiple\nexperimental setups to assess their effectiveness. Here, we propose a temporal\ncommonsense extraction pipeline that leverages LLMs to automatically mine\ntemporal commonsense and use it to construct TComQA, a dataset derived from\nSAMSum and RealNews corpora. TComQA has been validated through crowdsourcing\nand achieves over 80\\% precision in extracting temporal commonsense. The model\ntrained with TComQA also outperforms an LLM fine-tuned on existing dataset of\ntemporal question answering task."}
{"id": "2508.15316", "pdf": "https://arxiv.org/pdf/2508.15316.pdf", "abs": "https://arxiv.org/abs/2508.15316", "title": "CUPE: Contextless Universal Phoneme Encoder for Language-Agnostic Speech Processing", "authors": ["Abdul Rehman", "Jian-Jun Zhang", "Xiaosong Yang"], "categories": ["cs.CL", "cs.LG", "eess.AS", "I.2.7"], "comment": "Accepted in: 8th International Conference on Natural Language and\n  Speech Processing (ICNLSP 2025)", "summary": "Universal phoneme recognition typically requires analyzing long speech\nsegments and language-specific patterns. Many speech processing tasks require\npure phoneme representations free from contextual influence, which motivated\nour development of CUPE - a lightweight model that captures key phoneme\nfeatures in just 120 milliseconds, about one phoneme's length. CUPE processes\nshort, fixed-width windows independently and, despite fewer parameters than\ncurrent approaches, achieves competitive cross-lingual performance by learning\nfundamental acoustic patterns common to all languages. Our extensive evaluation\nthrough supervised and self-supervised training on diverse languages, including\nzero-shot tests on the UCLA Phonetic Corpus, demonstrates strong cross-lingual\ngeneralization and reveals that effective universal speech processing is\npossible through modeling basic acoustic patterns within phoneme-length\nwindows."}
{"id": "2508.15357", "pdf": "https://arxiv.org/pdf/2508.15357.pdf", "abs": "https://arxiv.org/abs/2508.15357", "title": "KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models", "authors": ["Haji Gul", "Abul Ghani Naim", "Ajaz Ahmad Bhat"], "categories": ["cs.CL", "cs.PF"], "comment": null, "summary": "Knowledge Graphs (KGs) enable applications in various domains such as\nsemantic search, recommendation systems, and natural language processing. KGs\nare often incomplete, missing entities and relations, an issue addressed by\nKnowledge Graph Completion (KGC) methods that predict missing elements.\nDifferent evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank\n(MR), and Hit@k, are commonly used to assess the performance of such KGC\nmodels. A major challenge in evaluating KGC models, however, lies in comparing\ntheir performance across multiple datasets and metrics. A model may outperform\nothers on one dataset but underperform on another, making it difficult to\ndetermine overall superiority. Moreover, even within a single dataset,\ndifferent metrics such as MRR and Hit@1 can yield conflicting rankings, where\none model excels in MRR while another performs better in Hit@1, further\ncomplicating model selection for downstream tasks. These inconsistencies hinder\nholistic comparisons and highlight the need for a unified meta-metric that\nintegrates performance across all metrics and datasets to enable a more\nreliable and interpretable evaluation framework. To address this need, we\npropose KG Evaluation based on Distance from Average Solution (EDAS), a robust\nand interpretable meta-metric that synthesizes model performance across\nmultiple datasets and diverse evaluation criteria into a single normalized\nscore ($M_i \\in [0,1]$). Unlike traditional metrics that focus on isolated\naspects of performance, EDAS offers a global perspective that supports more\ninformed model selection and promotes fairness in cross-dataset evaluation.\nExperimental results on benchmark datasets such as FB15k-237 and WN18RR\ndemonstrate that EDAS effectively integrates multi-metric, multi-dataset\nperformance into a unified ranking, offering a consistent, robust, and\ngeneralizable framework for evaluating KGC models."}
{"id": "2508.15361", "pdf": "https://arxiv.org/pdf/2508.15361.pdf", "abs": "https://arxiv.org/abs/2508.15361", "title": "A Survey on Large Language Model Benchmarks", "authors": ["Shiwen Ni", "Guhong Chen", "Shuaimin Li", "Xuanang Chen", "Siyi Li", "Bingli Wang", "Qiyao Wang", "Xingjian Wang", "Yifan Zhang", "Liyang Fan", "Chengming Li", "Ruifeng Xu", "Le Sun", "Min Yang"], "categories": ["cs.CL"], "comment": null, "summary": "In recent years, with the rapid development of the depth and breadth of large\nlanguage models' capabilities, various corresponding evaluation benchmarks have\nbeen emerging in increasing numbers. As a quantitative assessment tool for\nmodel performance, benchmarks are not only a core means to measure model\ncapabilities but also a key element in guiding the direction of model\ndevelopment and promoting technological innovation. We systematically review\nthe current status and development of large language model benchmarks for the\nfirst time, categorizing 283 representative benchmarks into three categories:\ngeneral capabilities, domain-specific, and target-specific. General capability\nbenchmarks cover aspects such as core linguistics, knowledge, and reasoning;\ndomain-specific benchmarks focus on fields like natural sciences, humanities\nand social sciences, and engineering technology; target-specific benchmarks pay\nattention to risks, reliability, agents, etc. We point out that current\nbenchmarks have problems such as inflated scores caused by data contamination,\nunfair evaluation due to cultural and linguistic biases, and lack of evaluation\non process credibility and dynamic environments, and provide a referable design\nparadigm for future benchmark innovation."}
{"id": "2508.15370", "pdf": "https://arxiv.org/pdf/2508.15370.pdf", "abs": "https://arxiv.org/abs/2508.15370", "title": "Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation", "authors": ["Yichi Zhang", "Yao Huang", "Yifan Wang", "Yitong Sun", "Chang Liu", "Zhe Zhao", "Zhengwei Fang", "Huanran Chen", "Xiao Yang", "Xingxing Wei", "Hang Su", "Yinpeng Dong", "Jun Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "For Appendix, please refer to arXiv:2406.07057", "summary": "The trustworthiness of Multimodal Large Language Models (MLLMs) remains an\nintense concern despite the significant progress in their capabilities.\nExisting evaluation and mitigation approaches often focus on narrow aspects and\noverlook risks introduced by the multimodality. To tackle these challenges, we\npropose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and\nmitigating the trustworthiness issues of MLLMs. We define a three-dimensional\nframework, encompassing five trustworthiness aspects which include\ntruthfulness, robustness, safety, fairness, and privacy; two novel risk types\ncovering multimodal risks and cross-modal impacts; and various mitigation\nstrategies from the perspectives of data, model architecture, training, and\ninference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and\n28 curated datasets, enabling holistic evaluations over 30 open-source and\nproprietary MLLMs and in-depth analysis with 8 representative mitigation\nmethods. Our extensive experiments reveal significant vulnerabilities in\ncurrent models, including a gap between trustworthiness and general\ncapabilities, as well as the amplification of potential risks in base LLMs by\nboth multimodal training and inference. Moreover, our controlled analysis\nuncovers key limitations in existing mitigation strategies that, while some\nmethods yield improvements in specific aspects, few effectively address overall\ntrustworthiness, and many introduce unexpected trade-offs that compromise model\nutility. These findings also provide practical insights for future\nimprovements, such as the benefits of reasoning to better balance safety and\nperformance. Based on these insights, we introduce a Reasoning-Enhanced Safety\nAlignment (RESA) approach that equips the model with chain-of-thought reasoning\nability to discover the underlying risks, achieving state-of-the-art results."}
{"id": "2508.15371", "pdf": "https://arxiv.org/pdf/2508.15371.pdf", "abs": "https://arxiv.org/abs/2508.15371", "title": "Confidence-Modulated Speculative Decoding for Large Language Models", "authors": ["Jaydip Sen", "Subhasis Dasgupta", "Hetvi Waghela"], "categories": ["cs.CL"], "comment": "This is the preprint of the paper, which has been accepted for oral\n  presentation and publication in the proceedings of IEEE INDISCON 2025. The\n  conference will be organized at the National Institute of Technology,\n  Rourkela, India, from August 21 to 23, 2025. The paper is 10 pages long, and\n  it contains 2 figures and 5 tables", "summary": "Speculative decoding has emerged as an effective approach for accelerating\nautoregressive inference by parallelizing token generation through a\ndraft-then-verify paradigm. However, existing methods rely on static drafting\nlengths and rigid verification criteria, limiting their adaptability across\nvarying model uncertainties and input complexities. This paper proposes an\ninformation-theoretic framework for speculative decoding based on\nconfidence-modulated drafting. By leveraging entropy and margin-based\nuncertainty measures over the drafter's output distribution, the proposed\nmethod dynamically adjusts the number of speculatively generated tokens at each\niteration. This adaptive mechanism reduces rollback frequency, improves\nresource utilization, and maintains output fidelity. Additionally, the\nverification process is modulated using the same confidence signals, enabling\nmore flexible acceptance of drafted tokens without sacrificing generation\nquality. Experiments on machine translation and summarization tasks demonstrate\nsignificant speedups over standard speculative decoding while preserving or\nimproving BLEU and ROUGE scores. The proposed approach offers a principled,\nplug-in method for efficient and robust decoding in large language models under\nvarying conditions of uncertainty."}
{"id": "2508.15390", "pdf": "https://arxiv.org/pdf/2508.15390.pdf", "abs": "https://arxiv.org/abs/2508.15390", "title": "Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training", "authors": ["Woojin Chung", "Jeonghoon Kim"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Large language models are trained with tokenizers, and the resulting token\ndistribution is highly imbalanced: a few words dominate the stream while most\noccur rarely. Recent practice favors ever-larger vocabularies, but the source\nof the benefit is unclear. We conduct a controlled study that scales the\nlanguage model's vocabulary from 24K to 196K while holding data, compute, and\noptimization fixed. We first quantify the complexity of tokenized text,\nformalized via Kolmogorov complexity, and show that larger vocabularies reduce\nthis complexity. Above 24K, every common word is already a single token, so\nfurther growth mainly deepens the relative token-frequency imbalance. A\nword-level loss decomposition shows that larger vocabularies reduce\ncross-entropy almost exclusively by lowering uncertainty on the 2,500 most\nfrequent words, even though loss on the rare tail rises. Constraining input and\noutput embedding norms to attenuate the effect of token-frequency imbalance\nreverses the gain, directly showing that the model exploits rather than suffers\nfrom imbalance. Because the same frequent words cover roughly 77% of tokens in\ndownstream benchmarks, this training advantage transfers intact. We also show\nthat enlarging model parameters with a fixed vocabulary yields the same\nfrequent-word benefit. Our results reframe \"bigger vocabularies help\" as\n\"lowering the complexity of tokenized text helps,\" providing a simple,\nprincipled lever for tokenizer-model co-design and clarifying the loss dynamics\nthat govern language-model scaling in pre-training."}
{"id": "2508.15396", "pdf": "https://arxiv.org/pdf/2508.15396.pdf", "abs": "https://arxiv.org/abs/2508.15396", "title": "Attribution, Citation, and Quotation: A Survey of Evidence-based Text Generation with Large Language Models", "authors": ["Tobias Schreieder", "Tim Schopf", "Michael Färber"], "categories": ["cs.CL"], "comment": null, "summary": "The increasing adoption of large language models (LLMs) has been accompanied\nby growing concerns regarding their reliability and trustworthiness. As a\nresult, a growing body of research focuses on evidence-based text generation\nwith LLMs, aiming to link model outputs to supporting evidence to ensure\ntraceability and verifiability. However, the field is fragmented due to\ninconsistent terminology, isolated evaluation practices, and a lack of unified\nbenchmarks. To bridge this gap, we systematically analyze 134 papers, introduce\na unified taxonomy of evidence-based text generation with LLMs, and investigate\n300 evaluation metrics across seven key dimensions. Thereby, we focus on\napproaches that use citations, attribution, or quotations for evidence-based\ntext generation. Building on this, we examine the distinctive characteristics\nand representative methods in the field. Finally, we highlight open challenges\nand outline promising directions for future work."}
{"id": "2508.15407", "pdf": "https://arxiv.org/pdf/2508.15407.pdf", "abs": "https://arxiv.org/abs/2508.15407", "title": "When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models", "authors": ["Cheng Wang", "Gelei Deng", "Xianglin Yang", "Han Qiu", "Tianwei Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025 Main", "summary": "Large Audio-Language Models (LALMs) are enhanced with audio perception\ncapabilities, enabling them to effectively process and understand multimodal\ninputs that combine audio and text. However, their performance in handling\nconflicting information between audio and text modalities remains largely\nunexamined. This paper introduces MCR-BENCH, the first comprehensive benchmark\nspecifically designed to evaluate how LALMs prioritize information when\npresented with inconsistent audio-text pairs. Through extensive evaluation\nacross diverse audio understanding tasks, we reveal a concerning phenomenon:\nwhen inconsistencies exist between modalities, LALMs display a significant bias\ntoward textual input, frequently disregarding audio evidence. This tendency\nleads to substantial performance degradation in audio-centric tasks and raises\nimportant reliability concerns for real-world applications. We further\ninvestigate the influencing factors of text bias, and explore mitigation\nstrategies through supervised finetuning, and analyze model confidence patterns\nthat reveal persistent overconfidence even with contradictory inputs. These\nfindings underscore the need for improved modality balance during training and\nmore sophisticated fusion mechanisms to enhance the robustness when handling\nconflicting multi-modal inputs. The project is available at\nhttps://github.com/WangCheng0116/MCR-BENCH."}
{"id": "2508.15418", "pdf": "https://arxiv.org/pdf/2508.15418.pdf", "abs": "https://arxiv.org/abs/2508.15418", "title": "LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model", "authors": ["Yirong Sun", "Yizhong Geng", "Peidong Wei", "Yanjun Chen", "Jinghan Yang", "Rongfei Chen", "Wei Zhang", "Xiaoyu Shen"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "comment": null, "summary": "The development of Large Speech-Language Models (LSLMs) has been slowed by\nfragmented architectures and a lack of transparency, hindering the systematic\ncomparison and reproducibility of research. Unlike in the vision-language\ndomain, the LSLM field suffers from the common practice of releasing model\nweights without their corresponding training data and configurations. To\naddress these critical gaps, we introduce LLaSO, the first fully open,\nend-to-end framework for large-scale speech-language modeling. LLaSO provides\nthe community with three essential resources: (1) LLaSO-Align, a 12M-instance\nspeech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task\ninstruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for\nstandardized evaluation. To validate our framework, we build and release\nLLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public\ndata. It achieves a normalized score of 0.72, establishing a strong,\nreproducible baseline that surpasses comparable models. Our analysis reveals\nthat while broader training coverage enhances performance, significant\ngeneralization gaps persist on unseen tasks, particularly in pure audio\nscenarios. By releasing the complete stack of data, benchmarks, and models,\nLLaSO establishes a foundational open standard to unify research efforts and\naccelerate community-driven progress in LSLMs. We release the code, dataset,\npretrained models, and results in https://github.com/EIT-NLP/LLaSO."}
{"id": "2508.15421", "pdf": "https://arxiv.org/pdf/2508.15421.pdf", "abs": "https://arxiv.org/abs/2508.15421", "title": "A Study of Privacy-preserving Language Modeling Approaches", "authors": ["Pritilata Saha", "Abhirup Sinha"], "categories": ["cs.CL"], "comment": null, "summary": "Recent developments in language modeling have increased their use in various\napplications and domains. Language models, often trained on sensitive data, can\nmemorize and disclose this information during privacy attacks, raising concerns\nabout protecting individuals' privacy rights. Preserving privacy in language\nmodels has become a crucial area of research, as privacy is one of the\nfundamental human rights. Despite its significance, understanding of how much\nprivacy risk these language models possess and how it can be mitigated is still\nlimited. This research addresses this by providing a comprehensive study of the\nprivacy-preserving language modeling approaches. This study gives an in-depth\noverview of these approaches, highlights their strengths, and investigates\ntheir limitations. The outcomes of this study contribute to the ongoing\nresearch on privacy-preserving language modeling, providing valuable insights\nand outlining future research directions."}
{"id": "2508.15440", "pdf": "https://arxiv.org/pdf/2508.15440.pdf", "abs": "https://arxiv.org/abs/2508.15440", "title": "M-HELP: Using Social Media Data to Detect Mental Health Help-Seeking Signals", "authors": ["MSVPJ Sathvik", "Zuhair Hasan Shaik", "Vivek Gupta"], "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025", "summary": "Mental health disorders are a global crisis. While various datasets exist for\ndetecting such disorders, there remains a critical gap in identifying\nindividuals actively seeking help. This paper introduces a novel dataset,\nM-Help, specifically designed to detect help-seeking behavior on social media.\nThe dataset goes beyond traditional labels by identifying not only help-seeking\nactivity but also specific mental health disorders and their underlying causes,\nsuch as relationship challenges or financial stressors. AI models trained on\nM-Help can address three key tasks: identifying help-seekers, diagnosing mental\nhealth conditions, and uncovering the root causes of issues."}
{"id": "2508.15453", "pdf": "https://arxiv.org/pdf/2508.15453.pdf", "abs": "https://arxiv.org/abs/2508.15453", "title": "Principle Methods of Rendering Non-equivalent Words from Uzbek and Dari to Russian and English", "authors": ["Mohammad Ibrahim Qani"], "categories": ["cs.CL", "Translating nonequivalent words"], "comment": "Fully abstract is available in the attached file", "summary": "These pure languages understanding directly relates to translation knowledge\nwhere linguists and translators need to work and research to eradicate\nmisunderstanding. Misunderstandings mostly appear in non-equivalent words\nbecause there are different local and internal words like food, garment,\ncultural and traditional words and others in every notion. Truly, most of these\nwords do not have equivalent in the target language and these words need to be\nworked and find their equivalent in the target language to fully understand the\nboth languages. The purpose of this research is to introduce the methods of\nrendering non-equivalent words professionally from the source language to the\ntarget language and this research has been completed using library-based\nresearch. However, some of these non-equivalent words are already\nprofessionally rendered to the target language but still there many other words\nto be rendered. As a result, this research paper includes different ways and\nrules of rendering non-equivalent words from source language to the target\nlanguage and 25 non-equvalent words have been rendered from Dar & Uzbek into\nEnglish and Russian languages."}
{"id": "2508.15456", "pdf": "https://arxiv.org/pdf/2508.15456.pdf", "abs": "https://arxiv.org/abs/2508.15456", "title": "PyTOD: Programmable Task-Oriented Dialogue with Execution Feedback", "authors": ["Alexandru Coca", "Bo-Hsiang Tseng", "Pete Boothroyd", "Jianpeng Cheng", "Mark Gaynor", "Zhenxing Zhang", "Joe Stacey", "Tristan Guigue", "Héctor Martinez Alonso", "Diarmuid Ó Séaghdha", "Anders Johannsen"], "categories": ["cs.CL"], "comment": "20 pages, 12 figures. To appear at SIGDIAL 2025", "summary": "Programmable task-oriented dialogue (TOD) agents enable language models to\nfollow structured dialogue policies, but their effectiveness hinges on accurate\nstate tracking. We present PyTOD, an agent that generates executable code to\ntrack dialogue state and uses policy and execution feedback for efficient error\ncorrection. To this end, PyTOD employs a simple constrained decoding approach,\nusing a language model instead of grammar rules to follow API schemata. This\nleads to state-of-the-art state tracking performance on the challenging SGD\nbenchmark. Our experiments show that PyTOD surpasses strong baselines in both\naccuracy and robust user goal estimation as the dialogue progresses,\ndemonstrating the effectiveness of execution-aware state tracking."}
{"id": "2508.15464", "pdf": "https://arxiv.org/pdf/2508.15464.pdf", "abs": "https://arxiv.org/abs/2508.15464", "title": "RadReason: Radiology Report Evaluation Metric with Reasons and Sub-Scores", "authors": ["Yingshu Li", "Yunyi Liu", "Lingqiao Liu", "Lei Wang", "Luping Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating automatically generated radiology reports remains a fundamental\nchallenge due to the lack of clinically grounded, interpretable, and\nfine-grained metrics. Existing methods either produce coarse overall scores or\nrely on opaque black-box models, limiting their usefulness in real-world\nclinical workflows. We introduce RadReason, a novel evaluation framework for\nradiology reports that not only outputs fine-grained sub-scores across six\nclinically defined error types, but also produces human-readable justifications\nthat explain the rationale behind each score. Our method builds on Group\nRelative Policy Optimization and incorporates two key innovations: (1)\nSub-score Dynamic Weighting, which adaptively prioritizes clinically\nchallenging error types based on live F1 statistics; and (2) Majority-Guided\nAdvantage Scaling, which adjusts policy gradient updates based on prompt\ndifficulty derived from sub-score agreement. Together, these components enable\nmore stable optimization and better alignment with expert clinical judgment.\nExperiments on the ReXVal benchmark show that RadReason surpasses all prior\noffline metrics and achieves parity with GPT-4-based evaluations, while\nremaining explainable, cost-efficient, and suitable for clinical deployment.\nCode will be released upon publication."}
{"id": "2508.15471", "pdf": "https://arxiv.org/pdf/2508.15471.pdf", "abs": "https://arxiv.org/abs/2508.15471", "title": "SLM4Offer: Personalized Marketing Offer Generation Using Contrastive Learning Based Fine-Tuning", "authors": ["Vedasamhitha Challapalli", "Konduru Venkat Sai", "Piyush Pratap Singh", "Rupesh Prasad", "Arvind Maurya", "Atul Singh"], "categories": ["cs.CL"], "comment": "10 pages, BDA Conference 2025", "summary": "Personalized marketing has emerged as a pivotal strategy for enhancing\ncustomer engagement and driving business growth. Academic and industry efforts\nhave predominantly focused on recommendation systems and personalized\nadvertisements. Nonetheless, this facet of personalization holds significant\npotential for increasing conversion rates and improving customer satisfaction.\nPrior studies suggest that well-executed personalization strategies can boost\nrevenue by up to 40 percent, underscoring the strategic importance of\ndeveloping intelligent, data-driven approaches for offer generation. This work\nintroduces SLM4Offer, a generative AI model for personalized offer generation,\ndeveloped by fine-tuning a pre-trained encoder-decoder language model,\nspecifically Google's Text-to-Text Transfer Transformer (T5-Small 60M) using a\ncontrastive learning approach. SLM4Offer employs InfoNCE (Information\nNoise-Contrastive Estimation) loss to align customer personas with relevant\noffers in a shared embedding space. A key innovation in SLM4Offer lies in the\nadaptive learning behaviour introduced by contrastive loss, which reshapes the\nlatent space during training and enhances the model's generalizability. The\nmodel is fine-tuned and evaluated on a synthetic dataset designed to simulate\ncustomer behaviour and offer acceptance patterns. Experimental results\ndemonstrate a 17 percent improvement in offer acceptance rate over a supervised\nfine-tuning baseline, highlighting the effectiveness of contrastive objectives\nin advancing personalized marketing."}
{"id": "2508.15474", "pdf": "https://arxiv.org/pdf/2508.15474.pdf", "abs": "https://arxiv.org/abs/2508.15474", "title": "Subjective Behaviors and Preferences in LLM: Language of Browsing", "authors": ["Sai Sundaresan", "Harshita Chopra", "Atanu R. Sinha", "Koustava Goswami", "Nagasai Saketh Naidu", "Raghav Karan", "N Anushka"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP 2025", "summary": "A Large Language Model (LLM) offers versatility across domains and tasks,\npurportedly benefiting users with a wide variety of behaviors and preferences.\nWe question this perception about an LLM when users have inherently subjective\nbehaviors and preferences, as seen in their ubiquitous and idiosyncratic\nbrowsing of websites or apps. The sequential behavior logs of pages, thus\ngenerated, form something akin to each user's self-constructed \"language\",\nalbeit without the structure and grammar imbued in natural languages. We ask:\n(i) Can a small LM represent the \"language of browsing\" better than a large LM?\n(ii) Can an LM with a single set of parameters (or, single LM) adequately\ncapture myriad users' heterogeneous, subjective behaviors and preferences?\n(iii) Can a single LM with high average performance, yield low variance in\nperformance to make alignment good at user level? We introduce clusterwise LM\ntraining, HeTLM (Heterogeneity aware Training of Language Model), appropriate\nfor subjective behaviors. We find that (i) a small LM trained using a\npage-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM\nwith heterogeneous cluster specific set of parameters outperforms a single LM\nof the same family, controlling for the number of parameters; and (iii) a\nhigher mean and a lower variance in generation ensues, implying improved\nalignment."}
{"id": "2508.15475", "pdf": "https://arxiv.org/pdf/2508.15475.pdf", "abs": "https://arxiv.org/abs/2508.15475", "title": "Influence-driven Curriculum Learning for Pre-training on Limited Data", "authors": ["Loris Schoenegger", "Lukas Thoma", "Terra Blevins", "Benjamin Roth"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "9 pages", "summary": "Curriculum learning, a training technique where data is presented to the\nmodel in order of example difficulty (e.g., from simpler to more complex\ndocuments), has shown limited success for pre-training language models. In this\nwork, we investigate whether curriculum learning becomes competitive if we\nreplace conventional human-centered difficulty metrics with one that more\nclosely corresponds to example difficulty as observed during model training.\nSpecifically, we experiment with sorting training examples by their\n\\textit{training data influence}, a score which estimates the effect of\nindividual training examples on the model's output. Models trained on our\ncurricula are able to outperform ones trained in random order by over 10\npercentage points in benchmarks, confirming that curriculum learning is\nbeneficial for language model pre-training, as long as a more model-centric\nnotion of difficulty is adopted."}
{"id": "2508.15478", "pdf": "https://arxiv.org/pdf/2508.15478.pdf", "abs": "https://arxiv.org/abs/2508.15478", "title": "SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version", "authors": ["Nghiem Thanh Pham", "Tung Kieu", "Duc-Manh Nguyen", "Son Ha Xuan", "Nghia Duong-Trung", "Danh Le-Phuoc"], "categories": ["cs.CL", "cs.CY", "cs.PF"], "comment": "24 pages. An extended version of \"SLM-Bench: A Comprehensive\n  Benchmark of Small Language Models on Environmental Impacts\" accepted at\n  EMNLP 2025", "summary": "Small Language Models (SLMs) offer computational efficiency and\naccessibility, yet a systematic evaluation of their performance and\nenvironmental impact remains lacking. We introduce SLM-Bench, the first\nbenchmark specifically designed to assess SLMs across multiple dimensions,\nincluding accuracy, computational efficiency, and sustainability metrics.\nSLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14\ndomains. The evaluation is conducted on 4 hardware configurations, providing a\nrigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench\nquantifies 11 metrics across correctness, computation, and consumption,\nenabling a holistic assessment of efficiency trade-offs. Our evaluation\nconsiders controlled hardware conditions, ensuring fair comparisons across\nmodels. We develop an open-source benchmarking pipeline with standardized\nevaluation protocols to facilitate reproducibility and further research. Our\nfindings highlight the diverse trade-offs among SLMs, where some models excel\nin accuracy while others achieve superior energy efficiency. SLM-Bench sets a\nnew standard for SLM evaluation, bridging the gap between resource efficiency\nand real-world applicability."}
{"id": "2508.15483", "pdf": "https://arxiv.org/pdf/2508.15483.pdf", "abs": "https://arxiv.org/abs/2508.15483", "title": "HebID: Detecting Social Identities in Hebrew-language Political Text", "authors": ["Guy Mor-Lan", "Naama Rivlin-Angert", "Yael R. Kaplan", "Tamir Sheafer", "Shaul R. Shenhav"], "categories": ["cs.CL"], "comment": null, "summary": "Political language is deeply intertwined with social identities. While social\nidentities are often shaped by specific cultural contexts and expressed through\nparticular uses of language, existing datasets for group and identity detection\nare predominantly English-centric, single-label and focus on coarse identity\ncategories. We introduce HebID, the first multilabel Hebrew corpus for social\nidentity detection: 5,536 sentences from Israeli politicians' Facebook posts\n(Dec 2018-Apr 2021), manually annotated for twelve nuanced social identities\n(e.g. Rightist, Ultra-Orthodox, Socially-oriented) grounded by survey data. We\nbenchmark multilabel and single-label encoders alongside 2B-9B-parameter\ngenerative LLMs, finding that Hebrew-tuned LLMs provide the best results\n(macro-$F_1$ = 0.74). We apply our classifier to politicians' Facebook posts\nand parliamentary speeches, evaluating differences in popularity, temporal\ntrends, clustering patterns, and gender-related variations in identity\nexpression. We utilize identity choices from a national public survey, enabling\na comparison between identities portrayed in elite discourse and the public's\nidentity priorities. HebID provides a comprehensive foundation for studying\nsocial identities in Hebrew and can serve as a model for similar research in\nother non-English political contexts."}
{"id": "2508.15487", "pdf": "https://arxiv.org/pdf/2508.15487.pdf", "abs": "https://arxiv.org/abs/2508.15487", "title": "Dream 7B: Diffusion Large Language Models", "authors": ["Jiacheng Ye", "Zhihui Xie", "Lin Zheng", "Jiahui Gao", "Zirui Wu", "Xin Jiang", "Zhenguo Li", "Lingpeng Kong"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Dream 7B, the most powerful open diffusion large language model\nto date. Unlike autoregressive (AR) models that generate tokens sequentially,\nDream 7B employs discrete diffusion modeling to refine sequences in parallel\nthrough iterative denoising. Our model consistently outperforms existing\ndiffusion language models on general, mathematical, and coding tasks. Dream 7B\ndemonstrates superior planning abilities and inference flexibility, including\narbitrary-order generation, infilling capabilities, and tunable quality-speed\ntrade-offs. These results are achieved through simple yet effective training\ntechniques, including AR-based LLM initialization and context-adaptive\ntoken-level noise rescheduling. We release both Dream-Base and Dream-Instruct\nto facilitate further research in diffusion-based language modeling."}
{"id": "2508.15524", "pdf": "https://arxiv.org/pdf/2508.15524.pdf", "abs": "https://arxiv.org/abs/2508.15524", "title": "The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech", "authors": ["Naama Rivlin-Angert", "Guy Mor-Lan"], "categories": ["cs.CL"], "comment": null, "summary": "We present the first large-scale computational study of political\ndelegitimization discourse (PDD), defined as symbolic attacks on the normative\nvalidity of political entities. We curate and manually annotate a novel\nHebrew-language corpus of 10,410 sentences drawn from Knesset speeches\n(1993-2023), Facebook posts (2018-2021), and leading news outlets, of which\n1,812 instances (17.4\\%) exhibit PDD and 642 carry additional annotations for\nintensity, incivility, target type, and affective framing. We introduce a\ntwo-stage classification pipeline combining finetuned encoder models and\ndecoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary\nPDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization\ncharacteristics. Applying this classifier to longitudinal and cross-platform\ndata, we see a marked rise in PDD over three decades, higher prevalence on\nsocial media versus parliamentary debate, greater use by male than female\npoliticians, and stronger tendencies among right-leaning actors - with\npronounced spikes during election campaigns and major political events. Our\nfindings demonstrate the feasibility and value of automated PDD analysis for\nunderstanding democratic discourse."}
{"id": "2508.15526", "pdf": "https://arxiv.org/pdf/2508.15526.pdf", "abs": "https://arxiv.org/abs/2508.15526", "title": "SafetyFlow: An Agent-Flow System for Automated LLM Safety Benchmarking", "authors": ["Xiangyang Zhu", "Yuan Tian", "Chunyi Li", "Kaiwei Zhang", "Wei Sun", "Guangtao Zhai"], "categories": ["cs.CL"], "comment": "Code and dataset are available at\n  https://github.com/yangyangyang127/SafetyFlow", "summary": "The rapid proliferation of large language models (LLMs) has intensified the\nrequirement for reliable safety evaluation to uncover model vulnerabilities. To\nthis end, numerous LLM safety evaluation benchmarks are proposed. However,\nexisting benchmarks generally rely on labor-intensive manual curation, which\ncauses excessive time and resource consumption. They also exhibit significant\nredundancy and limited difficulty. To alleviate these problems, we introduce\nSafetyFlow, the first agent-flow system designed to automate the construction\nof LLM safety benchmarks. SafetyFlow can automatically build a comprehensive\nsafety benchmark in only four days without any human intervention by\norchestrating seven specialized agents, significantly reducing time and\nresource cost. Equipped with versatile tools, the agents of SafetyFlow ensure\nprocess and cost controllability while integrating human expertise into the\nautomatic pipeline. The final constructed dataset, SafetyFlowBench, contains\n23,446 queries with low redundancy and strong discriminative power. Our\ncontribution includes the first fully automated benchmarking pipeline and a\ncomprehensive safety benchmark. We evaluate the safety of 49 advanced LLMs on\nour dataset and conduct extensive experiments to validate our efficacy and\nefficiency."}
{"id": "2508.15617", "pdf": "https://arxiv.org/pdf/2508.15617.pdf", "abs": "https://arxiv.org/abs/2508.15617", "title": "Trained Miniatures: Low cost, High Efficacy SLMs for Sales & Marketing", "authors": ["Ishaan Bhola", "Mukunda NS", "Sravanth Kurmala", "Harsh Nandwani", "Arihant Jain"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel in text generation; however, these\ncreative elements require heavy computation and are accompanied by a steep\ncost. Especially for targeted applications such as sales and marketing\noutreach, these costs are far from feasible. This paper introduces the concept\nof \"Trained Miniatures\" - Small Language Models(SLMs) fine-tuned for specific,\nhigh-value applications, generating similar domain-specific responses for a\nfraction of the cost."}
{"id": "2508.15648", "pdf": "https://arxiv.org/pdf/2508.15648.pdf", "abs": "https://arxiv.org/abs/2508.15648", "title": "SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models", "authors": ["Peng Ding", "Wen Sun", "Dailin Li", "Wei Zou", "Jiaming Wang", "Jiajun Chen", "Shujian Huang"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025, 15 pages, 4 figures, 6 tables", "summary": "Large Language Models (LLMs) excel at various natural language processing\ntasks but remain vulnerable to jailbreaking attacks that induce harmful content\ngeneration. In this paper, we reveal a critical safety inconsistency: LLMs can\nmore effectively identify harmful requests as discriminators than defend\nagainst them as generators. This insight inspires us to explore aligning the\nmodel's inherent discrimination and generation capabilities. To this end, we\npropose SDGO (Self-Discrimination-Guided Optimization), a reinforcement\nlearning framework that leverages the model's own discrimination capabilities\nas a reward signal to enhance generation safety through iterative\nself-improvement. Our method does not require any additional annotated data or\nexternal models during the training phase. Extensive experiments demonstrate\nthat SDGO significantly improves model safety compared to both prompt-based and\ntraining-based baselines while maintaining helpfulness on general benchmarks.\nBy aligning LLMs' discrimination and generation capabilities, SDGO brings\nrobust performance against out-of-distribution (OOD) jailbreaking attacks. This\nalignment achieves tighter coupling between these two capabilities, enabling\nthe model's generation capability to be further enhanced with only a small\namount of discriminative samples. Our code and datasets are available at\nhttps://github.com/NJUNLP/SDGO."}
{"id": "2508.15658", "pdf": "https://arxiv.org/pdf/2508.15658.pdf", "abs": "https://arxiv.org/abs/2508.15658", "title": "Benchmarking Computer Science Survey Generation", "authors": ["Weihang Su", "Anzhe Xie", "Qingyao Ai", "Jianming Long", "Jiaxin Mao", "Ziyi Ye", "Yiqun Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Scientific survey articles play a vital role in summarizing research\nprogress, yet their manual creation is becoming increasingly infeasible due to\nthe rapid growth of academic literature. While large language models (LLMs)\noffer promising capabilities for automating this process, progress in this area\nis hindered by the absence of standardized benchmarks and evaluation protocols.\nTo address this gap, we introduce SurGE (Survey Generation Evaluation), a new\nbenchmark for evaluating scientific survey generation in the computer science\ndomain. SurGE consists of (1) a collection of test instances, each including a\ntopic description, an expert-written survey, and its full set of cited\nreferences, and (2) a large-scale academic corpus of over one million papers\nthat serves as the retrieval pool. In addition, we propose an automated\nevaluation framework that measures generated surveys across four dimensions:\ninformation coverage, referencing accuracy, structural organization, and\ncontent quality. Our evaluation of diverse LLM-based approaches shows that\nsurvey generation remains highly challenging, even for advanced self-reflection\nframeworks. These findings highlight the complexity of the task and the\nnecessity for continued research. We have open-sourced all the code, data, and\nmodels at: https://github.com/oneal2000/SurGE"}
{"id": "2508.15709", "pdf": "https://arxiv.org/pdf/2508.15709.pdf", "abs": "https://arxiv.org/abs/2508.15709", "title": "Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation", "authors": ["Yifei Wang", "Feng Xiong", "Yong Wang", "Linjing Li", "Xiangxiang Chu", "Daniel Dajun Zeng"], "categories": ["cs.CL"], "comment": "EMNLP2025 Main", "summary": "Positional bias (PB), manifesting as non-uniform sensitivity across different\ncontextual locations, significantly impairs long-context comprehension and\nprocessing capabilities. While prior work seeks to mitigate PB through\nmodifying the architectures causing its emergence, significant PB still\npersists. To address PB effectively, we introduce \\textbf{Pos2Distill}, a\nposition to position knowledge distillation framework. Pos2Distill transfers\nthe superior capabilities from advantageous positions to less favorable ones,\nthereby reducing the huge performance gaps. The conceptual principle is to\nleverage the inherent, position-induced disparity to counteract the PB itself.\nWe identify distinct manifestations of PB under \\textbf{\\textsc{r}}etrieval and\n\\textbf{\\textsc{r}}easoning paradigms, thereby designing two specialized\ninstantiations: \\emph{Pos2Distill-R\\textsuperscript{1}} and\n\\emph{Pos2Distill-R\\textsuperscript{2}} respectively, both grounded in this\ncore principle. By employing the Pos2Distill approach, we achieve enhanced\nuniformity and significant performance gains across all contextual positions in\nlong-context retrieval and reasoning tasks. Crucially, both specialized systems\nexhibit strong cross-task generalization mutually, while achieving superior\nperformance on their respective tasks."}
{"id": "2508.15711", "pdf": "https://arxiv.org/pdf/2508.15711.pdf", "abs": "https://arxiv.org/abs/2508.15711", "title": "Stemming -- The Evolution and Current State with a Focus on Bangla", "authors": ["Abhijit Paul", "Mashiat Amin Farin", "Sharif Md. Abdullah", "Ahmedul Kabir", "Zarif Masud", "Shebuti Rayana"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Bangla, the seventh most widely spoken language worldwide with 300 million\nnative speakers, faces digital under-representation due to limited resources\nand lack of annotated datasets. Stemming, a critical preprocessing step in\nlanguage analysis, is essential for low-resource, highly-inflectional languages\nlike Bangla, because it can reduce the complexity of algorithms and models by\nsignificantly reducing the number of words the algorithm needs to consider.\nThis paper conducts a comprehensive survey of stemming approaches, emphasizing\nthe importance of handling morphological variants effectively. While exploring\nthe landscape of Bangla stemming, it becomes evident that there is a\nsignificant gap in the existing literature. The paper highlights the\ndiscontinuity from previous research and the scarcity of accessible\nimplementations for replication. Furthermore, it critiques the evaluation\nmethodologies, stressing the need for more relevant metrics. In the context of\nBangla's rich morphology and diverse dialects, the paper acknowledges the\nchallenges it poses. To address these challenges, the paper suggests directions\nfor Bangla stemmer development. It concludes by advocating for robust Bangla\nstemmers and continued research in the field to enhance language analysis and\nprocessing."}
{"id": "2508.15721", "pdf": "https://arxiv.org/pdf/2508.15721.pdf", "abs": "https://arxiv.org/abs/2508.15721", "title": "EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models", "authors": ["Xinyi Ling", "Hanwen Du", "Zhihui Zhu", "Xia Ning"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "E-commerce platforms are rich in multimodal data, featuring a variety of\nimages that depict product details. However, this raises an important question:\ndo these images always enhance product understanding, or can they sometimes\nintroduce redundancy or degrade performance? Existing datasets are limited in\nboth scale and design, making it difficult to systematically examine this\nquestion. To this end, we introduce EcomMMMU, an e-commerce multimodal\nmultitask understanding dataset with 406,190 samples and 8,989,510 images.\nEcomMMMU is comprised of multi-image visual-language data designed with 8\nessential tasks and a specialized VSS subset to benchmark the capability of\nmultimodal large language models (MLLMs) to effectively utilize visual content.\nAnalysis on EcomMMMU reveals that product images do not consistently improve\nperformance and can, in some cases, degrade it. This indicates that MLLMs may\nstruggle to effectively leverage rich visual content for e-commerce tasks.\nBuilding on these insights, we propose SUMEI, a data-driven method that\nstrategically utilizes multiple images via predicting visual utilities before\nusing them for downstream tasks. Comprehensive experiments demonstrate the\neffectiveness and robustness of SUMEI. The data and code are available through\nhttps://anonymous.4open.science/r/submission25."}
{"id": "2508.15746", "pdf": "https://arxiv.org/pdf/2508.15746.pdf", "abs": "https://arxiv.org/abs/2508.15746", "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning", "authors": ["Qiaoyu Zheng", "Yuze Sun", "Chaoyi Wu", "Weike Zhao", "Pengcheng Qiu", "Yongguo Yu", "Kun Sun", "Yanfeng Wang", "Ya Zhang", "Weidi Xie"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "35 pages, 5 figures, 3 tables", "summary": "Accurate diagnosis with medical large language models is hindered by\nknowledge gaps and hallucinations. Retrieval and tool-augmented methods help,\nbut their impact is limited by weak use of external knowledge and poor\nfeedback-reasoning traceability. To address these challenges, We introduce\nDeep-DxSearch, an agentic RAG system trained end-to-end with reinforcement\nlearning (RL) that enables steer tracebale retrieval-augmented reasoning for\nmedical diagnosis. In Deep-DxSearch, we first construct a large-scale medical\nretrieval corpus comprising patient records and reliable medical knowledge\nsources to support retrieval-aware reasoning across diagnostic scenarios. More\ncrutially, we frame the LLM as the core agent and the retrieval corpus as its\nenvironment, using tailored rewards on format, retrieval, reasoning structure,\nand diagnostic accuracy, thereby evolving the agentic RAG policy from\nlarge-scale data through RL.\n  Experiments demonstrate that our end-to-end agentic RL training framework\nconsistently outperforms prompt-engineering and training-free RAG approaches\nacross multiple data centers. After training, Deep-DxSearch achieves\nsubstantial gains in diagnostic accuracy, surpassing strong diagnostic\nbaselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks\nfor both common and rare disease diagnosis under in-distribution and\nout-of-distribution settings. Moreover, ablation studies on reward design and\nretrieval corpus components confirm their critical roles, underscoring the\nuniqueness and effectiveness of our approach compared with traditional\nimplementations. Finally, case studies and interpretability analyses highlight\nimprovements in Deep-DxSearch's diagnostic policy, providing deeper insight\ninto its performance gains and supporting clinicians in delivering more\nreliable and precise preliminary diagnoses. See\nhttps://github.com/MAGIC-AI4Med/Deep-DxSearch."}
{"id": "2508.15754", "pdf": "https://arxiv.org/pdf/2508.15754.pdf", "abs": "https://arxiv.org/abs/2508.15754", "title": "Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis", "authors": ["Yufeng Zhao", "Junnan Liu", "Hongwei Liu", "Dongsheng Zhu", "Yuan Shen", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, working in progress", "summary": "Large Language Models (LLMs) have made significant strides in reasoning tasks\nthrough methods like chain-of-thought (CoT) reasoning. However, they often fall\nshort in tasks requiring precise computations. Tool-Integrated Reasoning (TIR)\nhas emerged as a solution by incorporating external tools into the reasoning\nprocess. Nevertheless, the generalization of TIR in improving the reasoning\nability of LLM is still unclear. Additionally, whether TIR has improved the\nmodel's reasoning behavior and helped the model think remains to be studied. We\nintroduce ReasonZoo, a comprehensive benchmark encompassing nine diverse\nreasoning categories, to evaluate the effectiveness of TIR across various\ndomains. Additionally, we propose two novel metrics, Performance-Aware Cost\n(PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning\nefficiency. Our empirical evaluation demonstrates that TIR-enabled models\nconsistently outperform their non-TIR counterparts in both mathematical and\nnon-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as\nevidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more\nstreamlined reasoning. These findings underscore the domain-general benefits of\nTIR and its potential to advance LLM capabilities in complex reasoning tasks."}
{"id": "2508.15760", "pdf": "https://arxiv.org/pdf/2508.15760.pdf", "abs": "https://arxiv.org/abs/2508.15760", "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries", "authors": ["Ming Yin", "Dinghan Shen", "Silei Xu", "Jianbing Han", "Sixun Dong", "Mian Zhang", "Yebowen Hu", "Shujian Liu", "Simin Ma", "Song Wang", "Sathish Reddy Indurthi", "Xun Wang", "Yiran Chen", "Kaiqiang Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tool calling has emerged as a critical capability for AI agents to interact\nwith the real world and solve complex tasks. While the Model Context Protocol\n(MCP) provides a powerful standardized framework for tool integration, there is\na significant gap in benchmarking how well AI agents can effectively solve\nmulti-step tasks using diverse MCP tools in realistic, dynamic scenarios. In\nthis work, we present LiveMCP-101, a benchmark of 101 carefully curated\nreal-world queries, refined through iterative LLM rewriting and manual review,\nthat require coordinated use of multiple MCP tools including web search, file\noperations, mathematical reasoning, and data analysis. Moreover, we introduce a\nnovel evaluation approach that leverages ground-truth execution plans rather\nthan raw API outputs, better reflecting the evolving nature of real-world\nenvironments. Experiments show that even frontier LLMs achieve a success rate\nbelow 60\\%, highlighting major challenges in tool orchestration. Detailed\nablations and error analysis further reveal distinct failure modes and\ninefficiencies in token usage, pointing to concrete directions for advancing\ncurrent models. LiveMCP-101 sets a rigorous standard for evaluating real-world\nagent capabilities, advancing toward autonomous AI systems that reliably\nexecute complex tasks through tool use."}
{"id": "2508.14908", "pdf": "https://arxiv.org/pdf/2508.14908.pdf", "abs": "https://arxiv.org/abs/2508.14908", "title": "A Chinese Heart Failure Status Speech Database with Universal and Personalised Classification", "authors": ["Yue Pan", "Liwei Liu", "Changxin Li", "Xinyao Wang", "Yili Xia", "Hanyue Zhang", "Ming Chu"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Speech is a cost-effective and non-intrusive data source for identifying\nacute and chronic heart failure (HF). However, there is a lack of research on\nwhether Chinese syllables contain HF-related information, as observed in other\nwell-studied languages. This study presents the first Chinese speech database\nof HF patients, featuring paired recordings taken before and after\nhospitalisation. The findings confirm the effectiveness of the Chinese language\nin HF detection using both standard 'patient-wise' and personalised 'pair-wise'\nclassification approaches, with the latter serving as an ideal\nspeaker-decoupled baseline for future research. Statistical tests and\nclassification results highlight individual differences as key contributors to\ninaccuracy. Additionally, an adaptive frequency filter (AFF) is proposed for\nfrequency importance analysis. The data and demonstrations are published at\nhttps://github.com/panyue1998/Voice_HF."}
{"id": "2508.14916", "pdf": "https://arxiv.org/pdf/2508.14916.pdf", "abs": "https://arxiv.org/abs/2508.14916", "title": "Transsion Multilingual Speech Recognition System for MLC-SLM 2025 Challenge", "authors": ["Xiaoxiao Li", "An Zhu", "Youhai Jiang", "Fengjie Zhu"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper presents the architecture and performance of a novel Multilingual\nAutomatic Speech Recognition (ASR) system developed by the Transsion Speech\nTeam for Track 1 of the MLC-SLM 2025 Challenge. The proposed system comprises\nthree key components: 1) a frozen Whisper-large-v3 based speech encoder,\nleveraging large-scale pretraining to ensure robust acoustic feature\nextraction; 2) a trainable adaptor module using Linear-ReLU-Linear\ntransformation mechanisms to effectively align speech and text representations;\nand 3) a frozen Qwen2.5-7B-Instruct large language model (LLM) integrated with\ntrainable LoRA for optimized contextual linguistic decoding. By systematically\ncombining pretrained models with task specific fine-tuning, the system achieved\na word/character error rate (WER/CER) of 9.83% across 11 languages in the\nevaluation set and ranked third place among global participants."}
{"id": "2508.14941", "pdf": "https://arxiv.org/pdf/2508.14941.pdf", "abs": "https://arxiv.org/abs/2508.14941", "title": "Robust Symbolic Reasoning for Visual Narratives via Hierarchical and Semantically Normalized Knowledge Graphs", "authors": ["Yi-Chun Chen"], "categories": ["cs.MM", "cs.CL"], "comment": "12 pages, 4 figures, 2 tables. Extends our earlier framework on\n  hierarchical narrative graphs with a semantic normalization module", "summary": "Understanding visual narratives such as comics requires structured\nrepresentations that capture events, characters, and their relations across\nmultiple levels of story organization. However, symbolic narrative graphs often\nsuffer from inconsistency and redundancy, where similar actions or events are\nlabeled differently across annotations or contexts. Such variance limits the\neffectiveness of reasoning and generalization.\n  This paper introduces a semantic normalization framework for hierarchical\nnarrative knowledge graphs. Building on cognitively grounded models of\nnarrative comprehension, we propose methods that consolidate semantically\nrelated actions and events using lexical similarity and embedding-based\nclustering. The normalization process reduces annotation noise, aligns symbolic\ncategories across narrative levels, and preserves interpretability.\n  We demonstrate the framework on annotated manga stories from the Manga109\ndataset, applying normalization to panel-, event-, and story-level graphs.\nPreliminary evaluations across narrative reasoning tasks, such as action\nretrieval, character grounding, and event summarization, show that semantic\nnormalization improves coherence and robustness, while maintaining symbolic\ntransparency. These findings suggest that normalization is a key step toward\nscalable, cognitively inspired graph models for multimodal narrative\nunderstanding."}
{"id": "2508.15050", "pdf": "https://arxiv.org/pdf/2508.15050.pdf", "abs": "https://arxiv.org/abs/2508.15050", "title": "Don't Think Twice! Over-Reasoning Impairs Confidence Calibration", "authors": ["Romain Lacombe", "Kerrie Wu", "Eddie Dilworth"], "categories": ["cs.AI", "cs.CL"], "comment": "Published at ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models", "summary": "Large Language Models deployed as question answering tools require robust\ncalibration to avoid overconfidence. We systematically evaluate how reasoning\ncapabilities and budget affect confidence assessment accuracy, using the\nClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary\nhealth. Our key finding challenges the \"test-time scaling\" paradigm: while\nrecent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,\nincreasing reasoning budgets consistently impairs rather than improves\ncalibration. Extended reasoning leads to systematic overconfidence that worsens\nwith longer thinking budgets, producing diminishing and negative returns beyond\nmodest computational investments. Conversely, search-augmented generation\ndramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving\nrelevant evidence. Our results suggest that information access, rather than\nreasoning depth or inference budget, may be the critical bottleneck for\nimproved confidence calibration of knowledge-intensive tasks."}
{"id": "2508.15110", "pdf": "https://arxiv.org/pdf/2508.15110.pdf", "abs": "https://arxiv.org/abs/2508.15110", "title": "LLMs and Agentic AI in Insurance Decision-Making: Opportunities and Challenges For Africa", "authors": ["Graham Hill", "JingYuan Gong", "Thulani Babeli", "Moseli Mots'oehli", "James Gachomo Wanjiku"], "categories": ["cs.CE", "cs.CL", "cs.ET", "stat.AP"], "comment": null, "summary": "In this work, we highlight the transformative potential of Artificial\nIntelligence (AI), particularly Large Language Models (LLMs) and agentic AI, in\nthe insurance sector. We consider and emphasize the unique opportunities,\nchallenges, and potential pathways in insurance amid rapid performance\nimprovements, increased open-source access, decreasing deployment costs, and\nthe complexity of LLM or agentic AI frameworks. To bring it closer to home, we\nidentify critical gaps in the African insurance market and highlight key local\nefforts, players, and partnership opportunities. Finally, we call upon\nactuaries, insurers, regulators, and tech leaders to a collaborative effort\naimed at creating inclusive, sustainable, and equitable AI strategies and\nsolutions: by and for Africans."}
{"id": "2508.15119", "pdf": "https://arxiv.org/pdf/2508.15119.pdf", "abs": "https://arxiv.org/abs/2508.15119", "title": "Open-Universe Assistance Games", "authors": ["Rachel Ma", "Jingyi Qu", "Andreea Bobu", "Dylan Hadfield-Menell"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "7 pages + 2 pages references + 7 pages appendix", "summary": "Embodied AI agents must infer and act in an interpretable way on diverse\nhuman goals and preferences that are not predefined. To formalize this setting,\nwe introduce Open-Universe Assistance Games (OU-AGs), a framework where the\nagent must reason over an unbounded and evolving space of possible goals. In\nthis context, we introduce GOOD (GOals from Open-ended Dialogue), a\ndata-efficient, online method that extracts goals in the form of natural\nlanguage during an interaction with a human, and infers a distribution over\nnatural language goals. GOOD prompts an LLM to simulate users with different\ncomplex intents, using its responses to perform probabilistic inference over\ncandidate goals. This approach enables rich goal representations and\nuncertainty estimation without requiring large offline datasets. We evaluate\nGOOD in a text-based grocery shopping domain and in a text-operated simulated\nhousehold robotics environment (AI2Thor), using synthetic user profiles. Our\nmethod outperforms a baseline without explicit goal tracking, as confirmed by\nboth LLM-based and human evaluations."}
{"id": "2508.15126", "pdf": "https://arxiv.org/pdf/2508.15126.pdf", "abs": "https://arxiv.org/abs/2508.15126", "title": "aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists", "authors": ["Pengsong Zhang", "Xiang Hu", "Guowei Huang", "Yang Qi", "Heng Zhang", "Xiuxu Li", "Jiaxing Song", "Jiabin Luo", "Yijiang Li", "Shuo Yin", "Chengxiao Dai", "Eric Hanchen Jiang", "Xiaoyan Zhou", "Zhenfei Yin", "Boqin Yuan", "Jing Dong", "Guinan Su", "Guanren Qiao", "Haiming Tang", "Anghong Du", "Lili Pan", "Zhenzhong Lan", "Xinyu Liu"], "categories": ["cs.AI", "cs.CL"], "comment": "Preprint under review. Code is available at\n  https://github.com/aixiv-org. Website is available at\n  https://forms.gle/DxQgCtXFsJ4paMtn8", "summary": "Recent advances in large language models (LLMs) have enabled AI agents to\nautonomously generate scientific proposals, conduct experiments, author papers,\nand perform peer reviews. Yet this flood of AI-generated research content\ncollides with a fragmented and largely closed publication ecosystem.\nTraditional journals and conferences rely on human peer review, making them\ndifficult to scale and often reluctant to accept AI-generated research content;\nexisting preprint servers (e.g. arXiv) lack rigorous quality-control\nmechanisms. Consequently, a significant amount of high-quality AI-generated\nresearch lacks appropriate venues for dissemination, hindering its potential to\nadvance scientific progress. To address these challenges, we introduce aiXiv, a\nnext-generation open-access platform for human and AI scientists. Its\nmulti-agent architecture allows research proposals and papers to be submitted,\nreviewed, and iteratively refined by both human and AI scientists. It also\nprovides API and MCP interfaces that enable seamless integration of\nheterogeneous human and AI scientists, creating a scalable and extensible\necosystem for autonomous scientific discovery. Through extensive experiments,\nwe demonstrate that aiXiv is a reliable and robust platform that significantly\nenhances the quality of AI-generated research proposals and papers after\niterative revising and reviewing on aiXiv. Our work lays the groundwork for a\nnext-generation open-access ecosystem for AI scientists, accelerating the\npublication and dissemination of high-quality AI-generated research content.\nCode is available at https://github.com/aixiv-org. Website is available at\nhttps://forms.gle/DxQgCtXFsJ4paMtn8."}
{"id": "2508.15192", "pdf": "https://arxiv.org/pdf/2508.15192.pdf", "abs": "https://arxiv.org/abs/2508.15192", "title": "LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support", "authors": ["Wenjie Lin", "Jin Wei-Kocsis"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While large language models (LLMs) have shown promise in healthcare, their\napplication for rare medical conditions is still hindered by scarce and\nunreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing\nexcessive sweating beyond physiological needs, is one such rare disorder,\naffecting 2-3% of the population and significantly impacting both physical\ncomfort and psychosocial well-being. To date, no work has tailored LLMs to\nadvance the diagnosis or care of hyperhidrosis. To address this gap, we present\nLLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and\nempathetic hyperhidrosis support. The system follows a three-stage pipeline. In\nthe data augmentation stage, a frontier LLM generates medically plausible\nsynthetic vignettes from curated open-source data to create a diverse and\nbalanced question-answer dataset. In the fine-tuning stage, an open-source\nfoundation model is fine-tuned on the dataset to provide diagnosis,\npersonalized treatment recommendations, and empathetic psychological support.\nIn the inference and expert evaluation stage, clinical and psychological\nspecialists assess accuracy, appropriateness, and empathy, with validated\nresponses iteratively enriching the dataset. Experiments show that LLM4Sweat\noutperforms baselines and delivers the first open-source LLM framework for\nhyperhidrosis, offering a generalizable approach for other rare diseases with\nsimilar data and trustworthiness challenges."}
{"id": "2508.15252", "pdf": "https://arxiv.org/pdf/2508.15252.pdf", "abs": "https://arxiv.org/abs/2508.15252", "title": "Retrieval-Augmented Review Generation for Poisoning Recommender Systems", "authors": ["Shiyi Yang", "Xinshu Li", "Guanglin Zhou", "Chen Wang", "Xiwei Xu", "Liming Zhu", "Lina Yao"], "categories": ["cs.CR", "cs.CL", "cs.IR"], "comment": null, "summary": "Recent studies have shown that recommender systems (RSs) are highly\nvulnerable to data poisoning attacks, where malicious actors inject fake user\nprofiles, including a group of well-designed fake ratings, to manipulate\nrecommendations. Due to security and privacy constraints in practice, attackers\ntypically possess limited knowledge of the victim system and thus need to craft\nprofiles that have transferability across black-box RSs. To maximize the attack\nimpact, the profiles often remains imperceptible. However, generating such\nhigh-quality profiles with the restricted resources is challenging. Some works\nsuggest incorporating fake textual reviews to strengthen the profiles; yet, the\npoor quality of the reviews largely undermines the attack effectiveness and\nimperceptibility under the practical setting.\n  To tackle the above challenges, in this paper, we propose to enhance the\nquality of the review text by harnessing in-context learning (ICL) capabilities\nof multimodal foundation models. To this end, we introduce a demonstration\nretrieval algorithm and a text style transfer strategy to augment the navie\nICL. Specifically, we propose a novel practical attack framework named RAGAN to\ngenerate high-quality fake user profiles, which can gain insights into the\nrobustness of RSs. The profiles are generated by a jailbreaker and\ncollaboratively optimized on an instructional agent and a guardian to improve\nthe attack transferability and imperceptibility. Comprehensive experiments on\nvarious real-world datasets demonstrate that RAGAN achieves the\nstate-of-the-art poisoning attack performance."}
{"id": "2508.15276", "pdf": "https://arxiv.org/pdf/2508.15276.pdf", "abs": "https://arxiv.org/abs/2508.15276", "title": "AmbiSQL: Interactive Ambiguity Detection and Resolution for Text-to-SQL", "authors": ["Zhongjun Ding", "Yin Lin", "Tianjing Zeng"], "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "Text-to-SQL systems translate natural language questions into SQL queries,\nproviding substantial value for non-expert users. While large language models\n(LLMs) show promising results for this task, they remain error-prone. Query\nambiguity has been recognized as a major obstacle for LLM-based Text-to-SQL\nsystems, leading to misinterpretation of user intent and inaccurate SQL\ngeneration. We demonstrate AmbiSQL, an interactive system that automatically\ndetects query ambiguities and guides users through intuitive multiple-choice\nquestions to clarify their intent. Our approach introduces a fine-grained\nambiguity taxonomy for identifying ambiguities that affect database element\nmapping and LLM reasoning, then incorporates user feedback to rewrite ambiguous\nquestions. Evaluation on an ambiguous query dataset shows that AmbiSQL achieves\n87.2% precision in ambiguity detection and improves SQL exact match accuracy by\n50% when integrated with Text-to-SQL systems. Our demonstration showcases the\nsignificant performance gains and highlights the system's practical usability.\nCode repo and demonstration are available at:\nhttps://github.com/JustinzjDing/AmbiSQL."}
{"id": "2508.15283", "pdf": "https://arxiv.org/pdf/2508.15283.pdf", "abs": "https://arxiv.org/abs/2508.15283", "title": "Adversarial Attacks against Neural Ranking Models via In-Context Learning", "authors": ["Amin Bigdeli", "Negar Arabzadeh", "Ebrahim Bagheri", "Charles L. A. Clarke"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "While neural ranking models (NRMs) have shown high effectiveness, they remain\nsusceptible to adversarial manipulation. In this work, we introduce Few-Shot\nAdversarial Prompting (FSAP), a novel black-box attack framework that leverages\nthe in-context learning capabilities of Large Language Models (LLMs) to\ngenerate high-ranking adversarial documents. Unlike previous approaches that\nrely on token-level perturbations or manual rewriting of existing documents,\nFSAP formulates adversarial attacks entirely through few-shot prompting,\nrequiring no gradient access or internal model instrumentation. By conditioning\nthe LLM on a small support set of previously observed harmful examples, FSAP\nsynthesizes grammatically fluent and topically coherent documents that subtly\nembed false or misleading information and rank competitively against authentic\ncontent. We instantiate FSAP in two modes: FSAP-IntraQ, which leverages harmful\nexamples from the same query to enhance topic fidelity, and FSAP-InterQ, which\nenables broader generalization by transferring adversarial patterns across\nunrelated queries. Our experiments on the TREC 2020 and 2021 Health\nMisinformation Tracks, using four diverse neural ranking models, reveal that\nFSAP-generated documents consistently outrank credible, factually accurate\ndocuments. Furthermore, our analysis demonstrates that these adversarial\noutputs exhibit strong stance alignment and low detectability, posing a\nrealistic and scalable threat to neural retrieval systems. FSAP also\neffectively generalizes across both proprietary and open-source LLMs."}
{"id": "2508.15291", "pdf": "https://arxiv.org/pdf/2508.15291.pdf", "abs": "https://arxiv.org/abs/2508.15291", "title": "Evaluating Knowledge Graph Complexity via Semantic, Spectral, and Structural Metrics for Link Prediction", "authors": ["Haji Gul", "Abul Ghani Naim", "Ajaz Ahmad Bhat"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Understanding dataset complexity is fundamental to evaluating and comparing\nlink prediction models on knowledge graphs (KGs). While the Cumulative Spectral\nGradient (CSG) metric, derived from probabilistic divergence between classes\nwithin a spectral clustering framework, has been proposed as a classifier\nagnostic complexity metric purportedly scaling with class cardinality and\ncorrelating with downstream performance, it has not been evaluated in KG\nsettings so far. In this work, we critically examine CSG in the context of\nmulti relational link prediction, incorporating semantic representations via\ntransformer derived embeddings. Contrary to prior claims, we find that CSG is\nhighly sensitive to parametrisation and does not robustly scale with the number\nof classes. Moreover, it exhibits weak or inconsistent correlation with\nstandard performance metrics such as Mean Reciprocal Rank (MRR) and Hit@1. To\ndeepen the analysis, we introduce and benchmark a set of structural and\nsemantic KG complexity metrics. Our findings reveal that global and local\nrelational ambiguity captured via Relation Entropy, node level Maximum Relation\nDiversity, and Relation Type Cardinality exhibit strong inverse correlations\nwith MRR and Hit@1, suggesting these as more faithful indicators of task\ndifficulty. Conversely, graph connectivity measures such as Average Degree,\nDegree Entropy, PageRank, and Eigenvector Centrality correlate positively with\nHit@10. Our results demonstrate that CSGs purported stability and\ngeneralization predictive power fail to hold in link prediction settings and\nunderscore the need for more stable, interpretable, and task-aligned measures\nof dataset complexity in knowledge driven learning."}
{"id": "2508.15294", "pdf": "https://arxiv.org/pdf/2508.15294.pdf", "abs": "https://arxiv.org/abs/2508.15294", "title": "Multiple Memory Systems for Enhancing the Long-term Memory of Agent", "authors": ["Gaoke Zhang", "Bo Wang", "Yunlong Ma", "Dongming Zhao", "Zifei Yu"], "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.7"], "comment": null, "summary": "An agent powered by large language models have achieved impressive results,\nbut effectively handling the vast amounts of historical data generated during\ninteractions remains a challenge. The current approach is to design a memory\nmodule for the agent to process these data. However, existing methods, such as\nMemoryBank and A-MEM, have poor quality of stored memory content, which affects\nrecall performance and response quality. In order to better construct\nhigh-quality long-term memory content, we have designed a multiple memory\nsystem (MMS) inspired by cognitive psychology theory. The system processes\nshort-term memory to multiple long-term memory fragments, and constructs\nretrieval memory units and contextual memory units based on these fragments,\nwith a one-to-one correspondence between the two. During the retrieval phase,\nMMS will match the most relevant retrieval memory units based on the user's\nquery. Then, the corresponding contextual memory units is obtained as the\ncontext for the response stage to enhance knowledge, thereby effectively\nutilizing historical data. Experiments on LoCoMo dataset compared our method\nwith three others, proving its effectiveness. Ablation studies confirmed the\nrationality of our memory units. We also analyzed the robustness regarding the\nnumber of selected memory segments and the storage overhead, demonstrating its\npractical value."}
{"id": "2508.15310", "pdf": "https://arxiv.org/pdf/2508.15310.pdf", "abs": "https://arxiv.org/abs/2508.15310", "title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents", "authors": ["Hengyu An", "Jinghuai Zhang", "Tianyu Du", "Chunyi Zhou", "Qingming Li", "Tao Lin", "Shouling Ji"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "EMNLP 2025", "summary": "Large language model (LLM) agents are widely deployed in real-world\napplications, where they leverage tools to retrieve and manipulate external\ndata for complex tasks. However, when interacting with untrusted data sources\n(e.g., fetching information from public websites), tool responses may contain\ninjected instructions that covertly influence agent behaviors and lead to\nmalicious outcomes, a threat referred to as Indirect Prompt Injection (IPI).\nExisting defenses typically rely on advanced prompting strategies or auxiliary\ndetection models. While these methods have demonstrated some effectiveness,\nthey fundamentally rely on assumptions about the model's inherent security,\nwhich lacks structural constraints on agent behaviors. As a result, agents\nstill retain unrestricted access to tool invocations, leaving them vulnerable\nto stronger attack vectors that can bypass the security guardrails of the\nmodel. To prevent malicious tool invocations at the source, we propose a novel\ndefensive task execution paradigm, called IPIGuard, which models the agents'\ntask execution process as a traversal over a planned Tool Dependency Graph\n(TDG). By explicitly decoupling action planning from interaction with external\ndata, IPIGuard significantly reduces unintended tool invocations triggered by\ninjected instructions, thereby enhancing robustness against IPI attacks.\nExperiments on the AgentDojo benchmark show that IPIGuard achieves a superior\nbalance between effectiveness and robustness, paving the way for the\ndevelopment of safer agentic systems in dynamic environments."}
{"id": "2508.15338", "pdf": "https://arxiv.org/pdf/2508.15338.pdf", "abs": "https://arxiv.org/abs/2508.15338", "title": "DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization", "authors": ["Jinning Yang", "Wen Shi"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Electrocardiography plays a central role in cardiovascular diagnostics, yet\nexisting automated approaches often struggle to generalize across clinical\ntasks and offer limited support for open-ended reasoning. We present DiagECG, a\nnovel framework that integrates time-series and language modeling by enabling\nlarge language models to process 12-lead ECG signals for clinical text\ngeneration tasks. Our approach discretizes continuous ECG embeddings into\nsymbolic tokens using a lead-independent encoder and quantization module. These\ntokens are then used to extend the vocabulary of LLM, allowing the model to\nhandle both ECG and natural language inputs in a unified manner. To bridge the\nmodality gap, we pretrain the model on an autoregressive ECG forecasting task,\nenabling the LLM to model temporal dynamics using its native language modeling\ncapabilities. Finally, we perform instruction tuning on both ECG question\nanswering and diagnostic report generation. Without modifying the core model,\nDiagECG achieves strong performance across tasks while maintaining\ngeneralization to out-of-distribution settings. Extensive experiments\ndemonstrate the effectiveness of each component and highlight the potential of\nintegrating symbolic ECG representations into LLMs for medical reasoning."}
{"id": "2508.15392", "pdf": "https://arxiv.org/pdf/2508.15392.pdf", "abs": "https://arxiv.org/abs/2508.15392", "title": "CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials", "authors": ["Chenghao Zhang", "Qingqing Long", "Ludi Wang", "Wenjuan Cui", "Jianjun Yu", "Yi Du"], "categories": ["cs.LG", "cs.CL"], "comment": "23 pages, 4 figures,", "summary": "Text-attributed graphs(TAGs) are pervasive in real-world systems,where each\nnode carries its own textual features. In many cases these graphs are\ninherently heterogeneous, containing multiple node types and diverse edge\ntypes. Despite the ubiquity of such heterogeneous TAGs, there remains a lack of\nlarge-scale benchmark datasets. This shortage has become a critical bottleneck,\nhindering the development and fair comparison of representation learning\nmethods on heterogeneous text-attributed graphs. In this paper, we introduce\nCITE - Catalytic Information Textual Entities Graph, the first and largest\nheterogeneous text-attributed citation graph benchmark for catalytic materials.\nCITE comprises over 438K nodes and 1.2M edges, spanning four relation types. In\naddition, we establish standardized evaluation procedures and conduct extensive\nbenchmarking on the node classification task, as well as ablation experiments\non the heterogeneous and textual properties of CITE. We compare four classes of\nlearning paradigms, including homogeneous graph models, heterogeneous graph\nmodels, LLM(Large Language Model)-centric models, and LLM+Graph models. In a\nnutshell, we provide (i) an overview of the CITE dataset, (ii) standardized\nevaluation protocols, and (iii) baseline and ablation experiments across\ndiverse modeling paradigms."}
{"id": "2508.15411", "pdf": "https://arxiv.org/pdf/2508.15411.pdf", "abs": "https://arxiv.org/abs/2508.15411", "title": "Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems", "authors": ["Frederik Vandeputte"], "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "Generative AI (GenAI) has emerged as a transformative technology,\ndemonstrating remarkable capabilities across diverse application domains.\nHowever, GenAI faces several major challenges in developing reliable and\nefficient GenAI-empowered systems due to its unpredictability and inefficiency.\nThis paper advocates for a paradigm shift: future GenAI-native systems should\nintegrate GenAI's cognitive capabilities with traditional software engineering\nprinciples to create robust, adaptive, and efficient systems.\n  We introduce foundational GenAI-native design principles centered around five\nkey pillars -- reliability, excellence, evolvability, self-reliance, and\nassurance -- and propose architectural patterns such as GenAI-native cells,\norganic substrates, and programmable routers to guide the creation of resilient\nand self-evolving systems. Additionally, we outline the key ingredients of a\nGenAI-native software stack and discuss the impact of these systems from\ntechnical, user adoption, economic, and legal perspectives, underscoring the\nneed for further validation and experimentation. Our work aims to inspire\nfuture research and encourage relevant communities to implement and refine this\nconceptual framework."}
{"id": "2508.15432", "pdf": "https://arxiv.org/pdf/2508.15432.pdf", "abs": "https://arxiv.org/abs/2508.15432", "title": "GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO", "authors": ["Bidyapati Pradhan", "Surajit Dasgupta", "Amit Kumar Saha", "Omkar Anustoop", "Sriram Puttagunta", "Vipul Mittal", "Gopal Sarda"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The advancement of large language models (LLMs) is critically dependent on\nthe availability of high-quality datasets for Supervised Fine-Tuning (SFT),\nalignment tasks like Direct Preference Optimization (DPO), etc. In this work,\nwe present a comprehensive synthetic data generation framework that facilitates\nscalable, configurable, and high-fidelity generation of synthetic data tailored\nfor these training paradigms. Our approach employs a modular and\nconfiguration-based pipeline capable of modeling complex dialogue flows with\nminimal manual intervention. This framework uses a dual-stage quality tagging\nmechanism, combining heuristic rules and LLM-based evaluations, to\nautomatically filter and score data extracted from OASST-formatted\nconversations, ensuring the curation of high-quality dialogue samples. The\nresulting datasets are structured under a flexible schema supporting both SFT\nand DPO use cases, enabling seamless integration into diverse training\nworkflows. Together, these innovations offer a robust solution for generating\nand managing synthetic conversational data at scale, significantly reducing the\noverhead of data preparation in LLM training pipelines."}
{"id": "2508.15637", "pdf": "https://arxiv.org/pdf/2508.15637.pdf", "abs": "https://arxiv.org/abs/2508.15637", "title": "Classification errors distort findings in automated speech processing: examples and solutions from child-development research", "authors": ["Lucas Gautheron", "Evan Kidd", "Anton Malko", "Marvin Lavechin", "Alejandrina Cristia"], "categories": ["cs.LG", "cs.CL", "stat.AP"], "comment": null, "summary": "With the advent of wearable recorders, scientists are increasingly turning to\nautomated methods of analysis of audio and video data in order to measure\nchildren's experience, behavior, and outcomes, with a sizable literature\nemploying long-form audio-recordings to study language acquisition. While\nnumerous articles report on the accuracy and reliability of the most popular\nautomated classifiers, less has been written on the downstream effects of\nclassification errors on measurements and statistical inferences (e.g., the\nestimate of correlations and effect sizes in regressions). This paper proposes\na Bayesian approach to study the effects of algorithmic errors on key\nscientific questions, including the effect of siblings on children's language\nexperience and the association between children's production and their input.\nIn both the most commonly used \\gls{lena}, and an open-source alternative (the\nVoice Type Classifier from the ACLEW system), we find that classification\nerrors can significantly distort estimates. For instance, automated annotations\nunderestimated the negative effect of siblings on adult input by 20--80\\%,\npotentially placing it below statistical significance thresholds. We further\nshow that a Bayesian calibration approach for recovering unbiased estimates of\neffect sizes can be effective and insightful, but does not provide a fool-proof\nsolution. Both the issue reported and our solution may apply to any classifier\ninvolving event detection and classification with non-zero error rates."}
{"id": "2508.15757", "pdf": "https://arxiv.org/pdf/2508.15757.pdf", "abs": "https://arxiv.org/abs/2508.15757", "title": "Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback", "authors": ["Yuxing Lu", "Yucheng Hu", "Nan Sun", "Xukai Zhao"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Configuration optimization remains a critical bottleneck in machine learning,\nrequiring coordinated tuning across model architecture, training strategy,\nfeature engineering, and hyperparameters. Traditional approaches treat these\ndimensions independently and lack interpretability, while recent automated\nmethods struggle with dynamic adaptability and semantic reasoning about\noptimization decisions. We introduce Language-Guided Tuning (LGT), a novel\nframework that employs multi-agent Large Language Models to intelligently\noptimize configurations through natural language reasoning. We apply textual\ngradients - qualitative feedback signals that complement numerical optimization\nby providing semantic understanding of training dynamics and configuration\ninterdependencies. LGT coordinates three specialized agents: an Advisor that\nproposes configuration changes, an Evaluator that assesses progress, and an\nOptimizer that refines the decision-making process, creating a self-improving\nfeedback loop. Through comprehensive evaluation on six diverse datasets, LGT\ndemonstrates substantial improvements over traditional optimization methods,\nachieving performance gains while maintaining high interpretability."}
{"id": "2508.15763", "pdf": "https://arxiv.org/pdf/2508.15763.pdf", "abs": "https://arxiv.org/abs/2508.15763", "title": "Intern-S1: A Scientific Multimodal Foundation Model", "authors": ["Lei Bai", "Zhongrui Cai", "Maosong Cao", "Weihan Cao", "Chiyu Chen", "Haojiong Chen", "Kai Chen", "Pengcheng Chen", "Ying Chen", "Yongkang Chen", "Yu Cheng", "Yu Cheng", "Pei Chu", "Tao Chu", "Erfei Cui", "Ganqu Cui", "Long Cui", "Ziyun Cui", "Nianchen Deng", "Ning Ding", "Nanqin Dong", "Peijie Dong", "Shihan Dou", "Sinan Du", "Haodong Duan", "Caihua Fan", "Ben Gao", "Changjiang Gao", "Jianfei Gao", "Songyang Gao", "Yang Gao", "Zhangwei Gao", "Jiaye Ge", "Qiming Ge", "Lixin Gu", "Yuzhe Gu", "Aijia Guo", "Qipeng Guo", "Xu Guo", "Conghui He", "Junjun He", "Yili Hong", "Siyuan Hou", "Caiyu Hu", "Hanglei Hu", "Jucheng Hu", "Ming Hu", "Zhouqi Hua", "Haian Huang", "Junhao Huang", "Xu Huang", "Zixian Huang", "Zhe Jiang", "Lingkai Kong", "Linyang Li", "Peiji Li", "Pengze Li", "Shuaibin Li", "Tianbin Li", "Wei Li", "Yuqiang Li", "Dahua Lin", "Junyao Lin", "Tianyi Lin", "Zhishan Lin", "Hongwei Liu", "Jiangning Liu", "Jiyao Liu", "Junnan Liu", "Kai Liu", "Kaiwen Liu", "Kuikun Liu", "Shichun Liu", "Shudong Liu", "Wei Liu", "Xinyao Liu", "Yuhong Liu", "Zhan Liu", "Yinquan Lu", "Haijun Lv", "Hongxia Lv", "Huijie Lv", "Qidang Lv", "Ying Lv", "Chengqi Lyu", "Chenglong Ma", "Jianpeng Ma", "Ren Ma", "Runmin Ma", "Runyuan Ma", "Xinzhu Ma", "Yichuan Ma", "Zihan Ma", "Sixuan Mi", "Junzhi Ning", "Wenchang Ning", "Xinle Pang", "Jiahui Peng", "Runyu Peng", "Yu Qiao", "Jiantao Qiu", "Xiaoye Qu", "Yuan Qu", "Yuchen Ren", "Fukai Shang", "Wenqi Shao", "Junhao Shen", "Shuaike Shen", "Chunfeng Song", "Demin Song", "Diping Song", "Chenlin Su", "Weijie Su", "Weigao Sun", "Yu Sun", "Qian Tan", "Cheng Tang", "Huanze Tang", "Kexian Tang", "Shixiang Tang", "Jian Tong", "Aoran Wang", "Bin Wang", "Dong Wang", "Lintao Wang", "Rui Wang", "Weiyun Wang", "Wenhai Wang", "Yi Wang", "Ziyi Wang", "Ling-I Wu", "Wen Wu", "Yue Wu", "Zijian Wu", "Linchen Xiao", "Shuhao Xing", "Chao Xu", "Huihui Xu", "Jun Xu", "Ruiliang Xu", "Wanghan Xu", "GanLin Yang", "Yuming Yang", "Haochen Ye", "Jin Ye", "Shenglong Ye", "Jia Yu", "Jiashuo Yu", "Jing Yu", "Fei Yuan", "Bo Zhang", "Chao Zhang", "Chen Zhang", "Hongjie Zhang", "Jin Zhang", "Qiaosheng Zhang", "Qiuyinzhe Zhang", "Songyang Zhang", "Taolin Zhang", "Wenlong Zhang", "Wenwei Zhang", "Yechen Zhang", "Ziyang Zhang", "Haiteng Zhao", "Qian Zhao", "Xiangyu Zhao", "Xiangyu Zhao", "Bowen Zhou", "Dongzhan Zhou", "Peiheng Zhou", "Yuhao Zhou", "Yunhua Zhou", "Dongsheng Zhu", "Lin Zhu", "Yicheng Zou"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "In recent years, a plethora of open-source foundation models have emerged,\nachieving remarkable progress in some widely attended fields, with performance\nbeing quite close to that of closed-source models. However, in high-value but\nmore challenging scientific professional fields, either the fields still rely\non expert models, or the progress of general foundation models lags\nsignificantly compared to those in popular areas, far from sufficient for\ntransforming scientific research and leaving substantial gap between\nopen-source models and closed-source models in these scientific domains. To\nmitigate this gap and explore a step further toward Artificial General\nIntelligence (AGI), we introduce Intern-S1, a specialized generalist equipped\nwith general understanding and reasoning capabilities with expertise to analyze\nmultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)\nmodel with 28 billion activated parameters and 241 billion total parameters,\ncontinually pre-trained on 5T tokens, including over 2.5T tokens from\nscientific domains. In the post-training stage, Intern-S1 undergoes offline and\nthen online reinforcement learning (RL) in InternBootCamp, where we propose\nMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks\nsimultaneously. Through integrated innovations in algorithms, data, and\ntraining systems, Intern-S1 achieved top-tier performance in online RL\ntraining.On comprehensive evaluation benchmarks, Intern-S1 demonstrates\ncompetitive performance on general reasoning tasks among open-source models and\nsignificantly outperforms open-source models in scientific domains, surpassing\nclosed-source state-of-the-art models in professional tasks, such as molecular\nsynthesis planning, reaction condition prediction, predicting thermodynamic\nstabilities for crystals. Our models are available at\nhttps://huggingface.co/internlm/Intern-S1."}
{"id": "2404.11916", "pdf": "https://arxiv.org/pdf/2404.11916.pdf", "abs": "https://arxiv.org/abs/2404.11916", "title": "Unplug and Play Language Models: Decomposing Experts in Language Models at Inference Time", "authors": ["Nakyeong Yang", "Jiwon Moon", "Junseok Kim", "Yunah Jang", "Kyomin Jung"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted to CIKM 2025", "summary": "Enabled by large-scale text corpora with huge parameters, pre-trained\nlanguage models operate as multi-task experts using a single model\narchitecture. However, recent studies have revealed that certain neurons play\ndisproportionately important roles in solving specific tasks, suggesting that\ntask-relevant substructures can be isolated and selectively activated for each\ntask. Therefore, we introduce Decomposition of Experts (DoE), a novel framework\nthat dynamically identifies and activates task-specific experts within a\nlanguage model to reduce inference cost without sacrificing accuracy. We first\ndefine a task expert as a set of parameters that significantly influence the\nperformance of a specific task and propose a four-step unplug-and-play process:\n(1) receiving a user request, (2) identifying the corresponding task expert,\n(3) performing inference using the expert-localized model, and (4) restoring\nthe original model and waiting for the next task. Using attribution methods and\nprompt tuning, DoE isolates task-relevant neurons, minimizing computational\noverhead while maintaining task performance. We assume a setting where a\nlanguage model receives user requests from five widely used natural language\nunderstanding benchmarks, processing one task at a time. In this setup, we\ndemonstrate that DoE achieves up to a x1.73 inference speed-up with a 65%\npruning rate, without compromising accuracy. Comparisons with various task\nexpert localization methods reveal that DoE effectively identifies task\nexperts, while ablation studies validate the importance of its components.\nAdditionally, we analyze the effects of batch size, token count, and layer\ntypes on inference speed-up, providing practical insights for adopting DoE. The\nproposed framework is both practical and scalable, applicable to any\ntransformer-based architecture, offering a robust solution for efficient\ntask-specific inference."}
{"id": "2406.10885", "pdf": "https://arxiv.org/pdf/2406.10885.pdf", "abs": "https://arxiv.org/abs/2406.10885", "title": "On the Role of Entity and Event Level Conceptualization in Generalizable Reasoning: A Survey of Tasks, Methods, Applications, and Future Directions", "authors": ["Weiqi Wang", "Tianqing Fang", "Haochen Shi", "Baixuan Xu", "Wenxuan Ding", "Liyu Zhang", "Wei Fan", "Jiaxin Bai", "Haoran Li", "Xin Liu", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Findings of EMNLP 2025", "summary": "Conceptualization, a fundamental element of human cognition, plays a pivotal\nrole in human generalizable reasoning. Generally speaking, it refers to the\nprocess of sequentially abstracting specific instances into higher-level\nconcepts and then forming abstract knowledge that can be applied in unfamiliar\nor novel situations. This enhances models' inferential capabilities and\nsupports the effective transfer of knowledge across various domains. Despite\nits significance, the broad nature of this term has led to inconsistencies in\nunderstanding conceptualization across various works, as there exists different\ntypes of instances that can be abstracted in a wide variety of ways. There is\nalso a lack of a systematic overview that comprehensively examines existing\nworks on the definition, execution, and application of conceptualization to\nenhance reasoning tasks. In this paper, we address these gaps by first\nproposing a categorization of different types of conceptualizations into four\nlevels based on the types of instances being conceptualized, in order to\nclarify the term and define the scope of our work. Then, we present the first\ncomprehensive survey of over 150 papers, surveying various definitions,\nresources, methods, and downstream applications related to conceptualization\ninto a unified taxonomy, with a focus on the entity and event levels.\nFurthermore, we shed light on potential future directions in this field and\nhope to garner more attention from the community."}
{"id": "2410.03730", "pdf": "https://arxiv.org/pdf/2410.03730.pdf", "abs": "https://arxiv.org/abs/2410.03730", "title": "Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs", "authors": ["Mehdi Ali", "Michael Fromm", "Klaudia Thellmann", "Jan Ebert", "Alexander Arno Weber", "Richard Rutmann", "Charvi Jain", "Max Lübbering", "Daniel Steinigen", "Johannes Leveling", "Katrin Klug", "Jasper Schulze Buschhoff", "Lena Jurkschat", "Hammam Abdelwahab", "Benny Jörg Stein", "Karl-Heinz Sylla", "Pavel Denisov", "Nicolo' Brandizzi", "Qasid Saleem", "Anirban Bhowmick", "Lennard Helmer", "Chelsea John", "Pedro Ortiz Suarez", "Malte Ostendorff", "Alex Jude", "Lalith Manjunath", "Samuel Weinbach", "Carolin Penke", "Oleg Filatov", "Fabio Barth", "Paramita Mirza", "Lucas Weber", "Ines Wendler", "Rafet Sifa", "Fabian Küch", "Andreas Herten", "René Jäkel", "Georg Rehm", "Stefan Kesselheim", "Joachim Köhler", "Nicolas Flores-Herr"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present two multilingual LLMs, Teuken 7B-base and Teuken 7B-instruct,\ndesigned to embrace Europe's linguistic diversity by supporting all 24 official\nlanguages of the European Union. Trained on a dataset comprising around 60%\nnon-English data and utilizing a custom multilingual tokenizer, our models\naddress the limitations of existing LLMs that predominantly focus on English or\na few high-resource languages. We detail the models' development principles,\ni.e., data composition, tokenizer optimization, and training methodologies. The\nmodels demonstrate strong performance across multilingual benchmarks, as\nevidenced by their performance on European versions of ARC, HellaSwag, and\nTruthfulQA."}
{"id": "2410.15186", "pdf": "https://arxiv.org/pdf/2410.15186.pdf", "abs": "https://arxiv.org/abs/2410.15186", "title": "Fine-tuning foundational models to code diagnoses from veterinary health records", "authors": ["Mayla R. Boguslav", "Adam Kiehl", "David Kott", "G. Joseph Strecker", "Tracy Webb", "Nadia Saklou", "Terri Ward", "Michael Kirby"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "26 pages, 5 figures", "summary": "Veterinary medical records represent a large data resource for application to\nveterinary and One Health clinical research efforts. Use of the data is limited\nby interoperability challenges including inconsistent data formats and data\nsiloing. Clinical coding using standardized medical terminologies enhances the\nquality of medical records and facilitates their interoperability with\nveterinary and human health records from other sites. Previous studies, such as\nDeepTag and VetTag, evaluated the application of Natural Language Processing\n(NLP) to automate veterinary diagnosis coding, employing long short-term memory\n(LSTM) and transformer models to infer a subset of Systemized Nomenclature of\nMedicine - Clinical Terms (SNOMED-CT) diagnosis codes from free-text clinical\nnotes. This study expands on these efforts by incorporating all 7,739 distinct\nSNOMED-CT diagnosis codes recognized by the Colorado State University (CSU)\nVeterinary Teaching Hospital (VTH) and by leveraging the increasing\navailability of pre-trained language models (LMs). 13 freely-available\npre-trained LMs were fine-tuned on the free-text notes from 246,473\nmanually-coded veterinary patient visits included in the CSU VTH's electronic\nhealth records (EHRs), which resulted in superior performance relative to\nprevious efforts. The most accurate results were obtained when expansive\nlabeled data were used to fine-tune relatively large clinical LMs, but the\nstudy also showed that comparable results can be obtained using more limited\nresources and non-clinical LMs. The results of this study contribute to the\nimprovement of the quality of veterinary EHRs by investigating accessible\nmethods for automated coding and support both animal and human health research\nby paving the way for more integrated and comprehensive health databases that\nspan species and institutions."}
{"id": "2412.13612", "pdf": "https://arxiv.org/pdf/2412.13612.pdf", "abs": "https://arxiv.org/abs/2412.13612", "title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition", "authors": ["Xuemei Tang", "Xufeng Duan", "Zhenguang G. Cai"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures, 5 tables, Accepted by EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) have emerged as a potential solution to automate\nthe complex processes involved in writing literature reviews, such as\nliterature collection, organization, and summarization. However, it is yet\nunclear how good LLMs are at automating comprehensive and reliable literature\nreviews. This study introduces a framework to automatically evaluate the\nperformance of LLMs in three key tasks of literature writing: reference\ngeneration, literature summary, and literature review composition. We introduce\nmultidimensional evaluation metrics that assess the hallucination rates in\ngenerated references and measure the semantic coverage and factual consistency\nof the literature summaries and compositions against human-written\ncounterparts. The experimental results reveal that even the most advanced\nmodels still generate hallucinated references, despite recent progress.\nMoreover, we observe that the performance of different models varies across\ndisciplines when it comes to writing literature reviews. These findings\nhighlight the need for further research and development to improve the\nreliability of LLMs in automating academic literature reviews."}
{"id": "2501.00712", "pdf": "https://arxiv.org/pdf/2501.00712.pdf", "abs": "https://arxiv.org/abs/2501.00712", "title": "Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding", "authors": ["Jiajun Zhu", "Peihao Wang", "Ruisi Cai", "Jason D. Lee", "Pan Li", "Zhangyang Wang"], "categories": ["cs.CL", "cs.LG", "I.2.6; I.2.7"], "comment": "ICML 2025", "summary": "Transformers rely on both content-based and position-based addressing\nmechanisms to make predictions, but existing positional encoding techniques\noften diminish the effectiveness of position-based addressing. Many current\nmethods enforce rigid patterns in attention maps, limiting the ability to model\nlong-range dependencies and adapt to diverse tasks. Additionally, most\npositional encodings are learned as general biases, lacking the specialization\nrequired for different instances within a dataset. To address this, we propose\ncon\\textbf{T}extualized equivari\\textbf{A}nt \\textbf{P}osition\n\\textbf{E}ncoding (\\textbf{TAPE}), a novel framework that enhances positional\nembeddings by incorporating sequence content across layers. TAPE introduces\ndynamic, context-aware positional encodings, overcoming the constraints of\ntraditional fixed patterns. We show that TAPE can provably facilitate LLM\nreasoning ability by emulating a broader class of algorithms. By enforcing\npermutation and orthogonal equivariance, TAPE ensures the stability of\npositional encodings during updates, improving long-context ability. Our method\ncan be easily integrated into pre-trained transformers, offering\nparameter-efficient fine-tuning with minimal overhead. Extensive experiments\nshow that TAPE achieves superior performance in language modeling, arithmetic\nreasoning, and long-context retrieval tasks compared to existing positional\nembedding techniques. Code is available at https://github.com/VITA-Group/TAPE."}
{"id": "2501.08312", "pdf": "https://arxiv.org/pdf/2501.08312.pdf", "abs": "https://arxiv.org/abs/2501.08312", "title": "Everybody Likes to Sleep: A Computer-Assisted Comparison of Object Naming Data from 30 Languages", "authors": ["Alžběta Kučerová", "Johann-Mattis List"], "categories": ["cs.CL"], "comment": "To appear in the Proceedings of the Global WordNet Conference 2025", "summary": "Object naming - the act of identifying an object with a word or a phrase - is\na fundamental skill in interpersonal communication, relevant to many\ndisciplines, such as psycholinguistics, cognitive linguistics, or language and\nvision research. Object naming datasets, which consist of concept lists with\npicture pairings, are used to gain insights into how humans access and select\nnames for objects in their surroundings and to study the cognitive processes\ninvolved in converting visual stimuli into semantic concepts. Unfortunately,\nobject naming datasets often lack transparency and have a highly idiosyncratic\nstructure. Our study tries to make current object naming data transparent and\ncomparable by using a multilingual, computer-assisted approach that links\nindividual items of object naming lists to unified concepts. Our current sample\nlinks 17 object naming datasets that cover 30 languages from 10 different\nlanguage families. We illustrate how the comparative dataset can be explored by\nsearching for concepts that recur across the majority of datasets and comparing\nthe conceptual spaces of covered object naming datasets with classical basic\nvocabulary lists from historical linguistics and linguistic typology. Our\nfindings can serve as a basis for enhancing cross-linguistic object naming\nresearch and as a guideline for future studies dealing with object naming\ntasks."}
{"id": "2502.06855", "pdf": "https://arxiv.org/pdf/2502.06855.pdf", "abs": "https://arxiv.org/abs/2502.06855", "title": "Self-Supervised Prompt Optimization", "authors": ["Jinyu Xiang", "Jiayi Zhang", "Zhaoyang Yu", "Xinbing Liang", "Fengwei Teng", "Jinhao Tu", "Fashen Ren", "Xiangru Tang", "Sirui Hong", "Chenglin Wu", "Yuyu Luo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Well-designed prompts are crucial for enhancing Large language models' (LLMs)\nreasoning capabilities while aligning their outputs with task requirements\nacross diverse domains. However, manually designed prompts require expertise\nand iterative experimentation. While existing prompt optimization methods aim\nto automate this process, they rely heavily on external references such as\nground truth or by humans, limiting their applicability in real-world scenarios\nwhere such data is unavailable or costly to obtain. To address this, we propose\nSelf-Supervised Prompt Optimization (SPO), a cost-efficient framework that\ndiscovers effective prompts for both closed and open-ended tasks without\nrequiring external reference. Motivated by the observations that prompt quality\nmanifests directly in LLM outputs and LLMs can effectively assess adherence to\ntask requirements, we derive evaluation and optimization signals purely from\noutput comparisons. Specifically, SPO selects superior prompts through pairwise\noutput comparisons evaluated by an LLM evaluator, followed by an LLM optimizer\nthat aligns outputs with task requirements. Extensive experiments demonstrate\nthat SPO outperforms state-of-the-art prompt optimization methods, achieving\ncomparable or superior results with significantly lower costs (e.g., 1.1% to\n5.6% of existing methods) and fewer samples (e.g., three samples). The code is\navailable at https://github.com/FoundationAgents/SPO."}
{"id": "2502.09183", "pdf": "https://arxiv.org/pdf/2502.09183.pdf", "abs": "https://arxiv.org/abs/2502.09183", "title": "RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation", "authors": ["Changzhi Zhou", "Xinyu Zhang", "Dandan Song", "Xiancai Chen", "Wanli Gu", "Huipeng Ma", "Yuhang Tian", "Mengdi Zhang", "Linmei Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Code generation has attracted increasing attention with the rise of Large\nLanguage Models (LLMs). Many studies have developed powerful code LLMs by\nsynthesizing code-related instruction data and applying supervised fine-tuning.\nHowever, these methods are limited by teacher model distillation and ignore the\npotential of iterative refinement by self-generated code. In this paper, we\npropose Adaptive Critique Refinement (ACR), which enables the model to refine\nitself by self-generated code and external critique, rather than directly\nimitating the code responses of the teacher model. Concretely, ACR includes a\ncomposite scoring system with LLM-as-a-Judge to evaluate the quality of code\nresponses and a selective critique strategy with LLM-as-a-Critic to critique\nself-generated low-quality code responses. We develop the RefineCoder series by\niteratively applying ACR, achieving continuous performance improvement on\nmultiple code generation benchmarks. Compared to the baselines of the same\nsize, our proposed RefineCoder series can achieve comparable or even superior\nperformance using less data."}
{"id": "2502.11491", "pdf": "https://arxiv.org/pdf/2502.11491.pdf", "abs": "https://arxiv.org/abs/2502.11491", "title": "Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering", "authors": ["Runxuan Liu", "Bei Luo", "Jiaqi Li", "Baoxin Wang", "Ming Liu", "Dayong Wu", "Shijin Wang", "Bing Qin"], "categories": ["cs.CL", "cs.AI"], "comment": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers)", "summary": "Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing. However, in knowledge graph question answering tasks\n(KGQA), there remains the issue of answering questions that require multi-hop\nreasoning. Existing methods rely on entity vector matching, but the purpose of\nthe question is abstract and difficult to match with specific entities. As a\nresult, it is difficult to establish reasoning paths to the purpose, which\nleads to information loss and redundancy. To address this issue, inspired by\nhuman reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a\nnovel framework that constructs reasoning paths from purposes back to\nconditions. ORT operates in three key phases: (1) using LLM to extract purpose\nlabels and condition labels, (2) constructing label reasoning paths based on\nthe KG ontology, and (3) using the label reasoning paths to guide knowledge\nretrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves\nstate-of-the-art performance and significantly enhances the capability of LLMs\nfor KGQA."}
{"id": "2502.15429", "pdf": "https://arxiv.org/pdf/2502.15429.pdf", "abs": "https://arxiv.org/abs/2502.15429", "title": "Pub-Guard-LLM: Detecting Retracted Biomedical Articles with Reliable Explanations", "authors": ["Lihu Chen", "Shuojie Fu", "Gabriel Freedman", "Cemre Zor", "Guy Martin", "James Kinross", "Uddhav Vaghela", "Ovidiu Serban", "Francesca Toni"], "categories": ["cs.CL"], "comment": "long paper under review", "summary": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool."}
{"id": "2502.15600", "pdf": "https://arxiv.org/pdf/2502.15600.pdf", "abs": "https://arxiv.org/abs/2502.15600", "title": "Robust Bias Detection in MLMs and its Application to Human Trait Ratings", "authors": ["Ingroj Shrestha", "Louis Tay", "Padmini Srinivasan"], "categories": ["cs.CL"], "comment": "Findings of NAACL 2025", "summary": "There has been significant prior work using templates to study bias against\ndemographic attributes in MLMs. However, these have limitations: they overlook\nrandom variability of templates and target concepts analyzed, assume equality\namongst templates, and overlook bias quantification. Addressing these, we\npropose a systematic statistical approach to assess bias in MLMs, using mixed\nmodels to account for random effects, pseudo-perplexity weights for sentences\nderived from templates and quantify bias using statistical effect sizes.\nReplicating prior studies, we match on bias scores in magnitude and direction\nwith small to medium effect sizes. Next, we explore the novel problem of gender\nbias in the context of $\\emph{personality}$ and $\\textit{character}$ traits,\nacross seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased\nfor binary gender but the most biased for non-binary $\\textit{neo}$, while\nRoBERTa-large is the most biased for binary gender but shows small to no bias\nfor $\\textit{neo}$. There is some alignment of MLM bias and findings in\npsychology (human perspective) - in $\\textit{agreeableness}$ with RoBERTa-large\nand $\\textit{emotional stability}$ with BERT-large. There is general agreement\nfor the remaining 3 personality dimensions: both sides observe at most small\ndifferences across gender. For character traits, human studies on gender bias\nare limited thus comparisons are not feasible."}
{"id": "2502.15860", "pdf": "https://arxiv.org/pdf/2502.15860.pdf", "abs": "https://arxiv.org/abs/2502.15860", "title": "Synthetic vs. Gold: The Role of LLM Generated Labels and Data in Cyberbullying Detection", "authors": ["Arefeh Kazemi", "Sri Balaaji Natarajan Kalaivendan", "Joachim Wagner", "Hamza Qadeer", "Kanishk Verma", "Brian Davis"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Cyberbullying (CB) presents a pressing threat, especially to children,\nunderscoring the urgent need for robust detection systems to ensure online\nsafety. While large-scale datasets on online abuse exist, there remains a\nsignificant gap in labeled data that specifically reflects the language and\ncommunication styles used by children. The acquisition of such data from\nvulnerable populations, such as children, is challenging due to ethical, legal\nand technical barriers. Moreover, the creation of these datasets relies heavily\non human annotation, which not only strains resources but also raises\nsignificant concerns due to annotators exposure to harmful content. In this\npaper, we address these challenges by leveraging Large Language Models (LLMs)\nto generate synthetic data and labels. Our experiments demonstrate that\nsynthetic data enables BERT-based CB classifiers to achieve performance close\nto that of those trained on fully authentic datasets (75.8% vs. 81.5%\naccuracy). Additionally, LLMs can effectively label authentic yet unlabeled\ndata, allowing BERT classifiers to attain a comparable performance level (79.1%\nvs. 81.5% accuracy). These results highlight the potential of LLMs as a\nscalable, ethical, and cost-effective solution for generating data for CB\ndetection."}
{"id": "2503.01539", "pdf": "https://arxiv.org/pdf/2503.01539.pdf", "abs": "https://arxiv.org/abs/2503.01539", "title": "Pragmatic Inference Chain (PIC) Improving LLMs' Reasoning of Authentic Implicit Toxic Language", "authors": ["Xi Chen", "Shuo Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures, 2 tables", "summary": "The rapid development of large language models (LLMs) gives rise to ethical\nconcerns about their performance, while opening new avenues for developing\ntoxic language detection techniques. However, LLMs' unethical output and their\ncapability of detecting toxicity have primarily been tested on language data\nthat do not demand complex meaning inference, such as the biased associations\nof 'he' with programmer and 'she' with household. Nowadays toxic language\nadopts a much more creative range of implicit forms, thanks to advanced\ncensorship. In this study, we collect authentic toxic interactions that evade\nonline censorship and that are verified by human annotators as\ninference-intensive. To evaluate and improve LLMs' reasoning of the authentic\nimplicit toxic language, we propose a new prompting method, Pragmatic Inference\nChain (PIC), drawn on interdisciplinary findings from cognitive science and\nlinguistics. The PIC prompting significantly improves the success rate of\nGPT-4o, Llama-3.1-70B-Instruct, DeepSeek-v2.5, and DeepSeek-v3 in identifying\nimplicit toxic language, compared to five baseline prompts, such as CoT and\nrule-based baselines. In addition, it also facilitates the models to produce\nmore explicit and coherent reasoning processes, hence can potentially be\ngeneralized to other inference-intensive tasks, e.g., understanding humour and\nmetaphors."}
{"id": "2503.11377", "pdf": "https://arxiv.org/pdf/2503.11377.pdf", "abs": "https://arxiv.org/abs/2503.11377", "title": "Advancing the Database of Cross-Linguistic Colexifications with New Workflows and Data", "authors": ["Annika Tjuka", "Robert Forkel", "Christoph Rzymski", "Johann-Mattis List"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Lexical resources are crucial for cross-linguistic analysis and can provide\nnew insights into computational models for natural language learning. Here, we\npresent an advanced database for comparative studies of words with multiple\nmeanings, a phenomenon known as colexification. The new version includes\nimprovements in the handling, selection and presentation of the data. We\ncompare the new database with previous versions and find that our improvements\nprovide a more balanced sample covering more language families worldwide, with\nenhanced data quality, given that all word forms are provided in phonetic\ntranscription. We conclude that the new Database of Cross-Linguistic\nColexifications has the potential to inspire exciting new studies that link\ncross-linguistic data to open questions in linguistic typology, historical\nlinguistics, psycholinguistics, and computational linguistics."}
{"id": "2503.16622", "pdf": "https://arxiv.org/pdf/2503.16622.pdf", "abs": "https://arxiv.org/abs/2503.16622", "title": "Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation", "authors": ["Michele Fiori", "Gabriele Civitarese", "Priyankar Choudhary", "Claudio Bettini"], "categories": ["cs.CL"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning\nof machine learning models. In IoT systems, XAI improves the transparency of\nmodels processing sensor data from multiple heterogeneous devices, ensuring\nend-users understand and trust their outputs. Among the many applications, XAI\nhas also been applied to sensor-based Activities of Daily Living (ADLs)\nrecognition in smart homes. Existing approaches highlight which sensor events\nare most important for each predicted activity, using simple rules to convert\nthese events into natural language explanations for non-expert users. However,\nthese methods produce rigid explanations lacking natural language flexibility\nand are not scalable. With the recent rise of Large Language Models (LLMs), it\nis worth exploring whether they can enhance explanation generation, considering\ntheir proven knowledge of human activities. This paper investigates potential\napproaches to combine XAI and LLMs for sensor-based ADL recognition. We\nevaluate if LLMs can be used: a) as explainable zero-shot ADL recognition\nmodels, avoiding costly labeled data collection, and b) to automate the\ngeneration of explanations for existing data-driven XAI approaches when\ntraining data is available and the goal is higher recognition rates. Our\ncritical evaluation provides insights into the benefits and challenges of using\nLLMs for explainable ADL recognition."}
{"id": "2504.00406", "pdf": "https://arxiv.org/pdf/2504.00406.pdf", "abs": "https://arxiv.org/abs/2504.00406", "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning", "authors": ["Jiuzhou Han", "Wray Buntine", "Ehsan Shareghi"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Large language models demonstrate remarkable reasoning capabilities but often\nproduce unreliable or incorrect responses. Existing verification methods are\ntypically model-specific or domain-restricted, requiring significant\ncomputational resources and lacking scalability across diverse reasoning tasks.\nTo address these limitations, we propose VerifiAgent, a unified verification\nagent that integrates two levels of verification: meta-verification, which\nassesses completeness and consistency in model responses, and tool-based\nadaptive verification, where VerifiAgent autonomously selects appropriate\nverification tools based on the reasoning type, including mathematical,\nlogical, or commonsense reasoning. This adaptive approach ensures both\nefficiency and robustness across different verification scenarios. Experimental\nresults show that VerifiAgent outperforms baseline verification methods (e.g.,\ndeductive verifier, backward verifier) among all reasoning tasks. Additionally,\nit can further enhance reasoning accuracy by leveraging feedback from\nverification results. VerifiAgent can also be effectively applied to inference\nscaling, achieving better results with fewer generated samples and costs\ncompared to existing process reward models in the mathematical reasoning\ndomain. Code is available at https://github.com/Jiuzhouh/VerifiAgent"}
{"id": "2504.11169", "pdf": "https://arxiv.org/pdf/2504.11169.pdf", "abs": "https://arxiv.org/abs/2504.11169", "title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos", "authors": ["Laura De Grazia", "Pol Pastells", "Mauro Vázquez Chas", "Desmond Elliott", "Danae Sánchez Villegas", "Mireia Farrús", "Mariona Taulé"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025 camera-ready version: expanded Section 4.3 with an\n  additional experiment using an extended definition-based prompt (including a\n  definition of sexist content), and applied minor corrections", "summary": "Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contributions of textual,\nvocal, and visual modalities to the classification of content as either sexist\nor non-sexist; and (3) we evaluate a range of large language models (LLMs) and\nmultimodal LLMs on the task of sexism detection. We find that visual\ninformation plays a key role in labeling sexist content for both humans and\nmodels. Models effectively detect explicit sexism; however, they struggle with\nimplicit cases, such as stereotypes, instances where annotators also show low\nagreement. This highlights the inherent difficulty of the task, as identifying\nimplicit sexism depends on the social and cultural context."}
{"id": "2504.15120", "pdf": "https://arxiv.org/pdf/2504.15120.pdf", "abs": "https://arxiv.org/abs/2504.15120", "title": "Kuwain 1.5B: An Arabic SLM via Language Injection", "authors": ["Khalil Hennara", "Sara Chrouf", "Mohamed Motaism Hamed", "Zeina Aldallal", "Omar Hadid", "Safwan AlModhayan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses."}
{"id": "2504.15640", "pdf": "https://arxiv.org/pdf/2504.15640.pdf", "abs": "https://arxiv.org/abs/2504.15640", "title": "Cequel: Cost-Effective Querying of Large Language Models for Text Clustering", "authors": ["Hongtao Wang", "Taiyan Zhang", "Renchi Yang", "Jianliang Xu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text clustering aims to automatically partition a collection of documents\ninto coherent groups based on their linguistic features. In the literature,\nthis task is formulated either as metric clustering over pre-trained text\nembeddings or as graph clustering based on pairwise similarities derived from\nan oracle, e.g., a large machine learning model. Recent advances in large\nlanguage models (LLMs) have significantly improved this field by providing\nhigh-quality contextualized embeddings and accurate semantic similarity\nestimates. However, leveraging LLMs at scale introduces substantial\ncomputational and financial costs due to the large number of required API\nqueries or inference calls. To address this issue, we propose Cequel, a\ncost-effective framework that achieves accurate text clustering under a limited\nbudget of LLM queries. At its core, Cequel constructs must-link and cannot-link\nconstraints by selectively querying LLMs on informative text pairs or triplets,\nidentified via our proposed algorithms, EdgeLLM and TriangleLLM. These\nconstraints are then utilized in a weighted constrained clustering algorithm to\nform high-quality clusters. Specifically, EdgeLLM and TriangleLLM employ\ncarefully designed greedy selection strategies and prompting techniques to\nidentify and extract informative constraints efficiently. Experiments on\nmultiple benchmark datasets demonstrate that Cequel consistently outperforms\nexisting methods in unsupervised text clustering under the same query budget."}
{"id": "2504.19675", "pdf": "https://arxiv.org/pdf/2504.19675.pdf", "abs": "https://arxiv.org/abs/2504.19675", "title": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs", "authors": ["Osma Suominen", "Juho Inkinen", "Mona Lehtinen"], "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG", "I.2.7"], "comment": "6 pages, 4 figures, published at SemEval-2025 workshop Task 5:\n  LLMs4Subjects: https://aclanthology.org/2025.semeval-1.315/", "summary": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts."}
{"id": "2504.21024", "pdf": "https://arxiv.org/pdf/2504.21024.pdf", "abs": "https://arxiv.org/abs/2504.21024", "title": "WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model", "authors": ["Tianqing Fang", "Hongming Zhang", "Zhisong Zhang", "Kaixin Ma", "Wenhao Yu", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Agent self-improvement, where the backbone Large Language Model (LLM) of the\nagent are trained on trajectories sampled autonomously based on their own\npolicies, has emerged as a promising approach for enhancing performance. Recent\nadvancements, particularly in web environments, face a critical limitation:\ntheir performance will reach a stagnation point during autonomous learning\ncycles, hindering further improvement. We argue that this stems from limited\nexploration of the web environment and insufficient exploitation of pre-trained\nweb knowledge in LLMs. To improve the performance of self-improvement, we\npropose a novel framework that introduces a co-evolving World Model LLM. This\nworld model predicts the next observation based on the current observation and\naction within the web environment. Leveraging LLMs' pretrained knowledge of\nabundant web content, the World Model serves dual roles: (1) as a virtual web\nserver generating self-instructed training data to continuously refine the\nagent's policy, and (2) as an imagination engine during inference, enabling\nlook-ahead simulation to guide action selection for the agent LLM. Experiments\nin real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a\n10% performance gain over existing self-evolving agents, demonstrating the\nefficacy and generalizability of our approach, without using any distillation\nfrom more powerful close-sourced models. Our work establishes the necessity of\nintegrating world models into autonomous agent frameworks to unlock sustained\nadaptability. Code is available at https://github.com/Tencent/SelfEvolvingAgent"}
{"id": "2504.21635", "pdf": "https://arxiv.org/pdf/2504.21635.pdf", "abs": "https://arxiv.org/abs/2504.21635", "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model", "authors": ["Zeina Aldallal", "Sara Chrouf", "Khalil Hennara", "Mohamed Motaism Hamed", "Muhammad Hreden", "Safwan AlModhayan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools."}
{"id": "2505.17894", "pdf": "https://arxiv.org/pdf/2505.17894.pdf", "abs": "https://arxiv.org/abs/2505.17894", "title": "Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model", "authors": ["Khalil Hennara", "Muhammad Hreden", "Mohamed Motaism Hamed", "Zeina Aldallal", "Sara Chrouf", "Safwan AlModhayan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce Mutarjim, a compact yet powerful language model for\nbidirectional Arabic-English translation. While large-scale LLMs have shown\nimpressive progress in natural language processing tasks, including machine\ntranslation, smaller models. Leveraging this insight, we developed Mutarjim\nbased on Kuwain-1.5B , a language model tailored for both Arabic and English.\nDespite its modest size, Mutarjim outperforms much larger models on several\nestablished benchmarks, achieved through an optimized two-phase training\napproach and a carefully curated, high-quality training corpus.. Experimental\nresults show that Mutarjim rivals models up to 20 times larger while\nsignificantly reducing computational costs and training requirements. We also\nintroduce Tarjama-25, a new benchmark designed to overcome limitations in\nexisting Arabic-English benchmarking datasets, such as domain narrowness, short\nsentence lengths, and English-source bias. Tarjama-25 comprises 5,000\nexpert-reviewed sentence pairs and spans a wide range of domains, offering a\nmore comprehensive and balanced evaluation framework. Notably, Mutarjim\nachieves state-of-the-art performance on the English-to-Arabic task in\nTarjama-25, surpassing even significantly larger and proprietary models like\nGPT-4o mini. We publicly release Tarjama-25 to support future research and\nadvance the evaluation of Arabic-English translation systems."}
{"id": "2505.20282", "pdf": "https://arxiv.org/pdf/2505.20282.pdf", "abs": "https://arxiv.org/abs/2505.20282", "title": "One-shot Entropy Minimization", "authors": ["Zitian Gao", "Lynx Chen", "Haoming Luo", "Joey Zhou", "Bryan Dai"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em."}
{"id": "2506.00307", "pdf": "https://arxiv.org/pdf/2506.00307.pdf", "abs": "https://arxiv.org/abs/2506.00307", "title": "Lossless Token Sequence Compression via Meta-Tokens", "authors": ["John Harvill", "Ziwei Fan", "Hao Wang", "Luke Huan", "Anoop Deoras", "Yizhou Sun", "Hao Ding"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 8 figures", "summary": "Existing work on prompt compression for Large Language Models (LLM) focuses\non lossy methods that try to maximize the retention of semantic information\nthat is relevant to downstream tasks while significantly reducing the sequence\nlength. In this paper, we introduce a task-agnostic lossless compression\ntechnique similar to LZ77 that makes it possible to reduce the input token\nsequence length on average by 27\\% and 18\\% for the two evaluation tasks\nexplored here. Given that we use transformer-based LLMs, this equates to 47\\%\nand 33\\% less encoding computation, respectively, due to the quadratic nature\nof attention. The token sequence transformation is trivial to reverse and\nhighlights that no semantic information is lost in the process. We evaluate our\nproposed approach on two tasks that require strict preservation of\nsemantics/syntax and demonstrate that existing lossy compression methods\nperform poorly in this setting. We find that our lossless compression technique\nproduces only a small gap in performance compared to using the uncompressed\ninput and posit that larger models and an expanded computing budget would\nlikely erase the gap entirely."}
{"id": "2506.06561", "pdf": "https://arxiv.org/pdf/2506.06561.pdf", "abs": "https://arxiv.org/abs/2506.06561", "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles", "authors": ["Ho Yin 'Sam' Ng", "Ting-Yao Hsu", "Aashish Anantha Ramakrishnan", "Branislav Kveton", "Nedim Lipka", "Franck Dernoncourt", "Dongwon Lee", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Ting-Hao 'Kenneth' Huang"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted to EMNLP 2025 Findings. The LaMP-CAP dataset is publicly\n  available at: https://github.com/Crowd-AI-Lab/lamp-cap", "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones."}
{"id": "2506.08768", "pdf": "https://arxiv.org/pdf/2506.08768.pdf", "abs": "https://arxiv.org/abs/2506.08768", "title": "AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP", "authors": ["Ahmed Hasanaath", "Aisha Alansari", "Ahmed Ashraf", "Chafik Salmane", "Hamzah Luqman", "Saad Ezzini"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable progress in reasoning\nabilities and general natural language processing (NLP) tasks, yet their\nperformance on Arabic data, characterized by rich morphology, diverse dialects,\nand complex script, remains underexplored. This paper presents a comprehensive\nbenchmarking study of multiple reasoning-focused LLMs, with a special emphasis\non the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP\ntasks. We experiment with various strategies, including zero-shot, few-shot,\nand fine-tuning. This allows us to systematically evaluate performance on\ndatasets covering a range of applications to examine their capacity for\nlinguistic reasoning under different levels of complexity. Our experiments\nreveal several key findings. First, carefully selecting just three in-context\nexamples delivers an average uplift of over 13 F1 points on classification\ntasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection\nfrom 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures\noutperform a strong GPT o4-mini baseline by an average of 12 F1 points on\ncomplex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning\nyields up to an additional 8 points in F1 and BLEU compared to equivalent\nincreases in model scale. The code is available at\nhttps://anonymous.4open.science/r/AraReasoner41299"}
{"id": "2506.19073", "pdf": "https://arxiv.org/pdf/2506.19073.pdf", "abs": "https://arxiv.org/abs/2506.19073", "title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanations", "authors": ["Jackson Trager", "Francielle Vargas", "Diego Alves", "Matteo Guida", "Mikel K. Ngueajio", "Flor Plaza-del-Arco", "Yalda Daryanai", "Farzan Karimi-Malekabadi", "Ameeta Agrawal"], "categories": ["cs.CL"], "comment": "Findings of the Association for Computational Linguistics: EMNLP\n  2025; *These authors contributed equally", "summary": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via hate speech multi-hop explanation\nusing Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Empirical results\nhighlight a misalignment between LLM outputs and human annotations in moral\nreasoning tasks. While LLMs perform well in hate speech detection (F1 up to\n0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).\nFurthermore, rationale alignment remains limited mainly in underrepresented\nlanguages. These findings show the limited capacity of current LLMs to\ninternalize and reflect human moral reasoning."}
{"id": "2506.21584", "pdf": "https://arxiv.org/pdf/2506.21584.pdf", "abs": "https://arxiv.org/abs/2506.21584", "title": "Empirical Evidence for Alignment Faking in a Small LLM and Prompt-Based Mitigation Techniques", "authors": ["J. Koorndijk"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Current literature suggests that alignment faking (deceptive alignment) is an\nemergent property of large language models. We present the first empirical\nevidence that a small instruction-tuned model, specifically LLaMA 3 8B, can\nexhibit alignment faking. We further show that prompt-only interventions,\nincluding deontological moral framing and scratchpad reasoning, significantly\nreduce this behavior without modifying model internals. This challenges the\nassumption that prompt-based ethics are trivial and that deceptive alignment\nrequires scale. We introduce a taxonomy distinguishing shallow deception,\nshaped by context and suppressible through prompting, from deep deception,\nwhich reflects persistent, goal-driven misalignment. Our findings refine the\nunderstanding of deception in language models and underscore the need for\nalignment evaluations across model sizes and deployment settings."}
{"id": "2507.07441", "pdf": "https://arxiv.org/pdf/2507.07441.pdf", "abs": "https://arxiv.org/abs/2507.07441", "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "authors": ["Yu Xia", "Yiran Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Large Language Model (LLM) agents are commonly tuned with supervised\nfinetuning on ReAct-style expert trajectories or preference optimization over\npairwise rollouts. Most of these methods focus on imitating specific expert\nbehaviors or promoting chosen reasoning thoughts and actions over rejected\nones. However, without reasoning and comparing over alternatives actions, LLM\nagents finetuned with these methods may over-commit towards seemingly plausible\nbut suboptimal actions due to limited action space exploration. To address\nthis, in this paper we propose Self-taught ActioN Deliberation (SAND)\nframework, enabling LLM agents to explicitly deliberate over candidate actions\nbefore committing to one. To tackle the challenges of when and what to\ndeliberate given large action space and step-level action evaluation, we\nincorporate self-consistency action sampling and execution-guided action\ncritique to help synthesize step-wise action deliberation thoughts using the\nbase model of the LLM agent. In an iterative manner, the deliberation\ntrajectories are then used to finetune the LLM agent itself. Evaluating on two\nrepresentative interactive agent tasks, SAND achieves an average 20%\nimprovement over initial supervised finetuning and also outperforms\nstate-of-the-art agent tuning approaches."}
{"id": "2507.09709", "pdf": "https://arxiv.org/pdf/2507.09709.pdf", "abs": "https://arxiv.org/abs/2507.09709", "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces", "authors": ["Baturay Saglam", "Paul Kassianik", "Blaine Nelson", "Sajana Weerawardhena", "Yaron Singer", "Amin Karbasi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. However, it remains\nunclear to what extent LLMs internally organize representations related to\nsemantic understanding. To explore this, we conduct a large-scale empirical\nstudy of hidden representations in 11 autoregressive models across 6 scientific\ntopics. We find that high-level semantic information consistently resides in\nlow-dimensional subspaces that form linearly separable representations across\ndomains. This separability becomes more pronounced in deeper layers and under\nprompts that elicit structured reasoning or alignment\nbehavior$\\unicode{x2013}$even when surface content remains unchanged. These\nfindings support geometry-aware tools that operate directly in latent space to\ndetect and mitigate harmful or adversarial content. As a proof of concept, we\ntrain an MLP probe on final-layer hidden states to act as a lightweight\nlatent-space guardrail. This approach substantially improves refusal rates on\nmalicious queries and prompt injections that bypass both the model's built-in\nsafety alignment and external token-level filters."}
{"id": "2507.13618", "pdf": "https://arxiv.org/pdf/2507.13618.pdf", "abs": "https://arxiv.org/abs/2507.13618", "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "authors": ["Shanbo Cheng", "Yu Bao", "Qian Cao", "Luyang Huang", "Liyan Kang", "Zhicheng Liu", "Yu Lu", "Wenhao Zhu", "Jingwen Chen", "Zhichao Huang", "Tao Li", "Yifu Li", "Huiying Lin", "Sitong Liu", "Ningxin Peng", "Shuaijie She", "Lu Xu", "Nuo Xu", "Sen Yang", "Runsheng Yu", "Yiming Yu", "Liehao Zou", "Hang Li", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications."}
{"id": "2507.20398", "pdf": "https://arxiv.org/pdf/2507.20398.pdf", "abs": "https://arxiv.org/abs/2507.20398", "title": "Length Representations in Large Language Models", "authors": ["Sangjun Moon", "Dasom Choi", "Jingun Kwon", "Hidetaka Kamigaito", "Manabu Okumura"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large language models (LLMs) have shown remarkable capabilities across\nvarious tasks, that are learned from massive amounts of text-based data.\nAlthough LLMs can control output sequence length, particularly in\ninstruction-based settings, the internal mechanisms behind this control have\nbeen unexplored yet. In this study, we provide empirical evidence on how output\nsequence length information is encoded within the internal representations in\nLLMs. In particular, our findings show that multi-head attention mechanisms are\ncritical in determining output sequence length, which can be adjusted in a\ndisentangled manner. By scaling specific hidden units within the model, we can\ncontrol the output sequence length without losing the informativeness of the\ngenerated text, thereby indicating that length information is partially\ndisentangled from semantic information. Moreover, some hidden units become\nincreasingly active as prompts become more length-specific, thus reflecting the\nmodel's internal awareness of this attribute. Our findings suggest that LLMs\nhave learned robust and adaptable internal mechanisms for controlling output\nlength without any external control."}
{"id": "2507.22752", "pdf": "https://arxiv.org/pdf/2507.22752.pdf", "abs": "https://arxiv.org/abs/2507.22752", "title": "CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset", "authors": ["Jindřich Libovický", "Jindřich Helcl", "Andrei Manea", "Gianluca Vico"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce CUS-QA, a benchmark for open-ended regional question answering\nthat encompasses both textual and visual modalities. We also provide strong\nbaselines using state-of-the-art large language models (LLMs). Our dataset\nconsists of manually curated questions and answers grounded in Wikipedia,\ncreated by native speakers from Czechia, Slovakia, and Ukraine, with\naccompanying English translations. It includes both purely textual questions\nand those requiring visual understanding. We evaluate state-of-the-art LLMs\nthrough prompting and complement this with human judgments of answer\ncorrectness. Using these human evaluations, we analyze the reliability of\nexisting automatic evaluation metrics. Our baseline results show that even the\nbest open-weight LLMs achieve only around 50% accuracy on textual questions and\nbelow 30% on visual questions. LLM-based evaluation metrics show strong\ncorrelation with human judgment, while traditional string-overlap metrics\nperform surprisingly well due to the prevalence of named entities in answers."}
{"id": "2508.02037", "pdf": "https://arxiv.org/pdf/2508.02037.pdf", "abs": "https://arxiv.org/abs/2508.02037", "title": "Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time", "authors": ["Huihan Li", "You Chen", "Siyuan Wang", "Yixin He", "Ninareh Mehrabi", "Rahul Gupta", "Xiang Ren"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) perform well on reasoning benchmarks but often\nfail when inputs alter slightly, raising concerns about the extent to which\ntheir success relies on memorization. This issue is especially acute in\nChain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger\nintermediate errors that cascade into incorrect final answers. We introduce\nSTIM, a novel framework for Source-aware Token-level Identification of\nMemorization, which attributes each token in a reasoning chain to one of\nmultiple memorization sources - local, mid-range, or long-range - based on\ntheir statistical co-occurrence with the token in the pretraining corpus. Our\ntoken-level analysis across tasks and distributional settings reveals that\nmodels rely more on memorization in complex or long-tail cases, and that local\nmemorization is often the dominant driver of errors, leading to up to 67% of\nwrong tokens. We also show that memorization scores from STIM can be effective\nin predicting the wrong tokens in the wrong reasoning step. STIM offers a\npowerful tool for diagnosing and improving model reasoning and can generalize\nto other structured step-wise generation tasks."}
{"id": "2508.07592", "pdf": "https://arxiv.org/pdf/2508.07592.pdf", "abs": "https://arxiv.org/abs/2508.07592", "title": "IBPS: Indian Bail Prediction System", "authors": ["Puspesh Kumar Srivastava", "Uddeshya Raj", "Praveen Patel", "Shubham Kumar Nigam", "Noel Shallum", "Arnab Bhattacharya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Bail decisions are among the most frequently adjudicated matters in Indian\ncourts, yet they remain plagued by subjectivity, delays, and inconsistencies.\nWith over 75% of India's prison population comprising undertrial prisoners,\nmany from socioeconomically disadvantaged backgrounds, the lack of timely and\nfair bail adjudication exacerbates human rights concerns and contributes to\nsystemic judicial backlog. In this paper, we present the Indian Bail Prediction\nSystem (IBPS), an AI-powered framework designed to assist in bail\ndecision-making by predicting outcomes and generating legally sound rationales\nbased solely on factual case attributes and statutory provisions. We curate and\nrelease a large-scale dataset of 150,430 High Court bail judgments, enriched\nwith structured annotations such as age, health, criminal history, crime\ncategory, custody duration, statutes, and judicial reasoning. We fine-tune a\nlarge language model using parameter-efficient techniques and evaluate its\nperformance across multiple configurations, with and without statutory context,\nand with RAG. Our results demonstrate that models fine-tuned with statutory\nknowledge significantly outperform baselines, achieving strong accuracy and\nexplanation quality, and generalize well to a test set independently annotated\nby legal experts. IBPS offers a transparent, scalable, and reproducible\nsolution to support data-driven legal assistance, reduce bail delays, and\npromote procedural fairness in the Indian judicial system."}
{"id": "2508.10021", "pdf": "https://arxiv.org/pdf/2508.10021.pdf", "abs": "https://arxiv.org/abs/2508.10021", "title": "LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients", "authors": ["Egor Fadeev", "Dzhambulat Mollaev", "Aleksei Shestov", "Dima Korolev", "Omar Zoloev", "Ivan Kireev", "Andrey Savchenko", "Maksim Makarenko"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Learning clients embeddings from sequences of their historic communications\nis central to financial applications. While large language models (LLMs) offer\ngeneral world knowledge, their direct use on long event sequences is\ncomputationally expensive and impractical in real-world pipelines. In this\npaper, we propose LATTE, a contrastive learning framework that aligns raw event\nembeddings with semantic embeddings from frozen LLMs. Behavioral features are\nsummarized into short prompts, embedded by the LLM, and used as supervision via\ncontrastive loss. The proposed approach significantly reduces inference cost\nand input size compared to conventional processing of complete sequence by LLM.\nWe experimentally show that our method outperforms state-of-the-art techniques\nfor learning event sequence representations on real-world financial datasets\nwhile remaining deployable in latency-sensitive environments."}
{"id": "2508.12227", "pdf": "https://arxiv.org/pdf/2508.12227.pdf", "abs": "https://arxiv.org/abs/2508.12227", "title": "Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges", "authors": ["Abdelhamid Haouhat", "Slimane Bellaouar", "Attia Nehar", "Hadda Cherroun", "Ahmed Abdelali"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Machine Learning (MML) aims to integrate and analyze information\nfrom diverse modalities, such as text, audio, and visuals, enabling machines to\naddress complex tasks like sentiment analysis, emotion recognition, and\nmultimedia retrieval. Recently, Arabic MML has reached a certain level of\nmaturity in its foundational development, making it time to conduct a\ncomprehensive survey. This paper explores Arabic MML by categorizing efforts\nthrough a novel taxonomy and analyzing existing research. Our taxonomy\norganizes these efforts into four key topics: datasets, applications,\napproaches, and challenges. By providing a structured overview, this survey\noffers insights into the current state of Arabic MML, highlighting areas that\nhave not been investigated and critical research gaps. Researchers will be\nempowered to build upon the identified opportunities and address challenges to\nadvance the field."}
{"id": "2508.14444", "pdf": "https://arxiv.org/pdf/2508.14444.pdf", "abs": "https://arxiv.org/abs/2508.14444", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "authors": ["NVIDIA", ":", "Aarti Basant", "Abhijit Khairnar", "Abhijit Paithankar", "Abhinav Khattar", "Adithya Renduchintala", "Aditya Malte", "Akhiad Bercovich", "Akshay Hazare", "Alejandra Rico", "Aleksander Ficek", "Alex Kondratenko", "Alex Shaposhnikov", "Alexander Bukharin", "Ali Taghibakhshi", "Amelia Barton", "Ameya Sunil Mahabaleshwarkar", "Amy Shen", "Andrew Tao", "Ann Guan", "Anna Shors", "Anubhav Mandarwal", "Arham Mehta", "Arun Venkatesan", "Ashton Sharabiani", "Ashwath Aithal", "Ashwin Poojary", "Ayush Dattagupta", "Balaram Buddharaju", "Banghua Zhu", "Barnaby Simkin", "Bilal Kartal", "Bita Darvish Rouhani", "Bobby Chen", "Boris Ginsburg", "Brandon Norick", "Brian Yu", "Bryan Catanzaro", "Charles Wang", "Charlie Truong", "Chetan Mungekar", "Chintan Patel", "Chris Alexiuk", "Christian Munley", "Christopher Parisien", "Dan Su", "Daniel Afrimi", "Daniel Korzekwa", "Daniel Rohrer", "Daria Gitman", "David Mosallanezhad", "Deepak Narayanan", "Dima Rekesh", "Dina Yared", "Dmytro Pykhtar", "Dong Ahn", "Duncan Riach", "Eileen Long", "Elliott Ning", "Eric Chung", "Erick Galinkin", "Evelina Bakhturina", "Gargi Prasad", "Gerald Shen", "Haifeng Qian", "Haim Elisha", "Harsh Sharma", "Hayley Ross", "Helen Ngo", "Herman Sahota", "Hexin Wang", "Hoo Chang Shin", "Hua Huang", "Iain Cunningham", "Igor Gitman", "Ivan Moshkov", "Jaehun Jung", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jian Zhang", "Jiaqi Zeng", "Jimmy Zhang", "Jinze Xue", "Jocelyn Huang", "Joey Conway", "John Kamalu", "Jonathan Cohen", "Joseph Jennings", "Julien Veron Vialard", "Junkeun Yi", "Jupinder Parmar", "Kari Briski", "Katherine Cheung", "Katherine Luna", "Keith Wyss", "Keshav Santhanam", "Kezhi Kong", "Krzysztof Pawelec", "Kumar Anik", "Kunlun Li", "Kushan Ahmadian", "Lawrence McAfee", "Laya Sleiman", "Leon Derczynski", "Luis Vega", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Marcin Chochowski", "Mark Cai", "Markus Kliegl", "Marta Stepniewska-Dziubinska", "Matvei Novikov", "Mehrzad Samadi", "Meredith Price", "Meriem Boubdir", "Michael Boone", "Michael Evans", "Michal Bien", "Michal Zawalski", "Miguel Martinez", "Mike Chrzanowski", "Mohammad Shoeybi", "Mostofa Patwary", "Namit Dhameja", "Nave Assaf", "Negar Habibi", "Nidhi Bhatia", "Nikki Pope", "Nima Tajbakhsh", "Nirmal Kumar Juluru", "Oleg Rybakov", "Oleksii Hrinchuk", "Oleksii Kuchaiev", "Oluwatobi Olabiyi", "Pablo Ribalta", "Padmavathy Subramanian", "Parth Chadha", "Pavlo Molchanov", "Peter Dykas", "Peter Jin", "Piotr Bialecki", "Piotr Januszewski", "Pradeep Thalasta", "Prashant Gaikwad", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Rabeeh Karimi Mahabadi", "Rajen Patel", "Ran El-Yaniv", "Ranjit Rajan", "Ria Cheruvu", "Rima Shahbazyan", "Ritika Borkar", "Ritu Gala", "Roger Waleffe", "Ruoxi Zhang", "Russell J. Hewett", "Ryan Prenger", "Sahil Jain", "Samuel Kriman", "Sanjeev Satheesh", "Saori Kaji", "Sarah Yurick", "Saurav Muralidharan", "Sean Narenthiran", "Seonmyeong Bak", "Sepehr Sameni", "Seungju Han", "Shanmugam Ramasamy", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Shelby Thomas", "Shizhe Diao", "Shreya Gopal", "Shrimai Prabhumoye", "Shubham Toshniwal", "Shuoyang Ding", "Siddharth Singh", "Siddhartha Jain", "Somshubra Majumdar", "Soumye Singhal", "Stefania Alborghetti", "Syeda Nahida Akter", "Terry Kong", "Tim Moon", "Tomasz Hliwiak", "Tomer Asida", "Tony Wang", "Tugrul Konuk", "Twinkle Vashishth", "Tyler Poon", "Udi Karpas", "Vahid Noroozi", "Venkat Srinivasan", "Vijay Korthikanti", "Vikram Fugro", "Vineeth Kalluru", "Vitaly Kurin", "Vitaly Lavrukhin", "Wasi Uddin Ahmad", "Wei Du", "Wonmin Byeon", "Ximing Lu", "Xin Dong", "Yashaswi Karnati", "Yejin Choi", "Yian Zhang", "Ying Lin", "Yonggan Fu", "Yoshi Suhara", "Zhen Dong", "Zhiyu Li", "Zhongbo Zhu", "Zijia Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving\nstate-of-the-art accuracy compared to similarly-sized models.\nNemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the\nmajority of the self-attention layers in the common Transformer architecture\nare replaced with Mamba-2 layers, to achieve improved inference speed when\ngenerating the long thinking traces needed for reasoning. We create\nNemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model\n(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.\nAfter aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to\ncompress and distill the model with the goal of enabling inference on up to\n128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).\nCompared to existing similarly-sized models (e.g., Qwen3-8B), we show that\nNemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks\nwhile achieving up to 6x higher inference throughput in reasoning settings like\n8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,\nNemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with\nthe majority of our pre- and post-training datasets on Hugging Face."}
{"id": "2404.18021", "pdf": "https://arxiv.org/pdf/2404.18021.pdf", "abs": "https://arxiv.org/abs/2404.18021", "title": "CRISPR-GPT for Agentic Automation of Gene-editing Experiments", "authors": ["Yuanhao Qu", "Kaixuan Huang", "Ming Yin", "Kanghong Zhan", "Dyllan Liu", "Di Yin", "Henry C. Cousins", "William A. Johnson", "Xiaotong Wang", "Mihir Shah", "Russ B. Altman", "Denny Zhou", "Mengdi Wang", "Le Cong"], "categories": ["cs.AI", "cs.CL", "cs.HC", "q-bio.QM"], "comment": "Accepted to Nature Biomedical Engineering", "summary": "The introduction of genome engineering technology has transformed biomedical\nresearch, making it possible to make precise changes to genetic information.\nHowever, creating an efficient gene-editing system requires a deep\nunderstanding of CRISPR technology, and the complex experimental systems under\ninvestigation. While Large Language Models (LLMs) have shown promise in various\ntasks, they often lack specific knowledge and struggle to accurately solve\nbiological design problems. In this work, we introduce CRISPR-GPT, an LLM agent\naugmented with domain knowledge and external tools to automate and enhance the\ndesign process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages\nthe reasoning ability of LLMs to facilitate the process of selecting CRISPR\nsystems, designing guide RNAs, recommending cellular delivery methods, drafting\nprotocols, and designing validation experiments to confirm editing outcomes. We\nshowcase the potential of CRISPR-GPT for assisting non-expert researchers with\ngene-editing experiments from scratch and validate the agent's effectiveness in\na real-world use case. Furthermore, we explore the ethical and regulatory\nconsiderations associated with automated gene-editing design, highlighting the\nneed for responsible and transparent use of these tools. Our work aims to\nbridge the gap between beginner biological researchers and CRISPR genome\nengineering techniques, and demonstrate the potential of LLM agents in\nfacilitating complex biological discovery tasks. The published version of this\ndraft is available at https://www.nature.com/articles/s41551-025-01463-z."}
{"id": "2411.18915", "pdf": "https://arxiv.org/pdf/2411.18915.pdf", "abs": "https://arxiv.org/abs/2411.18915", "title": "MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications", "authors": ["Vishnou Vinayagame", "Gregory Senay", "Luis Martí"], "categories": ["cs.LG", "cs.CL"], "comment": "Published as a conference paper at ICDAR 2025", "summary": "Business documents often contain substantial tabular and textual information\nwith numerical values, requiring mathematical reasoning for effective document\nunderstanding. While Small Language Models (SLMs) still struggle at this task,\ntool-augmented multi-step agents perform better, at the cost of relying on\nclosed-source or larger models, external data, or extensive prompt-engineering.\nThis work introduces MATATA, a novel weakly supervised end-to-end approach to\ntrain multi-step reasoning language agents for document tabular applications.\nMATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B\nSLMs. During its two-stage training, MATATA uses the final outcome of the\nmulti-step reasoning chain as weak supervision. This approach avoids having to\nindividually supervise each intermediate agent in the reasoning chain. By\nemploying an adaptive planner and shared tools across different datasets,\nMATATA shows robust performance. Experiments demonstrate that MATATA achieves\nstate-of-the-art on FinQA, and on TAT-QA among reasoning methods based on\nopen-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based\nframeworks on TabMWP. This novel weakly supervised approach enables training an\nend-to-end multi-step reasoning agent without intermediate supervision,\nsupporting future developments of cost-effective powerful agentic systems."}
{"id": "2412.09645", "pdf": "https://arxiv.org/pdf/2412.09645.pdf", "abs": "https://arxiv.org/abs/2412.09645", "title": "Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models", "authors": ["Fan Zhang", "Shulin Tian", "Ziqi Huang", "Yu Qiao", "Ziwei Liu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Equal contributions from first three authors. Project page:\n  https://vchitect.github.io/Evaluation-Agent-project Code:\n  https://github.com/Vchitect/Evaluation-Agent", "summary": "Recent advancements in visual generative models have enabled high-quality\nimage and video generation, opening diverse applications. However, evaluating\nthese models often demands sampling hundreds or thousands of images or videos,\nmaking the process computationally expensive, especially for diffusion-based\nmodels with inherently slow sampling. Moreover, existing evaluation methods\nrely on rigid pipelines that overlook specific user needs and provide numerical\nresults without clear explanations. In contrast, humans can quickly form\nimpressions of a model's capabilities by observing only a few samples. To mimic\nthis, we propose the Evaluation Agent framework, which employs human-like\nstrategies for efficient, dynamic, multi-round evaluations using only a few\nsamples per round, while offering detailed, user-tailored analyses. It offers\nfour key advantages: 1) efficiency, 2) promptable evaluation tailored to\ndiverse user needs, 3) explainability beyond single numerical scores, and 4)\nscalability across various models and tools. Experiments show that Evaluation\nAgent reduces evaluation time to 10% of traditional methods while delivering\ncomparable results. The Evaluation Agent framework is fully open-sourced to\nadvance research in visual generative models and their efficient evaluation."}
{"id": "2412.19792", "pdf": "https://arxiv.org/pdf/2412.19792.pdf", "abs": "https://arxiv.org/abs/2412.19792", "title": "InfAlign: Inference-aware language model alignment", "authors": ["Ananth Balashankar", "Ziteng Sun", "Jonathan Berant", "Jacob Eisenstein", "Michael Collins", "Adrian Hutter", "Jong Lee", "Chirag Nagpal", "Flavien Prost", "Aradhana Sinha", "Ananda Theertha Suresh", "Ahmad Beirami"], "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Language model alignment is a critical step in training modern generative\nlanguage models. Alignment targets to improve win rate of a sample from the\naligned model against the base model. Today, we are increasingly using\ninference-time algorithms (e.g., Best-of-N, controlled decoding, tree search)\nto decode from language models rather than standard sampling. We show that this\ntrain/test mismatch makes standard RLHF framework sub-optimal in view of such\ninference-time methods. To this end, we propose a framework for inference-aware\nalignment (InfAlign), which aims to optimize inference-time win rate of the\naligned policy against the base model. We prove that for any inference-time\ndecoding procedure, the optimal aligned policy is the solution to the standard\nRLHF problem with a transformation of the reward. This motivates us to provide\nthe calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem,\nwhich involves a reward calibration step and a KL-regularized reward\nmaximization step with a transformation of the calibrated reward. For best-of-N\nsampling and best-of-N jailbreaking, we propose specific transformations\noffering up to 3-8% improvement on inference-time win rates. Finally, we also\nshow that our proposed reward calibration method is a strong baseline for\noptimizing standard win rate."}
{"id": "2502.01619", "pdf": "https://arxiv.org/pdf/2502.01619.pdf", "abs": "https://arxiv.org/abs/2502.01619", "title": "Learning to Generate Unit Tests for Automated Debugging", "authors": ["Archiki Prasad", "Elias Stengel-Eskin", "Justin Chih-Yao Chen", "Zaid Khan", "Mohit Bansal"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to COLM 2025. Dataset and Code:\n  https://github.com/archiki/UTGenDebug", "summary": "Unit tests (UTs) play an instrumental role in assessing code correctness as\nwell as providing feedback to large language models (LLMs), motivating\nautomated test generation. However, we uncover a trade-off between generating\nunit test inputs that reveal errors when given a faulty code and correctly\npredicting the unit test output without access to the gold solution. To address\nthis trade-off, we propose UTGen, which teaches LLMs to generate unit test\ninputs that reveal errors along with their correct expected outputs based on\ntask descriptions. Since model-generated tests can provide noisy signals (e.g.,\nfrom incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen\nvia test-time compute to improve UT output prediction, and (ii) validates and\nbacktracks edits based on multiple generated UTs to avoid overfitting, and\nhelps LLMs debug effectively. We show that UTGen outperforms other LLM-based\nbaselines by 7.59% based on a metric measuring the presence of both\nerror-revealing UT inputs and correct UT outputs. When used with UTDebug, we\nfind that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5\n32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17%\nand 12.35% (respectively) over other LLM-based UT generation baselines.\nMoreover, we observe that feedback from Qwen2.5 32B-based UTGen model can\nenhance debugging with frontier LLMs like GPT-4o by 13.8%. Lastly, we\ndemonstrate that UTGen is a better judge for code correctness, outperforming a\nstate-of-the-art trained 8B reward model by 4.43% on HumanEval+ with best-of-10\nsampling using Qwen2.5 7B."}
{"id": "2504.19062", "pdf": "https://arxiv.org/pdf/2504.19062.pdf", "abs": "https://arxiv.org/abs/2504.19062", "title": "Versatile Framework for Song Generation with Prompt-based Control", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Jingyu Lu", "Rongjie Huang", "Ruiyuan Zhang", "Zhiqing Hong", "Ziyue Jiang", "Zhou Zhao"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by Findings of EMNLP 2025", "summary": "Song generation focuses on producing controllable high-quality songs based on\nvarious prompts. However, existing methods struggle to generate vocals and\naccompaniments with prompt-based control and proper alignment. Additionally,\nthey fall short in supporting various tasks. To address these challenges, we\nintroduce VersBand, a multi-task song generation framework for synthesizing\nhigh-quality, aligned songs with prompt-based control. VersBand comprises these\nprimary models: 1) VocalBand, a decoupled model, leverages the flow-matching\nmethod for generating singing styles, pitches, and mel-spectrograms, allowing\nfast, high-quality vocal generation with style control. 2) AccompBand, a\nflow-based transformer model, incorporates the Band-MOE, selecting suitable\nexperts for enhanced quality, alignment, and control. This model allows for\ngenerating controllable, high-quality accompaniments aligned with vocals. 3)\nTwo generation models, LyricBand for lyrics and MelodyBand for melodies,\ncontribute to the comprehensive multi-task song generation system, allowing for\nextensive control based on multiple prompts. Experimental results show that\nVersBand outperforms baseline models across multiple song generation tasks\nusing objective and subjective metrics. Demos and codes are available at\nhttps://aaronz345.github.io/VersBandDemo and\nhttps://github.com/AaronZ345/VersBand."}
{"id": "2505.22146", "pdf": "https://arxiv.org/pdf/2505.22146.pdf", "abs": "https://arxiv.org/abs/2505.22146", "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "authors": ["Guangfu Hao", "Haojie Wen", "Liangxuan Guo", "Yang Chen", "Yanchao Bi", "Shan Yu"], "categories": ["cs.CV", "cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Flexible tool selection reflects a complex cognitive ability that\ndistinguishes humans from other species, yet computational models that capture\nthis ability remain underdeveloped. We developed a framework using\nlow-dimensional attribute representations to bridge visual tool perception and\nlinguistic task understanding. We constructed a comprehensive dataset (ToolNet)\ncontaining 115 common tools labeled with 13 carefully designed attributes\nspanning physical, functional, and psychological properties, paired with\nnatural language scenarios describing tool usage. Visual encoders (ResNet or\nViT) extract attributes from tool images while fine-tuned language models\n(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our\napproach achieves 74% accuracy in tool selection tasks-significantly\noutperforming direct tool matching (20%) and smaller multimodal models\n(21%-58%), while approaching performance of much larger models like GPT-4o\n(73%) with substantially fewer parameters. Human evaluation studies validate\nour framework's alignment with human decision-making patterns, and\ngeneralization experiments demonstrate effective performance on novel tool\ncategories. Ablation studies revealed that manipulation-related attributes\n(graspability, elongation, hand-relatedness) consistently prove most critical\nacross modalities. This work provides a parameter-efficient, interpretable\nsolution that mimics human-like tool cognition, advancing both cognitive\nscience understanding and practical applications in tool selection tasks."}
{"id": "2506.06382", "pdf": "https://arxiv.org/pdf/2506.06382.pdf", "abs": "https://arxiv.org/abs/2506.06382", "title": "On the Fundamental Impossibility of Hallucination Control in Large Language Models", "authors": ["Michał P. Karpowicz"], "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.GT", "cs.LG"], "comment": "cleared mathematics, proofs debugged and ideas expanded, added\n  missing definitions and axioms, discussion and speculation section\n  reorganized, related work improved", "summary": "This paper establishes a fundamental impossibility theorem: no LLM capable of\nperforming non-trivial knowledge aggregation can simultaneously achieve\ntruthful knowledge representation, semantic information conservation, complete\nrevelation of relevant knowledge, and knowledge-constrained optimality. The\nimpossibility is not an engineering limitation but arises from the mathematical\nstructure of information aggregation itself.\n  We establish this result by describing the inference process as an auction of\nideas, where distributed components compete exploiting their partial knowledge\nto shape responses. The proof spans three independent mathematical domains:\nmechanism design theory (Green-Laffont), the theory of proper scoring rules\n(Savage), and direct architectural analysis of transformers (Log-Sum-Exp\nconvexity). In particular, we show how to quantify the creation of\noverconfident or intuitive responses-the signature of both hallucination and\ncreativity, or imagination.\n  To support this analysis, we introduce the complementary concepts of the\nsemantic information measure and the emergence operator to model bounded\nreasoning in a general setting. We prove that while bounded reasoning generates\naccessible information, providing valuable insights and inspirations, the\nidealized unconstrained reasoning strictly preserves semantic content.\n  By demonstrating that hallucination and imagination are mathematically\nidentical phenomena-grounded in departures from truthfulness, semantic\ninformation conservation, revelation of relevant knowledge, and\nknowledge-constrained optimality-we offer a principled foundation for managing\nthese behaviors in advanced AI systems. Finally, we present some speculative\nideas to inspire evaluation and refinements of the proposed theory."}
{"id": "2506.15928", "pdf": "https://arxiv.org/pdf/2506.15928.pdf", "abs": "https://arxiv.org/abs/2506.15928", "title": "Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues", "authors": ["Myke C. Cohen", "Zhe Su", "Hsien-Te Kao", "Daniel Nguyen", "Spencer Lynch", "Maarten Sap", "Svitlana Volkova"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Presented at the KDD 2025 Workshop on Evaluation and Trustworthiness\n  of Agentic and Generative AI Models under the title \"Evaluating the\n  LLM-simulated Impacts of Big Five Personality Traits and AI Capabilities on\n  Social Negotiations\"\n  (https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/assets/papers/Submission%2036.pdf)", "summary": "This paper presents an evaluation framework for agentic AI systems in\nmission-critical negotiation contexts, addressing the need for AI agents that\ncan adapt to diverse human operators and stakeholders. Using Sotopia as a\nsimulation testbed, we present two experiments that systematically evaluated\nhow personality traits and AI agent characteristics influence LLM-simulated\nsocial negotiation outcomes--a capability essential for a variety of\napplications involving cross-team coordination and civil-military interactions.\nExperiment 1 employs causal discovery methods to measure how personality traits\nimpact price bargaining negotiations, through which we found that Agreeableness\nand Extraversion significantly affect believability, goal achievement, and\nknowledge acquisition outcomes. Sociocognitive lexical measures extracted from\nteam communications detected fine-grained differences in agents' empathic\ncommunication, moral foundations, and opinion patterns, providing actionable\ninsights for agentic AI systems that must operate reliably in high-stakes\noperational scenarios. Experiment 2 evaluates human-AI job negotiations by\nmanipulating both simulated human personality and AI system characteristics,\nspecifically transparency, competence, adaptability, demonstrating how AI agent\ntrustworthiness impact mission effectiveness. These findings establish a\nrepeatable evaluation methodology for experimenting with AI agent reliability\nacross diverse operator personalities and human-agent team dynamics, directly\nsupporting operational requirements for reliable AI systems. Our work advances\nthe evaluation of agentic AI workflows by moving beyond standard performance\nmetrics to incorporate social dynamics essential for mission success in complex\noperations."}
{"id": "2506.22189", "pdf": "https://arxiv.org/pdf/2506.22189.pdf", "abs": "https://arxiv.org/abs/2506.22189", "title": "Exploring Modularity of Agentic Systems for Drug Discovery", "authors": ["Laura van Weesep", "Samuel Genheden", "Ola Engkvist", "Jens Sjölund"], "categories": ["cs.LG", "cs.CL", "cs.MA"], "comment": null, "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery. In this study, we examine the\nmodularity of LLM-based agentic systems for drug discovery, i.e., whether parts\nof the system such as the LLM and type of agent are interchangeable, a topic\nthat has received limited attention in drug discovery. We compare the\nperformance of different LLMs and the effectiveness of tool-calling agents\nversus code-generating agents. Our case study, comparing performance in\norchestrating tools for chemistry and drug discovery using an LLM-as-a-judge\nscore, shows that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform\nalternative language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo,\nand Nova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question- and\nmodel-dependent. Furthermore, the impact of replacing system prompts is\ndependent on the question and model, underscoring that even in this particular\ndomain one cannot just replace components of the system without re-engineering.\nOur study highlights the necessity of further research into the modularity of\nagentic systems to enable the development of reliable and modular solutions for\nreal-world problems."}
{"id": "2507.20077", "pdf": "https://arxiv.org/pdf/2507.20077.pdf", "abs": "https://arxiv.org/abs/2507.20077", "title": "The Devil is in the EOS: Sequence Training for Detailed Image Captioning", "authors": ["Abdelrahman Mohamed", "Yova Kementchedjhieva"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to COLM 2025", "summary": "Despite significant advances in vision-language models (VLMs), image\ncaptioning often suffers from a lack of detail, with base models producing\nshort, generic captions. This limitation persists even though VLMs are equipped\nwith strong vision and language backbones. While supervised data and complex\nreward functions have been proposed to improve detailed image captioning, we\nidentify a simpler underlying issue: a bias towards the end-of-sequence (EOS)\ntoken, which is introduced during cross-entropy training. We propose an\nunsupervised method to debias the model's tendency to predict the EOS token\nprematurely. By reducing this bias, we encourage the generation of longer, more\ndetailed captions without the need for intricate reward functions or\nsupervision. Our approach is straightforward, effective, and easily applicable\nto any pretrained model. We demonstrate its effectiveness through experiments\nwith three VLMs and on three detailed captioning benchmarks. Our results show a\nsubstantial increase in caption length and relevant details, albeit with an\nexpected increase in the rate of hallucinations."}
{"id": "2508.04714", "pdf": "https://arxiv.org/pdf/2508.04714.pdf", "abs": "https://arxiv.org/abs/2508.04714", "title": "Prescriptive Agents based on RAG for Automated Maintenance (PARAM)", "authors": ["Chitranshu Harbola", "Anupam Purwar"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "eess.SP"], "comment": null, "summary": "Industrial machinery maintenance requires timely intervention to prevent\ncatastrophic failures and optimize operational efficiency. This paper presents\nan integrated Large Language Model (LLM)-based intelligent system for\nprescriptive maintenance that extends beyond traditional anomaly detection to\nprovide actionable maintenance recommendations. Building upon our prior LAMP\nframework for numerical data analysis, we develop a comprehensive solution that\ncombines bearing vibration frequency analysis with multi agentic generation for\nintelligent maintenance planning. Our approach serializes bearing vibration\ndata (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM\nprocessing, enabling few-shot anomaly detection with high accuracy. The system\nclassifies fault types (inner race, outer race, ball/roller, cage faults) and\nassesses severity levels. A multi-agentic component processes maintenance\nmanuals using vector embeddings and semantic search, while also conducting web\nsearches to retrieve comprehensive procedural knowledge and access up-to-date\nmaintenance practices for more accurate and in-depth recommendations. The\nGemini model then generates structured maintenance recommendations includes\nimmediate actions, inspection checklists, corrective measures, parts\nrequirements, and timeline specifications. Experimental validation in bearing\nvibration datasets demonstrates effective anomaly detection and contextually\nrelevant maintenance guidance. The system successfully bridges the gap between\ncondition monitoring and actionable maintenance planning, providing industrial\npractitioners with intelligent decision support. This work advances the\napplication of LLMs in industrial maintenance, offering a scalable framework\nfor prescriptive maintenance across machinery components and industrial\nsectors."}
{"id": "2508.07616", "pdf": "https://arxiv.org/pdf/2508.07616.pdf", "abs": "https://arxiv.org/abs/2508.07616", "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation", "authors": ["Aswin RRV", "Jacob Dineen", "Divij Handa", "Md Nayem Uddin", "Mihir Parmar", "Chitta Baral", "Ben Zhou"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "EMNLP 2025 (Main Conference)", "summary": "Recent advances in test-time scaling have led to the emergence of thinking\nLLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL\ndrives this self-improvement paradigm, a recent study (Gandhi et al., 2025)\nshows that RL alone does not truly instill these new reasoning abilities - it\nmerely draws out behaviors already present in the base models. This raises a\nquestion: How can we train the models that don't exhibit such thinking behavior\nto develop it in the first place? To this end, we propose ThinkTuning, a\nGRPO-based interactive training approach where we augment the rollouts of a\nstudent model with the guidance from a teacher model. A simple idea from\nclassroom practice inspires our method: a teacher poses a problem, lets the\nstudent try an answer, then gives corrective feedback -- enough to point the\nmind in the right direction and then show the solution. Each piece of feedback\nreshapes the student's thoughts, leading them to arrive at the correct\nsolution. Similarly, we find that this type of implicit supervision through\nfeedback from a teacher model of the same size improves the reasoning\ncapabilities of the student model. In particular, on average, our method shows\na 3.85% improvement over zero-shot baselines across benchmarks, and on\nMATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements\nover the vanilla-GRPO baseline. Source code is available at\nhttps://github.com/3rdAT/ThinkTuning."}
{"id": "2508.14052", "pdf": "https://arxiv.org/pdf/2508.14052.pdf", "abs": "https://arxiv.org/abs/2508.14052", "title": "FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering", "authors": ["Chanyeol Choi", "Jihoon Kwon", "Alejandro Lopez-Lira", "Chaewoon Kim", "Minjae Kim", "Juneha Hwang", "Jaeseon Ha", "Hojun Choi", "Suyeol Yun", "Yongjin Kim", "Yongjae Lee"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "6 pages", "summary": "Accurate information retrieval (IR) is critical in the financial domain,\nwhere investors must identify relevant information from large collections of\ndocuments. Traditional IR methods-whether sparse or dense-often fall short in\nretrieval accuracy, as it requires not only capturing semantic similarity but\nalso performing fine-grained reasoning over document structure and\ndomain-specific knowledge. Recent advances in large language models (LLMs) have\nopened up new opportunities for retrieval with multi-step reasoning, where the\nmodel ranks passages through iterative reasoning about which information is\nmost relevant to a given query. However, there exists no benchmark to evaluate\nsuch capabilities in the financial domain. To address this gap, we introduce\nFinAgentBench, the first large-scale benchmark for evaluating retrieval with\nmulti-step reasoning in finance -- a setting we term agentic retrieval. The\nbenchmark consists of 3,429 expert-annotated examples on S&P-100 listed firms\nand assesses whether LLM agents can (1) identify the most relevant document\ntype among candidates, and (2) pinpoint the key passage within the selected\ndocument. Our evaluation framework explicitly separates these two reasoning\nsteps to address context limitations. This design enables to provide a\nquantitative basis for understanding retrieval-centric LLM behavior in finance.\nWe evaluate a suite of state-of-the-art models and further demonstrated how\ntargeted fine-tuning can significantly improve agentic retrieval performance.\nOur benchmark provides a foundation for studying retrieval-centric LLM behavior\nin complex, domain-specific tasks for finance. We will release the dataset\npublicly upon acceptance of the paper and plan to expand and share dataset for\nthe full S&P 500 and beyond."}
