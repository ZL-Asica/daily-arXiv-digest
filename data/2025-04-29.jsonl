{"id": "2504.18691", "pdf": "https://arxiv.org/pdf/2504.18691.pdf", "abs": "https://arxiv.org/abs/2504.18691", "title": "From Prompts to Propositions: A Logic-Based Lens on Student-LLM Interactions", "authors": ["Ali Alfageeh", "Sadegh AlMahdi Kazemi Zarkouei", "Daye Nam", "Daniel Prol", "Matin Amoozadeh", "Souti Chattopadhyay", "James Prather", "Paul Denny", "Juho Leinonen", "Michael Hilton", "Sruti Srinivasa Ragavan", "Mohammad Amin Alipour"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Background and Context. The increasing integration of large language models\n(LLMs) in computing education presents an emerging challenge in understanding\nhow students use LLMs and craft prompts to solve computational tasks. Prior\nresearch has used both qualitative and quantitative methods to analyze\nprompting behavior, but these approaches lack scalability or fail to\neffectively capture the semantic evolution of prompts. Objective. In this\npaper, we investigate whether students prompts can be systematically analyzed\nusing propositional logic constraints. We examine whether this approach can\nidentify patterns in prompt evolution, detect struggling students, and provide\ninsights into effective and ineffective strategies. Method. We introduce\nPrompt2Constraints, a novel method that translates students prompts into\nlogical constraints. The constraints are able to represent the intent of the\nprompts in succinct and quantifiable ways. We used this approach to analyze a\ndataset of 1,872 prompts from 203 students solving introductory programming\ntasks. Findings. We find that while successful and unsuccessful attempts tend\nto use a similar number of constraints overall, when students fail, they often\nmodify their prompts more significantly, shifting problem-solving strategies\nmidway. We also identify points where specific interventions could be most\nhelpful to students for refining their prompts. Implications. This work offers\na new and scalable way to detect students who struggle in solving natural\nlanguage programming tasks. This work could be extended to investigate more\ncomplex tasks and integrated into programming tools to provide real-time\nsupport."}
{"id": "2504.18759", "pdf": "https://arxiv.org/pdf/2504.18759.pdf", "abs": "https://arxiv.org/abs/2504.18759", "title": "Beyond Isolation: Towards an Interactionist Perspective on Human Cognitive Bias and AI Bias", "authors": ["Nick von Felten"], "categories": ["cs.HC"], "comment": "Is published at the CHI 2025 Workshop \"Tools for Thought: Research\n  and Design for Understanding, Protecting, and Augmenting Human Cognition with\n  Generative AI\"", "summary": "Isolated perspectives have often paved the way for great scientific\ndiscoveries. However, many breakthroughs only emerged when moving away from\nsingular views towards interactions. Discussions on Artificial Intelligence\n(AI) typically treat human and AI bias as distinct challenges, leaving their\ndynamic interplay and compounding potential largely unexplored. Recent research\nsuggests that biased AI can amplify human cognitive biases, while\nwell-calibrated systems might help mitigate them. In this position paper, I\nadvocate for transcending beyond separate treatment of human and AI biases and\ninstead focus on their interaction effects. I argue that a comprehensive\nframework, one that maps (compound human-AI) biases to mitigation strategies,\nis essential for understanding and protecting human cognition, and I outline\nconcrete steps for its development."}
{"id": "2504.18807", "pdf": "https://arxiv.org/pdf/2504.18807.pdf", "abs": "https://arxiv.org/abs/2504.18807", "title": "Clones in the Machine: A Feminist Critique of Agency in Digital Cloning", "authors": ["Siân Brooke"], "categories": ["cs.HC", "cs.AI"], "comment": "ACM CHI Conference on Human Factors in Computing Systems 2025", "summary": "This paper critiques digital cloning in academic research, highlighting how\nit exemplifies AI solutionism. Digital clones, which replicate user data to\nsimulate behavior, are often seen as scalable tools for behavioral insights.\nHowever, this framing obscures ethical concerns around consent, agency, and\nrepresentation. Drawing on feminist theories of agency, the paper argues that\ndigital cloning oversimplifies human complexity and risks perpetuating systemic\nbiases. To address these issues, it proposes decentralized data repositories\nand dynamic consent models, promoting ethical, context-aware AI practices that\nchallenge the reductionist logic of AI solutionism"}
{"id": "2504.18817", "pdf": "https://arxiv.org/pdf/2504.18817.pdf", "abs": "https://arxiv.org/abs/2504.18817", "title": "Understanding Decentralized Social Feed Curation on Mastodon", "authors": ["Yuhan Liu", "Emmy Song", "Owen Xingjian Zhang", "Jewel Merriman", "Lei Zhang", "Andrés Monroy-Hernández"], "categories": ["cs.HC"], "comment": "Accepted at CSCW 2025", "summary": "As centralized social media platforms face growing concerns, more users are\nseeking greater control over their social feeds and turning to decentralized\nalternatives such as Mastodon. The decentralized nature of Mastodon creates\nunique opportunities for customizing feeds, yet user perceptions and curation\nstrategies on these platforms remain unknown. This paper presents findings from\na two-part interview study with 21 Mastodon users, exploring how they perceive,\ninteract with, and manage their current feeds, and how we can better empower\nusers to personalize their feeds on Mastodon. We use the qualitative findings\nof the first part of the study to guide the creation of Braids, a web-based\nprototype for feed curation. Results from the second part of our study, using\nBraids, highlighted opportunities and challenges for future research,\nparticularly in using seamful design to enhance people's acceptance of\nalgorithmic curation and nuanced trade-offs between machine learning-based and\nrule-based curation algorithms. To optimize user experience, we also discuss\nthe tension between creating new apps and building add-ons in the decentralized\nsocial media realm."}
{"id": "2504.18560", "pdf": "https://arxiv.org/pdf/2504.18560.pdf", "abs": "https://arxiv.org/abs/2504.18560", "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages", "authors": ["Alessio Buscemi", "Cédric Lothritz", "Sergio Morales", "Marcos Gomez-Vazquez", "Robert Clarisó", "Jordi Cabot", "German Castignani"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have exhibited impressive natural language\nprocessing capabilities but often perpetuate social biases inherent in their\ntraining data. To address this, we introduce MultiLingual Augmented Bias\nTesting (MLA-BiTe), a framework that improves prior bias evaluation methods by\nenabling systematic multilingual bias testing. MLA-BiTe leverages automated\ntranslation and paraphrasing techniques to support comprehensive assessments\nacross diverse linguistic settings. In this study, we evaluate the\neffectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six\nlanguages -- including two low-resource languages -- focusing on seven\nsensitive categories of discrimination."}
{"id": "2504.18919", "pdf": "https://arxiv.org/pdf/2504.18919.pdf", "abs": "https://arxiv.org/abs/2504.18919", "title": "Clinical knowledge in LLMs does not translate to human interactions", "authors": ["Andrew M. Bean", "Rebecca Payne", "Guy Parsons", "Hannah Rose Kirk", "Juan Ciro", "Rafael Mosquera", "Sara Hincapié Monsalve", "Aruna S. Ekanayaka", "Lionel Tarassenko", "Luc Rocher", "Adam Mahdi"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "52 pages, 4 figures", "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare."}
{"id": "2504.18639", "pdf": "https://arxiv.org/pdf/2504.18639.pdf", "abs": "https://arxiv.org/abs/2504.18639", "title": "Span-Level Hallucination Detection for LLM-Generated Answers", "authors": ["Passant Elchafei", "Mervet Abu-Elkheir"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting spans of hallucination in LLM-generated answers is crucial for\nimproving factual consistency. This paper presents a span-level hallucination\ndetection framework for the SemEval-2025 Shared Task, focusing on English and\nArabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose\nthe answer into atomic roles, which are then compared with a retrieved\nreference context obtained via question-based LLM prompting. Using a\nDeBERTa-based textual entailment model, we evaluate each role semantic\nalignment with the retrieved context. The entailment scores are further refined\nthrough token-level confidence measures derived from output logits, and the\ncombined scores are used to detect hallucinated spans. Experiments on the\nMu-SHROOM dataset demonstrate competitive performance. Additionally,\nhallucinated spans have been verified through fact-checking by prompting GPT-4\nand LLaMA. Our findings contribute to improving hallucination detection in\nLLM-generated responses."}
{"id": "2504.18932", "pdf": "https://arxiv.org/pdf/2504.18932.pdf", "abs": "https://arxiv.org/abs/2504.18932", "title": "AI Chatbots for Mental Health: Values and Harms from Lived Experiences of Depression", "authors": ["Dong Whi Yoo", "Jiayue Melissa Shi", "Violeta J. Rodriguez", "Koustuv Saha"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Recent advancements in LLMs enable chatbots to interact with individuals on a\nrange of queries, including sensitive mental health contexts. Despite\nuncertainties about their effectiveness and reliability, the development of\nLLMs in these areas is growing, potentially leading to harms. To better\nidentify and mitigate these harms, it is critical to understand how the values\nof people with lived experiences relate to the harms. In this study, we\ndeveloped a technology probe, a GPT-4o based chatbot called Zenny, enabling\nparticipants to engage with depression self-management scenarios informed by\nprevious research. We used Zenny to interview 17 individuals with lived\nexperiences of depression. Our thematic analysis revealed key values:\ninformational support, emotional support, personalization, privacy, and crisis\nmanagement. This work explores the relationship between lived experience\nvalues, potential harms, and design recommendations for mental health AI\nchatbots, aiming to enhance self-management support while minimizing risks."}
{"id": "2504.18673", "pdf": "https://arxiv.org/pdf/2504.18673.pdf", "abs": "https://arxiv.org/abs/2504.18673", "title": "Can Third-parties Read Our Emotions?", "authors": ["Jiayi Li", "Yingfan Zhou", "Pranav Narayanan Venkit", "Halima Binte Islam", "Sneha Arya", "Shomir Wilson", "Sarah Rajtmajer"], "categories": ["cs.CL"], "comment": null, "summary": "Natural Language Processing tasks that aim to infer an author's private\nstates, e.g., emotions and opinions, from their written text, typically rely on\ndatasets annotated by third-party annotators. However, the assumption that\nthird-party annotators can accurately capture authors' private states remains\nlargely unexamined. In this study, we present human subjects experiments on\nemotion recognition tasks that directly compare third-party annotations with\nfirst-party (author-provided) emotion labels. Our findings reveal significant\nlimitations in third-party annotations-whether provided by human annotators or\nlarge language models (LLMs)-in faithfully representing authors' private\nstates. However, LLMs outperform human annotators nearly across the board. We\nfurther explore methods to improve third-party annotation quality. We find that\ndemographic similarity between first-party authors and third-party human\nannotators enhances annotation performance. While incorporating first-party\ndemographic information into prompts leads to a marginal but statistically\nsignificant improvement in LLMs' performance. We introduce a framework for\nevaluating the limitations of third-party annotations and call for refined\nannotation practices to accurately represent and model authors' private states."}
{"id": "2504.18969", "pdf": "https://arxiv.org/pdf/2504.18969.pdf", "abs": "https://arxiv.org/abs/2504.18969", "title": "Advancing Face-to-Face Emotion Communication: A Multimodal Dataset (AFFEC)", "authors": ["Meisam J. Sekiavandi", "Laurits Dixen", "Jostein Fimland", "Sree Keerthi Desu", "Antonia-Bianca Zserai", "Ye Sul Lee", "Maria Barrett", "Paolo Burre"], "categories": ["cs.HC"], "comment": null, "summary": "Emotion recognition has the potential to play a pivotal role in enhancing\nhuman-computer interaction by enabling systems to accurately interpret and\nrespond to human affect. Yet, capturing emotions in face-to-face contexts\nremains challenging due to subtle nonverbal cues, variations in personal\ntraits, and the real-time dynamics of genuine interactions. Existing emotion\nrecognition datasets often rely on limited modalities or controlled conditions,\nthereby missing the richness and variability found in real-world scenarios.\n  In this work, we introduce Advancing Face-to-Face Emotion Communication\n(AFFEC), a multimodal dataset designed to address these gaps. AFFEC encompasses\n84 simulated emotional dialogues across six distinct emotions, recorded from 73\nparticipants over more than 5,000 trials and annotated with more than 20,000\nlabels. It integrates electroencephalography (EEG), eye-tracking, galvanic skin\nresponse (GSR), facial videos, and Big Five personality assessments. Crucially,\nAFFEC explicitly distinguishes between felt emotions (the participant's\ninternal affect) and perceived emotions (the observer's interpretation of the\nstimulus).\n  Baseline analyses spanning unimodal features and straightforward multimodal\nfusion demonstrate that even minimal processing yields classification\nperformance significantly above chance, especially for arousal. Incorporating\npersonality traits further improves predictions of felt emotions, highlighting\nthe importance of individual differences. By bridging controlled\nexperimentation with more realistic face-to-face stimuli, AFFEC offers a unique\nresource for researchers aiming to develop context-sensitive, adaptive, and\npersonalized emotion recognition models."}
{"id": "2504.18715", "pdf": "https://arxiv.org/pdf/2504.18715.pdf", "abs": "https://arxiv.org/abs/2504.18715", "title": "Spatial Speech Translation: Translating Across Space With Binaural Hearables", "authors": ["Tuochao Chen", "Qirui Wang", "Runlin He", "Shyam Gollakota"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by CHI2025", "summary": "Imagine being in a crowded space where people speak a different language and\nhaving hearables that transform the auditory space into your native language,\nwhile preserving the spatial cues for all speakers. We introduce spatial speech\ntranslation, a novel concept for hearables that translate speakers in the\nwearer's environment, while maintaining the direction and unique voice\ncharacteristics of each speaker in the binaural output. To achieve this, we\ntackle several technical challenges spanning blind source separation,\nlocalization, real-time expressive translation, and binaural rendering to\npreserve the speaker directions in the translated audio, while achieving\nreal-time inference on the Apple M2 silicon. Our proof-of-concept evaluation\nwith a prototype binaural headset shows that, unlike existing models, which\nfail in the presence of interference, we achieve a BLEU score of up to 22.01\nwhen translating between languages, despite strong interference from other\nspeakers in the environment. User studies further confirm the system's\neffectiveness in spatially rendering the translated speech in previously unseen\nreal-world reverberant environments. Taking a step back, this work marks the\nfirst step towards integrating spatial perception into speech translation."}
{"id": "2504.18988", "pdf": "https://arxiv.org/pdf/2504.18988.pdf", "abs": "https://arxiv.org/abs/2504.18988", "title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "authors": ["Saramsh Gautam", "Mahmood Jasim"], "categories": ["cs.HC", "cs.CL", "H.5.3"], "comment": "19 pages, 4 figures. Multimodal system design and evaluation study", "summary": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research."}
{"id": "2504.18718", "pdf": "https://arxiv.org/pdf/2504.18718.pdf", "abs": "https://arxiv.org/abs/2504.18718", "title": "Building UD Cairo for Old English in the Classroom", "authors": ["Lauren Levine", "Junghyun Min", "Amir Zeldes"], "categories": ["cs.CL"], "comment": "7 pages, 2 figures", "summary": "In this paper we present a sample treebank for Old English based on the UD\nCairo sentences, collected and annotated as part of a classroom curriculum in\nHistorical Linguistics. To collect the data, a sample of 20 sentences\nillustrating a range of syntactic constructions in the world's languages, we\nemploy a combination of LLM prompting and searches in authentic Old English\ndata. For annotation we assigned sentences to multiple students with limited\nprior exposure to UD, whose annotations we compare and adjudicate. Our results\nsuggest that while current LLM outputs in Old English do not reflect authentic\nsyntax, this can be mitigated by post-editing, and that although beginner\nannotators do not possess enough background to complete the task perfectly,\ntaken together they can produce good results and learn from the experience. We\nalso conduct preliminary parsing experiments using Modern English training\ndata, and find that although performance on Old English is poor, parsing on\nannotated features (lemma, hyperlemma, gloss) leads to improved performance."}
{"id": "2504.19010", "pdf": "https://arxiv.org/pdf/2504.19010.pdf", "abs": "https://arxiv.org/abs/2504.19010", "title": "Investigating the Prominence and Severity of Bugs and Glitches Within Games and Their Effects on Player Experience", "authors": ["Jessica Backus"], "categories": ["cs.HC"], "comment": "7 pages, 2 figures", "summary": "Different errors that occur in video games are often referred to as glitches\nor bugs. The goal of this exploratory research is to understand how these\nglitches and bugs within video games affect a players experience. To do this, I\nreviewed relevant literature and performed observations of these different\nerrors in different games via Twitch livestreams. I then performed thematic\nanalysis with the observation data and generated themes that tie back into to\nthe relevant literature. Most of the current literature focuses on the what and\nhow behind bugs in games, but very little on the implications of these bugs on\nthe overall experience for the players, and what patterns of behavior may\nemerge because of them."}
{"id": "2504.18736", "pdf": "https://arxiv.org/pdf/2504.18736.pdf", "abs": "https://arxiv.org/abs/2504.18736", "title": "EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers", "authors": ["Jianyou Wang", "Weili Cao", "Kaicheng Wang", "Xiaoyue Wang", "Ashish Dalvi", "Gino Prasad", "Qishan Liang", "Hsuan-lin Her", "Ming Wang", "Qin Yang", "Gene W. Yeo", "David E. Neal", "Maxim Khan", "Christopher D. Rosin", "Ramamohan Paturi", "Leon Bergen"], "categories": ["cs.CL"], "comment": null, "summary": "We study the task of automatically finding evidence relevant to hypotheses in\nbiomedical papers. Finding relevant evidence is an important step when\nresearchers investigate scientific hypotheses. We introduce EvidenceBench to\nmeasure models performance on this task, which is created by a novel pipeline\nthat consists of hypothesis generation and sentence-by-sentence annotation of\nbiomedical papers for relevant evidence, completely guided by and faithfully\nfollowing existing human experts judgment. We demonstrate the pipeline's\nvalidity and accuracy with multiple sets of human-expert annotations. We\nevaluated a diverse set of language models and retrieval systems on the\nbenchmark and found that model performances still fall significantly short of\nthe expert level on this task. To show the scalability of our proposed\npipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated\npapers with hypotheses to facilitate model training and development. Both\ndatasets are available at https://github.com/EvidenceBench/EvidenceBench"}
{"id": "2504.19038", "pdf": "https://arxiv.org/pdf/2504.19038.pdf", "abs": "https://arxiv.org/abs/2504.19038", "title": "Generative AI Literacy: A Comprehensive Framework for Literacy and Responsible Use", "authors": ["Chengzhi Zhang", "Brian Magerko"], "categories": ["cs.HC"], "comment": "14 pages", "summary": "After the release of several AI literacy guidelines, the rapid rise and\nwidespread adoption of generative AI, such as ChatGPT, Dall E, and Deepseek,\nhave transformed our lives. Unlike traditional AI algorithms (e.g.,\nconvolutional neural networks, semantic networks, classifiers) captured in\nexisting AI literacy frameworks, generative AI exhibits distinct and more\nnuanced characteristics. However, a lack of robust generative AI literacy is\nhindering individuals ability to evaluate critically and use these models\neffectively and responsibly. To address this gap, we propose a set of\nguidelines with 12 items for generative AI literacy, organized into four key\naspects: (1) Guidelines for Generative AI Tool Selection and Prompting, (2)\nGuidelines for Understanding Interaction with Generative AI, (3) Guidelines for\nUnderstanding Interaction with Generative AI, and (4) Guidelines for High Level\nUnderstanding of Generative AI. These guidelines aim to support schools,\ncompanies, educators, and organizations in developing frameworks that empower\ntheir members, such as students, employees, and stakeholders, to use generative\nAI in an efficient, ethical, and informed way."}
{"id": "2504.18762", "pdf": "https://arxiv.org/pdf/2504.18762.pdf", "abs": "https://arxiv.org/abs/2504.18762", "title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning", "authors": ["Ojasw Upadhyay", "Abishek Saravankumar", "Ayman Ismail"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI."}
{"id": "2504.19120", "pdf": "https://arxiv.org/pdf/2504.19120.pdf", "abs": "https://arxiv.org/abs/2504.19120", "title": "Beyond Levels of Driving Automation: A Triadic Framework of Human-AI Collaboration in On-Road Mobility", "authors": ["Gaojian Huang", "Yantong Jin", "Wei-Hsiang Lo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The goal of the current study is to introduce a triadic human-AI\ncollaboration framework for the automated vehicle domain. Previous\nclassifications (e.g., SAE Levels of Automation) focus on defining automation\nlevels based on who controls the vehicle. However, it remains unclear how human\nusers and AI should collaborate in real-time, especially in dynamic driving\ncontexts, where roles can shift frequently. To fill the gap, this study\nproposes a triadic human-AI collaboration framework with three AI roles (i.e.,\nAdvisor, Co-Pilot, and Guardian) that dynamically adapt to human needs.\nOverall, the study lays a foundation for developing adaptive, role-based\nhuman-AI collaboration strategies in automated vehicles."}
{"id": "2504.18805", "pdf": "https://arxiv.org/pdf/2504.18805.pdf", "abs": "https://arxiv.org/abs/2504.18805", "title": "Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation", "authors": ["Jong Inn Park", "Maanas Taneja", "Qianwen Wang", "Dongyeop Kang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page: https://minnesotanlp.github.io/scitalk-project-page/", "summary": "Generating engaging, accurate short-form videos from scientific papers is\nchallenging due to content complexity and the gap between expert authors and\nreaders. Existing end-to-end methods often suffer from factual inaccuracies and\nvisual artifacts, limiting their utility for scientific dissemination. To\naddress these issues, we propose SciTalk, a novel multi-LLM agentic framework,\ngrounding videos in various sources, such as text, figures, visual styles, and\navatars. Inspired by content creators' workflows, SciTalk uses specialized\nagents for content summarization, visual scene planning, and text and layout\nediting, and incorporates an iterative feedback mechanism where video agents\nsimulate user roles to give feedback on generated videos from previous\niterations and refine generation prompts. Experimental evaluations show that\nSciTalk outperforms simple prompting methods in generating scientifically\naccurate and engaging content over the refined loop of video generation.\nAlthough preliminary results are still not yet matching human creators'\nquality, our framework provides valuable insights into the challenges and\nbenefits of feedback-driven video generation. Our code, data, and generated\nvideos will be publicly available."}
{"id": "2504.19158", "pdf": "https://arxiv.org/pdf/2504.19158.pdf", "abs": "https://arxiv.org/abs/2504.19158", "title": "SnuggleSense: Empowering Online Harm Survivors Through a Structured Sensemaking Process", "authors": ["Sijia Xiao", "Haodi Zou", "Amy Mathews", "Jingshu Rui", "Coye Cheshire", "Niloufar Salehi"], "categories": ["cs.HC"], "comment": null, "summary": "Online interpersonal harm, such as cyberbullying and sexual harassment,\nremains a pervasive issue on social media platforms. Traditional approaches,\nprimarily content moderation, often overlook survivors' needs and agency. We\nintroduce SnuggleSense, a system that empowers survivors through structured\nsensemaking. Inspired by restorative justice practices, SnuggleSense guides\nsurvivors through reflective questions, offers personalized recommendations\nfrom similar survivors, and visualizes plans using interactive sticky notes. A\ncontrolled experiment demonstrates that SnuggleSense significantly enhances\nsensemaking compared to an unstructured process of making sense of the harm. We\nargue that SnuggleSense fosters community awareness, cultivates a supportive\nsurvivor network, and promotes a restorative justice-oriented approach toward\nrestoration and healing. We also discuss design insights, such as tailoring\ninformational support and providing guidance while preserving survivors'\nagency."}
{"id": "2504.18838", "pdf": "https://arxiv.org/pdf/2504.18838.pdf", "abs": "https://arxiv.org/abs/2504.18838", "title": "Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks", "authors": ["Yixin Cao", "Shibo Hong", "Xinze Li", "Jiahao Ying", "Yubo Ma", "Haiyuan Liang", "Yantao Liu", "Zijun Yao", "Xiaozhi Wang", "Dan Huang", "Wenxuan Zhang", "Lifu Huang", "Muhao Chen", "Lei Hou", "Qianru Sun", "Xingjun Ma", "Zuxuan Wu", "Min-Yen Kan", "David Lo", "Qi Zhang", "Heng Ji", "Jing Jiang", "Juanzi Li", "Aixin Sun", "Xuanjing Huang", "Tat-Seng Chua", "Yu-Gang Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are advancing at an amazing speed and have\nbecome indispensable across academia, industry, and daily applications. To keep\npace with the status quo, this survey probes the core challenges that the rise\nof LLMs poses for evaluation. We identify and analyze two pivotal transitions:\n(i) from task-specific to capability-based evaluation, which reorganizes\nbenchmarks around core competencies such as knowledge, reasoning, instruction\nfollowing, multi-modal understanding, and safety; and (ii) from manual to\nautomated evaluation, encompassing dynamic dataset curation and\n\"LLM-as-a-judge\" scoring.\n  Yet, even with these transitions, a crucial obstacle persists: the evaluation\ngeneralization issue. Bounded test sets cannot scale alongside models whose\nabilities grow seemingly without limit. We will dissect this issue, along with\nthe core challenges of the above two transitions, from the perspectives of\nmethods, datasets, evaluators, and metrics. Due to the fast evolving of this\nfield, we will maintain a living GitHub repository (links are in each section)\nto crowd-source updates and corrections, and warmly invite contributors and\ncollaborators."}
{"id": "2504.19345", "pdf": "https://arxiv.org/pdf/2504.19345.pdf", "abs": "https://arxiv.org/abs/2504.19345", "title": "Beyond Physical Reach: Comparing Head- and Cane-Mounted Cameras for Last-Mile Navigation by Blind Users", "authors": ["Apurv Varshney", "Lucas Nadolskis", "Tobias Höllerer", "Michael Beyeler"], "categories": ["cs.HC"], "comment": null, "summary": "Blind individuals face persistent challenges in last-mile navigation,\nincluding locating entrances, identifying obstacles, and navigating complex or\ncluttered spaces. Although wearable cameras are increasingly used in assistive\nsystems, there has been no systematic, vantage-focused comparison to guide\ntheir design. This paper addresses that gap through a two-part investigation.\nFirst, we surveyed ten experienced blind cane users, uncovering navigation\nstrategies, pain points, and technology preferences. Participants stressed the\nimportance of multi-sensory integration, destination-focused travel, and\nassistive tools that complement (rather than replace) the cane's tactile\nutility. Second, we conducted controlled data collection with a blind\nparticipant navigating five real-world environments using synchronized head-\nand cane-mounted cameras, isolating vantage placement as the primary variable.\nTo assess how each vantage supports spatial perception, we evaluated SLAM\nperformance (for localization and mapping) and NeRF-based 3D reconstruction\n(for downstream scene understanding). Head-mounted sensors delivered superior\nlocalization accuracy, while cane-mounted views offered broader ground-level\ncoverage and richer environmental reconstructions. A combined (head+cane)\nconfiguration consistently outperformed both. These results highlight the\ncomplementary strengths of different sensor placements and offer actionable\nguidance for developing hybrid navigation aids that are perceptive, robust, and\nuser-aligned."}
{"id": "2504.18839", "pdf": "https://arxiv.org/pdf/2504.18839.pdf", "abs": "https://arxiv.org/abs/2504.18839", "title": "Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning", "authors": ["Abdellah Ghassel", "Xianzhi Li", "Xiaodan Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are rapidly changing various domains. However,\ntheir capabilities in handling conversational breakdowns still require an\nin-depth exploration. This paper addresses the challenge of detecting and\nmitigating dialogue breakdowns within LLM-driven conversational systems. While\npowerful models from OpenAI and Anthropic excel in many dialogue tasks, they\ncan still produce incoherent or contradictory responses, commonly referred to\nas breakdowns, which undermine user trust. To tackle this, we propose an\napproach that combines specialized fine-tuning with advanced prompting\nstrategies, including few-shot learning, chain-of-thought reasoning, and\nanalogical prompting. In particular, we fine-tune a small 8B model and\ndemonstrate its robust classification and calibration capabilities in English\nand Japanese dialogue. We also validate its generalization on the BETOLD\ndataset, achieving a 7\\% accuracy improvement over its base model. Furthermore,\nwe introduce a real-time deployment architecture that selectively escalates\nsuspicious responses to more resource-intensive frontier models only when\nbreakdowns are detected, significantly cutting operational expenses and energy\nconsumption. Experimental results show our method surpasses prior\nstate-of-the-art specialized classifiers while also narrowing performance gaps\nbetween smaller open-source models and large proprietary ones. Our approach\noffers a scalable solution for robust conversational AI in high-impact domains\nby combining efficiency, interpretability, and reliability."}
{"id": "2504.19423", "pdf": "https://arxiv.org/pdf/2504.19423.pdf", "abs": "https://arxiv.org/abs/2504.19423", "title": "MER 2025: When Affective Computing Meets Large Language Models", "authors": ["Zheng Lian", "Rui Liu", "Kele Xu", "Bin Liu", "Xuefei Liu", "Yazhou Zhang", "Xin Liu", "Yong Li", "Zebang Cheng", "Haolin Zuo", "Ziyang Ma", "Xiaojiang Peng", "Xie Chen", "Ya Li", "Erik Cambria", "Guoying Zhao", "Björn W. Schuller", "Jianhua Tao"], "categories": ["cs.HC"], "comment": null, "summary": "MER2025 is the third year of our MER series of challenges, aiming to bring\ntogether researchers in the affective computing community to explore emerging\ntrends and future directions in the field. Previously, MER2023 focused on\nmulti-label learning, noise robustness, and semi-supervised learning, while\nMER2024 introduced a new track dedicated to open-vocabulary emotion\nrecognition. This year, MER2025 centers on the theme \"When Affective Computing\nMeets Large Language Models (LLMs)\".We aim to shift the paradigm from\ntraditional categorical frameworks reliant on predefined emotion taxonomies to\nLLM-driven generative methods, offering innovative solutions for more accurate\nand reliable emotion understanding. The challenge features four tracks:\nMER-SEMI focuses on fixed categorical emotion recognition enhanced by\nsemi-supervised learning; MER-FG explores fine-grained emotions, expanding\nrecognition from basic to nuanced emotional states; MER-DES incorporates\nmultimodal cues (beyond emotion words) into predictions to enhance model\ninterpretability; MER-PR investigates whether emotion prediction results can\nimprove personality recognition performance. For the first three tracks,\nbaseline code is available at MERTools, and datasets can be accessed via\nHugging Face. For the last track, the dataset and baseline code are available\non GitHub."}
{"id": "2504.18851", "pdf": "https://arxiv.org/pdf/2504.18851.pdf", "abs": "https://arxiv.org/abs/2504.18851", "title": "When2Call: When (not) to Call Tools", "authors": ["Hayley Ross", "Ameya Sunil Mahabaleshwarkar", "Yoshi Suhara"], "categories": ["cs.CL"], "comment": "NAACL 2025", "summary": "Leveraging external tools is a key feature for modern Language Models (LMs)\nto expand their capabilities and integrate them into existing systems. However,\nexisting benchmarks primarily focus on the accuracy of tool calling -- whether\nthe correct tool is called with the correct parameters -- and less on\nevaluating when LMs should (not) call tools. We develop a new benchmark,\nWhen2Call, which evaluates tool-calling decision-making: when to generate a\ntool call, when to ask follow-up questions and when to admit the question can't\nbe answered with the tools provided. We find that state-of-the-art tool-calling\nLMs show significant room for improvement on When2Call, indicating the\nimportance of this benchmark. We also develop a training set for When2Call and\nleverage the multiple-choice nature of the benchmark to develop a preference\noptimization training regime, which shows considerably more improvement than\ntraditional fine-tuning. We release the benchmark and training data as well as\nevaluation scripts at https://github.com/NVIDIA/When2Call."}
{"id": "2504.19460", "pdf": "https://arxiv.org/pdf/2504.19460.pdf", "abs": "https://arxiv.org/abs/2504.19460", "title": "A Real-Time Gesture-Based Control Framework", "authors": ["Mahya Khazaei", "Ali Bahrani", "George Tzanetakis"], "categories": ["cs.HC", "cs.AI"], "comment": "8 pages, 4 figures, 2025 International Computer Music Conference", "summary": "We introduce a real-time, human-in-the-loop gesture control framework that\ncan dynamically adapt audio and music based on human movement by analyzing live\nvideo input. By creating a responsive connection between visual and auditory\nstimuli, this system enables dancers and performers to not only respond to\nmusic but also influence it through their movements. Designed for live\nperformances, interactive installations, and personal use, it offers an\nimmersive experience where users can shape the music in real time.\n  The framework integrates computer vision and machine learning techniques to\ntrack and interpret motion, allowing users to manipulate audio elements such as\ntempo, pitch, effects, and playback sequence. With ongoing training, it\nachieves user-independent functionality, requiring as few as 50 to 80 samples\nto label simple gestures. This framework combines gesture training, cue\nmapping, and audio manipulation to create a dynamic, interactive experience.\nGestures are interpreted as input signals, mapped to sound control commands,\nand used to naturally adjust music elements, showcasing the seamless interplay\nbetween human interaction and machine response."}
{"id": "2504.18857", "pdf": "https://arxiv.org/pdf/2504.18857.pdf", "abs": "https://arxiv.org/abs/2504.18857", "title": "Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation", "authors": ["Yi Lu", "Wanxu Zhao", "Xin Zhou", "Chenxin An", "Chenglong Wang", "Shuo Li", "Yuming Yang", "Jun Zhao", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to process and generate coherent\ncontext when the number of input tokens exceeds the pre-trained length. Recent\nadvancements in long-context extension have significantly expanded the context\nwindow of LLMs but require expensive overhead to train the large-scale models\nwith longer context. In this work, we propose Dimension-Wise Positional\nEmbeddings Manipulation (DPE), a training-free framework to extrapolate the\ncontext window of LLMs by diving into RoPE's different hidden dimensions.\nInstead of manipulating all dimensions equally, DPE detects the effective\nlength for every dimension and finds the key dimensions for context extension.\nWe reuse the original position indices with their embeddings from the\npre-trained model and manipulate the key dimensions' position indices to their\nmost effective lengths. In this way, DPE adjusts the pre-trained models with\nminimal modifications while ensuring that each dimension reaches its optimal\nstate for extrapolation. DPE significantly surpasses well-known baselines such\nas YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of\n128k tokens without continual training and integrates seamlessly with Flash\nAttention 2. In addition to its impressive extrapolation capability, DPE also\ndramatically improves the models' performance within training length, such as\nLlama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When\ncompared with commercial models, Llama 3.1 70B with DPE even achieves better\nperformance than GPT-4-128K."}
{"id": "2504.19611", "pdf": "https://arxiv.org/pdf/2504.19611.pdf", "abs": "https://arxiv.org/abs/2504.19611", "title": "Scene2Hap: Combining LLMs and Physical Modeling for Automatically Generating Vibrotactile Signals for Full VR Scenes", "authors": ["Arata Jingu", "Easa AliAbbasi", "Paul Strohmeier", "Jürgen Steimle"], "categories": ["cs.HC"], "comment": null, "summary": "Haptic feedback contributes to immersive virtual reality (VR) experiences.\nDesigning such feedback at scale, for all objects within a VR scene and their\nrespective arrangements, remains a time-consuming task. We present Scene2Hap,\nan LLM-centered system that automatically designs object-level vibrotactile\nfeedback for entire VR scenes based on the objects' semantic attributes and\nphysical context. Scene2Hap employs a multimodal large language model to\nestimate the semantics and physical context of each object, including its\nmaterial properties and vibration behavior, from the multimodal information\npresent in the VR scene. This semantic and physical context is then used to\ncreate plausible vibrotactile signals by generating or retrieving audio signals\nand converting them to vibrotactile signals. For the more realistic spatial\nrendering of haptics in VR, Scene2Hap estimates the propagation and attenuation\nof vibration signals from their source across objects in the scene, considering\nthe estimated material properties and physical context, such as the distance\nand contact between virtual objects. Results from two user studies confirm that\nScene2Hap successfully estimates the semantics and physical context of VR\nscenes, and the physical modeling of vibration propagation improves usability,\nperceived materiality, and spatial awareness."}
{"id": "2504.18872", "pdf": "https://arxiv.org/pdf/2504.18872.pdf", "abs": "https://arxiv.org/abs/2504.18872", "title": "Latent Adversarial Training Improves the Representation of Refusal", "authors": ["Alexandra Abbas", "Nora Petrova", "Helios Ael Lyons", "Natalia Perez-Campanero"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent work has shown that language models' refusal behavior is primarily\nencoded in a single direction in their latent space, making it vulnerable to\ntargeted attacks. Although Latent Adversarial Training (LAT) attempts to\nimprove robustness by introducing noise during training, a key question\nremains: How does this noise-based training affect the underlying\nrepresentation of refusal behavior? Understanding this encoding is crucial for\nevaluating LAT's effectiveness and limitations, just as the discovery of linear\nrefusal directions revealed vulnerabilities in traditional supervised safety\nfine-tuning (SSFT).\n  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the\nrefusal behavior in the model's latent space compared to SSFT and embedding\nspace adversarial training (AT). By computing activation differences between\nharmful and harmless instruction pairs and applying Singular Value\nDecomposition (SVD), we find that LAT significantly alters the refusal\nrepresentation, concentrating it in the first two SVD components which explain\napproximately 75 percent of the activation differences variance - significantly\nhigher than in reference models. This concentrated representation leads to more\neffective and transferable refusal vectors for ablation attacks: LAT models\nshow improved robustness when attacked with vectors from reference models but\nbecome more vulnerable to self-generated vectors compared to SSFT and AT. Our\nfindings suggest that LAT's training perturbations enable a more comprehensive\nrepresentation of refusal behavior, highlighting both its potential strengths\nand vulnerabilities for improving model safety."}
{"id": "2504.19703", "pdf": "https://arxiv.org/pdf/2504.19703.pdf", "abs": "https://arxiv.org/abs/2504.19703", "title": "Interactive Discovery and Exploration of Visual Bias in Generative Text-to-Image Models", "authors": ["Johannes Eschner", "Roberto Labadie-Tamayo", "Matthias Zeppelzauer", "Manuela Waldner"], "categories": ["cs.HC"], "comment": null, "summary": "Bias in generative Text-to-Image (T2I) models is a known issue, yet\nsystematically analyzing such models' outputs to uncover it remains\nchallenging. We introduce the Visual Bias Explorer (ViBEx) to interactively\nexplore the output space of T2I models to support the discovery of visual bias.\nViBEx introduces a novel flexible prompting tree interface in combination with\nzero-shot bias probing using CLIP for quick and approximate bias exploration.\nIt additionally supports in-depth confirmatory bias analysis through visual\ninspection of forward, intersectional, and inverse bias queries. ViBEx is\nmodel-agnostic and publicly available. In four case study interviews, experts\nin AI and ethics were able to discover visual biases that have so far not been\ndescribed in literature."}
{"id": "2504.18884", "pdf": "https://arxiv.org/pdf/2504.18884.pdf", "abs": "https://arxiv.org/abs/2504.18884", "title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification", "authors": ["Junichiro Niimi"], "categories": ["cs.CL", "cs.AI"], "comment": "This manuscript has been accepted for the 30th International\n  Conference on Natural Language & Information Systems (NLDB 2025). The final\n  version will appear in the Springer LNCS proceedings. arXiv admin note: text\n  overlap with arXiv:2407.13069", "summary": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%."}
{"id": "2504.19767", "pdf": "https://arxiv.org/pdf/2504.19767.pdf", "abs": "https://arxiv.org/abs/2504.19767", "title": "Crafting a Personal Journaling Practice: Negotiating Ecosystems of Materials, Personal Context, and Community in Analog Journaling", "authors": ["Katherine Lin", "Juna Kawai-Yue", "Adira Sklar", "Lucy Hecht", "Sarah Sterman", "Tiffany Tseng"], "categories": ["cs.HC"], "comment": "Creativity and Cognition 2025", "summary": "Analog journaling has grown in popularity, with journaling on paper\nencompassing a range of motivations, styles, and practices including planning,\nhabit-tracking, and reflecting. Journalers develop strong personal preferences\naround the tools they use, the ideas they capture, and the layout in which they\nrepresent their ideas and memories. Understanding how analog journaling\npractices are individually shaped and crafted over time is critical to\nsupporting the varied benefits associated with journaling, including improved\nmental health and positive support for identity development. To understand this\ndevelopment, we qualitatively analyzed publicly-shared journaling content from\nYouTube and Instagram and interviewed 11 journalers. We report on our\nidentification of the journaling ecosystem in which journaling practices are\nshaped by materials, personal context, and communities, sharing how this\necosystem plays a role in the practices and identities of journalers as they\ncustomize their journaling routine to best suit their personal goals. Using\nthese insights, we discuss design opportunities for how future tools can better\nalign with and reflect the rich affordances and practices of journaling on\npaper."}
{"id": "2504.18938", "pdf": "https://arxiv.org/pdf/2504.18938.pdf", "abs": "https://arxiv.org/abs/2504.18938", "title": "MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction", "authors": ["Junhong Liang", "Yu Zhou"], "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. While Large Language Models (LLMs) have shown remarkable success\nin identifying and rectifying potential errors, they often struggle with\nmaintaining consistent output lengths and adapting to domain-specific\ncorrections. Furthermore, existing CSC task impose rigid constraints requiring\ninput and output lengths to be identical, limiting their applicability. In this\nwork, we extend traditional CSC to variable-length correction scenarios,\nincluding Chinese Splitting Error Correction (CSEC) and ASR N-best Error\nCorrection. To address domain adaptation and length consistency, we propose\nMTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection\nmechanism. Our approach constructs a retrieval database from domain-specific\ntraining data and dictionaries, fine-tuning retrievers to optimize performance\nfor error-containing inputs. Additionally, we introduce a multi-source\ncombination strategy with iterative length reflection to ensure output length\nfidelity. Experiments across diverse domain datasets demonstrate that our\nmethod significantly outperforms current approaches in correction quality,\nparticularly in handling domain-specific and variable-length error correction\ntasks."}
{"id": "2504.19772", "pdf": "https://arxiv.org/pdf/2504.19772.pdf", "abs": "https://arxiv.org/abs/2504.19772", "title": "Memento: Augmenting Personalized Memory via Practical Multimodal Wearable Sensing in Visual Search and Wayfinding Navigation", "authors": ["Indrajeet Ghosh", "Kasthuri Jayarajah", "Nicholas Waytowich", "Nirmalya Roy"], "categories": ["cs.HC", "cs.MM"], "comment": "This work has been accepted to the Proceedings of the ACM UMAP 2025", "summary": "Working memory involves the temporary retention of information over short\nperiods. It is a critical cognitive function that enables humans to perform\nvarious online processing tasks, such as dialing a phone number, recalling\nmisplaced items' locations, or navigating through a store. However, inherent\nlimitations in an individual's capacity to retain information often result in\nforgetting important details during such tasks. Although previous research has\nsuccessfully utilized wearable and assistive technologies to enhance long-term\nmemory functions (e.g., episodic memory), their application to supporting\nshort-term recall in daily activities remains underexplored. To address this\ngap, we present Memento, a framework that uses multimodal wearable sensor data\nto detect significant changes in cognitive state and provide intelligent in\nsitu cues to enhance recall. Through two user studies involving 15 and 25\nparticipants in visual search navigation tasks, we demonstrate that\nparticipants receiving visual cues from Memento achieved significantly better\nroute recall, improving approximately 20-23% compared to free recall.\nFurthermore, Memento reduced cognitive load and review time by 46% while also\nsubstantially reducing computation time (3.86 seconds vs. 15.35 seconds),\noffering an average of 75% effectiveness compared to computer vision-based cue\nselection approaches."}
{"id": "2504.18942", "pdf": "https://arxiv.org/pdf/2504.18942.pdf", "abs": "https://arxiv.org/abs/2504.18942", "title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes", "authors": ["Debarati Das", "Khanh Chi Le", "Ritik Sachin Parkar", "Karin De Langis", "Brendan Madson", "Chad M. Berryman", "Robin M. Willis", "Daniel H. Moses", "Brett McDonnell", "Daniel Schwarcz", "Dongyeop Kang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "submitted to COLM 2025", "summary": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Building on\nthese findings, we propose a set of design suggestions, rooted in empirical\nobservations, that align AI assistance with human goals of clarity,\ncompleteness, creativity, and efficiency, through hybrid planning, adaptive\nexecution, and decision-point support. Our results highlight both the current\nlimitations of LLMs in supporting complex legal workflows and opportunities for\ndeveloping more collaborative, reasoning-aware legal AI systems. All data and\ncode are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/)."}
{"id": "2504.19838", "pdf": "https://arxiv.org/pdf/2504.19838.pdf", "abs": "https://arxiv.org/abs/2504.19838", "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects", "authors": ["Guangyi Liu", "Pengxiang Zhao", "Liang Liu", "Yaxuan Guo", "Han Xiao", "Weifeng Lin", "Yuxiang Chai", "Yue Han", "Shuai Ren", "Hao Wang", "Xiaoyu Liang", "Wenhao Wang", "Tianze Wu", "Linghao Li", "Hao Wang", "Guanjing Xiong", "Yong Liu", "Hongsheng Li"], "categories": ["cs.HC"], "comment": "37 pages, 10 figures, 7 tables, Project Homepage:\n  https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents", "summary": "With the rapid rise of large language models (LLMs), phone automation has\nundergone transformative changes. This paper systematically reviews LLM-driven\nphone GUI agents, highlighting their evolution from script-based automation to\nintelligent, adaptive systems. We first contextualize key challenges, (i)\nlimited generality, (ii) high maintenance overhead, and (iii) weak intent\ncomprehension, and show how LLMs address these issues through advanced language\nunderstanding, multimodal perception, and robust decision-making. We then\npropose a taxonomy covering fundamental agent frameworks (single-agent,\nmulti-agent, plan-then-act), modeling approaches (prompt engineering,\ntraining-based), and essential datasets and benchmarks. Furthermore, we detail\ntask-specific architectures, supervised fine-tuning, and reinforcement learning\nstrategies that bridge user intent and GUI operations. Finally, we discuss open\nchallenges such as dataset diversity, on-device deployment efficiency,\nuser-centric adaptation, and security concerns, offering forward-looking\ninsights into this rapidly evolving field. By providing a structured overview\nand identifying pressing research gaps, this paper serves as a definitive\nreference for researchers and practitioners seeking to harness LLMs in\ndesigning scalable, user-friendly phone GUI agents."}
{"id": "2504.18992", "pdf": "https://arxiv.org/pdf/2504.18992.pdf", "abs": "https://arxiv.org/abs/2504.18992", "title": "Dynamic Fisher-weighted Model Merging via Bayesian Optimization", "authors": ["Sanwoo Lee", "Jiahao Liu", "Qifan Wang", "Jingang Wang", "Xunliang Cai", "Yunfang Wu"], "categories": ["cs.CL"], "comment": null, "summary": "The fine-tuning of pre-trained language models has resulted in the widespread\navailability of task-specific models. Model merging offers an efficient way to\ncreate multi-task models by combining these fine-tuned models at the parameter\nlevel, without the need for training data or joint training on multiple\ndatasets. Existing merging approaches typically involve scaling the parameters\nmodel-wise or integrating parameter importance parameter-wise. Both approaches\nexhibit their own weaknesses, leading to a notable performance gap compared to\nmulti-task fine-tuning. In this paper, we unify these seemingly distinct\nstrategies into a more general merging framework, and introduce Dynamic\nFisher-weighted Merging (DF-Merge). Specifically, candidate models are\nassociated with a set of coefficients that linearly scale their fine-tuned\nparameters. Bayesian optimization is applied to dynamically adjust these\ncoefficients, aiming to maximize overall performance on validation sets. Each\niteration of this process integrates parameter importance based on the Fisher\ninformation conditioned by the coefficients. Experimental results show that\nDF-Merge outperforms strong baselines across models of different sizes and a\nvariety of tasks. Our analysis shows that the effectiveness of DF-Merge arises\nfrom the unified view of merging and that near-optimal performance is\nachievable in a few iterations, even with minimal validation data."}
{"id": "2504.20016", "pdf": "https://arxiv.org/pdf/2504.20016.pdf", "abs": "https://arxiv.org/abs/2504.20016", "title": "Applying LLM-Powered Virtual Humans to Child Interviews in Child-Centered Design", "authors": ["Linshi Li", "Hanlin Cai"], "categories": ["cs.HC", "cs.CY", "cs.MM"], "comment": "This paper has been accepted as a Work-in-Progress (WiP) paper in the\n  24th annual ACM Interaction Design and Children (IDC) Conference", "summary": "In child-centered design, directly engaging children is crucial for deeply\nunderstanding their experiences. However, current research often prioritizes\nadult perspectives, as interviewing children involves unique challenges such as\nenvironmental sensitivities and the need for trust-building. AI-powered virtual\nhumans (VHs) offer a promising approach to facilitate engaging and multimodal\ninteractions with children. This study establishes key design guidelines for\nLLM-powered virtual humans tailored to child interviews, standardizing\nmultimodal elements including color schemes, voice characteristics, facial\nfeatures, expressions, head movements, and gestures. Using ChatGPT-based prompt\nengineering, we developed three distinct Human-AI workflows (LLM-Auto,\nLLM-Interview, and LLM-Analyze) and conducted a user study involving 15\nchildren aged 6 to 12. The results indicated that the LLM-Analyze workflow\noutperformed the others by eliciting longer responses, achieving higher user\nexperience ratings, and promoting more effective child engagement."}
{"id": "2504.19019", "pdf": "https://arxiv.org/pdf/2504.19019.pdf", "abs": "https://arxiv.org/abs/2504.19019", "title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs", "authors": ["Mohammad Akbar-Tajari", "Mohammad Taher Pilehvar", "Mohammad Mahmoody"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "19 pages, 1 figure, 6 tables", "summary": "The challenge of ensuring Large Language Models (LLMs) align with societal\nstandards is of increasing interest, as these models are still prone to\nadversarial jailbreaks that bypass their safety mechanisms. Identifying these\nvulnerabilities is crucial for enhancing the robustness of LLMs against such\nexploits. We propose Graph of ATtacks (GoAT), a method for generating\nadversarial prompts to test the robustness of LLM alignment using the Graph of\nThoughts framework [Besta et al., 2024]. GoAT excels at generating highly\neffective jailbreak prompts with fewer queries to the victim model than\nstate-of-the-art attacks, achieving up to five times better jailbreak success\nrate against robust models like Llama. Notably, GoAT creates high-quality,\nhuman-readable prompts without requiring access to the targeted model's\nparameters, making it a black-box attack. Unlike approaches constrained by\ntree-based reasoning, GoAT's reasoning is based on a more intricate graph\nstructure. By making simultaneous attack paths aware of each other's progress,\nthis dynamic framework allows a deeper integration and refinement of reasoning\npaths, significantly enhancing the collaborative exploration of adversarial\nvulnerabilities in LLMs. At a technical level, GoAT starts with a graph\nstructure and iteratively refines it by combining and improving thoughts,\nenabling synergy between different thought paths. The code for our\nimplementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks."}
{"id": "2504.20035", "pdf": "https://arxiv.org/pdf/2504.20035.pdf", "abs": "https://arxiv.org/abs/2504.20035", "title": "Cam-2-Cam: Exploring the Design Space of Dual-Camera Interactions for Smartphone-based Augmented Reality", "authors": ["Brandon Woodard", "Melvin He", "Mose Sakashita", "Jing Qian", "Zainab Iftikhar", "Joseph LaViola Jr"], "categories": ["cs.HC", "H.5.2; H.5.1; H.5.0; H.1.2"], "comment": null, "summary": "Off-the-shelf smartphone-based AR systems typically use a single front-facing\nor rear-facing camera, which restricts user interactions to a narrow field of\nview and small screen size, thus reducing their practicality. We present\n\\textit{Cam-2-Cam}, an interaction concept implemented in three\nsmartphone-based AR applications with interactions that span both cameras.\nResults from our qualitative analysis conducted on 30 participants presented\ntwo major design lessons that explore the interaction space of smartphone AR\nwhile maintaining critical AR interface attributes like embodiment and\nimmersion: (1) \\textit{Balancing Contextual Relevance and Feedback Quality}\nserves to outline a delicate balance between implementing familiar interactions\npeople do in the real world and the quality of multimodal AR responses and (2)\n\\textit{Preventing Disorientation using Simultaneous Capture and Alternating\nCameras} which details how to prevent disorientation during AR interactions\nusing the two distinct camera techniques we implemented in the paper.\nAdditionally, we consider observed user assumptions or natural tendencies to\ninform future implementations of dual-camera setups for smartphone-based AR. We\nenvision our design lessons as an initial pioneering step toward expanding the\ninteraction space of smartphone-based AR, potentially driving broader adoption\nand overcoming limitations of single-camera AR."}
{"id": "2504.19021", "pdf": "https://arxiv.org/pdf/2504.19021.pdf", "abs": "https://arxiv.org/abs/2504.19021", "title": "Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting", "authors": ["Zhyar Rzgar K Rostam", "Gábor Kertész"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure, 8 tables", "summary": "Efficient text classification is essential for handling the increasing volume\nof academic publications. This study explores the use of pre-trained language\nmodels (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on\nthe Web of Science (WoS-46985) dataset for scientific text classification. To\nenhance performance, we augment the dataset by executing seven targeted queries\nin the WoS database, retrieving 1,000 articles per category aligned with\nWoS-46985's main classes. PLMs predict labels for this unlabeled data, and a\nhard-voting strategy combines predictions for improved accuracy and confidence.\nFine-tuning on the expanded dataset with dynamic learning rates and early\nstopping significantly boosts classification accuracy, especially in\nspecialized domains. Domain-specific models like SciBERT and BioBERT\nconsistently outperform general-purpose models such as BERT. These findings\nunderscore the efficacy of dataset augmentation, inference-driven label\nprediction, hard-voting, and fine-tuning techniques in creating robust and\nscalable solutions for automated academic text classification."}
{"id": "2504.18702", "pdf": "https://arxiv.org/pdf/2504.18702.pdf", "abs": "https://arxiv.org/abs/2504.18702", "title": "Codetations: Intelligent, Persistent Notes and UIs for Programs and Other Documents", "authors": ["Edward Misback", "Erik Vank", "Zachary Tatlock", "Steven Tanimoto"], "categories": ["cs.SE", "cs.HC"], "comment": "24 pages, 5 figures, 2 tables", "summary": "Software developers maintain extensive mental models of code they produce and\nits context, often relying on memory to retrieve or reconstruct design\ndecisions, edge cases, and debugging experiences. These missing links and data\nobstruct both developers and, more recently, large language models (LLMs)\nworking with unfamiliar code. We present Codetations, a system that helps\ndevelopers contextualize documents with rich notes and tools. Unlike previous\napproaches, notes in Codetations stay outside the document to prevent code\nclutter, attaching to spans in the document using a hybrid\nedit-tracking/LLM-based method. Their content is dynamic, interactive, and\nsynchronized with code changes. A worked example shows that relevant notes with\ninteractively-collected data improve LLM performance during code repair. In our\nuser evaluation, developers praised these properties and saw significant\npotential in annotation types that we generated with an LLM in just a few\nminutes."}
{"id": "2504.19024", "pdf": "https://arxiv.org/pdf/2504.19024.pdf", "abs": "https://arxiv.org/abs/2504.19024", "title": "KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation", "authors": ["Jiabin Fan", "Guoqing Luo", "Michael Bowling", "Lili Mou"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel k-step return estimation method (called KETCHUP) for\nReinforcement Learning(RL)-based knowledge distillation (KD) in text generation\ntasks. Our idea is to induce a K-step return by using the Bellman Optimality\nEquation for multiple steps. Theoretical analysis shows that this K-step\nformulation reduces the variance of the gradient estimates, thus leading to\nimproved RL optimization especially when the student model size is large.\nEmpirical evaluation on three text generation tasks demonstrates that our\napproach yields superior performance in both standard task metrics and large\nlanguage model (LLM)-based evaluation. These results suggest that our K-step\nreturn induction offers a promising direction for enhancing RL-based KD in LLM\nresearch."}
{"id": "2504.19037", "pdf": "https://arxiv.org/pdf/2504.19037.pdf", "abs": "https://arxiv.org/abs/2504.19037", "title": "\"I Would Have Written My Code Differently'': Beginners Struggle to Understand LLM-Generated Code", "authors": ["Yangtian Zi", "Luisa Li", "Arjun Guha", "Carolyn Jane Anderson", "Molly Q Feldman"], "categories": ["cs.SE", "cs.HC"], "comment": "To appear in 33rd ACM International Conference on the Foundations of\n  Software Engineering (FSE Companion '25), June 23-28, 2025, Trondheim, Norway", "summary": "Large language models (LLMs) are being increasingly adopted for programming\nwork. Prior work shows that while LLMs accelerate task completion for\nprofessional programmers, beginning programmers struggle to prompt models\neffectively. However, prompting is just half of the code generation process --\nwhen code is generated, it must be read, evaluated, and integrated (or\nrejected). How accessible are these tasks for beginning programmers?\n  This paper measures how well beginners comprehend LLM-generated code and\nexplores the challenges students face in judging code correctness. We compare\nhow well students understand natural language descriptions of functions and\nLLM-generated implementations, studying 32 CS1 students on 160 task instances.\nOur results show a low per-task success rate of 32.5\\%, with indiscriminate\nstruggles across demographic populations. Key challenges include barriers for\nnon-native English speakers, unfamiliarity with Python syntax, and automation\nbias. Our findings highlight the barrier that code comprehension presents to\nbeginning programmers seeking to write code with LLMs."}
{"id": "2504.19044", "pdf": "https://arxiv.org/pdf/2504.19044.pdf", "abs": "https://arxiv.org/abs/2504.19044", "title": "Calibrating Translation Decoding with Quality Estimation on LLMs", "authors": ["Di Wu", "Yibin Lei", "Christof Monz"], "categories": ["cs.CL"], "comment": null, "summary": "Neural machine translation (NMT) systems typically employ maximum a\nposteriori (MAP) decoding to select the highest-scoring translation from the\ndistribution mass. However, recent evidence highlights the inadequacy of MAP\ndecoding, often resulting in low-quality or even pathological hypotheses -- the\ndecoding objective is not aligned with real-world translation quality. This\npaper proposes calibrating hypothesis likelihoods with translation quality from\na distribution view by directly optimizing their Pearson correlation -- thereby\nenhancing the effectiveness of translation decoding. With our method,\ntranslation on large language models (LLMs) improves substantially after\nlimited training (2K instances per direction). This improvement is orthogonal\nto those achieved through supervised fine-tuning, leading to substantial gains\nacross a broad range of metrics and human evaluations -- even when applied to\ntop-performing translation-specialized LLMs fine-tuned on high-quality\ntranslation data, such as Tower, or when compared to recent preference\noptimization methods, like CPO. Moreover, the calibrated translation likelihood\ncan directly serve as a strong proxy for translation quality, closely\napproximating or even surpassing some state-of-the-art translation quality\nestimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates\nthat calibration enhances the effectiveness of MAP decoding, thereby enabling\ngreater efficiency in real-world deployment. The resulting state-of-the-art\ntranslation model, which covers 10 languages, along with the accompanying code\nand human evaluation data, has been released to the community:\nhttps://github.com/moore3930/calibrating-llm-mt."}
{"id": "2504.19047", "pdf": "https://arxiv.org/pdf/2504.19047.pdf", "abs": "https://arxiv.org/abs/2504.19047", "title": "AI Recommendations and Non-instrumental Image Concerns", "authors": ["David Almog"], "categories": ["econ.GN", "cs.AI", "cs.HC", "q-fin.EC"], "comment": null, "summary": "There is growing enthusiasm about the potential for humans and AI to\ncollaborate by leveraging their respective strengths. Yet in practice, this\npromise often falls short. This paper uses an online experiment to identify\nnon-instrumental image concerns as a key reason individuals underutilize AI\nrecommendations. I show that concerns about how one is perceived, even when\nthose perceptions carry no monetary consequences, lead participants to\ndisregard AI advice and reduce task performance."}
{"id": "2504.19061", "pdf": "https://arxiv.org/pdf/2504.19061.pdf", "abs": "https://arxiv.org/abs/2504.19061", "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models", "authors": ["Anindya Bijoy Das", "Shibbir Ahmed", "Shahnewaz Karim Sakib"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, such as reasons for hospital admission, significant\nin-hospital events, and critical follow-up actions. In addition, we also assess\nthe prevalence of various types of hallucinations in the summaries produced by\nthese models. Detecting hallucinations is vital as it directly influences the\nreliability of the information, potentially affecting patient care and\ntreatment outcomes. We conduct comprehensive numerical simulations to\nrigorously evaluate the performance of these models, further probing the\naccuracy and fidelity of the extracted content in clinical summarization."}
{"id": "2504.19061", "pdf": "https://arxiv.org/pdf/2504.19061.pdf", "abs": "https://arxiv.org/abs/2504.19061", "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models", "authors": ["Anindya Bijoy Das", "Shibbir Ahmed", "Shahnewaz Karim Sakib"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, such as reasons for hospital admission, significant\nin-hospital events, and critical follow-up actions. In addition, we also assess\nthe prevalence of various types of hallucinations in the summaries produced by\nthese models. Detecting hallucinations is vital as it directly influences the\nreliability of the information, potentially affecting patient care and\ntreatment outcomes. We conduct comprehensive numerical simulations to\nrigorously evaluate the performance of these models, further probing the\naccuracy and fidelity of the extracted content in clinical summarization."}
{"id": "2504.19066", "pdf": "https://arxiv.org/pdf/2504.19066.pdf", "abs": "https://arxiv.org/abs/2504.19066", "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics", "authors": ["Deeksha Varshney", "Keane Ong", "Rui Mao", "Erik Cambria", "Gianmarco Mengaldo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.ao-ph"], "comment": null, "summary": "Accurate assessments of extreme weather events are vital for research and\npolicy, yet localized and granular data remain scarce in many parts of the\nworld. This data gap limits our ability to analyze potential outcomes and\nimplications of extreme weather events, hindering effective decision-making.\nLarge Language Models (LLMs) can process vast amounts of unstructured text\ndata, extract meaningful insights, and generate detailed assessments by\nsynthesizing information from multiple sources. Furthermore, LLMs can\nseamlessly transfer their general language understanding to smaller models,\nenabling these models to retain key knowledge while being fine-tuned for\nspecific tasks. In this paper, we propose Extreme Weather Reasoning-Aware\nAlignment (EWRA), a method that enhances small language models (SLMs) by\nincorporating structured reasoning paths derived from LLMs, and\nExtremeWeatherNews, a large dataset of extreme weather event-related news\narticles. EWRA and ExtremeWeatherNews together form the overall framework,\nClimaEmpact, that focuses on addressing three critical extreme-weather tasks:\ncategorization of tangible vulnerabilities/impacts, topic labeling, and emotion\nanalysis. By aligning SLMs with advanced reasoning strategies on\nExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for\nSLM alignment), EWRA improves the SLMs' ability to generate well-grounded and\ndomain-specific responses for extreme weather analytics. Our results show that\nthe approach proposed guides SLMs to output domain-aligned responses,\nsurpassing the performance of task-specific models and offering enhanced\nreal-world applicability for extreme weather analytics."}
{"id": "2504.19131", "pdf": "https://arxiv.org/pdf/2504.19131.pdf", "abs": "https://arxiv.org/abs/2504.19131", "title": "Making Physical Objects with Generative AI and Robotic Assembly: Considering Fabrication Constraints, Sustainability, Time, Functionality, and Accessibility", "authors": ["Alexander Htet Kyaw", "Se Hwan Jeon", "Miana Smith", "Neil Gershenfeld"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "3D generative AI enables rapid and accessible creation of 3D models from text\nor image inputs. However, translating these outputs into physical objects\nremains a challenge due to the constraints in the physical world. Recent\nstudies have focused on improving the capabilities of 3D generative AI to\nproduce fabricable outputs, with 3D printing as the main fabrication method.\nHowever, this workshop paper calls for a broader perspective by considering how\nfabrication methods align with the capabilities of 3D generative AI. As a case\nstudy, we present a novel system using discrete robotic assembly and 3D\ngenerative AI to make physical objects. Through this work, we identified five\nkey aspects to consider in a physical making process based on the capabilities\nof 3D generative AI. 1) Fabrication Constraints: Current text-to-3D models can\ngenerate a wide range of 3D designs, requiring fabrication methods that can\nadapt to the variability of generative AI outputs. 2) Time: While generative AI\ncan generate 3D models in seconds, fabricating physical objects can take hours\nor even days. Faster production could enable a closer iterative design loop\nbetween humans and AI in the making process. 3) Sustainability: Although\ntext-to-3D models can generate thousands of models in the digital world,\nextending this capability to the real world would be resource-intensive,\nunsustainable and irresponsible. 4) Functionality: Unlike digital outputs from\n3D generative AI models, the fabrication method plays a crucial role in the\nusability of physical objects. 5) Accessibility: While generative AI simplifies\n3D model creation, the need for fabrication equipment can limit participation,\nmaking AI-assisted creation less inclusive. These five key aspects provide a\nframework for assessing how well a physical making process aligns with the\ncapabilities of 3D generative AI and values in the world."}
{"id": "2504.19070", "pdf": "https://arxiv.org/pdf/2504.19070.pdf", "abs": "https://arxiv.org/abs/2504.19070", "title": "Sample-Efficient Language Model for Hinglish Conversational AI", "authors": ["Sakshi Singh", "Abhinav Prakash", "Aakriti Shah", "Chaitanya Sachdeva", "Sanjana Dumpala"], "categories": ["cs.CL", "I.2.7; I.2.6; H.5.2"], "comment": "5 pages, 2 tables, 2 figures", "summary": "This paper presents our process for developing a sample-efficient language\nmodel for a conversational Hinglish chatbot. Hinglish, a code-mixed language\nthat combines Hindi and English, presents a unique computational challenge due\nto inconsistent spelling, lack of standardization, and limited quality of\nconversational data. This work evaluates multiple pre-trained cross-lingual\nlanguage models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning\ntechniques to improve performance on Hinglish conversational tasks. The\nproposed approach integrates synthetically generated dialogues with insights\nfrom existing Hinglish datasets to address data scarcity. Experimental results\ndemonstrate that models with fewer parameters, when appropriately fine-tuned on\nhigh-quality code-mixed data, can achieve competitive performance for Hinglish\nconversation generation while maintaining computational efficiency."}
{"id": "2504.19556", "pdf": "https://arxiv.org/pdf/2504.19556.pdf", "abs": "https://arxiv.org/abs/2504.19556", "title": "Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment", "authors": ["Kristen Sussman", "Daniel Carter"], "categories": ["cs.CL", "cs.HC", "J.4; K.4.0; I.2.7"], "comment": "5 pages, 3 figures, Companion Proceedings of the ACM Web Conference\n  2025", "summary": "Given the subtle human-like effects of large language models on linguistic\npatterns, this study examines shifts in language over time to detect the impact\nof AI-mediated communication (AI- MC) on social media. We compare a replicated\ndataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the\nsame period in 2024, all of which mention Donald Trump during election periods.\nUsing a combination of Flesch-Kincaid readability and polarity scores, we\nanalyze changes in text complexity and sentiment. Our findings reveal a\nsignificant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift\nfrom predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more\npositive expressions (28.6% to 45.9%). These findings suggest not only an\nincreasing presence of AI in social media communication but also its impact on\nlanguage and emotional expression patterns."}
{"id": "2504.19095", "pdf": "https://arxiv.org/pdf/2504.19095.pdf", "abs": "https://arxiv.org/abs/2504.19095", "title": "Efficient Reasoning for LLMs through Speculative Chain-of-Thought", "authors": ["Jikai Wang", "Juntao Li", "Lijun Wu", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have\nrecently attracted widespread attention due to their impressive task-solving\nabilities. However, the enormous model size and the generation of lengthy\nthought chains introduce significant reasoning costs and response latency.\nExisting methods for efficient reasoning mainly focus on reducing the number of\nmodel parameters or shortening the chain-of-thought length. In this paper, we\nintroduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency\nfrom another perspective by accelerated average reasoning speed through large\nand small model collaboration. SCoT conducts thought-level drafting using a\nlightweight draft model. Then it selects the best CoT draft and corrects the\nerror cases with the target model. The proposed thinking behavior alignment\nimproves the efficiency of drafting and the draft selection strategy maintains\nthe prediction accuracy for complex problems. Experimental results on GSM8K,\nMATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces\nreasoning latency by 48\\%$\\sim$66\\% for Deepseek-R1-Distill-Qwen-32B while\nachieving near-target-model-level performance. Our code is available at\nhttps://github.com/Jikai0Wang/Speculative_CoT."}
{"id": "2504.19728", "pdf": "https://arxiv.org/pdf/2504.19728.pdf", "abs": "https://arxiv.org/abs/2504.19728", "title": "Hector UI: A Flexible Human-Robot User Interface for (Semi-)Autonomous Rescue and Inspection Robots", "authors": ["Stefan Fabian", "Oskar von Stryk"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "The remote human operator's user interface (UI) is an important link to make\nthe robot an efficient extension of the operator's perception and action. In\nrescue applications, several studies have investigated the design of operator\ninterfaces based on observations during major robotics competitions or field\ndeployments. Based on this research, guidelines for good interface design were\nempirically identified. The investigations on the UIs of teams participating in\ncompetitions are often based on external observations during UI application,\nwhich may miss some relevant requirements for UI flexibility. In this work, we\npresent an open-source and flexibly configurable user interface based on\nestablished guidelines and its exemplary use for wheeled, tracked, and walking\nrobots. We explain the design decisions and cover the insights we have gained\nduring its highly successful applications in multiple robotics competitions and\nevaluations. The presented UI can also be adapted for other robots with little\neffort and is available as open source."}
{"id": "2504.19101", "pdf": "https://arxiv.org/pdf/2504.19101.pdf", "abs": "https://arxiv.org/abs/2504.19101", "title": "Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation", "authors": ["Qianren Mao", "Qili Zhang", "Hanwen Hao", "Zhentao Han", "Runhua Xu", "Weifeng Jiang", "Qi Hu", "Zhijun Chen", "Tyler Zhou", "Bo Li", "Yangqiu Song", "Jin Dong", "Jianxin Li", "Philip S. Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution for enhancing the accuracy and credibility of Large Language Models\n(LLMs), particularly in Question & Answer tasks. This is achieved by\nincorporating proprietary and private data from integrated databases. However,\nprivate RAG systems face significant challenges due to the scarcity of private\ndomain data and critical data privacy issues. These obstacles impede the\ndeployment of private RAG systems, as developing privacy-preserving RAG systems\nrequires a delicate balance between data security and data availability. To\naddress these challenges, we regard federated learning (FL) as a highly\npromising technology for privacy-preserving RAG services. We propose a novel\nframework called Federated Retrieval-Augmented Generation (FedE4RAG). This\nframework facilitates collaborative training of client-side RAG retrieval\nmodels. The parameters of these models are aggregated and distributed on a\ncentral-server, ensuring data privacy without direct sharing of raw data. In\nFedE4RAG, knowledge distillation is employed for communication between the\nserver and client models. This technique improves the generalization of local\nRAG retrievers during the federated learning process. Additionally, we apply\nhomomorphic encryption within federated learning to safeguard model parameters\nand mitigate concerns related to data leakage. Extensive experiments conducted\non the real-world dataset have validated the effectiveness of FedE4RAG. The\nresults demonstrate that our proposed framework can markedly enhance the\nperformance of private RAG systems while maintaining robust data privacy\nprotection."}
{"id": "2311.10833", "pdf": "https://arxiv.org/pdf/2311.10833.pdf", "abs": "https://arxiv.org/abs/2311.10833", "title": "Generative AI has lowered the barriers to computational social sciences", "authors": ["Yongjun Zhang"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Generative artificial intelligence (AI) has revolutionized the field of\ncomputational social science (CSS), unleashing new possibilities for collecting\nand analyzing multimodal data, especially for scholars who may not have\nextensive programming expertise. This breakthrough carries profound\nimplications for social scientists. First, generative AI can significantly\nenhance the productivity of social scientists by automating the generation,\nannotation, and debugging of code. Second, it empowers researchers to delve\ninto sophisticated data analysis through the innovative use of prompt\nengineering. Last, the educational sphere of CSS stands to benefit immensely\nfrom these tools, given their exceptional ability to annotate and elucidate\ncomplex codes for learners, thereby simplifying the learning process and making\nthe technology more accessible."}
{"id": "2504.19110", "pdf": "https://arxiv.org/pdf/2504.19110.pdf", "abs": "https://arxiv.org/abs/2504.19110", "title": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries", "authors": ["Huajian Xin", "Luming Li", "Xiaoran Jin", "Jacques Fleuriot", "Wenda Li"], "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries."}
{"id": "2407.02896", "pdf": "https://arxiv.org/pdf/2407.02896.pdf", "abs": "https://arxiv.org/abs/2407.02896", "title": "Predicting and Understanding Turn-Taking Behavior in Open-Ended Group Activities in Virtual Reality", "authors": ["Portia Wang", "Eugy Han", "Anna C. M. Queiroz", "Cyan DeVeaux", "Jeremy N. Bailenson"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "In networked virtual reality (VR), user behaviors, individual differences,\nand group dynamics can serve as important signals into future speech behaviors,\nsuch as who the next speaker will be and the timing of turn-taking behaviors.\nThe ability to predict and understand these behaviors offers opportunities to\nprovide adaptive and personalized assistance, for example helping users with\nvarying sensory abilities navigate complex social scenes and instantiating\nvirtual moderators with natural behaviors. In this work, we predict turn-taking\nbehaviors using features extracted based on social dynamics literature. We\ndiscuss results from a large-scale VR classroom dataset consisting of 77\nsessions and 1660 minutes of small-group social interactions collected over\nfour weeks. In our evaluation, gradient boosting classifiers achieved the best\nperformance, with accuracies of 0.71--0.78 AUC (area under the ROC curve)\nacross three tasks concerning the \"what\", \"who\", and \"when\" of turn-taking\nbehaviors. In interpreting these models, we found that group size, listener\npersonality, speech-related behavior (e.g., time elapsed since the listener's\nlast speech event), group gaze (e.g., how much the group looks at the speaker),\nas well as the listener's and previous speaker's head pitch, head y-axis\nposition, and left hand y-axis position more saliently influenced predictions.\nResults suggested that these features remain reliable indicators in novel\nsocial VR settings, as prediction performance is robust over time and with\ngroups and activities not used in the training dataset. We discuss theoretical\nand practical implications of the work."}
{"id": "2504.19162", "pdf": "https://arxiv.org/pdf/2504.19162.pdf", "abs": "https://arxiv.org/abs/2504.19162", "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "authors": ["Jiaqi Chen", "Bang Zhang", "Ruotian Ma", "Peisong Wang", "Xiaodan Liang", "Zhaopeng Tu", "Xiaolong Li", "Kwan-Yee K. Wong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project: https://chen-judge.github.io/SPC/", "summary": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, applying\nSPC to guide the test-time search of diverse LLMs significantly improves their\nmathematical reasoning performance on MATH500 and AIME2024, outperforming\nstate-of-the-art process reward models."}
{"id": "2409.08775", "pdf": "https://arxiv.org/pdf/2409.08775.pdf", "abs": "https://arxiv.org/abs/2409.08775", "title": "What Should We Engineer in Prompts? Training Humans in Requirement-Driven LLM Use", "authors": ["Qianou Ma", "Weirui Peng", "Chenyang Yang", "Hua Shen", "Kenneth Koedinger", "Tongshuang Wu"], "categories": ["cs.HC", "cs.AI"], "comment": "15 pages; TOCHI 2025", "summary": "Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot)\nneeds humans to clearly articulate customized requirements (e.g., \"start the\nresponse with a tl;dr\"). However, existing prompt engineering instructions\noften lack focused training on requirement articulation and instead tend to\nemphasize increasingly automatable strategies (e.g., tricks like adding\nrole-plays and \"think step-by-step\"). To address the gap, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a randomized controlled\nexperiment with 30 novices, ROPE significantly outperforms conventional prompt\nengineering training (20% vs. 1% gains), a gap that automatic prompt\noptimization cannot close. Furthermore, we demonstrate a direct correlation\nbetween the quality of input requirements and LLM outputs. Our work paves the\nway to empower more end-users to build complex LLM applications."}
{"id": "2504.19191", "pdf": "https://arxiv.org/pdf/2504.19191.pdf", "abs": "https://arxiv.org/abs/2504.19191", "title": "WuNeng: Hybrid State with Attention", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "categories": ["cs.CL"], "comment": null, "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."}
{"id": "2501.01568", "pdf": "https://arxiv.org/pdf/2501.01568.pdf", "abs": "https://arxiv.org/abs/2501.01568", "title": "Interruption Handling for Conversational Robots", "authors": ["Shiye Cao", "Jiwon Moon", "Amama Mahmood", "Victor Nikhil Antony", "Ziang Xiao", "Anqi Liu", "Chien-Ming Huang"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Interruptions, a fundamental component of human communication, can enhance\nthe dynamism and effectiveness of conversations, but only when effectively\nmanaged by all parties involved. Despite advancements in robotic systems,\nstate-of-the-art systems still have limited capabilities in handling\nuser-initiated interruptions in real-time. Prior research has primarily focused\non post hoc analysis of interruptions. To address this gap, we present a system\nthat detects user-initiated interruptions and manages them in real-time based\non the interrupter's intent (i.e., cooperative agreement, cooperative\nassistance, cooperative clarification, or disruptive interruption). The system\nwas designed based on interaction patterns identified from human-human\ninteraction data. We integrated our system into an LLM-powered social robot and\nvalidated its effectiveness through a timed decision-making task and a\ncontentious discussion task with 21 participants. Our system successfully\nhandled 93.69% (n=104/111) of user-initiated interruptions. We discuss our\nlearnings and their implications for designing interruption-handling behaviors\nin conversational robots."}
{"id": "2504.19209", "pdf": "https://arxiv.org/pdf/2504.19209.pdf", "abs": "https://arxiv.org/abs/2504.19209", "title": "Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora", "authors": ["Elisabeth Fittschen", "Bella Xia", "Leib Celnik", "Paul Dilley", "Tom Lippincott"], "categories": ["cs.CL", "cs.LG"], "comment": "Under review", "summary": "We measure the effects of several implementation choices for the Dynamic\nEmbedded Topic Model, as applied to five distinct diachronic corpora, with the\ngoal of isolating important decisions for its use and further development. We\nidentify priorities that will maximize utility in applied scholarship,\nincluding the practical scalability of vocabulary size to best exploit the\nstrengths of embedded representations, and more flexible modeling of intervals\nto accommodate the uneven temporal distributions of historical writing. Of\nsimilar importance, we find performance is not significantly or consistently\naffected by several aspects that otherwise limit the model's application or\nmight consume the resources of a grid search."}
{"id": "2502.06009", "pdf": "https://arxiv.org/pdf/2502.06009.pdf", "abs": "https://arxiv.org/abs/2502.06009", "title": "Media Bias Detector: Designing and Implementing a Tool for Real-Time Selection and Framing Bias Analysis in News Coverage", "authors": ["Jenny S Wang", "Samar Haider", "Amir Tohidi", "Anushkaa Gupta", "Yuxuan Zhang", "Chris Callison-Burch", "David Rothschild", "Duncan J Watts"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Mainstream media, through their decisions on what to cover and how to frame\nthe stories they cover, can mislead readers without using outright falsehoods.\nTherefore, it is crucial to have tools that expose these editorial choices\nunderlying media bias. In this paper, we introduce the Media Bias Detector, a\ntool for researchers, journalists, and news consumers. By integrating large\nlanguage models, we provide near real-time granular insights into the topics,\ntone, political lean, and facts of news articles aggregated to the publisher\nlevel. We assessed the tool's impact by interviewing 13 experts from\njournalism, communications, and political science, revealing key insights into\nusability and functionality, practical applications, and AI's role in powering\nmedia bias tools. We explored this in more depth with a follow-up survey of 150\nnews consumers. This work highlights opportunities for AI-driven tools that\nempower users to critically engage with media content, particularly in\npolitically charged environments."}
{"id": "2504.19254", "pdf": "https://arxiv.org/pdf/2504.19254.pdf", "abs": "https://arxiv.org/abs/2504.19254", "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "UQLM repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs."}
{"id": "2502.13902", "pdf": "https://arxiv.org/pdf/2502.13902.pdf", "abs": "https://arxiv.org/abs/2502.13902", "title": "Grid Labeling: Crowdsourcing Task-Specific Importance from Visualizations", "authors": ["Minsuk Chang", "Yao Wang", "Huichen Will Wang", "Andreas Bulling", "Cindy Xiong Bearfield"], "categories": ["cs.HC"], "comment": "6 pages, 4 figures, Accepted to EuroVis 2025 (Short Paper Track)", "summary": "Knowing where people look in visualizations is key to effective design. Yet,\nexisting research primarily focuses on free-viewing-based saliency models -\nalthough visual attention is inherently task-dependent. Collecting\ntask-relevant importance data remains a resource-intensive challenge. To\naddress this, we introduce Grid Labeling - a novel annotation method for\ncollecting task-specific importance data to enhance saliency prediction models.\nGrid Labeling dynamically segments visualizations into Adaptive Grids, enabling\nefficient, low-effort annotation while adapting to visualization structure. We\nconducted a human subject study comparing Grid Labeling with existing\nannotation methods, ImportAnnots, and BubbleView across multiple metrics.\nResults show that Grid Labeling produces the least noisy data and the highest\ninter-participant agreement with fewer participants while requiring less\nphysical (e.g., clicks/mouse movements) and cognitive effort. An interactive\ndemo is available at https://jangsus1.github.io/Grid-Labeling."}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267.pdf", "abs": "https://arxiv.org/abs/2504.19267", "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment."}
{"id": "2503.08539", "pdf": "https://arxiv.org/pdf/2503.08539.pdf", "abs": "https://arxiv.org/abs/2503.08539", "title": "Desirable Unfamiliarity: Insights from Eye Movements on Engagement and Readability of Dictation Interfaces", "authors": ["Zhaohui Liang", "Yonglin Chen", "Naser Al Madi", "Can Liu"], "categories": ["cs.HC"], "comment": null, "summary": "Dictation interfaces support efficient text input, but the transcribed text\ncan be hard to read. To understand how users read and review dictated text, we\nconducted a controlled eye-tracking experiment with 20 participants to compare\nfive dictation interfaces: PLAIN (real-time transcription), AOC (periodic\ncorrections), RAKE (keyword highlights), GP-TSM (grammar-preserving\nhighlights), and SUMMARY (LLM-generated abstraction summary). The study\nanalyzed participants' gaze patterns during their speech composition and\nreviewing processes. The findings show that during composition, participants\nspent only 7--11% of their time actively reading, and they favored real-time\nfeedback and avoided distracting interface changes. During reviewing, although\nSUMMARY introduced unfamiliar words (requiring longer and more frequent\nfixation), they were easier to read (requiring fewer regressions). Participants\npreferred SUMMARY for the polished text that preserved fidelity to original\nmeanings. RAKE guided the reading of self-produced text better than GP-TSM.\nRAKE guides the reading of self-produced text better than GP-TSM. These\nsurprising findings suggest that dictation interfaces could consider showing\nsummaries or key information to support recall instead of raw transcripts."}
{"id": "2504.19298", "pdf": "https://arxiv.org/pdf/2504.19298.pdf", "abs": "https://arxiv.org/abs/2504.19298", "title": "AndroidGen: Building an Android Language Agent under Data Scarcity", "authors": ["Hanyu Lai", "Junjie Gao", "Xiao Liu", "Yifan Xu", "Shudan Zhang", "Yuxiao Dong", "Jie Tang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have opened up a world of possibilities for various NLP\ntasks, sparking optimism for the future. Despite their potential, LLMs have yet\nto be widely used as agents on real mobile devices. The main challenge is the\nneed for high-quality data sources. Time constraints and labor intensity often\nhinder human annotation. On the other hand, existing LLMs exhibit inadequate\ncompletion rates and need a robust data filtration strategy. Given these\nchallenges, we develop a framework called AndroidGen to enhance the\ncapabilities of LLM-based agents under data scarcity. In addition, we leverage\nAndroidGen to collect trajectories given human tasks and train open-source LLMs\non these trajectories to develop an open-source mobile agent without manually\nlabeled trajectories. We extensively evaluate AndroidGen with AndroidWorld,\nAitW, and various popular applications, demonstrating its improvements and\nrevealing potential areas for future improvement. Code, model, and data are\navailable at https://github.com/THUDM/AndroidGen."}
{"id": "2503.17620", "pdf": "https://arxiv.org/pdf/2503.17620.pdf", "abs": "https://arxiv.org/abs/2503.17620", "title": "A Case Study of Scalable Content Annotation Using Multi-LLM Consensus and Human Review", "authors": ["Mingyue Yuan", "Jieshan Chen", "Zhenchang Xing", "Gelareh Mohammadi", "Aaron Quigley"], "categories": ["cs.HC"], "comment": "7 pages, GenAICHI: CHI 2025 Workshop on Generative AI and HCI", "summary": "Content annotation at scale remains challenging, requiring substantial human\nexpertise and effort. This paper presents a case study in code documentation\nanalysis, where we explore the balance between automation efficiency and\nannotation accuracy. We present MCHR (Multi-LLM Consensus with Human Review), a\nnovel semi-automated framework that enhances annotation scalability through the\nsystematic integration of multiple LLMs and targeted human review. Our\nframework introduces a structured consensus-building mechanism among LLMs and\nan adaptive review protocol that strategically engages human expertise. Through\nour case study, we demonstrate that MCHR reduces annotation time by 32% to 100%\ncompared to manual annotation while maintaining high accuracy (85.5% to 98%)\nacross different difficulty levels, from basic binary classification to\nchallenging open-set scenarios."}
{"id": "2504.19314", "pdf": "https://arxiv.org/pdf/2504.19314.pdf", "abs": "https://arxiv.org/abs/2504.19314", "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese", "authors": ["Peilin Zhou", "Bruce Leon", "Xiang Ying", "Can Zhang", "Yifan Shao", "Qichen Ye", "Dading Chong", "Zhiling Jin", "Chenxuan Xie", "Meng Cao", "Yuxin Gu", "Sixin Hong", "Jing Ren", "Jian Chen", "Chao Liu", "Yining Hua"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH."}
{"id": "2504.08985", "pdf": "https://arxiv.org/pdf/2504.08985.pdf", "abs": "https://arxiv.org/abs/2504.08985", "title": "Learning from Elders: Making an LLM-powered Chatbot for Retirement Communities more Accessible through User-centered Design", "authors": ["Luna Xingyu Li", "Ray-yuan Chung", "Feng Chen", "Wenyu Zeng", "Yein Jeon", "Oleg Zaslavsky"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted as Research talk for Considering Cultural and Linguistic\n  Diversity in AI Applications workshop at CALD-AI@ASIS&T 2025", "summary": "Low technology and eHealth literacy among older adults in retirement\ncommunities hinder engagement with digital tools. To address this, we designed\nan LLM-powered chatbot prototype using a human-centered approach for a local\nretirement community. Through interviews and persona development, we\nprioritized accessibility and dual functionality: simplifying internal\ninformation retrieval and improving technology and eHealth literacy. A pilot\ntrial with residents demonstrated high satisfaction and ease of use, but also\nidentified areas for further improvement. Based on the feedback, we refined the\nchatbot using GPT-3.5 Turbo and Streamlit. The chatbot employs tailored prompt\nengineering to deliver concise responses. Accessible features like adjustable\nfont size, interface theme and personalized follow-up responses were\nimplemented. Future steps include enabling voice-to-text function and\nlongitudinal intervention studies. Together, our results highlight the\npotential of LLM-driven chatbots to empower older adults through accessible,\npersonalized interactions, bridging literacy gaps in retirement communities."}
{"id": "2504.19333", "pdf": "https://arxiv.org/pdf/2504.19333.pdf", "abs": "https://arxiv.org/abs/2504.19333", "title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing", "authors": ["James O' Neill", "Santhosh Subramanian", "Eric Lin", "Vaikkunth Mugunthan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The trend towards large language models (LLMs) for guardrailing against\nundesired behaviors is increasing and has shown promise for censoring user\ninputs. However, increased latency, memory consumption, hosting expenses and\nnon-structured outputs can make their use prohibitive.\n  In this work, we show that task-specific data generation can lead to\nfine-tuned classifiers that significantly outperform current state of the art\n(SoTA) while being orders of magnitude smaller. Secondly, we show that using a\nsingle model, \\texttt{MultiTaskGuard}, that is pretrained on a large\nsynthetically generated dataset with unique task instructions further improves\ngeneralization. Thirdly, our most performant models, \\texttt{UniGuard}, are\nfound using our proposed search-based model merging approach that finds an\noptimal set of parameters to combine single-policy models and multi-policy\nguardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,\nour efficient guardrail classifiers improve over the best performing SoTA\npublicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting\nunsafe and safe behaviors by an average F1 score improvement of \\textbf{29.92}\npoints over Aegis-LlamaGuard and \\textbf{21.62} over \\texttt{gpt-4o},\nrespectively. Lastly, our guardrail synthetic data generation process that uses\ncustom task-specific guardrail poli"}
{"id": "2504.13926", "pdf": "https://arxiv.org/pdf/2504.13926.pdf", "abs": "https://arxiv.org/abs/2504.13926", "title": "A Multi-Layered Research Framework for Human-Centered AI: Defining the Path to Explainability and Trust", "authors": ["Chameera De Silva", "Thilina Halloluwa", "Dhaval Vyas"], "categories": ["cs.HC", "cs.AI"], "comment": "I am requesting this withdrawal because I believe the current version\n  requires significant revisions and restructuring to better reflect the\n  intended research contributions. I plan to substantially improve the work and\n  may resubmit a revised version in the future. Thank you for your\n  understanding and support", "summary": "The integration of Artificial Intelligence (AI) into high-stakes domains such\nas healthcare, finance, and autonomous systems is often constrained by concerns\nover transparency, interpretability, and trust. While Human-Centered AI (HCAI)\nemphasizes alignment with human values, Explainable AI (XAI) enhances\ntransparency by making AI decisions more understandable. However, the lack of a\nunified approach limits AI's effectiveness in critical decision-making\nscenarios. This paper presents a novel three-layered framework that bridges\nHCAI and XAI to establish a structured explainability paradigm. The framework\ncomprises (1) a foundational AI model with built-in explainability mechanisms,\n(2) a human-centered explanation layer that tailors explanations based on\ncognitive load and user expertise, and (3) a dynamic feedback loop that refines\nexplanations through real-time user interaction. The framework is evaluated\nacross healthcare, finance, and software development, demonstrating its\npotential to enhance decision-making, regulatory compliance, and public trust.\nOur findings advance Human-Centered Explainable AI (HCXAI), fostering AI\nsystems that are transparent, adaptable, and ethically aligned."}
{"id": "2504.19339", "pdf": "https://arxiv.org/pdf/2504.19339.pdf", "abs": "https://arxiv.org/abs/2504.19339", "title": "Explanatory Summarization with Discourse-Driven Planning", "authors": ["Dongqi Liu", "Xi Yu", "Vera Demberg", "Mirella Lapata"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by the Transactions of the Association for Computational\n  Linguistics (TACL)", "summary": "Lay summaries for scientific documents typically include explanations to help\nreaders grasp sophisticated concepts or arguments. However, current automatic\nsummarization methods do not explicitly model explanations, which makes it\ndifficult to align the proportion of explanatory content with human-written\nsummaries. In this paper, we present a plan-based approach that leverages\ndiscourse frameworks to organize summary generation and guide explanatory\nsentences by prompting responses to the plan. Specifically, we propose two\ndiscourse-driven planning strategies, where the plan is conditioned as part of\nthe input or part of the output prefix, respectively. Empirical experiments on\nthree lay summarization datasets show that our approach outperforms existing\nstate-of-the-art methods in terms of summary quality, and it enhances model\nrobustness, controllability, and mitigates hallucination."}
{"id": "2504.15647", "pdf": "https://arxiv.org/pdf/2504.15647.pdf", "abs": "https://arxiv.org/abs/2504.15647", "title": "Promoting Real-Time Reflection in Synchronous Communication with Generative AI", "authors": ["Yi Wen", "Meng Xia"], "categories": ["cs.HC"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "Real-time reflection plays a vital role in synchronous communication. It\nenables users to adjust their communication strategies dynamically, thereby\nimproving the effectiveness of their communication. Generative AI holds\nsignificant potential to enhance real-time reflection due to its ability to\ncomprehensively understand the current context and generate personalized and\nnuanced content. However, it is challenging to design the way of interaction\nand information presentation to support the real-time workflow rather than\ndisrupt it. In this position paper, we present a review of existing research on\nsystems designed for reflection in different synchronous communication\nscenarios. Based on that, we discuss design implications on how to design\nhuman-AI interaction to support reflection in real time."}
{"id": "2504.19395", "pdf": "https://arxiv.org/pdf/2504.19395.pdf", "abs": "https://arxiv.org/abs/2504.19395", "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via Substitution Ciphers", "authors": ["Zhouxiang Fang", "Aayush Mishra", "Muhan Gao", "Anqi Liu", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": null, "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs."}
{"id": "2504.17171", "pdf": "https://arxiv.org/pdf/2504.17171.pdf", "abs": "https://arxiv.org/abs/2504.17171", "title": "Augmenting Captions with Emotional Cues: An AR Interface for Real-Time Accessible Communication", "authors": ["Sunday David Ubur"], "categories": ["cs.HC"], "comment": "Minor correction to references for better citation matching", "summary": "This paper introduces an augmented reality (AR) captioning framework designed\nto support Deaf and Hard of Hearing (DHH) learners in STEM classrooms by\nintegrating non-verbal emotional cues into live transcriptions. Unlike\nconventional captioning systems that offer only plain text, our system fuses\nreal-time speech recognition with affective and visual signal interpretation,\nincluding facial movements, gestures, and vocal tone, to produce emotionally\nenriched captions. These enhanced captions are rendered in an AR interface\ndeveloped with Unity and provide contextual annotations such as speaker tone\nmarkers (e.g., \"concerned\") and gesture indicators (e.g., \"nods\"). The system\nleverages live camera and microphone input, processed through AI models to\ndetect multimodal cues. Findings from preliminary evaluations suggest that this\nAR-based captioning approach significantly enhances comprehension and reduces\ncognitive effort compared to standard captions. Our work emphasizes the\npotential of immersive environments for inclusive, emotion-aware educational\naccessibility."}
{"id": "2504.19406", "pdf": "https://arxiv.org/pdf/2504.19406.pdf", "abs": "https://arxiv.org/abs/2504.19406", "title": "Context Selection and Rewriting for Video-based EducationalQuestion Generation", "authors": ["Mengxia Yu", "Bang Nguyen", "Olivia Zino", "Meng Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Educational question generation (EQG) is a crucial component of intelligent\neducational systems, significantly aiding self-assessment, active learning, and\npersonalized education. While EQG systems have emerged, existing datasets\ntypically rely on predefined, carefully edited texts, failing to represent\nreal-world classroom content, including lecture speech with a set of\ncomplementary slides. To bridge this gap, we collect a dataset of educational\nquestions based on lectures from real-world classrooms. On this realistic\ndataset, we find that current methods for EQG struggle with accurately\ngenerating questions from educational videos, particularly in aligning with\nspecific timestamps and target answers. Common challenges include selecting\ninformative contexts from extensive transcripts and ensuring generated\nquestions meaningfully incorporate the target answer. To address the\nchallenges, we introduce a novel framework utilizing large language models for\ndynamically selecting and rewriting contexts based on target timestamps and\nanswers. First, our framework selects contexts from both lecture transcripts\nand video keyframes based on answer relevance and temporal proximity. Then, we\nintegrate the contexts selected from both modalities and rewrite them into\nanswer-containing knowledge statements, to enhance the logical connection\nbetween the contexts and the desired answer. This approach significantly\nimproves the quality and relevance of the generated questions. Our dataset and\ncode are released in https://github.com/mengxiayu/COSER."}
{"id": "2406.14856", "pdf": "https://arxiv.org/pdf/2406.14856.pdf", "abs": "https://arxiv.org/abs/2406.14856", "title": "Accessible, At-Home Detection of Parkinson's Disease via Multi-task Video Analysis", "authors": ["Md Saiful Islam", "Tariq Adnan", "Jan Freyberg", "Sangwu Lee", "Abdelrahman Abdelkader", "Meghan Pawlik", "Cathe Schwartz", "Karen Jaffe", "Ruth B. Schneider", "E Ray Dorsey", "Ehsan Hoque"], "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Limited accessibility to neurological care leads to underdiagnosed\nParkinson's Disease (PD), preventing early intervention. Existing AI-based PD\ndetection methods primarily focus on unimodal analysis of motor or speech\ntasks, overlooking the multifaceted nature of the disease. To address this, we\nintroduce a large-scale, multi-task video dataset consisting of 1102 sessions\n(each containing videos of finger tapping, facial expression, and speech tasks\ncaptured via webcam) from 845 participants (272 with PD). We propose a novel\nUncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal\ndata to enhance diagnostic accuracy. UFNet employs independent task-specific\nnetworks, trained with Monte Carlo Dropout for uncertainty quantification,\nfollowed by self-attended fusion of features, with attention weights\ndynamically adjusted based on task-specific uncertainties. To ensure\npatient-centered evaluation, the participants were randomly split into three\nsets: 60% for training, 20% for model selection, and 20% for final performance\nevaluation. UFNet significantly outperformed single-task models in terms of\naccuracy, area under the ROC curve (AUROC), and sensitivity while maintaining\nnon-inferior specificity. Withholding uncertain predictions further boosted the\nperformance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9%\nsensitivity, and 92.6+-0.3% specificity, at the expense of not being able to\npredict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further\nanalysis suggests that the trained model does not exhibit any detectable bias\nacross sex and ethnic subgroups and is most effective for individuals aged\nbetween 50 and 80. Requiring only a webcam and microphone, our approach\nfacilitates accessible home-based PD screening, especially in regions with\nlimited healthcare resources."}
{"id": "2504.19413", "pdf": "https://arxiv.org/pdf/2504.19413.pdf", "abs": "https://arxiv.org/abs/2504.19413", "title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory", "authors": ["Prateek Chhikara", "Dev Khant", "Saket Aryan", "Taranjeet Singh", "Deshraj Yadav"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable prowess in\ngenerating contextually coherent responses, yet their fixed context windows\npose fundamental challenges for maintaining consistency over prolonged\nmulti-session dialogues. We introduce Mem0, a scalable memory-centric\narchitecture that addresses this issue by dynamically extracting,\nconsolidating, and retrieving salient information from ongoing conversations.\nBuilding on this foundation, we further propose an enhanced variant that\nleverages graph-based memory representations to capture complex relational\nstructures among conversational elements. Through comprehensive evaluations on\nLOCOMO benchmark, we systematically compare our approaches against six baseline\ncategories: (i) established memory-augmented systems, (ii) retrieval-augmented\ngeneration (RAG) with varying chunk sizes and k-values, (iii) a full-context\napproach that processes the entire conversation history, (iv) an open-source\nmemory solution, (v) a proprietary model system, and (vi) a dedicated memory\nmanagement platform. Empirical results show that our methods consistently\noutperform all existing memory systems across four question categories:\nsingle-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%\nrelative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with\ngraph memory achieves around 2% higher overall score than the base\nconfiguration. Beyond accuracy gains, we also markedly reduce computational\noverhead compared to full-context method. In particular, Mem0 attains a 91%\nlower p95 latency and saves more than 90% token cost, offering a compelling\nbalance between advanced reasoning capabilities and practical deployment\nconstraints. Our findings highlight critical role of structured, persistent\nmemory mechanisms for long-term conversational coherence, paving the way for\nmore reliable and efficient LLM-driven AI agents."}
{"id": "2407.10652", "pdf": "https://arxiv.org/pdf/2407.10652.pdf", "abs": "https://arxiv.org/abs/2407.10652", "title": "Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews", "authors": ["Lucas Joos", "Daniel A. Keim", "Maximilian T. Fischer"], "categories": ["cs.LG", "cs.DL", "cs.HC", "H.5.2"], "comment": "6 pages, 5 figures, 1 table", "summary": "Systematic literature reviews (SLRs) are essential but labor-intensive due to\nhigh publication volumes and inefficient keyword-based filtering. To streamline\nthis process, we evaluate Large Language Models (LLMs) for enhancing efficiency\nand accuracy in corpus filtration while minimizing manual effort. Our\nopen-source tool LLMSurver presents a visual interface to utilize LLMs for\nliterature filtration, evaluate the results, and refine queries in an\ninteractive way. We assess the real-world performance of our approach in\nfiltering over 8.3k articles during a recent survey construction, comparing\nresults with human efforts. The findings show that recent LLM models can reduce\nfiltering time from weeks to minutes. A consensus scheme ensures recall rates\n>98.8%, surpassing typical human error thresholds and improving selection\naccuracy. This work advances literature review methodologies and highlights the\npotential of responsible human-AI collaboration in academic research."}
{"id": "2504.19436", "pdf": "https://arxiv.org/pdf/2504.19436.pdf", "abs": "https://arxiv.org/abs/2504.19436", "title": "Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models", "authors": ["Jacky He", "Guiran Liu", "Binrong Zhu", "Hanlu Zhang", "Hongye Zheng", "Xiaokai Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper focuses on the dynamic optimization of the Retrieval-Augmented\nGeneration (RAG) architecture. It proposes a state-aware dynamic knowledge\nretrieval mechanism to enhance semantic understanding and knowledge scheduling\nefficiency in large language models for open-domain question answering and\ncomplex generation tasks. The method introduces a multi-level perceptive\nretrieval vector construction strategy and a differentiable document matching\npath. These components enable end-to-end joint training and collaborative\noptimization of the retrieval and generation modules. This effectively\naddresses the limitations of static RAG structures in context adaptation and\nknowledge access. Experiments are conducted on the Natural Questions dataset.\nThe proposed structure is thoroughly evaluated across different large models,\nincluding GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments\nfrom multiple perspectives confirm the significant improvements in BLEU and\nROUGE-L scores. The approach also demonstrates stronger robustness and\ngeneration consistency in tasks involving semantic ambiguity and multi-document\nfusion. These results highlight its broad application potential and practical\nvalue in building high-quality language generation systems."}
{"id": "2501.10428", "pdf": "https://arxiv.org/pdf/2501.10428.pdf", "abs": "https://arxiv.org/abs/2501.10428", "title": "Perception-Guided EEG Analysis: A Deep Learning Approach Inspired by Level of Detail (LOD) Theory", "authors": ["BG Tong"], "categories": ["eess.SP", "cs.HC", "cs.LG"], "comment": null, "summary": "Objective: This study explores a novel deep learning approach for EEG\nanalysis and perceptual state guidance, inspired by Level of Detail (LOD)\ntheory. The goal is to improve perceptual state identification accuracy and\nadvance personalized psychological therapy. Methods: Portable EEG devices and\nmusic rhythm signals were used for data collection. LOD theory was applied to\ndynamically adjust EEG signal processing, extracting core perceptual features.\nA Unity-based software system integrated EEG data with audio materials. The\ndeep learning model combined a CNN for feature extraction and classification,\nand a DQN for reinforcement learning to optimize rhythm adjustments. Results:\nThe CNN achieved 94.05% accuracy in perceptual state classification. The DQN\nguided subjects to target states with a 92.45% success rate, averaging 13.2\nrhythm cycles. However, only 50% of users reported psychological alignment with\nthe target state, indicating room for improvement. Discussion: The results\nvalidate the potential of LOD-based EEG biofeedback. Limitations include\ndataset source, label subjectivity, and reward function optimization. Future\nwork will expand to diverse subjects, incorporate varied musical elements, and\nrefine reward functions for better generalization and personalization."}
{"id": "2504.19445", "pdf": "https://arxiv.org/pdf/2504.19445.pdf", "abs": "https://arxiv.org/abs/2504.19445", "title": "Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks", "authors": ["Yi-Long Lu", "Chunhui Zhang", "Wei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in tasks such as\npsychological text analysis and decision-making in automated workflows.\nHowever, their reliability remains a concern due to potential biases inherited\nfrom their training process. In this study, we examine how different response\nformat: binary versus continuous, may systematically influence LLMs' judgments.\nIn a value statement judgments task and a text sentiment analysis task, we\nprompted LLMs to simulate human responses and tested both formats across\nseveral models, including both open-source and commercial models. Our findings\nrevealed a consistent negative bias: LLMs were more likely to deliver\n\"negative\" judgments in binary formats compared to continuous ones. Control\nexperiments further revealed that this pattern holds across both tasks. Our\nresults highlight the importance of considering response format when applying\nLLMs to decision tasks, as small changes in task design can introduce\nsystematic biases."}
{"id": "2502.00177", "pdf": "https://arxiv.org/pdf/2502.00177.pdf", "abs": "https://arxiv.org/abs/2502.00177", "title": "Evaluating Deep Human-in-the-Loop Optimization for Retinal Implants Using Sighted Participants", "authors": ["Eirini Schoinas", "Adyah Rastogi", "Anissa Carter", "Jacob Granley", "Michael Beyeler"], "categories": ["cs.LG", "cs.CV", "cs.HC", "I.2.10"], "comment": null, "summary": "Human-in-the-loop optimization (HILO) is a promising approach for\npersonalizing visual prostheses by iteratively refining stimulus parameters\nbased on user feedback. Previous work demonstrated HILO's efficacy in\nsimulation, but its performance with human participants remains untested. Here\nwe evaluate HILO using sighted participants viewing simulated prosthetic vision\nto assess its ability to optimize stimulation strategies under realistic\nconditions. Participants selected between phosphenes generated by competing\nencoders to iteratively refine a deep stimulus encoder (DSE). We tested HILO in\nthree conditions: standard optimization, threshold misspecifications, and\nout-of-distribution parameter sampling. Participants consistently preferred\nHILO-generated stimuli over both a naive encoder and the DSE alone, with log\nodds favoring HILO across all conditions. We also observed key differences\nbetween human and simulated decision-making, highlighting the importance of\nvalidating optimization strategies with human participants. These findings\nsupport HILO as a viable approach for adapting visual prostheses to\nindividuals. Clinical relevance: Validating HILO with sighted participants\nviewing simulated prosthetic vision is an important step toward personalized\ncalibration of future visual prostheses."}
{"id": "2504.19457", "pdf": "https://arxiv.org/pdf/2504.19457.pdf", "abs": "https://arxiv.org/abs/2504.19457", "title": "Towards Long Context Hallucination Detection", "authors": ["Siyi Liu", "Kishaloy Halder", "Zheng Qi", "Wei Xiao", "Nikolaos Pappas", "Phu Mon Htut", "Neha Anna John", "Yassine Benajiba", "Dan Roth"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. However, they are prone to contextual hallucination, generating\ninformation that is either unsubstantiated or contradictory to the given\ncontext. Although many studies have investigated contextual hallucinations in\nLLMs, addressing them in long-context inputs remains an open problem. In this\nwork, we take an initial step toward solving this problem by constructing a\ndataset specifically designed for long-context hallucination detection.\nFurthermore, we propose a novel architecture that enables pre-trained encoder\nmodels, such as BERT, to process long contexts and effectively detect\ncontextual hallucinations through a decomposition and aggregation mechanism.\nOur experimental results show that the proposed architecture significantly\noutperforms previous models of similar size as well as LLM-based models across\nvarious metrics, while providing substantially faster inference."}
{"id": "2502.02772", "pdf": "https://arxiv.org/pdf/2502.02772.pdf", "abs": "https://arxiv.org/abs/2502.02772", "title": "Cross-modality Force and Language Embeddings for Natural Human-Robot Communication", "authors": ["Ravi Tejwani", "Karl Velazquez", "John Payne", "Paolo Bonato", "Harry Asada"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "Under review in RSS 2025", "summary": "A method for cross-modality embedding of force profile and words is presented\nfor synergistic coordination of verbal and haptic communication. When two\npeople carry a large, heavy object together, they coordinate through verbal\ncommunication about the intended movements and physical forces applied to the\nobject. This natural integration of verbal and physical cues enables effective\ncoordination. Similarly, human-robot interaction could achieve this level of\ncoordination by integrating verbal and haptic communication modalities. This\npaper presents a framework for embedding words and force profiles in a unified\nmanner, so that the two communication modalities can be integrated and\ncoordinated in a way that is effective and synergistic. Here, it will be shown\nthat, although language and physical force profiles are deemed completely\ndifferent, the two can be embedded in a unified latent space and proximity\nbetween the two can be quantified. In this latent space, a force profile and\nwords can a) supplement each other, b) integrate the individual effects, and c)\nsubstitute in an exchangeable manner. First, the need for cross-modality\nembedding is addressed, and the basic architecture and key building block\ntechnologies are presented. Methods for data collection and implementation\nchallenges will be addressed, followed by experimental results and discussions."}
{"id": "2504.19467", "pdf": "https://arxiv.org/pdf/2504.19467.pdf", "abs": "https://arxiv.org/abs/2504.19467", "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text", "authors": ["Jiageng Wu", "Bowen Gu", "Ren Zhou", "Kevin Xie", "Doug Snyder", "Yixing Jiang", "Valentina Carducci", "Richard Wyss", "Rishi J Desai", "Emily Alsentzer", "Leo Anthony Celi", "Adam Rodman", "Sebastian Schneeweiss", "Jonathan H. Chen", "Santiago Romero-Brufau", "Kueiyu Joshua Lin", "Jie Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise for medical applications and\nare evolving rapidly, with new models being released at an accelerated pace.\nHowever, current evaluations of LLMs in clinical contexts remain limited. Most\nexisting benchmarks rely on medical exam-style questions or PubMed-derived\ntext, failing to capture the complexity of real-world electronic health record\n(EHR) data. Others focus narrowly on specific application scenarios, limiting\ntheir generalizability across broader clinical use. To address this gap, we\npresent BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks\nsourced from real-world clinical data sources across nine languages. We\nsystematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,\nGPT-4o, Gemini, and Llama 4) under various inference strategies. With a total\nof 13,572 experiments, our results reveal substantial performance variation\nacross model sizes, languages, natural language processing tasks, and clinical\nspecialties. Notably, we demonstrate that open-source LLMs can achieve\nperformance comparable to proprietary models, while medically fine-tuned LLMs\nbased on older architectures often underperform versus updated general-purpose\nmodels. The BRIDGE and its corresponding leaderboard serve as a foundational\nresource and a unique reference for the development and evaluation of new LLMs\nin real-world clinical text understanding."}
{"id": "2502.13013", "pdf": "https://arxiv.org/pdf/2502.13013.pdf", "abs": "https://arxiv.org/abs/2502.13013", "title": "HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit", "authors": ["Qingwei Ben", "Feiyu Jia", "Jia Zeng", "Junting Dong", "Dahua Lin", "Jiangmiao Pang"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Generalizable humanoid loco-manipulation poses significant challenges,\nrequiring coordinated whole-body control and precise, contact-rich object\nmanipulation. To address this, this paper introduces HOMIE, a semi-autonomous\nteleoperation system that combines a reinforcement learning policy for body\ncontrol mapped to a pedal, an isomorphic exoskeleton arm for arm control, and\nmotion-sensing gloves for hand control, forming a unified cockpit to freely\noperate humanoids and establish a data flywheel. The policy incorporates novel\ndesigns, including an upper-body pose curriculum, a height-tracking reward, and\nsymmetry utilization. These features enable the system to perform walking and\nsquatting to specific heights while seamlessly adapting to arbitrary upper-body\nposes. The exoskeleton, by eliminating the reliance on inverse dynamics,\ndelivers faster and more precise arm control. The gloves utilize Hall sensors\ninstead of servos, allowing even compact devices to achieve 15 or more degrees\nof freedom and freely adapt to any model of dexterous hands. Compared to\nprevious teleoperation systems, HOMIE stands out for its exceptional\nefficiency, completing tasks in half the time; its expanded working range,\nallowing users to freely reach high and low areas as well as interact with any\nobjects; and its affordability, with a price of just $500. The system is fully\nopen-source, demos and code can be found in our https://homietele.github.io/."}
{"id": "2504.19472", "pdf": "https://arxiv.org/pdf/2504.19472.pdf", "abs": "https://arxiv.org/abs/2504.19472", "title": "Conflicts in Texts: Data, Implications and Challenges", "authors": ["Siyi Liu", "Dan Roth"], "categories": ["cs.CL"], "comment": null, "summary": "As NLP models become increasingly integrated into real-world applications, it\nbecomes clear that there is a need to address the fact that models often rely\non and generate conflicting information. Conflicts could reflect the complexity\nof situations, changes that need to be explained and dealt with, difficulties\nin data annotation, and mistakes in generated outputs. In all cases,\ndisregarding the conflicts in data could result in undesired behaviors of\nmodels and undermine NLP models' reliability and trustworthiness. This survey\ncategorizes these conflicts into three key areas: (1) natural texts on the web,\nwhere factual inconsistencies, subjective biases, and multiple perspectives\nintroduce contradictions; (2) human-annotated data, where annotator\ndisagreements, mistakes, and societal biases impact model training; and (3)\nmodel interactions, where hallucinations and knowledge conflicts emerge during\ndeployment. While prior work has addressed some of these conflicts in\nisolation, we unify them under the broader concept of conflicting information,\nanalyze their implications, and discuss mitigation strategies. We highlight key\nchallenges and future directions for developing conflict-aware NLP systems that\ncan reason over and reconcile conflicting information more effectively."}
{"id": "2502.19546", "pdf": "https://arxiv.org/pdf/2502.19546.pdf", "abs": "https://arxiv.org/abs/2502.19546", "title": "Repurposing the scientific literature with vision-language models", "authors": ["Anton Alyakin", "Jaden Stryker", "Daniel Alexander Alber", "Karl L. Sangwon", "Jin Vivian Lee", "Brandon Duderstadt", "Akshay Save", "David Kurland", "Spencer Frome", "Shrutika Singh", "Jeff Zhang", "Eunice Yang", "Ki Yun Park", "Cordelia Orillac", "Aly A. Valliani", "Sean Neifert", "Albert Liu", "Aneek Patel", "Christopher Livia", "Darryl Lau", "Ilya Laufer", "Peter A. Rozman", "Eveline Teresa Hidalgo", "Howard Riina", "Rui Feng", "Todd Hollon", "Yindalon Aphinyanaphongs", "John G. Golfinos", "Laura Snyder", "Eric Leuthardt", "Douglas Kondziolka", "Eric Karl Oermann"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Leading vision-language models (VLMs) are trained on general Internet\ncontent, overlooking scientific journals' rich, domain-specific knowledge.\nTraining on specialty-specific literature could yield high-performance,\ntask-specific tools, enabling generative AI to match generalist models in\nspecialty publishing, educational, and clinical tasks. We created NeuroPubs, a\nmultimodal dataset of 23,000 Neurosurgery Publications articles (134M words,\n78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready\ngraphical abstracts (70% of 100 abstracts) and board-style questions\nindistinguishable from human-written ones (54% of 89,587 questions). We used\nthese questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded,\nrandomized controlled trial, our model demonstrated non-inferiority to then\nstate-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical\nutility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%,\np=0.3797). Our pilot study demonstrates how training generative AI models on\nspecialty-specific journal content - without large-scale internet data -\nresults in high-performance academic and clinical tools, enabling\ndomain-tailored AI across diverse fields."}
{"id": "2504.19556", "pdf": "https://arxiv.org/pdf/2504.19556.pdf", "abs": "https://arxiv.org/abs/2504.19556", "title": "Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment", "authors": ["Kristen Sussman", "Daniel Carter"], "categories": ["cs.CL", "cs.HC", "J.4; K.4.0; I.2.7"], "comment": "5 pages, 3 figures, Companion Proceedings of the ACM Web Conference\n  2025", "summary": "Given the subtle human-like effects of large language models on linguistic\npatterns, this study examines shifts in language over time to detect the impact\nof AI-mediated communication (AI- MC) on social media. We compare a replicated\ndataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the\nsame period in 2024, all of which mention Donald Trump during election periods.\nUsing a combination of Flesch-Kincaid readability and polarity scores, we\nanalyze changes in text complexity and sentiment. Our findings reveal a\nsignificant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift\nfrom predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more\npositive expressions (28.6% to 45.9%). These findings suggest not only an\nincreasing presence of AI in social media communication but also its impact on\nlanguage and emotional expression patterns."}
{"id": "2503.17401", "pdf": "https://arxiv.org/pdf/2503.17401.pdf", "abs": "https://arxiv.org/abs/2503.17401", "title": "AIJIM: A Scalable Model for Real-Time AI in Environmental Journalism", "authors": ["Torsten Tiltack"], "categories": ["cs.CY", "cs.AI", "cs.HC", "68T45", "I.2.10; H.3.5; J.4"], "comment": "22 pages, 10 figures, 5 tables. Keywords: Artificial Intelligence,\n  Environmental Journalism, Real-Time Reporting, Vision Transformers, Image\n  Recognition, Crowdsourced Validation, GPT-4, Automated News Generation, GIS\n  Integration, Data Privacy Compliance, Explainable AI (XAI), AI Ethics,\n  Sustainable Development", "summary": "This paper introduces AIJIM, the Artificial Intelligence Journalism\nIntegration Model -- a novel framework for integrating real-time AI into\nenvironmental journalism. AIJIM combines Vision Transformer-based hazard\ndetection, crowdsourced validation with 252 validators, and automated reporting\nwithin a scalable, modular architecture. A dual-layer explainability approach\nensures ethical transparency through fast CAM-based visual overlays and\noptional LIME-based box-level interpretations. Validated in a 2024 pilot on the\nisland of Mallorca using the NamicGreen platform, AIJIM achieved 85.4\\%\ndetection accuracy and 89.7\\% agreement with expert annotations, while reducing\nreporting latency by 40\\%. Unlike conventional approaches such as Data-Driven\nJournalism or AI Fact-Checking, AIJIM provides a transferable model for\nparticipatory, community-driven environmental reporting, advancing journalism,\nartificial intelligence, and sustainability in alignment with the UN\nSustainable Development Goals and the EU AI Act."}
{"id": "2504.19565", "pdf": "https://arxiv.org/pdf/2504.19565.pdf", "abs": "https://arxiv.org/abs/2504.19565", "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training", "authors": ["Meng Xiao", "Xunxin Cai", "Chengrui Wang", "Yuanchun Zhou"], "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "22 pages, Large Language Model, Agentic AI, Dataset Distillation,\n  Multi-agent Collaboration", "summary": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training."}
{"id": "2503.19225", "pdf": "https://arxiv.org/pdf/2503.19225.pdf", "abs": "https://arxiv.org/abs/2503.19225", "title": "CoinFT: A Coin-Sized, Capacitive 6-Axis Force Torque Sensor for Robotic Applications", "authors": ["Hojung Choi", "Jun En Low", "Tae Myung Huh", "Gabriela A. Uribe", "Seongheon Hong", "Kenneth A. W. Hoffman", "Julia Di", "Tony G. Chen", "Andrew A. Stanley", "Mark R. Cutkosky"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "We introduce CoinFT, a capacitive 6-axis force/torque (F/T) sensor that is\ncompact, light, low-cost, and robust with an average mean-squared error of\n0.11N for force and 0.84mNm for moment when the input ranges from 0~10N and\n0~4N in normal and shear directions, respectively. CoinFT is a stack of two\nrigid PCBs with comb-shaped electrodes connected by an array of silicone rubber\npillars. The microcontroller interrogates the electrodes in different subsets\nin order to enhance sensitivity for measuring 6-axis F/T. The combination of\ndesirable features of CoinFT enables various contact-rich robot interactions at\na scale, across different embodiment domains including drones, robot\nend-effectors, and wearable haptic devices. We demonstrate the utility of\nCoinFT on drones by performing an attitude-based force control to perform tasks\nthat require careful contact force modulation. The design, fabrication, and\nfirmware of CoinFT are open-sourced at\nhttps://hojung-choi.github.io/coinft.github.io/."}
{"id": "2504.19590", "pdf": "https://arxiv.org/pdf/2504.19590.pdf", "abs": "https://arxiv.org/abs/2504.19590", "title": "Arabic Metaphor Sentiment Classification Using Semantic Information", "authors": ["Israa Alsiyat"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1]\nusing newly designed automatic tools for sentiment classification for AMC based\non semantic tags. The tool incorporates semantic emotional tags for sentiment\nclassification. I evaluate the tool using standard methods, which are F-score,\nrecall, and precision. The method is to show the impact of Arabic online\nmetaphors on sentiment through the newly designed tools. To the best of our\nknowledge, this is the first approach to conduct sentiment classification for\nArabic metaphors using semantic tags to find the impact of the metaphor."}
{"id": "2504.16419", "pdf": "https://arxiv.org/pdf/2504.16419.pdf", "abs": "https://arxiv.org/abs/2504.16419", "title": "PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels", "authors": ["Qi Yang", "Weichen Bi", "Haiyang Shen", "Yaoqi Guo", "Yun Ma"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Graphical User Interface (GUI) datasets are crucial for various downstream\ntasks. However, GUI datasets often generate annotation information through\nautomatic labeling, which commonly results in inaccurate GUI element BBox\nannotations, including missing, duplicate, or meaningless BBoxes. These issues\ncan degrade the performance of models trained on these datasets, limiting their\neffectiveness in real-world applications. Additionally, existing GUI datasets\nonly provide BBox annotations visually, which restricts the development of\nvisually related GUI downstream tasks. To address these issues, we introduce\nPixelWeb, a large-scale GUI dataset containing over 100,000 annotated web\npages. PixelWeb is constructed using a novel automatic annotation approach that\nintegrates visual feature extraction and Document Object Model (DOM) structure\nanalysis through two core modules: channel derivation and layer analysis.\nChannel derivation ensures accurate localization of GUI elements in cases of\nocclusion and overlapping elements by extracting BGRA four-channel bitmap\nannotations. Layer analysis uses the DOM to determine the visibility and\nstacking order of elements, providing precise BBox annotations. Additionally,\nPixelWeb includes comprehensive metadata such as element images, contours, and\nmask annotations. Manual verification by three independent annotators confirms\nthe high quality and accuracy of PixelWeb annotations. Experimental results on\nGUI element detection tasks show that PixelWeb achieves performance on the\nmAP95 metric that is 3-7 times better than existing datasets. We believe that\nPixelWeb has great potential for performance improvement in downstream tasks\nsuch as GUI generation and automated user interaction."}
{"id": "2504.19606", "pdf": "https://arxiv.org/pdf/2504.19606.pdf", "abs": "https://arxiv.org/abs/2504.19606", "title": "Coreference Resolution for Vietnamese Narrative Texts", "authors": ["Hieu-Dai Tran", "Duc-Vu Nguyen", "Ngan Luu-Thuy Nguyen"], "categories": ["cs.CL"], "comment": "Accepted at PACLIC 2024", "summary": "Coreference resolution is a vital task in natural language processing (NLP)\nthat involves identifying and linking different expressions in a text that\nrefer to the same entity. This task is particularly challenging for Vietnamese,\na low-resource language with limited annotated datasets. To address these\nchallenges, we developed a comprehensive annotated dataset using narrative\ntexts from VnExpress, a widely-read Vietnamese online news platform. We\nestablished detailed guidelines for annotating entities, focusing on ensuring\nconsistency and accuracy. Additionally, we evaluated the performance of large\nlanguage models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.\nOur results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in\nterms of both accuracy and response consistency, making it a more reliable tool\nfor coreference resolution in Vietnamese."}
{"id": "2504.19627", "pdf": "https://arxiv.org/pdf/2504.19627.pdf", "abs": "https://arxiv.org/abs/2504.19627", "title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "authors": ["Run Luo", "Renke Shan", "Longze Chen", "Ziqiang Liu", "Lu Wang", "Min Yang", "Xiaobo Xia"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "VCM", "summary": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM."}
{"id": "2504.19645", "pdf": "https://arxiv.org/pdf/2504.19645.pdf", "abs": "https://arxiv.org/abs/2504.19645", "title": "A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks", "authors": ["Shadan Shukr Sabr", "Nazira Sabr Mustafa", "Talar Sabah Omar", "Salah Hwayyiz Rasool", "Nawzad Anwer Omer", "Darya Sabir Hamad", "Hemin Abdulhameed Shams", "Omer Mahmood Kareem", "Rozhan Noori Abdullah", "Khabat Atar Abdullah", "Mahabad Azad Mohammad", "Haneen Al-Raghefy", "Safar M. Asaad", "Sara Jamal Mohammed", "Twana Saeed Ali", "Fazil Shawrow", "Halgurd S. Maghdid"], "categories": ["cs.CL", "cs.AI", "K.5; K.7; J.7"], "comment": "25 pages, 4 figures, 2 tables", "summary": "- The field of natural language processing (NLP) has dramatically expanded\nwithin the last decade. Many human-being applications are conducted daily via\nNLP tasks, starting from machine translation, speech recognition, text\ngeneration and recommendations, Part-of-Speech tagging (POS), and Named-Entity\nRecognition (NER). However, low-resourced languages, such as the\nCentral-Kurdish language (CKL), mainly remain unexamined due to shortage of\nnecessary resources to support their development. The POS tagging task is the\nbase of other NLP tasks; for example, the POS tag set has been used to\nstandardized languages to provide the relationship between words among the\nsentences, followed by machine translation and text recommendation.\nSpecifically, for the CKL, most of the utilized or provided POS tagsets are\nneither standardized nor comprehensive. To this end, this study presented an\naccurate and comprehensive POS tagset for the CKL to provide better performance\nof the Kurdish NLP tasks. The article also collected most of the POS tags from\ndifferent studies as well as from Kurdish linguistic experts to standardized\npart-of-speech tags. The proposed POS tagset is designed to annotate a large\nCKL corpus and support Kurdish NLP tasks. The initial investigations of this\nstudy via comparison with the Universal Dependencies framework for standard\nlanguages, show that the proposed POS tagset can streamline or correct\nsentences more accurately for Kurdish NLP tasks."}
{"id": "2504.19669", "pdf": "https://arxiv.org/pdf/2504.19669.pdf", "abs": "https://arxiv.org/abs/2504.19669", "title": "Multimodal Conditioned Diffusive Time Series Forecasting", "authors": ["Chen Su", "Yuanhe Tian", "Yan Song"], "categories": ["cs.CL"], "comment": null, "summary": "Diffusion models achieve remarkable success in processing images and text,\nand have been extended to special domains such as time series forecasting\n(TSF). Existing diffusion-based approaches for TSF primarily focus on modeling\nsingle-modality numerical sequences, overlooking the rich multimodal\ninformation in time series data. To effectively leverage such information for\nprediction, we propose a multimodal conditioned diffusion model for TSF,\nnamely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for\ntime series modeling, especially for forecasting. Specifically, Timestamps are\ncombined with time series to establish temporal and semantic correlations among\ndifferent data points when aggregating information along the temporal\ndimension. Texts serve as supplementary descriptions of time series' history,\nand adaptively aligned with data points as well as dynamically controlled in a\nclassifier-free manner. Extensive experiments on real-world benchmark datasets\nacross eight domains demonstrate that the proposed MCD-TSF model achieves\nstate-of-the-art performance."}
{"id": "2504.19675", "pdf": "https://arxiv.org/pdf/2504.19675.pdf", "abs": "https://arxiv.org/abs/2504.19675", "title": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs", "authors": ["Osma Suominen", "Juho Inkinen", "Mona Lehtinen"], "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG", "I.2.7"], "comment": "6 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts."}
{"id": "2504.19720", "pdf": "https://arxiv.org/pdf/2504.19720.pdf", "abs": "https://arxiv.org/abs/2504.19720", "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving", "authors": ["Ranran Zhen", "Juntao Li", "Yixin Ji", "Zhenlin Yang", "Tong Liu", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": "work in progress;11 pages of main paper with 7 main figures, overall\n  20 pages", "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving."}
{"id": "2504.19734", "pdf": "https://arxiv.org/pdf/2504.19734.pdf", "abs": "https://arxiv.org/abs/2504.19734", "title": "LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding", "authors": ["Ying Na", "Shihui Feng"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Dialogue data has been a key source for understanding learning processes,\noffering critical insights into how students engage in collaborative\ndiscussions and how these interactions shape their knowledge construction. The\nadvent of Large Language Models (LLMs) has introduced promising opportunities\nfor advancing qualitative research, particularly in the automated coding of\ndialogue data. However, the inherent contextual complexity of dialogue presents\nunique challenges for these models, especially in understanding and\ninterpreting complex contextual information. This study addresses these\nchallenges by developing a novel LLM-assisted automated coding approach for\ndialogue data. The novelty of our proposed framework is threefold: 1) We\npredict the code for an utterance based on dialogue-specific characteristics --\ncommunicative acts and communicative events -- using separate prompts following\nthe role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs\nincluding GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We\nleveraged the interrelation between events and acts to implement consistency\nchecking using GPT-4o. In particular, our contextual consistency checking\nprovided a substantial accuracy improvement. We also found the accuracy of act\npredictions was consistently higher than that of event predictions. This study\ncontributes a new methodological framework for enhancing the precision of\nautomated coding of dialogue data as well as offers a scalable solution for\naddressing the contextual challenges inherent in dialogue analysis."}
{"id": "2504.19759", "pdf": "https://arxiv.org/pdf/2504.19759.pdf", "abs": "https://arxiv.org/abs/2504.19759", "title": "Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs", "authors": ["Huichi Zhou", "Zehao Xu", "Munan Zhao", "Kaihong Li", "Yiqiang Li", "Hongtao Wang"], "categories": ["cs.CL"], "comment": "5 pages, 2 figures", "summary": "In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)\nto evaluate the moral reasoning abilities of large language models (LLMs)\nacross five typologically diverse languages and three levels of contextual\ncomplexity: sentence, paragraph, and document. Our results show moral reasoning\nperformance degrades with increasing context complexity, particularly for\nlow-resource languages such as Vietnamese. We further fine-tune the open-source\nLLaMA-3-8B model using curated monolingual data for alignment and poisoning.\nSurprisingly, low-resource languages have a stronger impact on multilingual\nreasoning than high-resource ones, highlighting their critical role in\nmultilingual NLP."}
{"id": "2504.19811", "pdf": "https://arxiv.org/pdf/2504.19811.pdf", "abs": "https://arxiv.org/abs/2504.19811", "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance", "authors": ["Takuya Tamura", "Taro Yano", "Masafumi Enomoto", "Masafumi Oyamada"], "categories": ["cs.CL"], "comment": null, "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment."}
{"id": "2504.19850", "pdf": "https://arxiv.org/pdf/2504.19850.pdf", "abs": "https://arxiv.org/abs/2504.19850", "title": "To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels", "authors": ["Kyo Gerrits", "Ana Guerberof-Arenas"], "categories": ["cs.CL"], "comment": "This paper has been accepted to the MT Summit 2025 to be held in\n  Geneva on June 23-27 2025", "summary": "This article presents the results of a pilot study involving the reception of\na fictional short story translated from English into Dutch under four\nconditions: machine translation (MT), post-editing (PE), human translation (HT)\nand original source text (ST). The aim is to understand how creativity and\nerrors in different translation modalities affect readers, specifically\nregarding cognitive load. Eight participants filled in a questionnaire, read a\nstory using an eye-tracker, and conducted a retrospective think-aloud (RTA)\ninterview. The results show that units of creative potential (UCP) increase\ncognitive load and that this effect is highest for HT and lowest for MT; no\neffect of error was observed. Triangulating the data with RTAs leads us to\nhypothesize that the higher cognitive load in UCPs is linked to increases in\nreader enjoyment and immersion. The effect of translation creativity on\ncognitive load in different translation modalities at word-level is novel and\nopens up new avenues for further research. All the code and data are available\nat https://github.com/INCREC/Pilot_to_MT_or_not_to_MT"}
{"id": "2504.19856", "pdf": "https://arxiv.org/pdf/2504.19856.pdf", "abs": "https://arxiv.org/abs/2504.19856", "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language", "authors": ["Anastasia Zhukova", "Christian E. Matt", "Terry Ruas", "Bela Gipp"], "categories": ["cs.CL"], "comment": null, "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique\nthat further trains a language model (LM) on its pretraining task, e.g.,\nlanguage masking. Although popular, it requires a significant corpus of\ndomain-related data, which is difficult to obtain for specific domains in\nlanguages other than English, such as the process industry in the German\nlanguage. This paper introduces an efficient approach called ICL-augmented\npretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest\nneighbors (kNN) to augment target data with domain-related and in-domain texts,\nsignificantly reducing GPU time while maintaining strong model performance. Our\nresults show that this approach performs better than traditional DAPT by 3.5 of\nthe average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times\nless computing time, providing a cost-effective solution for industries with\nlimited computational capacity. The findings highlight the broader\napplicability of this framework to other low-resource industries, making\nNLP-based solutions more accessible and feasible in production environments."}
{"id": "2504.19867", "pdf": "https://arxiv.org/pdf/2504.19867.pdf", "abs": "https://arxiv.org/abs/2504.19867", "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage", "authors": ["Ke Hong", "Lufang Chen", "Zhong Wang", "Xiuhong Li", "Qiuli Mao", "Jianping Ma", "Chao Xiong", "Guanyu Wu", "Buhe Han", "Guohao Dai", "Yun Liang", "Yu Wang"], "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "18 pages, 16 figures", "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."}
{"id": "2504.19898", "pdf": "https://arxiv.org/pdf/2504.19898.pdf", "abs": "https://arxiv.org/abs/2504.19898", "title": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets", "authors": ["Mingqian He", "Fei Zhao", "Chonggang Lu", "Ziyan Liu", "Yue Wang", "Haofu Qian"], "categories": ["cs.CL"], "comment": null, "summary": "As a fundamental task in machine learning, text classification plays a\ncrucial role in many areas. With the rapid scaling of Large Language Models\n(LLMs), particularly through reinforcement learning (RL), there is a growing\nneed for more capable discriminators. Consequently, advances in classification\nare becoming increasingly vital for enhancing the overall capabilities of LLMs.\nTraditional discriminative methods map text to labels but overlook LLMs'\nintrinsic generative strengths. Generative classification addresses this by\nprompting the model to directly output labels. However, existing studies still\nrely on simple SFT alone, seldom probing the interplay between training and\ninference prompts, and no work has systematically leveraged RL for generative\ntext classifiers and unified SFT, RL, and inference-time prompting in one\nframework. We bridge this gap with GenCLS++, a framework that jointly optimizes\nSFT and RL while systematically exploring five high-level strategy\ndimensions-in-context learning variants, category definitions, explicit\nuncertainty labels, semantically irrelevant numeric labels, and\nperplexity-based decoding-during both training and inference. After an SFT\n\"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable\nextra gains. Across seven datasets, GenCLS++ achieves an average accuracy\nimprovement of 3.46% relative to the naive SFT baseline; on public datasets,\nthis improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that\nbenefit from explicit thinking processes, we find that classification tasks\nperform better without such reasoning steps. These insights into the role of\nexplicit reasoning provide valuable guidance for future LLM applications."}
{"id": "2504.19940", "pdf": "https://arxiv.org/pdf/2504.19940.pdf", "abs": "https://arxiv.org/abs/2504.19940", "title": "Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking", "authors": ["Luigia Costabile", "Gian Marco Orlando", "Valerio La Gatta", "Vincenzo Moscato"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "The growing spread of online misinformation has created an urgent need for\nscalable, reliable fact-checking solutions. Crowdsourced fact-checking - where\nnon-experts evaluate claim veracity - offers a cost-effective alternative to\nexpert verification, despite concerns about variability in quality and bias.\nEncouraged by promising results in certain contexts, major platforms such as X\n(formerly Twitter), Facebook, and Instagram have begun shifting from\ncentralized moderation to decentralized, crowd-based approaches.\n  In parallel, advances in Large Language Models (LLMs) have shown strong\nperformance across core fact-checking tasks, including claim detection and\nevidence evaluation. However, their potential role in crowdsourced workflows\nremains unexplored. This paper investigates whether LLM-powered generative\nagents - autonomous entities that emulate human behavior and decision-making -\ncan meaningfully contribute to fact-checking tasks traditionally reserved for\nhuman crowds. Using the protocol of La Barbera et al. (2024), we simulate\ncrowds of generative agents with diverse demographic and ideological profiles.\nAgents retrieve evidence, assess claims along multiple quality dimensions, and\nissue final veracity judgments.\n  Our results show that agent crowds outperform human crowds in truthfulness\nclassification, exhibit higher internal consistency, and show reduced\nsusceptibility to social and cognitive biases. Compared to humans, agents rely\nmore systematically on informative criteria such as Accuracy, Precision, and\nInformativeness, suggesting a more structured decision-making process. Overall,\nour findings highlight the potential of generative agents as scalable,\nconsistent, and less biased contributors to crowd-based fact-checking systems."}
{"id": "2504.19982", "pdf": "https://arxiv.org/pdf/2504.19982.pdf", "abs": "https://arxiv.org/abs/2504.19982", "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons", "authors": ["Emre Can Acikgoz", "Carl Guo", "Suvodip Dey", "Akul Datta", "Takyoung Kim", "Gokhan Tur", "Dilek Hakkani-Tür"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research."}
{"id": "2504.20000", "pdf": "https://arxiv.org/pdf/2504.20000.pdf", "abs": "https://arxiv.org/abs/2504.20000", "title": "Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom", "authors": ["Rishika Sen", "Sujoy Roychowdhury", "Sumit Soman", "H. G. Ranjani", "Srikhetra Mohanty"], "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "comment": "10 pages, 4 figures, 3 tables", "summary": "Knowledge Distillation (KD) is one of the approaches to reduce the size of\nLarge Language Models (LLMs). A LLM with smaller number of model parameters\n(student) is trained to mimic the performance of a LLM of a larger size\n(teacher model) on a specific task. For domain-specific tasks, it is not clear\nif teacher or student model, or both, must be considered for domain adaptation.\nIn this work, we study this problem from perspective of telecom domain\nQuestion-Answering (QA) task. We systematically experiment with Supervised\nFine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to\nKD. We design experiments to study the impact of vocabulary (same and\ndifferent) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the\ndistilled model. Multi-faceted evaluation of the distillation using 14\ndifferent metrics (N-gram, embedding and LLM-based metrics) is considered.\nExperimental results show that SFT of teacher improves performance of distilled\nmodel when both models have same vocabulary, irrespective of algorithm and\nmetrics. Overall, SFT of both teacher and student results in better performance\nacross all metrics, although the statistical significance of the same depends\non the vocabulary of the teacher models."}
{"id": "2504.20013", "pdf": "https://arxiv.org/pdf/2504.20013.pdf", "abs": "https://arxiv.org/abs/2504.20013", "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation", "authors": ["Beizhe Hu", "Qiang Sheng", "Juan Cao", "Yang Li", "Danding Wang"], "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": "ACM SIGIR 2025 Full Paper", "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems."}
{"id": "2504.20022", "pdf": "https://arxiv.org/pdf/2504.20022.pdf", "abs": "https://arxiv.org/abs/2504.20022", "title": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages", "authors": ["Pritika Rohera", "Chaitrali Ginimav", "Gayatri Sawant", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs."}
{"id": "2504.20039", "pdf": "https://arxiv.org/pdf/2504.20039.pdf", "abs": "https://arxiv.org/abs/2504.20039", "title": "AutoJudge: Judge Decoding Without Manual Annotation", "authors": ["Roman Garipov", "Fedor Velikonivtsev", "Ruslan Svirschevski", "Vage Egiazarian", "Max Ryabinin"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint, Work in progress", "summary": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks."}
{"id": "2504.18596", "pdf": "https://arxiv.org/pdf/2504.18596.pdf", "abs": "https://arxiv.org/abs/2504.18596", "title": "Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines", "authors": ["Anantha Sharma", "Swetha Devabhaktuni", "Eklove Mohan"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "math.PR"], "comment": "18 pages, 8 figures, 5 tables", "summary": "This paper explores the strategic use of modern synthetic data generation and\nadvanced data perturbation techniques to enhance security, maintain analytical\nutility, and improve operational efficiency when managing large datasets, with\na particular focus on the Banking, Financial Services, and Insurance (BFSI)\nsector. We contrast these advanced methods encompassing generative models like\nGANs, sophisticated context-aware PII transformation, configurable statistical\nperturbation, and differential privacy with traditional anonymization\napproaches.\n  The goal is to create realistic, privacy-preserving datasets that retain high\nutility for complex machine learning tasks and analytics, a critical need in\nthe data-sensitive industries like BFSI, Healthcare, Retail, and\nTelecommunications. We discuss how these modern techniques potentially offer\nsignificant improvements in balancing privacy preservation while maintaining\ndata utility compared to older methods. Furthermore, we examine the potential\nfor operational gains, such as reduced overhead and accelerated analytics, by\nusing these privacy-enhanced datasets. We also explore key use cases where\nthese methods can mitigate regulatory risks and enable scalable, data-driven\ninnovation without compromising sensitive customer information."}
{"id": "2504.18748", "pdf": "https://arxiv.org/pdf/2504.18748.pdf", "abs": "https://arxiv.org/abs/2504.18748", "title": "Generative Product Recommendations for Implicit Superlative Queries", "authors": ["Kaustubh D. Dhole", "Nikhita Vedula", "Saar Kuzi", "Giuseppe Castellucci", "Eugene Agichtein", "Shervin Malmasi"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "In Recommender Systems, users often seek the best products through indirect,\nvague, or under-specified queries, such as \"best shoes for trail running\". Such\nqueries, also referred to as implicit superlative queries, pose a significant\nchallenge for standard retrieval and ranking systems as they lack an explicit\nmention of attributes and require identifying and reasoning over complex\nfactors. We investigate how Large Language Models (LLMs) can generate implicit\nattributes for ranking as well as reason over them to improve product\nrecommendations for such queries. As a first step, we propose a novel\nfour-point schema for annotating the best product candidates for superlative\nqueries called SUPERB, paired with LLM-based product annotations. We then\nempirically evaluate several existing retrieval and ranking approaches on our\nnew dataset, providing insights and discussing their integration into\nreal-world e-commerce production systems."}
{"id": "2504.18919", "pdf": "https://arxiv.org/pdf/2504.18919.pdf", "abs": "https://arxiv.org/abs/2504.18919", "title": "Clinical knowledge in LLMs does not translate to human interactions", "authors": ["Andrew M. Bean", "Rebecca Payne", "Guy Parsons", "Hannah Rose Kirk", "Juan Ciro", "Rafael Mosquera", "Sara Hincapié Monsalve", "Aruna S. Ekanayaka", "Lionel Tarassenko", "Luc Rocher", "Adam Mahdi"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "52 pages, 4 figures", "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare."}
{"id": "2504.18988", "pdf": "https://arxiv.org/pdf/2504.18988.pdf", "abs": "https://arxiv.org/abs/2504.18988", "title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "authors": ["Saramsh Gautam", "Mahmood Jasim"], "categories": ["cs.HC", "cs.CL", "H.5.3"], "comment": "19 pages, 4 figures. Multimodal system design and evaluation study", "summary": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research."}
{"id": "2504.19056", "pdf": "https://arxiv.org/pdf/2504.19056.pdf", "abs": "https://arxiv.org/abs/2504.19056", "title": "Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions", "authors": ["Mohammad Mahdi Abootorabi", "Omid Ghahroodi", "Pardis Sadat Zahraei", "Hossein Behzadasl", "Alireza Mirrokni", "Mobina Salimipanah", "Arash Rasouli", "Bahar Behzadipour", "Sara Azarnoush", "Benyamin Maleki", "Erfan Sadraiye", "Kiarash Kiani Feriz", "Mahdi Teymouri Nahad", "Ali Moghadasi", "Abolfazl Eshagh Abianeh", "Nizi Nazar", "Hamid R. Rabiee", "Mahdieh Soleymani Baghshah", "Meisam Ahmadi", "Ehsaneddin Asgari"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "50 main pages, 30 pages appendix, 21 figures, 8 tables, GitHub\n  Repository:\n  https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey", "summary": "Generative AI is reshaping art, gaming, and most notably animation. Recent\nbreakthroughs in foundation and diffusion models have reduced the time and cost\nof producing animated content. Characters are central animation components,\ninvolving motion, emotions, gestures, and facial expressions. The pace and\nbreadth of advances in recent months make it difficult to maintain a coherent\nview of the field, motivating the need for an integrative review. Unlike\nearlier overviews that treat avatars, gestures, or facial animation in\nisolation, this survey offers a single, comprehensive perspective on all the\nmain generative AI applications for character animation. We begin by examining\nthe state-of-the-art in facial animation, expression rendering, image\nsynthesis, avatar creation, gesture modeling, motion synthesis, object\ngeneration, and texture synthesis. We highlight leading research, practical\ndeployments, commonly used datasets, and emerging trends for each area. To\nsupport newcomers, we also provide a comprehensive background section that\nintroduces foundational models and evaluation metrics, equipping readers with\nthe knowledge needed to enter the field. We discuss open challenges and map\nfuture research directions, providing a roadmap to advance AI-driven\ncharacter-animation technologies. This survey is intended as a resource for\nresearchers and developers entering the field of generative AI animation or\nadjacent fields. Resources are available at:\nhttps://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey."}
{"id": "2504.19062", "pdf": "https://arxiv.org/pdf/2504.19062.pdf", "abs": "https://arxiv.org/abs/2504.19062", "title": "Versatile Framework for Song Generation with Prompt-based Control", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Jingyu Lu", "Rongjie Huang", "Ruiyuan Zhang", "Zhiqing Hong", "Ziyue Jiang", "Zhou Zhao"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Song generation focuses on producing controllable high-quality songs based on\nvarious prompts. However, existing methods struggle to generate vocals and\naccompaniments with prompt-based control and proper alignment. Additionally,\nthey fall short in supporting various tasks. To address these challenges, we\nintroduce VersBand, a multi-task song generation framework for synthesizing\nhigh-quality, aligned songs with prompt-based control. VersBand comprises these\nprimary models: 1) VocalBand, a decoupled model, leverages the flow-matching\nmethod for generating singing styles, pitches, and mel-spectrograms, allowing\nfast, high-quality vocal generation with style control. 2) AccompBand, a\nflow-based transformer model, incorporates the Band-MOE, selecting suitable\nexperts for enhanced quality, alignment, and control. This model allows for\ngenerating controllable, high-quality accompaniments aligned with vocals. 3)\nTwo generation models, LyricBand for lyrics and MelodyBand for melodies,\ncontribute to the comprehensive multi-task song generation system, allowing for\nextensive control based on multiple prompts. Experimental results demonstrate\nthat VersBand performs better over baseline models across multiple song\ngeneration tasks using objective and subjective metrics. Audio samples are\navailable at https://VersBand.github.io."}
{"id": "2504.19188", "pdf": "https://arxiv.org/pdf/2504.19188.pdf", "abs": "https://arxiv.org/abs/2504.19188", "title": "Hierarchical Attention Generates Better Proofs", "authors": ["Jianlong Chen", "Chao Li", "Yang Yuan", "Andrew C Yao"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "comment": "15 pages with 3 figures", "summary": "Large language models (LLMs) have shown promise in formal theorem proving,\nbut their token-level processing often fails to capture the inherent\nhierarchical nature of mathematical proofs. We introduce \\textbf{Hierarchical\nAttention}, a regularization method that aligns LLMs' attention mechanisms with\nmathematical reasoning structures. Our approach establishes a five-level\nhierarchy from foundational elements to high-level concepts, ensuring\nstructured information flow in proof generation. Experiments demonstrate that\nour method improves proof success rates by 2.05\\% on miniF2F and 1.69\\% on\nProofNet while reducing proof complexity by 23.81\\% and 16.50\\% respectively.\nThe code is available at https://github.com/Car-pe/HAGBP."}
{"id": "2504.19276", "pdf": "https://arxiv.org/pdf/2504.19276.pdf", "abs": "https://arxiv.org/abs/2504.19276", "title": "Anyprefer: An Agentic Framework for Preference Data Synthesis", "authors": ["Yiyang Zhou", "Zhaoyang Wang", "Tianle Wang", "Shangyu Xing", "Peng Xia", "Bo Li", "Kaiyuan Zheng", "Zijian Zhang", "Zhaorun Chen", "Wenhao Zheng", "Xuchao Zhang", "Chetan Bansal", "Weitong Zhang", "Ying Wei", "Mohit Bansal", "Huaxiu Yao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "High-quality preference data is essential for aligning foundation models with\nhuman values through preference learning. However, manual annotation of such\ndata is often time-consuming and costly. Recent methods often adopt a\nself-rewarding approach, where the target model generates and annotates its own\npreference data, but this can lead to inaccuracies since the reward model\nshares weights with the target model, thereby amplifying inherent biases. To\naddress these issues, we propose Anyprefer, a framework designed to synthesize\nhigh-quality preference data for aligning the target model. Anyprefer frames\nthe data synthesis process as a cooperative two-player Markov Game, where the\ntarget model and the judge model collaborate together. Here, a series of\nexternal tools are introduced to assist the judge model in accurately rewarding\nthe target model's responses, mitigating biases in the rewarding process. In\naddition, a feedback mechanism is introduced to optimize prompts for both\nmodels, enhancing collaboration and improving data quality. The synthesized\ndata is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K\nhigh-quality preference pairs. Extensive experiments show that Anyprefer\nsignificantly improves model alignment performance across four main\napplications, covering 21 datasets, achieving average improvements of 18.55% in\nfive natural language generation datasets, 3.66% in nine vision-language\nunderstanding datasets, 30.05% in three medical image analysis datasets, and\n16.00% in four visuo-motor control tasks."}
{"id": "2504.19444", "pdf": "https://arxiv.org/pdf/2504.19444.pdf", "abs": "https://arxiv.org/abs/2504.19444", "title": "Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks", "authors": ["Kang Yang", "Xinjun Mao", "Shangwen Wang", "Yanlin Wang", "Tanghaoran Zhang", "Bo Lin", "Yihao Qin", "Zhang Zhang", "Yao Lu", "Kamal Al-Sabahi"], "categories": ["cs.SE", "cs.CL"], "comment": "Awarded the ACM SIGSOFT Distinguished Paper Award in ICPC 2025", "summary": "Pre-trained code models rely heavily on high-quality pre-training data,\nparticularly human-written reference comments that bridge code and natural\nlanguage. However, these comments often become outdated as software evolves,\ndegrading model performance. Large language models (LLMs) excel at generating\nhigh-quality code comments. We investigate whether replacing human-written\ncomments with LLM-generated ones improves pre-training datasets. Since standard\nmetrics cannot assess reference comment quality, we propose two novel\nreference-free evaluation tasks: code-comment inconsistency detection and\nsemantic code search. Results show that LLM-generated comments are more\nsemantically consistent with code than human-written ones, as confirmed by\nmanual evaluation. Leveraging this finding, we rebuild the CodeSearchNet\ndataset with LLM-generated comments and re-pre-train CodeT5. Evaluations\ndemonstrate that models trained on LLM-enhanced data outperform those using\noriginal human comments in code summarization, generation, and translation\ntasks. This work validates rebuilding pre-training datasets with LLMs to\nadvance code intelligence, challenging the traditional reliance on human\nreference comments."}
{"id": "2504.19458", "pdf": "https://arxiv.org/pdf/2504.19458.pdf", "abs": "https://arxiv.org/abs/2504.19458", "title": "Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective", "authors": ["Taoyu Su", "Jiawei Sheng", "Duohe Ma", "Xiaodong Li", "Juwei Yue", "Mengxiao Song", "Yingkai Tang", "Tingwen Liu"], "categories": ["cs.MM", "cs.CL", "cs.IR"], "comment": "Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,", "summary": "Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from\ndifferent Multi-Modal Knowledge Graphs (MMKGs), a critical information\nretrieval task. Existing studies have explored various fusion paradigms and\nconsistency constraints to improve the alignment of equivalent entities, while\noverlooking that the visual modality may not always contribute positively.\nEmpirically, entities with low-similarity images usually generate\nunsatisfactory performance, highlighting the limitation of overly relying on\nvisual features. We believe the model can be biased toward the visual modality,\nleading to a shortcut image-matching task. To address this, we propose a\ncounterfactual debiasing framework for MMEA, termed CDMEA, which investigates\nvisual modality bias from a causal perspective. Our approach aims to leverage\nboth visual and graph modalities to enhance MMEA while suppressing the direct\ncausal effect of the visual modality on model predictions. By estimating the\nTotal Effect (TE) of both modalities and excluding the Natural Direct Effect\n(NDE) of the visual modality, we ensure that the model predicts based on the\nTotal Indirect Effect (TIE), effectively utilizing both modalities and reducing\nvisual modality bias. Extensive experiments on 9 benchmark datasets show that\nCDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,\nhigh-noise, and low-resource data scenarios."}
{"id": "2504.19483", "pdf": "https://arxiv.org/pdf/2504.19483.pdf", "abs": "https://arxiv.org/abs/2504.19483", "title": "Improving Reasoning Performance in Large Language Models via Representation Engineering", "authors": ["Bertram Højer", "Oliver Jarvis", "Stefan Heinrich"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Has been accepted at \"The Thirteenth International Conference on\n  Learning Representations (ICLR 2025)\" Link to publication:\n  https://openreview.net/forum?id=IssPhpUsKt", "summary": "Recent advancements in large language models (LLMs) have resulted in\nincreasingly anthropomorphic language concerning the ability of LLMs to reason.\nWhether reasoning in LLMs should be understood to be inherently different is,\nhowever, widely debated. We propose utilizing a representation engineering\napproach wherein model activations are read from the residual stream of an LLM\nwhen processing a reasoning task. The activations are used to derive a control\nvector that is applied to the model as an inference-time intervention,\nmodulating the representational space of the model, to improve performance on\nthe specified task. We publish the code for deriving control vectors and\nanalyzing model representations. The method allows us to improve performance on\nreasoning benchmarks and assess how control vectors influence the final logit\ndistribution of a model via metrics such as KL divergence and entropy. We apply\ncontrol vectors to Mistral-7B-Instruct and a range of Pythia models on an\ninductive, a deductive and mathematical reasoning task. We show that an LLM\ncan, to a certain degree, be controlled to improve its perceived reasoning\nability by modulating activations. The intervention is dependent upon the\nability to reliably extract the model's typical state when correctly solving a\ntask. Our results suggest that reasoning performance can be modulated in the\nsame manner as other information-processing tasks performed by LLMs and\ndemonstrate that we are capable of improving performance on specific tasks via\na simple intervention on the residual stream with no additional training."}
{"id": "2504.19500", "pdf": "https://arxiv.org/pdf/2504.19500.pdf", "abs": "https://arxiv.org/abs/2504.19500", "title": "Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding", "authors": ["Yan Wang", "Baoxiong Jia", "Ziyu Zhu", "Siyuan Huang"], "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Open-vocabulary 3D scene understanding is pivotal for enhancing physical\nintelligence, as it enables embodied agents to interpret and interact\ndynamically within real-world environments. This paper introduces MPEC, a novel\nMasked Point-Entity Contrastive learning method for open-vocabulary 3D semantic\nsegmentation that leverages both 3D entity-language alignment and point-entity\nconsistency across different point cloud views to foster entity-specific\nfeature representations. Our method improves semantic discrimination and\nenhances the differentiation of unique instances, achieving state-of-the-art\nresults on ScanNet for open-vocabulary 3D semantic segmentation and\ndemonstrating superior zero-shot scene understanding capabilities. Extensive\nfine-tuning experiments on 8 datasets, spanning from low-level perception to\nhigh-level reasoning tasks, showcase the potential of learned 3D features,\ndriving consistent performance gains across varied 3D scene understanding\ntasks. Project website: https://mpec-3d.github.io/"}
{"id": "2504.19519", "pdf": "https://arxiv.org/pdf/2504.19519.pdf", "abs": "https://arxiv.org/abs/2504.19519", "title": "FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation", "authors": ["Ke Hong", "Xiuhong Li", "Minxu Liu", "Qiuli Mao", "Tianqi Wu", "Zixiao Huang", "Lufang Chen", "Zhong Wang", "Yichong Zhang", "Zhenhua Zhu", "Guohao Dai", "Yu Wang"], "categories": ["cs.DC", "cs.CL", "cs.LG"], "comment": "17 pages, 11 figures, 4 tables", "summary": "Generative models have achieved remarkable success across various\napplications, driving the demand for multi-GPU computing. Inter-GPU\ncommunication becomes a bottleneck in multi-GPU computing systems, particularly\non consumer-grade GPUs. By exploiting concurrent hardware execution,\noverlapping computation and communication latency is an effective technique for\nmitigating the communication overhead. We identify that an efficient and\nadaptable overlapping design should satisfy (1) tile-wise overlapping to\nmaximize the overlapping opportunity, (2) interference-free computation to\nmaintain the original computational performance, and (3) communication\nagnosticism to reduce the development burden against varying communication\nprimitives. Nevertheless, current designs fail to simultaneously optimize for\nall of those features.\n  To address the issue, we propose FlashOverlap, a lightweight design\ncharacterized by tile-wise overlapping, interference-free computation, and\ncommunication agnosticism. FlashOverlap utilizes a novel signaling mechanism to\nidentify tile-wise data dependency without interrupting the computation\nprocess, and reorders data to contiguous addresses, enabling communication by\nsimply calling NCCL APIs. Experiments show that such a lightweight design\nachieves up to 1.65x speedup, outperforming existing works in most cases."}
{"id": "2504.19583", "pdf": "https://arxiv.org/pdf/2504.19583.pdf", "abs": "https://arxiv.org/abs/2504.19583", "title": "Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning", "authors": ["Hanlu Zhang", "Yumeng Ma", "Shuo Wang", "Guiran Liu", "Binrong Zhu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "This paper proposes a parameter collaborative optimization algorithm for\nlarge language models, enhanced with graph spectral analysis. The goal is to\nimprove both fine-tuning efficiency and structural awareness during training.\nIn the proposed method, the parameters of a pre-trained language model are\ntreated as nodes in a graph. A weighted graph is constructed, and Laplacian\nspectral decomposition is applied to enable frequency-domain modeling and\nstructural representation of the parameter space. Based on this structure, a\njoint loss function is designed. It combines the task loss with a spectral\nregularization term to facilitate collaborative updates among parameters. In\naddition, a spectral filtering mechanism is introduced during the optimization\nphase. This mechanism adjusts gradients in a structure-aware manner, enhancing\nthe model's training stability and convergence behavior. The method is\nevaluated on multiple tasks, including traditional fine-tuning comparisons,\nfew-shot generalization tests, and convergence speed analysis. In all settings,\nthe proposed approach demonstrates superior performance. The experimental\nresults confirm that the spectral collaborative optimization framework\neffectively reduces parameter perturbations and improves fine-tuning quality\nwhile preserving overall model performance. This work contributes significantly\nto the field of artificial intelligence by advancing parameter-efficient\ntraining methodologies for large-scale models, reinforcing the importance of\nstructural signal processing in deep learning optimization, and offering a\nrobust, generalizable framework for enhancing language model adaptability and\nperformance."}
{"id": "2504.19730", "pdf": "https://arxiv.org/pdf/2504.19730.pdf", "abs": "https://arxiv.org/abs/2504.19730", "title": "Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge", "authors": ["Wenhan Mu", "Ling Xu", "Shuren Pei", "Le Mi", "Huichi Zhou"], "categories": ["cs.SE", "cs.CL"], "comment": "25 pages, 6 figures", "summary": "The widespread adoption of code language models in software engineering tasks\nhas exposed vulnerabilities to adversarial attacks, especially the identifier\nsubstitution attacks. Although existing identifier substitution attackers\ndemonstrate high success rates, they often produce adversarial examples with\nunnatural code patterns. In this paper, we systematically assess the quality of\nadversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%\nof adversarial examples generated by state-of-the-art identifier substitution\nattackers (e.g., ALERT) are actually detectable. Based on this insight, we\npropose EP-Shield, a unified framework for evaluating and purifying identifier\nsubstitution attacks via naturalness-aware reasoning. Specifically, we first\nevaluate the naturalness of code and identify the perturbed adversarial code,\nthen purify it so that the victim model can restore correct prediction.\nExtensive experiments demonstrate the superiority of EP-Shield over adversarial\nfine-tuning (up to 83.36% improvement) and its lightweight design 7B\nparameters) with GPT-4-level performance."}
{"id": "2504.19754", "pdf": "https://arxiv.org/pdf/2504.19754.pdf", "abs": "https://arxiv.org/abs/2504.19754", "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation", "authors": ["Carlo Merola", "Jaspinder Singh"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "13 pages, 2 figures, Second Workshop on Knowledge-Enhanced\n  Information Retrieval, ECIR 2025", "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for\nenhancing large language models (LLMs) by grounding their outputs in external\nknowledge sources. Yet, a critical question persists: how can vast volumes of\nexternal knowledge be managed effectively within the input constraints of LLMs?\nTraditional methods address this by chunking external documents into smaller,\nfixed-size segments. While this approach alleviates input limitations, it often\nfragments context, resulting in incomplete retrieval and diminished coherence\nin generation. To overcome these shortcomings, two advanced techniques, late\nchunking and contextual retrieval, have been introduced, both aiming to\npreserve global context. Despite their potential, their comparative strengths\nand limitations remain unclear. This study presents a rigorous analysis of late\nchunking and contextual retrieval, evaluating their effectiveness and\nefficiency in optimizing RAG systems. Our results indicate that contextual\nretrieval preserves semantic coherence more effectively but requires greater\ncomputational resources. In contrast, late chunking offers higher efficiency\nbut tends to sacrifice relevance and completeness."}
{"id": "2104.02496", "pdf": "https://arxiv.org/pdf/2104.02496.pdf", "abs": "https://arxiv.org/abs/2104.02496", "title": "A Bayesian approach to modeling topic-metadata relationships", "authors": ["P. Schulze", "S. Wiegrebe", "P. W. Thurner", "C. Heumann", "M. Aßenmacher"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "13 pages, 1 table, 5 figures", "summary": "The objective of advanced topic modeling is not only to explore latent\ntopical structures, but also to estimate relationships between the discovered\ntopics and theoretically relevant metadata. Methods used to estimate such\nrelationships must take into account that the topical structure is not directly\nobserved, but instead being estimated itself in an unsupervised fashion,\nusually by common topic models. A frequently used procedure to achieve this is\nthe method of composition, a Monte Carlo sampling technique performing multiple\nrepeated linear regressions of sampled topic proportions on metadata\ncovariates. In this paper, we propose two modifications of this approach:\nFirst, we substantially refine the existing implementation of the method of\ncomposition from the R package stm by replacing linear regression with the more\nappropriate Beta regression. Second, we provide a fundamental enhancement of\nthe entire estimation framework by substituting the current blending of\nfrequentist and Bayesian methods with a fully Bayesian approach. This allows\nfor a more appropriate quantification of uncertainty. We illustrate our\nimproved methodology by investigating relationships between Twitter posts by\nGerman parliamentarians and different metadata covariates related to their\nelectoral districts, using the Structural Topic Model to estimate topic\nproportions."}
{"id": "2110.08420", "pdf": "https://arxiv.org/pdf/2110.08420.pdf", "abs": "https://arxiv.org/abs/2110.08420", "title": "Understanding Dataset Difficulty with $\\mathcal{V}$-Usable Information", "authors": ["Kawin Ethayarajh", "Yejin Choi", "Swabha Swayamdipta"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2022 (Outstanding Paper)", "summary": "Estimating the difficulty of a dataset typically involves comparing\nstate-of-the-art models to humans; the bigger the performance gap, the harder\nthe dataset is said to be. However, this comparison provides little\nunderstanding of how difficult each instance in a given distribution is, or\nwhat attributes make the dataset difficult for a given model. To address these\nquestions, we frame dataset difficulty -- w.r.t. a model $\\mathcal{V}$ -- as\nthe lack of $\\mathcal{V}$-$\\textit{usable information}$ (Xu et al., 2019),\nwhere a lower value indicates a more difficult dataset for $\\mathcal{V}$. We\nfurther introduce $\\textit{pointwise $\\mathcal{V}$-information}$ (PVI) for\nmeasuring the difficulty of individual instances w.r.t. a given distribution.\nWhile standard evaluation metrics typically only compare different models for\nthe same dataset, $\\mathcal{V}$-$\\textit{usable information}$ and PVI also\npermit the converse: for a given model $\\mathcal{V}$, we can compare different\ndatasets, as well as different instances/slices of the same dataset.\nFurthermore, our framework allows for the interpretability of different input\nattributes via transformations of the input, which we use to discover\nannotation artefacts in widely-used NLP benchmarks."}
{"id": "2112.06876", "pdf": "https://arxiv.org/pdf/2112.06876.pdf", "abs": "https://arxiv.org/abs/2112.06876", "title": "Cognitive and Cultural Topology of Linguistic Categories:A Semantic-Pragmatic Metric Approach", "authors": ["Eugene Yu Ji"], "categories": ["cs.CL"], "comment": "12 Pages; 3 figures", "summary": "In recent years, the field of NLP has seen growing interest in modeling both\nsemantic and pragmatic dimensions. Despite this progress, two key challenges\npersist: firstly, the complex task of mapping and analyzing the interactions\nbetween semantic and pragmatic features; secondly, the insufficient\nincorporation of relevant insights from related disciplines outside NLP.\nAddressing these issues, this study introduces a novel geometric metric that\nutilizes word co-occurrence patterns. This metric maps two fundamental\nproperties - semantic typicality (cognitive) and pragmatic salience\n(socio-cultural) - for basic-level categories within a two-dimensional\nhyperbolic space. Our evaluations reveal that this semantic-pragmatic metric\nproduces mappings for basic-level categories that not only surpass traditional\ncognitive semantics benchmarks but also demonstrate significant socio-cultural\nrelevance. This finding proposes that basic-level categories, traditionally\nviewed as semantics-driven cognitive constructs, should be examined through the\nlens of both semantic and pragmatic dimensions, highlighting their role as a\ncognitive-cultural interface. The broad contribution of this paper lies in the\ndevelopment of medium-sized, interpretable, and human-centric language\nembedding models, which can effectively blend semantic and pragmatic dimensions\nto elucidate both the cognitive and socio-cultural significance of linguistic\ncategories."}
{"id": "2305.01920", "pdf": "https://arxiv.org/pdf/2305.01920.pdf", "abs": "https://arxiv.org/abs/2305.01920", "title": "Generative Meta-Learning for Zero-Shot Relation Triplet Extraction", "authors": ["Wanli Li", "Tieyun Qian", "Yi Song", "Zeyu Zhang", "Jiawei Li", "Zhuang Chen", "Lixin Zou"], "categories": ["cs.CL"], "comment": null, "summary": "Zero-shot Relation Triplet Extraction (ZeroRTE) aims to extract relation\ntriplets from texts containing unseen relation types. This capability benefits\nvarious downstream information retrieval (IR) tasks. The primary challenge lies\nin enabling models to generalize effectively to unseen relation categories.\nExisting approaches typically leverage the knowledge embedded in pre-trained\nlanguage models to accomplish the generalization process. However, these\nmethods focus solely on fitting the training data during training, without\nspecifically improving the model's generalization performance, resulting in\nlimited generalization capability. For this reason, we explore the integration\nof bi-level optimization (BLO) with pre-trained language models for learning\ngeneralized knowledge directly from the training data, and propose a generative\nmeta-learning framework which exploits the `learning-to-learn' ability of\nmeta-learning to boost the generalization capability of generative models.\n  Specifically, we introduce a BLO approach that simultaneously addresses data\nfitting and generalization. This is achieved by constructing an upper-level\nloss to focus on generalization and a lower-level loss to ensure accurate data\nfitting. Building on this, we subsequently develop three generative\nmeta-learning methods, each tailored to a distinct category of meta-learning.\nExtensive experimental results demonstrate that our framework performs well on\nthe ZeroRTE task. Our code is available at\nhttps://github.com/leeworry/TGM-MetaLearning."}
{"id": "2305.16326", "pdf": "https://arxiv.org/pdf/2305.16326.pdf", "abs": "https://arxiv.org/abs/2305.16326", "title": "Benchmarking large language models for biomedical natural language processing applications and recommendations", "authors": ["Qingyu Chen", "Yan Hu", "Xueqing Peng", "Qianqian Xie", "Qiao Jin", "Aidan Gilson", "Maxwell B. Singer", "Xuguang Ai", "Po-Ting Lai", "Zhizheng Wang", "Vipina Kuttichi Keloth", "Kalpana Raja", "Jiming Huang", "Huan He", "Fongci Lin", "Jingcheng Du", "Rui Zhang", "W. Jim Zheng", "Ron A. Adelman", "Zhiyong Lu", "Hua Xu"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "The rapid growth of biomedical literature poses challenges for manual\nknowledge curation and synthesis. Biomedical Natural Language Processing\n(BioNLP) automates the process. While Large Language Models (LLMs) have shown\npromise in general domains, their effectiveness in BioNLP tasks remains unclear\ndue to limited benchmarks and practical guidelines.\n  We perform a systematic evaluation of four LLMs, GPT and LLaMA\nrepresentatives on 12 BioNLP benchmarks across six applications. We compare\ntheir zero-shot, few-shot, and fine-tuning performance with traditional\nfine-tuning of BERT or BART models. We examine inconsistencies, missing\ninformation, hallucinations, and perform cost analysis. Here we show that\ntraditional fine-tuning outperforms zero or few shot LLMs in most tasks.\nHowever, closed-source LLMs like GPT-4 excel in reasoning-related tasks such as\nmedical question answering. Open source LLMs still require fine-tuning to close\nperformance gaps. We find issues like missing information and hallucinations in\nLLM outputs. These results offer practical insights for applying LLMs in\nBioNLP."}
{"id": "2309.15217", "pdf": "https://arxiv.org/pdf/2309.15217.pdf", "abs": "https://arxiv.org/abs/2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "categories": ["cs.CL"], "comment": "Reference-free (not tied to having ground truth available) evaluation\n  framework for retrieval agumented generation", "summary": "We introduce Ragas (Retrieval Augmented Generation Assessment), a framework\nfor reference-free evaluation of Retrieval Augmented Generation (RAG)\npipelines. RAG systems are composed of a retrieval and an LLM based generation\nmodule, and provide LLMs with knowledge from a reference textual database,\nwhich enables them to act as a natural language layer between a user and\ntextual databases, reducing the risk of hallucinations. Evaluating RAG\narchitectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to identify relevant and focused\ncontext passages, the ability of the LLM to exploit such passages in a faithful\nway, or the quality of the generation itself. With Ragas, we put forward a\nsuite of metrics which can be used to evaluate these different dimensions\n\\textit{without having to rely on ground truth human annotations}. We posit\nthat such a framework can crucially contribute to faster evaluation cycles of\nRAG architectures, which is especially important given the fast adoption of\nLLMs."}
{"id": "2405.13056", "pdf": "https://arxiv.org/pdf/2405.13056.pdf", "abs": "https://arxiv.org/abs/2405.13056", "title": "Large language models for newspaper sentiment analysis during COVID-19: The Guardian", "authors": ["Rohitash Chandra", "Baicheng Zhu", "Qingying Fang", "Eka Shinjikashvili"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "During the COVID-19 pandemic, the news media coverage encompassed a wide\nrange of topics that includes viral transmission, allocation of medical\nresources, and government response measures. There have been studies on\nsentiment analysis of social media platforms during COVID-19 to understand the\npublic response given the rise of cases and government strategies implemented\nto control the spread of the virus. Sentiment analysis can provide a better\nunderstanding of changes in societal opinions and emotional trends during the\npandemic. Apart from social media, newspapers have played a vital role in the\ndissemination of information, including information from the government,\nexperts, and also the public about various topics. A study of sentiment\nanalysis of newspaper sources during COVID-19 for selected countries can give\nan overview of how the media covered the pandemic. In this study, we select The\nGuardian newspaper and provide a sentiment analysis during various stages of\nCOVID-19 that includes initial transmission, lockdowns and vaccination. We\nemploy novel large language models (LLMs) and refine them with expert-labelled\nsentiment analysis data. We also provide an analysis of sentiments experienced\npre-pandemic for comparison. The results indicate that during the early\npandemic stages, public sentiment prioritised urgent crisis response, later\nshifting focus to addressing the impact on health and the economy. In\ncomparison with related studies about social media sentiment analyses, we found\na discrepancy between The Guardian with dominance of negative sentiments (sad,\nannoyed, anxious and denial), suggesting that social media offers a more\ndiversified emotional reflection. We found a grim narrative in The Guardian\nwith overall dominance of negative sentiments, pre and during COVID-19 across\nnews sections including Australia, UK, World News, and Opinion"}
{"id": "2407.02122", "pdf": "https://arxiv.org/pdf/2407.02122.pdf", "abs": "https://arxiv.org/abs/2407.02122", "title": "Fake News Detection: It's All in the Data!", "authors": ["Soveatin Kuntur", "Anna Wróblewska", "Marcin Paprzycki", "Maria Ganzha"], "categories": ["cs.CL"], "comment": null, "summary": "This comprehensive survey serves as an indispensable resource for researchers\nembarking on the journey of fake news detection. By highlighting the pivotal\nrole of dataset quality and diversity, it underscores the significance of these\nelements in the effectiveness and robustness of detection models. The survey\nmeticulously outlines the key features of datasets, various labeling systems\nemployed, and prevalent biases that can impact model performance. Additionally,\nit addresses critical ethical issues and best practices, offering a thorough\noverview of the current state of available datasets. Our contribution to this\nfield is further enriched by the provision of GitHub repository, which\nconsolidates publicly accessible datasets into a single, user-friendly portal.\nThis repository is designed to facilitate and stimulate further research and\ndevelopment efforts aimed at combating the pervasive issue of fake news."}
{"id": "2408.02239", "pdf": "https://arxiv.org/pdf/2408.02239.pdf", "abs": "https://arxiv.org/abs/2408.02239", "title": "Pula: Training Large Language Models for Setswana", "authors": ["Nathan Brown", "Vukosi Marivate"], "categories": ["cs.CL"], "comment": "NAACL 2025. 10 pages, 5 tables, 1 figure", "summary": "In this work we present Pula, a suite of bilingual language models proficient\nin both Setswana and English. Leveraging recent advancements in data\navailability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o\nand Gemini 1.5 Pro on English-Setswana translation tasks and achieve\nstate-of-the-art performance on Setswana reasoning tasks for their size. We\nrelease the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and\ntraining and evaluation code. Alongside Pula, we release the largest-ever\nSetswana text corpus, Marothodi, and the first comprehensive Setswana\ninstruction-tuning dataset, Medupi, consisting of reformatted datasets,\ntranslated corpora, and synthetic LLM-generated text. To accompany this data,\nwe release the code used for dataset construction, formatting, filtering, and\nscraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and\nGSM8K-tsn, to measure Setswana knowledge and reasoning capabilities."}
{"id": "2408.05906", "pdf": "https://arxiv.org/pdf/2408.05906.pdf", "abs": "https://arxiv.org/abs/2408.05906", "title": "AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising", "authors": ["Peinan Zhang", "Yusuke Sakai", "Masato Mita", "Hiroki Ouchi", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025", "summary": "With the increase in the fluency of ad texts automatically created by natural\nlanguage generation technology, there is high demand to verify the quality of\nthese creatives in a real-world setting. We propose AdTEC (Ad Text Evaluation\nBenchmark by CyberAgent), the first public benchmark to evaluate ad texts from\nmultiple perspectives within practical advertising operations. Our\ncontributions are as follows: (i) Defining five tasks for evaluating the\nquality of ad texts, as well as building a Japanese dataset based on the\npractical operational experiences of building a Japanese dataset based on the\npractical operational experiences of advertising agencies, which are typically\nkept in-house. (ii) Validating the performance of existing pre-trained language\nmodels (PLMs) and human evaluators on the dataset. (iii) Analyzing the\ncharacteristics and providing challenges of the benchmark. The results show\nthat while PLMs have already reached practical usage level in several tasks,\nhumans still outperform in certain domains, implying that there is significant\nroom for improvement in this area."}
{"id": "2408.08444", "pdf": "https://arxiv.org/pdf/2408.08444.pdf", "abs": "https://arxiv.org/abs/2408.08444", "title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering", "authors": ["Jinming Nian", "Zhiyuan Peng", "Qifan Wang", "Yi Fang"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "In knowledge-intensive tasks such as open-domain question answering (OpenQA),\nlarge language models (LLMs) often struggle to generate factual answers,\nrelying solely on their internal (parametric) knowledge. To address this\nlimitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by\nretrieving relevant information from external sources, thereby positioning the\nretriever as a pivotal component. Although dense retrieval demonstrates\nstate-of-the-art performance, its training poses challenges due to the scarcity\nof ground-truth evidence, largely attributed to the high costs of human\nannotation. In this paper, we propose W-RAG, a method that draws weak training\nsignals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the\nretriever to prioritize passages that most benefit the task. Specifically, we\nrerank the top-$k$ passages retrieved via BM25 by assessing the probability\nthat the LLM will generate the correct answer for a question given each\npassage. The highest-ranking passages are then used as positive fine-tuning\nexamples for dense retrieval. We conduct comprehensive experiments across four\npublicly available OpenQA datasets to demonstrate that our approach enhances\nboth retrieval and OpenQA performance compared to baseline models, achieving\nresults comparable to models fine-tuned with human-labeled data."}
{"id": "2409.06601", "pdf": "https://arxiv.org/pdf/2409.06601.pdf", "abs": "https://arxiv.org/abs/2409.06601", "title": "LaMsS: When Large Language Models Meet Self-Skepticism", "authors": ["Yetao Wu", "Yihong Wang", "Teng Chen", "Ningyuan Xi", "Qingqing Gu", "Hongyang Lei", "Luo Ji"], "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 6 figures, ICLR 2025 Workshop SSI-FM,", "summary": "Hallucination is a major challenge for large language models (LLMs),\npreventing their further application in some fields. The skeptical thinking of\nhumankind could be useful for LLMs to self-cognition, self-reflection and\nalleviate their hallucinations. Inspired by this consideration, we propose a\nnovel approach called LaMsS, which combines the semantic understanding\ncapability of LLMs with self-skepticism. By introducing a series of skepticism\ntokens and augmenting them into the vocabulary, we conduct both pertaining and\nfinetuning, which allow the LLM to decode each normal token followed by a\nskeptical token, representing different skepticism levels. By calculating the\nresponse skepticism given a query, one can define a new self-aware LLM which is\nonly willing to answer with relative lower skepticism level than the threshold.\nBy examining the accuracy, AUC and AP of willingly answering questions, we\ndemonstrate that LaMsS achieves better performance than baselines on both\nmulti-choice questions and open-domain question-answering benchmarks, and can\ngeneralize to multi-task and out-of-domain settings. Our study sheds some\nlights on the self-skepticism modeling on further artificial intelligence.\nProject code and model checkpoints can be found in\nhttps://anonymous.4open.science/r/SM-1E76."}
{"id": "2410.08800", "pdf": "https://arxiv.org/pdf/2410.08800.pdf", "abs": "https://arxiv.org/abs/2410.08800", "title": "Data Processing for the OpenGPT-X Model Family", "authors": ["Nicolo' Brandizzi", "Hammam Abdelwahab", "Anirban Bhowmick", "Lennard Helmer", "Benny Jörg Stein", "Pavel Denisov", "Qasid Saleem", "Michael Fromm", "Mehdi Ali", "Richard Rutmann", "Farzad Naderi", "Mohamad Saif Agy", "Alexander Schwirjow", "Fabian Küch", "Luzian Hahn", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Dennis Wegener", "Nicolas Flores-Herr", "Joachim Köhler", "Johannes Leveling"], "categories": ["cs.CL", "H.3.1; I.2.7"], "comment": null, "summary": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final datasets for model\ntraining. We distinguish between curated data and web data, as each of these\ncategories is handled by distinct pipelines, with curated data undergoing\nminimal filtering and web data requiring extensive filtering and deduplication.\nThis distinction guided the development of specialized algorithmic solutions\nfor both pipelines. In addition to describing the processing methodologies, we\nprovide an in-depth analysis of the datasets, increasing transparency and\nalignment with European data regulations. Finally, we share key insights and\nchallenges faced during the project, offering recommendations for future\nendeavors in large-scale multilingual data preparation for LLMs."}
{"id": "2410.11325", "pdf": "https://arxiv.org/pdf/2410.11325.pdf", "abs": "https://arxiv.org/abs/2410.11325", "title": "Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling", "authors": ["Wenda Xu", "Rujun Han", "Zifeng Wang", "Long T. Le", "Dhruv Madeka", "Lei Li", "William Yang Wang", "Rishabh Agarwal", "Chen-Yu Lee", "Tomas Pfister"], "categories": ["cs.CL", "cs.AI"], "comment": "ICLR2025", "summary": "Recent advances in knowledge distillation (KD) have enabled smaller student\nmodels to approach the performance of larger teacher models. However, popular\nmethods such as supervised KD and on-policy KD, are adversely impacted by the\nknowledge gaps between teacher-student in practical scenarios. Supervised KD\nsuffers from a distribution mismatch between training with a static dataset and\ninference over final student-generated outputs. Conversely, on-policy KD, which\nuses student-generated samples for training, can suffer from low-quality\ntraining examples with which teacher models are not familiar, resulting in\ninaccurate teacher feedback. To address these limitations, we introduce\nSpeculative Knowledge Distillation (SKD), a novel approach that leverages\ncooperation between student and teacher models to generate high-quality\ntraining data on-the-fly while aligning with the student's inference-time\ndistribution. In SKD, the student proposes tokens, and the teacher replaces\npoorly ranked ones based on its own distribution, transferring high-quality\nknowledge adaptively. We evaluate SKD on various text generation tasks,\nincluding translation, summarization, math, and instruction following, and show\nthat SKD consistently outperforms existing KD methods across different domains,\ndata sizes, and model initialization strategies."}
{"id": "2410.12311", "pdf": "https://arxiv.org/pdf/2410.12311.pdf", "abs": "https://arxiv.org/abs/2410.12311", "title": "Open Domain Question Answering with Conflicting Contexts", "authors": ["Siyi Liu", "Qiang Ning", "Kishaloy Halder", "Wei Xiao", "Zheng Qi", "Phu Mon Htut", "Yi Zhang", "Neha Anna John", "Bonan Min", "Yassine Benajiba", "Dan Roth"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Open domain question answering systems frequently rely on information\nretrieved from large collections of text (such as the Web) to answer questions.\nHowever, such collections of text often contain conflicting information, and\nindiscriminately depending on this information may result in untruthful and\ninaccurate answers. To understand the gravity of this problem, we collect a\nhuman-annotated dataset, Question Answering with Conflicting Contexts (QACC),\nand find that as much as 25% of unambiguous, open domain questions can lead to\nconflicting contexts when retrieved using Google Search. We evaluate and\nbenchmark three powerful Large Language Models (LLMs) with our dataset QACC and\ndemonstrate their limitations in effectively addressing questions with\nconflicting information. To explore how humans reason through conflicting\ncontexts, we request our annotators to provide explanations for their\nselections of correct answers. We demonstrate that by finetuning LLMs to\nexplain their answers, we can introduce richer information into their training\nthat guide them through the process of reasoning with conflicting contexts."}
{"id": "2410.13961", "pdf": "https://arxiv.org/pdf/2410.13961.pdf", "abs": "https://arxiv.org/abs/2410.13961", "title": "From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization", "authors": ["Catarina G. Belem", "Pouya Pezeshkpour", "Hayate Iso", "Seiji Maekawa", "Nikita Bhutani", "Estevam Hruschka"], "categories": ["cs.CL"], "comment": "NAACL 2025 - Findings", "summary": "Although many studies have investigated and reduced hallucinations in large\nlanguage models (LLMs) for single-document tasks, research on hallucination in\nmulti-document summarization (MDS) tasks remains largely unexplored.\nSpecifically, it is unclear how the challenges arising from handling multiple\ndocuments (e.g., repetition and diversity of information) affect models\noutputs. In this work, we investigate how hallucinations manifest in LLMs when\nsummarizing topic-specific information from multiple documents. Since no\nbenchmarks exist for investigating hallucinations in MDS, we use existing news\nand conversation datasets, annotated with topic-specific insights, to create\ntwo novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks,\nwe observe that on average, up to 75% of the content in LLM-generated summary\nis hallucinated, with hallucinations more likely to occur towards the end of\nthe summaries. Moreover, when summarizing non-existent topic-related\ninformation, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and\n44% of the time, raising concerns about their tendency to fabricate content. To\nunderstand the characteristics of these hallucinations, we manually evaluate\n700+ insights and find that most errors stem from either failing to follow\ninstructions or producing overly generic insights. Motivated by these\nobservations, we investigate the efficacy of simple post-hoc baselines in\nmitigating hallucinations but find them only moderately effective. Our results\nunderscore the need for more effective approaches to systematically mitigate\nhallucinations in MDS. We release our dataset and code at\ngithub.com/megagonlabs/Hallucination_MDS."}
{"id": "2411.00030", "pdf": "https://arxiv.org/pdf/2411.00030.pdf", "abs": "https://arxiv.org/abs/2411.00030", "title": "WikiNER-fr-gold: A Gold-Standard NER Corpus", "authors": ["Danrun Cao", "Nicolas Béchet", "Pierre-François Marteau"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "We address in this article the the quality of the WikiNER corpus, a\nmultilingual Named Entity Recognition corpus, and provide a consolidated\nversion of it. The annotation of WikiNER was produced in a semi-supervised\nmanner i.e. no manual verification has been carried out a posteriori. Such\ncorpus is called silver-standard. In this paper we propose WikiNER-fr-gold\nwhich is a revised version of the French proportion of WikiNER. Our corpus\nconsists of randomly sampled 20% of the original French sub-corpus (26,818\nsentences with 700k tokens). We start by summarizing the entity types included\nin each category in order to define an annotation guideline, and then we\nproceed to revise the corpus. Finally we present an analysis of errors and\ninconsistency observed in the WikiNER-fr corpus, and we discuss potential\nfuture work directions."}
{"id": "2411.07611", "pdf": "https://arxiv.org/pdf/2411.07611.pdf", "abs": "https://arxiv.org/abs/2411.07611", "title": "Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models", "authors": ["Shuai Niu", "Jing Ma", "Hongzhan Lin", "Liang Bai", "Zhihua Wang", "Yida Xu", "Yunya Song", "Xian Yang"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "13 pages. 7 figures", "summary": "Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable mutlimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in a same\nencoding space, enabling it naturally interpreted by SLMs while incorporating\ndomain knowledge for reliable rationale generation. Experiments on real-world\nmedical datasets show that ClinRaGen achieves state-of-the-art performance in\ndisease diagnosis and rationale generation, demonstrating the effectiveness of\ncombining LLM-driven reasoning with knowledge augmentation for improved\ninterpretability."}
{"id": "2411.17270", "pdf": "https://arxiv.org/pdf/2411.17270.pdf", "abs": "https://arxiv.org/abs/2411.17270", "title": "An Attempt to Develop a Neural Parser based on Simplified Head-Driven Phrase Structure Grammar on Vietnamese", "authors": ["Duc-Vu Nguyen", "Thang Chau Phan", "Quoc-Nam Nguyen", "Kiet Van Nguyen", "Ngan Luu-Thuy Nguyen"], "categories": ["cs.CL"], "comment": "Accepted at SoICT 2024", "summary": "In this paper, we aimed to develop a neural parser for Vietnamese based on\nsimplified Head-Driven Phrase Structure Grammar (HPSG). The existing corpora,\nVietTreebank and VnDT, had around 15% of constituency and dependency tree pairs\nthat did not adhere to simplified HPSG rules. To attempt to address the issue\nof the corpora not adhering to simplified HPSG rules, we randomly permuted\nsamples from the training and development sets to make them compliant with\nsimplified HPSG. We then modified the first simplified HPSG Neural Parser for\nthe Penn Treebank by replacing it with the PhoBERT or XLM-RoBERTa models, which\ncan encode Vietnamese texts. We conducted experiments on our modified\nVietTreebank and VnDT corpora. Our extensive experiments showed that the\nsimplified HPSG Neural Parser achieved a new state-of-the-art F-score of 82%\nfor constituency parsing when using the same predicted part-of-speech (POS)\ntags as the self-attentive constituency parser. Additionally, it outperformed\nprevious studies in dependency parsing with a higher Unlabeled Attachment Score\n(UAS). However, our parser obtained lower Labeled Attachment Score (LAS) scores\nlikely due to our focus on arc permutation without changing the original\nlabels, as we did not consult with a linguistic expert. Lastly, the research\nfindings of this paper suggest that simplified HPSG should be given more\nattention to linguistic expert when developing treebanks for Vietnamese natural\nlanguage processing."}
{"id": "2412.17592", "pdf": "https://arxiv.org/pdf/2412.17592.pdf", "abs": "https://arxiv.org/abs/2412.17592", "title": "Investigating Length Issues in Document-level Machine Translation", "authors": ["Ziqian Peng", "Rachel Bawden", "François Yvon"], "categories": ["cs.CL"], "comment": "Accepted at the MT Summit 2025", "summary": "Transformer architectures are increasingly effective at processing and\ngenerating very long chunks of texts, opening new perspectives for\ndocument-level machine translation (MT). In this work, we challenge the ability\nof MT systems to handle texts comprising up to several thousands of tokens. We\ndesign and implement a new approach designed to precisely measure the effect of\nlength increments on MT outputs. Our experiments with two representative\narchitectures unambiguously show that (a)~translation performance decreases\nwith the length of the input text; (b)~the position of sentences within the\ndocument matters, and translation quality is higher for sentences occurring\nearlier in a document. We further show that manipulating the distribution of\ndocument lengths and of positional embeddings only marginally mitigates such\nproblems. Our results suggest that even though document-level MT is\ncomputationally feasible, it does not yet match the performance of\nsentence-based MT."}
{"id": "2412.17596", "pdf": "https://arxiv.org/pdf/2412.17596.pdf", "abs": "https://arxiv.org/abs/2412.17596", "title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", "authors": ["Kai Ruan", "Xuan Wang", "Jixiang Hong", "Peng Wang", "Yang Liu", "Hao Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "Updated manuscript and title", "summary": "While Large Language Models (LLMs) demonstrate remarkable capabilities in\nscientific tasks such as literature analysis and experimental design (e.g.,\naccurately extracting key findings from papers or generating coherent\nexperimental procedures), existing evaluation benchmarks primarily assess\nperformance using rich contextual inputs. We introduce LiveIdeaBench, a\ncomprehensive benchmark evaluating LLMs' scientific idea generation by\nassessing divergent thinking capabilities using single-keyword prompts. Drawing\nfrom Guilford's creativity theory, our benchmark employs a dynamic panel of\nstate-of-the-art LLMs to assess generated ideas across five key dimensions:\noriginality, feasibility, fluency, flexibility, and clarity. Through extensive\nexperimentation with over 40 leading models across 1,180 keywords spanning 22\nscientific domains, we reveal that the scientific idea generation capabilities\nmeasured by our benchmark, are poorly predicted by standard metrics of general\nintelligence. Our results demonstrate that models like QwQ-32B-preview achieve\ncreative performance comparable to top-tier models such as\nclaude-3.7-sonnet:thinking, despite significant gaps in their general\nintelligence scores. These findings highlight the need for specialized\nevaluation benchmarks for scientific idea generation and suggest that enhancing\nthese idea generation capabilities in LLMs may require different training\nstrategies than those used for improving general problem-solving abilities,\npotentially enabling a wider range of AI tools tailored for different stages of\nthe scientific process."}
{"id": "2502.00090", "pdf": "https://arxiv.org/pdf/2502.00090.pdf", "abs": "https://arxiv.org/abs/2502.00090", "title": "Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora", "authors": ["Logan Born", "M. Willis Monroe", "Kathryn Kelley", "Anoop Sarkar"], "categories": ["cs.CL"], "comment": "Englund 1996 incorrectly reported the relative values of signs in the\n  decimal system. An earlier version of this paper used those values. This\n  update fixes those mistakes and retrains our models using the corrected\n  readings. Our analysis and discussion remain similar to the original, but the\n  performance of the baseline model is now stronger", "summary": "A numeration system encodes abstract numeric quantities as concrete strings\nof written characters. The numeration systems used by modern scripts tend to be\nprecise and unambiguous, but this was not so for the ancient and\npartially-deciphered proto-Elamite (PE) script, where written numerals can have\nup to four distinct readings depending on the system that is used to read them.\nWe consider the task of disambiguating between these readings in order to\ndetermine the values of the numeric quantities recorded in this corpus. We\nalgorithmically extract a list of possible readings for each PE numeral\nnotation, and contribute two disambiguation techniques based on structural\nproperties of the original documents and classifiers learned with the\nbootstrapping algorithm. We also contribute a test set for evaluating\ndisambiguation techniques, as well as a novel approach to cautious rule\nselection for bootstrapped classifiers. Our analysis confirms existing\nintuitions about this script and reveals previously-unknown correlations\nbetween tablet content and numeral magnitude. This work is crucial to\nunderstanding and deciphering PE, as the corpus is heavily accounting-focused\nand contains many more numeric tokens than tokens of text."}
{"id": "2502.01220", "pdf": "https://arxiv.org/pdf/2502.01220.pdf", "abs": "https://arxiv.org/abs/2502.01220", "title": "Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations", "authors": ["Hichem Ammar Khodja", "Frédéric Béchet", "Quentin Brabant", "Alexis Nasr", "Gwénolé Lecorvé"], "categories": ["cs.CL", "cs.LG"], "comment": "preprint v4", "summary": "This paper explores the robustness of language models (LMs) to variations in\nthe temporal context within factual knowledge. It examines whether LMs can\ncorrectly associate a temporal context with a past fact valid over a defined\nperiod, by asking them to differentiate correct from incorrect contexts. The\naccuracy of LMs is analyzed along two dimensions: the distance of the incorrect\ncontext from the validity period and the granularity of the context. To this\nend, a dataset called TimeStress is introduced, enabling the evaluation of 18\ndiverse LMs. Results reveal that the best LM achieves perfect accuracy for only\n6% of the studied facts, with critical errors that humans would not make. This\nwork highlights the limitations of current LMs in temporal representation. We\nprovide all data and code for further research."}
{"id": "2502.12257", "pdf": "https://arxiv.org/pdf/2502.12257.pdf", "abs": "https://arxiv.org/abs/2502.12257", "title": "InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context", "authors": ["Bryan L. M. de Oliveira", "Luana G. B. Martins", "Bruno Brandão", "Luckeciano C. Melo"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models excel at following explicit instructions, but they\noften struggle with ambiguous or incomplete user requests, defaulting to\nverbose, generic responses instead of seeking clarification. We introduce\nInfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents\nhandle hidden context in open-ended user requests. This benchmark presents\nintentionally ambiguous scenarios that require models to engage in\ninformation-seeking dialogue by asking clarifying questions before providing\nappropriate responses. Our evaluation of both open and closed models reveals\nthat, while proprietary models generally perform better, all current assistants\nstruggle to gather critical information effectively. They often require\nmultiple turns to infer user intent and frequently default to generic responses\nwithout proper clarification. We provide a systematic methodology for\ngenerating diverse scenarios and evaluating models' information-seeking\ncapabilities, which can be leveraged to automatically generate data for\nself-improvement. We also offer insights into the current limitations of\nlanguage models in handling ambiguous requests through multi-turn interactions."}
{"id": "2502.13019", "pdf": "https://arxiv.org/pdf/2502.13019.pdf", "abs": "https://arxiv.org/abs/2502.13019", "title": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation", "authors": ["Sha Li", "Naren Ramakrishnan"], "categories": ["cs.CL"], "comment": "16 pages", "summary": "Retrieval-Augmented Generation (RAG) aims to augment the capabilities of\nLarge Language Models (LLMs) by retrieving and incorporate external documents\nor chunks prior to generation. However, even improved retriever relevance can\nbrings erroneous or contextually distracting information, undermining the\neffectiveness of RAG in downstream tasks. We introduce a compact, efficient,\nand pluggable module designed to refine retrieved chunks before using them for\ngeneration. The module aims to extract and reorganize the most relevant and\nsupportive information into a concise, query-specific format. Through a\nthree-stage training paradigm - comprising supervised fine - tuning,\ncontrastive multi-task learning, and reinforcement learning-based alignment -\nit prioritizes critical knowledge and aligns it with the generator's\npreferences. This approach enables LLMs to produce outputs that are more\naccurate, reliable, and contextually appropriate."}
{"id": "2502.13881", "pdf": "https://arxiv.org/pdf/2502.13881.pdf", "abs": "https://arxiv.org/abs/2502.13881", "title": "PSCon: Product Search Through Conversations", "authors": ["Jie Zou", "Mohammad Aliannejadi", "Evangelos Kanoulas", "Shuxi Han", "Heli Ma", "Zheng Wang", "Yang Yang", "Heng Tao Shen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "11 pages. Accepted by SIGIR 2025", "summary": "Conversational Product Search ( CPS ) systems interact with users via natural\nlanguage to offer personalized and context-aware product lists. However, most\nexisting research on CPS is limited to simulated conversations, due to the lack\nof a real CPS dataset driven by human-like language. Moreover, existing\nconversational datasets for e-commerce are constructed for a particular market\nor a particular language and thus can not support cross-market and\nmulti-lingual usage. In this paper, we propose a CPS data collection protocol\nand create a new CPS dataset, called PSCon, which assists product search\nthrough conversations with human-like language. The dataset is collected by a\ncoached human-human data collection protocol and is available for dual markets\nand two languages. By formulating the task of CPS, the dataset allows for\ncomprehensive and in-depth research on six subtasks: user intent detection,\nkeyword extraction, system action prediction, question selection, item ranking,\nand response generation. Moreover, we present a concise analysis of the dataset\nand propose a benchmark model on the proposed CPS dataset. Our proposed dataset\nand model will be helpful for facilitating future research on CPS."}
{"id": "2502.14644", "pdf": "https://arxiv.org/pdf/2502.14644.pdf", "abs": "https://arxiv.org/abs/2502.14644", "title": "LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning", "authors": ["Yansheng Mao", "Yufei Xu", "Jiaqi Li", "Fanxu Meng", "Haotong Yang", "Zilong Zheng", "Xiyuan Wang", "Muhan Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper presents Long Input Fine-Tuning\n(LIFT), a novel framework for long-context modeling that can improve the\nlong-context performance of arbitrary (short-context) LLMs by dynamically\nadapting model parameters based on the long input. Importantly, LIFT, rather\nthan endlessly extending the context window size to accommodate increasingly\nlonger inputs in context, chooses to store and absorb the long input in\nparameter. By fine-tuning the long input into model parameters, LIFT allows\nshort-context LLMs to answer questions even when the required information is\nnot provided in the context during inference. Furthermore, to enhance LIFT\nperformance while maintaining the original in-context learning (ICL)\ncapabilities, we introduce Gated Memory, a specialized attention adapter that\nautomatically balances long input memorization and ICL. We provide a\ncomprehensive analysis of the strengths and limitations of LIFT on long context\nunderstanding, offering valuable directions for future research."}
{"id": "2502.15147", "pdf": "https://arxiv.org/pdf/2502.15147.pdf", "abs": "https://arxiv.org/abs/2502.15147", "title": "Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision", "authors": ["Zhouhang Xie", "Tushar Khot", "Bhavana Dalvi Mishra", "Harshit Surana", "Julian McAuley", "Peter Clark", "Bodhisattwa Prasad Majumder"], "categories": ["cs.CL"], "comment": "NAACL 2025", "summary": "Instruction-following LLMs have recently allowed systems to discover hidden\nconcepts from a collection of unstructured documents based on a natural\nlanguage description of the purpose of the discovery (i.e., goal). Still, the\nquality of the discovered concepts remains mixed, as it depends heavily on\nLLM's reasoning ability and drops when the data is noisy or beyond LLM's\nknowledge. We present Instruct-LF, a goal-oriented latent factor discovery\nsystem that integrates LLM's instruction-following ability with statistical\nmodels to handle large, noisy datasets where LLM reasoning alone falls short.\n  Instruct-LF uses LLMs to propose fine-grained, goal-related properties from\ndocuments, estimates their presence across the dataset, and applies\ngradient-based optimization to uncover hidden factors, where each factor is\nrepresented by a cluster of co-occurring properties. We evaluate latent factors\nproduced by Instruct-LF on movie recommendation, text-world navigation, and\nlegal document categorization tasks. These interpretable representations\nimprove downstream task performance by 5-52% than the best baselines and were\npreferred 1.8 times as often as the best alternative, on average, in human\nevaluation."}
{"id": "2502.20503", "pdf": "https://arxiv.org/pdf/2502.20503.pdf", "abs": "https://arxiv.org/abs/2502.20503", "title": "Protecting multimodal large language models against misleading visualizations", "authors": ["Jonathan Tonglet", "Tinne Tuytelaars", "Marie-Francine Moens", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Preprint. Code and data available at\n  https://github.com/UKPLab/arxiv2025-misleading-visualizations", "summary": "Visualizations play a pivotal role in daily communication in an increasingly\ndata-driven world. Research on multimodal large language models (MLLMs) for\nautomated chart understanding has accelerated massively, with steady\nimprovements on standard benchmarks. However, for MLLMs to be reliable, they\nmust be robust to misleading visualizations, charts that distort the underlying\ndata, leading readers to draw inaccurate conclusions that may support\ndisinformation. Here, we uncover an important vulnerability: MLLM\nquestion-answering accuracy on misleading visualizations drops on average to\nthe level of a random baseline. To address this, we introduce the first\ninference-time methods to improve performance on misleading visualizations,\nwithout compromising accuracy on non-misleading ones. The most effective method\nextracts the underlying data table and uses a text-only LLM to answer the\nquestion based on the table. Our findings expose a critical blind spot in\ncurrent research and establish benchmark results to guide future efforts in\nreliable MLLMs."}
{"id": "2503.10617", "pdf": "https://arxiv.org/pdf/2503.10617.pdf", "abs": "https://arxiv.org/abs/2503.10617", "title": "Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models", "authors": ["Andy Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICLR 2025 SCOPE", "summary": "Adapting large language models to multiple tasks can cause cross-skill\ninterference, where improvements for one skill degrade another. While methods\nsuch as LoRA impose orthogonality constraints at the weight level, they do not\nfully address interference in hidden-state representations. We propose\nCompositional Subspace Representation Fine-tuning (CS-ReFT), a novel\nrepresentation-based approach that learns multiple orthonormal subspace\ntransformations, each specializing in a distinct skill, and composes them via a\nlightweight router. By isolating these subspace edits in the hidden state,\nrather than weight matrices, CS-ReFT prevents cross-task conflicts more\neffectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B\nachieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring\nonly 0.0098% of model parameters. These findings show that specialized\nrepresentation edits, composed via a simple router, significantly enhance\nmulti-task instruction following with minimal overhead."}
{"id": "2504.06868", "pdf": "https://arxiv.org/pdf/2504.06868.pdf", "abs": "https://arxiv.org/abs/2504.06868", "title": "Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games", "authors": ["Seungwon Lim", "Seungbeen Lee", "Dongjun Min", "Youngjae Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Artificial agents are increasingly central to complex interactions and\ndecision-making tasks, yet aligning their behaviors with desired human values\nremains an open challenge. In this work, we investigate how human-like\npersonality traits influence agent behavior and performance within text-based\ninteractive environments. We introduce PANDA: Personality Adapted Neural\nDecision Agents, a novel method for projecting human personality traits onto\nagents to guide their behavior. To induce personality in a text-based game\nagent, (i) we train a personality classifier to identify what personality type\nthe agent's actions exhibit, and (ii) we integrate the personality profiles\ndirectly into the agent's policy-learning pipeline. By deploying agents\nembodying 16 distinct personality types across 25 text-based games and\nanalyzing their trajectories, we demonstrate that an agent's action decisions\ncan be guided toward specific personality profiles. Moreover, certain\npersonality types, such as those characterized by higher levels of Openness,\ndisplay marked advantages in performance. These findings underscore the promise\nof personality-adapted agents for fostering more aligned, effective, and\nhuman-centric decision-making in interactive environments."}
{"id": "2504.09714", "pdf": "https://arxiv.org/pdf/2504.09714.pdf", "abs": "https://arxiv.org/abs/2504.09714", "title": "Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish", "authors": ["Ayşe Aysu Cengiz", "Ahmet Kaan Sever", "Elif Ecem Ümütlü", "Naime Şeyma Erdem", "Burak Aytan", "Büşra Tufan", "Abdullah Topraksoy", "Esra Darıcı", "Cagri Toraman"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reliance on translated or adapted datasets from English or multilingual\nresources introduces challenges regarding linguistic and cultural suitability.\nThis study addresses the need for robust and culturally appropriate benchmarks\nby evaluating the quality of 17 commonly used Turkish benchmark datasets. Using\na comprehensive framework that assesses six criteria, both human and LLM-judge\nannotators provide detailed evaluations to identify dataset strengths and\nshortcomings.\n  Our results reveal that 70% of the benchmark datasets fail to meet our\nheuristic quality standards. The correctness of the usage of technical terms is\nthe strongest criterion, but 85% of the criteria are not satisfied in the\nexamined datasets. Although LLM judges demonstrate potential, they are less\neffective than human annotators, particularly in understanding cultural common\nsense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger\nlabeling capabilities for grammatical and technical tasks, while Llama3.3-70B\nexcels at correctness and cultural knowledge evaluation. Our findings emphasize\nthe urgent need for more rigorous quality control in creating and adapting\ndatasets for low-resource languages."}
{"id": "2504.10982", "pdf": "https://arxiv.org/pdf/2504.10982.pdf", "abs": "https://arxiv.org/abs/2504.10982", "title": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs", "authors": ["Yingjian Chen", "Feiyang Li", "Xingyu Song", "Tianxiao Li", "Zixin Xu", "Xiujie Chen", "Issey Sukeda", "Irene Li"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages."}
{"id": "2504.11975", "pdf": "https://arxiv.org/pdf/2504.11975.pdf", "abs": "https://arxiv.org/abs/2504.11975", "title": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes", "authors": ["Raúl Vázquez", "Timothee Mickus", "Elaine Zosa", "Teemu Vahtola", "Jörg Tiedemann", "Aman Sinha", "Vincent Segonne", "Fernando Sánchez-Vega", "Alessandro Raganato", "Jindřich Libovický", "Jussi Karlgren", "Shaoxiong Ji", "Jindřich Helcl", "Liane Guillou", "Ona de Gibert", "Jaione Bengoetxea", "Joseph Attieh", "Marianna Apidianaki"], "categories": ["cs.CL"], "comment": "Mu-SHROOM is part of SemEval-2025 (Task 3). TBP: Proceedings of the\n  19th International Workshop on Semantic Evaluation (SemEval-2025)", "summary": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans."}
{"id": "2504.12098", "pdf": "https://arxiv.org/pdf/2504.12098.pdf", "abs": "https://arxiv.org/abs/2504.12098", "title": "Gauging Overprecision in LLMs: An Empirical Study", "authors": ["Adil Bahaj", "Hamed Rahimi", "Mohamed Chetouani", "Mounir Ghogho"], "categories": ["cs.CL"], "comment": "16 pages", "summary": "Recently, overconfidence in large language models (LLMs) has garnered\nconsiderable attention due to its fundamental importance in quantifying the\ntrustworthiness of LLM generation. However, existing approaches prompt the\n\\textit{black box LLMs} to produce their confidence (\\textit{verbalized\nconfidence}), which can be subject to many biases and hallucinations. Inspired\nby a different aspect of overconfidence in cognitive science called\n\\textit{overprecision}, we designed a framework for its study in black box\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\nand 3) evaluation. In the generation phase we prompt the LLM to generate\nanswers to numerical questions in the form of intervals with a certain level of\nconfidence. This confidence level is imposed in the prompt and not required for\nthe LLM to generate as in previous approaches. We use various prompting\ntechniques and use the same prompt multiple times to gauge the effects of\nrandomness in the generation process. In the refinement phase, answers from the\nprevious phase are refined to generate better answers. The LLM answers are\nevaluated and studied in the evaluation phase to understand its internal\nworkings. This study allowed us to gain various insights into LLM\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2) there is\nno correlation between the length of the interval and the imposed confidence\nlevel, which can be symptomatic of a a) lack of understanding of the concept of\nconfidence or b) inability to adjust self-confidence by following instructions,\n{3) LLM numerical precision differs depending on the task, scale of answer and\nprompting technique 4) Refinement of answers doesn't improve precision in most\ncases. We believe this study offers new perspectives on LLM overconfidence and\nserves as a strong baseline for overprecision in LLMs."}
{"id": "2504.13828", "pdf": "https://arxiv.org/pdf/2504.13828.pdf", "abs": "https://arxiv.org/abs/2504.13828", "title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering", "authors": ["Shijie Xia", "Yiwei Qin", "Xuefeng Li", "Yan Ma", "Run-Ze Fan", "Steffi Chern", "Haoyang Zou", "Fan Zhou", "Xiangkun Hu", "Jiahe Jin", "Yanheng He", "Yixin Ye", "Yixiu Liu", "Pengfei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "v3: add the comparison to existing work part; fix some errors", "summary": "The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations such as\nknowledge latency, shallow reasoning, and constrained cognitive processes.\nDuring this era, prompt engineering emerged as our primary interface with AI,\nenabling dialogue-level communication through natural language. We now witness\nthe emergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering"}
{"id": "2504.15471", "pdf": "https://arxiv.org/pdf/2504.15471.pdf", "abs": "https://arxiv.org/abs/2504.15471", "title": "Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models", "authors": ["Tyler A. Chang", "Benjamin K. Bergen"], "categories": ["cs.CL"], "comment": null, "summary": "In Transformer language models, activation vectors transform from current\ntoken embeddings to next token predictions as they pass through the model. To\nisolate a minimal form of this transformation, we identify language model\nsubnetworks that make bigram predictions, naive next token predictions based\nonly on the current token. We find that bigram subnetworks can be found in\nfully trained language models up to 1B parameters, and these subnetworks are\ncritical for model performance even when they consist of less than 0.2% of\nmodel parameters. Bigram subnetworks are concentrated in the first Transformer\nMLP layer, and they overlap significantly with subnetworks trained to optimally\nprune a given model. Mechanistically, the bigram subnetworks often recreate a\npattern from the full models where the first layer induces a sharp change that\naligns activations with next token predictions rather than current token\nrepresentations. Our results demonstrate that bigram subnetworks comprise a\nminimal subset of parameters that are both necessary and sufficient for basic\nnext token predictions in language models, and they help drive the\ntransformation from current to next token activations in the residual stream.\nThese subnetworks can lay a foundation for studying more complex language model\ncircuits by building up from a minimal circuit."}
{"id": "2504.15900", "pdf": "https://arxiv.org/pdf/2504.15900.pdf", "abs": "https://arxiv.org/abs/2504.15900", "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning", "authors": ["Cheng Wen", "Tingwei Guo", "Shuaijiang Zhao", "Wei Zou", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding."}
{"id": "2504.16286", "pdf": "https://arxiv.org/pdf/2504.16286.pdf", "abs": "https://arxiv.org/abs/2504.16286", "title": "The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation", "authors": ["Li Weigang", "Pedro Carvalho Brom"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "24 pages, 3 figures", "summary": "The rapid advancement of large language models (LLMs) has reshaped the\nlandscape of machine translation, yet challenges persist in preserving poetic\nintent, cultural heritage, and handling specialized terminology in\nChinese-English translation. This study constructs a diverse corpus\nencompassing Chinese scientific terminology, historical translation paradoxes,\nand literary metaphors. Utilizing a back-translation and Friedman test-based\nevaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic\nsimilarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three\ntraditional translation tools. Key findings include: (1) Scientific abstracts\noften benefit from back-translation, while traditional tools outperform LLMs in\nlinguistically distinct texts; (2) LLMs struggle with cultural and literary\nretention, exemplifying the \"paradox of poetic intent\"; (3) Some models exhibit\n\"verbatim back-translation\", reflecting emergent memory behavior; (4) A novel\nBLEU variant using Jieba segmentation and n-gram weighting is proposed. The\nstudy contributes to the empirical evaluation of Chinese NLP performance and\nadvances understanding of cultural fidelity in AI-mediated translation."}
{"id": "2504.16511", "pdf": "https://arxiv.org/pdf/2504.16511.pdf", "abs": "https://arxiv.org/abs/2504.16511", "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining", "authors": ["Fengze Liu", "Weidong Zhou", "Binbin Liu", "Zhimiao Yu", "Yifan Zhang", "Haobin Lin", "Yifeng Yu", "Bingni Zhang", "Xiaohuan Zhou", "Taifeng Wang", "Yong Cao"], "categories": ["cs.CL"], "comment": null, "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity."}
{"id": "2504.16778", "pdf": "https://arxiv.org/pdf/2504.16778.pdf", "abs": "https://arxiv.org/abs/2504.16778", "title": "Evaluation Framework for AI Systems in \"the Wild\"", "authors": ["Sarah Jabbour", "Trenton Chang", "Anindya Das Antar", "Joseph Peper", "Insu Jang", "Jiachen Liu", "Jae-Won Chung", "Shiqi He", "Michael Wellman", "Bryan Goodman", "Elizabeth Bondi-Kelly", "Kevin Samy", "Rada Mihalcea", "Mosharaf Chowdhury", "David Jurgens", "Lu Wang"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "35 pages", "summary": "Generative AI (GenAI) models have become vital across industries, yet current\nevaluation methods have not adapted to their widespread use. Traditional\nevaluations often rely on benchmarks and fixed datasets, frequently failing to\nreflect real-world performance, which creates a gap between lab-tested outcomes\nand practical applications. This white paper proposes a comprehensive framework\nfor how we should evaluate real-world GenAI systems, emphasizing diverse,\nevolving inputs and holistic, dynamic, and ongoing assessment approaches. The\npaper offers guidance for practitioners on how to design evaluation methods\nthat accurately reflect real-time capabilities, and provides policymakers with\nrecommendations for crafting GenAI policies focused on societal impacts, rather\nthan fixed performance numbers or parameter sizes. We advocate for holistic\nframeworks that integrate performance, fairness, and ethics and the use of\ncontinuous, outcome-oriented methods that combine human and automated\nassessments while also being transparent to foster trust among stakeholders.\nImplementing these strategies ensures GenAI models are not only technically\nproficient but also ethically responsible and impactful."}
{"id": "2504.16884", "pdf": "https://arxiv.org/pdf/2504.16884.pdf", "abs": "https://arxiv.org/abs/2504.16884", "title": "Do Large Language Models know who did what to whom?", "authors": ["Joseph M. Denning", "Xiaohan Hannah Guo", "Bryor Snefjella", "Idan A. Blank"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are commonly criticized for not understanding\nlanguage. However, many critiques focus on cognitive abilities that, in humans,\nare distinct from language processing. Here, we instead study a kind of\nunderstanding tightly linked to language: inferring who did what to whom\n(thematic roles) in a sentence. Does the central training objective of\nLLMs-word prediction-result in sentence representations that capture thematic\nroles? In two experiments, we characterized sentence representations in four\nLLMs. In contrast to human similarity judgments, in LLMs the overall\nrepresentational similarity of sentence pairs reflected syntactic similarity\nbut not whether their agent and patient assignments were identical vs.\nreversed. Furthermore, we found little evidence that thematic role information\nwas available in any subset of hidden units. However, some attention heads\nrobustly captured thematic roles, independently of syntax. Therefore, LLMs can\nextract thematic roles but, relative to humans, this information influences\ntheir representations more weakly."}
{"id": "2504.17130", "pdf": "https://arxiv.org/pdf/2504.17130.pdf", "abs": "https://arxiv.org/abs/2504.17130", "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control", "authors": ["Hannah Cyberey", "David Evans"], "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector. Our code is publicly available\nat: https://github.com/hannahxchen/llm-censorship-steering"}
{"id": "2504.17192", "pdf": "https://arxiv.org/pdf/2504.17192.pdf", "abs": "https://arxiv.org/abs/2504.17192", "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning", "authors": ["Minju Seo", "Jinheon Baek", "Seongyun Lee", "Sung Ju Hwang"], "categories": ["cs.CL"], "comment": null, "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins. Code is available at:\nhttps://github.com/going-doer/Paper2Code."}
{"id": "2301.10140", "pdf": "https://arxiv.org/pdf/2301.10140.pdf", "abs": "https://arxiv.org/abs/2301.10140", "title": "The Semantic Scholar Open Data Platform", "authors": ["Rodney Kinney", "Chloe Anastasiades", "Russell Authur", "Iz Beltagy", "Jonathan Bragg", "Alexandra Buraczynski", "Isabel Cachola", "Stefan Candra", "Yoganand Chandrasekhar", "Arman Cohan", "Miles Crawford", "Doug Downey", "Jason Dunkelberger", "Oren Etzioni", "Rob Evans", "Sergey Feldman", "Joseph Gorney", "David Graham", "Fangzhou Hu", "Regan Huff", "Daniel King", "Sebastian Kohlmeier", "Bailey Kuehl", "Michael Langan", "Daniel Lin", "Haokun Liu", "Kyle Lo", "Jaron Lochner", "Kelsey MacMillan", "Tyler Murray", "Chris Newell", "Smita Rao", "Shaurya Rohatgi", "Paul Sayre", "Zejiang Shen", "Amanpreet Singh", "Luca Soldaini", "Shivashankar Subramanian", "Amber Tanaka", "Alex D. Wade", "Linda Wagner", "Lucy Lu Wang", "Chris Wilhelm", "Caroline Wu", "Jiangjiang Yang", "Angele Zamarron", "Madeleine Van Zuylen", "Daniel S. Weld"], "categories": ["cs.DL", "cs.CL"], "comment": "8 pages, 6 figures", "summary": "The volume of scientific output is creating an urgent need for automated\ntools to help scientists keep up with developments in their field. Semantic\nScholar (S2) is an open data platform and website aimed at accelerating science\nby helping scholars discover and understand scientific literature. We combine\npublic and proprietary data sources using state-of-the-art techniques for\nscholarly PDF content extraction and automatic knowledge graph construction to\nbuild the Semantic Scholar Academic Graph, the largest open scientific\nliterature graph to-date, with 200M+ papers, 80M+ authors, 550M+\npaper-authorship edges, and 2.4B+ citation edges. The graph includes advanced\nsemantic features such as structurally parsed text, natural language summaries,\nand vector embeddings. In this paper, we describe the components of the S2 data\nprocessing pipeline and the associated APIs offered by the platform. We will\nupdate this living document to reflect changes as we add new data offerings and\nimprove existing services."}
{"id": "2303.10430", "pdf": "https://arxiv.org/pdf/2303.10430.pdf", "abs": "https://arxiv.org/abs/2303.10430", "title": "NoisyHate: Mining Online Human-Written Perturbations for Realistic Robustness Benchmarking of Content Moderation Models", "authors": ["Yiran Ye", "Thai Le", "Dongwon Lee"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": "Accepted to International AAAI Conference on Web and Social Media\n  (ICWSM 2025)", "summary": "Online texts with toxic content are a clear threat to the users on social\nmedia in particular and society in general. Although many platforms have\nadopted various measures (e.g., machine learning-based hate-speech detection\nsystems) to diminish their effect, toxic content writers have also attempted to\nevade such measures by using cleverly modified toxic words, so-called\nhuman-written text perturbations. Therefore, to help build automatic detection\ntools to recognize those perturbations, prior methods have developed\nsophisticated techniques to generate diverse adversarial samples. However, we\nnote that these ``algorithms\"-generated perturbations do not necessarily\ncapture all the traits of ``human\"-written perturbations. Therefore, in this\npaper, we introduce a novel, high-quality dataset of human-written\nperturbations, named as NoisyHate, that was created from real-life\nperturbations that are both written and verified by human-in-the-loop. We show\nthat perturbations in NoisyHate have different characteristics than prior\nalgorithm-generated toxic datasets show, and thus can be in particular useful\nto help develop better toxic speech detection solutions. We thoroughly validate\nNoisyHate against state-of-the-art language models, such as BERT and RoBERTa,\nand black box APIs, such as Perspective API, on two tasks, such as perturbation\nnormalization and understanding."}
{"id": "2307.12369", "pdf": "https://arxiv.org/pdf/2307.12369.pdf", "abs": "https://arxiv.org/abs/2307.12369", "title": "Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans", "authors": ["Rumeng Li", "Xun Wang", "Dan Berlowitz", "Brian Silver", "Wen Hu", "Heather Keating", "Raelene Goodwin", "Weisong Liu", "Honghuang Lin", "Hong Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "An updated version is under review. Data and experiment results have\n  been updated", "summary": "Early prediction of Alzheimer's disease (AD) is crucial for timely\nintervention and treatment. This study aims to use machine learning approaches\nto analyze longitudinal electronic health records (EHRs) of patients with AD\nand identify signs and symptoms that can predict AD onset earlier. We used a\ncase-control design with longitudinal EHRs from the U.S. Department of Veterans\nAffairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA\npatients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9\nwith controls by age, sex and clinical utilization with replacement. We used a\npanel of AD-related keywords and their occurrences over time in a patient's\nlongitudinal EHRs as predictors for AD prediction with four machine learning\nmodels. We performed subgroup analyses by age, sex, and race/ethnicity, and\nvalidated the model in a hold-out and \"unseen\" VHA stations group. Model\ndiscrimination, calibration, and other relevant metrics were reported for\npredictions up to ten years before ICD-based diagnosis. The study population\nincluded 16,701 cases and 39,097 matched controls. The average number of\nAD-related keywords (e.g., \"concentration\", \"speaking\") per year increased\nrapidly for cases as diagnosis approached, from around 10 to over 40, while\nremaining flat at 10 for controls. The best model achieved high discriminative\naccuracy (ROCAUC 0.997) for predictions using data from at least ten years\nbefore ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow\ngoodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and\nrace/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine\nlearning models using AD-related keywords identified from EHR notes can predict\nfuture AD diagnoses, suggesting its potential use for identifying AD risk using\nEHR notes, offering an affordable way for early screening on large population."}
{"id": "2403.19103", "pdf": "https://arxiv.org/pdf/2403.19103.pdf", "abs": "https://arxiv.org/abs/2403.19103", "title": "Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation", "authors": ["Yutong He", "Alexander Robey", "Naoki Murata", "Yiding Jiang", "Joshua Nathaniel Williams", "George J. Pappas", "Hamed Hassani", "Yuki Mitsufuji", "Ruslan Salakhutdinov", "J. Zico Kolter"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Prompt engineering is an effective but labor-intensive way to control\ntext-to-image (T2I) generative models. Its time-intensive nature and complexity\nhave spurred the development of algorithms for automated prompt generation.\nHowever, these methods often struggle with transferability across T2I models,\nrequire white-box access to the underlying model, or produce non-intuitive\nprompts. In this work, we introduce PRISM, an algorithm that automatically\nproduces human-interpretable and transferable prompts that can effectively\ngenerate desired concepts given only black-box access to T2I models. Inspired\nby large language model (LLM) jailbreaking, PRISM leverages the in-context\nlearning ability of LLMs to iteratively refine the candidate prompt\ndistribution built upon the reference images. Our experiments demonstrate the\nversatility and effectiveness of PRISM in generating accurate prompts for\nobjects, styles, and images across multiple T2I models, including Stable\nDiffusion, DALL-E, and Midjourney."}
{"id": "2403.20331", "pdf": "https://arxiv.org/pdf/2403.20331.pdf", "abs": "https://arxiv.org/abs/2403.20331", "title": "Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models", "authors": ["Atsuyuki Miyai", "Jingkang Yang", "Jingyang Zhang", "Yifei Ming", "Qing Yu", "Go Irie", "Yixuan Li", "Hai Li", "Ziwei Liu", "Kiyoharu Aizawa"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Code: https://github.com/AtsuMiyai/UPD. Update from v2: Correction to\n  Figure 1", "summary": "This paper introduces a novel task to evaluate the robust understanding\ncapability of Large Multimodal Models (LMMs), termed $\\textbf{Unsolvable\nProblem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely\nused to assess the understanding capability of LMMs, but it does not guarantee\nthat LMMs truly comprehend the answer. UPD assesses the LMM's ability to\nwithhold answers when encountering unsolvable problems of MCQA, verifying\nwhether the model truly understands the answer. UPD encompasses three problems:\nAbsent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and\nIncompatible Visual Question Detection (IVQD), covering unsolvable cases like\nanswer-lacking or incompatible choices and image-question mismatches. For the\nevaluation, we introduce the MM-UPD Bench, a benchmark for assessing\nperformance across various ability dimensions. Our experiments reveal that even\nmost LMMs, which demonstrate adequate performance on existing benchmarks,\nstruggle significantly with MM-UPD, underscoring a novel aspect of\ntrustworthiness that current benchmarks have overlooked. A detailed analysis\nshows that LMMs have different bottlenecks and chain-of-thought and\nself-reflection improved performance for LMMs with the bottleneck in their LLM\ncapability. We hope our insights will enhance the broader understanding and\ndevelopment of more reliable LMMs."}
{"id": "2409.03140", "pdf": "https://arxiv.org/pdf/2409.03140.pdf", "abs": "https://arxiv.org/abs/2409.03140", "title": "GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase Recommendation", "authors": ["Ashirbad Mishra", "Soumik Dey", "Marshall Wu", "Jinyu Zhao", "He Yu", "Kaichen Ni", "Binbin Li", "Kamesh Madduri"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "Online sellers and advertisers are recommended keyphrases for their listed\nproducts, which they bid on to enhance their sales. One popular paradigm that\ngenerates such recommendations is Extreme Multi-Label Classification (XMC),\nwhich involves tagging/mapping keyphrases to items. We outline the limitations\nof using traditional item-query based tagging or mapping techniques for\nkeyphrase recommendations on E-Commerce platforms. We introduce GraphEx, an\ninnovative graph-based approach that recommends keyphrases to sellers using\nextraction of token permutations from item titles. Additionally, we demonstrate\nthat relying on traditional metrics such as precision/recall can be misleading\nin practical applications, thereby necessitating a combination of metrics to\nevaluate performance in real-world scenarios. These metrics are designed to\nassess the relevance of keyphrases to items and the potential for buyer\noutreach. GraphEx outperforms production models at eBay, achieving the\nobjectives mentioned above. It supports near real-time inferencing in\nresource-constrained production environments and scales effectively for\nbillions of items."}
{"id": "2409.09013", "pdf": "https://arxiv.org/pdf/2409.09013.pdf", "abs": "https://arxiv.org/abs/2409.09013", "title": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents", "authors": ["Zhe Su", "Xuhui Zhou", "Sanketh Rangreji", "Anubha Kabra", "Julia Mendelsohn", "Faeze Brahman", "Maarten Sap"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Truthfulness (adherence to factual accuracy) and utility (satisfying human\nneeds and instructions) are both fundamental aspects of Large Language Models,\nyet these goals often conflict (e.g., sell a car with known flaws), which makes\nit challenging to achieve both in real-world deployments. We propose AI-LieDar,\na framework to study how LLM-based agents navigate these scenarios in an\nmulti-turn interactive setting. We design a set of real-world scenarios where\nlanguage agents are instructed to achieve goals that are in conflict with being\ntruthful during a multi-turn conversation with simulated human agents. To\nevaluate the truthfulness at large scale, we develop a truthfulness detector\ninspired by psychological literature to assess the agents' responses. Our\nexperiment demonstrates that all models are truthful less than 50% of the time,\nthough truthfulness and goal achievement (utility) rates vary across models. We\nfurther test the steerability of LLMs towards truthfulness, finding that models\ncan be directed to be truthful or deceptive, and even truth-steered models\nstill lie. These findings reveal the complex nature of truthfulness in LLMs and\nunderscore the importance of further research to ensure the safe and reliable\ndeployment of LLMs and LLM-based agents."}
{"id": "2410.08847", "pdf": "https://arxiv.org/pdf/2410.08847.pdf", "abs": "https://arxiv.org/abs/2410.08847", "title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization", "authors": ["Noam Razin", "Sadhika Malladi", "Adithya Bhaskar", "Danqi Chen", "Sanjeev Arora", "Boris Hanin"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "Accepted to ICLR 2025; Code available at\n  https://github.com/princeton-nlp/unintentional-unalignment", "summary": "Direct Preference Optimization (DPO) and its variants are increasingly used\nfor aligning language models with human preferences. Although these methods are\ndesigned to teach a model to generate preferred responses more frequently\nrelative to dispreferred responses, prior work has observed that the likelihood\nof preferred responses often decreases during training. The current work sheds\nlight on the causes and implications of this counter-intuitive phenomenon,\nwhich we term likelihood displacement. We demonstrate that likelihood\ndisplacement can be catastrophic, shifting probability mass from preferred\nresponses to responses with an opposite meaning. As a simple example, training\na model to prefer $\\texttt{No}$ over $\\texttt{Never}$ can sharply increase the\nprobability of $\\texttt{Yes}$. Moreover, when aligning the model to refuse\nunsafe prompts, we show that such displacement can unintentionally lead to\nunalignment, by shifting probability mass from preferred refusal responses to\nharmful responses (e.g., reducing the refusal rate of Llama-3-8B-Instruct from\n74.4% to 33.4%). We theoretically characterize that likelihood displacement is\ndriven by preferences that induce similar embeddings, as measured by a centered\nhidden embedding similarity (CHES) score. Empirically, the CHES score enables\nidentifying which training samples contribute most to likelihood displacement\nin a given dataset. Filtering out these samples effectively mitigated\nunintentional unalignment in our experiments. More broadly, our results\nhighlight the importance of curating data with sufficiently distinct\npreferences, for which we believe the CHES score may prove valuable."}
{"id": "2410.12735", "pdf": "https://arxiv.org/pdf/2410.12735.pdf", "abs": "https://arxiv.org/abs/2410.12735", "title": "CREAM: Consistency Regularized Self-Rewarding Language Models", "authors": ["Zhaoyang Wang", "Weilei He", "Zhiyuan Liang", "Xuchao Zhang", "Chetan Bansal", "Ying Wei", "Weitong Zhang", "Huaxiu Yao"], "categories": ["cs.LG", "cs.CL"], "comment": "To appear at ICLR 2025", "summary": "Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe consistency of rewards across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM."}
{"id": "2410.18252", "pdf": "https://arxiv.org/pdf/2410.18252.pdf", "abs": "https://arxiv.org/abs/2410.18252", "title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models", "authors": ["Michael Noukhovitch", "Shengyi Huang", "Sophie Xhonneux", "Arian Hosseini", "Rishabh Agarwal", "Aaron Courville"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "accepted at ICLR 2025, code at\n  https://github.com/mnoukhov/async_rlhf, integrated into the open-instruct\n  library https://github.com/allenai/open-instruct", "summary": "The dominant paradigm for RLHF is online and on-policy RL: synchronously\ngenerating from the large language model (LLM) policy, labelling with a reward\nmodel, and learning using feedback on the LLM's own outputs. While performant,\nthis paradigm is computationally inefficient. Inspired by classical deep RL\nliterature, we propose separating generation and learning in RLHF. This enables\nasynchronous generation of new samples while simultaneously training on old\nsamples, leading to faster training and more compute-optimal scaling. However,\nasynchronous training relies on an underexplored regime, online but off-policy\nRLHF: learning on samples from previous iterations of our model which give a\nworse training signal. We tackle the fundamental challenge in this regime: how\nmuch off-policyness can we tolerate for asynchronous training to speed up\nlearning but maintain performance? Among several RLHF algorithms we test,\nonline DPO is found to be most robust to off-policy data, and robustness\nincreases with the scale of the policy model. We study further compute\noptimizations for asynchronous RLHF but find that they come at a performance\ncost, giving rise to a trade-off. We verify the scalability of asynchronous\nRLHF by training a general-purpose chatbot from LLaMA 3.1 8B on an\ninstruction-following task ~40% faster than a synchronous run while matching\nfinal performance. Finally, we extend our results to math and reasoning to\ndemonstrate asynchronous RL can finetune Rho 1B on GSM8k ~70% faster while\nmatching synchronous accuracy."}
{"id": "2410.21465", "pdf": "https://arxiv.org/pdf/2410.21465.pdf", "abs": "https://arxiv.org/abs/2410.21465", "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference", "authors": ["Hanshi Sun", "Li-Wen Chang", "Wenlei Bao", "Size Zheng", "Ningxin Zheng", "Xin Liu", "Harry Dong", "Yuejie Chi", "Beidi Chen"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."}
{"id": "2411.05060", "pdf": "https://arxiv.org/pdf/2411.05060.pdf", "abs": "https://arxiv.org/abs/2411.05060", "title": "A Guide to Misinformation Detection Data and Evaluation", "authors": ["Camille Thibault", "Jacob-Junqi Tian", "Gabrielle Peloquin-Skulski", "Taylor Lynn Curtis", "James Zhou", "Florence Laflamme", "Yuxiang Guan", "Reihaneh Rabbany", "Jean-François Godbout", "Kellin Pelrine"], "categories": ["cs.SI", "cs.CL", "cs.CY"], "comment": null, "summary": "Misinformation is a complex societal issue, and mitigating solutions are\ndifficult to create due to data deficiencies. To address this, we have curated\nthe largest collection of (mis)information datasets in the literature, totaling\n75. From these, we evaluated the quality of 36 datasets that consist of\nstatements or claims, as well as the 9 datasets that consist of data in purely\nparagraph form. We assess these datasets to identify those with solid\nfoundations for empirical work and those with flaws that could result in\nmisleading and non-generalizable results, such as spurious correlations, or\nexamples that are ambiguous or otherwise impossible to assess for veracity. We\nfind the latter issue is particularly severe and affects most datasets in the\nliterature. We further provide state-of-the-art baselines on all these\ndatasets, but show that regardless of label quality, categorical labels may no\nlonger give an accurate evaluation of detection model performance. Finally, we\npropose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the\nfield toward systemic solutions rather than inadvertently propagating issues in\nevaluation. Overall, this guide aims to provide a roadmap for higher quality\ndata and better grounded evaluations, ultimately improving research in\nmisinformation detection. All datasets and other artifacts are available at\nmisinfo-datasets.complexdatalab.com."}
{"id": "2501.15579", "pdf": "https://arxiv.org/pdf/2501.15579.pdf", "abs": "https://arxiv.org/abs/2501.15579", "title": "An Explainable Biomedical Foundation Model via Large-Scale Concept-Enhanced Vision-Language Pre-training", "authors": ["Yuxiang Nie", "Sunan He", "Yequan Bie", "Yihui Wang", "Zhixuan Chen", "Shu Yang", "Zhiyuan Cai", "Hongmei Wang", "Xi Wang", "Luyang Luo", "Mingxiang Wu", "Xian Wu", "Ronald Cheong Kin Chan", "Yuk Ming Lau", "Yefeng Zheng", "Pranav Rajpurkar", "Hao Chen"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The clinical adoption of artificial intelligence (AI) in medical imaging\nrequires models that are both diagnostically accurate and interpretable to\nclinicians. While current multimodal biomedical foundation models prioritize\nperformance, their black-box nature hinders explaining the decision-making\nprocess in clinically meaningful concepts. Here, we present ConceptCLIP, the\nfirst explainable biomedical foundation model that achieves state-of-the-art\ndiagnostic accuracy while delivering human-interpretable explanations across\ndiverse imaging modalities. We curate MedConcept-23M, the largest pre-training\ndataset comprising 23 million image-text-concept triplets across diverse\nmedical modalities, where clinical concepts are derived from the Unified\nMedical Language System. Leveraging this dataset, we develop ConceptCLIP\nthrough a novel dual-alignment approach that simultaneously learns global\nimage-text representations and fine-grained region-concept associations for\nprecise and interpretable medical image analysis. We curate the most extensive\nevaluation benchmark for multimodal biomedical foundation models, covering 52\nclinical tasks spanning 10 imaging modalities. Extensive experiments\ndemonstrate that ConceptCLIP outperforms existing state-of-the-art multimodal\nbiomedical foundation models. Importantly, ConceptCLIP demonstrates superior\ndiagnostic performance while providing human-understandable explanations\nvalidated by clinical experts. As the first precise and interpretable\nbiomedical foundation model, ConceptCLIP represents a critical milestone toward\nthe widespread clinical adoption of AI, thereby advancing trustworthy AI in\nmedicine."}
{"id": "2502.09573", "pdf": "https://arxiv.org/pdf/2502.09573.pdf", "abs": "https://arxiv.org/abs/2502.09573", "title": "Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering", "authors": ["Mark Beliaev", "Victor Yang", "Madhura Raju", "Jiachen Sun", "Xinghai Hu"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "9 pages", "summary": "In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification."}
{"id": "2502.11678", "pdf": "https://arxiv.org/pdf/2502.11678.pdf", "abs": "https://arxiv.org/abs/2502.11678", "title": "Exploring LLM-based Student Simulation for Metacognitive Cultivation", "authors": ["Haoxuan Li", "Jifan Yu", "Xin Cong", "Yang Dang", "Daniel Zhang-li", "Yisi Zhan", "Huiqin Liu", "Zhiyuan Liu"], "categories": ["cs.CY", "cs.CL"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Metacognitive education plays a crucial role in cultivating students'\nself-regulation and reflective thinking, providing essential support for those\nwith learning difficulties through academic advising. Simulating students with\ninsufficient learning capabilities using large language models offers a\npromising approach to refining pedagogical methods without ethical concerns.\nHowever, existing simulations often fail to authentically represent students'\nlearning struggles and face challenges in evaluation due to the lack of\nreliable metrics and ethical constraints in data collection. To address these\nissues, we propose a pipeline for automatically generating and filtering\nhigh-quality simulated student agents. Our approach leverages a two-round\nautomated scoring system validated by human experts and employs a score\npropagation module to obtain more consistent scores across the student graph.\nExperimental results demonstrate that our pipeline efficiently identifies\nhigh-quality student agents, and we discuss the traits that influence the\nsimulation's effectiveness. By simulating students with varying degrees of\nlearning difficulties, our work paves the way for broader applications in\npersonalized learning and educational assessment."}
{"id": "2502.19546", "pdf": "https://arxiv.org/pdf/2502.19546.pdf", "abs": "https://arxiv.org/abs/2502.19546", "title": "Repurposing the scientific literature with vision-language models", "authors": ["Anton Alyakin", "Jaden Stryker", "Daniel Alexander Alber", "Karl L. Sangwon", "Jin Vivian Lee", "Brandon Duderstadt", "Akshay Save", "David Kurland", "Spencer Frome", "Shrutika Singh", "Jeff Zhang", "Eunice Yang", "Ki Yun Park", "Cordelia Orillac", "Aly A. Valliani", "Sean Neifert", "Albert Liu", "Aneek Patel", "Christopher Livia", "Darryl Lau", "Ilya Laufer", "Peter A. Rozman", "Eveline Teresa Hidalgo", "Howard Riina", "Rui Feng", "Todd Hollon", "Yindalon Aphinyanaphongs", "John G. Golfinos", "Laura Snyder", "Eric Leuthardt", "Douglas Kondziolka", "Eric Karl Oermann"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Leading vision-language models (VLMs) are trained on general Internet\ncontent, overlooking scientific journals' rich, domain-specific knowledge.\nTraining on specialty-specific literature could yield high-performance,\ntask-specific tools, enabling generative AI to match generalist models in\nspecialty publishing, educational, and clinical tasks. We created NeuroPubs, a\nmultimodal dataset of 23,000 Neurosurgery Publications articles (134M words,\n78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready\ngraphical abstracts (70% of 100 abstracts) and board-style questions\nindistinguishable from human-written ones (54% of 89,587 questions). We used\nthese questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded,\nrandomized controlled trial, our model demonstrated non-inferiority to then\nstate-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical\nutility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%,\np=0.3797). Our pilot study demonstrates how training generative AI models on\nspecialty-specific journal content - without large-scale internet data -\nresults in high-performance academic and clinical tools, enabling\ndomain-tailored AI across diverse fields."}
{"id": "2502.20601", "pdf": "https://arxiv.org/pdf/2502.20601.pdf", "abs": "https://arxiv.org/abs/2502.20601", "title": "NutriGen: Personalized Meal Plan Generator Leveraging Large Language Models to Enhance Dietary and Nutritional Adherence", "authors": ["Saman Khamesian", "Asiful Arefeen", "Stephanie M. Carpenter", "Hassan Ghasemzadeh"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Maintaining a balanced diet is essential for overall health, yet many\nindividuals struggle with meal planning due to nutritional complexity, time\nconstraints, and lack of dietary knowledge. Personalized food recommendations\ncan help address these challenges by tailoring meal plans to individual\npreferences, habits, and dietary restrictions. However, existing dietary\nrecommendation systems often lack adaptability, fail to consider real-world\nconstraints such as food ingredient availability, and require extensive user\ninput, making them impractical for sustainable and scalable daily use. To\naddress these limitations, we introduce NutriGen, a framework based on large\nlanguage models (LLM) designed to generate personalized meal plans that align\nwith user-defined dietary preferences and constraints. By building a\npersonalized nutrition database and leveraging prompt engineering, our approach\nenables LLMs to incorporate reliable nutritional references like the USDA\nnutrition database while maintaining flexibility and ease-of-use. We\ndemonstrate that LLMs have strong potential in generating accurate and\nuser-friendly food recommendations, addressing key limitations in existing\ndietary recommendation systems by providing structured, practical, and scalable\nmeal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve\nthe lowest percentage errors of 1.55\\% and 3.68\\%, respectively, producing meal\nplans that closely align with user-defined caloric targets while minimizing\ndeviation and improving precision. Additionally, we compared the performance of\nDeepSeek V3 against several established models to evaluate its potential in\npersonalized nutrition planning."}
{"id": "2503.10619", "pdf": "https://arxiv.org/pdf/2503.10619.pdf", "abs": "https://arxiv.org/abs/2503.10619", "title": "Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search", "authors": ["Andy Zhou"], "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": "Accepted to ICLR 2025 Trustworthy LLM", "summary": "We introduce Siege, a multi-turn adversarial framework that models the\ngradual erosion of Large Language Model (LLM) safety through a tree search\nperspective. Unlike single-turn jailbreaks that rely on one meticulously\nengineered prompt, Siege expands the conversation at each turn in a\nbreadth-first fashion, branching out multiple adversarial prompts that exploit\npartial compliance from previous responses. By tracking these incremental\npolicy leaks and re-injecting them into subsequent queries, Siege reveals how\nminor concessions can accumulate into fully disallowed outputs. Evaluations on\nthe JailbreakBench dataset show that Siege achieves a 100% success rate on\nGPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries\nthan baselines such as Crescendo or GOAT. This tree search methodology offers\nan in-depth view of how model safeguards degrade over successive dialogue\nturns, underscoring the urgency of robust multi-turn testing procedures for\nlanguage models."}
{"id": "2504.03947", "pdf": "https://arxiv.org/pdf/2504.03947.pdf", "abs": "https://arxiv.org/abs/2504.03947", "title": "Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking", "authors": ["Chris Samarinas", "Hamed Zamani"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We present a novel approach for training small language models for\nreasoning-intensive document ranking that combines knowledge distillation with\nreinforcement learning optimization. While existing methods often rely on\nexpensive human annotations or large black-box language models, our methodology\nleverages web data and a teacher LLM to automatically generate high-quality\ntraining examples with relevance explanations. By framing document ranking as a\nreinforcement learning problem and incentivizing explicit reasoning\ncapabilities, we train a compact 3B parameter language model that achieves\nstate-of-the-art performance on the BRIGHT benchmark. Our model ranks third on\nthe leaderboard while using substantially fewer parameters than other\napproaches, outperforming models that are over 20 times larger. Through\nextensive experiments, we demonstrate that generating explanations during\ninference, rather than directly predicting relevance scores, enables more\neffective reasoning with smaller language models. The self-supervised nature of\nour method offers a scalable and interpretable solution for modern information\nretrieval systems."}
{"id": "2504.04736", "pdf": "https://arxiv.org/pdf/2504.04736.pdf", "abs": "https://arxiv.org/abs/2504.04736", "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use", "authors": ["Anna Goldie", "Azalia Mirhoseini", "Hao Zhou", "Irene Cai", "Christopher D. Manning"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning has been shown to improve the performance of large\nlanguage models. However, traditional approaches like RLHF or RLAIF treat the\nproblem as single-step. As focus shifts toward more complex reasoning and\nagentic tasks, language models must take multiple steps of text generation,\nreasoning and environment interaction before generating a solution. We propose\na synthetic data generation and RL methodology targeting multi-step\noptimization scenarios. This approach, called Step-Wise Reinforcement Learning\n(SWiRL), iteratively generates multi-step reasoning and tool use data, and then\nlearns from that data. It employs a simple step-wise decomposition that breaks\neach multi-step trajectory into multiple sub-trajectories corresponding to each\naction by the original model. It then applies synthetic data filtering and RL\noptimization on these sub-trajectories. We evaluated SWiRL on a number of\nmulti-step tool use, question answering, and mathematical reasoning tasks. Our\nexperiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%,\n14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA,\nMuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits\ngeneralization across tasks: for example, training only on HotPotQA (text\nquestion-answering) improves zero-shot performance on GSM8K (a math dataset) by\na relative 16.9%."}
{"id": "2504.07089", "pdf": "https://arxiv.org/pdf/2504.07089.pdf", "abs": "https://arxiv.org/abs/2504.07089", "title": "OmniCaptioner: One Captioner to Rule Them All", "authors": ["Yiting Lu", "Jiakang Yuan", "Zhen Li", "Shitian Zhao", "Qi Qin", "Xinyue Li", "Le Zhuo", "Licheng Wen", "Dongyang Liu", "Yuewen Cao", "Xiangchao Yan", "Xin Li", "Tianshuo Peng", "Shufei Zhang", "Botian Shi", "Tao Chen", "Zhibo Chen", "Lei Bai", "Bo Zhang", "Peng Gao"], "categories": ["cs.CV", "cs.CL"], "comment": "More visualizations on Homepage:\n  https://alpha-innovator.github.io/OmniCaptioner-project-page and Official\n  code: https://github.com/Alpha-Innovator/OmniCaptioner", "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities."}
{"id": "2504.15280", "pdf": "https://arxiv.org/pdf/2504.15280.pdf", "abs": "https://arxiv.org/abs/2504.15280", "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs", "authors": ["Chun-Hsiao Yeh", "Chenyu Wang", "Shengbang Tong", "Ta-Ying Cheng", "Ruoyu Wang", "Tianzhe Chu", "Yuexiang Zhai", "Yubei Chen", "Shenghua Gao", "Yi Ma"], "categories": ["cs.CV", "cs.CL"], "comment": "Project page: https://danielchyeh.github.io/All-Angles-Bench/", "summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/."}
{"id": "2504.17834", "pdf": "https://arxiv.org/pdf/2504.17834.pdf", "abs": "https://arxiv.org/abs/2504.17834", "title": "Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection", "authors": ["Haokai Zhang", "Shengtao Zhang", "Zijian Cai", "Heng Wang", "Ruixuan Zhu", "Zinan Zeng", "Minnan Luo"], "categories": ["cs.IR", "cs.CL"], "comment": "11 pages, 6 figures, under review", "summary": "Spoilers in movie reviews are important on platforms like IMDb and Rotten\nTomatoes, offering benefits and drawbacks. They can guide some viewers' choices\nbut also affect those who prefer no plot details in advance, making effective\nspoiler detection essential. Existing spoiler detection methods mainly analyze\nreview text, often overlooking the impact of movie genres and user bias,\nlimiting their effectiveness. To address this, we analyze movie review data,\nfinding genre-specific variations in spoiler rates and identifying that certain\nusers are more likely to post spoilers. Based on these findings, we introduce a\nnew spoiler detection framework called GUSD (The code is available at\nhttps://github.com/AI-explorer-123/GUSD) (Genre-aware and User-specific Spoiler\nDetection), which incorporates genre-specific data and user behavior bias. User\nbias is calculated through dynamic graph modeling of review history.\nAdditionally, the R2GFormer module combines RetGAT (Retentive Graph Attention\nNetwork) for graph information and GenreFormer for genre-specific aggregation.\nThe GMoE (Genre-Aware Mixture of Experts) model further assigns reviews to\nspecialized experts based on genre. Extensive testing on benchmark datasets\nshows that GUSD achieves state-of-the-art results. This approach advances\nspoiler detection by addressing genre and user-specific patterns, enhancing\nuser experience on movie review platforms."}
