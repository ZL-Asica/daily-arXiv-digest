{"id": "2508.10903", "pdf": "https://arxiv.org/pdf/2508.10903.pdf", "abs": "https://arxiv.org/abs/2508.10903", "title": "How do Data Journalists Design Maps to Tell Stories?", "authors": ["Arlindo Gomes", "Emilly Brito", "Luis Morais", "Nivan Ferreira"], "categories": ["cs.HC", "cs.CY"], "comment": "IEEE VIS 2025", "summary": "Maps are essential to news media as they provide a familiar way to convey\nspatial context and present engaging narratives. However, the design of\njournalistic maps may be challenging, as editorial teams need to balance\nmultiple aspects, such as aesthetics, the audience's expected data literacy,\ntight publication deadlines, and the team's technical skills. Data journalists\noften come from multiple areas and lack a cartography, data visualization, and\ndata science background, limiting their competence in creating maps. While\nprevious studies have examined spatial visualizations in data stories, this\nresearch seeks to gain a deeper understanding of the map design process\nemployed by news outlets. To achieve this, we strive to answer two specific\nresearch questions: what is the design space of journalistic maps? and how do\neditorial teams produce journalistic map articles? To answer the first one, we\ncollected and analyzed a large corpus of 462 journalistic maps used in news\narticles from five major news outlets published over three months. As a result,\nwe created a design space comprised of eight dimensions that involved both\nproperties describing the articles' aspects and the visual/interactive features\nof maps. We approach the second research question via semi-structured\ninterviews with four data journalists who create data-driven articles daily.\nThrough these interviews, we identified the most common design rationales made\nby editorial teams and potential gaps in current practices. We also collected\nthe practitioners' feedback on our design space to externally validate it. With\nthese results, we aim to provide researchers and journalists with empirical\ndata to design and study journalistic maps."}
{"id": "2508.10907", "pdf": "https://arxiv.org/pdf/2508.10907.pdf", "abs": "https://arxiv.org/abs/2508.10907", "title": "Designing for Engaging Communication Between Parents and Young Adult Children Through Shared Music Experiences", "authors": ["Euihyeok Lee", "Souneil Park", "Jin Yu", "Seungchul Lee", "Seungwoo Kang"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "This paper aims to foster social interaction between parents and young adult\nchildren living apart via music. Our approach transforms their music-listening\nmoment into an opportunity to listen to the other's favorite songs and enrich\ninteraction in their daily lives. To this end, we explore the current practice\nand needs of parent-child communication and the experience and perception of\nmusic-mediated interaction. Based on the findings, we developed DJ-Fam, a\nmobile application that enables parents and children to listen to their\nfavorite songs and use them as conversation starters to foster parent-child\ninteraction. From our deployment study with seven families over four weeks in\nSouth Korea, we show the potential of DJ-Fam to influence parent-child\ninteraction and their mutual understanding and relationship positively.\nSpecifically, DJ-Fam considerably increases the frequency of communication and\ndiversifies the communication channels and topics, all of which are\nsatisfactory to the participants."}
{"id": "2508.10911", "pdf": "https://arxiv.org/pdf/2508.10911.pdf", "abs": "https://arxiv.org/abs/2508.10911", "title": "Uncovering Latent Connections in Indigenous Heritage: Semantic Pipelines for Cultural Preservation in Brazil", "authors": ["Luis Vitor Zerkowski", "Nina S. T. Hirata"], "categories": ["cs.HC", "cs.CY", "cs.LG", "I.2.m"], "comment": "8 tables, 7 figures, submitted to AAAI2026", "summary": "Indigenous communities face ongoing challenges in preserving their cultural\nheritage, particularly in the face of systemic marginalization and urban\ndevelopment. In Brazil, the Museu Nacional dos Povos Indigenas through the\nTainacan platform hosts the country's largest online collection of Indigenous\nobjects and iconographies, providing a critical resource for cultural\nengagement. Using publicly available data from this repository, we present a\ndata-driven initiative that applies artificial intelligence to enhance\naccessibility, interpretation, and exploration. We develop two semantic\npipelines: a visual pipeline that models image-based similarity and a textual\npipeline that captures semantic relationships from item descriptions. These\nembedding spaces are projected into two dimensions and integrated into an\ninteractive visualization tool we also developed. In addition to\nsimilarity-based navigation, users can explore the collection through temporal\nand geographic lenses, enabling both semantic and contextualized perspectives.\nThe system supports curatorial tasks, aids public engagement, and reveals\nlatent connections within the collection. This work demonstrates how AI can\nethically contribute to cultural preservation practices."}
{"id": "2508.10914", "pdf": "https://arxiv.org/pdf/2508.10914.pdf", "abs": "https://arxiv.org/abs/2508.10914", "title": "Generation and Evaluation in the Human Invention Process through the Lens of Game Design", "authors": ["Katherine M. Collins", "Graham Todd", "Cedegao E. Zhang", "Adrian Weller", "Julian Togelius", "Junyi Chu", "Lionel Wong", "Thomas L. Griffiths", "Joshua B. Tenenbaum"], "categories": ["cs.HC"], "comment": "CogSci conference non-archival paper", "summary": "The human ability to learn rules and solve problems has been a central\nconcern of cognitive science research since the field's earliest days. But we\ndo not just follow rules and solve problems given to us by others: we modify\nthose rules, create new problems, and set new goals and tasks for ourselves and\nothers. Arguably, even more than rule following and problem solving, human\nintelligence is about creatively breaking and stretching the rules, changing\nthe game, and inventing new problems worth thinking about. Creating a good rule\nor a good problem depends not just on the ideas one can think up but on how one\nevaluates such proposals. Here, we study invention through the lens of game\ndesign. We focus particularly on the early stages of novice, \"everyday\" game\ncreation, where the stakes are low. We draw on a dataset of over 450 human\ncreated games, created by participants who saw an initial seed set of\ntwo-player grid-based strategy games. We consider two different cognitive\nmechanisms that may be at work during the early processes of intuitive game\ninvention: an associative proposal based on previous games one has seen and\ncompute-bounded model-based evaluation that an everyday game creator may use to\nrefine their initial draft proposals. In our preliminary work, we conduct a\nmodel-based analysis of how people invented new games based on prior experience\nand find that generated games are best described by a model which incorporates\nmodel-based estimates of game quality at a population level. Our work points to\nhow human invention is based not only on what people propose, but how they\nevaluate and offers a computational toolkit to scale empirical studies of\nmodel-based simulation in open-ended human innovation."}
{"id": "2508.10904", "pdf": "https://arxiv.org/pdf/2508.10904.pdf", "abs": "https://arxiv.org/abs/2508.10904", "title": "A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation", "authors": ["Jie Lei", "Ruofan Jia", "J. Andrew Zhang", "Hao Zhang"], "categories": ["cs.CL", "cs.AR", "cs.PL"], "comment": "15 pages, 6 figures", "summary": "In wireless communication systems, stringent requirements such as ultra-low\nlatency and power consumption have significantly increased the demand for\nefficient algorithm-to-hardware deployment. However, a persistent and\nsubstantial gap remains between algorithm design and hardware implementation.\nBridging this gap traditionally requires extensive domain expertise and\ntime-consuming manual development, due to fundamental mismatches between\nhigh-level programming languages like MATLAB and hardware description languages\n(HDLs) such as Verilog-in terms of memory access patterns, data processing\nmanners, and datatype representations. To address this challenge, we propose\nA2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large\nlanguage models (LLMs), designed to enable agile and reliable\nalgorithm-to-hardware translation. A2HCoder introduces a hierarchical framework\nthat enhances both robustness and interpretability while suppressing common\nhallucination issues in LLM-generated code. In the horizontal dimension,\nA2HCoder decomposes complex algorithms into modular functional blocks,\nsimplifying code generation and improving consistency. In the vertical\ndimension, instead of relying on end-to-end generation, A2HCoder performs\nstep-by-step, fine-grained translation, leveraging external toolchains such as\nMATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured\nprocess significantly mitigates hallucinations and ensures hardware-level\ncorrectness. We validate A2HCoder through a real-world deployment case in the\n5G wireless communication domain, demonstrating its practicality, reliability,\nand deployment efficiency."}
{"id": "2508.10916", "pdf": "https://arxiv.org/pdf/2508.10916.pdf", "abs": "https://arxiv.org/abs/2508.10916", "title": "Multimodal Quantitative Measures for Multiparty Behaviour Evaluation", "authors": ["Ojas Shirekar", "Wim Pouw", "Chenxu Hao", "Vrushank Phadnis", "Thabo Beeler", "Chirag Raman"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.MA"], "comment": null, "summary": "Digital humans are emerging as autonomous agents in multiparty interactions,\nyet existing evaluation metrics largely ignore contextual coordination\ndynamics. We introduce a unified, intervention-driven framework for objective\nassessment of multiparty social behaviour in skeletal motion data, spanning\nthree complementary dimensions: (1) synchrony via Cross-Recurrence\nQuantification Analysis, (2) temporal alignment via Multiscale Empirical Mode\nDecompositionbased Beat Consistency, and (3) structural similarity via Soft\nDynamic Time Warping. We validate metric sensitivity through three\ntheory-driven perturbations -- gesture kinematic dampening, uniform\nspeech-gesture delays, and prosodic pitch-variance reduction-applied to\n$\\approx 145$ 30-second thin slices of group interactions from the DnD dataset.\nMixed-effects analyses reveal predictable, joint-independent shifts: dampening\nincreases CRQA determinism and reduces beat consistency, delays weaken\ncross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A\ncomplementary perception study ($N=27$) compares judgments of full-video and\nskeleton-only renderings to quantify representation effects. Our three measures\ndeliver orthogonal insights into spatial structure, timing alignment, and\nbehavioural variability. Thereby forming a robust toolkit for evaluating and\nrefining socially intelligent agents. Code available on\n\\href{https://github.com/tapri-lab/gig-interveners}{GitHub}."}
{"id": "2508.10906", "pdf": "https://arxiv.org/pdf/2508.10906.pdf", "abs": "https://arxiv.org/abs/2508.10906", "title": "PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins", "authors": ["Sihan Chen", "John P. Lalor", "Yi Yang", "Ahmed Abbasi"], "categories": ["cs.CL"], "comment": "Presented at the Generation, Evaluation & Metrics (GEM) Workshop at\n  ACL 2025", "summary": "While large language models (LLMs) afford new possibilities for user modeling\nand approximation of human behaviors, they often fail to capture the\nmultidimensional nuances of individual users. In this work, we introduce\nPersonaTwin, a multi-tier prompt conditioning framework that builds adaptive\ndigital twins by integrating demographic, behavioral, and psychometric data.\nUsing a comprehensive data set in the healthcare context of more than 8,500\nindividuals, we systematically benchmark PersonaTwin against standard LLM\noutputs, and our rigorous evaluation unites state-of-the-art text similarity\nmetrics with dedicated demographic parity assessments, ensuring that generated\nresponses remain accurate and unbiased. Experimental results show that our\nframework produces simulation fidelity on par with oracle settings. Moreover,\ndownstream models trained on persona-twins approximate models trained on\nindividuals in terms of prediction and fairness metrics across both\nGPT-4o-based and Llama-based models. Together, these findings underscore the\npotential for LLM digital twin-based approaches in producing realistic and\nemotionally nuanced user simulations, offering a powerful tool for personalized\ndigital user modeling and behavior analysis."}
{"id": "2508.10917", "pdf": "https://arxiv.org/pdf/2508.10917.pdf", "abs": "https://arxiv.org/abs/2508.10917", "title": "Managing the unexpected: Operator behavioural data and its value in predicting correct alarm responses", "authors": ["Chidera W. Amazu", "Joseph Mietkiewicz", "Ammar N. Abbas", "Gabriele Baldissone", "Davide Fissore", "Micaela Demichela", "Anders L. Madsen", "Maria Chiara Leva"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Data from psychophysiological measures can offer new insight into control\nroom operators' behaviour, cognition, and mental workload status. This can be\nparticularly helpful when combined with appraisal of capacity to respond to\npossible critical plant conditions (i.e. critical alarms response scenarios).\nHowever, wearable physiological measurement tools such as eye tracking and EEG\ncaps can be perceived as intrusive and not suitable for usage in daily\noperations. Therefore, this article examines the potential of using real-time\ndata from process and operator-system interactions during abnormal scenarios\nthat can be recorded and retrieved from the distributed control system's\nhistorian or process log, and their capacity to provide insight into operator\nbehavior and predict their response outcomes, without intruding on daily tasks.\nData for this study were obtained from a design of experiment using a\nformaldehyde production plant simulator and four human-in-the-loop experimental\nsupport configurations. A comparison between the different configurations in\nterms of both behaviour and performance is presented in this paper. A step-wise\nlogistic regression and a Bayesian network models were used to achieve this\nobjective. The results identified some predictive metrics and the paper discuss\ntheir value as precursor or predictor of overall system performance in alarm\nresponse scenarios. Knowledge of relevant and predictive behavioural metrics\naccessible in real time can better equip decision-makers to predict outcomes\nand provide timely support measures for operators."}
{"id": "2508.10925", "pdf": "https://arxiv.org/pdf/2508.10925.pdf", "abs": "https://arxiv.org/abs/2508.10925", "title": "gpt-oss-120b & gpt-oss-20b Model Card", "authors": ["OpenAI", ":", "Sandhini Agarwal", "Lama Ahmad", "Jason Ai", "Sam Altman", "Andy Applebaum", "Edwin Arbus", "Rahul K. Arora", "Yu Bai", "Bowen Baker", "Haiming Bao", "Boaz Barak", "Ally Bennett", "Tyler Bertao", "Nivedita Brett", "Eugene Brevdo", "Greg Brockman", "Sebastien Bubeck", "Che Chang", "Kai Chen", "Mark Chen", "Enoch Cheung", "Aidan Clark", "Dan Cook", "Marat Dukhan", "Casey Dvorak", "Kevin Fives", "Vlad Fomenko", "Timur Garipov", "Kristian Georgiev", "Mia Glaese", "Tarun Gogineni", "Adam Goucher", "Lukas Gross", "Katia Gil Guzman", "John Hallman", "Jackie Hehir", "Johannes Heidecke", "Alec Helyar", "Haitang Hu", "Romain Huet", "Jacob Huh", "Saachi Jain", "Zach Johnson", "Chris Koch", "Irina Kofman", "Dominik Kundel", "Jason Kwon", "Volodymyr Kyrylov", "Elaine Ya Le", "Guillaume Leclerc", "James Park Lennon", "Scott Lessans", "Mario Lezcano-Casado", "Yuanzhi Li", "Zhuohan Li", "Ji Lin", "Jordan Liss", "Lily", "Liu", "Jiancheng Liu", "Kevin Lu", "Chris Lu", "Zoran Martinovic", "Lindsay McCallum", "Josh McGrath", "Scott McKinney", "Aidan McLaughlin", "Song Mei", "Steve Mostovoy", "Tong Mu", "Gideon Myles", "Alexander Neitz", "Alex Nichol", "Jakub Pachocki", "Alex Paino", "Dana Palmie", "Ashley Pantuliano", "Giambattista Parascandolo", "Jongsoo Park", "Leher Pathak", "Carolina Paz", "Ludovic Peran", "Dmitry Pimenov", "Michelle Pokrass", "Elizabeth Proehl", "Huida Qiu", "Gaby Raila", "Filippo Raso", "Hongyu Ren", "Kimmy Richardson", "David Robinson", "Bob Rotsted", "Hadi Salman", "Suvansh Sanjeev", "Max Schwarzer", "D. Sculley", "Harshit Sikchi", "Kendal Simon", "Karan Singhal", "Yang Song", "Dane Stuckey", "Zhiqing Sun", "Philippe Tillet", "Sam Toizer", "Foivos Tsimpourlas", "Nikhil Vyas", "Eric Wallace", "Xin Wang", "Miles Wang", "Olivia Watkins", "Kevin Weil", "Amy Wendling", "Kevin Whinnery", "Cedric Whitney", "Hannah Wong", "Lin Yang", "Yu Yang", "Michihiro Yasunaga", "Kristen Ying", "Wojciech Zaremba", "Wenting Zhan", "Cyril Zhang", "Brian Zhang", "Eddie Zhang", "Shengjia Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models\nthat push the frontier of accuracy and inference cost. The models use an\nefficient mixture-of-expert transformer architecture and are trained using\nlarge-scale distillation and reinforcement learning. We optimize the models to\nhave strong agentic capabilities (deep research browsing, python tool use, and\nsupport for developer-provided functions), all while using a rendered chat\nformat that enables clear instruction following and role delineation. Both\nmodels achieve strong results on benchmarks ranging from mathematics, coding,\nand safety. We release the model weights, inference implementations, tool\nenvironments, and tokenizers under an Apache 2.0 license to enable broad use\nand further research."}
{"id": "2508.10919", "pdf": "https://arxiv.org/pdf/2508.10919.pdf", "abs": "https://arxiv.org/abs/2508.10919", "title": "Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?", "authors": ["Mohammed Saqr", "Kamila Misiejuk", "Sonsoles López-Pernas"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "While research on human-AI collaboration exists, it mainly examined language\nlearning and used traditional counting methods with little attention to\nevolution and dynamics of collaboration on cognitively demanding tasks. This\nstudy examines human-AI interactions while solving a complex problem.\nStudent-AI interactions were qualitatively coded and analyzed with transition\nnetwork analysis, sequence analysis and partial correlation networks as well as\ncomparison of frequencies using chi-square and Person-residual shaded Mosaic\nplots to map interaction patterns, their evolution, and their relationship to\nproblem complexity and student performance. Findings reveal a dominant\nInstructive pattern with interactions characterized by iterative ordering\nrather than collaborative negotiation. Oftentimes, students engaged in long\nthreads that showed misalignment between their prompts and AI output that\nexemplified a lack of synergy that challenges the prevailing assumptions about\nLLMs as collaborative partners. We also found no significant correlations\nbetween assignment complexity, prompt length, and student grades suggesting a\nlack of cognitive depth, or effect of problem difficulty. Our study indicates\nthat the current LLMs, optimized for instruction-following rather than\ncognitive partnership, compound their capability to act as cognitively\nstimulating or aligned collaborators. Implications for designing AI systems\nthat prioritize cognitive alignment and collaboration are discussed."}
{"id": "2508.10927", "pdf": "https://arxiv.org/pdf/2508.10927.pdf", "abs": "https://arxiv.org/abs/2508.10927", "title": "Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News", "authors": ["Jiaxin Pei", "Soumya Vadlamannati", "Liang-Kang Huang", "Daniel Preotiuc-Pietro", "Xinyu Hua"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Identifying risks associated with a company is important to investors and the\nwell-being of the overall financial market. In this study, we build a\ncomputational framework to automatically extract company risk factors from news\narticles. Our newly proposed schema comprises seven distinct aspects, such as\nsupply chain, regulations, and competitions. We sample and annotate 744 news\narticles and benchmark various machine learning models. While large language\nmodels have achieved huge progress in various types of NLP tasks, our\nexperiment shows that zero-shot and few-shot prompting state-of-the-art LLMs\n(e.g. LLaMA-2) can only achieve moderate to low performances in identifying\nrisk factors. And fine-tuned pre-trained language models are performing better\non most of the risk factors. Using this model, we analyze over 277K Bloomberg\nnews articles and demonstrate that identifying risk factors from news could\nprovide extensive insight into the operations of companies and industries."}
{"id": "2508.11022", "pdf": "https://arxiv.org/pdf/2508.11022.pdf", "abs": "https://arxiv.org/abs/2508.11022", "title": "GhostObjects: Instructing Robots by Manipulating Spatially Aligned Virtual Twins in Augmented Reality", "authors": ["Lauren W. Wang", "Parastoo Abtahi"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Robots are increasingly capable of autonomous operations, yet human\ninteraction remains essential for issuing personalized instructions. Instead of\ndirectly controlling robots through Programming by Demonstration (PbD) or\nteleoperation, we propose giving instructions by interacting with\nGhostObjects-world-aligned, life-size virtual twins of physical objects-in\naugmented reality (AR). By direct manipulation of GhostObjects, users can\nprecisely specify physical goals and spatial parameters, with features\nincluding real-world lasso selection of multiple objects and snapping back to\ndefault positions, enabling tasks beyond simple pick-and-place."}
{"id": "2508.10971", "pdf": "https://arxiv.org/pdf/2508.10971.pdf", "abs": "https://arxiv.org/abs/2508.10971", "title": "Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules", "authors": ["Nasim Shirvani-Mahdavi", "Chengkai Li"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2507.23740", "summary": "Knowledge graphs (KGs) can be enhanced through rule mining; however, the\nresulting logical rules are often difficult for humans to interpret due to\ntheir inherent complexity and the idiosyncratic labeling conventions of\nindividual KGs. This work presents Rule2Text, a comprehensive framework that\nleverages large language models (LLMs) to generate natural language\nexplanations for mined logical rules, thereby improving KG accessibility and\nusability. We conduct extensive experiments using multiple datasets, including\nFreebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the\nogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically\nevaluate several LLMs across a comprehensive range of prompting strategies,\nincluding zero-shot, few-shot, variable type incorporation, and\nChain-of-Thought reasoning. To systematically assess models' performance, we\nconduct a human evaluation of generated explanations on correctness and\nclarity. To address evaluation scalability, we develop and validate an\nLLM-as-a-judge framework that demonstrates strong agreement with human\nevaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,\nand human-in-the-loop feedback, we construct high-quality ground truth\ndatasets, which we use to fine-tune the open-source Zephyr model. Our results\ndemonstrate significant improvements in explanation quality after fine-tuning,\nwith particularly strong gains in the domain-specific dataset. Additionally, we\nintegrate a type inference module to support KGs lacking explicit type\ninformation. All code and data are publicly available at\nhttps://github.com/idirlab/KGRule2NL."}
{"id": "2508.11030", "pdf": "https://arxiv.org/pdf/2508.11030.pdf", "abs": "https://arxiv.org/abs/2508.11030", "title": "Families' Vision of Generative AI Agents for Household Safety Against Digital and Physical Threats", "authors": ["Zikai Wen", "Lanjing Liu", "Yaxing Yao"], "categories": ["cs.HC"], "comment": null, "summary": "As families face increasingly complex safety challenges in digital and\nphysical environments, generative AI (GenAI) presents new opportunities to\nsupport household safety through multiple specialized AI agents. Through a\ntwo-phase qualitative study consisting of individual interviews and\ncollaborative sessions with 13 parent-child dyads, we explored families'\nconceptualizations of GenAI and their envisioned use of AI agents in daily\nfamily life. Our findings reveal that families preferred to distribute\nsafety-related support across multiple AI agents, each embodying a familiar\ncaregiving role: a household manager coordinating routine tasks and mitigating\nrisks such as digital fraud and home accidents; a private tutor providing\npersonalized educational support, including safety education; and a family\ntherapist offering emotional support to address sensitive safety issues such as\ncyberbullying and digital harassment. Families emphasized the need for\nagent-specific privacy boundaries, recognized generational differences in trust\ntoward AI agents, and stressed the importance of maintaining open family\ncommunication alongside the assistance of AI agents. Based on these findings,\nwe propose a multi-agent system design featuring four privacy-preserving\nprinciples: memory segregation, conversational consent, selective data sharing,\nand progressive memory management to help balance safety, privacy, and autonomy\nwithin family contexts."}
{"id": "2508.10995", "pdf": "https://arxiv.org/pdf/2508.10995.pdf", "abs": "https://arxiv.org/abs/2508.10995", "title": "Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling", "authors": ["Tejomay Kishor Padole", "Suyash P Awate", "Pushpak Bhattacharyya"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted as a main conference submission in the European Conference\n  on Artificial Intelligence (ECAI 2025)", "summary": "Masked diffusion language models (MDMs) have recently gained traction as a\nviable generative framework for natural language. This can be attributed to its\nscalability and ease of training compared to other diffusion model paradigms\nfor discrete data, establishing itself as the state-of-the-art\nnon-autoregressive generator for discrete data. Diffusion models, in general,\nhave shown excellent ability to improve the generation quality by leveraging\ninference-time scaling either by increasing the number of denoising steps or by\nusing external verifiers on top of the outputs of each step to guide the\ngeneration. In this work, we propose a verifier-based inference-time scaling\nmethod that aids in finding a better candidate generation during the denoising\nprocess of the MDM. Our experiments demonstrate the application of MDMs for\nstandard text-style transfer tasks and establish MDMs as a better alternative\nto autoregressive language models. Additionally, we show that a simple\nsoft-value-based verifier setup for MDMs using off-the-shelf pre-trained\nembedding models leads to significant gains in generation quality even when\nused on top of typical classifier-free guidance setups in the existing\nliterature."}
{"id": "2508.11052", "pdf": "https://arxiv.org/pdf/2508.11052.pdf", "abs": "https://arxiv.org/abs/2508.11052", "title": "AI That Helps Us Help Each Other: A Proactive System for Scaffolding Mentor-Novice Collaboration in Entrepreneurship Coaching", "authors": ["Evey Jiaxin Huang", "Matthew Easterday", "Elizabeth Gerber"], "categories": ["cs.HC", "cs.AI", "68T35 (Primary), 68U99 (Secondary)", "H.5.2"], "comment": "To appear in CSCW 2025 Volume 9", "summary": "Entrepreneurship requires navigating open-ended, ill-defined problems:\nidentifying risks, challenging assumptions, and making strategic decisions\nunder deep uncertainty. Novice founders often struggle with these metacognitive\ndemands, while mentors face limited time and visibility to provide tailored\nsupport. We present a human-AI coaching system that combines a domain-specific\ncognitive model of entrepreneurial risk with a large language model (LLM) to\nproactively scaffold both novice and mentor thinking. The system proactively\nposes diagnostic questions that challenge novices' thinking and helps both\nnovices and mentors plan for more focused and emotionally attuned meetings.\nCritically, mentors can inspect and modify the underlying cognitive model,\nshaping the logic of the system to reflect their evolving needs. Through an\nexploratory field deployment, we found that using the system supported novice\nmetacognition, helped mentors plan emotionally attuned strategies, and improved\nmeeting depth, intentionality, and focus--while also surfaced key tensions\naround trust, misdiagnosis, and expectations of AI. We contribute design\nprinciples for proactive AI systems that scaffold metacognition and human-human\ncollaboration in complex, ill-defined domains, offering implications for\nsimilar domains like healthcare, education, and knowledge work."}
{"id": "2508.11009", "pdf": "https://arxiv.org/pdf/2508.11009.pdf", "abs": "https://arxiv.org/abs/2508.11009", "title": "SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth", "authors": ["Wenpeng Xing", "Lanyi Wei", "Haixiao Hu", "Rongchang Li", "Mohan Li", "Changting Lin", "Meng Han"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid proliferation of large language models (LLMs) in applications\ntargeting children and adolescents necessitates a fundamental reassessment of\nprevailing AI safety frameworks, which are largely tailored to adult users and\nneglect the distinct developmental vulnerabilities of minors. This paper\nhighlights key deficiencies in existing LLM safety benchmarks, including their\ninadequate coverage of age-specific cognitive, emotional, and social risks\nspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence\n(13--18). To bridge these gaps, we introduce SproutBench, an innovative\nevaluation suite comprising 1,283 developmentally grounded adversarial prompts\ndesigned to probe risks such as emotional dependency, privacy violations, and\nimitation of hazardous behaviors. Through rigorous empirical evaluation of 47\ndiverse LLMs, we uncover substantial safety vulnerabilities, corroborated by\nrobust inter-dimensional correlations (e.g., between Safety and Risk\nPrevention) and a notable inverse relationship between Interactivity and Age\nAppropriateness. These insights yield practical guidelines for advancing\nchild-centric AI design and deployment."}
{"id": "2508.11059", "pdf": "https://arxiv.org/pdf/2508.11059.pdf", "abs": "https://arxiv.org/abs/2508.11059", "title": "Stories and Systems: Educational Interactive Storytelling to Teach Media Literacy and Systemic Thinking", "authors": ["Christian Roth", "Rahmin Bender-Salazar", "Breanne Pitt"], "categories": ["cs.HC", "cs.ET"], "comment": "Under submission (May, 2025)", "summary": "This paper explores how Interactive Digital Narratives (IDNs) can support\nlearners in developing the critical literacies needed to address complex\nsocietal challenges, so-called wicked problems, such as climate change,\npandemics, and social inequality. While digital technologies offer broad access\nto narratives and data, they also contribute to misinformation and the\noversimplification of interconnected issues. IDNs enable learners to navigate\nnonlinear, interactive stories, fostering deeper understanding and engagement.\nWe introduce Systemic Learning IDNs: interactive narrative experiences\nexplicitly designed to help learners explore and reflect on complex systems and\ninterdependencies. To guide their creation and use, we propose the CLASS\nframework, a structured model that integrates systems thinking, design\nthinking, and storytelling. This transdisciplinary approach supports learners\nin developing curiosity, critical thinking, and collaborative problem-solving.\nFocusing on the classroom context, we apply CLASS to two cases, one commercial\nnarrative simulation and one educational prototype, offering a comparative\nanalysis and practical recommendations for future design and implementation. By\ncombining narrative, systems mapping, and participatory design, this paper\nhighlights how IDNs can become powerful tools for transformative,\nsystems-oriented learning in an increasingly complex world."}
{"id": "2508.11017", "pdf": "https://arxiv.org/pdf/2508.11017.pdf", "abs": "https://arxiv.org/abs/2508.11017", "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics", "authors": ["Carter Blum", "Katja Filipova", "Ann Yuan", "Asma Ghandeharioun", "Julian Zimmert", "Fred Zhang", "Jessica Hoffmann", "Tal Linzen", "Martin Wattenberg", "Lucas Dixon", "Mor Geva"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs."}
{"id": "2508.11062", "pdf": "https://arxiv.org/pdf/2508.11062.pdf", "abs": "https://arxiv.org/abs/2508.11062", "title": "Human-in-the-Loop Systems for Adaptive Learning Using Generative AI", "authors": ["Bhavishya Tarun", "Haoze Du", "Dinesh Kannan", "Edward F. Gehringer"], "categories": ["cs.HC", "cs.LG"], "comment": "Accepted for presentation at the Frontiers in Education Conference,\n  Nashville, Tennessee, USA, 2-5 November 2025", "summary": "A Human-in-the-Loop (HITL) approach leverages generative AI to enhance\npersonalized learning by directly integrating student feedback into\nAI-generated solutions. Students critique and modify AI responses using\npredefined feedback tags, fostering deeper engagement and understanding. This\nempowers students to actively shape their learning, with AI serving as an\nadaptive partner. The system uses a tagging technique and prompt engineering to\npersonalize content, informing a Retrieval-Augmented Generation (RAG) system to\nretrieve relevant educational material and adjust explanations in real time.\nThis builds on existing research in adaptive learning, demonstrating how\nstudent-driven feedback loops can modify AI-generated responses for improved\nstudent retention and engagement, particularly in STEM education. Preliminary\nfindings from a study with STEM students indicate improved learning outcomes\nand confidence compared to traditional AI tools. This work highlights AI's\npotential to create dynamic, feedback-driven, and personalized learning\nenvironments through iterative refinement."}
{"id": "2508.11027", "pdf": "https://arxiv.org/pdf/2508.11027.pdf", "abs": "https://arxiv.org/abs/2508.11027", "title": "Hell or High Water: Evaluating Agentic Recovery from External Failures", "authors": ["Andrew Wang", "Sophia Hager", "Adi Asija", "Daniel Khashabi", "Nicholas Andrews"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "As language model agents are applied to real world problems of increasing\ncomplexity, they will be expected to formulate plans across large search\nspaces. If those plans fail for reasons beyond their control, how well do\nlanguage agents search for alternative ways to achieve their goals? We devise a\nspecialized agentic planning benchmark to study this question. Each planning\nproblem is solved via combinations of function calls. The agent searches for\nrelevant functions from a set of over four thousand possibilities, and observes\nenvironmental feedback in the form of function outputs or error messages. Our\nbenchmark confronts the agent with external failures in its workflow, such as\nfunctions that suddenly become unavailable. At the same time, even with the\nintroduction of these failures, we guarantee that the task remains solvable.\nIdeally, an agent's performance on the planning task should not be affected by\nthe presence of external failures. Overall, we find that language agents\nstruggle to formulate and execute backup plans in response to environment\nfeedback. While state-of-the-art models are often able to identify the correct\nfunction to use in the right context, they struggle to adapt to feedback from\nthe environment and often fail to pursue alternate courses of action, even when\nthe search space is artificially restricted. We provide a systematic analysis\nof the failures of both open-source and commercial models, examining the\neffects of search space size, as well as the benefits of scaling model size in\nour setting. Our analysis identifies key challenges for current generative\nmodels as well as promising directions for future work."}
{"id": "2508.11072", "pdf": "https://arxiv.org/pdf/2508.11072.pdf", "abs": "https://arxiv.org/abs/2508.11072", "title": "DriveSimQuest: A VR Driving Simulator and Research Platform on Meta Quest with Unity", "authors": ["Nishanth Chidambaram", "Weichen Liu", "Manas Satish Bedmutha", "Nadir Weibel", "Chen Chen"], "categories": ["cs.HC", "H.5.m; J.m"], "comment": "3 pages, 2 figures, In the Proceedings of the 38th Annual ACM\n  Symposium on User Interface Software and Technology (UIST Adjunct '25),\n  September 28 - October 1, 2025, Busan, Republic of Korea", "summary": "Using head-mounted Virtual Reality (VR) displays to simulate driving is\ncritical to studying driving behavior and designing driver assistance systems.\nBut existing VR driving simulators are often limited to tracking only eye\nmovements. The bulky outside-in tracking setup and Unreal-based architecture\nalso present significant engineering challenges for interaction researchers and\npractitioners. We present DriveSimQuest, a VR driving simulator and research\nplatform built on the Meta Quest Pro and Unity, capable of capturing rich\nbehavioral signals such as gaze, facial expressions, hand activities, and\nfull-body gestures in real-time. DriveSimQuest offers a preliminary,\neasy-to-deploy platform that supports researchers and practitioners in studying\ndrivers' affective states and behaviors, and in designing future context-aware\ndriving assistance systems."}
{"id": "2508.11061", "pdf": "https://arxiv.org/pdf/2508.11061.pdf", "abs": "https://arxiv.org/abs/2508.11061", "title": "BIPOLAR: Polarization-based granular framework for LLM bias evaluation", "authors": ["Martin Pavlíček", "Tomáš Filip", "Petr Sosík"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to exhibit biases in downstream tasks,\nespecially when dealing with sensitive topics such as political discourse,\ngender identity, ethnic relations, or national stereotypes. Although\nsignificant progress has been made in bias detection and mitigation techniques,\ncertain challenges remain underexplored. This study proposes a reusable,\ngranular, and topic-agnostic framework to evaluate polarisation-related biases\nin LLM (both open-source and closed-source). Our approach combines\npolarisation-sensitive sentiment metrics with a synthetically generated\nbalanced dataset of conflict-related statements, using a predefined set of\nsemantic categories.\n  As a case study, we created a synthetic dataset that focusses on the\nRussia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,\nMistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with\na general trend for more positive sentiment toward Ukraine, the framework\nallowed fine-grained analysis with considerable variation between semantic\ncategories, uncovering divergent behavioural patterns among models. Adaptation\nto prompt modifications showed further bias towards preconceived language and\ncitizenship modification.\n  Overall, the framework supports automated dataset generation and fine-grained\nbias assessment, is applicable to a variety of polarisation-driven scenarios\nand topics, and is orthogonal to many other bias-evaluation strategies."}
{"id": "2508.11149", "pdf": "https://arxiv.org/pdf/2508.11149.pdf", "abs": "https://arxiv.org/abs/2508.11149", "title": "Toward Needs-Conscious Design: Co-Designing a Human-Centered Framework for AI-Mediated Communication", "authors": ["Robert Wolfe", "Aayushi Dangol", "JaeWon Kim", "Alexis Hiniker"], "categories": ["cs.HC"], "comment": "Accepted for publication at AIES 2025", "summary": "We introduce Needs-Conscious Design, a human-centered framework for\nAI-mediated communication that builds on the principles of Nonviolent\nCommunication (NVC). We conducted an interview study with N=14 certified NVC\ntrainers and a diary study and co-design with N=13 lay users of online\ncommunication technologies to understand how NVC might inform design that\ncenters human relationships. We define three pillars of Needs-Conscious Design:\nIntentionality, Presence, and Receptiveness to Needs. Drawing on participant\nco-designs, we provide design concepts and illustrative examples for each of\nthese pillars. We further describe a problematic emergent property of\nAI-mediated communication identified by participants, which we call Empathy\nFog, and which is characterized by uncertainty over how much empathy,\nattention, and effort a user has actually invested via an AI-facilitated online\ninteraction. Finally, because even well-intentioned designs may alter user\nbehavior and process emotional data, we provide guiding questions for\nconsentful Needs-Conscious Design, applying an affirmative consent framework\nused in social media contexts. Needs-Conscious Design offers a foundation for\nleveraging AI to facilitate human connection, rather than replacing or\nobscuring it."}
{"id": "2508.11068", "pdf": "https://arxiv.org/pdf/2508.11068.pdf", "abs": "https://arxiv.org/abs/2508.11068", "title": "Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs", "authors": ["Nicolas Goulet", "Alexandre Blondin Massé", "Moussa Abdendi"], "categories": ["cs.CL"], "comment": null, "summary": "Abstract meaning representation (AMR) is a semantic formalism used to\nrepresent the meaning of sentences as directed acyclic graphs. In this paper,\nwe describe how real digital dictionaries can be embedded into AMR directed\ngraphs (digraphs), using state-of-the-art pre-trained large language models.\nThen, we reduce those graphs in a confluent manner, i.e. with transformations\nthat preserve their circuit space. Finally, the properties of these reduces\ndigraphs are analyzed and discussed in relation to the symbol grounding\nproblem."}
{"id": "2508.11150", "pdf": "https://arxiv.org/pdf/2508.11150.pdf", "abs": "https://arxiv.org/abs/2508.11150", "title": "From Misunderstandings to Learning Opportunities: Leveraging Generative AI in Discussion Forums to Support Student Learning", "authors": ["Stanislav Pozdniakov", "Jonathan Brazil", "Oleksandra Poquet", "Stephan Krusche", "Santiago Berrezueta-Guzman", "Shazia Sadiq", "Hassan Khosravi"], "categories": ["cs.HC"], "comment": "Artificial Intelligence in Education (AIED 2025)", "summary": "In the contemporary educational landscape, particularly in large classroom\nsettings, discussion forums have become a crucial tool for promoting\ninteraction and addressing student queries. These forums foster a collaborative\nlearning environment where students engage with both the teaching team and\ntheir peers. However, the sheer volume of content generated in these forums\nposes two significant interconnected challenges: How can we effectively\nidentify common misunderstandings that arise in student discussions? And once\nidentified, how can instructors use these insights to address them effectively?\nThis paper explores the approach to integrating large language models (LLMs)\nand Retrieval-Augmented Generation (RAG) to tackle these challenges. We then\ndemonstrate the approach Misunderstanding to Mastery (M2M) with authentic data\nfrom three computer science courses, involving 1355 students with 2878 unique\nposts, followed by an evaluation with five instructors teaching these courses.\nResults show that instructors found the approach promising and valuable for\nteaching, effectively identifying misunderstandings and generating actionable\ninsights. Instructors highlighted the need for more fine-grained groupings,\nclearer metrics, validation of the created resources, and ethical\nconsiderations around data anonymity."}
{"id": "2508.11120", "pdf": "https://arxiv.org/pdf/2508.11120.pdf", "abs": "https://arxiv.org/abs/2508.11120", "title": "Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning", "authors": ["Lorenzo Jaime Yu Flores", "Junyi Shen", "Xiaoyuan Gu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) enabled the development of AI\nagents that can plan and interact with tools to complete complex tasks.\nHowever, literature on their reliability in real-world applications remains\nlimited. In this paper, we introduce a multi-agent framework for a marketing\ntask: audience curation. To solve this, we introduce a framework called RAMP\nthat iteratively plans, calls tools, verifies the output, and generates\nsuggestions to improve the quality of the audience generated. Additionally, we\nequip the model with a long-term memory store, which is a knowledge base of\nclient-specific facts and past queries. Overall, we demonstrate the use of LLM\nplanning and memory, which increases accuracy by 28 percentage points on a set\nof 88 evaluation queries. Moreover, we show the impact of iterative\nverification and reflection on more ambiguous queries, showing progressively\nbetter recall (roughly +20 percentage points) with more verify/reflect\niterations on a smaller challenge set, and higher user satisfaction. Our\nresults provide practical insights for deploying reliable LLM-based systems in\ndynamic, industry-facing environments."}
{"id": "2508.11278", "pdf": "https://arxiv.org/pdf/2508.11278.pdf", "abs": "https://arxiv.org/abs/2508.11278", "title": "Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas", "authors": ["Francesco Sovrano", "Gabriele Dominici", "Rita Sevastjanova", "Alessandra Stramiglio", "Alberto Bacchelli"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Human cognitive biases in software engineering can lead to costly errors.\nWhile general-purpose AI (GPAI) systems may help mitigate these biases due to\ntheir non-human nature, their training on human-generated data raises a\ncritical question: Do GPAI systems themselves exhibit cognitive biases?\n  To investigate this, we present the first dynamic benchmarking framework to\nevaluate data-induced cognitive biases in GPAI within software engineering\nworkflows. Starting with a seed set of 16 hand-crafted realistic tasks, each\nfeaturing one of 8 cognitive biases (e.g., anchoring, framing) and\ncorresponding unbiased variants, we test whether bias-inducing linguistic cues\nunrelated to task logic can lead GPAI systems from correct to incorrect\nconclusions.\n  To scale the benchmark and ensure realism, we develop an on-demand\naugmentation pipeline relying on GPAI systems to generate task variants that\npreserve bias-inducing cues while varying surface details. This pipeline\nensures correctness (88--99% on average, according to human evaluation),\npromotes diversity, and controls reasoning complexity by leveraging\nProlog-based reasoning and LLM-as-a-judge validation. It also verifies that the\nembedded biases are both harmful and undetectable by logic-based, unbiased\nreasoners.\n  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent\ntendency to rely on shallow linguistic heuristics over deep reasoning. All\nsystems exhibit cognitive biases (ranging from 5.9% to 35% across types), with\nbias sensitivity increasing sharply with task complexity (up to 49%),\nhighlighting critical risks in real-world software engineering deployments."}
{"id": "2508.11133", "pdf": "https://arxiv.org/pdf/2508.11133.pdf", "abs": "https://arxiv.org/abs/2508.11133", "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents", "authors": ["Tomer Wolfson", "Harsh Trivedi", "Mor Geva", "Yoav Goldberg", "Dan Roth", "Tushar Khot", "Ashish Sabharwal", "Reut Tsarfaty"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print", "summary": "Large language models (LLMs) are emerging as a go-to tool for querying\ninformation. However, current LLM benchmarks rarely feature natural questions\nthat are both information-seeking as well as genuinely time-consuming for\nhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural\nand complex questions that require dozens, and at times hundreds, of\nintermediate steps to solve -- far more than any existing QA benchmark. To\nbuild MoNaCo, we developed a decomposed annotation pipeline to elicit and\nmanually answer natural time-consuming questions at scale. Frontier LLMs\nevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and\nhallucinations. Our results underscore the need for reasoning models that\nbetter handle the complexity and sheer breadth of real-world\ninformation-seeking questions -- with MoNaCo providing an effective resource\nfor tracking such progress. The MONACO benchmark, codebase, prompts and models\npredictions are publicly available at: https://tomerwolgithub.github.io/monaco"}
{"id": "2508.11304", "pdf": "https://arxiv.org/pdf/2508.11304.pdf", "abs": "https://arxiv.org/abs/2508.11304", "title": "GulliVR: A Walking-Oriented Technique for Navigation in Virtual Reality Games Based on Virtual Body Resizing", "authors": ["Andrey Krekhov", "Sebastian Cmentowski", "Katharina Emmerich", "Maic Masuch", "Jens Krüger"], "categories": ["cs.HC"], "comment": "author version", "summary": "Virtual reality games are often centered around our feeling of \"being there\".\nThat presence can be significantly enhanced by supporting physical walking.\nAlthough modern virtual reality systems enable room-scale motions, the size of\nour living rooms is not enough to explore vast virtual environments. Developers\nbypass that limitation by adding virtual navigation such as teleportation.\nAlthough such techniques are intended (or designed) to extend but not replace\nnatural walking, what we often observe are nonmoving players beaming to a\nlocation that is one real step ahead. Our navigation metaphor emphasizes\nphysical walking by promoting players into giants on demand to cover large\ndistances. In contrast to flying, our technique proportionally increases the\nmodeled eye distance, preventing cybersickness and creating the feeling of\nbeing in a miniature world. Our evaluations underpin a significantly increased\npresence and walking distance compared to the teleportation approach. Finally,\nwe derive a set of game design implications related to the integration of our\ntechnique."}
{"id": "2508.11163", "pdf": "https://arxiv.org/pdf/2508.11163.pdf", "abs": "https://arxiv.org/abs/2508.11163", "title": "MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering", "authors": ["Hikaru Asano", "Hiroki Ouchi", "Akira Kasuga", "Ryo Yonetani"], "categories": ["cs.CL"], "comment": "23 pages, 12 figures", "summary": "This paper presents MobQA, a benchmark dataset designed to evaluate the\nsemantic understanding capabilities of large language models (LLMs) for human\nmobility data through natural language question answering.\n  While existing models excel at predicting human movement patterns, it remains\nunobvious how much they can interpret the underlying reasons or semantic\nmeaning of those patterns. MobQA provides a comprehensive evaluation framework\nfor LLMs to answer questions about diverse human GPS trajectories spanning\ndaily to weekly granularities. It comprises 5,800 high-quality question-answer\npairs across three complementary question types: factual retrieval (precise\ndata extraction), multiple-choice reasoning (semantic inference), and free-form\nexplanation (interpretive description), which all require spatial, temporal,\nand semantic reasoning. Our evaluation of major LLMs reveals strong performance\non factual retrieval but significant limitations in semantic reasoning and\nexplanation question answering, with trajectory length substantially impacting\nmodel effectiveness. These findings demonstrate the achievements and\nlimitations of state-of-the-art LLMs for semantic mobility\nunderstanding.\\footnote{MobQA dataset is available at\nhttps://github.com/CyberAgentAILab/mobqa.}"}
{"id": "2508.11314", "pdf": "https://arxiv.org/pdf/2508.11314.pdf", "abs": "https://arxiv.org/abs/2508.11314", "title": "Outpace Reality: A Novel Augmented-Walking Technique for Virtual Reality Games", "authors": ["Sebastian Cmentowski", "Fabian Kievelitz", "Jens Krüger"], "categories": ["cs.HC"], "comment": "author version", "summary": "The size of most virtual environments exceeds the tracking space available\nfor physical walking. One solution to this disparity is to extend the available\nwalking range by augmenting users' actual movements. However, the resulting\nincrease in visual flow can easily cause cybersickness. Therefore, we present a\nnovel augmented-walking approach for virtual reality games. Our core concept is\na virtual tunnel that spans the entire travel distance when viewed from the\noutside. However, its interior is only a fraction as long, allowing users to\ncover the distance by real walking. Whereas the tunnel hides the visual flow\nfrom the applied movement acceleration, windows on the tunnel's walls still\nreveal the actual expedited motion. Our evaluation reveals that our approach\navoids cybersickness while enhancing physical activity and preserving presence.\nWe finish our paper with a discussion of the design considerations and\nlimitations of our proposed locomotion technique."}
{"id": "2508.11166", "pdf": "https://arxiv.org/pdf/2508.11166.pdf", "abs": "https://arxiv.org/abs/2508.11166", "title": "Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification", "authors": ["Anusha M D", "Deepthi Vikram", "Bharathi Raja Chakravarthi", "Parameshwar R Hegde"], "categories": ["cs.CL"], "comment": "20 pages, 3 tables, 3 figures. Submitted to Language Resources and\n  Evaluation (Springer)", "summary": "Tulu, a low-resource Dravidian language predominantly spoken in southern\nIndia, has limited computational resources despite its growing digital\npresence. This study presents the first benchmark dataset for Offensive\nLanguage Identification (OLI) in code-mixed Tulu social media content,\ncollected from YouTube comments across various domains. The dataset, annotated\nwith high inter-annotator agreement (Krippendorff's alpha = 0.984), includes\n3,845 comments categorized into four classes: Not Offensive, Not Tulu,\nOffensive Untargeted, and Offensive Targeted. We evaluate a suite of deep\nlearning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based\nvariants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU\nmodel with self-attention achieves the best performance with 82% accuracy and a\n0.81 macro F1-score. Transformer models underperform, highlighting the\nlimitations of multilingual pretraining in code-mixed, under-resourced\ncontexts. This work lays the foundation for further NLP research in Tulu and\nsimilar low-resource, code-mixed languages."}
{"id": "2508.11327", "pdf": "https://arxiv.org/pdf/2508.11327.pdf", "abs": "https://arxiv.org/abs/2508.11327", "title": "The User-first Approach to AI Ethics: Preferences for Ethical Principles in AI Systems across Cultures and Contexts", "authors": ["Benjamin J. Carroll", "Jianlong Zhou", "Paul F. Burke", "Sabine Ammon"], "categories": ["cs.HC"], "comment": null, "summary": "As AI systems increasingly permeate everyday life, designers and developers\nface mounting pressure to balance innovation with ethical design choices. To\ndate, the operationalisation of AI ethics has predominantly depended on\nframeworks that prescribe which ethical principles should be embedded within AI\nsystems. However, the extent to which users value these principles remains\nlargely unexplored in the existing literature. In a discrete choice experiment\nconducted in four countries, we quantify user preferences for 11 ethical\nprinciples. Our findings indicate that, while users generally prioritise\nprivacy, justice & fairness, and transparency, their preferences exhibit\nsignificant variation based on culture and application context. Latent class\nanalysis further revealed four distinct user cohorts, the largest of which is\nethically disengaged and defers to regulatory oversight. Our findings offer (1)\nempirical evidence of uneven user prioritisation of AI ethics principles, (2)\nactionable guidance for operationalising ethics tailored to culture and\ncontext, (3) support for the development of robust regulatory mechanisms, and\n(4) a foundation for advancing a user-centred approach to AI ethics, motivated\nindependently from abstract moral theory."}
{"id": "2508.11184", "pdf": "https://arxiv.org/pdf/2508.11184.pdf", "abs": "https://arxiv.org/abs/2508.11184", "title": "Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Jian Zhan", "Mengze Li", "Kun Kuang", "Fei Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Distractors, incorrect but plausible answer choices in multiple-choice\nquestions (MCQs), play a critical role in educational assessment by diagnosing\nstudent misconceptions. Recent work has leveraged large language models (LLMs)\nto generate shared, group-level distractors by learning common error patterns\nacross large student populations. However, such distractors often fail to\ncapture the diverse reasoning errors of individual students, limiting their\ndiagnostic effectiveness. To address this limitation, we introduce the task of\npersonalized distractor generation, which aims to generate tailored distractors\nbased on individual misconceptions inferred from each student's past\nquestion-answering (QA) records, ensuring every student receives options that\neffectively exposes their specific reasoning errors. While promising, this task\nis challenging because each student typically has only a few QA records, which\noften lack the student's underlying reasoning processes, making training-based\ngroup-level approaches infeasible. To overcome this, we propose a training-free\ntwo-stage framework. In the first stage, we construct a student-specific\nmisconception prototype by applying Monte Carlo Tree Search (MCTS) to recover\nthe student's reasoning trajectories from past incorrect answers. In the second\nstage, this prototype guides the simulation of the student's reasoning on new\nquestions, enabling the generation of personalized distractors that align with\nthe student's recurring misconceptions. Experiments show that our approach\nachieves the best performance in generating plausible, personalized distractors\nfor 140 students, and also effectively generalizes to group-level settings,\nhighlighting its robustness and adaptability."}
{"id": "2508.11335", "pdf": "https://arxiv.org/pdf/2508.11335.pdf", "abs": "https://arxiv.org/abs/2508.11335", "title": "Towards Smart Workplaces: Understanding Mood-Influencing Factors of the Physical Workspace in Collaborative Group Settings", "authors": ["Tzu-Hui Wu", "Sebastian Cmentowski", "Yunyin Lou", "Jun Hu", "Regina Bernhaupt"], "categories": ["cs.HC"], "comment": "preprint, submitted to INTERACT 2025", "summary": "Group mood plays a crucial role in shaping workspace experiences, influencing\ngroup dynamics, team performance, and creativity. The perceived group mood\ndepends on many, often subconscious, aspects such as individual emotional\nstates or group life, which make it challenging to maintain a positive\natmosphere. Intelligent technology could support mood regulation in physical\noffice environments, for example, as adaptive ambient lighting for mood\nregulation. However, little is known about the relationship between the\nphysical workspace and group mood dynamics. To address this knowledge gap, we\nconducted a qualitative user study (N=8 workgroups and overall 26 participants)\nto explore how the physical workspace shapes group mood experiences and\ninvestigate employees' perspectives on intelligent mood-aware technologies. Our\nfindings reveal key factors influencing group mood, and participants'\nexpectations for supportive technology to preserve privacy and autonomy. Our\nwork highlights the potential of adaptive and responsive workspaces while also\nemphasizing the need for human-centered, technology-driven interventions that\nbenefit group well-being."}
{"id": "2508.11189", "pdf": "https://arxiv.org/pdf/2508.11189.pdf", "abs": "https://arxiv.org/abs/2508.11189", "title": "Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation", "authors": ["Chenyang Le", "Yinfeng Xia", "Huiyan Li", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Interspeech 2025", "summary": "Recent advancements in speech-to-text translation have led to the development\nof multilingual models capable of handling multiple language pairs\nsimultaneously. However, these unified models often suffer from large parameter\nsizes, making it challenging to balance inference efficiency and performance,\nparticularly in local deployment scenarios. We propose an innovative Parasitic\nDual-Scale Approach, which combines an enhanced speculative sampling method\nwith model compression and knowledge distillation techniques. Building on the\nWhisper Medium model, we enhance it for multilingual speech translation into\nwhisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art\n(SOTA) performance across six popular languages with improved inference\nefficiency. KVSPN enables a 40\\% speedup with no BLEU score degradation.\nCombined with distillation methods, it represents a 2.6$\\times$ speedup over\nthe original Whisper Medium with superior performance."}
{"id": "2508.11398", "pdf": "https://arxiv.org/pdf/2508.11398.pdf", "abs": "https://arxiv.org/abs/2508.11398", "title": "Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis", "authors": ["Mithat Can Ozgun", "Jiahuan Pei", "Koen Hindriks", "Lucia Donatelli", "Qingzhi Liu", "Xin Sun", "Junxiao Wang"], "categories": ["cs.HC", "cs.AI", "cs.IR"], "comment": "Accepted by CIKM 2025 as a full paper", "summary": "LLM-based agents have emerged as transformative tools capable of executing\ncomplex tasks through iterative planning and action, achieving significant\nadvancements in understanding and addressing user needs. Yet, their\neffectiveness remains limited in specialized domains such as mental health\ndiagnosis, where they underperform compared to general applications. Current\napproaches to integrating diagnostic capabilities into LLMs rely on scarce,\nhighly sensitive mental health datasets, which are challenging to acquire.\nThese methods also fail to emulate clinicians' proactive inquiry skills, lack\nmulti-turn conversational comprehension, and struggle to align outputs with\nexpert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the\nfirst LLM-based agent workflow designed to autonomously generate DSM-5 Level-1\ndiagnostic questionnaires. By simulating therapist-client dialogues with\nspecific client profiles, the framework delivers transparent, step-by-step\ndisorder predictions, producing explainable and trustworthy results. This\nworkflow serves as a complementary tool for mental health diagnosis, ensuring\nadherence to ethical and legal standards. Through comprehensive experiments, we\nevaluate leading LLMs across three critical dimensions: conversational realism,\ndiagnostic accuracy, and explainability. Our datasets and implementations are\nfully open-sourced."}
{"id": "2508.11197", "pdf": "https://arxiv.org/pdf/2508.11197.pdf", "abs": "https://arxiv.org/abs/2508.11197", "title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection", "authors": ["Ahmad Mousavi", "Yeganeh Abdollahinejad", "Roberto Corizzo", "Nathalie Japkowicz", "Zois Boukouvalas"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "comment": null, "summary": "Detecting multimodal misinformation on social media remains challenging due\nto inconsistencies between modalities, changes in temporal patterns, and\nsubstantial class imbalance. Many existing methods treat posts independently\nand fail to capture the event-level structure that connects them across time\nand modality. We propose E-CaTCH, an interpretable and scalable framework for\nrobustly detecting misinformation. If needed, E-CaTCH clusters posts into\npseudo-events based on textual similarity and temporal proximity, then\nprocesses each event independently. Within each event, textual and visual\nfeatures are extracted using pre-trained BERT and ResNet encoders, refined via\nintra-modal self-attention, and aligned through bidirectional cross-modal\nattention. A soft gating mechanism fuses these representations to form\ncontextualized, content-aware embeddings of each post. To model temporal\nevolution, E-CaTCH segments events into overlapping time windows and uses a\ntrend-aware LSTM, enhanced with semantic shift and momentum signals, to encode\nnarrative progression over time. Classification is performed at the event\nlevel, enabling better alignment with real-world misinformation dynamics. To\naddress class imbalance and promote stable learning, the model integrates\nadaptive class weighting, temporal consistency regularization, and hard-example\nmining. The total loss is aggregated across all events. Extensive experiments\non Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH\nconsistently outperforms state-of-the-art baselines. Cross-dataset evaluations\nfurther demonstrate its robustness, generalizability, and practical\napplicability across diverse misinformation scenarios."}
{"id": "2508.11401", "pdf": "https://arxiv.org/pdf/2508.11401.pdf", "abs": "https://arxiv.org/abs/2508.11401", "title": "FACET:Teacher-Centred LLM-Based Multi-Agent Systems-Towards Personalized Educational Worksheets", "authors": ["Jana Gonnermann-Müller", "Jennifer Haase", "Konstantin Fackeldey", "Sebastian Pokutta"], "categories": ["cs.HC", "cs.MA"], "comment": null, "summary": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials."}
{"id": "2508.11247", "pdf": "https://arxiv.org/pdf/2508.11247.pdf", "abs": "https://arxiv.org/abs/2508.11247", "title": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering", "authors": ["Changjian Wang", "Weihong Deng", "Weili Guan", "Quan Lu", "Ning Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency."}
{"id": "2508.11412", "pdf": "https://arxiv.org/pdf/2508.11412.pdf", "abs": "https://arxiv.org/abs/2508.11412", "title": "Towards Embodied Conversational Agents for Reducing Oral Exam Anxiety in Extended Reality", "authors": ["Jens Grubert", "Yvonne Sedelmaier", "Dieter Landes"], "categories": ["cs.HC"], "comment": "Accepted to the IEEE ISMAR-Adjunct Proceedings 2025", "summary": "Oral examinations are a prevalent but psychologically demanding form of\nassessment in higher education. Many students experience intense anxiety, which\ncan impair cognitive performance and hinder academic success. This position\npaper explores the potential of embodied conversational agents (ECAs) in\nextended reality (XR) environments to support students preparing for oral\nexams. We propose a system concept that integrates photorealistic ECAs with\nreal-time capable large language models (LLMs) to enable psychologically safe,\nadaptive, and repeatable rehearsal of oral examination scenarios. We also\ndiscuss the potential benefits and challenges of such an envisioned system."}
{"id": "2508.11260", "pdf": "https://arxiv.org/pdf/2508.11260.pdf", "abs": "https://arxiv.org/abs/2508.11260", "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?", "authors": ["Mukund Choudhary", "KV Aditya Srivatsa", "Gaurja Aeron", "Antara Raaghavi Bhattacharya", "Dang Khoa Dang Dinh", "Ikhlasul Akmal Hanif", "Daria Kotova", "Ekaterina Kochmar", "Monojit Choudhury"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages."}
{"id": "2508.11426", "pdf": "https://arxiv.org/pdf/2508.11426.pdf", "abs": "https://arxiv.org/abs/2508.11426", "title": "ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality", "authors": ["Steffen Hauck", "Diar Abdlkarim", "John Dudley", "Per Ola Kristensson", "Eyal Ofek", "Jens Grubert"], "categories": ["cs.HC", "cs.RO"], "comment": "To appear in Proceedings of IEEE ISMAR 2025", "summary": "Human-Robot-Collaboration can enhance workflows by leveraging the mutual\nstrengths of human operators and robots. Planning and understanding robot\nmovements remain major challenges in this domain. This problem is prevalent in\ndynamic environments that might need constant robot motion path adaptation. In\nthis paper, we investigate whether a minimalistic encoding of the reachability\nof a point near an object of interest, which we call ReachVox, can aid the\ncollaboration between a remote operator and a robotic arm in VR. Through a user\nstudy (n=20), we indicate the strength of the visualization relative to a\npoint-based reachability check-up."}
{"id": "2508.11280", "pdf": "https://arxiv.org/pdf/2508.11280.pdf", "abs": "https://arxiv.org/abs/2508.11280", "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought", "authors": ["Ruiyan Qi", "Congding Wen", "Weibo Zhou", "Shangsong Liang", "Lingbo Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks."}
{"id": "2508.11544", "pdf": "https://arxiv.org/pdf/2508.11544.pdf", "abs": "https://arxiv.org/abs/2508.11544", "title": "Grand Challenge: Mediating Between Confirmatory and Exploratory Research Cultures in Health Sciences and Visual Analytics", "authors": ["Viktor von Wyl", "Jürgen Bernard"], "categories": ["cs.HC"], "comment": "2+1 pages, 1 figure, position paper for the Visual Analytics in\n  Healthcare Workshop at IEEE VIS, Vienna, 2025", "summary": "Collaboration between health science and visual analytics research is often\nhindered by different, sometimes incompatible approaches to research design.\nHealth science often follows hypothesis-driven protocols, registered in\nadvance, and focuses on reproducibility and risk mitigation. Visual analytics,\nin contrast, relies on iterative data exploration, prioritizing insight\ngeneration and analytic refinement through user interaction. These differences\ncreate challenges in interdisciplinary projects, including misaligned\nterminology, unrealistic expectations about data readiness, divergent\nvalidation norms, or conflicting explainability requirements. To address these\npersistent tensions, we identify seven research needs and actions: (1)\nguidelines for broader community adoption, (2) agreement on quality and\nvalidation benchmarks, (3) frameworks for aligning research tasks, (4)\nintegrated workflows combining confirmatory and exploratory stages, (5) tools\nfor harmonizing terminology across disciplines, (6) dedicated bridging roles\nfor transdisciplinary work, and (7) cultural adaptation and mutual recognition.\nWe organize these needs in a framework with three areas: culture, standards,\nand processes. They can constitute a research agenda for developing reliable,\nreproducible, and clinically relevant data-centric methods."}
{"id": "2508.11281", "pdf": "https://arxiv.org/pdf/2508.11281.pdf", "abs": "https://arxiv.org/abs/2508.11281", "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection", "authors": ["Axel Delaval", "Shujian Yang", "Haicheng Wang", "Han Qiu", "Jialiang Lu"], "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "14 pages, 5 figures, 8 tables. This paper introduces TOXIFRENCH, a\n  new large-scale benchmark for French toxicity detection, and proposes a\n  Chain-of-Thought (CoT) fine-tuning method with a dynamic weighted loss. The\n  resulting fine-tuned 4B parameter model, ToxiFrench, achieves\n  state-of-the-art performance, outperforming larger models like GPT-4o", "summary": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks."}
{"id": "2508.11613", "pdf": "https://arxiv.org/pdf/2508.11613.pdf", "abs": "https://arxiv.org/abs/2508.11613", "title": "Adaptive Cardio Load Targets for Improving Fitness and Performance", "authors": ["Justin Phillips", "Daniel Roggen", "Cathy Speed", "Robert Harle"], "categories": ["cs.HC"], "comment": null, "summary": "Cardio Load, introduced by Google in 2024, is a measure of cardiovascular\nwork (also known as training load) resulting from all the user's activities\nacross the day. It is based on heart rate reserve and captures both activity\nintensity and duration. Thanks to feedback from users and internal research, we\nintroduce adaptive and personalized targets which will be set weekly. This\nfeature will be available in the Public Preview of the Fitbit app after\nSeptember 2025. This white paper provides a comprehensive overview of Cardio\nLoad (CL) and how weekly CL targets are established, with examples shown to\nillustrate the effect of varying CL on the weekly target. We compare Cardio\nLoad and Active Zone Minutes (AZMs), highlighting their distinct purposes, i.e.\nAZMs for health guidelines and CL for performance measurement. We highlight\nthat CL is accumulated both during active workouts and incidental daily\nactivities, so users are able top-up their CL score with small bouts of\nactivity across the day."}
{"id": "2508.11285", "pdf": "https://arxiv.org/pdf/2508.11285.pdf", "abs": "https://arxiv.org/abs/2508.11285", "title": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries", "authors": ["Arya VarastehNezhad", "Reza Tavasoli", "Soroush Elyasi", "MohammadHossein LotfiNia", "Hamed Farbeh"], "categories": ["cs.CL"], "comment": null, "summary": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes."}
{"id": "2508.11620", "pdf": "https://arxiv.org/pdf/2508.11620.pdf", "abs": "https://arxiv.org/abs/2508.11620", "title": "Grab-n-Go: On-the-Go Microgesture Recognition with Objects in Hand", "authors": ["Chi-Jung Lee", "Jiaxin Li", "Tianhong Catherine Yu", "Ruidong Zhang", "Vipin Gunda", "François Guimbretière", "Cheng Zhang"], "categories": ["cs.HC"], "comment": null, "summary": "As computing devices become increasingly integrated into daily life, there is\na growing need for intuitive, always-available interaction methods, even when\nusers' hands are occupied. In this paper, we introduce Grab-n-Go, the first\nwearable device that leverages active acoustic sensing to recognize subtle hand\nmicrogestures while holding various objects. Unlike prior systems that focus\nsolely on free-hand gestures or basic hand-object activity recognition,\nGrab-n-Go simultaneously captures information about hand microgestures,\ngrasping poses, and object geometries using a single wristband, enabling the\nrecognition of fine-grained hand movements occurring within activities\ninvolving occupied hands. A deep learning framework processes these complex\nsignals to identify 30 distinct microgestures, with 6 microgestures for each of\nthe 5 grasping poses. In a user study with 10 participants and 25 everyday\nobjects, Grab-n-Go achieved an average recognition accuracy of 92.0%. A\nfollow-up study further validated Grab-n-Go's robustness against 10 more\nchallenging, deformable objects. These results underscore the potential of\nGrab-n-Go to provide seamless, unobtrusive interactions without requiring\nmodifications to existing objects. The complete dataset, comprising data from\n18 participants performing 30 microgestures with 35 distinct objects, is\npublicly available at https://github.com/cjlisalee/Grab-n-Go_Data with the DOI:\nhttps://doi.org/10.7298/7kbd-vv75."}
{"id": "2508.11290", "pdf": "https://arxiv.org/pdf/2508.11290.pdf", "abs": "https://arxiv.org/abs/2508.11290", "title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory", "authors": ["Utsav Maskey", "Sumit Yadav", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals."}
{"id": "2508.10918", "pdf": "https://arxiv.org/pdf/2508.10918.pdf", "abs": "https://arxiv.org/abs/2508.10918", "title": "Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder", "authors": ["Samantha Aziz", "Oleg Komogortsev"], "categories": ["cs.CV", "cs.HC"], "comment": "IJCB 2025; 11 pages, 7 figures", "summary": "We present a privacy-enhancing mechanism for gaze signals using a\nlatent-noise autoencoder that prevents users from being re-identified across\nplay sessions without their consent, while retaining the usability of the data\nfor benign tasks. We evaluate privacy-utility trade-offs across biometric\nidentification and gaze prediction tasks, showing that our approach\nsignificantly reduces biometric identifiability with minimal utility\ndegradation. Unlike prior methods in this direction, our framework retains\nphysiologically plausible gaze patterns suitable for downstream use, which\nproduces favorable privacy-utility trade-off. This work advances privacy in\ngaze-based systems by providing a usable and effective mechanism for protecting\nsensitive gaze data."}
{"id": "2508.11310", "pdf": "https://arxiv.org/pdf/2508.11310.pdf", "abs": "https://arxiv.org/abs/2508.11310", "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems", "authors": ["Beichen Guo", "Zhiyuan Wen", "Yu Yang", "Peng Gao", "Ruosong Yang", "Jiaxing Shen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to The 21st International Conference on Advanced Data Mining\n  and Applications (ADMA2025)", "summary": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments."}
{"id": "2508.10942", "pdf": "https://arxiv.org/pdf/2508.10942.pdf", "abs": "https://arxiv.org/abs/2508.10942", "title": "Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram", "authors": ["Liming Xu", "Dave Towey", "Andrew P. French", "Steve Benford"], "categories": ["cs.CV", "cs.HC", "cs.MM", "I.4.10; I.5.4"], "comment": "This work is an extension of an ACM MM'17 workshop paper (Xu et al,\n  2017), which was completed in late 2017 and early 2018 during the first\n  author's doctoral studies at the University of Nottingham. This paper\n  includes 42 pages, 25 figures, 7 tables, and 13,536 words", "summary": "The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it\nis expected that our everyday environment may soon be decorating with objects\nconnecting with virtual elements. Alerting to the presence of these objects is\ntherefore the first step for motivating follow-up further inspection and\ntriggering digital material attached to the objects. This work studies a\nspecial kind of these objects -- Artcodes -- a human-meaningful and\nmachine-readable decorative markers that camouflage themselves with freeform\nappearance by encoding information into their topology. We formulate this\nproblem of recongising the presence of Artcodes as Artcode proposal detection,\na distinct computer vision task that classifies topologically similar but\ngeometrically and semantically different objects as a same class. To deal with\nthis problem, we propose a new feature descriptor, called the shape of\norientation histogram, to describe the generic topological structure of an\nArtcode. We collect datasets and conduct comprehensive experiments to evaluate\nthe performance of the Artcode detection proposer built upon this new feature\nvector. Our experimental results show the feasibility of the proposed feature\nvector for representing topological structures and the effectiveness of the\nsystem for detecting Artcode proposals. Although this work is an initial\nattempt to develop a feature-based system for detecting topological objects\nlike Artcodes, it would open up new interaction opportunities and spark\npotential applications of topological object detection."}
{"id": "2508.11318", "pdf": "https://arxiv.org/pdf/2508.11318.pdf", "abs": "https://arxiv.org/abs/2508.11318", "title": "LLM Compression: How Far Can We Go in Balancing Size and Performance?", "authors": ["Sahil Sk", "Debasish Dhal", "Sonal Khosla", "Sk Shahid", "Sambit Shekhar", "Akash Dhaka", "Shantipriya Parida", "Dilip K. Prasad", "Ondřej Bojar"], "categories": ["cs.CL"], "comment": "This paper has been accepted for presentation at the RANLP 2025\n  conference", "summary": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments."}
{"id": "2508.10972", "pdf": "https://arxiv.org/pdf/2508.10972.pdf", "abs": "https://arxiv.org/abs/2508.10972", "title": "Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision", "authors": ["Rosiana Natalie", "Wenqian Xu", "Ruei-Che Chang", "Rada Mihalcea", "Anhong Guo"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Advances in vision language models (VLMs) have enabled the simulation of\ngeneral human behavior through their reasoning and problem solving\ncapabilities. However, prior research has not investigated such simulation\ncapabilities in the accessibility domain. In this paper, we evaluate the extent\nto which VLMs can simulate the vision perception of low vision individuals when\ninterpreting images. We first compile a benchmark dataset through a survey\nstudy with 40 low vision participants, collecting their brief and detailed\nvision information and both open-ended and multiple-choice image perception and\nrecognition responses to up to 25 images. Using these responses, we construct\nprompts for VLMs (GPT-4o) to create simulated agents of each participant,\nvarying the included information on vision information and example image\nresponses. We evaluate the agreement between VLM-generated responses and\nparticipants' original answers. Our results indicate that VLMs tend to infer\nbeyond the specified vision ability when given minimal prompts, resulting in\nlow agreement (0.59). The agreement between the agent' and participants'\nresponses remains low when only either the vision information (0.59) or example\nimage responses (0.59) are provided, whereas a combination of both\nsignificantly increase the agreement (0.70, p < 0.0001). Notably, a single\nexample combining both open-ended and multiple-choice responses, offers\nsignificant performance improvements over either alone (p < 0.0001), while\nadditional examples provided minimal benefits (p > 0.05)."}
{"id": "2508.11343", "pdf": "https://arxiv.org/pdf/2508.11343.pdf", "abs": "https://arxiv.org/abs/2508.11343", "title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis", "authors": ["Haitong Luo", "Weiyao Zhang", "Suhang Wang", "Wenji Zou", "Chungang Lin", "Xuying Meng", "Yujun Zhang"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge."}
{"id": "2508.11093", "pdf": "https://arxiv.org/pdf/2508.11093.pdf", "abs": "https://arxiv.org/abs/2508.11093", "title": "Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "Accepted at Human-Centered Robot Autonomy for Human-Robot Teams\n  (HuRoboT) at IEEE RO-MAN 2025, Eindhoven, the Netherlands", "summary": "Human-robot collaboration requires robots to quickly infer user intent,\nprovide transparent reasoning, and assist users in achieving their goals. Our\nrecent work introduced GUIDER, our framework for inferring navigation and\nmanipulation intents. We propose augmenting GUIDER with a vision-language model\n(VLM) and a text-only language model (LLM) to form a semantic prior that\nfilters objects and locations based on the mission prompt. A vision pipeline\n(YOLO for object detection and the Segment Anything Model for instance\nsegmentation) feeds candidate object crops into the VLM, which scores their\nrelevance given an operator prompt; in addition, the list of detected object\nlabels is ranked by a text-only LLM. These scores weight the existing\nnavigation and manipulation layers of GUIDER, selecting context-relevant\ntargets while suppressing unrelated objects. Once the combined belief exceeds a\nthreshold, autonomy changes occur, enabling the robot to navigate to the\ndesired area and retrieve the desired object, while adapting to any changes in\nthe operator's intent. Future work will evaluate the system on Isaac Sim using\na Franka Emika arm on a Ridgeback base, with a focus on real-time assistance."}
{"id": "2508.11364", "pdf": "https://arxiv.org/pdf/2508.11364.pdf", "abs": "https://arxiv.org/abs/2508.11364", "title": "Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning", "authors": ["Sylvio Rüdian", "Yassin Elsir", "Marvin Kretschmer", "Sabine Cayrou", "Niels Pinkwart"], "categories": ["cs.CL"], "comment": "11 pages, one table", "summary": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research."}
{"id": "2508.11115", "pdf": "https://arxiv.org/pdf/2508.11115.pdf", "abs": "https://arxiv.org/abs/2508.11115", "title": "UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring", "authors": ["Haotang Li", "Zhenyu Qi", "Sen He", "Kebin Peng", "Sheng Tan", "Yili Ren", "Tomas Cerny", "Jiyue Zhao", "Zi Wang"], "categories": ["cs.CV", "cs.HC", "eess.SP"], "comment": null, "summary": "Improper sitting posture during prolonged computer use has become a\nsignificant public health concern. Traditional posture monitoring solutions\nface substantial barriers, including privacy concerns with camera-based systems\nand user discomfort with wearable sensors. This paper presents\nUWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that\nadvances mobile technologies for preventive health management through\ncontinuous, contactless monitoring of ergonomic sitting posture. Our system\nleverages commercial UWB devices, utilizing comprehensive feature engineering\nto extract multiple ergonomic sitting posture features. We develop PoseGBDT to\neffectively capture temporal dependencies in posture patterns, addressing\nlimitations of traditional frame-wise classification approaches. Extensive\nreal-world evaluation across 10 participants and 19 distinct postures\ndemonstrates exceptional performance, achieving 99.11% accuracy while\nmaintaining robustness against environmental variables such as clothing\nthickness, additional devices, and furniture configurations. Our system\nprovides a scalable, privacy-preserving mobile health solution on existing\nplatforms for proactive ergonomic management, improving quality of life at low\ncosts."}
{"id": "2508.11383", "pdf": "https://arxiv.org/pdf/2508.11383.pdf", "abs": "https://arxiv.org/abs/2508.11383", "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs", "authors": ["Mikhail Seleznyov", "Mikhail Chaichuk", "Gleb Ershov", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters."}
{"id": "2508.11360", "pdf": "https://arxiv.org/pdf/2508.11360.pdf", "abs": "https://arxiv.org/abs/2508.11360", "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks", "authors": ["Songqin Nong", "Jingxuan Xu", "Sheng Zhou", "Jianfeng Chen", "Xiaoxuan Tang", "Tao Jiang", "Wenhao Xu"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "As autonomous agents become adept at understanding and interacting with\ngraphical user interface (GUI) environments, a new era of automated task\nexecution is emerging. Recent studies have demonstrated that Reinforcement\nLearning (RL) can effectively enhance agents' performance in dynamic\ninteractive GUI environments. However, these methods face two key limitations:\n(1) they overlook the significant variation in difficulty across different GUI\ntasks by treating the entire training data as a uniform set, which hampers the\nagent's ability to adapt its learning process; and (2) most approaches collapse\ntask-specific nuances into a single, coarse reward, leaving the agent with a\nuniform signal that yields inefficient policy updates. To address these\nlimitations, we propose CRAFT-GUI, a curriculum learning framework based on\nGroup Relative Policy Optimization (GRPO) that explicitly accounts for the\nvarying difficulty across trajectories. To enable more fine-grained policy\noptimization, we design a reward function that combines simple rule-based\nsignals with model-judged evaluation, providing richer and more nuanced\nfeedback during training. Experimental results demonstrate that our method\nachieves significant improvements over previous state-of-the-art approaches,\noutperforming them by 5.6% on public benchmarks Android Control and 10.3% on\nour internal online benchmarks, respectively. These findings empirically\nvalidate the effectiveness of integrating reinforcement learning with\ncurriculum learning in GUI interaction tasks."}
{"id": "2508.11386", "pdf": "https://arxiv.org/pdf/2508.11386.pdf", "abs": "https://arxiv.org/abs/2508.11386", "title": "Retrieval-augmented reasoning with lean language models", "authors": ["Ryan Sze-Yin Chan", "Federico Nanni", "Tomas Lazauskas", "Rosie Wood", "Penelope Yong", "Lionel Tarassenko", "Mark Girolami", "James Geddes", "Andrew Duncan"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains."}
{"id": "2508.11404", "pdf": "https://arxiv.org/pdf/2508.11404.pdf", "abs": "https://arxiv.org/abs/2508.11404", "title": "An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration", "authors": ["Junyeon Kim", "Tianshu Ruan", "Cesar Alan Contreras", "Manolis Chiou"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Structural inspection in nuclear facilities is vital for maintaining\noperational safety and integrity. Traditional methods of manual inspection pose\nsignificant challenges, including safety risks, high cognitive demands, and\npotential inaccuracies due to human limitations. Recent advancements in\nArtificial Intelligence (AI) and robotic technologies have opened new\npossibilities for safer, more efficient, and accurate inspection methodologies.\nSpecifically, Human-Robot Collaboration (HRC), leveraging robotic platforms\nequipped with advanced detection algorithms, promises significant improvements\nin inspection outcomes and reductions in human workload. This study explores\nthe effectiveness of AI-assisted visual crack detection integrated into a\nmobile Jackal robot platform. The experiment results indicate that HRC enhances\ninspection accuracy and reduces operator workload, resulting in potential\nsuperior performance outcomes compared to traditional manual methods."}
{"id": "2508.11388", "pdf": "https://arxiv.org/pdf/2508.11388.pdf", "abs": "https://arxiv.org/abs/2508.11388", "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization", "authors": ["Marc Brinner", "Sina Zarriess"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Concurrent to the rapid progress in the development of neural-network based\nmodels in areas like natural language processing and computer vision, the need\nfor creating explanations for the predictions of these black-box models has\nrisen steadily. We propose a new method to generate extractive explanations for\npredictions made by neural networks, that is based on masking parts of the\ninput which the model does not consider to be indicative of the respective\nclass. The masking is done using gradient-based optimization combined with a\nnew regularization scheme that enforces sufficiency, comprehensiveness and\ncompactness of the generated explanation, three properties that are known to be\ndesirable from the related field of rationale extraction in natural language\nprocessing. In this way, we bridge the gap between model interpretability and\nrationale extraction, thereby proving that the latter of which can be performed\nwithout training a specialized model, only on the basis of a trained\nclassifier. We further apply the same method to image inputs and obtain high\nquality explanations for image classifications, which indicates that the\nconditions proposed for rationale extraction in natural language processing are\nmore broadly applicable to different input types."}
{"id": "2508.11452", "pdf": "https://arxiv.org/pdf/2508.11452.pdf", "abs": "https://arxiv.org/abs/2508.11452", "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking."}
{"id": "2508.11393", "pdf": "https://arxiv.org/pdf/2508.11393.pdf", "abs": "https://arxiv.org/abs/2508.11393", "title": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "authors": ["Marc Brinner", "Sina Zarrieß"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We propose an end-to-end differentiable training paradigm for stable training\nof a rationalized transformer classifier. Our approach results in a single\nmodel that simultaneously classifies a sample and scores input tokens based on\ntheir relevance to the classification. To this end, we build on the widely-used\nthree-player-game for training rationalized models, which typically relies on\ntraining a rationale selector, a classifier and a complement classifier. We\nsimplify this approach by making a single model fulfill all three roles,\nleading to a more efficient training paradigm that is not susceptible to the\ncommon training instabilities that plague existing approaches. Further, we\nextend this paradigm to produce class-wise rationales while incorporating\nrecent advances in parameterizing and regularizing the resulting rationales,\nthus leading to substantially improved and state-of-the-art alignment with\nhuman annotations without any explicit supervision."}
{"id": "2401.05999", "pdf": "https://arxiv.org/pdf/2401.05999.pdf", "abs": "https://arxiv.org/abs/2401.05999", "title": "Boosting Mixed-Initiative Co-Creativity in Game Design: A Tutorial", "authors": ["Solange Margarido", "Licínio Roque", "Penousal Machado", "Pedro Martins"], "categories": ["cs.HC"], "comment": "37 pages, 1 table, 19 figures; expanded introduction to section 3,\n  subsection 4.1, and closing discussion in section 5; restructured subsection\n  4.2 for greater clarity", "summary": "In recent years, there has been a growing application of mixed-initiative\nco-creative approaches in the creation of video games. The rapid advances in\nthe capabilities of artificial intelligence (AI) systems further propel\ncreative collaboration between humans and computational agents. In this\ntutorial, we present guidelines for researchers and practitioners to develop\ngame design tools with a high degree of mixed-initiative co-creativity\n(MI-CCy). We begin by reviewing a selection of current works that will serve as\ncase studies and categorize them by the type of game content they address. We\nintroduce the MI-CCy Quantifier, a framework that can be used by researchers\nand developers to assess co-creative tools on their level of MI-CCy through a\nvisual scheme of quantifiable criteria scales. We demonstrate the usage of the\nMI-CCy Quantifier by applying it to the selected works. This analysis enabled\nus to discern prevalent patterns within these tools, as well as features that\ncontribute to a higher level of MI-CCy. We highlight current gaps in MI-CCy\napproaches within game design, which we propose as pivotal aspects to tackle in\nthe development of forthcoming approaches."}
{"id": "2508.11414", "pdf": "https://arxiv.org/pdf/2508.11414.pdf", "abs": "https://arxiv.org/abs/2508.11414", "title": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions", "authors": ["Shangrui Nie", "Florian Mai", "David Kaczér", "Charles Welch", "Zhixue Zhao", "Lucie Flek"], "categories": ["cs.CL"], "comment": "7 pages 1 figure", "summary": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior."}
{"id": "2504.08670", "pdf": "https://arxiv.org/pdf/2504.08670.pdf", "abs": "https://arxiv.org/abs/2504.08670", "title": "Once Upon an AI: Six Scaffolds for Child-AI Interaction Design, Inspired by Disney", "authors": ["Nomisha Kurian"], "categories": ["cs.HC", "cs.AI"], "comment": "28 pages", "summary": "To build AI that children can intuitively understand and benefit from,\ndesigners need a design grammar that serves their developmental needs. This\npaper bridges artificial intelligence design for children - an emerging field\nstill defining its best practices - and animation, a well established field\nwith decades of experience in engaging children through accessible\nstorytelling. Pairing Piagetian developmental theory with design pattern\nextraction from 52 works of animation, the paper presents a six scaffold\nframework that integrates design insights transferable to child centred AI\ndesign: (1) signals for visual animacy and clarity, (2) sound for musical and\nauditory scaffolding, (3) synchrony in audiovisual cues, (4) sidekick style\npersonas, (5) storyplay that supports symbolic play and imaginative\nexploration, and (6) structure in the form of predictable narratives. These\nstrategies, long refined in animation, function as multimodal scaffolds for\nattention, understanding, and attunement, supporting learning and comfort. This\nstructured design grammar is transferable to AI design. By reframing cinematic\nstorytelling and child development theory as design logic for AI, the paper\noffers heuristics for AI that aligns with the cognitive stages and emotional\nneeds of young users. The work contributes to design theory by showing how\nsensory, affective, and narrative techniques can inform developmentally attuned\nAI design. Future directions include empirical testing, cultural adaptation,\nand participatory co design."}
{"id": "2508.11429", "pdf": "https://arxiv.org/pdf/2508.11429.pdf", "abs": "https://arxiv.org/abs/2508.11429", "title": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor", "authors": ["Shivam Dubey"], "categories": ["cs.CL"], "comment": null, "summary": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy."}
{"id": "2504.16741", "pdf": "https://arxiv.org/pdf/2504.16741.pdf", "abs": "https://arxiv.org/abs/2504.16741", "title": "Search Timelines: Visualizing Search History to Enable Cross-Session Exploratory Search", "authors": ["Orland Hoeber", "Md Nazmul Islam", "Miriam Boon", "Dale Storie", "Veronica Ramshaw"], "categories": ["cs.HC", "cs.IR"], "comment": null, "summary": "Purpose: The timespan over which exploratory searching can occur, as well as\nthe scope and volume of the search activities undertaken, can make it difficult\nfor searchers to remember key details about their search activities. These\ndifficulties are present both in the midst of searching as well as when\nresuming a search that spans multiple sessions. In this paper, we present a\nsearch interface design and prototype implementation to support cross-session\nexploratory search in a public digital library context.\n  Methods: Search Timelines provides a visualization of current and past search\nactivities via a dynamic timeline of the search activity (queries and saved\nresources). This timeline is presented at two levels of detail. An Overview\nTimeline is provided alongside the search results in a typical search engine\nresults page design. A Detailed Timeline is provided in the workspace, where\nsearchers can review the history of their search activities and their saved\nresources. A controlled laboratory study (n=32) was conducted to compare this\napproach to a baseline interface modelled after a typical public digital\nlibrary search/workspace interface.\n  Results: Participants who used Search Timelines reported higher levels of\nuser engagement, usability, and perceived knowledge gain, during an initial\nsearch session and when resuming the search after a 7-8 day interval. This came\nat the expense of the searchers taking more time to complete the search task,\nwhich we view as positive evidence of engagement in cross-session exploratory\nsearch processes.\n  Conclusion: Search Timelines serves as an example of how lightweight\nvisualization approaches can be used to enhance typical search interface\ndesigns to support exploratory search. The results highlight the value of\nproviding persistent representations of past search activities within the\nsearch interface.}"}
{"id": "2508.11434", "pdf": "https://arxiv.org/pdf/2508.11434.pdf", "abs": "https://arxiv.org/abs/2508.11434", "title": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse", "authors": ["Aditi Dutta", "Susan Banducci"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces."}
{"id": "2508.02133", "pdf": "https://arxiv.org/pdf/2508.02133.pdf", "abs": "https://arxiv.org/abs/2508.02133", "title": "Hierarchical MoE: Continuous Multimodal Emotion Recognition with Incomplete and Asynchronous Inputs", "authors": ["Yitong Zhu", "Lei Han", "Guanxuan Jiang", "PengYuan Zhou", "Yuyang Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Multimodal emotion recognition (MER) is crucial for human-computer\ninteraction, yet real-world challenges like dynamic modality incompleteness and\nasynchrony severely limit its robustness. Existing methods often assume\nconsistently complete data or lack dynamic adaptability. To address these\nlimitations, we propose a novel Hi-MoE~(Hierarchical Mixture-of-Experts)\nframework for robust continuous emotion prediction. This framework employs a\ndual-layer expert structure. A Modality Expert Bank utilizes soft routing to\ndynamically handle missing modalities and achieve robust information fusion. A\nsubsequent Emotion Expert Bank leverages differential-attention routing to\nflexibly attend to emotional prototypes, enabling fine-grained emotion\nrepresentation. Additionally, a cross-modal alignment module explicitly\naddresses temporal shifts and semantic inconsistencies between modalities.\nExtensive experiments on benchmark datasets DEAP and DREAMER demonstrate our\nmodel's state-of-the-art performance in continuous emotion regression,\nshowcasing exceptional robustness under challenging conditions such as dynamic\nmodality absence and asynchronous sampling. This research significantly\nadvances the development of intelligent emotion systems adaptable to complex\nreal-world environments."}
{"id": "2508.11442", "pdf": "https://arxiv.org/pdf/2508.11442.pdf", "abs": "https://arxiv.org/abs/2508.11442", "title": "CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity", "authors": ["Bowen Zhang", "Zixin Song", "Chunquan Chen", "Qian-Wen Zhang", "Di Yin", "Xing Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Learning unified text embeddings that excel across diverse downstream tasks\nis a central goal in representation learning, yet negative transfer remains a\npersistent obstacle. This challenge is particularly pronounced when jointly\ntraining a single encoder for Information Retrieval (IR) and Semantic Textual\nSimilarity (STS), two essential but fundamentally disparate tasks for which\nnaive co-training typically yields steep performance trade-offs. We argue that\nresolving this conflict requires systematically decoupling task-specific\nlearning signals throughout the training pipeline. To this end, we introduce\nCoDiEmb, a unified framework that reconciles the divergent requirements of IR\nand STS in a collaborative yet distinct manner. CoDiEmb integrates three key\ninnovations for effective joint optimization: (1) Task-specialized objectives\npaired with a dynamic sampler that forms single-task batches and balances\nper-task updates, thereby preventing gradient interference. For IR, we employ a\ncontrastive loss with multiple positives and hard negatives, augmented by\ncross-device sampling. For STS, we adopt order-aware objectives that directly\noptimize correlation and ranking consistency. (2) A delta-guided model fusion\nstrategy that computes fine-grained merging weights for checkpoints by\nanalyzing each parameter's deviation from its pre-trained initialization,\nproving more effective than traditional Model Soups. (3) An efficient,\nsingle-stage training pipeline that is simple to implement and converges\nstably. Extensive experiments on 15 standard IR and STS benchmarks across three\nbase encoders validate CoDiEmb. Our results and analysis demonstrate that the\nframework not only mitigates cross-task trade-offs but also measurably improves\nthe geometric properties of the embedding space."}
{"id": "2508.03182", "pdf": "https://arxiv.org/pdf/2508.03182.pdf", "abs": "https://arxiv.org/abs/2508.03182", "title": "StoryEnsemble: Enabling Dynamic Exploration & Iteration in the Design Process with AI and Forward-Backward Propagation", "authors": ["Sangho Suh", "Michael Lai", "Kevin Pu", "Steven P. Dow", "Tovi Grossman"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Design processes involve exploration, iteration, and movement across\ninterconnected stages such as persona creation, problem framing, solution\nideation, and prototyping. However, time and resource constraints often hinder\ndesigners from exploring broadly, collecting feedback, and revisiting earlier\nassumptions-making it difficult to uphold core design principles in practice.\nTo better understand these challenges, we conducted a formative study with 15\nparticipants-comprised of UX practitioners, students, and instructors. Based on\nthe findings, we developed StoryEnsemble, a tool that integrates AI into a\nnode-link interface and leverages forward and backward propagation to support\ndynamic exploration and iteration across the design process. A user study with\n10 participants showed that StoryEnsemble enables rapid, multi-directional\niteration and flexible navigation across design stages. This work advances our\nunderstanding of how AI can foster more iterative design practices by\nintroducing novel interactions that make exploration and iteration more fluid,\naccessible, and engaging."}
{"id": "2508.11454", "pdf": "https://arxiv.org/pdf/2508.11454.pdf", "abs": "https://arxiv.org/abs/2508.11454", "title": "Reference Points in LLM Sentiment Analysis: The Role of Structured Context", "authors": ["Junichiro Niimi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment."}
{"id": "2501.01212", "pdf": "https://arxiv.org/pdf/2501.01212.pdf", "abs": "https://arxiv.org/abs/2501.01212", "title": "Towards Consumer-Grade Cybersickness Prediction: Multi-Model Alignment for Real-Time Vision-Only Inference", "authors": ["Yitong Zhu", "Zhuowen Liang", "Yiming Wu", "Tangyao Li", "Yuyang Wang"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Cybersickness remains a major obstacle to the widespread adoption of\nimmersive virtual reality (VR), particularly in consumer-grade environments.\nWhile prior methods rely on invasive signals such as electroencephalography\n(EEG) for high predictive accuracy, these approaches require specialized\nhardware and are impractical for real-world applications. In this work, we\npropose a scalable, deployable framework for personalized cybersickness\nprediction leveraging only non-invasive signals readily available from\ncommercial VR headsets, including head motion, eye tracking, and physiological\nresponses. Our model employs a modality-specific graph neural network enhanced\nwith a Difference Attention Module to extract temporal-spatial embeddings\ncapturing dynamic changes across modalities. A cross-modal alignment module\njointly trains the video encoder to learn personalized traits by aligning video\nfeatures with sensor-derived representations. Consequently, the model\naccurately predicts individual cybersickness using only video input during\ninference. Experimental results show our model achieves 88.4\\% accuracy,\nclosely matching EEG-based approaches (89.16\\%), while reducing deployment\ncomplexity. With an average inference latency of 90ms, our framework supports\nreal-time applications, ideal for integration into consumer-grade VR platforms\nwithout compromising personalization or performance. The code will be relesed\nat https://github.com/U235-Aurora/PTGNN."}
{"id": "2508.11534", "pdf": "https://arxiv.org/pdf/2508.11534.pdf", "abs": "https://arxiv.org/abs/2508.11534", "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models", "authors": ["Monika Jotautaitė", "Lucius Caviola", "David A. Brewster", "Thilo Hagendorff"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence."}
{"id": "2503.06195", "pdf": "https://arxiv.org/pdf/2503.06195.pdf", "abs": "https://arxiv.org/abs/2503.06195", "title": "Human-AI Experience in Integrated Development Environments: A Systematic Literature Review", "authors": ["Agnia Sergeyuk", "Ilya Zakharov", "Ekaterina Koshchenko", "Maliheh Izadi"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "Submitted to Empirical Software Engineering (EMSE) special issue\n  Human-Centered AI for Software Engineering (HumanAISE), 37 pages, 7 figure", "summary": "The integration of Artificial Intelligence (AI) into Integrated Development\nEnvironments (IDEs) is reshaping software development, fundamentally altering\nhow developers interact with their tools. This shift marks the emergence of\nHuman-AI Experience in Integrated Development Environment (in-IDE HAX), a field\nthat explores the evolving dynamics of Human-Computer Interaction in\nAI-assisted coding environments. Despite rapid adoption, research on in-IDE HAX\nremains fragmented, which highlights the need for a unified overview of current\npractices, challenges, and opportunities. To provide a structured overview of\nexisting research, we conduct a systematic literature review of 90 studies,\nsummarizing current findings and outlining areas for further investigation.\n  We organize key insights from reviewed studies into three aspects: Impact,\nDesign, and Quality of AI-based systems inside IDEs. Impact findings show that\nAI-assisted coding enhances developer productivity but also introduces\nchallenges, such as verification overhead and over-reliance. Design studies\nshow that effective interfaces surface context, provide explanations and\ntransparency of suggestion, and support user control. Quality studies document\nrisks in correctness, maintainability, and security. For future research,\npriorities include productivity studies, design of assistance, and audit of\nAI-generated code. The agenda calls for larger and longer evaluations, stronger\naudit and verification assets, broader coverage across the software life cycle,\nand adaptive assistance under user control."}
{"id": "2508.11536", "pdf": "https://arxiv.org/pdf/2508.11536.pdf", "abs": "https://arxiv.org/abs/2508.11536", "title": "Language models align with brain regions that represent concepts across modalities", "authors": ["Maria Ryskina", "Greta Tuckute", "Alexander Fung", "Ashley Malkin", "Evelina Fedorenko"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025. Code and data can be found at\n  https://github.com/ryskina/concepts-brain-llms", "summary": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning."}
{"id": "2504.00799", "pdf": "https://arxiv.org/pdf/2504.00799.pdf", "abs": "https://arxiv.org/abs/2504.00799", "title": "Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users", "authors": ["Shiyang Zhang", "Fanfei Meng", "Xi Wang", "Lan Li"], "categories": ["cs.CL", "cs.HC", "H.5.2; I.2.7"], "comment": "The scope of the work has evolved significantly since initial\n  submission, and we are preparing a revised version that better reflects the\n  current direction of the research", "summary": "Electronic dictionaries have largely replaced paper dictionaries and become\ncentral tools for L2 learners seeking to expand their vocabulary. Users often\nassume these resources are reliable and rarely question the validity of the\ndefinitions provided. The accuracy of major E-dictionaries is seldom\nscrutinized, and little attention has been paid to how their corpora are\nconstructed. Research on dictionary use, particularly the limitations of\nelectronic dictionaries, remains scarce. This study adopts a combined method of\nexperimentation, user survey, and dictionary critique to examine Youdao, one of\nthe most widely used E-dictionaries in China. The experiment involved a\ntranslation task paired with retrospective reflection. Participants were asked\nto translate sentences containing words that are insufficiently or inaccurately\ndefined in Youdao. Their consultation behavior was recorded to analyze how\nfaulty definitions influenced comprehension. Results show that incomplete or\nmisleading definitions can cause serious misunderstandings. Additionally,\nstudents exhibited problematic consultation habits. The study further explores\nhow such flawed definitions originate, highlighting issues in data processing\nand the integration of AI and machine learning technologies in dictionary\nconstruction. The findings suggest a need for better training in dictionary\nliteracy for users, as well as improvements in the underlying AI models used to\nbuild E-dictionaries."}
{"id": "2508.11567", "pdf": "https://arxiv.org/pdf/2508.11567.pdf", "abs": "https://arxiv.org/abs/2508.11567", "title": "AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment", "authors": ["Jinpeng Hu", "Ao Wang", "Qianqian Xie", "Hui Ma", "Zhuo Li", "Dan Guo"], "categories": ["cs.CL"], "comment": null, "summary": "Mental health assessment is crucial for early intervention and effective\ntreatment, yet traditional clinician-based approaches are limited by the\nshortage of qualified professionals. Recent advances in artificial intelligence\nhave sparked growing interest in automated psychological assessment, yet most\nexisting approaches are constrained by their reliance on static text analysis,\nlimiting their ability to capture deeper and more informative insights that\nemerge through dynamic interaction and iterative questioning. Therefore, in\nthis paper, we propose a multi-agent framework for mental health evaluation\nthat simulates clinical doctor-patient dialogues, with specialized agents\nassigned to questioning, adequacy evaluation, scoring, and updating. We\nintroduce an adaptive questioning mechanism in which an evaluation agent\nassesses the adequacy of user responses to determine the necessity of\ngenerating targeted follow-up queries to address ambiguity and missing\ninformation. Additionally, we employ a tree-structured memory in which the root\nnode encodes the user's basic information, while child nodes (e.g., topic and\nstatement) organize key information according to distinct symptom categories\nand interaction turns. This memory is dynamically updated throughout the\ninteraction to reduce redundant questioning and further enhance the information\nextraction and contextual tracking capabilities. Experimental results on the\nDAIC-WOZ dataset illustrate the effectiveness of our proposed method, which\nachieves better performance than existing approaches."}
{"id": "2507.04996", "pdf": "https://arxiv.org/pdf/2507.04996.pdf", "abs": "https://arxiv.org/abs/2507.04996", "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems", "authors": ["Jiangbo Yu"], "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.HC", "cs.RO"], "comment": null, "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are viewed as vehicular systems capable of\nperceiving their environment and executing pre-programmed tasks independently\nof external input. However, both research and real-world deployments\nincreasingly showcase vehicles that demonstrate behaviors beyond this\ndefinition (including the SAE levels 0 to 5); Examples of this outpace include\nthe interaction with humans with natural language, goal adaptation, contextual\nreasoning, external tool use, and unseen ethical dilemma handling, largely\nempowered by multi-modal large language models (LLMs). These developments\nreveal a conceptual gap between technical autonomy and the broader cognitive\nand social capabilities needed for future human-centered mobility systems. To\naddress this gap, this paper introduces the concept of agentic vehicles (AgVs),\nreferring to vehicles that integrate agentic AI systems to reason, adapt, and\ninteract within complex environments. This paper proposes the term AgVs and\ntheir distinguishing characteristics from conventional AuVs. It synthesizes\nrelevant advances in integrating LLMs and AuVs and highlights how AgVs might\ntransform future mobility systems and ensure the systems are human-centered.\nThe paper concludes by identifying key challenges in the development and\ngovernance of AgVs, and how they can play a significant role in future agentic\ntransportation systems."}
{"id": "2508.11582", "pdf": "https://arxiv.org/pdf/2508.11582.pdf", "abs": "https://arxiv.org/abs/2508.11582", "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models", "authors": ["Qiguang Chen", "Dengyun Peng", "Jinhao Liu", "HuiKang Su", "Jiannan Guan", "Libo Qin", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement."}
{"id": "2508.10603", "pdf": "https://arxiv.org/pdf/2508.10603.pdf", "abs": "https://arxiv.org/abs/2508.10603", "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality", "authors": ["Agnes Axelsson", "Merle Reimann", "Ronald Cumbal", "Hannah Pelikan", "Divesh Lala"], "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages", "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods."}
{"id": "2508.11598", "pdf": "https://arxiv.org/pdf/2508.11598.pdf", "abs": "https://arxiv.org/abs/2508.11598", "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens", "authors": ["Greta Tuckute", "Klemen Kotar", "Evelina Fedorenko", "Daniel L. K. Yamins"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete \\textbf{cochlear tokens}.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks."}
{"id": "2508.11605", "pdf": "https://arxiv.org/pdf/2508.11605.pdf", "abs": "https://arxiv.org/abs/2508.11605", "title": "Dataset Creation for Visual Entailment using Generative AI", "authors": ["Rob Reijtenbach", "Suzan Verberne", "Gijs Wijnholds"], "categories": ["cs.CL"], "comment": "NALOMA: Natural Logic meets Machine Learning workshop @ ESSLLI 2025", "summary": "In this paper we present and validate a new synthetic dataset for training\nvisual entailment models. Existing datasets for visual entailment are small and\nsparse compared to datasets for textual entailment. Manually creating datasets\nis labor-intensive. We base our synthetic dataset on the SNLI dataset for\ntextual entailment. We take the premise text from SNLI as input prompts in a\ngenerative image model, Stable Diffusion, creating an image to replace each\ntextual premise. We evaluate our dataset both intrinsically and extrinsically.\nFor extrinsic evaluation, we evaluate the validity of the generated images by\nusing them as training data for a visual entailment classifier based on CLIP\nfeature vectors. We find that synthetic training data only leads to a slight\ndrop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when\ntrained on real data. We also compare the quality of our generated training\ndata to original training data on another dataset: SICK-VTE. Again, there is\nonly a slight drop in F-score: from 0.400 to 0.384. These results indicate that\nin settings with data sparsity, synthetic data can be a promising solution for\ntraining visual entailment models."}
{"id": "2508.11607", "pdf": "https://arxiv.org/pdf/2508.11607.pdf", "abs": "https://arxiv.org/abs/2508.11607", "title": "TinyTim: A Family of Language Models for Divergent Generation", "authors": ["Christopher J. Agostino"], "categories": ["cs.CL"], "comment": "7 pages, 3 figures, submitted to NeurIPS Creative AI track, code and\n  model available at https://hf.co/npc-worldwide/TinyTimV1", "summary": "This work introduces TinyTim, a family of large language models fine-tuned on\nJames Joyce's `Finnegans Wake'. Through quantitative evaluation against\nbaseline models, we demonstrate that TinyTim V1 produces a statistically\ndistinct generative profile characterized by high lexical diversity and low\nsemantic coherence. These findings are interpreted through theories of\ncreativity and complex problem-solving, arguing that such specialized models\ncan function as divergent knowledge sources within more extensive creative\narchitectures, powering automated discovery mechanisms in diverse settings."}
{"id": "2506.20844", "pdf": "https://arxiv.org/pdf/2506.20844.pdf", "abs": "https://arxiv.org/abs/2506.20844", "title": "The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers", "authors": ["Xingyu Deng", "Xi Wang", "Mark Stevenson"], "categories": ["cs.IR", "cs.CL"], "comment": "Accepted for ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR'25)", "summary": "Scientific fact-checking aims to determine the veracity of scientific claims\nby retrieving and analysing evidence from research literature. The problem is\ninherently more complex than general fact-checking since it must accommodate\nthe evolving nature of scientific knowledge, the structural complexity of\nacademic literature and the challenges posed by long-form, multimodal\nscientific expression. However, existing approaches focus on simplified\nversions of the problem based on small-scale datasets consisting of abstracts\nrather than full papers, thereby avoiding the distinct challenges associated\nwith processing complete documents. This paper examines the limitations of\ncurrent scientific fact-checking systems and reveals the many potential\nfeatures and resources that could be exploited to advance their performance. It\nidentifies key research challenges within evidence retrieval, including (1)\nevidence-driven retrieval that addresses semantic limitations and topic\nimbalance (2) time-aware evidence retrieval with citation tracking to mitigate\noutdated information, (3) structured document parsing to leverage long-range\ncontext, (4) handling complex scientific expressions, including tables,\nfigures, and domain-specific terminology and (5) assessing the credibility of\nscientific literature. Preliminary experiments were conducted to substantiate\nthese challenges and identify potential solutions. This perspective paper aims\nto advance scientific fact-checking with a specialised IR system tailored for\nreal-world applications."}
{"id": "2508.10955", "pdf": "https://arxiv.org/pdf/2508.10955.pdf", "abs": "https://arxiv.org/abs/2508.10955", "title": "Empowering Multimodal LLMs with External Tools: A Comprehensive Survey", "authors": ["Wenbin An", "Jiahao Nie", "Yaqiang Wu", "Feng Tian", "Shijian Lu", "Qinghua Zheng"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "21 pages, 361 references", "summary": "By integrating the perception capabilities of multimodal encoders with the\ngenerative power of Large Language Models (LLMs), Multimodal Large Language\nModels (MLLMs), exemplified by GPT-4V, have achieved great success in various\nmultimodal tasks, pointing toward a promising pathway to artificial general\nintelligence. Despite this progress, the limited quality of multimodal data,\npoor performance on many complex downstream tasks, and inadequate evaluation\nprotocols continue to hinder the reliability and broader applicability of MLLMs\nacross diverse domains. Inspired by the human ability to leverage external\ntools for enhanced reasoning and problem-solving, augmenting MLLMs with\nexternal tools (e.g., APIs, expert models, and knowledge bases) offers a\npromising strategy to overcome these challenges. In this paper, we present a\ncomprehensive survey on leveraging external tools to enhance MLLM performance.\nOur discussion is structured along four key dimensions about external tools:\n(1) how they can facilitate the acquisition and annotation of high-quality\nmultimodal data; (2) how they can assist in improving MLLM performance on\nchallenging downstream tasks; (3) how they enable comprehensive and accurate\nevaluation of MLLMs; (4) the current limitations and future directions of\ntool-augmented MLLMs. Through this survey, we aim to underscore the\ntransformative potential of external tools in advancing MLLM capabilities,\noffering a forward-looking perspective on their development and applications.\nThe project page of this paper is publicly available\nathttps://github.com/Lackel/Awesome-Tools-for-MLLMs."}
{"id": "2508.10975", "pdf": "https://arxiv.org/pdf/2508.10975.pdf", "abs": "https://arxiv.org/abs/2508.10975", "title": "BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining", "authors": ["Pratyush Maini", "Vineeth Dorna", "Parth Doshi", "Aldo Carranza", "Fan Pan", "Jack Urbanek", "Paul Burstein", "Alex Fang", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Charvi Bannur", "Christina Baek", "Darren Teh", "David Schwab", "Haakon Mongstad", "Haoli Yin", "Josh Wills", "Kaleigh Mentzer", "Luke Merrick", "Ricardo Monti", "Rishabh Adiga", "Siddharth Joshi", "Spandan Das", "Zhengping Wang", "Bogdan Gaza", "Ari Morcos", "Matthew Leavitt"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in large language model (LLM) pretraining have shown that\nsimply scaling data quantity eventually leads to diminishing returns, hitting a\ndata wall. In response, the use of synthetic data for pretraining has emerged\nas a promising paradigm for pushing the frontier of performance. Despite this,\nthe factors affecting synthetic data quality remain poorly understood. In this\nwork, we introduce BeyondWeb, a synthetic data generation framework that\nproduces high-quality synthetic data for pretraining. BeyondWeb significantly\nextends the capabilities of traditional web-scale datasets, outperforming\nstate-of-the-art synthetic pretraining datasets such as Cosmopedia and\nNemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1\npercentage points (pp) and 2.6pp, respectively, when averaged across a suite of\n14 benchmark evaluations. It delivers up to 7.7x faster training than open web\ndata and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for\n180B tokens on BeyondWeb outperforms an 8B model trained for the same token\nbudget on Cosmopedia. We also present several insights from BeyondWeb on\nsynthetic data for pretraining: what drives its benefits, which data to\nrephrase and how, and the impact of model size and family on data quality.\nOverall, our work shows that there's no silver bullet for generating\nhigh-quality synthetic pretraining data. The best outcomes require jointly\noptimizing many factors, a challenging task that requires rigorous science and\npractical expertise. Naive approaches can yield modest improvements,\npotentially at great cost, while well-executed methods can yield transformative\nimprovements, as exemplified by BeyondWeb."}
{"id": "2508.10993", "pdf": "https://arxiv.org/pdf/2508.10993.pdf", "abs": "https://arxiv.org/abs/2508.10993", "title": "Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models", "authors": ["Basile Lewandowski", "Robert Birke", "Lydia Y. Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Text-to-image (T2I) models based on diffusion and transformer architectures\nadvance rapidly. They are often pretrained on large corpora, and openly shared\non a model platform, such as HuggingFace. Users can then build up AI\napplications, e.g., generating media contents, by adopting pretrained T2I\nmodels and fine-tuning them on the target dataset. While public pretrained T2I\nmodels facilitate the democratization of the models, users face a new\nchallenge: which model can be best fine-tuned based on the target data domain?\nModel selection is well addressed in classification tasks, but little is known\nin (pretrained) T2I models and their performance indication on the target\ndomain. In this paper, we propose the first model selection framework, M&C,\nwhich enables users to efficiently choose a pretrained T2I model from a model\nplatform without exhaustively fine-tuning them all on the target dataset. The\ncore of M&C is a matching graph, which consists of: (i) nodes of available\nmodels and profiled datasets, and (ii) edges of model-data and data-data pairs\ncapturing the fine-tuning performance and data similarity, respectively. We\nthen build a model that, based on the inputs of model/data feature, and,\ncritically, the graph embedding feature, extracted from the matching graph,\npredicts the model achieving the best quality after fine-tuning for the target\ndomain. We evaluate M&C on choosing across ten T2I models for 32 datasets\nagainst three baselines. Our results show that M&C successfully predicts the\nbest model for fine-tuning in 61.3% of the cases and a closely performing model\nfor the rest."}
{"id": "2508.11021", "pdf": "https://arxiv.org/pdf/2508.11021.pdf", "abs": "https://arxiv.org/abs/2508.11021", "title": "Can Multi-modal (reasoning) LLMs detect document manipulation?", "authors": ["Zisheng Liang", "Kidus Zewde", "Rudra Pratap Singh", "Disha Patil", "Zexi Chen", "Jiayu Xue", "Yao Yao", "Yifei Chen", "Qinzhe Liu", "Simiao Ren"], "categories": ["cs.CV", "cs.CL"], "comment": "arXiv admin note: text overlap with arXiv:2503.20084", "summary": "Document fraud poses a significant threat to industries reliant on secure and\nverifiable documentation, necessitating robust detection mechanisms. This study\ninvestigates the efficacy of state-of-the-art multi-modal large language models\n(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,\nGrok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and\n3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against\neach other and prior work on document fraud detection techniques using a\nstandard dataset with real transactional documents. Through prompt optimization\nand detailed analysis of the models' reasoning processes, we evaluate their\nability to identify subtle indicators of fraud, such as tampered text,\nmisaligned formatting, and inconsistent transactional sums. Our results reveal\nthat top-performing multi-modal LLMs demonstrate superior zero-shot\ngeneralization, outperforming conventional methods on out-of-distribution\ndatasets, while several vision LLMs exhibit inconsistent or subpar performance.\nNotably, model size and advanced reasoning capabilities show limited\ncorrelation with detection accuracy, suggesting task-specific fine-tuning is\ncritical. This study underscores the potential of multi-modal LLMs in enhancing\ndocument fraud detection systems and provides a foundation for future research\ninto interpretable and scalable fraud mitigation strategies."}
{"id": "2508.11110", "pdf": "https://arxiv.org/pdf/2508.11110.pdf", "abs": "https://arxiv.org/abs/2508.11110", "title": "Diffusion is a code repair operator and generator", "authors": ["Mukul Singh", "Gust Verbruggen", "Vu Le", "Sumit Gulwani"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "12 pages", "summary": "Code diffusion models generate code by iteratively removing noise from the\nlatent representation of a code snippet. During later steps of the diffusion\nprocess, when the code snippet has almost converged, differences between\ndiscrete representations of these snippets look like last-mile repairs applied\nto broken or incomplete code. We evaluate the extent to which this resemblance\ncan be exploited to leverage pre-trained code diffusion models for the problem\nof last-mile repair by considering two applications with significant potential.\nFirst, we can leverage the diffusion model for last-mile repair by adding noise\nto a broken code snippet and resuming the diffusion process. Second, we can\nleverage the diffusion model to generate arbitrary amount of training data for\nlast-mile repair tasks (that are computationally more efficient) by sampling an\nintermediate program (input) and the final program (output) from the diffusion\nprocess. We perform experiments on 3 domains (Python, Excel and PowerShell) to\nevaluate applications, as well as analyze properties."}
{"id": "2508.11116", "pdf": "https://arxiv.org/pdf/2508.11116.pdf", "abs": "https://arxiv.org/abs/2508.11116", "title": "PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing", "authors": ["Zhuoqun Li", "Xuanang Chen", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Paper search is an important activity for researchers, typically involving\nusing a query with description of a topic to find relevant papers. As research\ndeepens, paper search requirements may become more flexible, sometimes\ninvolving specific details such as module configuration rather than being\nlimited to coarse-grained topics. However, previous paper search systems are\nunable to meet these flexible-grained requirements, as these systems mainly\ncollect paper abstracts to construct index of corpus, which lack detailed\ninformation to support retrieval by finer-grained queries. In this work, we\npropose PaperRegister, consisted of offline hierarchical indexing and online\nadaptive retrieval, transforming traditional abstract-based index into\nhierarchical index tree for paper search, thereby supporting queries at\nflexible granularity. Experiments on paper search tasks across a range of\ngranularity demonstrate that PaperRegister achieves the state-of-the-art\nperformance, and particularly excels in fine-grained scenarios, highlighting\nthe good potential as an effective solution for flexible-grained paper search\nin real-world applications. Code for this work is in\nhttps://github.com/Li-Z-Q/PaperRegister."}
{"id": "2508.11122", "pdf": "https://arxiv.org/pdf/2508.11122.pdf", "abs": "https://arxiv.org/abs/2508.11122", "title": "+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking", "authors": ["Xingyu Deng", "Xi Wang", "Mark Stevenson"], "categories": ["cs.IR", "cs.CL"], "comment": "Accpeted for the 34th ACM International Conference on Information and\n  Knowledge Management (CIKM'25)", "summary": "Identification of appropriate supporting evidence is critical to the success\nof scientific fact checking. However, existing approaches rely on off-the-shelf\nInformation Retrieval algorithms that rank documents based on relevance rather\nthan the evidence they provide to support or refute the claim being checked.\nThis paper proposes +VeriRel which includes verification success in the\ndocument ranking. Experimental results on three scientific fact checking\ndatasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently\nleading performance by +VeriRel for document evidence retrieval and a positive\nimpact on downstream verification. This study highlights the potential of\nintegrating verification feedback to document relevance assessment for\neffective scientific fact checking systems. It shows promising future work to\nevaluate fine-grained relevance when examining complex documents for advanced\nscientific fact checking."}
{"id": "2508.11141", "pdf": "https://arxiv.org/pdf/2508.11141.pdf", "abs": "https://arxiv.org/abs/2508.11141", "title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations", "authors": ["Bin Ma", "Yifei Zhang", "Yongjin Xian", "Qi Li", "Linna Zhou", "Gongxun Miao"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Existing rumor detection methods often neglect the content within images as\nwell as the inherent relationships between contexts and images across different\nvisual scales, thereby resulting in the loss of critical information pertinent\nto rumor identification. To address these issues, this paper presents a novel\ncross-modal rumor detection scheme based on contrastive learning, namely the\nMulti-scale Image and Context Correlation exploration algorithm (MICC).\nSpecifically, we design an SCLIP encoder to generate unified semantic\nembeddings for text and multi-scale image patches through contrastive\npretraining, enabling their relevance to be measured via dot-product\nsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is\nintroduced to identify image regions most relevant to the textual semantics,\nguided by mutual information maximization and the information bottleneck\nprinciple, through a Top-K selection strategy based on a cross-modal relevance\nmatrix constructed between the text and multi-scale image patches. Moreover, a\nscale-aware fusion network is designed to integrate the highly correlated\nmulti-scale image features with global text features by assigning adaptive\nweights to image regions based on their semantic importance and cross-modal\nrelevance. The proposed methodology has been extensively evaluated on two\nreal-world datasets. The experimental results demonstrate that it achieves a\nsubstantial performance improvement over existing state-of-the-art approaches\nin rumor detection, highlighting its effectiveness and potential for practical\napplications."}
{"id": "2508.11187", "pdf": "https://arxiv.org/pdf/2508.11187.pdf", "abs": "https://arxiv.org/abs/2508.11187", "title": "Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style", "authors": ["Wonjune Kang", "Deb Roy"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to ASRU 2025", "summary": "We introduce the task of expressive speech retrieval, where the goal is to\nretrieve speech utterances spoken in a given style based on a natural language\ndescription of that style. While prior work has primarily focused on performing\nspeech retrieval based on what was said in an utterance, we aim to do so based\non how something was said. We train speech and text encoders to embed speech\nand text descriptions of speaking styles into a joint latent space, which\nenables using free-form text prompts describing emotions or styles as queries\nto retrieve matching expressive speech segments. We perform detailed analyses\nof various aspects of our proposed framework, including encoder architectures,\ntraining criteria for effective cross-modal alignment, and prompt augmentation\nfor improved generalization to arbitrary text queries. Experiments on multiple\ndatasets encompassing 22 speaking styles demonstrate that our approach achieves\nstrong retrieval performance as measured by Recall@k."}
{"id": "2508.11214", "pdf": "https://arxiv.org/pdf/2508.11214.pdf", "abs": "https://arxiv.org/abs/2508.11214", "title": "How Causal Abstraction Underpins Computational Explanation", "authors": ["Atticus Geiger", "Jacqueline Harding", "Thomas Icard"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Explanations of cognitive behavior often appeal to computations over\nrepresentations. What does it take for a system to implement a given\ncomputation over suitable representational vehicles within that system? We\nargue that the language of causality -- and specifically the theory of causal\nabstraction -- provides a fruitful lens on this topic. Drawing on current\ndiscussions in deep learning with artificial neural networks, we illustrate how\nclassical themes in the philosophy of computation and cognition resurface in\ncontemporary machine learning. We offer an account of computational\nimplementation grounded in causal abstraction, and examine the role for\nrepresentation in the resulting picture. We argue that these issues are most\nprofitably explored in connection with generalization and prediction."}
{"id": "2508.11222", "pdf": "https://arxiv.org/pdf/2508.11222.pdf", "abs": "https://arxiv.org/abs/2508.11222", "title": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal", "authors": ["Haonan Zhang", "Dongxia Wang", "Yi Liu", "Kexin Chen", "Jiashui Wang", "Xinlei Ying", "Long Liu", "Wenhai Wang"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously\nrejecting benign queries due to overly conservative safety measures - a\ncritical functional flaw that undermines their reliability and usability.\nCurrent methods for testing this behavior are demonstrably inadequate,\nsuffering from flawed benchmarks and limited test generation capabilities, as\nhighlighted by our empirical user study. To the best of our knowledge, this\npaper introduces the first evolutionary testing framework, ORFuzz, for the\nsystematic detection and analysis of LLM over-refusals. ORFuzz uniquely\nintegrates three core components: (1) safety category-aware seed selection for\ncomprehensive test coverage, (2) adaptive mutator optimization using reasoning\nLLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge\nmodel validated to accurately reflect user perception of toxicity and refusal.\nOur extensive evaluations demonstrate that ORFuzz generates diverse, validated\nover-refusal instances at a rate (6.98% average) more than double that of\nleading baselines, effectively uncovering vulnerabilities. Furthermore,\nORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly\ntransferable test cases that achieves a superior 63.56% average over-refusal\nrate across 10 diverse LLMs, significantly outperforming existing datasets.\nORFuzz and ORFuzzSet provide a robust automated testing framework and a\nvaluable community resource, paving the way for developing more reliable and\ntrustworthy LLM-based software systems."}
{"id": "2508.11224", "pdf": "https://arxiv.org/pdf/2508.11224.pdf", "abs": "https://arxiv.org/abs/2508.11224", "title": "Benchmarking Prosody Encoding in Discrete Speech Tokens", "authors": ["Kentaro Onda", "Satoru Fukayama", "Daisuke Saito", "Nobuaki Minematsu"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted by ASRU2025", "summary": "Recently, discrete tokens derived from self-supervised learning (SSL) models\nvia k-means clustering have been actively studied as pseudo-text in speech\nlanguage models and as efficient intermediate representations for various\ntasks. However, these discrete tokens are typically learned in advance,\nseparately from the training of language models or downstream tasks. As a\nresult, choices related to discretization, such as the SSL model used or the\nnumber of clusters, must be made heuristically. In particular, speech language\nmodels are expected to understand and generate responses that reflect not only\nthe semantic content but also prosodic features. Yet, there has been limited\nresearch on the ability of discrete tokens to capture prosodic information. To\naddress this gap, this study conducts a comprehensive analysis focusing on\nprosodic encoding based on their sensitivity to the artificially modified\nprosody, aiming to provide practical guidelines for designing discrete tokens."}
{"id": "2508.11252", "pdf": "https://arxiv.org/pdf/2508.11252.pdf", "abs": "https://arxiv.org/abs/2508.11252", "title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information", "authors": ["Youcheng Huang", "Bowen Qin", "Chen Huang", "Duanyu Feng", "Xi Yang", "Wenqiang Lei"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving\nabilities in mathematics, as evaluated by existing benchmarks exclusively on\nwell-defined problems. However, such evaluation setup constitutes a critical\ngap, since a genuine intelligent agent should not only solve problems (as a\nmath quiz solver), but also be able~to ask for information when the problems\nlack sufficient information, enabling proactivity in responding users'\nrequests. To bridge such gap, we proposes a new dataset consisting of two types\nof incomplete problems with diverse contexts. Based on the dataset, our\nsystematical evaluation of LRMs reveals their inability in proactively asking\nfor information. In addition, we uncover the behaviors related to overthinking\nand hallucination of LRMs, and highlight the potential and challenges of\nsupervised fine-tuning in learning such ability. We hope to provide new\ninsights in developing LRMs with genuine intelligence, rather than just solving\nproblems."}
{"id": "2508.11258", "pdf": "https://arxiv.org/pdf/2508.11258.pdf", "abs": "https://arxiv.org/abs/2508.11258", "title": "Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing", "authors": ["Ruicheng Xian", "Yuxuan Wan", "Han Zhao"], "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": null, "summary": "Instruction fine-tuned large language models (LLMs) enable a simple zero-shot\nor few-shot prompting paradigm, also known as in-context learning, for building\nprediction models. This convenience, combined with continued advances in LLM\ncapability, has the potential to drive their adoption across a broad range of\ndomains, including high-stakes applications where group fairness -- preventing\ndisparate impacts across demographic groups -- is essential. The majority of\nexisting approaches to enforcing group fairness on LLM-based classifiers rely\non traditional fair algorithms applied via model fine-tuning or head-tuning on\nfinal-layer embeddings, but they are no longer applicable to closed-weight LLMs\nunder the in-context learning setting, which include some of the most capable\ncommercial models today, such as GPT-4, Gemini, and Claude. In this paper, we\npropose a framework for deriving fair classifiers from closed-weight LLMs via\nprompting: the LLM is treated as a feature extractor, and features are elicited\nfrom its probabilistic predictions (e.g., token log probabilities) using\nprompts strategically designed for the specified fairness criterion to obtain\nsufficient statistics for fair classification; a fair algorithm is then applied\nto these features to train a lightweight fair classifier in a post-hoc manner.\nExperiments on five datasets, including three tabular ones, demonstrate strong\naccuracy-fairness tradeoffs for the classifiers derived by our framework from\nboth open-weight and closed-weight LLMs; in particular, our framework is\ndata-efficient and outperforms fair classifiers trained on LLM embeddings\n(i.e., head-tuning) or from scratch on raw tabular features."}
{"id": "2508.11328", "pdf": "https://arxiv.org/pdf/2508.11328.pdf", "abs": "https://arxiv.org/abs/2508.11328", "title": "Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning", "authors": ["Haitong Luo", "Suhang Wang", "Weiyao Zhang", "Ruiqi Meng", "Xuying Meng", "Yujun Zhang"], "categories": ["cs.LG", "cs.CL"], "comment": "Under Review", "summary": "Graph ``pre-training and prompt-tuning'' aligns downstream tasks with\npre-trained objectives to enable efficient knowledge transfer under limited\nsupervision. However, existing methods rely on homophily-based low-frequency\nknowledge, failing to handle diverse spectral distributions in real-world\ngraphs with varying homophily. Our theoretical analysis reveals a spectral\nspecificity principle: optimal knowledge transfer requires alignment between\npre-trained spectral filters and the intrinsic spectrum of downstream graphs.\nUnder limited supervision, large spectral gaps between pre-training and\ndownstream tasks impede effective adaptation. To bridge this gap, we propose\nthe HS-GPPT model, a novel framework that ensures spectral alignment throughout\nboth pre-training and prompt-tuning. We utilize a hybrid spectral filter\nbackbone and local-global contrastive learning to acquire abundant spectral\nknowledge. Then we design prompt graphs to align the spectral distribution with\npretexts, facilitating spectral knowledge transfer across homophily and\nheterophily. Extensive experiments validate the effectiveness under both\ntransductive and inductive learning settings. Our code is available at\nhttps://anonymous.4open.science/r/HS-GPPT-62D2/."}
{"id": "2508.11452", "pdf": "https://arxiv.org/pdf/2508.11452.pdf", "abs": "https://arxiv.org/abs/2508.11452", "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking."}
{"id": "2508.11566", "pdf": "https://arxiv.org/pdf/2508.11566.pdf", "abs": "https://arxiv.org/abs/2508.11566", "title": "Emphasis Sensitivity in Speech Representations", "authors": ["Shaun Cassini", "Thomas Hain", "Anton Ragni"], "categories": ["eess.AS", "cs.CL"], "comment": "Accepted to IEEE ASRU 2025", "summary": "This work investigates whether modern speech models are sensitive to prosodic\nemphasis - whether they encode emphasized and neutral words in systematically\ndifferent ways. Prior work typically relies on isolated acoustic correlates\n(e.g., pitch, duration) or label prediction, both of which miss the relational\nstructure of emphasis. This paper proposes a residual-based framework, defining\nemphasis as the difference between paired neutral and emphasized word\nrepresentations. Analysis on self-supervised speech models shows that these\nresiduals correlate strongly with duration changes and perform poorly at word\nidentity prediction, indicating a structured, relational encoding of prosodic\nemphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% more\ncompact than in pre-trained models, further suggesting that emphasis is encoded\nas a consistent, low-dimensional transformation that becomes more structured\nwith task-specific learning."}
{"id": "2508.11616", "pdf": "https://arxiv.org/pdf/2508.11616.pdf", "abs": "https://arxiv.org/abs/2508.11616", "title": "Controlling Multimodal LLMs via Reward-guided Decoding", "authors": ["Oscar Mañas", "Pierluca D'Oro", "Koustuv Sinha", "Adriana Romero-Soriano", "Michal Drozdzal", "Aishwarya Agrawal"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Published at ICCV 2025", "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods."}
{"id": "2402.18013", "pdf": "https://arxiv.org/pdf/2402.18013.pdf", "abs": "https://arxiv.org/abs/2402.18013", "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems", "authors": ["Zihao Yi", "Jiarui Ouyang", "Zhe Xu", "Yuwen Liu", "Tianhao Liao", "Haohao Luo", "Ying Shen"], "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 10 figures, ACM Computing Surveys", "summary": "This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems."}
{"id": "2410.01671", "pdf": "https://arxiv.org/pdf/2410.01671.pdf", "abs": "https://arxiv.org/abs/2410.01671", "title": "Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding", "authors": ["Yanming Liu", "Xinyue Peng", "Jiannan Cao", "Yanxin Shen", "Tianyu Du", "Sheng Cheng", "Xun Wang", "Jianwei Yin", "Xuhong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025 camera ready version, with second updated metadata", "summary": "Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing; however, they still face difficulties when tasked with\nunderstanding lengthy contexts and executing effective question answering.\nThese challenges often arise due to the complexity and ambiguity present in\nlonger texts. To enhance the performance of LLMs in such scenarios, we\nintroduce the Long Question Coreference Adaptation (LQCA) method. This\ninnovative framework focuses on coreference resolution tailored to long\ncontexts, allowing the model to identify and manage references effectively. The\nLQCA method encompasses four key steps: resolving coreferences within\nsub-documents, computing the distances between mentions, defining a\nrepresentative mention for coreference, and answering questions through mention\nreplacement. By processing information systematically, the framework provides\neasier-to-handle partitions for LLMs, promoting better understanding.\nExperimental evaluations on a range of LLMs and datasets have yielded positive\nresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,\nhighlighting the effectiveness of leveraging coreference resolution to bridge\ncontext gaps in question answering. Our code is public at\nhttps://github.com/OceannTwT/LQCA."}
{"id": "2410.16502", "pdf": "https://arxiv.org/pdf/2410.16502.pdf", "abs": "https://arxiv.org/abs/2410.16502", "title": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning", "authors": ["Jason Chan", "Robert Gaizauskas", "Zhixue Zhao"], "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "Formal logic enables computers to reason in natural language by representing\nsentences in symbolic forms and applying rules to derive conclusions. However,\nin what our study characterizes as \"rulebreaker\" scenarios, this method can\nlead to conclusions that are typically not inferred or accepted by humans given\ntheir common sense and factual knowledge. Inspired by works in cognitive\nscience, we create RULEBREAKERS, the first dataset for rigorously evaluating\nthe ability of large language models (LLMs) to recognize and respond to\nrulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven\nLLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on\nRULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules\nunlike what is expected from typical human reasoners. Further analysis suggests\nthat this apparent failure is potentially associated with the models' poor\nutilization of their world knowledge and their attention distribution patterns.\nWhilst revealing a limitation of current LLMs, our study also provides a timely\ncounterbalance to a growing body of recent works that propose methods relying\non formal logic to improve LLMs' general reasoning capabilities, highlighting\ntheir risk of further increasing divergence between LLMs and human-like\nreasoning."}
{"id": "2412.11736", "pdf": "https://arxiv.org/pdf/2412.11736.pdf", "abs": "https://arxiv.org/abs/2412.11736", "title": "Personalized LLM for Generating Customized Responses to the Same Query from Different Users", "authors": ["Hang Zeng", "Chaoyue Niu", "Fan Wu", "Chengfei Lv", "Guihai Chen"], "categories": ["cs.CL"], "comment": "Accepted by CIKM'25", "summary": "Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLMs, but overlooked the diversity of queriers.\nIn this work, we propose a new form of querier-aware LLM personalization,\ngenerating different responses even for the same query from different queriers.\nWe design a dual-tower model architecture with a cross-querier general encoder\nand a querier-specific encoder. We further apply contrastive learning with\nmulti-view augmentation, pulling close the dialogue representations of the same\nquerier, while pulling apart those of different queriers. To mitigate the\nimpact of query diversity on querier-contrastive learning, we cluster the\ndialogues based on query similarity and restrict the scope of contrastive\nlearning within each cluster. To address the lack of datasets designed for\nquerier-aware personalization, we also build a multi-querier dataset from\nEnglish and Chinese scripts, as well as WeChat records, called MQDialog,\ncontaining 173 queriers and 12 responders. Extensive evaluations demonstrate\nthat our design significantly improves the quality of personalized response\ngeneration, achieving relative improvement of 8.4% to 48.7% in ROUGE-L scores\nand winning rates ranging from 54% to 82% compared with various baseline\nmethods."}
{"id": "2502.12052", "pdf": "https://arxiv.org/pdf/2502.12052.pdf", "abs": "https://arxiv.org/abs/2502.12052", "title": "A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability", "authors": ["Xinyu Hu", "Mingqi Gao", "Li Lin", "Zhenghan Yu", "Xiaojun Wan"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "In NLG meta-evaluation, evaluation metrics are typically assessed based on\ntheir consistency with humans. However, we identify some limitations in\ntraditional NLG meta-evaluation approaches, such as issues in handling human\nratings and ambiguous selections of correlation measures, which undermine the\neffectiveness of meta-evaluation. In this work, we propose a dual-perspective\nNLG meta-evaluation framework that focuses on different evaluation\ncapabilities, thereby providing better interpretability. In addition, we\nintroduce a method of automatically constructing the corresponding benchmarks\nwithout requiring new human annotations. Furthermore, we conduct experiments\nwith 16 representative LLMs as the evaluators based on our proposed framework,\ncomprehensively analyzing their evaluation performance from different\nperspectives."}
{"id": "2502.16636", "pdf": "https://arxiv.org/pdf/2502.16636.pdf", "abs": "https://arxiv.org/abs/2502.16636", "title": "Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries", "authors": ["Yin Wu", "Quanyu Long", "Jing Li", "Jianfei Yu", "Wenya Wang"], "categories": ["cs.CL", "cs.CV"], "comment": "21 pages, 6 figures, 17 tables", "summary": "Retrieval-augmented generation (RAG) is a paradigm that augments large\nlanguage models (LLMs) with external knowledge to tackle knowledge-intensive\nquestion answering. While several benchmarks evaluate Multimodal LLMs (MLLMs)\nunder Multimodal RAG settings, they predominantly retrieve from textual corpora\nand do not explicitly assess how models exploit visual evidence during\ngeneration. Consequently, there still lacks benchmark that isolates and\nmeasures the contribution of retrieved images in RAG. We introduce Visual-RAG,\na question-answering benchmark that targets visually grounded,\nknowledge-intensive questions. Unlike prior work, Visual-RAG requires\ntext-to-image retrieval and the integration of retrieved clue images to extract\nvisual evidence for answer generation. With Visual-RAG, we evaluate 5\nopen-source and 3 proprietary MLLMs, showcasing that images provide strong\nevidence in augmented generation. However, even state-of-the-art models\nstruggle to efficiently extract and utilize visual knowledge. Our results\nhighlight the need for improved visual retrieval, grounding, and attribution in\nmultimodal RAG systems."}
{"id": "2503.01307", "pdf": "https://arxiv.org/pdf/2503.01307.pdf", "abs": "https://arxiv.org/abs/2503.01307", "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs", "authors": ["Kanishk Gandhi", "Ayush Chakravarthy", "Anikait Singh", "Nathan Lile", "Noah D. Goodman"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time inference has emerged as a powerful paradigm for enabling language\nmodels to ``think'' longer and more carefully about complex challenges, much\nlike skilled human experts. While reinforcement learning (RL) can drive\nself-improvement in language models on verifiable tasks, some models exhibit\nsubstantial gains while others quickly plateau. For instance, we find that\nQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game\nof Countdown. This discrepancy raises a critical question: what intrinsic\nproperties enable effective self-improvement? We introduce a framework to\ninvestigate this question by analyzing four key cognitive behaviors --\nverification, backtracking, subgoal setting, and backward chaining -- that both\nexpert human problem solvers and successful language models employ. Our study\nreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama\ninitially lacks them. In systematic experimentation with controlled behavioral\ndatasets, we find that priming Llama with examples containing these reasoning\nbehaviors enables substantial improvements during RL, matching or exceeding\nQwen's performance. Importantly, the presence of reasoning behaviors, rather\nthan correctness of answers, proves to be the critical factor -- models primed\nwith incorrect solutions containing proper reasoning patterns achieve\ncomparable performance to those trained on correct solutions. Finally,\nleveraging continued pretraining with OpenWebMath data, filtered to amplify\nreasoning behaviors, enables the Llama model to match Qwen's self-improvement\ntrajectory. Our findings establish a fundamental relationship between initial\nreasoning behaviors and the capacity for improvement, explaining why some\nlanguage models effectively utilize additional computation while others\nplateau."}
{"id": "2503.17811", "pdf": "https://arxiv.org/pdf/2503.17811.pdf", "abs": "https://arxiv.org/abs/2503.17811", "title": "Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models", "authors": ["Wenqi Pei", "Hailing Xu", "Hengyuan Zhao", "Shizheng Hou", "Han Chen", "Zining Zhang", "Pingyi Luo", "Bingsheng He"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "DL4C @ ICLR 2025", "summary": "Natural Language to SQL (NL2SQL) has seen significant advancements with large\nlanguage models (LLMs). However, these models often depend on closed-source\nsystems and high computational resources, posing challenges in data privacy and\ndeployment. In contrast, small language models (SLMs) struggle with NL2SQL\ntasks, exhibiting poor performance and incompatibility with existing\nframeworks. To address these issues, we introduce Feather-SQL, a new\nlightweight framework tailored for SLMs. Feather-SQL improves SQL executability\nand accuracy through 1) schema pruning and linking, 2) multi-path and\nmulti-candidate generation. Additionally, we introduce the 1+1 Model\nCollaboration Paradigm, which pairs a strong general-purpose chat model with a\nfine-tuned SQL specialist, combining strong analytical reasoning with\nhigh-precision SQL generation. Experimental results on BIRD demonstrate that\nFeather-SQL improves NL2SQL performance on SLMs, with around 10% boost for\nmodels without fine-tuning. The proposed paradigm raises the accuracy ceiling\nof SLMs to 54.76%, highlighting its effectiveness."}
{"id": "2504.00799", "pdf": "https://arxiv.org/pdf/2504.00799.pdf", "abs": "https://arxiv.org/abs/2504.00799", "title": "Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users", "authors": ["Shiyang Zhang", "Fanfei Meng", "Xi Wang", "Lan Li"], "categories": ["cs.CL", "cs.HC", "H.5.2; I.2.7"], "comment": "The scope of the work has evolved significantly since initial\n  submission, and we are preparing a revised version that better reflects the\n  current direction of the research", "summary": "Electronic dictionaries have largely replaced paper dictionaries and become\ncentral tools for L2 learners seeking to expand their vocabulary. Users often\nassume these resources are reliable and rarely question the validity of the\ndefinitions provided. The accuracy of major E-dictionaries is seldom\nscrutinized, and little attention has been paid to how their corpora are\nconstructed. Research on dictionary use, particularly the limitations of\nelectronic dictionaries, remains scarce. This study adopts a combined method of\nexperimentation, user survey, and dictionary critique to examine Youdao, one of\nthe most widely used E-dictionaries in China. The experiment involved a\ntranslation task paired with retrospective reflection. Participants were asked\nto translate sentences containing words that are insufficiently or inaccurately\ndefined in Youdao. Their consultation behavior was recorded to analyze how\nfaulty definitions influenced comprehension. Results show that incomplete or\nmisleading definitions can cause serious misunderstandings. Additionally,\nstudents exhibited problematic consultation habits. The study further explores\nhow such flawed definitions originate, highlighting issues in data processing\nand the integration of AI and machine learning technologies in dictionary\nconstruction. The findings suggest a need for better training in dictionary\nliteracy for users, as well as improvements in the underlying AI models used to\nbuild E-dictionaries."}
{"id": "2504.21681", "pdf": "https://arxiv.org/pdf/2504.21681.pdf", "abs": "https://arxiv.org/abs/2504.21681", "title": "Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders", "authors": ["Andrei-Alexandru Manea", "Jindřich Libovický"], "categories": ["cs.CL"], "comment": null, "summary": "Most pre-trained Vision-Language (VL) models and training data for the\ndownstream tasks are only available in English. Therefore, multilingual VL\ntasks are solved using cross-lingual transfer: fine-tune a multilingual\npre-trained model or transfer the text encoder using parallel data. We study\nthe alternative approach: transferring an already trained encoder using\nparallel data. We investigate the effect of parallel data: domain and the\nnumber of languages, which were out of focus in previous work. Our results show\nthat even machine-translated task data are the best on average, caption-like\nauthentic parallel data outperformed it in some languages. Further, we show\nthat most languages benefit from multilingual training."}
{"id": "2506.06371", "pdf": "https://arxiv.org/pdf/2506.06371.pdf", "abs": "https://arxiv.org/abs/2506.06371", "title": "Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models", "authors": ["Panagiotis Koletsis", "Christos Panagiotopoulos", "Georgios Th. Papadopoulos", "Vasilis Efthymiou"], "categories": ["cs.CL"], "comment": null, "summary": "Over the past few years, table interpretation tasks have made significant\nprogress due to their importance and the introduction of new technologies and\nbenchmarks in the field. This work experiments with a hybrid approach for\ndetecting relationships among columns of unlabeled tabular data, using a\nKnowledge Graph (KG) as a reference point, a task known as CPA. This approach\nleverages large language models (LLMs) while employing statistical analysis to\nreduce the search space of potential KG relations. The main modules of this\napproach for reducing the search space are domain and range constraints\ndetection, as well as relation co-appearance analysis. The experimental\nevaluation on two benchmark datasets provided by the SemTab challenge assesses\nthe influence of each module and the effectiveness of different\nstate-of-the-art LLMs at various levels of quantization. The experiments were\nperformed, as well as at different prompting techniques. The proposed\nmethodology, which is publicly available on github, proved to be competitive\nwith state-of-the-art approaches on these datasets."}
{"id": "2508.00344", "pdf": "https://arxiv.org/pdf/2508.00344.pdf", "abs": "https://arxiv.org/abs/2508.00344", "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "authors": ["Keer Lu", "Chong Chen", "Bin Cui", "Huang Leng", "Wentao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale."}
{"id": "2508.09057", "pdf": "https://arxiv.org/pdf/2508.09057.pdf", "abs": "https://arxiv.org/abs/2508.09057", "title": "MVISU-Bench: Benchmarking Mobile Agents for Real-World Tasks by Multi-App, Vague, Interactive, Single-App and Unethical Instructions", "authors": ["Zeyu Huang", "Juyuan Wang", "Longfeng Chen", "Boyi Xiao", "Leng Cai", "Yawen Zeng", "Jin Xu"], "categories": ["cs.CL"], "comment": "ACM MM 2025", "summary": "Given the significant advances in Large Vision Language Models (LVLMs) in\nreasoning and visual understanding, mobile agents are rapidly emerging to meet\nusers' automation needs. However, existing evaluation benchmarks are\ndisconnected from the real world and fail to adequately address the diverse and\ncomplex requirements of users. From our extensive collection of user\nquestionnaire, we identified five tasks: Multi-App, Vague, Interactive,\nSingle-App, and Unethical Instructions. Around these tasks, we present\n\\textbf{MVISU-Bench}, a bilingual benchmark that includes 404 tasks across 137\nmobile applications. Furthermore, we propose Aider, a plug-and-play module that\nacts as a dynamic prompt prompter to mitigate risks and clarify user intent for\nmobile agents. Our Aider is easy to integrate into several frameworks and has\nsuccessfully improved overall success rates by 19.55\\% compared to the current\nstate-of-the-art (SOTA) on MVISU-Bench. Specifically, it achieves success rate\nimprovements of 53.52\\% and 29.41\\% for unethical and interactive instructions,\nrespectively. Through extensive experiments and analysis, we highlight the gap\nbetween existing mobile agents and real-world user expectations."}
{"id": "2508.09666", "pdf": "https://arxiv.org/pdf/2508.09666.pdf", "abs": "https://arxiv.org/abs/2508.09666", "title": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation", "authors": ["Ziyang Ma", "Qingyue Yuan", "Linhai Zhang", "Deyu Zhou"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs."}
{"id": "2508.09991", "pdf": "https://arxiv.org/pdf/2508.09991.pdf", "abs": "https://arxiv.org/abs/2508.09991", "title": "Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry", "authors": ["Lovedeep Gondara", "Gregory Arbour", "Raymond Ng", "Jonathan Simkin", "Shebnum Devji"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "Automating data extraction from clinical documents offers significant\npotential to improve efficiency in healthcare settings, yet deploying Natural\nLanguage Processing (NLP) solutions presents practical challenges. Drawing upon\nour experience implementing various NLP models for information extraction and\nclassification tasks at the British Columbia Cancer Registry (BCCR), this paper\nshares key lessons learned throughout the project lifecycle. We emphasize the\ncritical importance of defining problems based on clear business objectives\nrather than solely technical accuracy, adopting an iterative approach to\ndevelopment, and fostering deep interdisciplinary collaboration and co-design\ninvolving domain experts, end-users, and ML specialists from inception. Further\ninsights highlight the need for pragmatic model selection (including hybrid\napproaches and simpler methods where appropriate), rigorous attention to data\nquality (representativeness, drift, annotation), robust error mitigation\nstrategies involving human-in-the-loop validation and ongoing audits, and\nbuilding organizational AI literacy. These practical considerations,\ngeneralizable beyond cancer registries, provide guidance for healthcare\norganizations seeking to successfully implement AI/NLP solutions to enhance\ndata management processes and ultimately improve patient care and public health\noutcomes."}
{"id": "2508.10016", "pdf": "https://arxiv.org/pdf/2508.10016.pdf", "abs": "https://arxiv.org/abs/2508.10016", "title": "Training-Free Multimodal Large Language Model Orchestration", "authors": ["Tianyu Xie", "Yuhang Wu", "Yongdong Luo", "Jiayi Ji", "Xiawu Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "Different Multimodal Large Language Models (MLLMs) cannot be integrated into\na unified multimodal input-output system directly. In previous work, training\nhas been considered as an inevitable component due to challenges in modal\nalignment, Text-to-Speech efficiency and other integration issues. In this\npaper, we introduce Multimodal Large Language Model Orchestration, an effective\napproach for creating interactive multimodal AI systems without additional\ntraining. MLLM Orchestration leverages the inherent reasoning capabilities of\nlarge language models to coordinate specialized models through explicit\nworkflows, enabling natural multimodal interactions while maintaining\nmodularity, improving interpretability, and significantly enhancing\ncomputational efficiency. Our orchestration framework is built upon three key\ninnovations: (1) a central controller LLM that analyzes user inputs and\ndynamically routes tasks to appropriate specialized models through carefully\ndesigned agents; (2) a parallel Text-to-Speech architecture that enables true\nfull-duplex interaction with seamless interruption handling and natural\nconversational flow; and (3) a cross-modal memory integration system that\nmaintains coherent context across modalities through intelligent information\nsynthesis and retrieval, selectively avoiding unnecessary modality calls in\ncertain scenarios to improve response speed. Extensive evaluations demonstrate\nthat MLLM Orchestration achieves comprehensive multimodal capabilities without\nadditional training, performance improvements of up to 7.8% over traditional\njointly-trained approaches on standard benchmarks, reduced latency by 10.3%,\nand significantly enhanced interpretability through explicit orchestration\nprocesses."}
{"id": "2508.10482", "pdf": "https://arxiv.org/pdf/2508.10482.pdf", "abs": "https://arxiv.org/abs/2508.10482", "title": "When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing", "authors": ["Mahdi Dhaini", "Stephen Meisenbacher", "Ege Erdogan", "Florian Matthes", "Gjergji Kasneci"], "categories": ["cs.CL"], "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "In the study of trustworthy Natural Language Processing (NLP), a number of\nimportant research fields have emerged, including that of explainability and\nprivacy. While research interest in both explainable and privacy-preserving NLP\nhas increased considerably in recent years, there remains a lack of\ninvestigation at the intersection of the two. This leaves a considerable gap in\nunderstanding of whether achieving both explainability and privacy is possible,\nor whether the two are at odds with each other. In this work, we conduct an\nempirical investigation into the privacy-explainability trade-off in the\ncontext of NLP, guided by the popular overarching methods of Differential\nPrivacy (DP) and Post-hoc Explainability. Our findings include a view into the\nintricate relationship between privacy and explainability, which is formed by a\nnumber of factors, including the nature of the downstream task and choice of\nthe text privatization and explainability method. In this, we highlight the\npotential for privacy and explainability to co-exist, and we summarize our\nfindings in a collection of practical recommendations for future work at this\nimportant intersection."}
{"id": "2406.03807", "pdf": "https://arxiv.org/pdf/2406.03807.pdf", "abs": "https://arxiv.org/abs/2406.03807", "title": "Tool-Planner: Task Planning with Clusters across Multiple Tools", "authors": ["Yanming Liu", "Xinyue Peng", "Jiannan Cao", "Yuwei Zhang", "Xuhong Zhang", "Sheng Cheng", "Xun Wang", "Jianwei Yin", "Tianyu Du"], "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": "ICLR 2025 Camera Ready version", "summary": "Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\nhttps://github.com/OceannTwT/Tool-Planner"}
{"id": "2406.10450", "pdf": "https://arxiv.org/pdf/2406.10450.pdf", "abs": "https://arxiv.org/abs/2406.10450", "title": "TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation", "authors": ["Haohao Qu", "Wenqi Fan", "Zihuai Zhao", "Qing Li"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted by IEEE TKDE. Codes and data are available at\n  https://github.com/Quhaoh233/TokenRec", "summary": "There is a growing interest in utilizing large-scale language models (LLMs)\nto advance next-generation Recommender Systems (RecSys), driven by their\noutstanding language understanding and in-context learning capabilities. In\nthis scenario, tokenizing (i.e., indexing) users and items becomes essential\nfor ensuring a seamless alignment of LLMs with recommendations. While several\nstudies have made progress in representing users and items through textual\ncontents or latent representations, challenges remain in efficiently capturing\nhigh-order collaborative knowledge into discrete tokens that are compatible\nwith LLMs. Additionally, the majority of existing tokenization approaches often\nface difficulties in generalizing effectively to new/unseen users or items that\nwere not in the training corpus. To address these challenges, we propose a\nnovel framework called TokenRec, which introduces not only an effective ID\ntokenization strategy but also an efficient retrieval paradigm for LLM-based\nrecommendations. Specifically, our tokenization strategy, Masked\nVector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item\nrepresentations learned from collaborative filtering into discrete tokens, thus\nachieving a smooth incorporation of high-order collaborative knowledge and a\ngeneralizable tokenization of users and items for LLM-based RecSys. Meanwhile,\nour generative retrieval paradigm is designed to efficiently recommend top-$K$\nitems for users to eliminate the need for the time-consuming auto-regressive\ndecoding and beam search processes used by LLMs, thus significantly reducing\ninference time. Comprehensive experiments validate the effectiveness of the\nproposed methods, demonstrating that TokenRec outperforms competitive\nbenchmarks, including both traditional recommender systems and emerging\nLLM-based recommender systems."}
{"id": "2502.06173", "pdf": "https://arxiv.org/pdf/2502.06173.pdf", "abs": "https://arxiv.org/abs/2502.06173", "title": "Uncertainty-Aware Adaptation of Large Language Models for Protein-Protein Interaction Analysis", "authors": ["Sanket Jantre", "Tianle Wang", "Gilchan Park", "Kriti Chopra", "Nicholas Jeon", "Xiaoning Qian", "Nathan M. Urban", "Byung-Jun Yoon"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.AP", "stat.ML"], "comment": null, "summary": "Identification of protein-protein interactions (PPIs) helps derive cellular\nmechanistic understanding, particularly in the context of complex conditions\nsuch as neurodegenerative disorders, metabolic syndromes, and cancer. Large\nLanguage Models (LLMs) have demonstrated remarkable potential in predicting\nprotein structures and interactions via automated mining of vast biomedical\nliterature; yet their inherent uncertainty remains a key challenge for deriving\nreproducible findings, critical for biomedical applications. In this study, we\npresent an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging\nfine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we\nintegrate LoRA ensembles and Bayesian LoRA models for uncertainty\nquantification (UQ), ensuring confidence-calibrated insights into protein\nbehavior. Our approach achieves competitive performance in PPI identification\nacross diverse disease contexts while addressing model uncertainty, thereby\nenhancing trustworthiness and reproducibility in computational biology. These\nfindings underscore the potential of uncertainty-aware LLM adaptation for\nadvancing precision medicine and biomedical research."}
{"id": "2502.12159", "pdf": "https://arxiv.org/pdf/2502.12159.pdf", "abs": "https://arxiv.org/abs/2502.12159", "title": "Causal Language in Observational Studies: Sociocultural Backgrounds and Team Composition", "authors": ["Jun Wang", "Bei Yu"], "categories": ["physics.soc-ph", "cs.CL"], "comment": "17 pages, 3 figures, 3 tables", "summary": "The use of causal language in observational studies has raised concerns about\noverstatement in scientific communication. While some argue that such language\nshould be reserved for randomized controlled trials, others contend that\nrigorous causal inference methods can justify causal claims in observational\nresearch. Ideally, causal language should align with the strength of the\nunderlying evidence. However, through the analysis of over 90,000 abstracts\nfrom observational studies using computational linguistic and regression\nmethods, we found that causal language are more common in work by less\nexperienced authors, smaller research teams, male last authors, and researchers\nfrom countries with higher uncertainty avoidance indices. Our findings suggest\nthat the use of causal language is not solely driven by the strength of\nevidence, but also by the sociocultural backgrounds of authors and their team\ncomposition. This work provides a new perspective for understanding systematic\nvariations in scientific communication and emphasizes the importance of\nrecognizing these human factors when evaluating scientific claims."}
{"id": "2503.22402", "pdf": "https://arxiv.org/pdf/2503.22402.pdf", "abs": "https://arxiv.org/abs/2503.22402", "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing", "authors": ["Yizhang Zhu", "Runzhi Jiang", "Boyan Li", "Nan Tang", "Yuyu Luo"], "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": "COLM 2025", "summary": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL. Our\nsource code and model are available at https://elliesql.github.io/."}
{"id": "2505.05422", "pdf": "https://arxiv.org/pdf/2505.05422.pdf", "abs": "https://arxiv.org/abs/2505.05422", "title": "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation", "authors": ["Haokun Lin", "Teng Wang", "Yixiao Ge", "Yuying Ge", "Zhichao Lu", "Ying Wei", "Qingfu Zhang", "Zhenan Sun", "Ying Shan"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Technical Report", "summary": "Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP."}
{"id": "2506.07468", "pdf": "https://arxiv.org/pdf/2506.07468.pdf", "abs": "https://arxiv.org/abs/2506.07468", "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models", "authors": ["Mickel Liu", "Liwei Jiang", "Yancheng Liang", "Simon Shaolei Du", "Yejin Choi", "Tim Althoff", "Natasha Jaques"], "categories": ["cs.LG", "cs.CL", "cs.MA"], "comment": null, "summary": "Conventional language model (LM) safety alignment relies on a reactive,\ndisjoint procedure: attackers exploit a static model, followed by defensive\nfine-tuning to patch exposed vulnerabilities. This sequential approach creates\na mismatch -- attackers overfit to obsolete defenses, while defenders\nperpetually lag behind emerging threats. To address this, we propose\nSelf-RedTeam, an online self-play reinforcement learning algorithm where an\nattacker and defender agent co-evolve through continuous interaction. We cast\nsafety alignment as a two-player zero-sum game, where a single model alternates\nbetween attacker and defender roles -- generating adversarial prompts and\nsafeguarding against them -- while a reward LM adjudicates outcomes. This\nenables dynamic co-adaptation. Grounded in the game-theoretic framework of\nzero-sum games, we establish a theoretical safety guarantee which motivates the\ndesign of our method: if self-play converges to a Nash Equilibrium, the\ndefender will reliably produce safe responses to any adversarial input.\nEmpirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared\nto attackers trained against static defenders and achieves higher robustness on\nsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained\nagainst static attackers. We further propose hidden Chain-of-Thought, allowing\nagents to plan privately, which boosts adversarial diversity and reduces\nover-refusals. Our results motivate a shift from reactive patching to proactive\nco-evolution in LM safety training, enabling scalable, autonomous, and robust\nself-improvement of LMs via multi-agent reinforcement learning (MARL)."}
{"id": "2506.10054", "pdf": "https://arxiv.org/pdf/2506.10054.pdf", "abs": "https://arxiv.org/abs/2506.10054", "title": "Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs", "authors": ["Shangpin Peng", "Weinong Wang", "Zhuotao Tian", "Senqiao Yang", "Xing Wu", "Haotian Xu", "Chengquan Zhang", "Takashi Isobe", "Baotian Hu", "Min Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Direct Preference Optimization (DPO) has become a cornerstone of\nreinforcement learning from human feedback (RLHF) due to its simplicity and\nefficiency. However, existing DPO-based approaches typically treat all\npreference pairs uniformly, ignoring critical variations in their inherent\nquality and learning utility, leading to suboptimal data utilization and\nperformance. To address this challenge, we propose Omni-DPO, a dual-perspective\noptimization framework that jointly accounts for (1) the inherent quality of\neach preference pair and (2) the model's evolving performance on those pairs.\nBy adaptively weighting samples according to both data quality and the model's\nlearning dynamics during training, Omni-DPO enables more effective training\ndata utilization and achieves better performance. Experimental results on\nvarious models and benchmarks demonstrate the superiority and generalization\ncapabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it\nfinetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant\nmargin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning\ntasks, Omni-DPO consistently outperforms the baseline methods across all\nbenchmarks, providing strong empirical evidence for the effectiveness and\nrobustness of our approach. Code and models will be available at\nhttps://github.com/pspdada/Omni-DPO."}
{"id": "2507.04996", "pdf": "https://arxiv.org/pdf/2507.04996.pdf", "abs": "https://arxiv.org/abs/2507.04996", "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems", "authors": ["Jiangbo Yu"], "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.HC", "cs.RO"], "comment": null, "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are viewed as vehicular systems capable of\nperceiving their environment and executing pre-programmed tasks independently\nof external input. However, both research and real-world deployments\nincreasingly showcase vehicles that demonstrate behaviors beyond this\ndefinition (including the SAE levels 0 to 5); Examples of this outpace include\nthe interaction with humans with natural language, goal adaptation, contextual\nreasoning, external tool use, and unseen ethical dilemma handling, largely\nempowered by multi-modal large language models (LLMs). These developments\nreveal a conceptual gap between technical autonomy and the broader cognitive\nand social capabilities needed for future human-centered mobility systems. To\naddress this gap, this paper introduces the concept of agentic vehicles (AgVs),\nreferring to vehicles that integrate agentic AI systems to reason, adapt, and\ninteract within complex environments. This paper proposes the term AgVs and\ntheir distinguishing characteristics from conventional AuVs. It synthesizes\nrelevant advances in integrating LLMs and AuVs and highlights how AgVs might\ntransform future mobility systems and ensure the systems are human-centered.\nThe paper concludes by identifying key challenges in the development and\ngovernance of AgVs, and how they can play a significant role in future agentic\ntransportation systems."}
{"id": "2507.15887", "pdf": "https://arxiv.org/pdf/2507.15887.pdf", "abs": "https://arxiv.org/abs/2507.15887", "title": "AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?", "authors": ["Ori Press", "Brandon Amos", "Haoyu Zhao", "Yikai Wu", "Samuel K. Ainsworth", "Dominik Krupke", "Patrick Kidger", "Touqir Sajed", "Bartolomeo Stellato", "Jisun Park", "Nathanael Bosch", "Eli Meril", "Albert Steppi", "Arman Zharmagambetov", "Fangzhao Zhang", "David Perez-Pineiro", "Alberto Mercurio", "Ni Zhan", "Talor Abramovich", "Kilian Lieret", "Hanlin Zhang", "Shirley Huang", "Matthias Bethge", "Ofir Press"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite progress in language model (LM) capabilities, evaluations have thus\nfar focused on models' performance on tasks that humans have previously solved,\nincluding in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,\n2024). We therefore propose testing models' ability to design and implement\nalgorithms in an open-ended benchmark: We task LMs with writing code that\nefficiently solves computationally challenging problems in computer science,\nphysics, and mathematics. Our AlgoTune benchmark consists of 154 coding tasks\ncollected from domain experts and a framework for validating and timing\nLM-synthesized solution code, which is compared to reference implementations\nfrom popular open-source packages. In addition, we develop a baseline LM agent,\nAlgoTuner, and evaluate its performance across a suite of frontier models.\nAlgoTuner uses a simple, budgeted loop that edits code, compiles and runs it,\nprofiles performance, verifies correctness on tests, and selects the fastest\nvalid version. AlgoTuner achieves an average 1.72x speedup against our\nreference solvers, which use libraries such as SciPy, sk-learn and CVXPY.\nHowever, we find that current models fail to discover algorithmic innovations,\ninstead preferring surface-level optimizations. We hope that AlgoTune catalyzes\nthe development of LM agents exhibiting creative problem solving beyond\nstate-of-the-art human performance."}
{"id": "2507.18932", "pdf": "https://arxiv.org/pdf/2507.18932.pdf", "abs": "https://arxiv.org/abs/2507.18932", "title": "MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning Benchmark for ESG Tasks", "authors": ["Lei Zhang", "Xin Zhou", "Chaoyue He", "Di Wang", "Yi Wu", "Hong Xu", "Wei Liu", "Chunyan Miao"], "categories": ["cs.MM", "cs.CL"], "comment": "Accepted at ACM MM 2025", "summary": "Environmental, Social, and Governance (ESG) reports are essential for\nevaluating sustainability practices, ensuring regulatory compliance, and\npromoting financial transparency. However, these documents are often lengthy,\nstructurally diverse, and multimodal, comprising dense text, structured tables,\ncomplex figures, and layout-dependent semantics. Existing AI systems often\nstruggle to perform reliable document-level reasoning in such settings, and no\ndedicated benchmark currently exists in ESG domain. To fill the gap, we\nintroduce \\textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted\nto evaluate multimodal understanding and complex reasoning across structurally\ndiverse and multi-source ESG documents. This dataset is constructed via a\nhuman-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates\ncandidate question-answer (QA) pairs by jointly interpreting rich textual,\ntabular, and visual information from layout-aware document pages. Second, an\nLLM verifies the semantic accuracy, completeness, and reasoning complexity of\neach QA pair. This automated process is followed by an expert-in-the-loop\nvalidation, where domain specialists validate and calibrate QA pairs to ensure\nquality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs\nderived from 45 ESG documents, spanning across seven distinct document types\nand three major ESG source categories. Questions are categorized as\nsingle-page, cross-page, or unanswerable, with each accompanied by fine-grained\nmultimodal evidence. Initial experiments validate that multimodal and\nretrieval-augmented models substantially outperform text-only baselines,\nparticularly on visually grounded and cross-page tasks. MMESGBench is publicly\navailable as an open-source dataset at\nhttps://github.com/Zhanglei1103/MMESGBench."}
{"id": "2508.05118", "pdf": "https://arxiv.org/pdf/2508.05118.pdf", "abs": "https://arxiv.org/abs/2508.05118", "title": "Exploring Superior Function Calls via Reinforcement Learning", "authors": ["Bingguang Hao", "Maolin Wang", "Zengzhuang Xu", "Yicheng Chen", "Cunyin Peng", "Jinjie GU", "Chenyi Zhuang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Function calling capabilities are crucial for deploying Large Language Models\nin real-world applications, yet current training approaches fail to develop\nrobust reasoning strategies. Supervised fine-tuning produces models that rely\non superficial pattern matching, while standard reinforcement learning methods\nstruggle with the complex action space of structured function calls. We present\na novel reinforcement learning framework designed to enhance group relative\npolicy optimization through strategic entropy based exploration specifically\ntailored for function calling tasks. Our approach addresses three critical\nchallenges in function calling: insufficient exploration during policy\nlearning, lack of structured reasoning in chain-of-thought generation, and\ninadequate verification of parameter extraction. Our two-stage data preparation\npipeline ensures high-quality training samples through iterative LLM evaluation\nand abstract syntax tree validation. Extensive experiments on the Berkeley\nFunction Calling Leaderboard demonstrate that this framework achieves\nstate-of-the-art performance among open-source models with 86.02\\% overall\naccuracy, outperforming standard GRPO by up to 6\\% on complex multi-function\nscenarios. Notably, our method shows particularly strong improvements on\ncode-pretrained models, suggesting that structured language generation\ncapabilities provide an advantageous starting point for reinforcement learning\nin function calling tasks. We will release all the code, models and dataset to\nbenefit the community."}
{"id": "2508.06401", "pdf": "https://arxiv.org/pdf/2508.06401.pdf", "abs": "https://arxiv.org/abs/2508.06401", "title": "A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges", "authors": ["Andrew Brown", "Muhammad Roman", "Barry Devereux"], "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.IR"], "comment": "58 pages, This work has been submitted to the IEEE for possible\n  publication", "summary": "This systematic review of the research literature on retrieval-augmented\ngeneration (RAG) provides a focused analysis of the most highly cited studies\npublished between 2020 and May 2025. A total of 128 articles met our inclusion\ncriteria. The records were retrieved from ACM Digital Library, IEEE Xplore,\nScopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).\nRAG couples a neural retriever with a generative language model, grounding\noutput in up-to-date, non-parametric memory while retaining the semantic\ngeneralisation stored in model weights. Guided by the PRISMA 2020 framework, we\n(i) specify explicit inclusion and exclusion criteria based on citation count\nand research questions, (ii) catalogue datasets, architectures, and evaluation\npractices, and (iii) synthesise empirical evidence on the effectiveness and\nlimitations of RAG. To mitigate citation-lag bias, we applied a lower\ncitation-count threshold to papers published in 2025 so that emerging\nbreakthroughs with naturally fewer citations were still captured. This review\nclarifies the current research landscape, highlights methodological gaps, and\ncharts priority directions for future research."}
{"id": "2508.08715", "pdf": "https://arxiv.org/pdf/2508.08715.pdf", "abs": "https://arxiv.org/abs/2508.08715", "title": "MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation Tutor with LLMs", "authors": ["Xiaoxue Gao", "Huayun Zhang", "Nancy F. Chen"], "categories": ["eess.AS", "cs.AI", "cs.CL", "eess.SP"], "comment": "We are withdrawing the manuscript to revise the title and contents of\n  figures for better alignment with the paper's contributions", "summary": "Generative speech models have demonstrated significant potential in\npersonalizing teacher-student interactions, offering valuable real-world\napplications for language learning in children's education. However, achieving\nhigh-quality, child-friendly speech generation remains challenging,\nparticularly for low-resource languages across diverse languages and cultural\ncontexts. In this paper, we propose MultiAiTutor, an educational multilingual\ngenerative AI tutor with child-friendly designs, leveraging LLM architecture\nfor speech generation tailored for educational purposes. We propose to\nintegrate age-appropriate multilingual speech generation using LLM\narchitectures, facilitating young children's language learning through\nculturally relevant image-description tasks in three low-resource languages:\nSingaporean-accent Mandarin, Malay, and Tamil. Experimental results from both\nobjective metrics and subjective evaluations demonstrate the superior\nperformance of the proposed MultiAiTutor compared to baseline methods."}
{"id": "2508.09023", "pdf": "https://arxiv.org/pdf/2508.09023.pdf", "abs": "https://arxiv.org/abs/2508.09023", "title": "E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and Efficiency", "authors": ["Dongjie Xu", "Yue Cui", "Weijie Shi", "Qingzhi Ma", "Hanghui Guo", "Jiaming Li", "Yao Zhao", "Ruiyuan Zhang", "Shimin Di", "Jia Zhu", "Kai Zheng", "Jiajie Xu"], "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "SQL query rewriting aims to reformulate a query into a more efficient form\nwhile preserving equivalence. Most existing methods rely on predefined rewrite\nrules. However, such rule-based approaches face fundamental limitations: (1)\nfixed rule sets generalize poorly to novel query patterns and struggle with\ncomplex queries; (2) a wide range of effective rewriting strategies cannot be\nfully captured by declarative rules. To overcome these issues, we propose using\nlarge language models (LLMs) to generate rewrites. LLMs can capture complex\nstrategies, such as evaluation reordering and CTE rewriting. Despite this\npotential, directly applying LLMs often results in performance regressions or\nnon-equivalent rewrites due to a lack of execution awareness and semantic\ngrounding. To address these challenges, We present E3-Rewrite, an LLM-based SQL\nrewriting framework that produces executable, equivalent, and efficient\nqueries. It integrates two core components: a context construction module and a\nreinforcement learning framework. First, the context module leverages execution\nplans and retrieved demonstrations to build bottleneck-aware prompts that guide\ninference-time rewriting. Second, we design a reward function targeting\nexecutability, equivalence, and efficiency, evaluated via syntax checks,\nequivalence verification, and cost estimation. Third, to ensure stable\nmulti-objective learning, we adopt a staged curriculum that first emphasizes\nexecutability and equivalence, then gradually incorporates efficiency. Across\nmultiple SQL benchmarks, our experiments demonstrate that E3-Rewrite can\nshorten query execution time by as much as 25.6% relative to leading baselines,\nwhile also producing up to 24.4% more rewrites that meet strict equivalence\ncriteria. These gains extend to challenging query patterns that prior\napproaches could not effectively optimize."}
