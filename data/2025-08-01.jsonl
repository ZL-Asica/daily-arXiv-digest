{"id": "2507.22889", "pdf": "https://arxiv.org/pdf/2507.22889.pdf", "abs": "https://arxiv.org/abs/2507.22889", "title": "Knowledge Is More Than Performance: How Knowledge Diversity Drives Human-Human and Human-AI Interaction Synergy and Reveals Pure-AI Interaction Shortfalls", "authors": ["Tom Sheffer", "Alon Miron", "Yaniv Dover", "Ariel Goldstein"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Conversations transform individual knowledge into collective insight,\nallowing groups of humans and increasingly groups of artificial intelligence\n(AI) agents to collaboratively solve complex problems. Whether interactions\nbetween AI agents can replicate the synergy observed in human discussions\nremains an open question. To investigate this, we systematically compared four\nconversational configurations: pairs of large language models (LLM-LLM), trios\nof LLMs, trios of humans, and mixed human-LLM pairs. After agents answered\nquestions individually, they engaged in open-ended discussions and then\nreconsidered their initial answers. Interactions involving humans consistently\nled to accuracy improvements after the conversations, benefiting both stronger\nand weaker participants. By contrast, purely LLM-based pairs and trios\nexhibited declines in accuracy, demonstrating limited conversational synergy.\nAnalysis of participants' confidence and answer-switching behavior revealed\nthat knowledge diversity is a critical factor enabling collaborative\nimprovement. Crucially, the lack of gains in LLM-LLM interactions did not stem\nfrom a fundamental limitation of the models' ability to collaborate, but from\nhighly similar knowledge states that left little room for productive exchange.\nOur findings argue for a paradigm shift in AI development: rather than\noptimizing individual models solely for standalone performance, explicitly\ncultivating diversity across agents, even at the cost of slightly lower\nindividual accuracy, may yield AI collaborators that are more effective in\ngroup settings with humans or other AI systems."}
{"id": "2507.22890", "pdf": "https://arxiv.org/pdf/2507.22890.pdf", "abs": "https://arxiv.org/abs/2507.22890", "title": "Evaluating LLMs for Visualization Generation and Understanding", "authors": ["Saadiq Rauf Khan", "Vinit Chandak", "Sougata Mukherjea"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Information Visualization has been utilized to gain insights from complex\ndata. In recent times, Large Language models (LLMs) have performed very well in\nmany tasks. In this paper, we showcase the capabilities of different popular\nLLMs to generate code for visualization based on simple prompts. We also\nanalyze the power of LLMs to understand some common visualizations by answering\nquestions. Our study shows that LLMs could generate code for some simpler\nvisualizations such as bar and pie charts. Moreover, they could answer simple\nquestions about visualizations. However, LLMs also have several limitations.\nFor example, some of them had difficulty generating complex visualizations,\nsuch as violin plot. LLMs also made errors in answering some questions about\nvisualizations, for example, identifying relationships between close boundaries\nand determining lengths of shapes. We believe that our insights can be used to\nimprove both LLMs and Information Visualization systems."}
{"id": "2507.22891", "pdf": "https://arxiv.org/pdf/2507.22891.pdf", "abs": "https://arxiv.org/abs/2507.22891", "title": "Real-time energy monitoring infrastructure for residential collective self-consumption operations using Linky meter", "authors": ["Jérôme Ferrari", "Benoit Delinchant", "Frédéric Wurtz", "Olga Rouchouze"], "categories": ["cs.HC"], "comment": "Cired 2025, Jun 2025, Gen{\\`e}ve (CH), Switzerland", "summary": "As part of the energy transition and the rise in energy prices, the number of\ncollective self-consumption operations in France is steadily increasing.\nHowever, energy flow monitoring currently relies on historical ''day+1'' data\nprovided by Linky meters, which does not offer real time feedback to help\nparticipants adapt their energy consumption behaviors. This article introduces\na new open-source infrastructure for real-time monitoring based on Linky meter\ndata, enabling participants to make informed decisions and take timely actions.\nIt includes a description of the xKy device, applied to a collective\nself-consumption operation involving nine participants, supported by the Energy\nTransition Observatory (OTE). The project encompasses the implementation of\ngateways in participants' homes and the development and operation of real-time\nmonitoring website, aimed at increasing participants' self-consumption rate."}
{"id": "2507.22892", "pdf": "https://arxiv.org/pdf/2507.22892.pdf", "abs": "https://arxiv.org/abs/2507.22892", "title": "Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation", "authors": ["Ismail Hossain", "Mridul Banik"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Conventional augmentative and alternative communication (AAC) systems and\nlanguage-learning platforms often fail to adapt in real time to the user's\ncognitive and linguistic needs, especially in neurological conditions such as\npost-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in\nnoninvasive electroencephalography (EEG)--based brain-computer interfaces\n(BCIs) and transformer--based large language models (LLMs) offer complementary\nstrengths: BCIs capture users' neural intent with low fatigue, while LLMs\ngenerate contextually tailored language content. We propose and evaluate a\nnovel hybrid framework that leverages real-time EEG signals to drive an\nLLM-powered language rehabilitation assistant. This system aims to: (1) enable\nusers with severe speech or motor impairments to navigate language-learning\nmodules via mental commands; (2) dynamically personalize vocabulary,\nsentence-construction exercises, and corrective feedback; and (3) monitor\nneural markers of cognitive effort to adjust task difficulty on the fly."}
{"id": "2507.22910", "pdf": "https://arxiv.org/pdf/2507.22910.pdf", "abs": "https://arxiv.org/abs/2507.22910", "title": "Large Language Models in the Travel Domain: An Industrial Experience", "authors": ["Sergio Di Meglio", "Aniello Somma", "Luigi Libero Lucio Starace", "Fabio Scippacercola", "Giancarlo Sperlì", "Sergio Di Martino"], "categories": ["cs.CL", "cs.AI"], "comment": "Manuscript accepted to the International Conference on Software\n  Engineering and Knowledge Engineering (SEKE) 2025", "summary": "Online property booking platforms are widely used and rely heavily on\nconsistent, up-to-date information about accommodation facilities, often\nsourced from third-party providers. However, these external data sources are\nfrequently affected by incomplete or inconsistent details, which can frustrate\nusers and result in a loss of market. In response to these challenges, we\npresent an industrial case study involving the integration of Large Language\nModels (LLMs) into CALEIDOHOTELS, a property reservation platform developed by\nFERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,\nfine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.\nBoth models were assessed based on their ability to generate consistent and\nhomogeneous descriptions while minimizing hallucinations. Mixtral 8x7B\noutperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision\n(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet\nmore concise content (249 vs. 277 words on average). However, this came at a\nsignificantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB\nand $0.16/hour for Mistral 7B. Our findings provide practical insights into the\ntrade-offs between model quality and resource efficiency, offering guidance for\ndeploying LLMs in production environments and demonstrating their effectiveness\nin enhancing the consistency and reliability of accommodation data."}
{"id": "2507.22893", "pdf": "https://arxiv.org/pdf/2507.22893.pdf", "abs": "https://arxiv.org/abs/2507.22893", "title": "Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure", "authors": ["Giuseppe Riva"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Contemporary human-AI interaction research overlooks how AI systems\nfundamentally reshape human cognition pre-consciously, a critical blind spot\nfor understanding distributed cognition. This paper introduces \"Cognitive\nInfrastructure Studies\" (CIS) as a new interdisciplinary domain to\nreconceptualize AI as \"cognitive infrastructures\": foundational, often\ninvisible systems conditioning what is knowable and actionable in digital\nsocieties. These semantic infrastructures transport meaning, operate through\nanticipatory personalization, and exhibit adaptive invisibility, making their\ninfluence difficult to detect. Critically, they automate \"relevance judgment,\"\nshifting the \"locus of epistemic agency\" to non-human systems. Through\nnarrative scenarios spanning individual (cognitive dependency), collective\n(democratic deliberation), and societal (governance) scales, we describe how\ncognitive infrastructures reshape human cognition, public reasoning, and social\nepistemologies. CIS aims to address how AI preprocessing reshapes distributed\ncognition across individual, collective, and cultural scales, requiring\nunprecedented integration of diverse disciplinary methods. The framework also\naddresses critical gaps across disciplines: cognitive science lacks\npopulation-scale preprocessing analysis capabilities, digital sociology cannot\naccess individual cognitive mechanisms, and computational approaches miss\ncultural transmission dynamics. To achieve this goal CIS also provides\nmethodological innovations for studying invisible algorithmic influence:\n\"infrastructure breakdown methodologies\", experimental approaches that reveal\ncognitive dependencies by systematically withdrawing AI preprocessing after\nperiods of habituation."}
{"id": "2507.22911", "pdf": "https://arxiv.org/pdf/2507.22911.pdf", "abs": "https://arxiv.org/abs/2507.22911", "title": "ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing", "authors": ["Jinzhi Wang", "Qingke Peng", "Haozhou Li", "Zeyuan Zeng", "Qinfeng Song", "Kaixuan Yang", "Jiangbo Zhang", "Yaoying Wang", "Ruimeng Li", "Biyi Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Electric power marketing customer service plays a critical role in addressing\ninquiries, complaints, and service requests. However, current systems, such as\nChina's 95598 hotline, often struggle with slow response times, inflexible\nprocedures, and limited accuracy in domain-specific tasks. While large language\nmodels (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,\nthey lack the domain expertise and empathy required in this field. To bridge\nthis gap, we introduce ElectriQ, the first benchmark designed to evaluate and\nenhance LLMs in electric power marketing scenarios. ElectriQ consists of a\ndialogue dataset covering six key service categories and introduces four\nevaluation metrics: professionalism, popularity, readability, and\nuser-friendliness. We further incorporate a domain-specific knowledge base and\npropose a knowledge augmentation method to boost model performance. Experiments\non 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and\naugmented, can surpass GPT-4o in terms of professionalism and\nuser-friendliness. ElectriQ establishes a comprehensive foundation for\ndeveloping LLMs tailored to the needs of power marketing services."}
{"id": "2507.22894", "pdf": "https://arxiv.org/pdf/2507.22894.pdf", "abs": "https://arxiv.org/abs/2507.22894", "title": "When no one shows up (at first): Navigating the uncertainties of participatory workshops in interdisciplinary research", "authors": ["Monique Munarini"], "categories": ["cs.HC"], "comment": "Presented at HHAI25:The 4th International Conference Series on Hybrid\n  Human-Artificial Intelligence, workshop Mind the AI-GAP 2025:Co-Designing\n  Socio-Technical Systems. (June 9-13, 2025 in Pisa, Italy)", "summary": "This reflective paper explores often-unspoken challenges of designing and\nfacilitating co-design and participatory workshops, offering practical\nstrategies for early career researchers (ECRs) navigating these methods.\nDrawing from personal experience conducting a series of workshops titled: How\nto Think About Equity in the AI Ecosystem. It follows the full arc of the\nworkshop experience, from conceptualization and activity planning to\nparticipant recruitment and facilitation, offering a grounded account of what\nhappens when participation does not go as expected. The paper examines the\nmethodological challenges of engaging non-expert participants, particularly\nwhen operating without institutional support, financial incentives, or\nintegration into larger events. Despite initial difficulties such as low\nattendance, the workshop fostered rich discussions among a demographically\ndiverse group and ultimately led to one participant volunteering to\nco-facilitate a subsequent session. This transition from participant to\nco-facilitator exemplifies the redistribution of epistemic authority,\npositioning lived experience as central to research and engagement practices.\nBy reframing perceived failure as a productive site of learning, the paper\noffers practical strategies for ECRs working across disciplines who often\nnavigate unfamiliar methodological terrains, contributing to broader\nconversations on the realities of doing interdisciplinary, participatory work\nin practice."}
{"id": "2507.22912", "pdf": "https://arxiv.org/pdf/2507.22912.pdf", "abs": "https://arxiv.org/abs/2507.22912", "title": "A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms", "authors": ["Navid Yazdanjue", "Morteza Rakhshaninejad", "Hossein Yazdanjouei", "Mohammad Sadegh Khorshidi", "Mikko S. Niemela", "Fang Chen", "Amir H. Gandomi"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50"], "comment": "16 pages, 5 figures, 9 tables", "summary": "Illegal marketplaces have increasingly shifted to concealed parts of the\ninternet, including the deep and dark web, as well as platforms such as\nTelegram, Reddit, and Pastebin. These channels enable the anonymous trade of\nillicit goods including drugs, weapons, and stolen credentials. Detecting and\ncategorizing such content remains challenging due to limited labeled data, the\nevolving nature of illicit language, and the structural heterogeneity of online\nsources. This paper presents a hierarchical classification framework that\ncombines fine-tuned language models with a semi-supervised ensemble learning\nstrategy to detect and classify illicit marketplace content across diverse\nplatforms. We extract semantic representations using ModernBERT, a transformer\nmodel for long documents, finetuned on domain-specific data from deep and dark\nweb pages, Telegram channels, Subreddits, and Pastebin pastes to capture\nspecialized jargon and ambiguous linguistic patterns. In addition, we\nincorporate manually engineered features such as document structure, embedded\npatterns including Bitcoin addresses, emails, and IPs, and metadata, which\ncomplement language model embeddings. The classification pipeline operates in\ntwo stages. The first stage uses a semi-supervised ensemble of XGBoost, Random\nForest, and SVM with entropy-based weighted voting to detect sales-related\ndocuments. The second stage further classifies these into drug, weapon, or\ncredential sales. Experiments on three datasets, including our multi-source\ncorpus, DUTA, and CoDA, show that our model outperforms several baselines,\nincluding BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The\nmodel achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of\n0.95388, demonstrating strong generalization, robustness under limited\nsupervision, and effectiveness in real-world illicit content detection."}
{"id": "2507.22895", "pdf": "https://arxiv.org/pdf/2507.22895.pdf", "abs": "https://arxiv.org/abs/2507.22895", "title": "Brain motor intention Extraction Amplifier: Non-invasive brain-muscle interface", "authors": ["Ye Sun", "Bowei Zhao", "Dezhong Yao", "Rui Zhang", "Bohan Zhang", "Xiaoyuan Li", "Jing Wang", "Mingxuan Qu", "Gang Liu"], "categories": ["cs.HC"], "comment": "18 pages, 9 figures", "summary": "Brain-computer interfaces (BCIs) enable real-time interaction between the\nbrain and external devices by decoding neural signals. However, existing\nmotor-based BCI paradigms, like motor imagery BCI, face challenges with\nimprecise labeling in real-world use. This mismatch between EEG signals and\ntrue behavioral intentions leads to pseudo-labels, undermining decoding\naccuracy and system robustness. To overcome this bottleneck, this paper first\nproposes a novel motor intention extraction framework based on a non-invasive\nbrain-muscle interface (BMuI)($\\text{BCI} =\n\\frac{\\text{Brain}}{\\text{Computer}} \\text{ Interface} =\n\\frac{\\text{Brain}}{\\not\\text{Muscle}}\\! \\text{ (BMuI)} \\times\n\\!\\frac{\\not\\text{Muscle}}{\\text{Computer}}\\! \\text{ Interface}$). This method\nsimulates the neural pathway from the brain to the muscles in order to capture\nand enhance the weak motor intention signals originating in the brain. It then\nuses EMG as a high-fidelity relay medium to achieve more accurate intention\nrecognition and transmission. To systematically validate the feasibility and\neffectiveness of this approach, we conducted both offline experiments (to\nrepeatedly verify feasibility) and online experiments (to construct a real-time\ninteractive system and evaluate its performance). The results show that BMuI is\nfeasible, achieving a prediction accuracy of 0.8314; in the online experiment,\nall participants are able to successfully control the Unity virtual arm."}
{"id": "2507.22913", "pdf": "https://arxiv.org/pdf/2507.22913.pdf", "abs": "https://arxiv.org/abs/2507.22913", "title": "A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models", "authors": ["Jinyu Liu", "Xiaoying Song", "Diana Zhang", "Jason Thomale", "Daqing He", "Lingzi Hong"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures, accepted by ASIST 2025", "summary": "Providing subject access to information resources is an essential function of\nany library management system. Large language models (LLMs) have been widely\nused in classification and summarization tasks, but their capability to perform\nsubject analysis is underexplored. Multi-label classification with traditional\nmachine learning (ML) models has been used for subject analysis but struggles\nwith unseen cases. LLMs offer an alternative but often over-generate and\nhallucinate. Therefore, we propose a hybrid framework that integrates\nembedding-based ML models with LLMs. This approach uses ML models to (1)\npredict the optimal number of LCSH labels to guide LLM predictions and (2)\npost-edit the predicted terms with actual LCSH terms to mitigate\nhallucinations. We experimented with LLMs and the hybrid framework to predict\nthe subject terms of books using the Library of Congress Subject Headings\n(LCSH). Experiment results show that providing initial predictions to guide LLM\ngenerations and imposing post-edits result in more controlled and\nvocabulary-aligned outputs."}
{"id": "2507.22896", "pdf": "https://arxiv.org/pdf/2507.22896.pdf", "abs": "https://arxiv.org/abs/2507.22896", "title": "iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous Improvement", "authors": ["Kohou Wang", "ZhaoXiang Liu", "Lin Bai", "Kun Fan", "Xiang Liu", "Huan Hu", "Kai Wang", "Shiguo Lian"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.RO"], "comment": "17 pages, 12 figures", "summary": "It is crucial that robots' performance can be improved after deployment, as\nthey are inherently likely to encounter novel scenarios never seen before. This\npaper presents an innovative solution: an interactive learning-based robot\nsystem powered by a Multi-modal Large Language Model(MLLM). A key feature of\nour system is its ability to learn from natural dialogues with non-expert\nusers. We also propose chain of question to clarify the exact intent of the\nquestion before providing an answer and dual-modality retrieval modules to\nleverage these interaction events to avoid repeating same mistakes, ensuring a\nseamless user experience before model updates, which is in contrast to current\nmainstream MLLM-based robotic systems. Our system marks a novel approach in\nrobotics by integrating interactive learning, paving the way for superior\nadaptability and performance in diverse environments. We demonstrate the\neffectiveness and improvement of our method through experiments, both\nquantitively and qualitatively."}
{"id": "2507.22914", "pdf": "https://arxiv.org/pdf/2507.22914.pdf", "abs": "https://arxiv.org/abs/2507.22914", "title": "Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs", "authors": ["Victor Eiti Yamamoto", "Hideaki Takeda"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge graphs (KGs) are powerful tools for representing and reasoning over\nstructured information. Their main components include schema, identity, and\ncontext. While schema and identity matching are well-established in ontology\nand entity matching research, context matching remains largely unexplored. This\nis particularly important because real-world KGs often vary significantly in\nsource, size, and information density - factors not typically represented in\nthe datasets on which current entity matching methods are evaluated. As a\nresult, existing approaches may fall short in scenarios where diverse and\ncomplex contexts need to be integrated.\n  To address this gap, we propose a novel KG integration method consisting of\nlabel matching and triple matching. We use string manipulation, fuzzy matching,\nand vector similarity techniques to align entity and predicate labels. Next, we\nidentify mappings between triples that convey comparable information, using\nthese mappings to improve entity-matching accuracy. Our approach demonstrates\ncompetitive performance compared to leading systems in the OAEI competition and\nagainst supervised methods, achieving high accuracy across diverse test cases.\nAdditionally, we introduce a new dataset derived from the benchmark dataset to\nevaluate the triple-matching step more comprehensively."}
{"id": "2507.22897", "pdf": "https://arxiv.org/pdf/2507.22897.pdf", "abs": "https://arxiv.org/abs/2507.22897", "title": "RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems", "authors": ["Luyu Chen", "Quanyu Dai", "Zeyu Zhang", "Xueyang Feng", "Mingyu Zhang", "Pengcheng Tang", "Xu Chen", "Yue Zhu", "Zhenhua Dong"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted by TheWebConf'25 Industry Track", "summary": "Conversational recommender systems (CRS) enhance user experience through\nmulti-turn interactions, yet evaluating CRS remains challenging. User\nsimulators can provide comprehensive evaluations through interactions with CRS,\nbut building realistic and diverse simulators is difficult. While recent work\nleverages large language models (LLMs) to simulate user interactions, they\nstill fall short in emulating individual real users across diverse scenarios\nand lack explicit rating mechanisms for quantitative evaluation. To address\nthese gaps, we propose RecUserSim, an LLM agent-based user simulator with\nenhanced simulation realism and diversity while providing explicit scores.\nRecUserSim features several key modules: a profile module for defining\nrealistic and diverse user personas, a memory module for tracking interaction\nhistory and discovering unknown preferences, and a core action module inspired\nby Bounded Rationality theory that enables nuanced decision-making while\ngenerating more fine-grained actions and personalized responses. To further\nenhance output control, a refinement module is designed to fine-tune final\nresponses. Experiments demonstrate that RecUserSim generates diverse,\ncontrollable outputs and produces realistic, high-quality dialogues, even with\nsmaller base LLMs. The ratings generated by RecUserSim show high consistency\nacross different base LLMs, highlighting its effectiveness for CRS evaluation."}
{"id": "2507.22915", "pdf": "https://arxiv.org/pdf/2507.22915.pdf", "abs": "https://arxiv.org/abs/2507.22915", "title": "Theoretical Foundations and Mitigation of Hallucination in Large Language Models", "authors": ["Esmail Gumaan"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "Hallucination in Large Language Models (LLMs) refers to the generation of\ncontent that is not faithful to the input or the real-world facts. This paper\nprovides a rigorous treatment of hallucination in LLMs, including formal\ndefinitions and theoretical analyses. We distinguish between intrinsic and\nextrinsic hallucinations, and define a \\textit{hallucination risk} for models.\nWe derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes\nand Rademacher complexity). We then survey detection strategies for\nhallucinations, such as token-level uncertainty estimation, confidence\ncalibration, and attention alignment checks. On the mitigation side, we discuss\napproaches including retrieval-augmented generation, hallucination-aware\nfine-tuning, logit calibration, and the incorporation of fact-verification\nmodules. We propose a unified detection and mitigation workflow, illustrated\nwith a diagram, to integrate these strategies. Finally, we outline evaluation\nprotocols for hallucination, recommending datasets, metrics, and experimental\nsetups to quantify and reduce hallucinations. Our work lays a theoretical\nfoundation and practical guidelines for addressing the crucial challenge of\nhallucination in LLMs."}
{"id": "2507.22898", "pdf": "https://arxiv.org/pdf/2507.22898.pdf", "abs": "https://arxiv.org/abs/2507.22898", "title": "Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment", "authors": ["Julian Acosta", "Scott Adams", "Julius Kernbach", "Romain Hardy", "Sung Eun Kim", "Luyang Luo", "Xiaoman Zhang", "Shreya Johri", "Mohammed Baharoon", "Pranav Rajpurkar"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "We developed a voice-driven artificial intelligence (AI) system that guides\nanyone - from paramedics to family members - through expert-level stroke\nevaluations using natural conversation, while also enabling smartphone video\ncapture of key examination components for documentation and potential expert\nreview. This addresses a critical gap in emergency care: current stroke\nrecognition by first responders is inconsistent and often inaccurate, with\nsensitivity for stroke detection as low as 58%, causing life-threatening delays\nin treatment. Three non-medical volunteers used our AI system to assess ten\nsimulated stroke patients, including cases with likely large vessel occlusion\n(LVO) strokes and stroke-like conditions, while we measured diagnostic\naccuracy, completion times, user confidence, and expert physician review of the\nAI-generated reports. The AI system correctly identified 84% of individual\nstroke signs and detected 75% of likely LVOs, completing evaluations in just\nover 6 minutes. Users reported high confidence (median 4.5/5) and ease of use\n(mean 4.67/5). The system successfully identified 86% of actual strokes but\nalso incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert\nphysician reviewed the AI reports with videos, they identified the correct\ndiagnosis in 100% of cases, but felt confident enough to make preliminary\ntreatment decisions in only 40% of cases due to observed AI errors including\nincorrect scoring and false information. While the current system's limitations\nnecessitate human oversight, ongoing rapid advancements in speech-to-speech AI\nmodels suggest that future versions are poised to enable highly accurate\nassessments. Achieving human-level voice interaction could transform emergency\nmedical care, putting expert-informed assessment capabilities in everyone's\nhands."}
{"id": "2507.22917", "pdf": "https://arxiv.org/pdf/2507.22917.pdf", "abs": "https://arxiv.org/abs/2507.22917", "title": "Reading Between the Timelines: RAG for Answering Diachronic Questions", "authors": ["Kwun Hang Lau", "Ruiyuan Zhang", "Weijie Shi", "Xiaofang Zhou", "Xiaojun Cheng"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) excels at injecting static,\nfactual knowledge into Large Language Models (LLMs), it exhibits a critical\ndeficit in handling longitudinal queries that require tracking entities and\nphenomena across time. This blind spot arises because conventional,\nsemantically-driven retrieval methods are not equipped to gather evidence that\nis both topically relevant and temporally coherent for a specified duration. We\naddress this challenge by proposing a new framework that fundamentally\nredesigns the RAG pipeline to infuse temporal logic. Our methodology begins by\ndisentangling a user's query into its core subject and its temporal window. It\nthen employs a specialized retriever that calibrates semantic matching against\ntemporal relevance, ensuring the collection of a contiguous evidence set that\nspans the entire queried period. To enable rigorous evaluation of this\ncapability, we also introduce the Analytical Diachronic Question Answering\nBenchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus\nof real and synthetic financial news. Empirical results on ADQAB show that our\napproach yields substantial gains in answer accuracy, surpassing standard RAG\nimplementations by 13% to 27%. This work provides a validated pathway toward\nRAG systems capable of performing the nuanced, evolutionary analysis required\nfor complex, real-world questions. The dataset and code for this study are\npublicly available at https://github.com/kwunhang/TA-RAG."}
{"id": "2507.22899", "pdf": "https://arxiv.org/pdf/2507.22899.pdf", "abs": "https://arxiv.org/abs/2507.22899", "title": "A visual analytics tool for taxonomy-based trajectory data exploration", "authors": ["Ivan A. Hanono Cozzetti", "Ahmad Abdou"], "categories": ["cs.HC"], "comment": "71 pages, 92 figures", "summary": "The analysis of spatio-temporal data presents significant challenges due to\nthe complexity and heterogeneity of movement patterns. This project proposes a\ndata analytics tool that combines data visualization and statistical\ncomputation to facilitate spatio-temporal data analysis through a multi-level\napproach. The tool categorizes moving objects into distinct taxonomies using\nMachine Learning models, adding meaningful structure to the analysis. Two case\nstudies demonstrate the methodology's effectiveness. The first analyzed Arctic\nfox trajectories, successfully identifying and labeling foxes with Geometric or\nKinematic-based behaviors, further categorized into Curvature and Acceleration\ngroups. Statistical indicators revealed that foxes with Acceleration-based\nbehavior showed constant, steady acceleration, while those with Curvature-based\nbehavior exhibited acceleration peaks and sudden deceleration. The second case\nstudy examined tropical cyclone data, labeling trajectories with Speed,\nCurvature, and hybrid Geometric-based behaviors through unique statistical\nvariables. Analysis of hybrid Geometric behavior (Curvature and Indentation\ncombined) identified specific angles with the highest impact on hurricane shape\nand geometry. The proposed method and tool demonstrate that spatio-temporal\ndata, despite inherent complexity, can be analyzed and explained in detail,\nproviding a theoretical and practical blueprint applicable to multiple domains."}
{"id": "2507.22918", "pdf": "https://arxiv.org/pdf/2507.22918.pdf", "abs": "https://arxiv.org/abs/2507.22918", "title": "Semantic Convergence: Investigating Shared Representations Across Scaled LLMs", "authors": ["Daniel Son", "Sanjana Rathore", "Andrew Rufail", "Adrian Simon", "Daniel Zhang", "Soham Dave", "Cole Blondin", "Kevin Zhu", "Sean O'Brien"], "categories": ["cs.CL", "cs.LG", "68T50", "I.2.6; I.2.7"], "comment": "Submitted to ACL 2025 Student Research Workshop (poster)", "summary": "We investigate feature universality in Gemma-2 language models (Gemma-2-2B\nand Gemma-2-9B), asking whether models with a four-fold difference in scale\nstill converge on comparable internal concepts. Using the Sparse Autoencoder\n(SAE) dictionary-learning pipeline, we utilize SAEs on each model's\nresidual-stream activations, align the resulting monosemantic features via\nactivation correlation, and compare the matched feature spaces with SVCCA and\nRSA. Middle layers yield the strongest overlap, while early and late layers\nshow far less similarity. Preliminary experiments extend the analysis from\nsingle tokens to multi-token subspaces, showing that semantically similar\nsubspaces interact similarly with language models. These results strengthen the\ncase that large language models carve the world into broadly similar,\ninterpretable features despite size differences, reinforcing universality as a\nfoundation for cross-model interpretability."}
{"id": "2507.22900", "pdf": "https://arxiv.org/pdf/2507.22900.pdf", "abs": "https://arxiv.org/abs/2507.22900", "title": "Tool or Trouble? Exploring Student Attitudes Toward AI Coding Assistants", "authors": ["Sergio Rojas-Galeano"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This exploratory study examines how AI code assistants shape novice\nprogrammers' experiences during a two-part exam in an introductory programming\ncourse. In the first part, students completed a programming task with access to\nAI support; in the second, they extended their solutions without AI. We\ncollected Likert-scale and open-ended responses from 20 students to evaluate\ntheir perceptions and challenges. Findings suggest that AI tools were perceived\nas helpful for understanding code and increasing confidence, particularly\nduring initial development. However, students reported difficulties\ntransferring knowledge to unaided tasks, revealing possible overreliance and\ngaps in conceptual understanding. These insights highlight the need for\npedagogical strategies that integrate AI meaningfully while reinforcing\nfoundational programming skills."}
{"id": "2507.22919", "pdf": "https://arxiv.org/pdf/2507.22919.pdf", "abs": "https://arxiv.org/abs/2507.22919", "title": "A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations", "authors": ["Qixuan Hu", "Xumou Zhang", "Jinman Kim", "Florence Bourgeois", "Adam G. Dunn"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objectives: With accurate estimates of expected safety results, clinical\ntrials could be designed to avoid terminations and limit exposing participants\nto unnecessary risks. We evaluated methods for predicting serious adverse event\n(SAE) results in clinical trials using information only from their\nregistrations prior to the trial. Material and Methods: We analysed 22,107\ntwo-arm parallel interventional clinical trials from ClinicalTrials.gov with\nstructured summary results. Two prediction models were developed: a classifier\npredicting will experimental arm have higher SAE rates (area under the receiver\noperating characteristic curve; AUC) than control arm, and a regression model\nto predict the proportion of SAEs in control arms (root mean squared error;\nRMSE). A transfer learning approach using pretrained language models (e.g.,\nClinicalT5, BioBERT) was used for feature extraction, combined with downstream\nmodel for prediction. To maintain semantic representation in long trial texts\nexceeding localised language model input limits, a sliding window method was\ndeveloped for embedding extraction. Results: The best model\n(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a\nhigher proportion of patients with SAEs. When predicting proportion of\nparticipants experiencing SAE in the control arm, the same model achieved RMSE\nof 18.6%. The sliding window approach consistently outperformed methods without\nit. Across 12 classifiers, the average absolute AUC increase was 2.00%; across\n12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:\nSummary results data available at ClinicalTrials.gov remains underutilised. The\npotential to estimate results of trials before they start is an opportunity to\nimprove trial design and flag discrepancies between expected and reported\nsafety results."}
{"id": "2507.22901", "pdf": "https://arxiv.org/pdf/2507.22901.pdf", "abs": "https://arxiv.org/abs/2507.22901", "title": "Accelerated and Optimized Search of Imperceptible Color Vibration for Embedding Information into LCD images", "authors": ["Shingo Hattori", "Takefumi Hiraki"], "categories": ["cs.HC"], "comment": "Presented at ACM SIGGRAPH Asia 2022 Posters", "summary": "Large, high-resolution displays are installed throughout the city as public\ndisplays. By superimposing invisible information on the images of these\ndisplays, large numbers of devices with cameras and sensors can communicate\nwith the displays without prior pairing. Several applications have been\nproposed, such as operating robots or communicating information to users by\ndisplaying 2D codes on images. However, the display of 2D codes has the problem\nof compromising the appearance of displayed content.\n  Abe et al. proposed a method of communicating with devices by superimposing\ninvisible information using color vibration on images displayed on\noff-the-shelf liquid-crystal displays (LCD). Using this method, we can embed\nthe information for devices in images without interfering with the displayed\ncontent. Abe et al. uses a simple serial loop operation to search for color\npairs comprising a color vibration, which requires a very long processing time\ndue to the huge search space.\n  In this paper, we propose an accelerated and optimized search method for\ncolor pairs that constitute the imperceptible color vibration for embedding\ninformation on LCD images. To achieve fast color pair search, we parallelized\nthe search process, which is previously done individually, by using arrays\nrepresenting the amount of movement and an operation to extract elements from\nthe array that satisfy the conditions. In addition, we investigate the amount\nof information that can be superimposed on nine color images using the\nimperceptible color vibration and clarify the applicability of embedding\ninformation into images using the color vibration."}
{"id": "2507.22920", "pdf": "https://arxiv.org/pdf/2507.22920.pdf", "abs": "https://arxiv.org/abs/2507.22920", "title": "Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey", "authors": ["Jindong Li", "Yali Fu", "Jiahong Liu", "Linxiao Cao", "Wei Ji", "Menglin Yang", "Irwin King", "Ming-Hsuan Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has intensified the\nneed for effective mechanisms to transform continuous multimodal data into\ndiscrete representations suitable for language-based processing. Discrete\ntokenization, with vector quantization (VQ) as a central approach, offers both\ncomputational efficiency and compatibility with LLM architectures. Despite its\ngrowing importance, there is a lack of a comprehensive survey that\nsystematically examines VQ techniques in the context of LLM-based systems. This\nwork fills this gap by presenting the first structured taxonomy and analysis of\ndiscrete tokenization methods designed for LLMs. We categorize 8 representative\nVQ variants that span classical and modern paradigms and analyze their\nalgorithmic principles, training dynamics, and integration challenges with LLM\npipelines. Beyond algorithm-level investigation, we discuss existing research\nin terms of classical applications without LLMs, LLM-based single-modality\nsystems, and LLM-based multimodal systems, highlighting how quantization\nstrategies influence alignment, reasoning, and generation performance. In\naddition, we identify key challenges including codebook collapse, unstable\ngradient estimation, and modality-specific encoding constraints. Finally, we\ndiscuss emerging research directions such as dynamic and task-adaptive\nquantization, unified tokenization frameworks, and biologically inspired\ncodebook learning. This survey bridges the gap between traditional vector\nquantization and modern LLM applications, serving as a foundational reference\nfor the development of efficient and generalizable multimodal systems. A\ncontinuously updated version is available at:\nhttps://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey."}
{"id": "2507.22902", "pdf": "https://arxiv.org/pdf/2507.22902.pdf", "abs": "https://arxiv.org/abs/2507.22902", "title": "Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting", "authors": ["Hashim Hayat", "Maksim Kudrautsau", "Evgeniy Makarov", "Vlad Melnichenko", "Tim Tsykunou", "Piotr Varaksin", "Matt Pavelle", "Adam Z. Oskowitz"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Background: Globally we face a projected shortage of 11 million healthcare\npractitioners by 2030, and administrative burden consumes 50% of clinical time.\nArtificial intelligence (AI) has the potential to help alleviate these\nproblems. However, no end-to-end autonomous large language model (LLM)-based AI\nsystem has been rigorously evaluated in real-world clinical practice. In this\nstudy, we evaluated whether a multi-agent LLM-based AI framework can function\nautonomously as an AI doctor in a virtual urgent care setting. Methods: We\nretrospectively compared the performance of the multi-agent AI system Doctronic\nand board-certified clinicians across 500 consecutive urgent-care telehealth\nencounters. The primary end points: diagnostic concordance, treatment plan\nconsistency, and safety metrics, were assessed by blinded LLM-based\nadjudication and expert human review. Results: The top diagnosis of Doctronic\nand clinician matched in 81% of cases, and the treatment plan aligned in 99.2%\nof cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not\nsupported by clinical findings). In an expert review of discordant cases, AI\nperformance was superior in 36.1%, and human performance was superior in 9.3%;\nthe diagnoses were equivalent in the remaining cases. Conclusions: In this\nfirst large-scale validation of an autonomous AI doctor, we demonstrated strong\ndiagnostic and treatment plan concordance with human clinicians, with AI\nperformance matching and in some cases exceeding that of practicing clinicians.\nThese findings indicate that multi-agent AI systems achieve comparable clinical\ndecision-making to human providers and offer a potential solution to healthcare\nworkforce shortages."}
{"id": "2507.22921", "pdf": "https://arxiv.org/pdf/2507.22921.pdf", "abs": "https://arxiv.org/abs/2507.22921", "title": "Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers", "authors": ["Lee Harris"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Language models can capture complex relationships in given text, but these\nare notorious for being costly and for producing information that does not\nexist (i.e., hallucinations). Furthermore, the resources invested into\nproducing this information would be wasted if it were incorrect. We address\nthese issues by proposing, implementing, and applying the Language Model Chain\n(LMC) algorithm. In this, a language model's response to a given prompt about\ngiven text is only correct if it exists in the collection of possible (i.e.,\ncandidate) answers, and text corresponding to incorrect responses is fed into a\nmore predictive (but slower) language model. This process is repeated for a\ncollection of language models, or until all predictions about the text are\ncorrect. We used the LMC algorithm to extract patient dates of birth from\nmedical documents, and combining a collection of language models in a\nmulti-stage cascade significantly increased prediction speed and accuracy over\nindividual language models, while greatly reducing the number of corresponding\nhallucinations. We believe that the novel LMC algorithm significantly\ncontributes to the knowledge extraction field, and that this should be explored\nmuch further in the future."}
{"id": "2507.22903", "pdf": "https://arxiv.org/pdf/2507.22903.pdf", "abs": "https://arxiv.org/abs/2507.22903", "title": "A blessing or a burden? Exploring worker perspectives of using a social robot in a church", "authors": ["Andrew Blair", "Peggy Gregory", "Mary Ellen Foster"], "categories": ["cs.HC", "cs.RO"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (ROMAN)", "summary": "Recent technological advances have allowed robots to assist in the service\nsector, and consequently accelerate job and sector transformation. Less\nattention has been paid to the use of robots in real-world organisations where\nsocial benefits, as opposed to profits, are the primary motivator. To explore\nthese opportunities, we have partnered with a working church and visitor\nattraction. We conducted interviews with 15 participants from a range of\nstakeholder groups within the church to understand worker perspectives of\nintroducing a social robot to the church and analysed the results using\nreflexive thematic analysis. Findings indicate mixed responses to the use of a\nrobot, with participants highlighting the empathetic responsibility the church\nhas towards people and the potential for unintended consequences. However,\ninformation provision and alleviation of menial or mundane tasks were\nidentified as potential use cases. This highlights the need to consider not\nonly the financial aspects of robot introduction, but also how social and\nintangible values shape what roles a robot should take on within an\norganisation."}
{"id": "2507.22922", "pdf": "https://arxiv.org/pdf/2507.22922.pdf", "abs": "https://arxiv.org/abs/2507.22922", "title": "Predicting stock prices with ChatGPT-annotated Reddit sentiment", "authors": ["Mateusz Kmak", "Kamil Chmurzyński", "Kamil Matejuk", "Paweł Kotzbach", "Jan Kocoń"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "International Conference on Computational Science 2025", "summary": "The surge of retail investor activity on social media, exemplified by the\n2021 GameStop short squeeze, raised questions about the influence of online\nsentiment on stock prices. This paper explores whether sentiment derived from\nsocial media discussions can meaningfully predict stock market movements. We\nfocus on Reddit's r/wallstreetbets and analyze sentiment related to two\ncompanies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's\nrole, we employ two existing text-based sentiment analysis methods and\nintroduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model\ndesigned to better interpret the informal language and emojis prevalent in\nsocial media discussions. We use correlation and causality metrics to determine\nthese models' predictive power. Surprisingly, our findings suggest that social\nmedia sentiment has only a weak correlation with stock prices. At the same\ntime, simpler metrics, such as the volume of comments and Google search trends,\nexhibit stronger predictive signals. These results highlight the complexity of\nretail investor behavior and suggest that traditional sentiment analysis may\nnot fully capture the nuances of market-moving online discussions."}
{"id": "2507.22904", "pdf": "https://arxiv.org/pdf/2507.22904.pdf", "abs": "https://arxiv.org/abs/2507.22904", "title": "SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches", "authors": ["Ehsan Latif", "Zirak Khan", "Xiaoming Zhai"], "categories": ["cs.HC", "cs.AI"], "comment": "Submitted to NeurIPS2025", "summary": "Scientific sketches (e.g., models) offer a powerful lens into students'\nconceptual understanding, yet AI-powered automated assessment of such\nfree-form, visually diverse artifacts remains a critical challenge. Existing\nsolutions often treat sketch evaluation as either an image classification task\nor monolithic vision-language models, which lack interpretability, pedagogical\nalignment, and adaptability across cognitive levels. To address these\nlimitations, we present SketchMind, a cognitively grounded, multi-agent\nframework for evaluating and improving student-drawn scientific sketches.\nSketchMind comprises modular agents responsible for rubric parsing, sketch\nperception, cognitive alignment, and iterative feedback with sketch\nmodification, enabling personalized and transparent evaluation. We evaluate\nSketchMind on a curated dataset of 3,575 student-generated sketches across six\nscience assessment items with different highest order of Bloom's level that\nrequire students to draw models to explain phenomena. Compared to baseline\nGPT-4o performance without SRG (average accuracy: 55.6%), and with SRG\nintegration achieves 77.1% average accuracy (+21.4% average absolute gain). We\nalso demonstrate that multi-agent orchestration with SRG enhances SketchMind\nperformance, for example, GPT-4.1 gains an average 8.9% increase in sketch\nprediction accuracy, outperforming single-agent pipelines across all items.\nHuman evaluators rated the feedback and co-created sketches generated by\n\\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5,\nsignificantly higher than those of baseline models (e.g., 2.3 for GPT-4o).\nExperts noted the system's potential to meaningfully support conceptual growth\nthrough guided revision. Our code and (pending approval) dataset will be\nreleased to support reproducibility and future research in AI-driven education."}
{"id": "2507.22923", "pdf": "https://arxiv.org/pdf/2507.22923.pdf", "abs": "https://arxiv.org/abs/2507.22923", "title": "How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting", "authors": ["Aman Gupta", "Yingying Zhuang", "Zhou Yu", "Ziji Zhang", "Anurag Beniwal"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Prompt Optimization KDD '25", "summary": "Despite advances in the multilingual capabilities of Large Language Models\n(LLMs), their performance varies substantially across different languages and\ntasks. In multilingual retrieval-augmented generation (RAG)-based systems,\nknowledge bases (KB) are often shared from high-resource languages (such as\nEnglish) to low-resource ones, resulting in retrieved information from the KB\nbeing in a different language than the rest of the context. In such scenarios,\ntwo common practices are pre-translation to create a mono-lingual prompt and\ncross-lingual prompting for direct inference. However, the impact of these\nchoices remains unclear. In this paper, we systematically evaluate the impact\nof different prompt translation strategies for classification tasks with\nRAG-enhanced LLMs in multilingual systems. Experimental results show that an\noptimized prompting strategy can significantly improve knowledge sharing across\nlanguages, therefore improve the performance on the downstream classification\ntask. The findings advocate for a broader utilization of multilingual resource\nsharing and cross-lingual prompt optimization for non-English languages,\nespecially the low-resource ones."}
{"id": "2507.22905", "pdf": "https://arxiv.org/pdf/2507.22905.pdf", "abs": "https://arxiv.org/abs/2507.22905", "title": "Exploring LLM-generated Culture-specific Affective Human-Robot Tactile Interaction", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "categories": ["cs.HC"], "comment": null, "summary": "As large language models (LLMs) become increasingly integrated into robotic\nsystems, their potential to generate socially and culturally appropriate\naffective touch remains largely unexplored. This study investigates whether\nLLMs-specifically GPT-3.5, GPT-4, and GPT-4o --can generate culturally adaptive\ntactile behaviours to convey emotions in human-robot interaction. We produced\ntext based touch descriptions for 12 distinct emotions across three cultural\ncontexts (Chinese, Belgian, and unspecified), and examined their\ninterpretability in both robot-to-human and human-to-robot scenarios. A total\nof 90 participants (36 Chinese, 36 Belgian, and 18 culturally unspecified)\nevaluated these LLM-generated tactile behaviours for emotional decoding and\nperceived appropriateness. Results reveal that: (1) under matched cultural\nconditions, participants successfully decoded six out of twelve emotions-mainly\nsocially oriented emotions such as love and Ekman emotions such as anger,\nhowever, self-focused emotions like pride and embarrassment were more difficult\nto interpret; (2) tactile behaviours were perceived as more appropriate when\ndirected from human to robot than from robot to human, revealing an asymmetry\nin social expectations based on interaction roles; (3) behaviours interpreted\nas aggressive (e.g., anger), overly intimate (e.g., love), or emotionally\nambiguous (i.e., not clearly decodable) were significantly more likely to be\nrated as inappropriate; and (4) cultural mismatches reduced decoding accuracy\nand increased the likelihood of behaviours being judged as inappropriate."}
{"id": "2507.22924", "pdf": "https://arxiv.org/pdf/2507.22924.pdf", "abs": "https://arxiv.org/abs/2507.22924", "title": "Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers", "authors": ["Brittney Exline", "Melanie Duffin", "Brittany Harbison", "Chrissa da Gomez", "David Joyner"], "categories": ["cs.CL", "I.2.7; K.3.1"], "comment": null, "summary": "Graduate-level CS programs in the U.S. increasingly enroll international\nstudents, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.\nstudents. Many of these students take online courses, where peer feedback is\nused to engage students and improve pedagogy in a scalable manner. Since these\ncourses are conducted in English, many students study in a language other than\ntheir first. This paper examines how native versus non-native English speaker\nstatus affects three metrics of peer feedback experience in online U.S.-based\ncomputing courses. Using the Twitter-roBERTa-based model, we analyze the\nsentiment of peer reviews written by and to a random sample of 500 students. We\nthen relate sentiment scores and peer feedback ratings to students' language\nbackground. Results show that native English speakers rate feedback less\nfavorably, while non-native speakers write more positively but receive less\npositive sentiment in return. When controlling for sex and age, significant\ninteractions emerge, suggesting that language background plays a modest but\ncomplex role in shaping peer feedback experiences."}
{"id": "2507.22952", "pdf": "https://arxiv.org/pdf/2507.22952.pdf", "abs": "https://arxiv.org/abs/2507.22952", "title": "Automated Label Placement on Maps via Large Language Models", "authors": ["Harry Shomer", "Jiejun Xu"], "categories": ["cs.HC", "cs.CV", "cs.LG"], "comment": "Workshop on AI for Data Editing (AI4DE) at KDD 2025", "summary": "Label placement is a critical aspect of map design, serving as a form of\nspatial annotation that directly impacts clarity and interpretability. Despite\nits importance, label placement remains largely manual and difficult to scale,\nas existing automated systems struggle to integrate cartographic conventions,\nadapt to context, or interpret labeling instructions. In this work, we\nintroduce a new paradigm for automatic label placement (ALP) that formulates\nthe task as a data editing problem and leverages large language models (LLMs)\nfor context-aware spatial annotation. To support this direction, we curate\nMAPLE, the first known benchmarking dataset for evaluating ALP on real-world\nmaps, encompassing diverse landmark types and label placement annotations from\nopen-source data. Our method retrieves labeling guidelines relevant to each\nlandmark type leveraging retrieval-augmented generation (RAG), integrates them\ninto prompts, and employs instruction-tuned LLMs to generate ideal label\ncoordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall\nperformance and generalization across different types of landmarks. This\nincludes both zero-shot and instruction-tuned performance. Our results\ndemonstrate that LLMs, when guided by structured prompts and domain-specific\nretrieval, can learn to perform accurate spatial edits, aligning the generated\noutputs with expert cartographic standards. Overall, our work presents a\nscalable framework for AI-assisted map finishing and demonstrates the potential\nof foundation models in structured data editing tasks. The code and data can be\nfound at https://github.com/HarryShomer/MAPLE."}
{"id": "2507.22925", "pdf": "https://arxiv.org/pdf/2507.22925.pdf", "abs": "https://arxiv.org/abs/2507.22925", "title": "Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents", "authors": ["Haoran Sun", "Shaoning Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-term memory is one of the key factors influencing the reasoning\ncapabilities of Large Language Model Agents (LLM Agents). Incorporating a\nmemory mechanism that effectively integrates past interactions can\nsignificantly enhance decision-making and contextual coherence of LLM Agents.\nWhile recent works have made progress in memory storage and retrieval, such as\nencoding memory into dense vectors for similarity-based search or organizing\nknowledge in the form of graph, these approaches often fall short in structured\nmemory organization and efficient retrieval. To address these limitations, we\npropose a Hierarchical Memory (H-MEM) architecture for LLM Agents that\norganizes and updates memory in a multi-level fashion based on the degree of\nsemantic abstraction. Each memory vector is embedded with a positional index\nencoding pointing to its semantically related sub-memories in the next layer.\nDuring the reasoning phase, an index-based routing mechanism enables efficient,\nlayer-by-layer retrieval without performing exhaustive similarity computations.\nWe evaluate our method on five task settings from the LoCoMo dataset.\nExperimental results show that our approach consistently outperforms five\nbaseline methods, demonstrating its effectiveness in long-term dialogue\nscenarios."}
{"id": "2507.23096", "pdf": "https://arxiv.org/pdf/2507.23096.pdf", "abs": "https://arxiv.org/abs/2507.23096", "title": "ChatVis: Large Language Model Agent for Generating Scientific Visualizations", "authors": ["Tom Peterka", "Tanwi Mallick", "Orcun Yildiz", "David Lenz", "Cory Quammen", "Berk Geveci"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are rapidly increasing in capability, but they\nstill struggle with highly specialized programming tasks such as scientific\nvisualization. We present an LLM assistant, ChatVis, that aids the LLM to\ngenerate Python code for ParaView scientific visualization tasks, without the\nneed for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought\nprompt simplification, retrieval-augmented prompt generation using a vector\ndatabase of documentation and code examples, and error checking with iterative\nprompt feedback to correct errors until a visualization is produced. An\nintegral part of our approach is a benchmark suite of canonical visualization\ntasks, ParaView regression tests, and scientific use cases that includes\ncomprehensive evaluation metrics. We evaluate our visualization assistant by\ncomparing results with a variety of top-performing unassisted LLMs. We find\nthat all the metrics are significantly improved with ChatVis."}
{"id": "2507.22926", "pdf": "https://arxiv.org/pdf/2507.22926.pdf", "abs": "https://arxiv.org/abs/2507.22926", "title": "Multi-Relation Extraction in Entity Pairs using Global Context", "authors": ["Nilesh", "Atul Gupta", "Avinash C Panday"], "categories": ["cs.CL", "cs.IR"], "comment": "11 pages, 9 figures", "summary": "In document-level relation extraction, entities may appear multiple times in\na document, and their relationships can shift from one context to another.\nAccurate prediction of the relationship between two entities across an entire\ndocument requires building a global context spanning all relevant sentences.\nPrevious approaches have focused only on the sentences where entities are\nmentioned, which fails to capture the complete document context necessary for\naccurate relation extraction. Therefore, this paper introduces a novel input\nembedding approach to capture the positions of mentioned entities throughout\nthe document rather than focusing solely on the span where they appear. The\nproposed input encoding approach leverages global relationships and\nmulti-sentence reasoning by representing entities as standalone segments,\nindependent of their positions within the document. The performance of the\nproposed method has been tested on three benchmark relation extraction\ndatasets, namely DocRED, Re-DocRED, and REBEL. The experimental results\ndemonstrated that the proposed method accurately predicts relationships between\nentities in a document-level setting. The proposed research also has\ntheoretical and practical implications. Theoretically, it advances global\ncontext modeling and multi-sentence reasoning in document-level relation\nextraction. Practically, it enhances relationship detection, enabling improved\nperformance in real-world NLP applications requiring comprehensive entity-level\ninsights and interpretability."}
{"id": "2507.23190", "pdf": "https://arxiv.org/pdf/2507.23190.pdf", "abs": "https://arxiv.org/abs/2507.23190", "title": "Accessibility Scout: Personalized Accessibility Scans of Built Environments", "authors": ["William Huang", "Xia Su", "Jon E. Froehlich", "Yang Zhang"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.MA"], "comment": "18 pages, 16 figures. Presented at ACM UIST 2025", "summary": "Assessing the accessibility of unfamiliar built environments is critical for\npeople with disabilities. However, manual assessments, performed by users or\ntheir personal health professionals, are laborious and unscalable, while\nautomatic machine learning methods often neglect an individual user's unique\nneeds. Recent advances in Large Language Models (LLMs) enable novel approaches\nto this problem, balancing personalization with scalability to enable more\nadaptive and context-aware assessments of accessibility. We present\nAccessibility Scout, an LLM-based accessibility scanning system that identifies\naccessibility concerns from photos of built environments. With use,\nAccessibility Scout becomes an increasingly capable \"accessibility scout\",\ntailoring accessibility scans to an individual's mobility level, preferences,\nand specific environmental interests through collaborative Human-AI\nassessments. We present findings from three studies: a formative study with six\nparticipants to inform the design of Accessibility Scout, a technical\nevaluation of 500 images of built environments, and a user study with 10\nparticipants of varying mobility. Results from our technical evaluation and\nuser study show that Accessibility Scout can generate personalized\naccessibility scans that extend beyond traditional ADA considerations. Finally,\nwe conclude with a discussion on the implications of our work and future steps\nfor building more scalable and personalized accessibility assessments of the\nphysical world."}
{"id": "2507.22927", "pdf": "https://arxiv.org/pdf/2507.22927.pdf", "abs": "https://arxiv.org/abs/2507.22927", "title": "PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation", "authors": ["Zhehao Tan", "Yihan Jiao", "Dan Yang", "Lei Liu", "Jie Feng", "Duolin Sun", "Yue Shen", "Jian Wang", "Peng Wei", "Jinjie Gu"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge, where the LLM's ability to generate responses\nbased on the combination of a given query and retrieved documents is crucial.\nHowever, most benchmarks focus on overall RAG system performance, rarely\nassessing LLM-specific capabilities. Current benchmarks emphasize broad aspects\nsuch as noise robustness, but lack a systematic and granular evaluation\nframework on document utilization. To this end, we introduce\n\\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,\nemphasizing the following progressive dimensions: (1) multi-level filtering\nabilities, (2) combination abilities, and (3) reference reasoning. To provide a\nmore nuanced understanding of LLMs' roles in RAG systems, we formulate an\ninnovative placeholder-based approach to decouple the contributions of the\nLLM's parametric knowledge and the external knowledge. Experiments demonstrate\nthe limitations of representative LLMs in the RAG system's generation\ncapabilities, particularly in error resilience and context faithfulness. Our\nbenchmark provides a reproducible framework for developing more reliable and\nefficient RAG systems. Our code is available in\nhttps://github.com/Alipay-Med/PRGB."}
{"id": "2507.23215", "pdf": "https://arxiv.org/pdf/2507.23215.pdf", "abs": "https://arxiv.org/abs/2507.23215", "title": "Silent Impact: Tracking Tennis Shots from the Passive Arm", "authors": ["Junyong Park", "Saelyne Yang", "Sungho Jo"], "categories": ["cs.HC", "H.5.2; I.5.4"], "comment": "15 pages, 9 figures,", "summary": "Wearable technology has transformed sports analytics, offering new dimensions\nin enhancing player experience. Yet, many solutions involve cumbersome setups\nthat inhibit natural motion. In tennis, existing products require sensors on\nthe racket or dominant arm, causing distractions and discomfort. We propose\nSilent Impact, a novel and user-friendly system that analyzes tennis shots\nusing a sensor placed on the passive arm. Collecting Inertial Measurement Unit\nsensor data from 20 recreational tennis players, we developed neural networks\nthat exclusively utilize passive arm data to detect and classify six shots,\nachieving a classification accuracy of 88.2% and a detection F1 score of 86.0%,\ncomparable to the dominant arm. These models were then incorporated into an\nend-to-end prototype, which records passive arm motion through a smartwatch and\ndisplays a summary of shots on a mobile app. User study (N=10) showed that\nparticipants felt less burdened physically and mentally using Silent Impact on\nthe passive arm. Overall, our research establishes the passive arm as an\neffective, comfortable alternative for tennis shot analysis, advancing\nuser-friendly sports analytics."}
{"id": "2507.22928", "pdf": "https://arxiv.org/pdf/2507.22928.pdf", "abs": "https://arxiv.org/abs/2507.22928", "title": "How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding", "authors": ["Xi Chen", "Aske Plaat", "Niki van Stein"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on\nmulti-step tasks, yet whether the generated \"thoughts\" reflect the true\ninternal reasoning process is unresolved. We present the first feature-level\ncausal study of CoT faithfulness. Combining sparse autoencoders with activation\npatching, we extract monosemantic features from Pythia-70M and Pythia-2.8B\nwhile they tackle GSM8K math problems under CoT and plain (noCoT) prompting.\nSwapping a small set of CoT-reasoning features into a noCoT run raises answer\nlog-probabilities significantly in the 2.8B model, but has no reliable effect\nin 70M, revealing a clear scale threshold. CoT also leads to significantly\nhigher activation sparsity and feature interpretability scores in the larger\nmodel, signalling more modular internal computation. For example, the model's\nconfidence in generating correct answers improves from 1.2 to 4.3. We introduce\npatch-curves and random-feature patching baselines, showing that useful CoT\ninformation is not only present in the top-K patches but widely distributed.\nOverall, our results indicate that CoT can induce more interpretable internal\nstructures in high-capacity LLMs, validating its role as a structured prompting\nmethod."}
{"id": "2507.23298", "pdf": "https://arxiv.org/pdf/2507.23298.pdf", "abs": "https://arxiv.org/abs/2507.23298", "title": "Real-time Generation of Various Types of Nodding for Avatar Attentive Listening System", "authors": ["Kazushi Kato", "Koji Inoue", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "Accepted by 27th ACM International Conference on Multimodal\n  Interaction (ICMI '25), Long paper", "summary": "In human dialogue, nonverbal information such as nodding and facial\nexpressions is as crucial as verbal information, and spoken dialogue systems\nare also expected to express such nonverbal behaviors. We focus on nodding,\nwhich is critical in an attentive listening system, and propose a model that\npredicts both its timing and type in real time. The proposed model builds on\nthe voice activity projection (VAP) model, which predicts voice activity from\nboth listener and speaker audio. We extend it to prediction of various types of\nnodding in a continuous and real-time manner unlike conventional models. In\naddition, the proposed model incorporates multi-task learning with verbal\nbackchannel prediction and pretraining on general dialogue data. In the timing\nand type prediction task, the effectiveness of multi-task learning was\nsignificantly demonstrated. We confirmed that reducing the processing rate\nenables real-time operation without a substantial drop in accuracy, and\nintegrated the model into an avatar attentive listening system. Subjective\nevaluations showed that it outperformed the conventional method, which always\ndoes nodding in sync with verbal backchannel. The code and trained models are\navailable at https://github.com/MaAI-Kyoto/MaAI."}
{"id": "2507.22929", "pdf": "https://arxiv.org/pdf/2507.22929.pdf", "abs": "https://arxiv.org/abs/2507.22929", "title": "EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow", "authors": ["Xiaoyu Pan", "Yang Bai", "Ke Zou", "Yang Zhou", "Jun Zhou", "Huazhu Fu", "Yih-Chung Tham", "Yong Liu"], "categories": ["cs.CL", "cs.CV", "cs.MA"], "comment": "9 figures, 5 tables. submit/6621751", "summary": "Medical Large Language Models (MLLMs) play a crucial role in ophthalmic\ndiagnosis, holding significant potential to address vision-threatening\ndiseases. However, their accuracy is constrained by hallucinations stemming\nfrom limited ophthalmic knowledge, insufficient visual localization and\nreasoning capabilities, and a scarcity of multimodal ophthalmic data, which\ncollectively impede precise lesion detection and disease diagnosis.\nFurthermore, existing medical benchmarks fail to effectively evaluate various\ntypes of hallucinations or provide actionable solutions to mitigate them. To\naddress the above challenges, we introduce EH-Benchmark, a novel ophthalmology\nbenchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'\nhallucinations based on specific tasks and error types into two primary\nclasses: Visual Understanding and Logical Composition, each comprising multiple\nsubclasses. Given that MLLMs predominantly rely on language-based reasoning\nrather than visual processing, we propose an agent-centric, three-phase\nframework, including the Knowledge-Level Retrieval stage, the Task-Level Case\nStudies stage, and the Result-Level Validation stage. Experimental results show\nthat our multi-agent framework significantly mitigates both types of\nhallucinations, enhancing accuracy, interpretability, and reliability. Our\nproject is available at https://github.com/ppxy1/EH-Benchmark."}
{"id": "2507.23454", "pdf": "https://arxiv.org/pdf/2507.23454.pdf", "abs": "https://arxiv.org/abs/2507.23454", "title": "Breaking the mould of Social Mixed Reality -- State-of-the-Art and Glossary", "authors": ["Marta Bieńkiewicz", "Julia Ayache", "Panayiotis Charalambous", "Cristina Becchio", "Marco Corragio", "Bertram Taetz", "Francesco De Lellis", "Antonio Grotta", "Anna Server", "Daniel Rammer", "Richard Kulpa", "Franck Multon", "Azucena Garcia-Palacios", "Jessica Sutherland", "Kathleen Bryson", "Stéphane Donikian", "Didier Stricker", "Benoît Bardy"], "categories": ["cs.HC", "cs.CY", "cs.ET", "cs.GR", "q-bio.NC", "I.3.0; I.2; J.4; K.4"], "comment": "pre-print", "summary": "This article explores a critical gap in Mixed Reality (MR) technology: while\nadvances have been made, MR still struggles to authentically replicate human\nembodiment and socio-motor interaction. For MR to enable truly meaningful\nsocial experiences, it needs to incorporate multi-modal data streams and\nmulti-agent interaction capabilities. To address this challenge, we present a\ncomprehensive glossary covering key topics such as Virtual Characters and\nAutonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges\nof Social MR within Neuroscience, Embodiment, and Technology. Our aim is to\ndrive the transformative evolution of MR technologies that prioritize\nhuman-centric innovation, fostering richer digital connections. We advocate for\nMR systems that enhance social interaction and collaboration between humans and\nvirtual autonomous agents, ensuring inclusivity, ethical design and\npsychological safety in the process."}
{"id": "2507.22930", "pdf": "https://arxiv.org/pdf/2507.22930.pdf", "abs": "https://arxiv.org/abs/2507.22930", "title": "Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection", "authors": ["Shalini Jangra", "Suparna De", "Nishanth Sastry", "Saeed Fadaei"], "categories": ["cs.CL", "cs.SI"], "comment": "15 pages, 4 Figures, Accepted in \"The 17th International Conference\n  on Advances in Social Networks Analysis and Mining -ASONAM-2025\"", "summary": "Social platforms such as Reddit have a network of communities of shared\ninterests, with a prevalence of posts and comments from which one can infer\nusers' Personal Information Identifiers (PIIs). While such self-disclosures can\nlead to rewarding social interactions, they pose privacy risks and the threat\nof online harms. Research into the identification and retrieval of such risky\nself-disclosures of PIIs is hampered by the lack of open-source labeled\ndatasets. To foster reproducible research into PII-revealing text detection, we\ndevelop a novel methodology to create synthetic equivalents of PII-revealing\ndata that can be safely shared. Our contributions include creating a taxonomy\nof 19 PII-revealing categories for vulnerable populations and the creation and\nrelease of a synthetic PII-labeled multi-text span dataset generated from 3\ntext generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and\nzephyr-7b-beta, with sequential instruction prompting to resemble the original\nReddit posts. The utility of our methodology to generate this synthetic dataset\nis evaluated with three metrics: First, we require reproducibility equivalence,\ni.e., results from training a model on the synthetic data should be comparable\nto those obtained by training the same models on the original posts. Second, we\nrequire that the synthetic data be unlinkable to the original users, through\ncommon mechanisms such as Google Search. Third, we wish to ensure that the\nsynthetic data be indistinguishable from the original, i.e., trained humans\nshould not be able to tell them apart. We release our dataset and code at\nhttps://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster\nreproducible research into PII privacy risks in online social media."}
{"id": "2507.23470", "pdf": "https://arxiv.org/pdf/2507.23470.pdf", "abs": "https://arxiv.org/abs/2507.23470", "title": "Automated Feedback on Student-Generated UML and ER Diagrams Using Large Language Models", "authors": ["Sebastian Gürtl", "Gloria Schimetta", "David Kerschbaumer", "Michael Liut", "Alexander Steinmaurer"], "categories": ["cs.HC", "cs.AI"], "comment": "Learnersourcing: Student-generated Content @ Scale Workshop at L@S\n  2025", "summary": "UML and ER diagrams are foundational in computer science education but come\nwith challenges for learners due to the need for abstract thinking, contextual\nunderstanding, and mastery of both syntax and semantics. These complexities are\ndifficult to address through traditional teaching methods, which often struggle\nto provide scalable, personalized feedback, especially in large classes. We\nintroduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,\nwhich converts a reference diagram and a student-submitted diagram into a\ntextual representation and provides structured feedback based on the\ndifferences. It uses a multi-stage LLM pipeline to compare diagrams and\ngenerate reflective feedback. Furthermore, the tool enables analytical insights\nfor educators, aiming to foster self-directed learning and inform instructional\nstrategies. We evaluated DUET through semi-structured interviews with six\nparticipants, including two educators and four teaching assistants. They\nidentified strengths such as accessibility, scalability, and learning support\nalongside limitations, including reliability and potential misuse. Participants\nalso suggested potential improvements, such as bulk upload functionality and\ninteractive clarification features. DUET presents a promising direction for\nintegrating LLMs into modeling education and offers a foundation for future\nclassroom integration and empirical evaluation."}
{"id": "2507.22931", "pdf": "https://arxiv.org/pdf/2507.22931.pdf", "abs": "https://arxiv.org/abs/2507.22931", "title": "Enhancing RAG Efficiency with Adaptive Context Compression", "authors": ["Shuyu Guo", "Zhaochun Ren"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy."}
{"id": "2507.23492", "pdf": "https://arxiv.org/pdf/2507.23492.pdf", "abs": "https://arxiv.org/abs/2507.23492", "title": "Digital literacy interventions can boost humans in discerning deepfakes", "authors": ["Dominique Geissler", "Claire Robertson", "Stefan Feuerriegel"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Deepfakes, i.e., images generated by artificial intelligence (AI), can erode\ntrust in institutions and compromise election outcomes, as people often\nstruggle to discern real images from deepfakes. Improving digital literacy can\nhelp address these challenges, yet scalable and effective approaches remain\nlargely unexplored. Here, we compare the efficacy of five digital literacy\ninterventions to boost people's ability to discern deepfakes: (1) textual\nguidance on common indicators of deepfakes; (2) visual demonstrations of these\nindicators; (3) a gamified exercise for identifying deepfakes; (4) implicit\nlearning through repeated exposure and feedback; and (5) explanations of how\ndeepfakes are generated with the help of AI. We conducted an experiment with\nN=1,200 participants from the United States to test the immediate and long-term\neffectiveness of our interventions. Our results show that our interventions can\nboost deepfake discernment by up to 13 percentage points while maintaining\ntrust in real images. Altogether, our approach is scalable, suitable for\ndiverse populations, and highly effective for boosting deepfake detection while\nmaintaining trust in truthful information."}
{"id": "2507.22932", "pdf": "https://arxiv.org/pdf/2507.22932.pdf", "abs": "https://arxiv.org/abs/2507.22932", "title": "FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification", "authors": ["Baptiste Lefort", "Eric Benhamou", "Beatrice Guez", "Jean-Jacques Ohana", "Ethan Setrouk", "Alban Etienne"], "categories": ["cs.CL", "q-fin.GN"], "comment": "8 pages", "summary": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility."}
{"id": "2507.23585", "pdf": "https://arxiv.org/pdf/2507.23585.pdf", "abs": "https://arxiv.org/abs/2507.23585", "title": "Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web", "authors": ["Sophia Liu", "Shm Garanganao Almeda"], "categories": ["cs.HC"], "comment": "To appear in: Adjunct Proceedings of the 36th ACM Conference on\n  Hypertext and Social Media, Chicago, IL, USA, September 15-18, 2025", "summary": "Today's algorithm-driven interfaces, from recommendation feeds to GenAI\ntools, often prioritize engagement and efficiency at the expense of user\nagency. As systems take on more decision-making, users have less control over\nwhat they see and how meaning or relationships between content are constructed.\nThis paper introduces \"Hypertextual Friction,\" a conceptual design stance that\nrepositions classical hypertext principles--friction, traceability, and\nstructure--as actionable values for reclaiming agency in algorithmically\nmediated environments. Through a comparative analysis of real-world\ninterfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image\ntools--we examine how different systems structure user experience, navigation,\nand authorship. We show that hypertext systems emphasize provenance,\nassociative thinking, and user-driven meaning-making, while algorithmic systems\ntend to obscure process and flatten participation. We contribute: (1) a\ncomparative analysis of how interface structures shape agency in user-driven\nversus agent-driven systems, and (2) a conceptual stance that offers\nhypertextual values as design commitments for reclaiming agency in an\nincreasingly algorithmic web."}
{"id": "2507.22933", "pdf": "https://arxiv.org/pdf/2507.22933.pdf", "abs": "https://arxiv.org/abs/2507.22933", "title": "Augmented Vision-Language Models: A Systematic Review", "authors": ["Anthony C Davis", "Burhan Sadiq", "Tianmin Shu", "Chien-Ming Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in visual-language machine learning models have demonstrated\nexceptional ability to use natural language and understand visual scenes by\ntraining on large, unstructured datasets. However, this training paradigm\ncannot produce interpretable explanations for its outputs, requires retraining\nto integrate new information, is highly resource-intensive, and struggles with\ncertain forms of logical reasoning. One promising solution involves integrating\nneural networks with external symbolic information systems, forming neural\nsymbolic systems that can enhance reasoning and memory abilities. These neural\nsymbolic systems provide more interpretable explanations to their outputs and\nthe capacity to assimilate new information without extensive retraining.\nUtilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural\ncomponent, augmented by external systems, offers a pragmatic approach to\nrealizing the benefits of neural-symbolic integration. This systematic\nliterature review aims to categorize techniques through which visual-language\nunderstanding can be improved by interacting with external symbolic information\nsystems."}
{"id": "2507.22936", "pdf": "https://arxiv.org/pdf/2507.22936.pdf", "abs": "https://arxiv.org/abs/2507.22936", "title": "Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis", "authors": ["Md Talha Mohsin"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "q-fin.CP"], "comment": "22 Pages, 6 Tables, 7 Figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide variety of Financial Natural Language Processing (FinNLP) tasks.\nHowever, systematic comparisons among widely used LLMs remain underexplored.\nGiven the rapid advancement and growing influence of LLMs in financial\nanalysis, this study conducts a thorough comparative evaluation of five leading\nLLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the\n'Magnificent Seven' technology companies. We create a set of domain-specific\nprompts and then use three methodologies to evaluate model performance: human\nannotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,\nJaccard), and model behavior diagnostics (prompt-level variance and\nacross-model similarity). The results show that GPT gives the most coherent,\nsemantically aligned, and contextually relevant answers; followed by Claude and\nPerplexity. Gemini and DeepSeek, on the other hand, have more variability and\nless agreement. Also, the similarity and stability of outputs change from\ncompany to company and over time, showing that they are sensitive to how\nprompts are written and what source material is used."}
{"id": "2507.22934", "pdf": "https://arxiv.org/pdf/2507.22934.pdf", "abs": "https://arxiv.org/abs/2507.22934", "title": "Deep Learning Approaches for Multimodal Intent Recognition: A Survey", "authors": ["Jingwei Zhao", "Yuhua Wen", "Qifei Li", "Minchi Hu", "Yingying Zhou", "Jingyao Xue", "Junyang Wu", "Yingming Gao", "Zhengqi Wen", "Jianhua Tao", "Ya Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to ACM Computing Surveys", "summary": "Intent recognition aims to identify users' underlying intentions,\ntraditionally focusing on text in natural language processing. With growing\ndemands for natural human-computer interaction, the field has evolved through\ndeep learning and multimodal approaches, incorporating data from audio, vision,\nand physiological signals. Recently, the introduction of Transformer-based\nmodels has led to notable breakthroughs in this domain. This article surveys\ndeep learning methods for intent recognition, covering the shift from unimodal\nto multimodal techniques, relevant datasets, methodologies, applications, and\ncurrent challenges. It provides researchers with insights into the latest\ndevelopments in multimodal intent recognition (MIR) and directions for future\nresearch."}
{"id": "2507.22956", "pdf": "https://arxiv.org/pdf/2507.22956.pdf", "abs": "https://arxiv.org/abs/2507.22956", "title": "LLM-Assisted Cheating Detection in Korean Language via Keystrokes", "authors": ["Dong Hyun Roh", "Rajesh Kumar", "An Ngo"], "categories": ["cs.LG", "cs.HC", "K.3.1"], "comment": "This paper has 11 pages, 6 figures, 2 tables, and has been accepted\n  for publication at IEEE-IJCB 2025", "summary": "This paper presents a keystroke-based framework for detecting LLM-assisted\ncheating in Korean, addressing key gaps in prior research regarding language\ncoverage, cognitive context, and the granularity of LLM involvement. Our\nproposed dataset includes 69 participants who completed writing tasks under\nthree conditions: Bona fide writing, paraphrasing ChatGPT responses, and\ntranscribing ChatGPT responses. Each task spans six cognitive processes defined\nin Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and\ncreate). We extract interpretable temporal and rhythmic features and evaluate\nmultiple classifiers under both Cognition-Aware and Cognition-Unaware settings.\nTemporal features perform well under Cognition-Aware evaluation scenarios,\nwhile rhythmic features generalize better under cross-cognition scenarios.\nMoreover, detecting bona fide and transcribed responses was easier than\nparaphrased ones for both the proposed models and human evaluators, with the\nmodels significantly outperforming the humans. Our findings affirm that\nkeystroke dynamics facilitate reliable detection of LLM-assisted writing across\nvarying cognitive demands and writing strategies, including paraphrasing and\ntranscribing LLM-generated responses."}
{"id": "2507.22935", "pdf": "https://arxiv.org/pdf/2507.22935.pdf", "abs": "https://arxiv.org/abs/2507.22935", "title": "Trusted Knowledge Extraction for Operations and Maintenance Intelligence", "authors": ["Kathleen Mealey", "Jonathan A. Karr Jr.", "Priscila Saboia Moreira", "Paul R. Brenner", "Charles F. Vardeman II"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Deriving operational intelligence from organizational data repositories is a\nkey challenge due to the dichotomy of data confidentiality vs data integration\nobjectives, as well as the limitations of Natural Language Processing (NLP)\ntools relative to the specific knowledge structure of domains such as\noperations and maintenance. In this work, we discuss Knowledge Graph\nconstruction and break down the Knowledge Extraction process into its Named\nEntity Recognition, Coreference Resolution, Named Entity Linking, and Relation\nExtraction functional components. We then evaluate sixteen NLP tools in concert\nwith or in comparison to the rapidly advancing capabilities of Large Language\nModels (LLMs). We focus on the operational and maintenance intelligence use\ncase for trusted applications in the aircraft industry. A baseline dataset is\nderived from a rich public domain US Federal Aviation Administration dataset\nfocused on equipment failures or maintenance requirements. We assess the\nzero-shot performance of NLP and LLM tools that can be operated within a\ncontrolled, confidential environment (no data is sent to third parties). Based\non our observation of significant performance limitations, we discuss the\nchallenges related to trusted NLP and LLM tools as well as their Technical\nReadiness Level for wider use in mission-critical industries such as aviation.\nWe conclude with recommendations to enhance trust and provide our open-source\ncurated dataset to support further baseline testing and evaluation."}
{"id": "2507.23088", "pdf": "https://arxiv.org/pdf/2507.23088.pdf", "abs": "https://arxiv.org/abs/2507.23088", "title": "Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance", "authors": ["Lalithkumar Seenivasan", "Jiru Xu", "Roger D. Soberanis Mukul", "Hao Ding", "Grayson Byrd", "Yu-Chun Ku", "Jose L. Porras", "Masaru Ishii", "Mathias Unberath"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Emerging surgical data science and robotics solutions, especially those\ndesigned to provide assistance in situ, require natural human-machine\ninterfaces to fully unlock their potential in providing adaptive and intuitive\naid. Contemporary AI-driven solutions remain inherently rigid, offering limited\nflexibility and restricting natural human-machine interaction in dynamic\nsurgical environments. These solutions rely heavily on extensive task-specific\npre-training, fixed object categories, and explicit manual-prompting. This work\nintroduces a novel Perception Agent that leverages speech-integrated\nprompt-engineered large language models (LLMs), segment anything model (SAM),\nand any-point tracking foundation models to enable a more natural human-machine\ninteraction in real-time intraoperative surgical assistance. Incorporating a\nmemory repository and two novel mechanisms for segmenting unseen elements,\nPerception Agent offers the flexibility to segment both known and unseen\nelements in the surgical scene through intuitive interaction. Incorporating the\nability to memorize novel elements for use in future surgeries, this work takes\na marked step towards human-machine symbiosis in surgical procedures. Through\nquantitative analysis on a public dataset, we show that the performance of our\nagent is on par with considerably more labor-intensive manual-prompting\nstrategies. Qualitatively, we show the flexibility of our agent in segmenting\nnovel elements (instruments, phantom grafts, and gauze) in a custom-curated\ndataset. By offering natural human-machine interaction and overcoming rigidity,\nour Perception Agent potentially brings AI-based real-time assistance in\ndynamic surgical environments closer to reality."}
{"id": "2507.22936", "pdf": "https://arxiv.org/pdf/2507.22936.pdf", "abs": "https://arxiv.org/abs/2507.22936", "title": "Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis", "authors": ["Md Talha Mohsin"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "q-fin.CP"], "comment": "22 Pages, 6 Tables, 7 Figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide variety of Financial Natural Language Processing (FinNLP) tasks.\nHowever, systematic comparisons among widely used LLMs remain underexplored.\nGiven the rapid advancement and growing influence of LLMs in financial\nanalysis, this study conducts a thorough comparative evaluation of five leading\nLLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the\n'Magnificent Seven' technology companies. We create a set of domain-specific\nprompts and then use three methodologies to evaluate model performance: human\nannotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,\nJaccard), and model behavior diagnostics (prompt-level variance and\nacross-model similarity). The results show that GPT gives the most coherent,\nsemantically aligned, and contextually relevant answers; followed by Claude and\nPerplexity. Gemini and DeepSeek, on the other hand, have more variability and\nless agreement. Also, the similarity and stability of outputs change from\ncompany to company and over time, showing that they are sensitive to how\nprompts are written and what source material is used."}
{"id": "2507.23429", "pdf": "https://arxiv.org/pdf/2507.23429.pdf", "abs": "https://arxiv.org/abs/2507.23429", "title": "Chatting with your ERP: A Recipe", "authors": ["Jorge Ruiz Gómez", "Lidia Andrés Susinos", "Jorge Alamo Olivé", "Sonia Rey Osorno", "Manuel Luis Gonzalez Hernández"], "categories": ["cs.AI", "cs.DB", "cs.ET", "cs.HC", "cs.MA", "68T50, 68P20", "I.2.7; H.2.5; H.2.8; H.5.m"], "comment": "11 pages, includes 3 tables summarizing schema and model performance.\n  Submitted on July 31, 2025. Targets integration of LLM agents with ERP\n  systems using open-weight models and Ollama deployment", "summary": "This paper presents the design, implementation, and evaluation behind a Large\nLanguage Model (LLM) agent that chats with an industrial production-grade ERP\nsystem. The agent is capable of interpreting natural language queries and\ntranslating them into executable SQL statements, leveraging open-weight LLMs. A\nnovel dual-agent architecture combining reasoning and critique stages was\nproposed to improve query generation reliability."}
{"id": "2507.22937", "pdf": "https://arxiv.org/pdf/2507.22937.pdf", "abs": "https://arxiv.org/abs/2507.22937", "title": "CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering", "authors": ["Jinkun Zhao", "Yuanshuai Wang", "Xingjian Zhang", "Ruibo Chen", "Xingchuang Liao", "Junle Wang", "Lei Huang", "Kui Zhang", "Wenjun Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid evolution of artificial intelligence, AIOps has emerged as a\nprominent paradigm in DevOps. Lots of work has been proposed to improve the\nperformance of different AIOps phases. However, constrained by domain-specific\nknowledge, a single model can only handle the operation requirement of a\nspecific task,such as log parser,root cause analysis. Meanwhile, combining\nmultiple models can achieve more efficient results, which have been proved in\nboth previous ensemble learning and the recent LLM training domain. Inspired by\nthese works,to address the similar challenges in AIOPS, this paper first\nproposes a collaboration-of-expert framework(CoE-Ops) incorporating a\ngeneral-purpose large language model task classifier. A retrieval-augmented\ngeneration mechanism is introduced to improve the framework's capability in\nhandling both Question-Answering tasks with high-level(Code,build,Test,etc.)\nand low-level(fault analysis,anomaly detection,etc.). Finally, the proposed\nmethod is implemented in the AIOps domain, and extensive experiments are\nconducted on the DevOps-EVAL dataset. Experimental results demonstrate that\nCoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps\ntasks compared to existing CoE methods, delivers up to 8% accuracy enhancement\nover single AIOps models in DevOps problem resolution, and outperforms\nlarger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy."}
{"id": "2507.23544", "pdf": "https://arxiv.org/pdf/2507.23544.pdf", "abs": "https://arxiv.org/abs/2507.23544", "title": "User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals", "authors": ["Ryo Miyoshi", "Yuki Okafuji", "Takuya Iwamoto", "Junya Nakanishi", "Jun Baba"], "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "This paper has been accepted for presentation at IEEE/RSJ\n  International Conference on Intelligent Robots and Systems 2025 (IROS 2025)", "summary": "In recent years, the demand for social robots has grown, requiring them to\nadapt their behaviors based on users' states. Accurately assessing user\nexperience (UX) in human-robot interaction (HRI) is crucial for achieving this\nadaptability. UX is a multi-faceted measure encompassing aspects such as\nsentiment and engagement, yet existing methods often focus on these\nindividually. This study proposes a UX estimation method for HRI by leveraging\nmultimodal social signals. We construct a UX dataset and develop a\nTransformer-based model that utilizes facial expressions and voice for\nestimation. Unlike conventional models that rely on momentary observations, our\napproach captures both short- and long-term interaction patterns using a\nmulti-instance learning framework. This enables the model to capture temporal\ndynamics in UX, providing a more holistic representation. Experimental results\ndemonstrate that our method outperforms third-party human evaluators in UX\nestimation."}
{"id": "2507.22938", "pdf": "https://arxiv.org/pdf/2507.22938.pdf", "abs": "https://arxiv.org/abs/2507.22938", "title": "A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents", "authors": ["Sumit Soman", "H. G. Ranjani", "Sujoy Roychowdhury", "Venkata Dharma Surya Narayana Sastry", "Akshat Jain", "Pranav Gangrade", "Ayaaz Khan"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Accepted for publication at the KDD 2025 Workshop on Structured\n  Knowledge for Large Language Models", "summary": "Question-Answering (QA) from technical documents often involves questions\nwhose answers are present in figures, such as flowcharts or flow diagrams.\nText-based Retrieval Augmented Generation (RAG) systems may fail to answer such\nquestions. We leverage graph representations of flowcharts obtained from Visual\nlarge Language Models (VLMs) and incorporate them in a text-based RAG system to\nshow that this approach can enable image retrieval for QA in the telecom\ndomain. We present the end-to-end approach from processing technical documents,\nclassifying image types, building graph representations, and incorporating them\nwith the text embedding pipeline for efficient retrieval. We benchmark the same\non a QA dataset created based on proprietary telecom product information\ndocuments. Results show that the graph representations obtained using a\nfine-tuned VLM model have lower edit distance with respect to the ground truth,\nwhich illustrate the robustness of these representations for flowchart images.\nFurther, the approach for QA using these representations gives good retrieval\nperformance using text-based embedding models, including a telecom-domain\nadapted one. Our approach also alleviates the need for a VLM in inference,\nwhich is an important cost benefit for deployed QA systems."}
{"id": "2507.23592", "pdf": "https://arxiv.org/pdf/2507.23592.pdf", "abs": "https://arxiv.org/abs/2507.23592", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "authors": ["Haiyun Zhang", "Stefano Dalla Gasperina", "Saad N. Yousaf", "Toshimitsu Tsuboi", "Tetsuya Narita", "Ashish D. Deshpande"], "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "comment": "8 pages, 10 figures, submitted to RA-L", "summary": "Hand exoskeletons are critical tools for dexterous teleoperation and\nimmersive manipulation interfaces, but achieving accurate hand tracking remains\na challenge due to user-specific anatomical variability and donning\ninconsistencies. These issues lead to kinematic misalignments that degrade\ntracking performance and limit applicability in precision tasks. We propose a\nsubject-specific calibration framework for exoskeleton-based hand tracking that\nuses redundant joint sensing and a residual-weighted optimization strategy to\nestimate virtual link parameters. Implemented on the Maestro exoskeleton, our\nmethod improves joint angle and fingertip position estimation across users with\nvarying hand geometries. We introduce a data-driven approach to empirically\ntune cost function weights using motion capture ground truth, enabling more\naccurate and consistent calibration across participants. Quantitative results\nfrom seven subjects show substantial reductions in joint and fingertip tracking\nerrors compared to uncalibrated and evenly weighted models. Qualitative\nvisualizations using a Unity-based virtual hand further confirm improvements in\nmotion fidelity. The proposed framework generalizes across exoskeleton designs\nwith closed-loop kinematics and minimal sensing, and lays the foundation for\nhigh-fidelity teleoperation and learning-from-demonstration applications."}
{"id": "2507.22939", "pdf": "https://arxiv.org/pdf/2507.22939.pdf", "abs": "https://arxiv.org/abs/2507.22939", "title": "PARROT: An Open Multilingual Radiology Reports Dataset", "authors": ["Bastien Le Guellec", "Kokou Adambounou", "Lisa C Adams", "Thibault Agripnidis", "Sung Soo Ahn", "Radhia Ait Chalal", "Tugba Akinci D Antonoli", "Philippe Amouyel", "Henrik Andersson", "Raphael Bentegeac", "Claudio Benzoni", "Antonino Andrea Blandino", "Felix Busch", "Elif Can", "Riccardo Cau", "Armando Ugo Cavallo", "Christelle Chavihot", "Erwin Chiquete", "Renato Cuocolo", "Eugen Divjak", "Gordana Ivanac", "Barbara Dziadkowiec Macek", "Armel Elogne", "Salvatore Claudio Fanni", "Carlos Ferrarotti", "Claudia Fossataro", "Federica Fossataro", "Katarzyna Fulek", "Michal Fulek", "Pawel Gac", "Martyna Gachowska", "Ignacio Garcia Juarez", "Marco Gatti", "Natalia Gorelik", "Alexia Maria Goulianou", "Aghiles Hamroun", "Nicolas Herinirina", "Krzysztof Kraik", "Dominik Krupka", "Quentin Holay", "Felipe Kitamura", "Michail E Klontzas", "Anna Kompanowska", "Rafal Kompanowski", "Alexandre Lefevre", "Tristan Lemke", "Maximilian Lindholz", "Lukas Muller", "Piotr Macek", "Marcus Makowski", "Luigi Mannacio", "Aymen Meddeb", "Antonio Natale", "Beatrice Nguema Edzang", "Adriana Ojeda", "Yae Won Park", "Federica Piccione", "Andrea Ponsiglione", "Malgorzata Poreba", "Rafal Poreba", "Philipp Prucker", "Jean Pierre Pruvo", "Rosa Alba Pugliesi", "Feno Hasina Rabemanorintsoa", "Vasileios Rafailidis", "Katarzyna Resler", "Jan Rotkegel", "Luca Saba", "Ezann Siebert", "Arnaldo Stanzione", "Ali Fuat Tekin", "Liz Toapanta Yanchapaxi", "Matthaios Triantafyllou", "Ekaterini Tsaoulia", "Evangelia Vassalou", "Federica Vernuccio", "Johan Wasselius", "Weilang Wang", "Szymon Urban", "Adrian Wlodarczak", "Szymon Wlodarczak", "Andrzej Wysocki", "Lina Xu", "Tomasz Zatonski", "Shuhang Zhang", "Sebastian Ziegelmayer", "Gregory Kuchcinski", "Keno K Bressem"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rationale and Objectives: To develop and validate PARROT (Polyglottal\nAnnotated Radiology Reports for Open Testing), a large, multicentric,\nopen-access dataset of fictional radiology reports spanning multiple languages\nfor testing natural language processing applications in radiology. Materials\nand Methods: From May to September 2024, radiologists were invited to\ncontribute fictional radiology reports following their standard reporting\npractices. Contributors provided at least 20 reports with associated metadata\nincluding anatomical region, imaging modality, clinical context, and for\nnon-English reports, English translations. All reports were assigned ICD-10\ncodes. A human vs. AI report differentiation study was conducted with 154\nparticipants (radiologists, healthcare professionals, and non-healthcare\nprofessionals) assessing whether reports were human-authored or AI-generated.\nResults: The dataset comprises 2,658 radiology reports from 76 authors across\n21 countries and 13 languages. Reports cover multiple imaging modalities (CT:\n36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical\nregions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%)\nbeing most prevalent. In the differentiation study, participants achieved 53.9%\naccuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated\nreports, with radiologists performing significantly better (56.9%, 95% CI:\n53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the\nlargest open multilingual radiology report dataset, enabling development and\nvalidation of natural language processing applications across linguistic,\ngeographic, and clinical boundaries without privacy constraints."}
{"id": "2507.23756", "pdf": "https://arxiv.org/pdf/2507.23756.pdf", "abs": "https://arxiv.org/abs/2507.23756", "title": "Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System", "authors": ["Diana Mortagua"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "This study centers on overcoming the challenge of selecting the best\nannotators for each query in Active Learning (AL), with the objective of\nminimizing misclassifications. AL recognizes the challenges related to cost and\ntime when acquiring labeled data, and decreases the number of labeled data\nneeded. Nevertheless, there is still the necessity to reduce annotation errors,\naiming to be as efficient as possible, to achieve the expected accuracy faster.\nMost strategies for query-annotator pairs do not consider internal factors that\naffect productivity, such as mood, attention, motivation, and fatigue levels.\nThis work addresses this gap in the existing literature, by not only\nconsidering how the internal factors influence annotators (mood and fatigue\nlevels) but also presenting a new query-annotator pair strategy, using a\nKnowledge-Based Recommendation System (RS). The RS ranks the available\nannotators, allowing to choose one or more to label the queried instance using\ntheir past accuracy values, and their mood and fatigue levels, as well as\ninformation about the instance queried. This work bases itself on existing\nliterature on mood and fatigue influence on human performance, simulating\nannotators in a realistic manner, and predicting their performance with the RS.\nThe results show that considering past accuracy values, as well as mood and\nfatigue levels reduces the number of annotation errors made by the annotators,\nand the uncertainty of the model through its training, when compared to not\nusing internal factors. Accuracy and F1-score values were also better in the\nproposed approach, despite not being as substantial as the aforementioned. The\nmethodologies and findings presented in this study begin to explore the open\nchallenge of human cognitive factors affecting AL."}
{"id": "2507.22940", "pdf": "https://arxiv.org/pdf/2507.22940.pdf", "abs": "https://arxiv.org/abs/2507.22940", "title": "Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes", "authors": ["Rui Jiao", "Yue Zhang", "Jinku Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy\nfor Confidence Enhancement), a novel framework addressing a critical\nvulnerability in Large Language Models (LLMs): the prevalence of factual\ninaccuracies within intermediate reasoning steps despite correct final answers.\nThis phenomenon poses substantial risks in high-stakes domains including\nhealthcare, legal analysis, and scientific research, where erroneous yet\nconfidently presented reasoning can mislead users into dangerous decisions. Our\nframework integrates three core components: (1) a specialized fact-checking\nclassifier trained on counterfactually augmented data to detect subtle factual\ninconsistencies within reasoning chains; (2) a Group Relative Policy\nOptimization (GRPO) reinforcement learning approach that balances factuality,\ncoherence, and structural correctness through multi-dimensional rewards; and\n(3) a mechanistic interpretability module examining how factuality improvements\nmanifest in model activations during reasoning processes. Extensive evaluation\nacross ten state-of-the-art models reveals concerning patterns: even leading\nmodels like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of\nonly 81.93% and 82.57% respectively. RELIANCE significantly enhances factual\nrobustness (up to 49.90% improvement) while maintaining or improving\nperformance on challenging benchmarks including Math-500, AIME-2024, and GPQA.\nFurthermore, our activation-level analysis provides actionable insights into\nhow factual enhancements reshape reasoning trajectories within model\narchitectures, establishing foundations for future training methodologies that\nexplicitly target factual robustness through activation-guided optimization."}
{"id": "2309.12365", "pdf": "https://arxiv.org/pdf/2309.12365.pdf", "abs": "https://arxiv.org/abs/2309.12365", "title": "An Efficient Intelligent Semi-Automated Warehouse Inventory Stocktaking System", "authors": ["Chunan Tong"], "categories": ["cs.HC", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "In the context of evolving supply chain management, the significance of\nefficient inventory management has grown substantially for businesses. However,\nconventional manual and experience-based approaches often struggle to meet the\ncomplexities of modern market demands. This research introduces an intelligent\ninventory management system to address challenges related to inaccurate data,\ndelayed monitoring, and overreliance on subjective experience in forecasting.\nThe proposed system integrates bar code and distributed flutter application\ntechnologies for intelligent perception, alongside comprehensive big data\nanalytics to enable data-driven decision-making. Through meticulous analysis,\nsystem design, critical technology exploration, and simulation validation, the\neffectiveness of the proposed system is successfully demonstrated. The\nintelligent system facilitates second-level monitoring, high-frequency checks,\nand artificial intelligence-driven forecasting, consequently enhancing the\nautomation, precision, and intelligence of inventory management. This system\ncontributes to cost reduction and optimized inventory sizes through accurate\npredictions and informed decisions, ultimately achieving a mutually beneficial\nscenario. The outcomes of this research offer"}
{"id": "2507.22941", "pdf": "https://arxiv.org/pdf/2507.22941.pdf", "abs": "https://arxiv.org/abs/2507.22941", "title": "SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology", "authors": ["Paul Minchella", "Loïc Verlingue", "Stéphane Chrétien", "Rémi Vaucher", "Guillaume Metzler"], "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.AP"], "comment": "12 pages, 2 figures, accepted for ECML PKDD 2025", "summary": "Electronic medical reports (EHR) contain a vast amount of information that\ncan be leveraged for machine learning applications in healthcare. However,\nexisting survival analysis methods often struggle to effectively handle the\ncomplexity of textual data, particularly in its sequential form. Here, we\npropose SigBERT, an innovative temporal survival analysis framework designed to\nefficiently process a large number of clinical reports per patient. SigBERT\nprocesses timestamped medical reports by extracting and averaging word\nembeddings into sentence embeddings. To capture temporal dynamics from the time\nseries of sentence embedding coordinates, we apply signature extraction from\nrough path theory to derive geometric features for each patient, which\nsignificantly enhance survival model performance by capturing complex temporal\ndynamics. These features are then integrated into a LASSO-penalized Cox model\nto estimate patient-specific risk scores. The model was trained and evaluated\non a real-world oncology dataset from the L\\'eon B\\'erard Center corpus, with a\nC-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT\nintegrates sequential medical data to enhance risk estimation, advancing\nnarrative-based survival analysis."}
{"id": "2408.09186", "pdf": "https://arxiv.org/pdf/2408.09186.pdf", "abs": "https://arxiv.org/abs/2408.09186", "title": "EEG-SCMM: Soft Contrastive Masked Modeling for Cross-Corpus EEG-Based Emotion Recognition", "authors": ["Qile Liu", "Weishan Ye", "Lingli Zhang", "Zhen Liang"], "categories": ["cs.HC", "cs.AI"], "comment": "18 pages, 10 figures, 14 tables. Accepted in ACMMM 2025", "summary": "Emotion recognition using electroencephalography (EEG) signals has attracted\nincreasing attention in recent years. However, existing methods often lack\ngeneralization in cross-corpus settings, where a model trained on one dataset\nis directly applied to another without retraining, due to differences in data\ndistribution and recording conditions. To tackle the challenge of cross-corpus\nEEG-based emotion recognition, we propose a novel framework termed Soft\nContrastive Masked Modeling (SCMM). Grounded in the theory of emotional\ncontinuity, SCMM integrates soft contrastive learning with a hybrid masking\nstrategy to effectively capture emotion dynamics (refer to short-term\ncontinuity). Specifically, in the self-supervised learning stage, we propose a\nsoft weighting mechanism that assigns similarity scores to sample pairs,\nenabling fine-grained modeling of emotional transitions and capturing the\ntemporal continuity of human emotions. To further enhance representation\nlearning, we design a similarity-aware aggregator that fuses complementary\ninformation from semantically related samples based on pairwise similarities,\nthereby improving feature expressiveness and reconstruction quality. This dual\ndesign contributes to a more discriminative and transferable representation,\nwhich is crucial for robust cross-corpus generalization. Extensive experiments\non the SEED, SEED-IV, and DEAP datasets show that SCMM achieves\nstate-of-the-art (SOTA) performance, outperforming the second-best method by an\naverage accuracy of 4.26% under both same-class and different-class\ncross-corpus settings. The source code is available at\nhttps://github.com/Kyler-RL/SCMM."}
{"id": "2507.22943", "pdf": "https://arxiv.org/pdf/2507.22943.pdf", "abs": "https://arxiv.org/abs/2507.22943", "title": "A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies", "authors": ["Shirley V Wang", "Georg Hahn", "Sushama Kattinakere Sreedhara", "Mufaddal Mahesri", "Haritha S. Pillai", "Rajendra Aldis", "Joyce Lii", "Sarah K. Dutcher", "Rhoda Eniafe", "Jamal T. Jones", "Keewan Kim", "Jiwei He", "Hana Lee", "Sengwee Toh", "Rishi J Desai", "Jie Yang"], "categories": ["cs.CL", "stat.ME"], "comment": null, "summary": "Background: One of the ways to enhance analyses conducted with large claims\ndatabases is by validating the measurement characteristics of code-based\nalgorithms used to identify health outcomes or other key study parameters of\ninterest. These metrics can be used in quantitative bias analyses to assess the\nrobustness of results for an inferential study given potential bias from\noutcome misclassification. However, extensive time and resource allocation are\ntypically re-quired to create reference-standard labels through manual chart\nreview of free-text notes from linked electronic health records. Methods: We\ndescribe an expedited process that introduces efficiency in a validation study\nus-ing two distinct mechanisms: 1) use of natural language processing (NLP) to\nreduce time spent by human reviewers to review each chart, and 2) a multi-wave\nadaptive sampling approach with pre-defined criteria to stop the validation\nstudy once performance characteristics are identified with sufficient\nprecision. We illustrate this process in a case study that validates the\nperformance of a claims-based outcome algorithm for intentional self-harm in\npatients with obesity. Results: We empirically demonstrate that the\nNLP-assisted annotation process reduced the time spent on review per chart by\n40% and use of the pre-defined stopping rule with multi-wave samples would have\nprevented review of 77% of patient charts with limited compromise to precision\nin derived measurement characteristics. Conclusion: This approach could\nfacilitate more routine validation of code-based algorithms used to define key\nstudy parameters, ultimately enhancing understanding of the reliability of\nfind-ings derived from database studies."}
{"id": "2409.11911", "pdf": "https://arxiv.org/pdf/2409.11911.pdf", "abs": "https://arxiv.org/abs/2409.11911", "title": "AI vs. Human Paintings? Deciphering Public Interactions and Perceptions towards AI-Generated Paintings on TikTok", "authors": ["Jiajun Wang", "Xiangzhe Yuan", "Siying Hu", "Zhicong Lu"], "categories": ["cs.HC"], "comment": "Published online in International Journal of Human Computer\n  Interaction", "summary": "With the development of generative AI technology, a vast array of\nAI-generated paintings (AIGP) have gone viral on social media like TikTok.\nHowever, some negative news about AIGP has also emerged. For example, in 2022,\nnumerous painters worldwide organized a large-scale anti-AI movement because of\nthe infringement in generative AI model training. This event reflected a social\nissue that, with the development and application of generative AI, public\nfeedback and feelings towards it may have been overlooked. Therefore, to\ninvestigate public interactions and perceptions towards AIGP on social media,\nwe analyzed user engagement level and comment sentiment scores of AIGP using\nhuman painting videos as a baseline. In analyzing user engagement, we also\nconsidered the possible moderating effect of the aesthetic quality of\nPaintings. Utilizing topic modeling, we identified seven reasons, including\nhyperrealistic quality, ambivalent reactions, perceived theft of art, etc.,\nleading to negative public perceptions of AIGP. Our work may provide\ninstructive suggestions for future generative AI technology development and\navoid potential crises in human-AI collaboration."}
{"id": "2507.22944", "pdf": "https://arxiv.org/pdf/2507.22944.pdf", "abs": "https://arxiv.org/abs/2507.22944", "title": "Opacity as Authority: Arbitrariness and the Preclusion of Contestation", "authors": ["Naomi Omeonga wa Kayembe"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This article redefines arbitrariness not as a normative flaw or a symptom of\ndomination, but as a foundational functional mechanism structuring human\nsystems and interactions. Diverging from critical traditions that conflate\narbitrariness with injustice, it posits arbitrariness as a semiotic trait: a\nproperty enabling systems - linguistic, legal, or social - to operate\neffectively while withholding their internal rationale. Building on Ferdinand\nde Saussure's concept of l'arbitraire du signe, the analysis extends this\nprinciple beyond language to demonstrate its cross-domain applicability,\nparticularly in law and social dynamics. The paper introduces the \"Motivation\n-> Constatability -> Contestability\" chain, arguing that motivation functions\nas a crucial interface rendering an act's logic vulnerable to intersubjective\ncontestation. When this chain is broken through mechanisms like\n\"immotivization\" or \"Conflict Lateralization\" (exemplified by \"the blur of the\nwolf drowned in the fish\"), acts produce binding effects without exposing their\nrationale, thus precluding justiciability. This structural opacity, while\nappearing illogical, is a deliberate design protecting authority from\naccountability. Drawing on Shannon's entropy model, the paper formalizes\narbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern\ntheory of arbitrariness as a neutral operator central to control as well as\ncare, an overlooked dimension of interpersonal relations. While primarily\ndeveloped through human social systems, this framework also illuminates a new\npathway for analyzing explainability in advanced artificial intelligence\nsystems."}
{"id": "2410.07486", "pdf": "https://arxiv.org/pdf/2410.07486.pdf", "abs": "https://arxiv.org/abs/2410.07486", "title": "Visual Story-Writing: Writing by Manipulating Visual Representations of Stories", "authors": ["Damien Masson", "Zixin Zhao", "Fanny Chevalier"], "categories": ["cs.HC"], "comment": "In Proceedings of the 38th Annual ACM Symposium on User Interface\n  Software and Technology (UIST '25)", "summary": "We define \"visual story-writing\" as using visual representations of story\nelements to support writing and revising narrative texts. To demonstrate this\napproach, we developed a text editor that automatically visualizes a graph of\nentity interactions, movement between locations, and a timeline of story\nevents. Interacting with these visualizations results in suggested text edits:\nfor example, connecting two characters in the graph creates an interaction\nbetween them, moving an entity updates their described location, and\nrearranging events on the timeline reorganizes the narrative sequence. Through\ntwo user studies on narrative text editing and writing, we found that visuals\nsupported participants in planning high-level revisions, tracking story\nelements, and exploring story variations in ways that encourage creativity.\nBroadly, our work lays the foundation for writing support, not just through\nwords, but also visuals."}
{"id": "2507.22968", "pdf": "https://arxiv.org/pdf/2507.22968.pdf", "abs": "https://arxiv.org/abs/2507.22968", "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations", "authors": ["Chengqian Ma", "Wei Tao", "Yiwen Guo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Spoken Dialogue Models (SDMs) have recently attracted significant attention\nfor their ability to generate voice responses directly to users' spoken\nqueries. Despite their increasing popularity, there exists a gap in research\nfocused on comprehensively understanding their practical effectiveness in\ncomprehending and emulating human conversations. This is especially true\ncompared to text-based Large Language Models (LLMs), which benefit from\nextensive benchmarking. Human voice interactions are inherently more complex\nthan text due to characteristics unique to spoken dialogue. Ambiguity poses one\nchallenge, stemming from semantic factors like polysemy, as well as\nphonological aspects such as heterograph, heteronyms, and stress patterns.\nAdditionally, context-dependency, like omission, coreference, and multi-turn\ninteraction, adds further complexity to human conversational dynamics. To\nilluminate the current state of SDM development and to address these\nchallenges, we present a benchmark dataset in this paper, which comprises 1,079\ninstances in English and Chinese. Accompanied by an LLM-based evaluation method\nthat closely aligns with human judgment, this dataset facilitates a\ncomprehensive exploration of the performance of SDMs in tackling these\npractical challenges."}
{"id": "2505.19101", "pdf": "https://arxiv.org/pdf/2505.19101.pdf", "abs": "https://arxiv.org/abs/2505.19101", "title": "Agentic Visualization: Extracting Agent-based Design Patterns from Visualization Systems", "authors": ["Vaishali Dhanoa", "Anton Wolter", "Gabriela Molina León", "Hans-Jörg Schulz", "Niklas Elmqvist"], "categories": ["cs.HC"], "comment": null, "summary": "Autonomous agents powered by Large Language Models are transforming AI,\ncreating an imperative for the visualization field to embrace agentic\nframeworks. However, our field's focus on a human in the sensemaking loop\nraises critical questions about autonomy, delegation, and coordination for such\n\\textit{agentic visualization} that preserve human agency while amplifying\nanalytical capabilities. This paper addresses these questions by reinterpreting\nexisting visualization systems with semi-automated or fully automatic AI\ncomponents through an agentic lens. Based on this analysis, we extract a\ncollection of design patterns for agentic visualization, including agentic\nroles, communication and coordination. These patterns provide a foundation for\nfuture agentic visualization systems that effectively harness AI agents while\nmaintaining human insight and control."}
{"id": "2507.23063", "pdf": "https://arxiv.org/pdf/2507.23063.pdf", "abs": "https://arxiv.org/abs/2507.23063", "title": "Math Natural Language Inference: this should be easy!", "authors": ["Valeria de Paiva", "Qiyue Gao", "Hai Hu", "Pavel Kovalev", "Yikang Liu", "Lawrence S. Moss", "Zhiheng Qian"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "9 pages plus appendices", "summary": "We ask whether contemporary LLMs are able to perform natural language\ninference (NLI) tasks on mathematical texts. We call this the Math NLI problem.\nWe construct a corpus of Math NLI pairs whose premises are from extant\nmathematical text and whose hypotheses and gold labels were provided by people\nwith experience in both research-level mathematics and also in the NLI field.\nWe also investigate the quality of corpora using the same premises but whose\nhypotheses are provided by LLMs themselves. We not only investigate the\nperformance but also the inter-group consistency of the diverse group of LLMs.\nWe have both positive and negative findings. Among our positive findings: in\nsome settings, using a majority vote of LLMs is approximately equivalent to\nusing human-labeled data in the Math NLI area. On the negative side: LLMs still\nstruggle with mathematical language. They occasionally fail at even basic\ninferences. Current models are not as prone to hypothesis-only \"inference\" in\nour data the way the previous generation had been. In addition to our findings,\nwe also provide our corpora as data to support future work on Math NLI."}
{"id": "2507.06460", "pdf": "https://arxiv.org/pdf/2507.06460.pdf", "abs": "https://arxiv.org/abs/2507.06460", "title": "Ragged Blocks: Rendering Structured Text with Style", "authors": ["Sam Cohen", "Ravi Chugh"], "categories": ["cs.HC"], "comment": "UIST 2025 Paper + Appendices", "summary": "Whether it be source code in a programming language, prose in natural\nlanguage, or otherwise, text is highly structured. Currently, text\nvisualizations are confined either to _flat, line-based_ decorations, which can\nconvey only limited information about textual structure, or _nested boxes_,\nwhich convey structure but often destroy the typographic layout of the\nunderlying text. We hypothesize that the lack of rich styling options limits\nthe kinds of information that are displayed alongside text, wherever it may be\ndisplayed.\n  In this paper, we show that it is possible to achieve arbitrarily nested\ndecorations while minimally disturbing the underlying typographic layout.\nSpecifically, we present a layout algorithm that generates _ragged blocks_, or\n_rocks_, which are rectilinear polygons that allow nested text to be compactly\nrendered even when styled with borders and padding. Our layout algorithm is\nevaluated on a benchmark suite comprising representative source code files in\nmultiple programming languages. The (ragged block) layouts produced by our\nalgorithm are substantially more compact than the (rectangular block) layouts\nproduced by conventional techniques, when uniformly styling every element in\nthe syntax tree with borders and padding."}
{"id": "2507.23082", "pdf": "https://arxiv.org/pdf/2507.23082.pdf", "abs": "https://arxiv.org/abs/2507.23082", "title": "Exploring In-Context Learning for Frame-Semantic Parsing", "authors": ["Diego Garat", "Guillermo Moncecchi", "Dina Wonsever"], "categories": ["cs.CL"], "comment": null, "summary": "Frame Semantic Parsing (FSP) entails identifying predicates and labeling\ntheir arguments according to Frame Semantics. This paper investigates the use\nof In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP\nwithout model fine-tuning. We propose a method that automatically generates\ntask-specific prompts for the Frame Identification (FI) and Frame Semantic Role\nLabeling (FSRL) subtasks, relying solely on the FrameNet database. These\nprompts, constructed from frame definitions and annotated examples, are used to\nguide six different LLMs. Experiments are conducted on a subset of frames\nrelated to violent events. The method achieves competitive results, with F1\nscores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers\na practical and effective alternative to traditional fine-tuning for\ndomain-specific FSP tasks."}
{"id": "2507.22252", "pdf": "https://arxiv.org/pdf/2507.22252.pdf", "abs": "https://arxiv.org/abs/2507.22252", "title": "Multidimensional Assessment of Takeover Performance in Conditionally Automated Driving", "authors": ["Kexin Liang", "Jan Luca Kästle", "Bani Anvari", "Simeon C. Calvert", "J. W. C. van Lint"], "categories": ["cs.HC"], "comment": null, "summary": "When automated driving systems encounter complex situations beyond their\noperational capabilities, they issue takeover requests, prompting drivers to\nresume vehicle control and return to the driving loop as a critical safety\nbackup. However, this control transition places significant demands on drivers,\nrequiring them to promptly respond to takeover requests while executing\nhigh-quality interventions. To ensure safe and comfortable control transitions,\nit is essential to develop a deep understanding of the key factors influencing\nvarious takeover performance aspects. This study evaluates drivers' takeover\nperformance across three dimensions: response efficiency, user experience, and\ndriving safety - using a driving simulator experiment. EXtreme Gradient\nBoosting (XGBoost) models are used to investigate the contributions of two\ncritical factors, i.e., Situational Awareness (SA) and Spare Capacity (SC), in\npredicting various takeover performance metrics by comparing the predictive\nresults to the baseline models that rely solely on basic Driver Characteristics\n(DC). The results reveal that (i) higher SA enables drivers to respond to\ntakeover requests more quickly, particularly for reflexive responses; and (ii)\nSC shows a greater overall impact on takeover quality than SA, where higher SC\ngenerally leads to enhanced subjective rating scores and objective execution\ntrajectories. These findings highlight the distinct yet complementary roles of\nSA and SC in shaping performance components, offering valuable insights for\noptimizing human-vehicle interactions and enhancing automated driving system\ndesign."}
{"id": "2507.23083", "pdf": "https://arxiv.org/pdf/2507.23083.pdf", "abs": "https://arxiv.org/abs/2507.23083", "title": "Context-aware Rotary Position Embedding", "authors": ["Ali Veisi", "Delaram Fartoot", "Hamidreza Amirzadeh"], "categories": ["cs.CL"], "comment": "4 pages, 1 table", "summary": "Positional encoding is a vital component of Transformer architectures,\nenabling models to incorporate sequence order into self-attention mechanisms.\nRotary Positional Embeddings (RoPE) have become a widely adopted solution due\nto their compatibility with relative position encoding and computational\nefficiency. However, RoPE relies on static, input-independent sinusoidal\nfrequency patterns, limiting its ability to model context-sensitive\nrelationships. In this work, we propose CARoPE (Context-Aware Rotary Positional\nEmbedding), a novel generalization of RoPE that dynamically generates\nhead-specific frequency patterns conditioned on token embeddings. This design\nintroduces token- and context-sensitive positional representations while\npreserving RoPE efficiency and architectural simplicity. CARoPE computes\ninput-dependent phase shifts using a bounded transformation of token embeddings\nand integrates them into the rotary mechanism across attention heads. We\nevaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on\nnext-token prediction tasks. Experimental results show that CARoPE consistently\noutperforms RoPE and other common positional encoding baselines, achieving\nsignificantly lower perplexity, even at longer context lengths. Additionally,\nCARoPE enables faster training throughput without sacrificing model stability.\nThese findings demonstrate that CARoPE offers a scalable, expressive, and\nefficient upgrade to existing positional encoding strategies in Transformer\nmodels."}
{"id": "2507.22810", "pdf": "https://arxiv.org/pdf/2507.22810.pdf", "abs": "https://arxiv.org/abs/2507.22810", "title": "VRISE: A Virtual Reality Platfrom for Immersive and Interactive Surveying Education", "authors": ["Daniel Udekwe", "Dimitrios Bolkas", "Eren Erman Ozguven", "Ren Moses", "Qianwen Guo"], "categories": ["cs.HC", "cs.ET", "cs.SE"], "comment": null, "summary": "Surveying is a core component of civil engineering education, requiring\nstudents to engage in hands-on spatial measurement, instrumentation handling,\nand field-based decision-making. However, traditional instruction often poses\nlogistical and cognitive challenges that can hinder accessibility and student\nengagement. While virtual laboratories have gained traction in engineering\neducation, few are purposefully designed to support flexible, adaptive learning\nin surveying. To address this gap, we developed Virtual Reality for Immersive\nand Interactive Surveying Education (VRISE), an immersive virtual reality\nlaboratory that replicates ground-based and aerial surveying tasks through\ncustomizable, accessible, and user-friendly modules. VRISE features interactive\nexperiences such as differential leveling with a digital level equipment and\nwaypoint-based drone navigation, enhanced by input smoothing, adaptive\ninterfaces, and real-time feedback to accommodate diverse learning styles.\nEvaluation across multiple user sessions demonstrated consistent gains in\nmeasurement accuracy, task efficiency, and interaction quality, with a clear\nprogression in skill development across the ground-based and aerial surveying\nmodalities. By reducing cognitive load and physical demands, even in tasks\nrequiring fine motor control and spatial reasoning, VRISE demonstrates the\npotential of immersive, repeatable digital environments to enhance surveying\neducation, broaden participation, and strengthen core competencies in a safe\nand engaging setting."}
{"id": "2507.23095", "pdf": "https://arxiv.org/pdf/2507.23095.pdf", "abs": "https://arxiv.org/abs/2507.23095", "title": "SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity", "authors": ["Ishani Mondal", "Meera Bharadwaj", "Ayush Roy", "Aparna Garimella", "Jordan Lee Boyd-Graber"], "categories": ["cs.CL", "cs.AI"], "comment": "Under Submission", "summary": "We present SMART-Editor, a framework for compositional layout and content\nediting across structured (posters, websites) and unstructured (natural images)\ndomains. Unlike prior models that perform local edits, SMART-Editor preserves\nglobal coherence through two strategies: Reward-Refine, an inference-time\nrewardguided refinement method, and RewardDPO, a training-time preference\noptimization approach using reward-aligned layout pairs. To evaluate model\nperformance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain,\ncascading edit scenarios. SMART-Editor outperforms strong baselines like\nInstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in\nstructured settings and Reward-Refine showing advantages on natural images.\nAutomatic and human evaluations confirm the value of reward-guided planning in\nproducing semantically consistent and visually aligned edits."}
{"id": "2401.13481", "pdf": "https://arxiv.org/pdf/2401.13481.pdf", "abs": "https://arxiv.org/abs/2401.13481", "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment", "authors": ["Joshua Ashkinaze", "Julia Mendelsohn", "Li Qiwei", "Ceren Budak", "Eric Gilbert"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "Accepted at ACM Collective Intelligence 2025. Originally posted 2024", "summary": "Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- speaks to the interdependent process of cultural creation: creative ideas\nare built upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI and that participants may knowingly adopt AI ideas when the\ntask is difficult. Our findings suggest that introducing AI ideas may increase\ncollective diversity but not individual creativity."}
{"id": "2507.23104", "pdf": "https://arxiv.org/pdf/2507.23104.pdf", "abs": "https://arxiv.org/abs/2507.23104", "title": "RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL", "authors": ["Jeffrey Eben", "Aitzaz Ahmad", "Stephen Lau"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite advances in large language model (LLM)-based natural language\ninterfaces for databases, scaling to enterprise-level data catalogs remains an\nunder-explored challenge. Prior works addressing this challenge rely on\ndomain-specific fine-tuning - complicating deployment - and fail to leverage\nimportant semantic context contained within database metadata. To address these\nlimitations, we introduce a component-based retrieval architecture that\ndecomposes database schemas and metadata into discrete semantic units, each\nseparately indexed for targeted retrieval. Our approach prioritizes effective\ntable identification while leveraging column-level information, ensuring the\ntotal number of retrieved tables remains within a manageable context budget.\nExperiments demonstrate that our method maintains high recall and accuracy,\nwith our system outperforming baselines over massive databases with varying\nstructure and available metadata. Our solution enables practical text-to-SQL\nsystems deployable across diverse enterprise settings without specialized\nfine-tuning, addressing a critical scalability gap in natural language database\ninterfaces."}
{"id": "2504.14928", "pdf": "https://arxiv.org/pdf/2504.14928.pdf", "abs": "https://arxiv.org/abs/2504.14928", "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "comment": "Paper URL: https://aclanthology.org/2025.acl-long.1576 ;Presentation\n  Video: https://www.youtube.com/watch?v=j63ooKE50I0", "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness."}
{"id": "2507.23121", "pdf": "https://arxiv.org/pdf/2507.23121.pdf", "abs": "https://arxiv.org/abs/2507.23121", "title": "Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity", "authors": ["Xinwei Wu", "Haojie Li", "Hongyu Liu", "Xinyu Ji", "Ruohan Li", "Yule Chen", "Yigeng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at KDD workshop on Evaluation and Trustworthiness of Agentic\n  and Generative AI Models (Agentic & GenAI Evaluation Workshop KDD '25)", "summary": "In this work, we study a critical research problem regarding the\ntrustworthiness of large language models (LLMs): how LLMs behave when\nencountering ambiguous narrative text, with a particular focus on Chinese\ntextual ambiguity. We created a benchmark dataset by collecting and generating\nambiguous sentences with context and their corresponding disambiguated pairs,\nrepresenting multiple possible interpretations. These annotated examples are\nsystematically categorized into 3 main categories and 9 subcategories. Through\nexperiments, we discovered significant fragility in LLMs when handling\nambiguity, revealing behavior that differs substantially from humans.\nSpecifically, LLMs cannot reliably distinguish ambiguous text from unambiguous\ntext, show overconfidence in interpreting ambiguous text as having a single\nmeaning rather than multiple meanings, and exhibit overthinking when attempting\nto understand the various possible meanings. Our findings highlight a\nfundamental limitation in current LLMs that has significant implications for\ntheir deployment in real-world applications where linguistic ambiguity is\ncommon, calling for improved approaches to handle uncertainty in language\nunderstanding. The dataset and code are publicly available at this GitHub\nrepository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation."}
{"id": "2506.18199", "pdf": "https://arxiv.org/pdf/2506.18199.pdf", "abs": "https://arxiv.org/abs/2506.18199", "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review", "authors": ["Bushra Asseri", "Estabrag Abdelaziz", "Areej Al-Wabil"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "Research is incomplete", "summary": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility."}
{"id": "2507.23135", "pdf": "https://arxiv.org/pdf/2507.23135.pdf", "abs": "https://arxiv.org/abs/2507.23135", "title": "ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans", "authors": ["Ananya Sadana", "Yash Kumar Lal", "Jiawei Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding causal relationships across modalities is a core challenge for\nmultimodal models operating in real-world environments. We introduce ISO-Bench,\na benchmark for evaluating whether models can infer causal dependencies between\nvisual observations and procedural text. Each example presents an image of a\ntask step and a text snippet from a plan, with the goal of deciding whether the\nvisual step occurs before or after the referenced text step. Evaluation results\non ten frontier vision-language models show underwhelming performance: the best\nzero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest\ngains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further\nhighlights concrete directions for improving causal understanding in multimodal\nmodels."}
{"id": "2507.23158", "pdf": "https://arxiv.org/pdf/2507.23158.pdf", "abs": "https://arxiv.org/abs/2507.23158", "title": "User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal", "authors": ["Yuhan Liu", "Michael J. Q. Zhang", "Eunsol Choi"], "categories": ["cs.CL"], "comment": "Earlier version of this paper was presented at 2nd Workshop on Models\n  of Human Feedback for AI Alignment (MoFA), ICML 2025", "summary": "Once language models (LMs) are deployed, they can interact with users\nlong-term, ideally evolving continuously based on their feedback. Asking for\ndirect user feedback can be disruptive; thus, we study harvesting user feedback\nfrom user-LM interaction logs. We study implicit user feedback in two user-LM\ninteraction datasets (WildChat and LMSYS). First, we analyze user feedback in\nthe user-LLM conversation trajectory, providing insights into when and why such\nfeedback occurs. Second, we study harvesting learning signals from such\nimplicit user feedback. We find that the contents of user feedback (e.g., user\nwanted clarification), not just the polarity (e.g., users were unhappy with the\nprevious model response), can improve model performance in short human-designed\nquestions (MTBench) but not on longer and more complex questions (WildBench).\nWe also find that the usefulness of user feedback is largely tied to the\nquality of the user's initial prompt. Together, we provide an in-depth study of\nimplicit user feedback, showing its potential and limitations."}
{"id": "2507.23167", "pdf": "https://arxiv.org/pdf/2507.23167.pdf", "abs": "https://arxiv.org/abs/2507.23167", "title": "LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration", "authors": ["Jizhou Guo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, with different models excelling in distinct domains and specific\nabilities. Effectively combining the predictions of multiple LLMs is crucial\nfor enhancing system robustness and performance. However, existing ensemble\nmethods often rely on simple techniques like voting or logits ensembling, which\noverlook the varying confidence and reliability of models in different\ncontexts. In this work, we propose LENS (Learning ENsemble confidence from\nNeural States), a novel approach that learns to estimate model confidence by\nanalyzing internal representations. For each LLM, we train a lightweight linear\nconfidence predictor that leverages layer-wise hidden states and normalized\nprobabilities as inputs. This allows for more nuanced weighting of model\npredictions based on their context-dependent reliability. Our method does not\nrequire modifying the model parameters and requires negligible additional\ncomputation. Experimental results on multiple-choice and boolean\nquestion-answering tasks demonstrate that LENS outperforms traditional ensemble\nmethods by a substantial margin. Our findings suggest that internal\nrepresentations provide valuable signals for determining model confidence and\ncan be effectively leveraged for ensemble learning."}
{"id": "2507.23194", "pdf": "https://arxiv.org/pdf/2507.23194.pdf", "abs": "https://arxiv.org/abs/2507.23194", "title": "Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks", "authors": ["Jianghui Wang", "Vinay Joshi", "Saptarshi Majumder", "Xu Chao", "Bin Ding", "Ziqiong Liu", "Pratik Prabhanjan Brahma", "Dong Li", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The demand for AI-generated GPU kernels is rapidly growing, influenced by the\nneed for scalable, hardware-optimized solutions in both industry and academia.\nAs deep learning workloads grow in complexity and diversity, it is imperative\nto automate low-level kernel development to meet performance and productivity\ndemands. Major cloud providers, semiconductor companies, and research\ninstitutions are now investing heavily in AI-driven code generation for GPUs,\naiming to reduce manual optimization efforts while achieving near-expert\nperformance on hardware like AMD MI300X. The Triton language, a Python-based\nDSL for GPU programming, has emerged as a popular target for such AI-generated\nkernels due to its balance of performance and ease-of-coding. In this work, we\npresent an evaluation suite for Triton-based GPU kernels and GEAK (Generating\nEfficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs\nto generate performant Triton code specifically for AMD GPUs, including the AMD\nMI300X and MI250. GEAK leverages inference-time compute scaling to produce\nTriton-based GPU kernels using a reasoning loop adapted from Reflexion-style\nfeedback mechanisms. On two evaluation benchmarks, GEAK significantly\noutperformed the baselines of directly prompting frontier LLMs as well as\nReflexion-based generation pipelines by achieving correctness up to $63$% and\nexecution speed up of up to $2.59$X. These results highlight the promise of\nGEAK-like agentic code generation for accelerating the adoption of diverse\nhardware platforms and democratizing access to expert-level kernel performance."}
{"id": "2507.23211", "pdf": "https://arxiv.org/pdf/2507.23211.pdf", "abs": "https://arxiv.org/abs/2507.23211", "title": "Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples", "authors": ["Yunhao Liang", "Ruixuan Ying", "Takuya Taniguchi", "Zhe Cui"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models exhibit powerful few-shot in-context learning (ICL)\ncapabilities, but the performance is highly sensitive to provided examples.\n  Recent research has focused on retrieving corresponding examples for each\ninput query, not only enhancing the efficiency and scalability of the learning\nprocess but also mitigating inherent biases in manual example selection.\n  However, these studies have primarily emphasized leveraging Positive samples\nwhile overlooking the additional information within Negative samples for\ncontextual learning.\n  We propose a novel method that utilizes Negative samples to better select\nPositive sample examples, thereby enhancing the performance of few-shot ICL.\nInitially, we construct Positive and Negative sample corpora based on\nZero-Shot-Cot. Then, during inference, we employ a semantic similarity-based\napproach to select the most similar examples from both the Positive and\nNegative corpora for a given query. Subsequently, we further retrieve Positive\nexamples from the Positive sample corpus based on semantic similarity to the\nNegative examples, then concatenating them with the previously selected\nPositive examples to serve as ICL demonstrations. Experimental results\ndemonstrate that our approach surpasses methods solely relying on the most\nsimilar positive examples for context, validating that the additional\ninformation in negative samples aids in enhancing ICL performance through\nimproved Positive sample selection."}
{"id": "2507.23220", "pdf": "https://arxiv.org/pdf/2507.23220.pdf", "abs": "https://arxiv.org/abs/2507.23220", "title": "Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders", "authors": ["Carolina Zheng", "Nicolas Beltran-Velez", "Sweta Karlekar", "Claudia Shi", "Achille Nazaret", "Asif Mallik", "Amir Feder", "David M. Blei"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Traditional topic models are effective at uncovering latent themes in large\ntext collections. However, due to their reliance on bag-of-words\nrepresentations, they struggle to capture semantically abstract features. While\nsome neural variants use richer representations, they are similarly constrained\nby expressing topics as word lists, which limits their ability to articulate\ncomplex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic\nmodels that operate on interpretable features learned by sparse autoencoders\n(SAEs). By defining topics over this semantically rich space, MTMs can reveal\ndeeper conceptual themes with expressive feature descriptions. Moreover,\nuniquely among topic models, MTMs enable controllable text generation using\ntopic-based steering vectors. To properly evaluate MTM topics against\nword-list-based approaches, we propose \\textit{topic judge}, an LLM-based\npairwise comparison evaluation framework. Across five datasets, MTMs match or\nexceed traditional and neural baselines on coherence metrics, are consistently\npreferred by topic judge, and enable effective steering of LLM outputs."}
{"id": "2507.23227", "pdf": "https://arxiv.org/pdf/2507.23227.pdf", "abs": "https://arxiv.org/abs/2507.23227", "title": "Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs", "authors": ["Sophie Kearney", "Shu Yang", "Zixuan Wen", "Bojian Hou", "Duy Duong-Tran", "Tianlong Chen", "Jason Moore", "Marylyn Ritchie", "Li Shen"], "categories": ["cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Early and accurate diagnosis of Alzheimer's disease (AD), a complex\nneurodegenerative disorder, requires analysis of heterogeneous biomarkers\n(e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal\nfluid proteins) typically represented in a tabular format. With flexible\nfew-shot reasoning, multimodal integration, and natural-language-based\ninterpretability, large language models (LLMs) offer unprecedented\nopportunities for prediction with structured biomedical data. We propose a\nnovel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts\nTableGPT2, a multimodal tabular-specialized LLM originally developed for\nbusiness intelligence tasks, for AD diagnosis using structured biomarker data\nwith small sample sizes. Our approach constructs few-shot tabular prompts using\nin-context learning examples from structured biomedical data and finetunes\nTableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary\nclassification task of AD or cognitively normal (CN). The TAP-GPT framework\nharnesses the powerful tabular understanding ability of TableGPT2 and the\nencoded prior knowledge of LLMs to outperform more advanced general-purpose\nLLMs and a tabular foundation model (TFM) developed for prediction tasks. To\nour knowledge, this is the first application of LLMs to the prediction task\nusing tabular biomarker data, paving the way for future LLM-driven multi-agent\nframeworks in biomedical informatics."}
{"id": "2507.23247", "pdf": "https://arxiv.org/pdf/2507.23247.pdf", "abs": "https://arxiv.org/abs/2507.23247", "title": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication", "authors": ["Sneha Oram", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "There has been an increase in recent advancements in the explainability and\ndevelopment of personalized chatbots for mental health. However, the reasoning\naspects for explainability and dialogue discourse have not been explored\npreviously for mental health. Hence, we are investigating the pragmatic\nreasoning capability of large language models (LLMs) in this domain. We\nintroduce P-ReMe dataset, and propose a modified definition for the pragmatic\nphenomena of implicature (implied meaning) and presupposition (implicit\nassumption) in mental health. Following the definition, we formulate two tasks\nin implicature and one task in presupposition. To benchmark the dataset and the\npresented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and\nQwen. The results of the experiments suggest that Mistral and Qwen show\nsubstantial reasoning capabilities in the domain. In addition, we also propose\nStiPRompts to study the stigma around mental health with the state-of-the-art\nLLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings\nshow that Claude-3.5-haiku deals with the stigma more responsibly compared to\nthe other two LLMs."}
{"id": "2507.23248", "pdf": "https://arxiv.org/pdf/2507.23248.pdf", "abs": "https://arxiv.org/abs/2507.23248", "title": "Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis", "authors": ["Shimanto Bhowmik", "Tawsif Tashwar Dipto", "Md Sazzad Islam", "Sheryl Hsu", "Tahsin Reasat"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Bengali is an underrepresented language in NLP research. However, it remains\na challenge due to its unique linguistic structure and computational\nconstraints. In this work, we systematically investigate the challenges that\nhinder Bengali NLP performance by focusing on the absence of standardized\nevaluation benchmarks. We then evaluated 10 recent open source Large Language\nModels (LLMs) in 8 of the translated datasets and performed a comprehensive\nerror analysis to pinpoint their primary failure modes. Our findings reveal\nconsistent performance gaps for Bengali compared to English, particularly for\nsmaller models and specific model families like Mistral. We also identified\npromising robustness in certain architectures, such as DeepSeek, that maintain\nmore stable performance across languages. Our analysis reveals an inverse\nrelationship between tokenization efficiency and LLM accuracy where models tend\nto perform worse when inputs are excessively tokenized, whereas more efficient\n\\& concise tokenization results in improved performance. These findings\nhighlight critical areas where current models fall short and underscore the\nneed for improved dataset quality and evaluation methodologies tailored to\nmultilingual contexts. This work will catalyze further research on NLP for\nunderrepresented languages, helping to democratize access to advanced language\ntechnologies worldwide. The code and dataset used in this research is publicly\navailable at https://github.com/BengaliAI/bn-llm-benchmark."}
{"id": "2507.23279", "pdf": "https://arxiv.org/pdf/2507.23279.pdf", "abs": "https://arxiv.org/abs/2507.23279", "title": "Unveiling Super Experts in Mixture-of-Experts Large Language Models", "authors": ["Zunhai Su", "Qingyuan Li", "Hao Zhang", "YuLei Qian", "Yuchen Xie", "Kehong Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Sparsely activated Mixture-of-Experts (MoE) models have shown promise in\nenhancing the learning capacity of large language models (LLMs). Leveraging the\nintrinsic importance differences among experts, recent research has explored\nexpert-level compression techniques to improve the efficiency of MoE LLMs.\nHowever, existing approaches often rely on empirical criteria to identify\ncritical experts, lacking a deeper exploration and understanding of the\nheterogeneous importance of experts. In this study, we present the first\ndiscovery and investigation of a distinct subset of experts that play a crucial\nrole in the underlying mechanisms during the model's forward inference. These\nexperts are prevalent in open-source MoE LLMs, and despite their limited\nnumber, pruning them leads to a significant decline in model performance (e.g.,\npruning three causes Qwen3-30B-A3B to produce repetitive and uninformative\noutputs). We refer to these experts as Super Experts (SEs). Our comprehensive\nanalysis provides progressively deeper insights into SEs. (i) SEs are\ncharacterized by rare but extreme activation outliers in the output of the\ndown_proj, which give rise to massive activations in the hidden states between\ndecoder layers. Moreover, the distribution of SEs remains model-specific and is\nunaffected by post-training processes. (ii) By pruning SEs, we assess their\nsignificance across a variety of tasks, revealing their considerable impact on\nthe model's overall performance, particularly in mathematical reasoning. (iii)\nWe further enhance our understanding of the influence of SEs compression. Our\nfindings confirm that MoE LLMs rely on SEs to induce attention sinks, which are\ncrucial for the distribution of attention scores but are significantly\ndisrupted by SE pruning. The code is available at\nhttps://github.com/ZunhaiSu/Super-Experts-Profilling."}
{"id": "2507.23319", "pdf": "https://arxiv.org/pdf/2507.23319.pdf", "abs": "https://arxiv.org/abs/2507.23319", "title": "What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content", "authors": ["Alfio Ferrara", "Sergio Picascia", "Laura Pinnavaia", "Vojimir Ranitovic", "Elisabetta Rocchetti", "Alice Tuveri"], "categories": ["cs.CL"], "comment": null, "summary": "Proprietary Large Language Models (LLMs) have shown tendencies toward\npoliteness, formality, and implicit content moderation. While previous research\nhas primarily focused on explicitly training models to moderate and detoxify\nsensitive content, there has been limited exploration of whether LLMs\nimplicitly sanitize language without explicit instructions. This study\nempirically analyzes the implicit moderation behavior of GPT-4o-mini when\nparaphrasing sensitive content and evaluates the extent of sensitivity shifts.\nOur experiments indicate that GPT-4o-mini systematically moderates content\ntoward less sensitive classes, with substantial reductions in derogatory and\ntaboo language. Also, we evaluate the zero-shot capabilities of LLMs in\nclassifying sentence sensitivity, comparing their performances against\ntraditional methods."}
{"id": "2507.23334", "pdf": "https://arxiv.org/pdf/2507.23334.pdf", "abs": "https://arxiv.org/abs/2507.23334", "title": "MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation", "authors": ["Daeyong Kwon", "SeungHeon Doh", "Juhan Nam"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "8 pages, 2 figures", "summary": "Recent advancements in Large language models (LLMs) have demonstrated\nremarkable capabilities across diverse domains. While they exhibit strong\nzero-shot performance on various tasks, LLMs' effectiveness in music-related\napplications remains limited due to the relatively small proportion of\nmusic-specific knowledge in their training data. To address this limitation, we\npropose MusT-RAG, a comprehensive framework based on Retrieval Augmented\nGeneration (RAG) to adapt general-purpose LLMs for text-only music question\nanswering (MQA) tasks. RAG is a technique that provides external knowledge to\nLLMs by retrieving relevant context information when generating answers to\nquestions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a\nmusic-specialized vector database for the retrieval stage, and (2) utilizes\ncontext information during both inference and fine-tuning processes to\neffectively transform general-purpose LLMs into music-specific models. Our\nexperiment demonstrates that MusT-RAG significantly outperforms traditional\nfine-tuning approaches in enhancing LLMs' music domain adaptation capabilities,\nshowing consistent improvements across both in-domain and out-of-domain MQA\nbenchmarks. Additionally, our MusWikiDB proves substantially more effective\nthan general Wikipedia corpora, delivering superior performance and\ncomputational efficiency."}
{"id": "2507.23358", "pdf": "https://arxiv.org/pdf/2507.23358.pdf", "abs": "https://arxiv.org/abs/2507.23358", "title": "Text-to-SQL Task-oriented Dialogue Ontology Construction", "authors": ["Renato Vukovic", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Hsien-Chin Lin", "Shutong Feng", "Nurul Lubis", "Milica Gasic"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability."}
{"id": "2507.23382", "pdf": "https://arxiv.org/pdf/2507.23382.pdf", "abs": "https://arxiv.org/abs/2507.23382", "title": "MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models", "authors": ["Yiyan Ji", "Haoran Chen", "Qiguang Chen", "Chengyue Wu", "Libo Qin", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.8; I.2.10"], "comment": "Accepted to ACM Multimedia 2025", "summary": "Multimodal planning capabilities refer to the ability to predict, reason, and\ndesign steps for task execution with multimodal context, which is essential for\ncomplex reasoning and decision-making across multiple steps. However, current\nbenchmarks face two key challenges: (1) they cannot directly assess multimodal\nreal-world planning capabilities, and (2) they lack constraints or implicit\nconstraints across modalities. To address these issues, we introduce Multimodal\nPlanning with Complex Constraints (MPCC), the first benchmark to systematically\nevaluate MLLMs' ability to handle multimodal constraints in planning. To\naddress the first challenge, MPCC focuses on three real-world tasks: Flight\nPlanning, Calendar Planning, and Meeting Planning. To solve the second\nchallenge, we introduce complex constraints (e.g. budget, temporal, and\nspatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to\nseparate constraint complexity from search space expansion. Experiments on 13\nadvanced MLLMs reveal significant challenges: closed-source models achieve only\n21.3% feasible plans, while open-source models average below 11%. Additionally,\nwe observe that MLLMs are highly sensitive to constraint complexity and that\ntraditional multimodal prompting strategies fail in multi-constraint scenarios.\nOur work formalizes multimodal constraints in planning, provides a rigorous\nevaluation framework, and highlights the need for advancements in\nconstraint-aware reasoning for real-world MLLM applications."}
{"id": "2507.23386", "pdf": "https://arxiv.org/pdf/2507.23386.pdf", "abs": "https://arxiv.org/abs/2507.23386", "title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models", "authors": ["Ailiang Lin", "Zhuoyun Li", "Kotaro Funakoshi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods."}
{"id": "2507.23399", "pdf": "https://arxiv.org/pdf/2507.23399.pdf", "abs": "https://arxiv.org/abs/2507.23399", "title": "Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators", "authors": ["Peter Sandrini"], "categories": ["cs.CL", "cs.CY", "I.2.7; K.4.3"], "comment": null, "summary": "The rapid proliferation of Large Language Models presents both opportunities\nand challenges for the translation field. While commercial, cloud-based AI\nchatbots have garnered significant attention in translation studies, concerns\nregarding data privacy, security, and equitable access necessitate exploration\nof alternative deployment models. This paper investigates the feasibility and\nperformance of locally deployable, free language models as a viable alternative\nto proprietary, cloud-based AI solutions. This study evaluates three\nopen-source models installed on CPU-based platforms and compared against\ncommercially available online chat-bots. The evaluation focuses on functional\nperformance rather than a comparative analysis of human-machine translation\nquality, an area already subject to extensive research. The platforms assessed\nwere chosen for their accessibility and ease of use across various operating\nsystems. While local deployment introduces its own challenges, the benefits of\nenhanced data control, improved privacy, and reduced dependency on cloud\nservices are compelling. The findings of this study contribute to a growing\nbody of knowledge concerning the democratization of AI technology and inform\nfuture research and development efforts aimed at making LLMs more accessible\nand practical for a wider range of users, specifically focusing on the needs of\nindividual translators and small businesses."}
{"id": "2507.23400", "pdf": "https://arxiv.org/pdf/2507.23400.pdf", "abs": "https://arxiv.org/abs/2507.23400", "title": "MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization", "authors": ["Yongbing Zhang", "Fang Nan", "Shengxiang Gao", "Yuxin Huang", "Kaiwen Tan", "Zhengtao Yu"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The core challenge faced by multi-document summarization is the complexity of\nrelationships among documents and the presence of information redundancy. Graph\nclustering is an effective paradigm for addressing this issue, as it models the\ncomplex relationships among documents using graph structures and reduces\ninformation redundancy through clustering, achieving significant research\nprogress. However, existing methods often only consider single-relational\ngraphs and require a predefined number of clusters, which hinders their ability\nto fully represent rich relational information and adaptively partition\nsentence groups to reduce redundancy. To overcome these limitations, we propose\nMRGSEM-Sum, an unsupervised multi-document summarization framework based on\nmulti-relational graphs and structural entropy minimization. Specifically, we\nconstruct a multi-relational graph that integrates semantic and discourse\nrelations between sentences, comprehensively modeling the intricate and dynamic\nconnections among sentences across documents. We then apply a two-dimensional\nstructural entropy minimization algorithm for clustering, automatically\ndetermining the optimal number of clusters and effectively organizing sentences\ninto coherent groups. Finally, we introduce a position-aware compression\nmechanism to distill each cluster, generating concise and informative\nsummaries. Extensive experiments on four benchmark datasets (Multi-News,\nDUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently\noutperforms previous unsupervised methods and, in several cases, achieves\nperformance comparable to supervised models and large language models. Human\nevaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high\nconsistency and coverage, approaching human-level quality."}
{"id": "2507.23404", "pdf": "https://arxiv.org/pdf/2507.23404.pdf", "abs": "https://arxiv.org/abs/2507.23404", "title": "Enhanced Arabic Text Retrieval with Attentive Relevance Scoring", "authors": ["Salah Eddine Bekhouche", "Azeddine Benlamoudi", "Yazid Bounab", "Fadi Dornaika", "Abdenour Hadid"], "categories": ["cs.CL"], "comment": null, "summary": "Arabic poses a particular challenge for natural language processing (NLP) and\ninformation retrieval (IR) due to its complex morphology, optional diacritics\nand the coexistence of Modern Standard Arabic (MSA) and various dialects.\nDespite the growing global significance of Arabic, it is still underrepresented\nin NLP research and benchmark resources. In this paper, we present an enhanced\nDense Passage Retrieval (DPR) framework developed specifically for Arabic. At\nthe core of our approach is a novel Attentive Relevance Scoring (ARS) that\nreplaces standard interaction mechanisms with an adaptive scoring function that\nmore effectively models the semantic relevance between questions and passages.\nOur method integrates pre-trained Arabic language models and architectural\nrefinements to improve retrieval performance and significantly increase ranking\naccuracy when answering Arabic questions. The code is made publicly available\nat \\href{https://github.com/Bekhouche/APR}{GitHub}."}
{"id": "2507.23407", "pdf": "https://arxiv.org/pdf/2507.23407.pdf", "abs": "https://arxiv.org/abs/2507.23407", "title": "Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration", "authors": ["Ante Wang", "Yujie Lin", "Jingyao Liu", "Suhang Wu", "Hao Liu", "Xinyan Xiao", "Jinsong Su"], "categories": ["cs.CL"], "comment": null, "summary": "Critical thinking is essential for building robust AI systems, preventing\nthem from blindly accepting flawed data or biased reasoning. However, prior\nwork has primarily focused on passive critical thinking, where models simply\nreject problematic queries without taking constructive steps to address user\nrequests. In this work, we introduce proactive critical thinking, a paradigm\nwhere models actively seek missing or clarifying information from users to\nresolve their queries better. To evaluate this capability, we present GSM-MC\nand GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical\nreasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math\nproblems with a key variable deliberately removed, requiring models to identify\nand request the missing information. GSM-MCE further increases the difficulty\nby introducing irrelevant details to test robustness against distractions.\nExperiments on Qwen3 and Llama series models show that, while these models\nexcel in traditional reasoning tasks due to extensive post-training and\ninference-time scaling, they struggle with proactive critical thinking,\nespecially smaller ones. However, we demonstrate that reinforcement learning\n(RL) can significantly improve this ability. Using our enhanced RL algorithm,\nwe achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to\n73.98% on GSM-MC. We hope this work advances models that collaborate more\neffectively with users in problem-solving through proactive critical thinking."}
{"id": "2507.23465", "pdf": "https://arxiv.org/pdf/2507.23465.pdf", "abs": "https://arxiv.org/abs/2507.23465", "title": "Role-Aware Language Models for Secure and Contextualized Access Control in Organizations", "authors": ["Saeed Almheiri", "Yerulan Kongrat", "Adrian Santosh", "Ruslan Tasmukhanov", "Josemaria Vera", "Muhammad Dehan Al Kautsar", "Fajri Koto"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in enterprise\nsettings, controlling model behavior based on user roles becomes an essential\nrequirement. Existing safety methods typically assume uniform access and focus\non preventing harmful or toxic outputs, without addressing role-specific access\nconstraints. In this work, we investigate whether LLMs can be fine-tuned to\ngenerate responses that reflect the access privileges associated with different\norganizational roles. We explore three modeling strategies: a BERT-based\nclassifier, an LLM-based classifier, and role-conditioned generation. To\nevaluate these approaches, we construct two complementary datasets. The first\nis adapted from existing instruction-tuning corpora through clustering and role\nlabeling, while the second is synthetically generated to reflect realistic,\nrole-sensitive enterprise scenarios. We assess model performance across varying\norganizational structures and analyze robustness to prompt injection, role\nmismatch, and jailbreak attempts."}
{"id": "2507.23486", "pdf": "https://arxiv.org/pdf/2507.23486.pdf", "abs": "https://arxiv.org/abs/2507.23486", "title": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains", "authors": ["Shirui Wang", "Zhihui Tang", "Huaxia Yang", "Qiuhong Gong", "Tiantian Gu", "Hongyang Ma", "Yongxin Wang", "Wubin Sun", "Zeliang Lian", "Kehang Mao", "Yinan Jiang", "Zhicheng Huang", "Lingyun Ma", "Wenjie Shen", "Yajie Ji", "Yunhui Tan", "Chunbo Wang", "Yunlu Gao", "Qianling Ye", "Rui Lin", "Mingyu Chen", "Lijuan Niu", "Zhihao Wang", "Peng Yu", "Mengran Lang", "Yue Liu", "Huimin Zhang", "Haitao Shen", "Long Chen", "Qiguang Zhao", "Si-Xuan Liu", "Lina Zhou", "Hua Gao", "Dongqiang Ye", "Lingmin Meng", "Youtao Yu", "Naixin Liang", "Jianxiong Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments."}
{"id": "2507.23541", "pdf": "https://arxiv.org/pdf/2507.23541.pdf", "abs": "https://arxiv.org/abs/2507.23541", "title": "Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning", "authors": ["Keer Lu", "Zheng Liang", "Youquan Li", "Jiejun Tan", "Da Pan", "Shusen Zhang", "Guosheng Dong", "Huang Leng"], "categories": ["cs.CL"], "comment": null, "summary": "In medical scenarios, effectively retrieving external knowledge and\nleveraging it for rigorous logical reasoning is of significant importance.\nDespite their potential, existing work has predominantly focused on enhancing\neither retrieval or reasoning capabilities of the models in isolation, with\nlittle attention given to their joint optimization, which leads to limited\ncoordination between the two processes. Additionally, current methods rely\nheavily on supervised fine-tuning (SFT), which can cause models to memorize\nexisting problem-solving pathways, thereby restricting their generalization\nability when confronted with novel problem contexts. Furthermore, while some\nstudies have explored to improve retrieval-augmented reasoning in general\ndomains via reinforcement learning, their reward function designs do not\nadequately capture the specific demands of the medical domain. To address these\nchallenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented\n**R**easoning framework driven by progressive **R**einforcement learning. In\nthis framework, we first develop the model's ability to perform logical\nreasoning over medical problems. Subsequently, on the basis of this foundation,\nwe adaptively optimize the retrieval capability to better align with the\ncharacteristics of knowledge corpus and external information utilization\nthroughout the reasoning process. Finally, we conduct joint optimization of the\nmodel's retrieval and reasoning coordination. Extensive experiments indicate\nthat **Med-R$^3$** could achieve state-of-the-art performances, with\nLLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by\n3.93\\% at a comparable parameter scale, while Qwen2.5-14B augmented with\nMed-R$^3$ shows a more substantial gain of 13.53\\%."}
{"id": "2507.23577", "pdf": "https://arxiv.org/pdf/2507.23577.pdf", "abs": "https://arxiv.org/abs/2507.23577", "title": "T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text", "authors": ["Alva West", "Luodan Zhang", "Liuliu Zhang", "Minjun Zhu", "Yixuan Weng", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of sophisticated text generation models necessitates the\ndevelopment of robust detection methods capable of identifying\nmachine-generated content, particularly text designed to evade detection\nthrough adversarial perturbations. Existing zero-shot detectors often rely on\nstatistical measures that implicitly assume Gaussian distributions, a premise\nthat falters when confronted with the heavy-tailed statistical artifacts\ncharacteristic of adversarial or non-native English texts. This paper\nintroduces T-Detect, a novel detection method that fundamentally redesigns the\nstatistical core of curvature-based detectors. Our primary innovation is the\nreplacement of standard Gaussian normalization with a heavy-tailed discrepancy\nscore derived from the Student's t-distribution. This approach is theoretically\ngrounded in the empirical observation that adversarial texts exhibit\nsignificant leptokurtosis, rendering traditional statistical assumptions\ninadequate. T-Detect computes a detection score by normalizing the\nlog-likelihood of a passage against the expected moments of a t-distribution,\nproviding superior resilience to statistical outliers. We validate our approach\non the challenging RAID benchmark for adversarial text and the comprehensive\nHART dataset. Experiments show that T-Detect provides a consistent performance\nuplift over strong baselines, improving AUROC by up to 3.9\\% in targeted\ndomains. When integrated into a two-dimensional detection framework (CT), our\nmethod achieves state-of-the-art performance, with an AUROC of 0.926 on the\nBooks domain of RAID. Our contributions are a new, theoretically-justified\nstatistical foundation for text detection, an ablation-validated method that\ndemonstrates superior robustness, and a comprehensive analysis of its\nperformance under adversarial conditions. Ours code are released at\nhttps://github.com/ResearAI/t-detect."}
{"id": "2507.23588", "pdf": "https://arxiv.org/pdf/2507.23588.pdf", "abs": "https://arxiv.org/abs/2507.23588", "title": "DiffLoRA: Differential Low-Rank Adapters for Large Language Models", "authors": ["Alexandre Misrahi", "Nadezhda Chirkova", "Maxime Louis", "Vassilina Nikoulina"], "categories": ["cs.CL"], "comment": null, "summary": "Differential Transformer has recently been proposed to improve performance in\nTransformer models by canceling out noise through a denoiser attention\nmechanism. In this work, we introduce DiffLoRA, a parameter-efficient\nadaptation of the differential attention mechanism, with low-rank adapters on\nboth positive and negative attention terms. This approach retains the\nefficiency of LoRA while aiming to benefit from the performance gains of\ndifferential attention. We evaluate DiffLoRA across a broad range of NLP tasks,\nincluding general benchmarks, many-shot in-context learning, RAG, and\nlong-context tests. We observe that, although DiffLoRA falls short of other\nparameter-efficient fine-tuning methods in most evaluation tasks, it shows\ninteresting results in certain domains (+11 pts on LoRA for HumanEval). We\nanalyze the attention patterns post-finetuning to identify the reasons for this\nbehavior."}
{"id": "2507.23661", "pdf": "https://arxiv.org/pdf/2507.23661.pdf", "abs": "https://arxiv.org/abs/2507.23661", "title": "Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning", "authors": ["Salam Thabet Doghmash", "Motaz Saad"], "categories": ["cs.CL", "I.2.7"], "comment": "23 pages, 5 figures", "summary": "Hate speech identification in social media has become an increasingly\nimportant issue in recent years. In this research, we address two problems: 1)\nto detect hate speech in Arabic text, 2) to clean a given text from hate\nspeech. The meaning of cleaning here is replacing each bad word with stars\nbased on the number of letters for each word. Regarding the first problem, we\nconduct several experiments using deep learning models and transformers to\ndetermine the best model in terms of the F1 score. Regarding second problem, we\nconsider it as a machine translation task, where the input is a sentence\ncontaining dirty text and the output is the same sentence with masking the\ndirty text. The presented methods achieve the best model in hate speech\ndetection with a 92\\% Macro F1 score and 95\\% accuracy. Regarding the text\ncleaning experiment, the best result in the hate speech masking model reached\n0.3 in BLEU score with 1-gram, which is a good result compared with the state\nof the art machine translation systems."}
{"id": "2507.23740", "pdf": "https://arxiv.org/pdf/2507.23740.pdf", "abs": "https://arxiv.org/abs/2507.23740", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "authors": ["Nasim Shirvani-Mahdavi", "Devin Wingfield", "Amin Ghasemi", "Chengkai Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Knowledge graphs (KGs) often contain sufficient information to support the\ninference of new facts. Identifying logical rules not only improves the\ncompleteness of a knowledge graph but also enables the detection of potential\nerrors, reveals subtle data patterns, and enhances the overall capacity for\nreasoning and interpretation. However, the complexity of such rules, combined\nwith the unique labeling conventions of each KG, can make them difficult for\nhumans to understand. In this paper, we explore the potential of large language\nmodels to generate natural language explanations for logical rules.\nSpecifically, we extract logical rules using the AMIE 3.5.1 rule discovery\nalgorithm from the benchmark dataset FB15k-237 and two large-scale datasets,\nFB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including\nzero- and few-shot prompting, including variable entity types, and\nchain-of-thought reasoning. We conduct a comprehensive human evaluation of the\ngenerated explanations based on correctness, clarity, and hallucination, and\nalso assess the use of large language models as automatic judges. Our results\ndemonstrate promising performance in terms of explanation correctness and\nclarity, although several challenges remain for future research. All scripts\nand data used in this study are publicly available at\nhttps://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL."}
{"id": "2507.23776", "pdf": "https://arxiv.org/pdf/2507.23776.pdf", "abs": "https://arxiv.org/abs/2507.23776", "title": "Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities", "authors": ["Yunxiang Yan", "Tomohiro Sawada", "Kartik Goyal"], "categories": ["cs.CL"], "comment": "Under review", "summary": "While question-answering~(QA) benchmark performance is an automatic and\nscalable method to compare LLMs, it is an indirect method of evaluating their\nunderlying problem-solving capabilities. Therefore, we propose a holistic and\ngeneralizable framework based on \\emph{cascaded question disclosure} that\nprovides a more accurate estimate of the models' problem-solving capabilities\nwhile maintaining the scalability and automation. This approach collects model\nresponses in a stagewise manner with each stage revealing partial information\nabout the question designed to elicit generalized reasoning in LLMs. We find\nthat our approach not only provides a better comparison between LLMs, but also\ninduces better intermediate traces in models compared to the standard QA\nparadigm. We empirically verify this behavior on diverse reasoning and\nknowledge-heavy QA datasets by comparing LLMs of varying sizes and families.\nOur approach narrows the performance gap observed in the standard QA evaluation\nsettings, indicating that the prevalent indirect QA paradigm of evaluation\noverestimates the differences in performance between models. We further\nvalidate our findings by extensive ablation studies."}
{"id": "2507.22892", "pdf": "https://arxiv.org/pdf/2507.22892.pdf", "abs": "https://arxiv.org/abs/2507.22892", "title": "Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation", "authors": ["Ismail Hossain", "Mridul Banik"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Conventional augmentative and alternative communication (AAC) systems and\nlanguage-learning platforms often fail to adapt in real time to the user's\ncognitive and linguistic needs, especially in neurological conditions such as\npost-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in\nnoninvasive electroencephalography (EEG)--based brain-computer interfaces\n(BCIs) and transformer--based large language models (LLMs) offer complementary\nstrengths: BCIs capture users' neural intent with low fatigue, while LLMs\ngenerate contextually tailored language content. We propose and evaluate a\nnovel hybrid framework that leverages real-time EEG signals to drive an\nLLM-powered language rehabilitation assistant. This system aims to: (1) enable\nusers with severe speech or motor impairments to navigate language-learning\nmodules via mental commands; (2) dynamically personalize vocabulary,\nsentence-construction exercises, and corrective feedback; and (3) monitor\nneural markers of cognitive effort to adjust task difficulty on the fly."}
{"id": "2507.22898", "pdf": "https://arxiv.org/pdf/2507.22898.pdf", "abs": "https://arxiv.org/abs/2507.22898", "title": "Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment", "authors": ["Julian Acosta", "Scott Adams", "Julius Kernbach", "Romain Hardy", "Sung Eun Kim", "Luyang Luo", "Xiaoman Zhang", "Shreya Johri", "Mohammed Baharoon", "Pranav Rajpurkar"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "We developed a voice-driven artificial intelligence (AI) system that guides\nanyone - from paramedics to family members - through expert-level stroke\nevaluations using natural conversation, while also enabling smartphone video\ncapture of key examination components for documentation and potential expert\nreview. This addresses a critical gap in emergency care: current stroke\nrecognition by first responders is inconsistent and often inaccurate, with\nsensitivity for stroke detection as low as 58%, causing life-threatening delays\nin treatment. Three non-medical volunteers used our AI system to assess ten\nsimulated stroke patients, including cases with likely large vessel occlusion\n(LVO) strokes and stroke-like conditions, while we measured diagnostic\naccuracy, completion times, user confidence, and expert physician review of the\nAI-generated reports. The AI system correctly identified 84% of individual\nstroke signs and detected 75% of likely LVOs, completing evaluations in just\nover 6 minutes. Users reported high confidence (median 4.5/5) and ease of use\n(mean 4.67/5). The system successfully identified 86% of actual strokes but\nalso incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert\nphysician reviewed the AI reports with videos, they identified the correct\ndiagnosis in 100% of cases, but felt confident enough to make preliminary\ntreatment decisions in only 40% of cases due to observed AI errors including\nincorrect scoring and false information. While the current system's limitations\nnecessitate human oversight, ongoing rapid advancements in speech-to-speech AI\nmodels suggest that future versions are poised to enable highly accurate\nassessments. Achieving human-level voice interaction could transform emergency\nmedical care, putting expert-informed assessment capabilities in everyone's\nhands."}
{"id": "2507.22902", "pdf": "https://arxiv.org/pdf/2507.22902.pdf", "abs": "https://arxiv.org/abs/2507.22902", "title": "Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting", "authors": ["Hashim Hayat", "Maksim Kudrautsau", "Evgeniy Makarov", "Vlad Melnichenko", "Tim Tsykunou", "Piotr Varaksin", "Matt Pavelle", "Adam Z. Oskowitz"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Background: Globally we face a projected shortage of 11 million healthcare\npractitioners by 2030, and administrative burden consumes 50% of clinical time.\nArtificial intelligence (AI) has the potential to help alleviate these\nproblems. However, no end-to-end autonomous large language model (LLM)-based AI\nsystem has been rigorously evaluated in real-world clinical practice. In this\nstudy, we evaluated whether a multi-agent LLM-based AI framework can function\nautonomously as an AI doctor in a virtual urgent care setting. Methods: We\nretrospectively compared the performance of the multi-agent AI system Doctronic\nand board-certified clinicians across 500 consecutive urgent-care telehealth\nencounters. The primary end points: diagnostic concordance, treatment plan\nconsistency, and safety metrics, were assessed by blinded LLM-based\nadjudication and expert human review. Results: The top diagnosis of Doctronic\nand clinician matched in 81% of cases, and the treatment plan aligned in 99.2%\nof cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not\nsupported by clinical findings). In an expert review of discordant cases, AI\nperformance was superior in 36.1%, and human performance was superior in 9.3%;\nthe diagnoses were equivalent in the remaining cases. Conclusions: In this\nfirst large-scale validation of an autonomous AI doctor, we demonstrated strong\ndiagnostic and treatment plan concordance with human clinicians, with AI\nperformance matching and in some cases exceeding that of practicing clinicians.\nThese findings indicate that multi-agent AI systems achieve comparable clinical\ndecision-making to human providers and offer a potential solution to healthcare\nworkforce shortages."}
{"id": "2507.22947", "pdf": "https://arxiv.org/pdf/2507.22947.pdf", "abs": "https://arxiv.org/abs/2507.22947", "title": "ELMES: An Automated Framework for Evaluating Large Language Models in Educational Scenarios", "authors": ["Shou'ang Wei", "Xinyun Wang", "Shuzhen Bi", "Jian Chen", "Ruijia Li", "Bo Jiang", "Xin Lin", "Min Zhang", "Yu Song", "BingDong Li", "Aimin Zhou", "Hao Hao"], "categories": ["cs.CY", "cs.CL", "cs.LG"], "comment": null, "summary": "The emergence of Large Language Models (LLMs) presents transformative\nopportunities for education, generating numerous novel application scenarios.\nHowever, significant challenges remain: evaluation metrics vary substantially\nacross different educational scenarios, while many emerging scenarios lack\nappropriate assessment metrics. Current benchmarks predominantly measure\ngeneral intelligence rather than pedagogical capabilities. To address this gap,\nwe introduce ELMES, an open-source automated evaluation framework specifically\ndesigned for assessing LLMs in educational settings. ELMES features a modular\narchitecture that enables researchers to create dynamic, multi-agent dialogues\nthrough simple configuration files, facilitating flexible scenario design\nwithout requiring extensive programming expertise. The framework incorporates a\nhybrid evaluation engine that objectively quantifies traditionally subjective\npedagogical metrics using an LLM-as-a-Judge methodology. We conduct systematic\nbenchmarking of state-of-the-art LLMs across four critical educational\nscenarios: Knowledge Point Explanation, Guided Problem-Solving Teaching,\nInterdisciplinary Lesson Plan Generation, and Contextualized Question\nGeneration, employing fine-grained metrics developed in collaboration with\neducation specialists. Our results demonstrate distinct capability\ndistributions among models, revealing context-specific strengths and\nlimitations. ELMES provides educators and researchers with an accessible\nevaluation framework that significantly reduces adaptation barriers for diverse\neducational applications while advancing the practical implementation of LLMs\nin pedagogy. The framework is publicly available at\n\\emph{https://github.com/sii-research/elmes.git}."}
{"id": "2507.22964", "pdf": "https://arxiv.org/pdf/2507.22964.pdf", "abs": "https://arxiv.org/abs/2507.22964", "title": "Exploring Dynamic Parameters for Vietnamese Gender-Independent ASR", "authors": ["Sotheara Leang", "Éric Castelli", "Dominique Vaufreydaz", "Sethserey Sam"], "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "comment": null, "summary": "The dynamic characteristics of speech signal provides temporal information\nand play an important role in enhancing Automatic Speech Recognition (ASR). In\nthis work, we characterized the acoustic transitions in a ratio plane of\nSpectral Subband Centroid Frequencies (SSCFs) using polar parameters to capture\nthe dynamic characteristics of the speech and minimize spectral variation.\nThese dynamic parameters were combined with Mel-Frequency Cepstral Coefficients\n(MFCCs) in Vietnamese ASR to capture more detailed spectral information. The\nSSCF0 was used as a pseudo-feature for the fundamental frequency (F0) to\ndescribe the tonal information robustly. The findings showed that the proposed\nparameters significantly reduce word error rates and exhibit greater gender\nindependence than the baseline MFCCs."}
{"id": "2507.23242", "pdf": "https://arxiv.org/pdf/2507.23242.pdf", "abs": "https://arxiv.org/abs/2507.23242", "title": "Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents", "authors": ["Sungguk Cha", "DongWook Kim", "Taeseung Hahn", "Mintae Kim", "Youngsub Han", "Byoung-Ki Jeon"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems rely heavily on effective query\nformulation to unlock external knowledge, yet optimizing queries for diverse,\nunstructured real-world documents remains a challenge. We introduce\n\\textbf{RL-QR}, a reinforcement learning framework for retriever-specific query\nrewriting that eliminates the need for human-annotated datasets and extends\napplicability to both text-only and multi-modal databases. By synthesizing\nscenario-question pairs and leveraging Generalized Reward Policy Optimization\n(GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing\nretrieval performance across varied domains. Experiments on industrial in-house\ndata demonstrate significant improvements, with\n$\\text{RL-QR}_{\\text{multi-modal}}$ achieving an 11\\% relative gain in NDCG@3\nfor multi-modal RAG and $\\text{RL-QR}_{\\text{lexical}}$ yielding a 9\\% gain for\nlexical retrievers. However, challenges persist with semantic and hybrid\nretrievers, where rewriters failed to improve performance, likely due to\ntraining misalignments. Our findings highlight RL-QR's potential to\nrevolutionize query optimization for RAG systems, offering a scalable,\nannotation-free solution for real-world retrieval tasks, while identifying\navenues for further refinement in semantic retrieval contexts."}
{"id": "2507.23292", "pdf": "https://arxiv.org/pdf/2507.23292.pdf", "abs": "https://arxiv.org/abs/2507.23292", "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy", "authors": ["RJ Skerry-Ryan", "Julian Salazar", "Soroosh Mariooryad", "David Kao", "Daisy Stanton", "Eric Battenberg", "Matt Shannon", "Ron J. Weiss", "Robin Scheibler", "Jonas Rothfuss", "Tom Bagby"], "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE", "eess.AS"], "comment": null, "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers."}
{"id": "2507.23336", "pdf": "https://arxiv.org/pdf/2507.23336.pdf", "abs": "https://arxiv.org/abs/2507.23336", "title": "DSBC : Data Science task Benchmarking with Context engineering", "authors": ["Ram Mohan Rao Kadiyala", "Siddhant Gupta", "Jebish Purbey", "Giulio Martini", "Suman Debnath", "Hamza Farooq"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "32 pages", "summary": "Recent advances in large language models (LLMs) have significantly impacted\ndata science workflows, giving rise to specialized data science agents designed\nto automate analytical tasks. Despite rapid adoption, systematic benchmarks\nevaluating the efficacy and limitations of these agents remain scarce. In this\npaper, we introduce a comprehensive benchmark specifically crafted to reflect\nreal-world user interactions with data science agents by observing usage of our\ncommercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,\nGemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with\ncontext engineering, multi-step with context engineering, and with SmolAgent.\nOur benchmark assesses performance across a diverse set of eight data science\ntask categories, additionally exploring the sensitivity of models to common\nprompting issues, such as data leakage and slightly ambiguous instructions. We\nfurther investigate the influence of temperature parameters on overall and\ntask-specific outcomes for each model and approach. Our findings reveal\ndistinct performance disparities among the evaluated models and methodologies,\nhighlighting critical factors that affect practical deployment. The benchmark\ndataset and evaluation framework introduced herein aim to provide a foundation\nfor future research of more robust and effective data science agents."}
{"id": "2507.23348", "pdf": "https://arxiv.org/pdf/2507.23348.pdf", "abs": "https://arxiv.org/abs/2507.23348", "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution", "authors": ["Han Li", "Yuling Shi", "Shaoxin Lin", "Xiaodong Gu", "Heng Lian", "Xin Wang", "Yantao Jia", "Tao Huang", "Qianxiang Wang"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Debate", "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin."}
{"id": "2507.23361", "pdf": "https://arxiv.org/pdf/2507.23361.pdf", "abs": "https://arxiv.org/abs/2507.23361", "title": "SWE-Exp: Experience-Driven Software Issue Resolution", "authors": ["Silin Chen", "Shaoxin Lin", "Xiaodong Gu", "Yuling Shi", "Heng Lian", "Longfei Yun", "Dong Chen", "Weiguo Sun", "Lin Cao", "Qianxiang Wang"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp", "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution."}
{"id": "2507.23364", "pdf": "https://arxiv.org/pdf/2507.23364.pdf", "abs": "https://arxiv.org/abs/2507.23364", "title": "Holistic Evaluations of Topic Models", "authors": ["Thomas Compton"], "categories": ["cs.IR", "cs.CL"], "comment": "10 pages, 6 tables", "summary": "Topic models are gaining increasing commercial and academic interest for\ntheir ability to summarize large volumes of unstructured text. As unsupervised\nmachine learning methods, they enable researchers to explore data and help\ngeneral users understand key themes in large text collections. However, they\nrisk becoming a 'black box', where users input data and accept the output as an\naccurate summary without scrutiny. This article evaluates topic models from a\ndatabase perspective, drawing insights from 1140 BERTopic model runs. The goal\nis to identify trade-offs in optimizing model parameters and to reflect on what\nthese findings mean for the interpretation and responsible use of topic models"}
{"id": "2507.23453", "pdf": "https://arxiv.org/pdf/2507.23453.pdf", "abs": "https://arxiv.org/abs/2507.23453", "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems", "authors": ["Lijia Liu", "Takumi Kondo", "Kyohei Atarashi", "Koh Takeuchi", "Jiyi Li", "Shigeru Saito", "Hisashi Kashima"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs."}
{"id": "2507.23511", "pdf": "https://arxiv.org/pdf/2507.23511.pdf", "abs": "https://arxiv.org/abs/2507.23511", "title": "MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks", "authors": ["Yadong Niu", "Tianzi Wang", "Heinrich Dinkel", "Xingwei Sun", "Jiahao Zhou", "Gang Li", "Jizhong Liu", "Xunying Liu", "Junbo Zhang", "Jian Luan"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "9 main pages, 5 figures, 3 tables, and 14 appendix pages", "summary": "While large audio-language models have advanced open-ended audio\nunderstanding, they still fall short of nuanced human-level comprehension. This\ngap persists largely because current benchmarks, limited by data annotations\nand evaluation metrics, fail to reliably distinguish between generic and highly\ndetailed model outputs. To this end, this work introduces MECAT, a Multi-Expert\nConstructed Benchmark for Fine-Grained Audio Understanding Tasks. Generated via\na pipeline that integrates analysis from specialized expert models with\nChain-of-Thought large language model reasoning, MECAT provides\nmulti-perspective, fine-grained captions and open-set question-answering pairs.\nThe benchmark is complemented by a novel metric: DATE (Discriminative-Enhanced\nAudio Text Evaluation). This metric penalizes generic terms and rewards\ndetailed descriptions by combining single-sample semantic similarity with\ncross-sample discriminability. A comprehensive evaluation of state-of-the-art\naudio models is also presented, providing new insights into their current\ncapabilities and limitations. The data and code are available at\nhttps://github.com/xiaomi-research/mecat"}
{"id": "2507.23607", "pdf": "https://arxiv.org/pdf/2507.23607.pdf", "abs": "https://arxiv.org/abs/2507.23607", "title": "Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates", "authors": ["Tien Huu Do", "Antoine Masquelier", "Nae Eoun Lee", "Jonathan Crowther"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Clinical trials are a systematic endeavor to assess the safety and efficacy\nof new drugs or treatments. Conducting such trials typically demands\nsignificant financial investment and meticulous planning, highlighting the need\nfor accurate predictions of trial outcomes. Accurately predicting patient\nenrollment, a key factor in trial success, is one of the primary challenges\nduring the planning phase. In this work, we propose a novel deep learning-based\nmethod to address this critical challenge. Our method, implemented as a neural\nnetwork model, leverages pre-trained language models (PLMs) to capture the\ncomplexities and nuances of clinical documents, transforming them into\nexpressive representations. These representations are then combined with\nencoded tabular features via an attention mechanism. To account for\nuncertainties in enrollment prediction, we enhance the model with a\nprobabilistic layer based on the Gamma distribution, which enables range\nestimation. We apply the proposed model to predict clinical trial duration,\nassuming site-level enrollment follows a Poisson-Gamma process. We carry out\nextensive experiments on real-world clinical trial data, and show that the\nproposed method can effectively predict the number of patients enrolled at a\nnumber of sites for a given clinical trial, outperforming established baseline\nmodels."}
{"id": "2507.23674", "pdf": "https://arxiv.org/pdf/2507.23674.pdf", "abs": "https://arxiv.org/abs/2507.23674", "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses", "authors": ["Muhammad Taha Cheema", "Abeer Aamir", "Khawaja Gul Muhammad", "Naveed Anwar Bhatti", "Ihsan Ayyub Qazi", "Zafar Ayyub Qazi"], "categories": ["cs.LG", "cs.CL"], "comment": "13 pages, 9 figures", "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."}
{"id": "2507.23701", "pdf": "https://arxiv.org/pdf/2507.23701.pdf", "abs": "https://arxiv.org/abs/2507.23701", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "authors": ["Long Phan", "Mantas Mazeika", "Andy Zou", "Dan Hendrycks"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai."}
{"id": "2507.23726", "pdf": "https://arxiv.org/pdf/2507.23726.pdf", "abs": "https://arxiv.org/abs/2507.23726", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "authors": ["Luoxin Chen", "Jinming Gu", "Liankai Huang", "Wenhao Huang", "Zhicheng Jiang", "Allan Jie", "Xiaoran Jin", "Xing Jin", "Chenggang Li", "Kaijing Ma", "Cheng Ren", "Jiawei Shen", "Wenlei Shi", "Tong Sun", "He Sun", "Jiahui Wang", "Siran Wang", "Zhihong Wang", "Chenrui Wei", "Shufa Wei", "Yonghui Wu", "Yuchen Wu", "Yihang Xia", "Huajian Xin", "Fan Yang", "Huaiyuan Ying", "Hongyi Yuan", "Zheng Yuan", "Tianyang Zhan", "Chi Zhang", "Yue Zhang", "Ge Zhang", "Tianyun Zhao", "Jianqiu Zhao", "Yichi Zhou", "Thomas Hanwen Zhu"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\n\\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning."}
{"id": "2507.23751", "pdf": "https://arxiv.org/pdf/2507.23751.pdf", "abs": "https://arxiv.org/abs/2507.23751", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "authors": ["Ping Yu", "Jack Lanchantin", "Tianlu Wang", "Weizhe Yuan", "Olga Golovneva", "Ilia Kulikov", "Sainbayar Sukhbaatar", "Jason Weston", "Jing Xu"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the\ngiven seed tasks, and then to generate a new synthetic prompt of similar\nquality and complexity for use in LLM training, followed by filtering for\nhigh-quality data with automatic metrics. In verifiable reasoning, our\nsynthetic data significantly outperforms existing training datasets, such as\ns1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For\nnon-verifiable instruction-following tasks, our method surpasses the\nperformance of human or standard self-instruct prompts on both AlpacaEval 2.0\nand Arena-Hard."}
{"id": "2507.23773", "pdf": "https://arxiv.org/pdf/2507.23773.pdf", "abs": "https://arxiv.org/abs/2507.23773", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "authors": ["Mingkai Deng", "Jinyu Hou", "Yilin Shen", "Hongxia Jin", "Graham Neubig", "Zhiting Hu", "Eric Xing"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": null, "summary": "AI agents built on large language models (LLMs) hold enormous promise, but\ncurrent practice focuses on a one-task-one-agent approach, which not only falls\nshort of scalability and generality, but also suffers from the fundamental\nlimitations of autoregressive LLMs. On the other hand, humans are general\nagents who reason by mentally simulating the outcomes of their actions and\nplans. Moving towards a more general and powerful AI agent, we introduce\nSimuRA, a goal-oriented architecture for generalized agentic reasoning. Based\non a principled formulation of optimal agent in any environment, \\modelname\novercomes the limitations of autoregressive reasoning by introducing a world\nmodel for planning via simulation. The generalized world model is implemented\nusing LLM, which can flexibly plan in a wide range of environments using the\nconcept-rich latent space of natural language. Experiments on difficult web\nbrowsing tasks show that \\modelname improves the success of flight search from\n0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent\nadvantage of up to 124\\% over autoregressive planning, demonstrating the\nadvantage of world model simulation as a reasoning paradigm. We are excited\nabout the possibility for training a single, general agent model based on LLMs\nthat can act superintelligently in all environments. To start, we make SimuRA,\na web-browsing agent built on \\modelname with pretrained LLMs, available as a\nresearch demo for public testing."}
{"id": "2404.12829", "pdf": "https://arxiv.org/pdf/2404.12829.pdf", "abs": "https://arxiv.org/abs/2404.12829", "title": "LiMe: a Latin Corpus of Late Medieval Criminal Sentences", "authors": ["Alessandra Bassani", "Beatrice Del Bo", "Alfio Ferrara", "Marta Mangini", "Sergio Picascia", "Ambra Stefanello"], "categories": ["cs.CL"], "comment": null, "summary": "The Latin language has received attention from the computational linguistics\nresearch community, which has built, over the years, several valuable\nresources, ranging from detailed annotated corpora to sophisticated tools for\nlinguistic analysis. With the recent advent of large language models,\nresearchers have also started developing models capable of generating vector\nrepresentations of Latin texts. The performances of such models remain behind\nthe ones for modern languages, given the disparity in available data. In this\npaper, we present the LiMe dataset, a corpus of 325 documents extracted from a\nseries of medieval manuscripts called Libri sententiarum potestatis Mediolani,\nand thoroughly annotated by experts, in order to be employed for masked\nlanguage model, as well as supervised natural language processing tasks."}
{"id": "2404.18154", "pdf": "https://arxiv.org/pdf/2404.18154.pdf", "abs": "https://arxiv.org/abs/2404.18154", "title": "Explaining vague language", "authors": ["Paul Égré", "Benjamin Spector"], "categories": ["cs.CL", "cs.GT", "cs.IT", "math.IT", "91A86", "I.2.7"], "comment": null, "summary": "Why is language vague? Vagueness may be explained and rationalized if it can\nbe shown that vague language is more useful to speaker and hearer than precise\nlanguage. In a well-known paper, Lipman proposes a game-theoretic account of\nvagueness in terms of mixed strategy that leads to a puzzle: vagueness cannot\nbe strictly better than precision at equilibrium. More recently, \\'Egr\\'e,\nSpector, Mortier and Verheyen have put forward a Bayesian account of vagueness\nestablishing that using vague words can be strictly more informative than using\nprecise words. This paper proposes to compare both results and to explain why\nthey are not in contradiction. Lipman's definition of vagueness relies\nexclusively on a property of signaling strategies, without making any\nassumptions about the lexicon, whereas \\'Egr\\'e et al.'s involves a layer of\nsemantic content. We argue that the semantic account of vagueness is needed,\nand more adequate and explanatory of vagueness."}
{"id": "2406.14313", "pdf": "https://arxiv.org/pdf/2406.14313.pdf", "abs": "https://arxiv.org/abs/2406.14313", "title": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with Unanswerability", "authors": ["Riya Sawhney", "Samrat Yadav", "Indrajit Bhattacharya", "Mausam"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well."}
{"id": "2406.15444", "pdf": "https://arxiv.org/pdf/2406.15444.pdf", "abs": "https://arxiv.org/abs/2406.15444", "title": "Cutting Through the Noise: Boosting LLM Performance on Math Word Problems", "authors": ["Ujjwala Anantheswaran", "Himanshu Gupta", "Kevin Scaria", "Shreyas Verma", "Chitta Baral", "Swaroop Mishra"], "categories": ["cs.CL"], "comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs", "summary": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, PROBLEMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and improved ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to 6%."}
{"id": "2410.02744", "pdf": "https://arxiv.org/pdf/2410.02744.pdf", "abs": "https://arxiv.org/abs/2410.02744", "title": "Neutral Residues: Revisiting Adapters for Model Extension", "authors": ["Franck Signe Talla", "Edouard Grave", "Hervé Jégou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English."}
{"id": "2411.18337", "pdf": "https://arxiv.org/pdf/2411.18337.pdf", "abs": "https://arxiv.org/abs/2411.18337", "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation", "authors": ["T. G. D. K. Sumanathilaka", "Nicholas Micallef", "Julian Hough"], "categories": ["cs.CL"], "comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security", "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication."}
{"id": "2412.11167", "pdf": "https://arxiv.org/pdf/2412.11167.pdf", "abs": "https://arxiv.org/abs/2412.11167", "title": "Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette", "authors": ["Jiahao Yuan", "Zixiang Di", "Shangzixin Zhao", "Zhiqing Cui", "Hanqing Wang", "Guisong Yang", "Usman Naseem"], "categories": ["cs.CL"], "comment": "20 pages, 10 figures", "summary": "Large language models (LLMs) face challenges in aligning with diverse\ncultural values despite their remarkable performance in generation, which stems\nfrom inherent monocultural biases and difficulties in capturing nuanced\ncultural semantics. Existing methods struggle to adapt to unknown culture after\nfine-tuning. Inspired by cultural geography across five continents, we propose\nCultural Palette, a multi-agent framework that redefines cultural alignment as\nan adaptive \"color-blending\" process for country-specific adaptation. Our\napproach harnesses cultural geography across five continents (Africa, America,\nAsia, Europe, Oceania) through three key steps: First, we synthesize the\nPentachromatic Cultural Palette Dataset using GPT-4o, refining\ncontinental-level dialogues with Hofstede's cultural dimensions to establish\nfoundational cultural representations. Second, five continent-level alignment\nagents form specialized cultural communities that generate region-specific\ndraft responses. Third, a Meta Agent employs Cultural MoErges to dynamically\nblend these cultural \"colors\" through attention-gated parameter merging, akin\nto mixing pigments on a palette, resolving conflicts while preserving cultural\nnuances to produce the final culturally-aligned response. Extensive experiments\nacross various countries demonstrate that Cultural Palette surpasses existing\nbaselines in cultural alignment."}
{"id": "2503.15299", "pdf": "https://arxiv.org/pdf/2503.15299.pdf", "abs": "https://arxiv.org/abs/2503.15299", "title": "Inside-Out: Hidden Factual Knowledge in LLMs", "authors": ["Zorik Gekhman", "Eyal Ben David", "Hadas Orgad", "Eran Ofek", "Yonatan Belinkov", "Idan Szpektor", "Jonathan Herzig", "Roi Reichart"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first."}
{"id": "2503.15768", "pdf": "https://arxiv.org/pdf/2503.15768.pdf", "abs": "https://arxiv.org/abs/2503.15768", "title": "Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer", "authors": ["Alexandra DeLucia", "Mark Dredze"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box."}
{"id": "2504.04640", "pdf": "https://arxiv.org/pdf/2504.04640.pdf", "abs": "https://arxiv.org/abs/2504.04640", "title": "Splits! A Flexible Dataset and Evaluation Framework for Sociocultural Linguistic Investigation", "authors": ["Eylon Caplan", "Tania Chakraborty", "Dan Goldwasser"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, under review", "summary": "Variation in language use, shaped by speakers' sociocultural background and\nspecific context of use, offers a rich lens into cultural perspectives, values,\nand opinions. However, the computational study of these Sociocultural\nLinguistic Phenomena (SLP) has often been limited to bespoke analyses of\nspecific groups or topics, hindering the pace of scientific discovery. To\naddress this, we introduce Splits!, a 9.7 million-post dataset from Reddit\ndesigned for systematic and flexible research. The dataset contains posts from\nover 53,000 users across 6 demographic groups, organized into 89 discussion\ntopics to enable comparative analysis. We validate Splits! via\nself-identification and by successfully replicating several known SLPs from\nexisting literature. We complement this dataset with a framework that leverages\nefficient retrieval methods to rapidly validate potential SLPs (PSLPs) by\nautomatically evaluating whether a given hypothesis is supported by our data.\nCrucially, to distinguish between novel and obvious insights, the framework\nincorporates a human-validated measure of a hypothesis's ``unexpectedness.'' We\ndemonstrate that the two-stage process reduces the number of statistically\nsignificant findings requiring manual inspection by a factor of 1.5-1.8x,\nstreamlining the discovery of promising phenomena for further investigation."}
{"id": "2504.09753", "pdf": "https://arxiv.org/pdf/2504.09753.pdf", "abs": "https://arxiv.org/abs/2504.09753", "title": "Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Siddhant Gupta", "Drishti Sharma", "Jebish Purbey", "Kanwal Mehreen", "Muhammad Arham", "Suman Debnath", "Hamza Farooq"], "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 18 figures", "summary": "Large Language Models (LLMs) have shown remarkable capabilities, but their\ndevelopment has primarily focused on English and other high-resource languages,\nleaving many languages underserved. We present our latest Hindi-English\nbi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark\nscores over both languages, outperforming models twice its size. Using a\ncurated dataset composed of English and Hindi instruction data of 485K samples,\nwe instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve\nperformance over both English and Hindi. Our experiments encompassing seven\ndifferent LLMs of varying parameter sizes and over 140 training attempts with\nvarying English-Hindi training data ratios demonstrated that it is possible to\nsignificantly improve multilingual performance without compromising native\nperformance. Further, our approach avoids resource-intensive techniques like\nvocabulary expansion or architectural modifications, thus keeping the model\nsize small. Our results indicate that modest fine-tuning with culturally and\nlocally informed data can bridge performance gaps without incurring significant\ncomputational overhead. We release our training code, datasets, and models\nunder mit and apache licenses to aid further research towards under-represented\nand low-resource languages."}
{"id": "2504.11952", "pdf": "https://arxiv.org/pdf/2504.11952.pdf", "abs": "https://arxiv.org/abs/2504.11952", "title": "Robust and Fine-Grained Detection of AI Generated Texts", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Kanwal Mehreen", "Drishti Sharma", "Siddhant Gupta", "Jebish Purbey", "Ashay Srivastava", "Subhasya TippaReddy", "Arvind Reddy Bobbili", "Suraj Telugara Chandrashekhar", "Modabbir Adeeb", "Srinadh Vura", "Suman Debnath", "Hamza Farooq"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 pages, 6 figures", "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts."}
{"id": "2504.16060", "pdf": "https://arxiv.org/pdf/2504.16060.pdf", "abs": "https://arxiv.org/abs/2504.16060", "title": "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation", "authors": ["Ziqiao Ma", "Jing Ding", "Xuejun Zhang", "Dezhi Luo", "Jiahe Ding", "Sihan Xu", "Yuchen Huang", "Run Peng", "Joyce Chai"], "categories": ["cs.CL"], "comment": "COLM 2025 & CVinW @ CVPR 2025 (Spotlight). Homepage:\n  https://vlm-reg.github.io/", "summary": "Referring Expression Generation (REG) is a core task for evaluating the\npragmatic competence of vision-language systems, requiring not only accurate\nsemantic grounding but also adherence to principles of cooperative\ncommunication (Grice, 1975). However, current evaluations of vision-language\nmodels (VLMs) often overlook the pragmatic dimension, reducing REG to a\nregion-based captioning task and neglecting Gricean maxims. In this work, we\nrevisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of\n1.5k images annotated with both written and spoken referring expressions.\nThrough a systematic evaluation of state-of-the-art VLMs, we identify three key\nfailures of pragmatic competence: (1) failure to uniquely identify the\nreferent, (2) inclusion of excessive or irrelevant information, and (3)\nmisalignment with human pragmatic preference, such as the underuse of minimal\nspatial cues. We also show that standard automatic evaluations fail to capture\nthese pragmatic violations, reinforcing superficial cues rather than genuine\nreferential success. Our findings call for a renewed focus on pragmatically\ninformed models and evaluation frameworks that align with real human\ncommunication."}
{"id": "2505.02851", "pdf": "https://arxiv.org/pdf/2505.02851.pdf", "abs": "https://arxiv.org/abs/2505.02851", "title": "Leveraging LLMs to Create Content Corpora for Niche Domains", "authors": ["Franklin Zhang", "Sonya Zhang", "Alon Halevy"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; H.3.1; H.3.3"], "comment": "9 pages (main content), 5 figures. Supplementary materials can be\n  found at https://github.com/pigfyy/30DayGen-Supplementary-Materials", "summary": "Constructing specialized content corpora from vast, unstructured web sources\nfor domain-specific applications poses substantial data curation challenges. In\nthis paper, we introduce a streamlined approach for generating high-quality,\ndomain-specific corpora by efficiently acquiring, filtering, structuring, and\ncleaning web-based data. We showcase how Large Language Models (LLMs) can be\nleveraged to address complex data curation at scale, and propose a strategical\nframework incorporating LLM-enhanced techniques for structured content\nextraction and semantic deduplication. We validate our approach in the behavior\neducation domain through its integration into 30 Day Me, a habit formation\napplication. Our data pipeline, named 30DayGen, enabled the extraction and\nsynthesis of 3,531 unique 30-day challenges from over 15K webpages. A user\nsurvey reports a satisfaction score of 4.3 out of 5, with 91% of respondents\nindicating willingness to use the curated content for their habit-formation\ngoals."}
{"id": "2505.14874", "pdf": "https://arxiv.org/pdf/2505.14874.pdf", "abs": "https://arxiv.org/abs/2505.14874", "title": "Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages", "authors": ["Chin-Jou Li", "Eunjung Yeo", "Kwanghee Choi", "Paula Andrea Pérez-Toro", "Masao Someki", "Rohan Kumar Das", "Zhengjun Yue", "Juan Rafael Orozco-Arroyave", "Elmar Nöth", "David R. Mortensen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "5 pages, 1 figure, Accepted to Interspeech 2025", "summary": "Automatic speech recognition (ASR) for dysarthric speech remains challenging\ndue to data scarcity, particularly in non-English languages. To address this,\nwe fine-tune a voice conversion model on English dysarthric speech (UASpeech)\nto encode both speaker characteristics and prosodic distortions, then apply it\nto convert healthy non-English speech (FLEURS) into non-English dysarthric-like\nspeech. The generated data is then used to fine-tune a multilingual ASR model,\nMassively Multilingual Speech (MMS), for improved dysarthric speech\nrecognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE\n(Tamil) demonstrates that VC with both speaker and prosody conversion\nsignificantly outperforms the off-the-shelf MMS performance and conventional\naugmentation techniques such as speed and tempo perturbation. Objective and\nsubjective analyses of the generated data further confirm that the generated\nspeech simulates dysarthric characteristics."}
{"id": "2505.18497", "pdf": "https://arxiv.org/pdf/2505.18497.pdf", "abs": "https://arxiv.org/abs/2505.18497", "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models", "authors": ["Kefan Yu", "Qingcheng Zeng", "Weihao Xuan", "Wanxin Li", "Jingyi Wu", "Rob Voigt"], "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution and\ntheory-of-mind reasoning, both of which require substantial pragmatic\nunderstanding. However, how LLMs acquire this pragmatic competence throughout\nthe training process remains poorly understood. In this work, we introduce\nALTPRAG, a dataset grounded in the pragmatic concept of alternatives, to\nevaluate whether LLMs at different training stages can accurately infer nuanced\nspeaker intentions. Each instance pairs two equally plausible yet pragmatically\ndivergent continuations and requires the model to (i) infer the speaker's\nintended meaning and (ii) explain when and why a speaker would choose one\nutterance over its alternative, thus directly probing pragmatic competence\nthrough contrastive reasoning. We systematically evaluate 22 LLMs across 3 key\ntraining stages: after pre-training, supervised fine-tuning (SFT), and\npreference optimization, to examine the development of pragmatic competence.\nOur results show that even base models exhibit notable sensitivity to pragmatic\ncues, which improves consistently with increases in model and data scale.\nAdditionally, SFT and RLHF contribute further gains, particularly in\ncognitive-pragmatic scenarios. These findings highlight pragmatic competence as\nan emergent and compositional property of LLM training and offer new insights\nfor aligning models with human communicative norms."}
{"id": "2505.23628", "pdf": "https://arxiv.org/pdf/2505.23628.pdf", "abs": "https://arxiv.org/abs/2505.23628", "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora", "authors": ["Jiaxin Bai", "Wei Fan", "Qi Hu", "Qing Zong", "Chunyang Li", "Hong Ting Tsang", "Hongyu Luo", "Yauwai Yim", "Haoyu Huang", "Xiao Zhou", "Feng Qin", "Tianshi Zheng", "Xi Peng", "Xin Yao", "Huiwen Yang", "Leijie Wu", "Yi Ji", "Gong Zhang", "Renhai Chen", "Yangqiu Song"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, preprint, code:\n  https://github.com/HKUST-KnowComp/AutoSchemaKG", "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models."}
{"id": "2506.00068", "pdf": "https://arxiv.org/pdf/2506.00068.pdf", "abs": "https://arxiv.org/abs/2506.00068", "title": "Framing Political Bias in Multilingual LLMs Across Pakistani Languages", "authors": ["Afrozah Nadeem", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) increasingly shape public discourse, yet most\nevaluations of political and economic bias have focused on high-resource,\nWestern languages and contexts. This leaves critical blind spots in\nlow-resource, multilingual regions such as Pakistan, where linguistic identity\nis closely tied to political, religious, and regional ideologies. We present a\nsystematic evaluation of political bias in 13 state-of-the-art LLMs across five\nPakistani languages: Urdu, Punjabi, Sindhi, Pashto, and Balochi. Our framework\nintegrates a culturally adapted Political Compass Test (PCT) with multi-level\nframing analysis, capturing both ideological stance (economic/social axes) and\nstylistic framing (content, tone, emphasis). Prompts are aligned with 11\nsocio-political themes specific to the Pakistani context. Results show that\nwhile LLMs predominantly reflect liberal-left orientations consistent with\nWestern training data, they exhibit more authoritarian framing in regional\nlanguages, highlighting language-conditioned ideological modulation. We also\nidentify consistent model-specific bias patterns across languages. These\nfindings show the need for culturally grounded, multilingual bias auditing\nframeworks in global NLP."}
{"id": "2506.07106", "pdf": "https://arxiv.org/pdf/2506.07106.pdf", "abs": "https://arxiv.org/abs/2506.07106", "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models", "authors": ["Samir Abdaljalil", "Hasan Kurban", "Khalid Qaraqe", "Erchin Serpedin"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 KnowFM", "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought."}
{"id": "2506.08184", "pdf": "https://arxiv.org/pdf/2506.08184.pdf", "abs": "https://arxiv.org/abs/2506.08184", "title": "Unable to Forget: Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length", "authors": ["Chupei Wang", "Jiaqiu Vince Sun"], "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "comment": "Accepted at ICML 2025 Workshop on Long Context Foundation Models\n  (ICFM). Code: https://github.com/zhuangziGiantfish/Unable-to-Forget", "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval."}
{"id": "2506.12365", "pdf": "https://arxiv.org/pdf/2506.12365.pdf", "abs": "https://arxiv.org/abs/2506.12365", "title": "Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics", "authors": ["Asifullah Khan", "Muhammad Zaeem Khan", "Saleha Jamshed", "Sadia Ahmad", "Aleesha Zainab", "Kaynat Khatib", "Faria Bibi", "Abdul Rehman"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "This survey paper outlines the key developments in the field of Large\nLanguage Models (LLMs), including enhancements to their reasoning skills,\nadaptability to various tasks, increased computational efficiency, and the\nability to make ethical decisions. The techniques that have been most effective\nin bridging the gap between human and machine communications include the\nChain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from\nHuman Feedback. The improvements in multimodal learning and few-shot or\nzero-shot techniques have further empowered LLMs to handle complex jobs with\nminor input. A significant focus is placed on efficiency, detailing scaling\nstrategies, optimization techniques, and the influential Mixture-of-Experts\n(MoE) architecture, which strategically routes inputs to specialized\nsubnetworks to boost predictive accuracy, while optimizing resource allocation.\nThis survey also offers a broader perspective on recent advancements in LLMs,\ngoing beyond isolated aspects such as model architecture or ethical concerns.\nAdditionally, it explores the role of LLMs in Agentic AI and their use as\nAutonomous Decision-Making Systems, and categorizes emerging methods that\nenhance LLM reasoning, efficiency, and ethical alignment. The survey also\nidentifies underexplored areas such as interpretability, cross-modal\nintegration, and sustainability. While significant advancements have been made\nin LLMs, challenges such as high computational costs, biases, and ethical risks\nremain. Overcoming these requires a focus on bias mitigation, transparent\ndecision-making, and explicit ethical guidelines. Future research will\ngenerally focus on enhancing the model's ability to handle multiple inputs,\nthereby making it more intelligent, safe, and reliable."}
{"id": "2506.18199", "pdf": "https://arxiv.org/pdf/2506.18199.pdf", "abs": "https://arxiv.org/abs/2506.18199", "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review", "authors": ["Bushra Asseri", "Estabrag Abdelaziz", "Areej Al-Wabil"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "Research is incomplete", "summary": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility."}
{"id": "2506.21875", "pdf": "https://arxiv.org/pdf/2506.21875.pdf", "abs": "https://arxiv.org/abs/2506.21875", "title": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation", "authors": ["Jian Zhang", "Linhao Zhang", "Bokai Lei", "Chuhan Wu", "Wei Jia", "Xiao Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation."}
{"id": "2507.06448", "pdf": "https://arxiv.org/pdf/2507.06448.pdf", "abs": "https://arxiv.org/abs/2507.06448", "title": "Perception-Aware Policy Optimization for Multimodal Reasoning", "authors": ["Zhenhailong Wang", "Xuehang Guo", "Sofia Stoica", "Haiyang Xu", "Hongru Wang", "Hyeonjeong Ha", "Xiusi Chen", "Yangyi Chen", "Ming Yan", "Fei Huang", "Heng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose PAPO, a novel policy\ngradient algorithm that encourages the model to learn to perceive while\nlearning to reason. Specifically, we introduce the Implicit Perception Loss in\nthe form of a KL divergence term, which can be seamlessly plugged into\nmainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely\non additional data curation, reward models, or stronger teacher models. To\nfurther enhance the training stability of PAPO, we introduce the Double Entropy\nLoss, which effectively regularizes the new KL objective without compromising\nperformance. Despite its simplicity, PAPO yields significant overall\nimprovements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements\nare more pronounced, approaching 8.0%-19.1%, on tasks with high vision\ndependency. We also observe a substantial reduction of 30.5% in perception\nerrors, indicating improved perceptual capabilities with PAPO. Overall, our\nwork introduces a deeper integration of perception-aware supervision into core\nlearning objectives and lays the groundwork for a new RL framework that\nencourages visually grounded reasoning. Code and data will be made publicly\navailable for research purposes. Project page:\nhttps://mikewangwzhl.github.io/PAPO."}
{"id": "2507.07695", "pdf": "https://arxiv.org/pdf/2507.07695.pdf", "abs": "https://arxiv.org/abs/2507.07695", "title": "KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities", "authors": ["Hruday Markondapatnaikuni", "Basem Suleiman", "Abdelkarim Erradi", "Shijing Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 14 figures", "summary": "Fine-tuning is an immensely resource-intensive process when retraining Large\nLanguage Models (LLMs) to incorporate a larger body of knowledge. Although many\nfine-tuning techniques have been developed to reduce the time and computational\ncost involved, the challenge persists as LLMs continue to grow in size and\ncomplexity. To address this, a new approach to knowledge expansion in LLMs is\nneeded. Retrieval-Augmented Generation (RAG) offers one such alternative by\nstoring external knowledge in a database and retrieving relevant chunks to\nsupport question answering. However, naive implementations of RAG face\nsignificant limitations in scalability and answer accuracy. This paper\nintroduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome\nthese limitations. Inspired by the divide-and-conquer paradigm, K2RAG\nintegrates dense and sparse vector search, knowledge graphs, and text\nsummarization to improve retrieval quality and system efficiency. The framework\nalso includes a preprocessing step that summarizes the training data,\nsignificantly reducing the training time. K2RAG was evaluated using the\nMultiHopRAG dataset, where the proposed pipeline was trained on the document\ncorpus and tested on a separate evaluation set. Results demonstrated notable\nimprovements over common naive RAG implementations. K2RAG achieved the highest\nmean answer similarity score of 0.57, and reached the highest third quartile\n(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.\nIn addition to improved accuracy, the framework proved highly efficient. The\nsummarization step reduced the average training time of individual components\nby 93%, and execution speed was up to 40% faster than traditional knowledge\ngraph-based RAG systems. K2RAG also demonstrated superior scalability,\nrequiring three times less VRAM than several naive RAG implementations tested\nin this study."}
{"id": "2507.08606", "pdf": "https://arxiv.org/pdf/2507.08606.pdf", "abs": "https://arxiv.org/abs/2507.08606", "title": "DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures", "authors": ["Benno Uthayasooriyar", "Antoine Ly", "Franck Vermet", "Caio Corro"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce DocPolarBERT, a layout-aware BERT model for document\nunderstanding that eliminates the need for absolute 2D positional embeddings.\nWe extend self-attention to take into account text block positions in relative\npolar coordinate system rather than the Cartesian one. Despite being\npre-trained on a dataset more than six times smaller than the widely used\nIIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results\ndemonstrate that a carefully designed attention mechanism can compensate for\nreduced pre-training data, offering an efficient and effective alternative for\ndocument understanding."}
{"id": "2507.10073", "pdf": "https://arxiv.org/pdf/2507.10073.pdf", "abs": "https://arxiv.org/abs/2507.10073", "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "authors": ["Simon Münker"], "categories": ["cs.CL", "cs.AI"], "comment": "15pages, 1 figure, 2 tables", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape."}
{"id": "2507.11832", "pdf": "https://arxiv.org/pdf/2507.11832.pdf", "abs": "https://arxiv.org/abs/2507.11832", "title": "ILID: Native Script Language Identification for Indian Languages", "authors": ["Yash Ingle", "Pruthwik Mishra"], "categories": ["cs.CL"], "comment": "10 pages, 1 figure, 6 tables, Paper accepted in RANLP 2025", "summary": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script, making the task even more challenging.\nTaking all these challenges into account, we develop and release a dataset of\n250K sentences consisting of 23 languages including English and all 22 official\nIndian languages labeled with their language identifiers, where data in most\nlanguages are newly created. We also develop and release baseline models using\nstate-of-the-art approaches in machine learning and fine-tuning pre-trained\ntransformer models. Our models outperforms the state-of-the-art pre-trained\ntransformer models for the language identification task. The dataset and the\ncodes are available at https://yashingle-ai.github.io/ILID/ and in Huggingface\nopen source libraries."}
{"id": "2507.16725", "pdf": "https://arxiv.org/pdf/2507.16725.pdf", "abs": "https://arxiv.org/abs/2507.16725", "title": "RAVine: Reality-Aligned Evaluation for Agentic Search", "authors": ["Yilong Xu", "Xiang Long", "Zhi Zheng", "Jinhua Gao"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine."}
{"id": "2507.17186", "pdf": "https://arxiv.org/pdf/2507.17186.pdf", "abs": "https://arxiv.org/abs/2507.17186", "title": "FinGAIA: A Chinese Benchmark for AI Agents in Real-World Financial Domain", "authors": ["Lingfeng Zeng", "Fangqi Lou", "Zixuan Wang", "Jiajie Xu", "Jinyi Niu", "Mengping Li", "Yifan Dong", "Qi Qi", "Wei Zhang", "Ziwei Yang", "Jun Han", "Ruilun Feng", "Ruiqi Hu", "Lejie Zhang", "Zhengbo Feng", "Yicheng Ren", "Xin Guo", "Zhaowei Liu", "Dongpo Cheng", "Weige Cai", "Liwen Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The booming development of AI agents presents unprecedented opportunities for\nautomating complex tasks across various domains. However, their multi-step,\nmulti-tool collaboration capabilities in the financial sector remain\nunderexplored. This paper introduces FinGAIA, an end-to-end benchmark designed\nto evaluate the practical abilities of AI agents in the financial domain.\nFinGAIA comprises 407 meticulously crafted tasks, spanning seven major\nfinancial sub-domains: securities, funds, banking, insurance, futures, trusts,\nand asset management. These tasks are organized into three hierarchical levels\nof scenario depth: basic business analysis, asset decision support, and\nstrategic risk management. We evaluated 10 mainstream AI agents in a zero-shot\nsetting. The best-performing agent, ChatGPT, achieved an overall accuracy of\n48.9\\%, which, while superior to non-professionals, still lags financial\nexperts by over 35 percentage points. Error analysis has revealed five\nrecurring failure patterns: Cross-modal Alignment Deficiency, Financial\nTerminological Bias, Operational Process Awareness Barrier, among others. These\npatterns point to crucial directions for future research. Our work provides the\nfirst agent benchmark closely related to the financial domain, aiming to\nobjectively assess and promote the development of agents in this crucial field.\nPartial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA."}
{"id": "2507.21568", "pdf": "https://arxiv.org/pdf/2507.21568.pdf", "abs": "https://arxiv.org/abs/2507.21568", "title": "Multi-Hypothesis Distillation of Multilingual Neural Translation Models for Low-Resource Languages", "authors": ["Aarón Galiano-Jiménez", "Juan Antonio Pérez-Ortiz", "Felipe Sánchez-Martínez", "Víctor M. Sánchez-Cartagena"], "categories": ["cs.CL"], "comment": "17 pages, 12 figures", "summary": "This paper explores sequence-level knowledge distillation (KD) of\nmultilingual pre-trained encoder-decoder translation models. We argue that the\nteacher model's output distribution holds valuable insights for the student,\nbeyond the approximated mode obtained through beam search (the standard\ndecoding method), and present Multi-Hypothesis Distillation (MHD), a\nsequence-level KD method that generates multiple translations for each source\nsentence. This provides a larger representation of the teacher model\ndistribution and exposes the student model to a wider range of target-side\nprefixes. We leverage $n$-best lists from beam search to guide the student's\nlearning and examine alternative decoding methods to address issues like low\nvariability and the under-representation of infrequent tokens. For low-resource\nlanguages, our research shows that while sampling methods may slightly\ncompromise translation quality compared to beam search based approaches, they\nenhance the generated corpora with greater variability and lexical richness.\nThis ultimately improves student model performance and mitigates the gender\nbias amplification often associated with KD."}
{"id": "2507.22581", "pdf": "https://arxiv.org/pdf/2507.22581.pdf", "abs": "https://arxiv.org/abs/2507.22581", "title": "Unveiling the Influence of Amplifying Language-Specific Neurons", "authors": ["Inaya Rahmanisa", "Lyzander Marciano Andrylie", "Mahardika Krisna Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "categories": ["cs.CL", "cs.LG"], "comment": "Our code and dataset are made available at\n  https://github.com/tauimbz/lang-task-neuron", "summary": "Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer."}
{"id": "2401.13481", "pdf": "https://arxiv.org/pdf/2401.13481.pdf", "abs": "https://arxiv.org/abs/2401.13481", "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment", "authors": ["Joshua Ashkinaze", "Julia Mendelsohn", "Li Qiwei", "Ceren Budak", "Eric Gilbert"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "Accepted at ACM Collective Intelligence 2025. Originally posted 2024", "summary": "Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- speaks to the interdependent process of cultural creation: creative ideas\nare built upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI and that participants may knowingly adopt AI ideas when the\ntask is difficult. Our findings suggest that introducing AI ideas may increase\ncollective diversity but not individual creativity."}
{"id": "2410.05343", "pdf": "https://arxiv.org/pdf/2410.05343.pdf", "abs": "https://arxiv.org/abs/2410.05343", "title": "EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos referring to Procedural Texts", "authors": ["Yuto Haneji", "Taichi Nishimura", "Hirotaka Kameko", "Keisuke Shirai", "Tomoya Yoshida", "Keiya Kajimura", "Koki Yamamoto", "Taiyu Cui", "Tomohiro Nishimoto", "Shinsuke Mori"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Main 8 pages, supplementary 6 pages", "summary": "Mistake action detection is crucial for developing intelligent archives that\ndetect workers' errors and provide feedback. Existing studies have focused on\nvisually apparent mistakes in free-style activities, resulting in video-only\napproaches to mistake detection. However, in text-following activities, models\ncannot determine the correctness of some actions without referring to the\ntexts. Additionally, current mistake datasets rarely use procedural texts for\nvideo recording except for cooking. To fill these gaps, this paper proposes the\nEgoOops dataset, where egocentric videos record erroneous activities when\nfollowing procedural texts across diverse domains. It features three types of\nannotations: video-text alignment, mistake labels, and descriptions for\nmistakes. We also propose a mistake detection approach, combining video-text\nalignment and mistake label classification to leverage the texts. Our\nexperimental results show that incorporating procedural texts is essential for\nmistake detection. Data is available through\nhttps://y-haneji.github.io/EgoOops-project-page/."}
{"id": "2412.19792", "pdf": "https://arxiv.org/pdf/2412.19792.pdf", "abs": "https://arxiv.org/abs/2412.19792", "title": "InfAlign: Inference-aware language model alignment", "authors": ["Ananth Balashankar", "Ziteng Sun", "Jonathan Berant", "Jacob Eisenstein", "Michael Collins", "Adrian Hutter", "Jong Lee", "Chirag Nagpal", "Flavien Prost", "Aradhana Sinha", "Ananda Theertha Suresh", "Ahmad Beirami"], "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Language model alignment is a critical step in training modern generative\nlanguage models. Alignment targets to improve win rate of a sample from the\naligned model against the base model. Today, we are increasingly using\ninference-time algorithms (e.g., Best-of-N, controlled decoding, tree search)\nto decode from language models rather than standard sampling. We show that this\ntrain/test mismatch makes standard RLHF framework sub-optimal in view of such\ninference-time methods. To this end, we propose a framework for inference-aware\nalignment (InfAlign), which aims to optimize inference-time win rate of the\naligned policy against the base model. We prove that for any inference-time\ndecoding procedure, the optimal aligned policy is the solution to the standard\nRLHF problem with a transformation of the reward. This motivates us to provide\nthe calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem,\nwhich involves a reward calibration step and a KL-regularized reward\nmaximization step with a transformation of the calibrated reward. For best-of-N\nsampling and best-of-N jailbreaking, we propose specific transformations\noffering up to 3-8% improvement on inference-time win rates. Finally, we also\nshow that our proposed reward calibration method is a strong baseline for\noptimizing standard win rate."}
{"id": "2503.15621", "pdf": "https://arxiv.org/pdf/2503.15621.pdf", "abs": "https://arxiv.org/abs/2503.15621", "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning", "authors": ["Federico Cocchi", "Nicholas Moratelli", "Davide Caffagni", "Sara Sarto", "Lorenzo Baraldi", "Marcella Cornia", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "ICCV 2025 Workshop on What is Next in Multimodal Foundation Models", "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE."}
{"id": "2503.18666", "pdf": "https://arxiv.org/pdf/2503.18666.pdf", "abs": "https://arxiv.org/abs/2503.18666", "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "summary": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identify 87.26%\nof the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios."}
{"id": "2504.14928", "pdf": "https://arxiv.org/pdf/2504.14928.pdf", "abs": "https://arxiv.org/abs/2504.14928", "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "comment": "Paper URL: https://aclanthology.org/2025.acl-long.1576 ;Presentation\n  Video: https://www.youtube.com/watch?v=j63ooKE50I0", "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness."}
{"id": "2505.18102", "pdf": "https://arxiv.org/pdf/2505.18102.pdf", "abs": "https://arxiv.org/abs/2505.18102", "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?", "authors": ["Takashi Ishida", "Thanawat Lodkaew", "Ikko Yamane"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME"], "comment": "Extended version of the paper presented as an Oral at the ICML 2025\n  Workshop on the Impact of Memorization on Trustworthy Foundation Models", "summary": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies."}
{"id": "2507.05903", "pdf": "https://arxiv.org/pdf/2507.05903.pdf", "abs": "https://arxiv.org/abs/2507.05903", "title": "AI-Reporter: A Path to a New Genre of Scientific Communication", "authors": ["Gerd Graßhoff"], "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "The AI-Reporter represents a paradigmatic shift in scientific publication\npractice. This document demonstrates through a concrete case study how our\nsystem transforms academic presentations into publication-ready chapters -- in\nless than three minutes. Using Arno Simons' lecture on Large Language Models\nfrom the ``Large Language Models for the History, Philosophy, and Sociology of\nScience'' workshop (NEPI) as an example, we show how technological innovation\nbridges the gap between ephemeral presentation and permanent scientific\ndocumentation."}
{"id": "2507.14534", "pdf": "https://arxiv.org/pdf/2507.14534.pdf", "abs": "https://arxiv.org/abs/2507.14534", "title": "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion", "authors": ["Yu Zhang", "Baotong Tian", "Zhiyao Duan"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Zero-shot online voice conversion (VC) holds significant promise for\nreal-time communications and entertainment. However, current VC models struggle\nto preserve semantic fidelity under real-time constraints, deliver\nnatural-sounding conversions, and adapt effectively to unseen speaker\ncharacteristics. To address these challenges, we introduce Conan, a chunkwise\nonline zero-shot voice conversion model that preserves the content of the\nsource while matching the voice timbre and styles of reference speech. Conan\ncomprises three core components: 1) a Stream Content Extractor that leverages\nEmformer for low-latency streaming content encoding; 2) an Adaptive Style\nEncoder that extracts fine-grained stylistic features from reference speech for\nenhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully\ncausal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations\ndemonstrate that Conan outperforms baseline models in subjective and objective\nmetrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo."}
{"id": "2507.19060", "pdf": "https://arxiv.org/pdf/2507.19060.pdf", "abs": "https://arxiv.org/abs/2507.19060", "title": "PurpCode: Reasoning for Safer Code Generation", "authors": ["Jiawei Liu", "Nirav Diwan", "Zhe Wang", "Haoyu Zhai", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Muntasir Wahed", "Yinlin Deng", "Hadjer Benkraouda", "Yuxiang Wei", "Lingming Zhang", "Ismini Lourentzou", "Gang Wang"], "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "We introduce PurpCode, the first post-training recipe for training safe code\nreasoning models towards generating secure code and defending against malicious\ncyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule\nLearning, which explicitly teaches the model to reference cybersafety rules to\ngenerate vulnerability-free code and to avoid facilitating malicious\ncyberactivities; and (ii) Reinforcement Learning, which optimizes model safety\nand preserves model utility through diverse, multi-objective reward mechanisms.\nTo empower the training pipelines with comprehensive cybersafety data, we\nconduct internal red-teaming to synthesize comprehensive and high-coverage\nprompts based on real-world tasks for inducing unsafe cyberactivities in the\nmodel. Based on PurpCode, we develop a reasoning-based coding model, namely\nPurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming\nvarious frontier models. Meanwhile, our alignment method decreases the model\noverrefusal rates in both general and cybersafety-specific scenarios, while\npreserving model utility in both code generation and common security knowledge."}
{"id": "2507.21903", "pdf": "https://arxiv.org/pdf/2507.21903.pdf", "abs": "https://arxiv.org/abs/2507.21903", "title": "Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation", "authors": ["Tiviatis Sim", "Kaiwen Yang", "Shen Xin", "Kenji Kawaguchi"], "categories": ["cs.SI", "cs.CL", "cs.IR"], "comment": null, "summary": "As news reporting becomes increasingly global and decentralized online,\ntracking related events across multiple sources presents significant\nchallenges. Existing news summarization methods typically utilizes Large\nLanguage Models and Graphical methods on article-based summaries. However, this\nis not effective since it only considers the textual content of similarly dated\narticles to understand the gist of the event. To counteract the lack of\nanalysis on the parties involved, it is essential to come up with a novel\nframework to gauge the importance of stakeholders and the connection of related\nevents through the relevant entities involved. Therefore, we present SUnSET:\nSynergistic Understanding of Stakeholder, Events and Time for the task of\nTimeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)\nto build SET triplets and introduced the use of stakeholder-based ranking to\nconstruct a $Relevancy$ metric, which can be extended into general situations.\nOur experimental results outperform all prior baselines and emerged as the new\nState-of-the-Art, highlighting the impact of stakeholder information within\nnews article."}
{"id": "2507.22062", "pdf": "https://arxiv.org/pdf/2507.22062.pdf", "abs": "https://arxiv.org/abs/2507.22062", "title": "Meta CLIP 2: A Worldwide Scaling Recipe", "authors": ["Yung-Sung Chuang", "Yang Li", "Dong Wang", "Ching-Feng Yeh", "Kehan Lyu", "Ramya Raghavendra", "James Glass", "Lifei Huang", "Jason Weston", "Luke Zettlemoyer", "Xinlei Chen", "Zhuang Liu", "Saining Xie", "Wen-tau Yih", "Shang-Wen Li", "Hu Xu"], "categories": ["cs.CV", "cs.CL"], "comment": "10 pages", "summary": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, Meta CLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval."}
{"id": "2507.22359", "pdf": "https://arxiv.org/pdf/2507.22359.pdf", "abs": "https://arxiv.org/abs/2507.22359", "title": "LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models", "authors": ["Qianhong Guo", "Wei Xie", "Xiaofang Cai", "Enze Wang", "Shuoyoucheng Ma", "Kai Chen", "Xiaofeng Wang", "Baosheng Wang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Although large language models (LLMs) demonstrate remarkable capabilities\nacross various tasks, evaluating their capabilities remains a challenging task.\nExisting evaluation methods suffer from issues such as data contamination,\nblack-box operation, and subjective preference. These issues make it difficult\nto evaluate the LLMs' true capabilities comprehensively. To tackle these\nchallenges, we propose a novel benchmark-free evaluation paradigm,\nLLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,\nand evaluate mutually. This method integrates four key evaluation criteria:\ndynamic, transparent, objective, and professional, which existing evaluation\nmethods cannot satisfy simultaneously. Experiments on eight mainstream LLMs\nacross mathematics and programming verify the advantages of our method in\ndistinguishing LLM performance. Furthermore, our study reveals several novel\nfindings that are difficult for traditional methods to detect, including but\nnot limited to: (1) Gemini demonstrates the highest original and professional\nquestion-design capabilities among others; (2) Some LLMs exhibit\n''memorization-based answering'' by misrecognizing questions as familiar ones\nwith a similar structure; (3) LLM evaluation results demonstrate high\nconsistency (robustness)."}
{"id": "2507.22607", "pdf": "https://arxiv.org/pdf/2507.22607.pdf", "abs": "https://arxiv.org/abs/2507.22607", "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning", "authors": ["Ruifeng Yuan", "Chenghao Xiao", "Sicong Leng", "Jianyu Wang", "Long Li", "Weiwen Xu", "Hou Pong Chan", "Deli Zhao", "Tingyang Xu", "Zhongyu Wei", "Hao Zhang", "Yu Rong"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "21 pages, 5 figures, 6 tables. Work in progress", "summary": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach."}
{"id": "2507.22879", "pdf": "https://arxiv.org/pdf/2507.22879.pdf", "abs": "https://arxiv.org/abs/2507.22879", "title": "RecGPT Technical Report", "authors": ["Chao Yi", "Dian Chen", "Gaoyang Guo", "Jiakai Tang", "Jian Wu", "Jing Yu", "Mao Zhang", "Sunhao Dai", "Wen Chen", "Wenjun Yang", "Yuning Jiang", "Zhujin Gao", "Bo Zheng", "Chi Li", "Dimin Wang", "Dixuan Wang", "Fan Li", "Fan Zhang", "Haibin Chen", "Haozhuang Liu", "Jialin Zhu", "Jiamang Wang", "Jiawei Wu", "Jin Cui", "Ju Huang", "Kai Zhang", "Kan Liu", "Lang Tian", "Liang Rao", "Longbin Li", "Lulu Zhao", "Na He", "Peiyang Wang", "Qiqi Huang", "Tao Luo", "Wenbo Su", "Xiaoxiao He", "Xin Tong", "Xu Chen", "Xunke Xi", "Yang Li", "Yaxuan Wu", "Yeqiu Yang", "Yi Hu", "Yinnan Song", "Yuchen Li", "Yujie Luo", "Yujin Yuan", "Yuliang Yan", "Zhengyang Wang", "Zhibo Xiao", "Zhixin Ma", "Zile Zhou", "Ziqi Zhang"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem."}
