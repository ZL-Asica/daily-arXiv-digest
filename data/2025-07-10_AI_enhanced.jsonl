{"id": "2507.06235", "pdf": "https://arxiv.org/pdf/2507.06235.pdf", "abs": "https://arxiv.org/abs/2507.06235", "title": "Super Kawaii Vocalics: Amplifying the \"Cute\" Factor in Computer Voice", "authors": ["Yuto Mandai", "Katie Seaborn", "Tomoyasu Nakano", "Xin Sun", "Yijia Wang", "Jun Kato"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI '25", "summary": "\"Kawaii\" is the Japanese concept of cute, which carries sociocultural\nconnotations related to social identities and emotional responses. Yet,\nvirtually all work to date has focused on the visual side of kawaii, including\nin studies of computer agents and social robots. In pursuit of formalizing the\nnew science of kawaii vocalics, we explored what elements of voice relate to\nkawaii and how they might be manipulated, manually and automatically. We\nconducted a four-phase study (grand N = 512) with two varieties of computer\nvoices: text-to-speech (TTS) and game character voices. We found kawaii \"sweet\nspots\" through manipulation of fundamental and formant frequencies, but only\nfor certain voices and to a certain extent. Findings also suggest a ceiling\neffect for the kawaii vocalics of certain voices. We offer empirical validation\nof the preliminary kawaii vocalics model and an elementary method for\nmanipulating kawaii perceptions of computer voice.", "AI": {"tldr": "This paper investigates the vocal aspects of the Japanese concept of 'kawaii' (cuteness) in computer voices, providing insights into manipulating voice characteristics to enhance perceived cuteness.", "motivation": "To explore the under-researched area of kawaii vocalics in computer voices and determine how voice characteristics can be manipulated to evoke kawaii perceptions.", "method": "Conducted a four-phase study with 512 participants examining two types of computer voices (text-to-speech and game character) to identify elements of voice that correlate with kawaii.", "result": "Identified 'sweet spots' in vocal manipulation, highlighting certain fundamental and formant frequency adjustments that enhance kawaii perception, with noted limitations based on voice types.", "conclusion": "Established an empirical basis for a kawaii vocalics model and provided an initial method for manipulating perceptions of computer voice in relation to cuteness.", "key_contributions": ["Introduced the concept of kawaii vocalics to the field of HCI.", "Provided empirical data connecting voice characteristics with kawaii perception.", "Developed a basic method for manipulating voices to evoke kawaii responses."], "limitations": "Findings indicate a ceiling effect for certain voices in relation to kawaii vocalics, suggesting manipulation effectiveness may vary.", "keywords": ["kawaii", "vocalics", "HCI", "computer voices", "emotional response"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.06460", "pdf": "https://arxiv.org/pdf/2507.06460.pdf", "abs": "https://arxiv.org/abs/2507.06460", "title": "Ragged Blocks: Rendering Structured Text with Style", "authors": ["Sam Cohen", "Ravi Chugh"], "categories": ["cs.HC"], "comment": null, "summary": "Whether it be source code in a programming language, prose in natural\nlanguage, or otherwise, text is highly structured. Currently, text\nvisualizations are confined either to _flat, line-based_ decorations, which can\nconvey only limited information about textual structure, or _nested boxes_,\nwhich convey structure but often destroy the typographic layout of the\nunderlying text. We hypothesize that the lack of rich styling options limits\nthe kinds of information that are displayed alongside text, wherever it may be\ndisplayed.\n  In this paper, we show that it is possible to achieve arbitrarily nested\ndecorations while minimally disturbing the underlying typographic layout.\nSpecifically, we present a layout algorithm that generates _ragged blocks_, or\n_rocks_, which are rectilinear polygons that allow nested text to be compactly\nrendered even when styled with borders and padding.\n  We evaluate our layout algorithm in two ways. First, on a benchmark suite\ncomprising representative source code files in multiple programming languages,\nwe show that the (ragged block) layouts produced by our algorithm are\nsubstantially more compact than the (rectangular block) layouts produced by\nconventional techniques, when uniformly styling every element in the syntax\ntree with borders and padding. Second, through a small gallery of usage\nscenarios, we demonstrate how future code editors, word processors, and other\ndocument-rendering GUIs might convey rich semantic information through\ndomain-specific styling of ragged blocks.", "AI": {"tldr": "The paper presents a layout algorithm that enables the use of 'ragged blocks' for rich text visualizations while preserving typography.", "motivation": "To improve text visualizations beyond current flat decorations and nested boxes that disrupt typographic layout.", "method": "A layout algorithm is introduced that generates ragged blocks or rocks, allowing for compact nested text rendering with rich styling.", "result": "The ragged block layouts are shown to be significantly more compact than conventional rectangular layouts in a benchmark of various programming languages' source code.", "conclusion": "Future text editors and document-rendering GUIs can effectively convey rich semantic information through the styled ragged block layout.", "key_contributions": ["Introduction of a new layout algorithm for ragged blocks.", "Demonstration of substantial compactness improvements over traditional layouts.", "Presentation of use cases for enhanced semantic styling in document rendering."], "limitations": "", "keywords": ["text visualization", "layout algorithm", "ragged blocks", "semantic information", "document rendering"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.06483", "pdf": "https://arxiv.org/pdf/2507.06483.pdf", "abs": "https://arxiv.org/abs/2507.06483", "title": "Learning Japanese with Jouzu: Interaction Outcomes with Stylized Dialogue Fictional Agents", "authors": ["Zackary Rackauckas", "Julia Hirschberg"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "This study investigates how stylized, voiced agents shape user interaction in\na multimodal language learning environment. We conducted a mixed-methods\nevaluation of 54 participants interacting with anime-inspired characters\npowered by large language models and expressive text-to-speech synthesis. These\nagents responded in Japanese character language, offering users asynchronous,\nsemi-structured conversation in varying speech styles and emotional tones. We\nanalyzed user engagement patterns, perceived usability, emotional responses,\nand learning behaviors, with particular attention to how agent stylization\ninfluenced interaction across language proficiency levels and cultural\nbackgrounds. Our findings reveal that agent design, especially voice, persona,\nand linguistic style, substantially affected user experience, motivation, and\nstrategy. This work contributes to the understanding of affective, culturally\nstylized agents in human-agent interaction and offers guidance for designing\nmore engaging, socially responsive systems.", "AI": {"tldr": "This study explores the impact of stylized, voiced agents on user interaction in a multimodal language learning environment, highlighting how agent design influences user engagement and learning.", "motivation": "To investigate how animation and voice characteristics of powered agents affect user experience and learning in a language learning setting.", "method": "Mixed-methods evaluation involving 54 participants interacting with anime-inspired characters utilizing large language models and expressive speech synthesis.", "result": "Findings indicated that agent design significantly influenced user experience, motivation, and learning strategies, especially across different language proficiency levels and cultural backgrounds.", "conclusion": "The design of affective, stylized agents is crucial for enhancing user interaction and learning in technology-mediated environments.", "key_contributions": ["Exploration of how stylized agents affect user interaction and engagement.", "Insights into the role of voice, persona, and linguistic style on user experience.", "Guidance on designing socially responsive systems based on cultural awareness."], "limitations": "Limited to a specific cultural context (Japanese), which may affect generalizability to other languages or cultures.", "keywords": ["human-agent interaction", "language learning", "stylized agents", "user engagement", "expressive speech synthesis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06561", "pdf": "https://arxiv.org/pdf/2507.06561.pdf", "abs": "https://arxiv.org/abs/2507.06561", "title": "Towards Designing Social Interventions for Online Climate Change Denialism Discussions", "authors": ["Ruican zhong", "Shruti Phadke", "Beth Goldberg", "Tanushree Mitra"], "categories": ["cs.HC", "cs.SI"], "comment": null, "summary": "As conspiracy theories gain traction, it has become crucial to research\neffective intervention strategies that can foster evidence and science-based\ndiscussions in conspiracy theory communities online. This study presents a\nnovel framework using insider language to contest conspiracy theory ideology in\nclimate change denialism on Reddit. Focusing on discussions in two Reddit\ncommunities, our research investigates reactions to pro-social and\nevidence-based intervention messages for two cohorts of users: climate change\ndeniers and climate change supporters. Specifically, we combine manual and\ngenerative AI-based methods to craft intervention messages and deploy the\ninterventions as replies on Reddit posts and comments through transparently\nlabeled bot accounts. On the one hand, we find that evidence-based\ninterventions with neutral language foster positive engagement, encouraging\nopen discussions among believers of climate change denialism. On the other,\nclimate change supporters respond positively, actively participating and\npresenting additional evidence. Our study contributes valuable insights into\nthe process and challenges of automatically delivering interventions in\nconspiracy theory communities on social media, and helps inform future research\non social media interventions.", "AI": {"tldr": "Research on using insider language for effective interventions in conspiracy theory discussions about climate change on Reddit, combining AI and manual crafting methods.", "motivation": "The rise of conspiracy theories necessitates effective intervention strategies that promote science-based discussions online.", "method": "The study employs a novel framework utilizing insider language to contest conspiracy theory ideology, deploying manual and generative AI methods to craft intervention messages in Reddit communities.", "result": "Evidence-based interventions with neutral language enhance engagement, encouraging open discussions among climate change deniers, while climate change supporters contribute positively with additional evidence.", "conclusion": "The study offers insights into challenges and processes of automating interventions in conspiracy theory communities on social media, guiding future research in this area.", "key_contributions": ["Developing a framework using insider language for interventions", "Combining AI and manual methods for message crafting", "Demonstrating positive engagement through evidence-based strategies"], "limitations": "", "keywords": ["conspiracy theory", "climate change", "intervention strategies", "social media", "AI"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.06261", "pdf": "https://arxiv.org/pdf/2507.06261.pdf", "abs": "https://arxiv.org/abs/2507.06261", "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities", "authors": ["Gheorghe Comanici", "Eric Bieber", "Mike Schaekermann", "Ice Pasupat", "Noveen Sachdeva", "Inderjit Dhillon", "Marcel Blistein", "Ori Ram", "Dan Zhang", "Evan Rosen", "Luke Marris", "Sam Petulla", "Colin Gaffney", "Asaf Aharoni", "Nathan Lintz", "Tiago Cardal Pais", "Henrik Jacobsson", "Idan Szpektor", "Nan-Jiang Jiang", "Krishna Haridasan", "Ahmed Omran", "Nikunj Saunshi", "Dara Bahri", "Gaurav Mishra", "Eric Chu", "Toby Boyd", "Brad Hekman", "Aaron Parisi", "Chaoyi Zhang", "Kornraphop Kawintiranon", "Tania Bedrax-Weiss", "Oliver Wang", "Ya Xu", "Ollie Purkiss", "Uri Mendlovic", "Ilaï Deutel", "Nam Nguyen", "Adam Langley", "Flip Korn", "Lucia Rossazza", "Alexandre Ramé", "Sagar Waghmare", "Helen Miller", "Vaishakh Keshava", "Ying Jian", "Xiaofan Zhang", "Raluca Ada Popa", "Kedar Dhamdhere", "Blaž Bratanič", "Kyuyeun Kim", "Terry Koo", "Ferran Alet", "Yi-ting Chen", "Arsha Nagrani", "Hannah Muckenhirn", "Zhiyuan Zhang", "Corbin Quick", "Filip Pavetić", "Duc Dung Nguyen", "Joao Carreira", "Michael Elabd", "Haroon Qureshi", "Fabian Mentzer", "Yao-Yuan Yang", "Danielle Eisenbud", "Anmol Gulati", "Ellie Talius", "Eric Ni", "Sahra Ghalebikesabi", "Edouard Yvinec", "Alaa Saade", "Thatcher Ulrich", "Lorenzo Blanco", "Dan A. Calian", "Muhuan Huang", "Aäron van den Oord", "Naman Goyal", "Terry Chen", "Praynaa Rawlani", "Christian Schallhart", "Swachhand Lokhande", "Xianghong Luo", "Jyn Shan", "Ceslee Montgomery", "Victoria Krakovna", "Federico Piccinini", "Omer Barak", "Jingyu Cui", "Yiling Jia", "Mikhail Dektiarev", "Alexey Kolganov", "Shiyu Huang", "Zhe Chen", "Xingyu Wang", "Jessica Austin", "Peter de Boursac", "Evgeny Sluzhaev", "Frank Ding", "Huijian Li", "Surya Bhupatiraju", "Mohit Agarwal", "Sławek Kwasiborski", "Paramjit Sandhu", "Patrick Siegler", "Ahmet Iscen", "Eyal Ben-David", "Shiraz Butt", "Miltos Allamanis", "Seth Benjamin", "Robert Busa-Fekete", "Felix Hernandez-Campos", "Sasha Goldshtein", "Matt Dibb", "Weiyang Zhang", "Annie Marsden", "Carey Radebaugh", "Stephen Roller", "Abhishek Nayyar", "Jacob Austin", "Tayfun Terzi", "Bhargav Kanagal Shamanna", "Pete Shaw", "Aayush Singh", "Florian Luisier", "Artur Mendonça", "Vaibhav Aggarwal", "Larisa Markeeva", "Claudio Fantacci", "Sergey Brin", "HyunJeong Choe", "Guanyu Wang", "Hartwig Adam", "Avigail Dabush", "Tatsuya Kiyono", "Eyal Marcus", "Jeremy Cole", "Theophane Weber", "Hongrae Lee", "Ronny Huang", "Alex Muzio", "Leandro Kieliger", "Maigo Le", "Courtney Biles", "Long Le", "Archit Sharma", "Chengrun Yang", "Avery Lamp", "Dave Dopson", "Nate Hurley", "Katrina", "Xu", "Zhihao Shan", "Shuang Song", "Jiewen Tan", "Alexandre Senges", "George Zhang", "Chong You", "Yennie Jun", "David Raposo", "Susanna Ricco", "Xuan Yang", "Weijie Chen", "Prakhar Gupta", "Arthur Szlam", "Kevin Villela", "Chun-Sung Ferng", "Daniel Kasenberg", "Chen Liang", "Rui Zhu", "Arunachalam Narayanaswamy", "Florence Perot", "Paul Pucciarelli", "Anna Shekhawat", "Alexey Stern", "Rishikesh Ingale", "Stefani Karp", "Sanaz Bahargam", "Adrian Goedeckemeyer", "Jie Han", "Sicheng Li", "Andrea Tacchetti", "Dian Yu", "Abhishek Chakladar", "Zhiying Zhang", "Mona El Mahdy", "Xu Gao", "Dale Johnson", "Samrat Phatale", "AJ Piergiovanni", "Hyeontaek Lim", "Clement Farabet", "Carl Lebsack", "Theo Guidroz", "John Blitzer", "Nico Duduta", "David Madras", "Steve Li", "Daniel von Dincklage", "Xin Li", "Mahdis Mahdieh", "George Tucker", "Ganesh Jawahar", "Owen Xiao", "Danny Tarlow", "Robert Geirhos", "Noam Velan", "Daniel Vlasic", "Kalesha Bullard", "SK Park", "Nishesh Gupta", "Kellie Webster", "Ayal Hitron", "Jieming Mao", "Julian Eisenschlos", "Laurel Prince", "Nina D'Souza", "Kelvin Zheng", "Sara Nasso", "Gabriela Botea", "Carl Doersch", "Caglar Unlu", "Chris Alberti", "Alexey Svyatkovskiy", "Ankita Goel", "Krzysztof Choromanski", "Pan-Pan Jiang", "Richard Nguyen", "Four Flynn", "Daria Ćurko", "Peter Chen", "Nicholas Roth", "Kieran Milan", "Caleb Habtegebriel", "Shashi Narayan", "Michael Moffitt", "Jake Marcus", "Thomas Anthony", "Brendan McMahan", "Gowoon Cheon", "Ruibo Liu", "Megan Barnes", "Lukasz Lew", "Rebeca Santamaria-Fernandez", "Mayank Upadhyay", "Arjun Akula", "Arnar Mar Hrafnkelsson", "Alvaro Caceres", "Andrew Bunner", "Michal Sokolik", "Subha Puttagunta", "Lawrence Moore", "Berivan Isik", "Weilun Chen", "Jay Hartford", "Lawrence Chan", "Pradeep Shenoy", "Dan Holtmann-Rice", "Jane Park", "Fabio Viola", "Alex Salcianu", "Sujeevan Rajayogam", "Ian Stewart-Binks", "Zelin Wu", "Richard Everett", "Xi Xiong", "Pierre-Antoine Manzagol", "Gary Leung", "Carl Saroufim", "Bo Pang", "Dawid Wegner", "George Papamakarios", "Jennimaria Palomaki", "Helena Pankov", "Guangda Lai", "Guilherme Tubone", "Shubin Zhao", "Theofilos Strinopoulos", "Seth Neel", "Mingqiu Wang", "Joe Kelley", "Li Li", "Pingmei Xu", "Anitha Vijayakumar", "Andrea D'olimpio", "Omer Levy", "Massimo Nicosia", "Grigory Rozhdestvenskiy", "Ni Lao", "Sirui Xie", "Yash Katariya", "Jon Simon", "Sanjiv Kumar", "Florian Hartmann", "Michael Kilgore", "Jinhyuk Lee", "Aroma Mahendru", "Roman Ring", "Tom Hennigan", "Fiona Lang", "Colin Cherry", "David Steiner", "Dawsen Hwang", "Ray Smith", "Pidong Wang", "Jeremy Chen", "Ming-Hsuan Yang", "Sam Kwei", "Philippe Schlattner", "Donnie Kim", "Ganesh Poomal Girirajan", "Nikola Momchev", "Ayushi Agarwal", "Xingyi Zhou", "Ilkin Safarli", "Zachary Garrett", "AJ Pierigiovanni", "Sarthak Jauhari", "Alif Raditya Rochman", "Shikhar Vashishth", "Quan Yuan", "Christof Angermueller", "Jon Blanton", "Xinying Song", "Nitesh Bharadwaj Gundavarapu", "Thi Avrahami", "Maxine Deines", "Subhrajit Roy", "Manish Gupta", "Christopher Semturs", "Shobha Vasudevan", "Aditya Srikanth Veerubhotla", "Shriya Sharma", "Josh Jacob", "Zhen Yang", "Andreas Terzis", "Dan Karliner", "Auriel Wright", "Tania Rojas-Esponda", "Ashley Brown", "Abhijit Guha Roy", "Pawan Dogra", "Andrei Kapishnikov", "Peter Young", "Wendy Kan", "Vinodh Kumar Rajendran", "Maria Ivanova", "Salil Deshmukh", "Chia-Hua Ho", "Mike Kwong", "Stav Ginzburg", "Annie Louis", "KP Sawhney", "Slav Petrov", "Jing Xie", "Yunfei Bai", "Georgi Stoyanov", "Alex Fabrikant", "Rajesh Jayaram", "Yuqi Li", "Joe Heyward", "Justin Gilmer", "Yaqing Wang", "Radu Soricut", "Luyang Liu", "Qingnan Duan", "Jamie Hayes", "Maura O'Brien", "Gaurav Singh Tomar", "Sivan Eiger", "Bahar Fatemi", "Jeffrey Hui", "Catarina Barros", "Adaeze Chukwuka", "Alena Butryna", "Saksham Thakur", "Austin Huang", "Zhufeng Pan", "Haotian Tang", "Serkan Cabi", "Tulsee Doshi", "Michiel Bakker", "Sumit Bagri", "Ruy Ley-Wild", "Adam Lelkes", "Jennie Lees", "Patrick Kane", "David Greene", "Shimu Wu", "Jörg Bornschein", "Gabriela Surita", "Sarah Hodkinson", "Fangtao Li", "Chris Hidey", "Sébastien Pereira", "Sean Ammirati", "Phillip Lippe", "Adam Kraft", "Pu Han", "Sebastian Gerlach", "Zifeng Wang", "Liviu Panait", "Feng Han", "Brian Farris", "Yingying Bi", "Hannah DeBalsi", "Miaosen Wang", "Gladys Tyen", "James Cohan", "Susan Zhang", "Jarred Barber", "Da-Woon Chung", "Jaeyoun Kim", "Markus Kunesch", "Steven Pecht", "Nami Akazawa", "Abe Friesen", "James Lyon", "Ali Eslami", "Junru Wu", "Jie Tan", "Yue Song", "Ravi Kumar", "Chris Welty", "Ilia Akolzin", "Gena Gibson", "Sean Augenstein", "Arjun Pillai", "Nancy Yuen", "Du Phan", "Xin Wang", "Iain Barr", "Heiga Zen", "Nan Hua", "Casper Liu", "Jilei", "Wang", "Tanuj Bhatia", "Hao Xu", "Oded Elyada", "Pushmeet Kohli", "Mirek Olšák", "Ke Chen", "Azalia Mirhoseini", "Noam Shazeer", "Shoshana Jakobovits", "Maggie Tran", "Nolan Ramsden", "Tarun Bharti", "Fred Alcober", "Yunjie Li", "Shilpa Shetty", "Jing Chen", "Dmitry Kalashnikov", "Megha Nawhal", "Sercan Arik", "Hanwen Chen", "Michiel Blokzijl", "Shubham Gupta", "James Rubin", "Rigel Swavely", "Sophie Bridgers", "Ian Gemp", "Chen Su", "Arun Suggala", "Juliette Pluto", "Mary Cassin", "Alain Vaucher", "Kaiyang Ji", "Jiahao Cai", "Andrew Audibert", "Animesh Sinha", "David Tian", "Efrat Farkash", "Amy Hua", "Jilin Chen", "Duc-Hieu Tran", "Edward Loper", "Nicole Brichtova", "Lara McConnaughey", "Ballie Sandhu", "Robert Leland", "Doug DeCarlo", "Andrew Over", "James Huang", "Xing Wu", "Connie Fan", "Eric Li", "Yun Lei", "Deepak Sharma", "Cosmin Paduraru", "Luo Yu", "Matko Bošnjak", "Phuong Dao", "Min Choi", "Sneha Kudugunta", "Jakub Adamek", "Carlos Guía", "Ali Khodaei", "Jie Feng", "Wenjun Zeng", "David Welling", "Sandeep Tata", "Christina Butterfield", "Andrey Vlasov", "Seliem El-Sayed", "Swaroop Mishra", "Tara Sainath", "Shentao Yang", "RJ Skerry-Ryan", "Jeremy Shar", "Robert Berry", "Arunkumar Rajendran", "Arun Kandoor", "Andrea Burns", "Deepali Jain", "Tom Stone", "Wonpyo Park", "Shibo Wang", "Albin Cassirer", "Guohui Wang", "Hayato Kobayashi", "Sergey Rogulenko", "Vineetha Govindaraj", "Mikołaj Rybiński", "Nadav Olmert", "Colin Evans", "Po-Sen Huang", "Kelvin Xu", "Premal Shah", "Terry Thurk", "Caitlin Sikora", "Mu Cai", "Jin Xie", "Elahe Dabir", "Saloni Shah", "Norbert Kalb", "Carrie Zhang", "Shruthi Prabhakara", "Amit Sabne", "Artiom Myaskovsky", "Vikas Raunak", "Blanca Huergo", "Behnam Neyshabur", "Jon Clark", "Ye Zhang", "Shankar Krishnan", "Eden Cohen", "Dinesh Tewari", "James Lottes", "Yumeya Yamamori", "Hui", "Li", "Mohamed Elhawaty", "Ada Maksutaj Oflazer", "Adrià Recasens", "Sheryl Luo", "Duy Nguyen", "Taylor Bos", "Kalyan Andra", "Ana Salazar", "Ed Chi", "Jeongwoo Ko", "Matt Ginsberg", "Anders Andreassen", "Anian Ruoss", "Todor Davchev", "Elnaz Davoodi", "Chenxi Liu", "Min Kim", "Santiago Ontanon", "Chi Ming To", "Dawei Jia", "Rosemary Ke", "Jing Wang", "Anna Korsun", "Moran Ambar", "Ilya Kornakov", "Irene Giannoumis", "Toni Creswell", "Denny Zhou", "Yi Su", "Ishaan Watts", "Aleksandr Zaks", "Evgenii Eltyshev", "Ziqiang Feng", "Sidharth Mudgal", "Alex Kaskasoli", "Juliette Love", "Kingshuk Dasgupta", "Sam Shleifer", "Richard Green", "Sungyong Seo", "Chansoo Lee", "Dale Webster", "Prakash Shroff", "Ganna Raboshchuk", "Isabel Leal", "James Manyika", "Sofia Erell", "Daniel Murphy", "Zhisheng Xiao", "Anton Bulyenov", "Julian Walker", "Mark Collier", "Matej Kastelic", "Nelson George", "Sushant Prakash", "Sailesh Sidhwani", "Alexey Frolov", "Steven Hansen", "Petko Georgiev", "Tiberiu Sosea", "Chris Apps", "Aishwarya Kamath", "David Reid", "Emma Cooney", "Charlotte Magister", "Oriana Riva", "Alec Go", "Pu-Chin Chen", "Sebastian Krause", "Nir Levine", "Marco Fornoni", "Ilya Figotin", "Nick Roy", "Parsa Mahmoudieh", "Vladimir Magay", "Mukundan Madhavan", "Jin Miao", "Jianmo Ni", "Yasuhisa Fujii", "Ian Chou", "George Scrivener", "Zak Tsai", "Siobhan Mcloughlin", "Jeremy Selier", "Sandra Lefdal", "Jeffrey Zhao", "Abhijit Karmarkar", "Kushal Chauhan", "Shivanker Goel", "Zhaoyi Zhang", "Vihan Jain", "Parisa Haghani", "Mostafa Dehghani", "Jacob Scott", "Erin Farnese", "Anastasija Ilić", "Steven Baker", "Julia Pawar", "Li Zhong", "Josh Camp", "Yoel Zeldes", "Shravya Shetty", "Anand Iyer", "Vít Listík", "Jiaxian Guo", "Luming Tang", "Mark Geller", "Simon Bucher", "Yifan Ding", "Hongzhi Shi", "Carrie Muir", "Dominik Grewe", "Ramy Eskander", "Octavio Ponce", "Boqing Gong", "Derek Gasaway", "Samira Khan", "Umang Gupta", "Angelos Filos", "Weicheng Kuo", "Klemen Kloboves", "Jennifer Beattie", "Christian Wright", "Leon Li", "Alicia Jin", "Sandeep Mariserla", "Miteyan Patel", "Jens Heitkaemper", "Dilip Krishnan", "Vivek Sharma", "David Bieber", "Christian Frank", "John Lambert", "Paul Caron", "Martin Polacek", "Mai Giménez", "Himadri Choudhury", "Xing Yu", "Sasan Tavakkol", "Arun Ahuja", "Franz Och", "Rodolphe Jenatton", "Wojtek Skut", "Bryan Richter", "David Gaddy", "Andy Ly", "Misha Bilenko", "Megh Umekar", "Ethan Liang", "Martin Sevenich", "Mandar Joshi", "Hassan Mansoor", "Rebecca Lin", "Sumit Sanghai", "Abhimanyu Singh", "Xiaowei Li", "Sudheendra Vijayanarasimhan", "Zaheer Abbas", "Yonatan Bitton", "Hansa Srinivasan", "Manish Reddy Vuyyuru", "Alexander Frömmgen", "Yanhua Sun", "Ralph Leith", "Alfonso Castaño", "DJ Strouse", "Le Yan", "Austin Kyker", "Satish Kambala", "Mary Jasarevic", "Thibault Sellam", "Chao Jia", "Alexander Pritzel", "Raghavender R", "Huizhong Chen", "Natalie Clay", "Sudeep Gandhe", "Sean Kirmani", "Sayna Ebrahimi", "Hannah Kirkwood", "Jonathan Mallinson", "Chao Wang", "Adnan Ozturel", "Kuo Lin", "Shyam Upadhyay", "Vincent Cohen-Addad", "Sean Purser-haskell", "Yichong Xu", "Ebrahim Songhori", "Babi Seal", "Alberto Magni", "Almog Gueta", "Tingting Zou", "Guru Guruganesh", "Thais Kagohara", "Hung Nguyen", "Khalid Salama", "Alejandro Cruzado Ruiz", "Justin Frye", "Zhenkai Zhu", "Matthias Lochbrunner", "Simon Osindero", "Wentao Yuan", "Lisa Lee", "Aman Prasad", "Lam Nguyen Thiet", "Daniele Calandriello", "Victor Stone", "Qixuan Feng", "Han Ke", "Maria Voitovich", "Geta Sampemane", "Lewis Chiang", "Ling Wu", "Alexander Bykovsky", "Matt Young", "Luke Vilnis", "Ishita Dasgupta", "Aditya Chawla", "Qin Cao", "Bowen Liang", "Daniel Toyama", "Szabolcs Payrits", "Anca Stefanoiu", "Dimitrios Vytiniotis", "Ankesh Anand", "Tianxiao Shen", "Blagoj Mitrevski", "Michael Tschannen", "Sreenivas Gollapudi", "Aishwarya P S", "José Leal", "Zhe Shen", "Han Fu", "Wei Wang", "Arvind Kannan", "Doron Kukliansky", "Sergey Yaroshenko", "Svetlana Grant", "Umesh Telang", "David Wood", "Alexandra Chronopoulou", "Alexandru Ţifrea", "Tao Zhou", "Tony", "Nguy\\~ên", "Muge Ersoy", "Anima Singh", "Meiyan Xie", "Emanuel Taropa", "Woohyun Han", "Eirikur Agustsson", "Andrei Sozanschi", "Hui Peng", "Alex Chen", "Yoel Drori", "Efren Robles", "Yang Gao", "Xerxes Dotiwalla", "Ying Chen", "Anudhyan Boral", "Alexei Bendebury", "John Nham", "Chris Tar", "Luis Castro", "Jiepu Jiang", "Canoee Liu", "Felix Halim", "Jinoo Baek", "Andy Wan", "Jeremiah Liu", "Yuan Cao", "Shengyang Dai", "Trilok Acharya", "Ruoxi Sun", "Fuzhao Xue", "Saket Joshi", "Morgane Lustman", "Yongqin Xian", "Rishabh Joshi", "Deep Karkhanis", "Nora Kassner", "Jamie Hall", "Xiangzhuo Ding", "Gan Song", "Gang Li", "Chen Zhu", "Yana Kulizhskaya", "Bin Ni", "Alexey Vlaskin", "Solomon Demmessie", "Lucio Dery", "Salah Zaiem", "Yanping Huang", "Cindy Fan", "Felix Gimeno", "Ananth Balashankar", "Koji Kojima", "Hagai Taitelbaum", "Maya Meng", "Dero Gharibian", "Sahil Singla", "Wei Chen", "Ambrose Slone", "Guanjie Chen", "Sujee Rajayogam", "Max Schumacher", "Suyog Kotecha", "Rory Blevins", "Qifei Wang", "Mor Hazan Taege", "Alex Morris", "Xin Liu", "Fayaz Jamil", "Richard Zhang", "Pratik Joshi", "Ben Ingram", "Tyler Liechty", "Ahmed Eleryan", "Scott Baird", "Alex Grills", "Gagan Bansal", "Shan Han", "Kiran Yalasangi", "Shawn Xu", "Majd Al Merey", "Isabel Gao", "Felix Weissenberger", "Igor Karpov", "Robert Riachi", "Ankit Anand", "Gautam Prasad", "Kay Lamerigts", "Reid Hayes", "Jamie Rogers", "Mandy Guo", "Ashish Shenoy", "Qiong", "Hu", "Kyle He", "Yuchen Liu", "Polina Zablotskaia", "Sagar Gubbi", "Yifan Chang", "Jay Pavagadhi", "Kristian Kjems", "Archita Vadali", "Diego Machado", "Yeqing Li", "Renshen Wang", "Dipankar Ghosh", "Aahil Mehta", "Dana Alon", "George Polovets", "Alessio Tonioni", "Nate Kushman", "Joel D'sa", "Lin Zhuo", "Allen Wu", "Rohin Shah", "John Youssef", "Jiayu Ye", "Justin Snyder", "Karel Lenc", "Senaka Buthpitiya", "Matthew Tung", "Jichuan Chang", "Tao Chen", "David Saxton", "Jenny Lee", "Lydia Lihui Zhang", "James Qin", "Prabakar Radhakrishnan", "Maxwell Chen", "Piotr Ambroszczyk", "Metin Toksoz-Exley", "Yan Zhong", "Nitzan Katz", "Brendan O'Donoghue", "Tamara von Glehn", "Adi Gerzi Rosenthal", "Aga Świetlik", "Xiaokai Zhao", "Nick Fernando", "Jinliang Wei", "Jieru Mei", "Sergei Vassilvitskii", "Diego Cedillo", "Pranjal Awasthi", "Hui Zheng", "Koray Kavukcuoglu", "Itay Laish", "Joseph Pagadora", "Marc Brockschmidt", "Christopher A. Choquette-Choo", "Arunkumar Byravan", "Yifeng Lu", "Xu Chen", "Mia Chen", "Kenton Lee", "Rama Pasumarthi", "Sijal Bhatnagar", "Aditya Shah", "Qiyin Wu", "Zhuoyuan Chen", "Zack Nado", "Bartek Perz", "Zixuan Jiang", "David Kao", "Ganesh Mallya", "Nino Vieillard", "Lantao Mei", "Sertan Girgin", "Mandy Jordan", "Yeongil Ko", "Alekh Agarwal", "Yaxin Liu", "Yasemin Altun", "Raoul de Liedekerke", "Anastasios Kementsietsidis", "Daiyi Peng", "Dangyi Liu", "Utku Evci", "Peter Humphreys", "Austin Tarango", "Xiang Deng", "Yoad Lewenberg", "Kevin Aydin", "Chengda Wu", "Bhavishya Mittal", "Tsendsuren Munkhdalai", "Kleopatra Chatziprimou", "Rodrigo Benenson", "Uri First", "Xiao Ma", "Jinning Li", "Armand Joulin", "Hamish Tomlinson", "Tingnan Zhang", "Milad Nasr", "Zhi Hong", "Michaël Sander", "Lisa Anne Hendricks", "Anuj Sharma", "Andrew Bolt", "Eszter Vértes", "Jiri Simsa", "Tomer Levinboim", "Olcan Sercinoglu", "Divyansh Shukla", "Austin Wu", "Craig Swanson", "Danny Vainstein", "Fan Bu", "Bo Wang", "Ryan Julian", "Charles Yoon", "Sergei Lebedev", "Antonious Girgis", "Bernd Bandemer", "David Du", "Todd Wang", "Xi Chen", "Ying Xiao", "Peggy Lu", "Natalie Ha", "Vlad Ionescu", "Simon Rowe", "Josip Matak", "Federico Lebron", "Andreas Steiner", "Lalit Jain", "Manaal Faruqui", "Nicolas Lacasse", "Georgie Evans", "Neesha Subramaniam", "Dean Reich", "Giulia Vezzani", "Aditya Pandey", "Joe Stanton", "Tianhao Zhou", "Liam McCafferty", "Henry Griffiths", "Verena Rieser", "Soheil Hassas Yeganeh", "Eleftheria Briakou", "Lu Huang", "Zichuan Wei", "Liangchen Luo", "Erik Jue", "Gabby Wang", "Victor Cotruta", "Myriam Khan", "Jongbin Park", "Qiuchen Guo", "Peiran Li", "Rong Rong", "Diego Antognini", "Anastasia Petrushkina", "Chetan Tekur", "Eli Collins", "Parul Bhatia", "Chester Kwak", "Wenhu Chen", "Arvind Neelakantan", "Immanuel Odisho", "Sheng Peng", "Vincent Nallatamby", "Vaibhav Tulsyan", "Fabian Pedregosa", "Peng Xu", "Raymond Lin", "Yulong Wang", "Emma Wang", "Sholto Douglas", "Reut Tsarfaty", "Elena Gribovskaya", "Renga Aravamudhan", "Manu Agarwal", "Mara Finkelstein", "Qiao Zhang", "Elizabeth Cole", "Phil Crone", "Sarmishta Velury", "Anil Das", "Chris Sauer", "Luyao Xu", "Danfeng Qin", "Chenjie Gu", "Dror Marcus", "CJ Zheng", "Wouter Van Gansbeke", "Sobhan Miryoosefi", "Haitian Sun", "YaGuang Li", "Charlie Chen", "Jae Yoo", "Pavel Dubov", "Alex Tomala", "Adams Yu", "Paweł Wesołowski", "Alok Gunjan", "Eddie Cao", "Jiaming Luo", "Nikhil Sethi", "Arkadiusz Socala", "Laura Graesser", "Tomas Kocisky", "Arturo BC", "Minmin Chen", "Edward Lee", "Sophie Wang", "Weize Kong", "Qiantong Xu", "Nilesh Tripuraneni", "Yiming Li", "Xinxin Yu", "Allen Porter", "Paul Voigtlaender", "Biao Zhang", "Arpi Vezer", "Sarah York", "Qing Wei", "Geoffrey Cideron", "Mark Kurzeja", "Seungyeon Kim", "Benny Li", "Angéline Pouget", "Hyo Lee", "Kaspar Daugaard", "Yang Li", "Dave Uthus", "Aditya Siddhant", "Paul Cavallaro", "Sriram Ganapathy", "Maulik Shah", "Rolf Jagerman", "Jeff Stanway", "Piermaria Mendolicchio", "Li Xiao", "Kayi Lee", "Tara Thompson", "Shubham Milind Phal", "Jason Chase", "Sun Jae Lee", "Adrian N Reyes", "Disha Shrivastava", "Zhen Qin", "Roykrong Sukkerd", "Seth Odoom", "Lior Madmoni", "John Aslanides", "Jonathan Herzig", "Elena Pochernina", "Sheng Zhang", "Parker Barnes", "Daisuke Ikeda", "Qiujia Li", "Shuo-yiin Chang", "Shakir Mohamed", "Jim Sproch", "Richard Powell", "Bidisha Samanta", "Domagoj Ćevid", "Anton Kovsharov", "Shrestha Basu Mallick", "Srinivas Tadepalli", "Anne Zheng", "Kareem Ayoub", "Andreas Noever", "Christian Reisswig", "Zhuo Xu", "Junhyuk Oh", "Martin Matysiak", "Tim Blyth", "Shereen Ashraf", "Julien Amelot", "Boone Severson", "Michele Bevilacqua", "Motoki Sano", "Ethan Dyer", "Ofir Roval", "Anu Sinha", "Yin Zhong", "Sagi Perel", "Tea Sabolić", "Johannes Mauerer", "Willi Gierke", "Mauro Verzetti", "Rodrigo Cabrera", "Alvin Abdagic", "Steven Hemingray", "Austin Stone", "Jong Lee", "Farooq Ahmad", "Karthik Raman", "Lior Shani", "Jonathan Lai", "Orhan Firat", "Nathan Waters", "Eric Ge", "Mo Shomrat", "Himanshu Gupta", "Rajeev Aggarwal", "Tom Hudson", "Bill Jia", "Simon Baumgartner", "Palak Jain", "Joe Kovac", "Junehyuk Jung", "Ante Žužul", "Will Truong", "Morteza Zadimoghaddam", "Songyou Peng", "Marco Liang", "Rachel Sterneck", "Balaji Lakshminarayanan", "Machel Reid", "Oliver Woodman", "Tong Zhou", "Jianling Wang", "Vincent Coriou", "Arjun Narayanan", "Jay Hoover", "Yenai Ma", "Apoorv Jindal", "Clayton Sanford", "Doug Reid", "Swaroop Ramaswamy", "Alex Kurakin", "Roland Zimmermann", "Yana Lunts", "Dragos Dena", "Zalán Borsos", "Vered Cohen", "Shujian Zhang", "Will Grathwohl", "Robert Dadashi", "Morgan Redshaw", "Joshua Kessinger", "Julian Odell", "Silvano Bonacina", "Zihang Dai", "Grace Chen", "Ayush Dubey", "Pablo Sprechmann", "Mantas Pajarskas", "Wenxuan Zhou", "Niharika Ahuja", "Tara Thomas", "Martin Nikoltchev", "Matija Kecman", "Bharath Mankalale", "Andrey Ryabtsev", "Jennifer She", "Christian Walder", "Jiaming Shen", "Lu Li", "Carolina Parada", "Sheena Panthaplackel", "Okwan Kwon", "Matt Lawlor", "Utsav Prabhu", "Yannick Schroecker", "Marc'aurelio Ranzato", "Pete Blois", "Iurii Kemaev", "Ting Yu", "Dmitry", "Lepikhin", "Hao Xiong", "Sahand Sharifzadeh", "Oleaser Johnson", "Jeremiah Willcock", "Rui Yao", "Greg Farquhar", "Sujoy Basu", "Hidetoshi Shimokawa", "Nina Anderson", "Haiguang Li", "Khiem Pham", "Yizhong Liang", "Sebastian Borgeaud", "Alexandre Moufarek", "Hideto Kazawa", "Blair Kutzman", "Marcin Sieniek", "Sara Smoot", "Ruth Wang", "Natalie Axelsson", "Nova Fallen", "Prasha Sundaram", "Yuexiang Zhai", "Varun Godbole", "Petros Maniatis", "Alek Wang", "Ilia Shumailov", "Santhosh Thangaraj", "Remi Crocker", "Nikita Gupta", "Gang Wu", "Phil Chen", "Gellért Weisz", "Celine Smith", "Mojtaba Seyedhosseini", "Boya Fang", "Xiyang Luo", "Roey Yogev", "Zeynep Cankara", "Andrew Hard", "Helen Ran", "Rahul Sukthankar", "George Necula", "Gaël Liu", "Honglong Cai", "Praseem Banzal", "Daniel Keysers", "Sanjay Ghemawat", "Connie Tao", "Emma Dunleavy", "Aditi Chaudhary", "Wei Li", "Maciej Mikuła", "Chen-Yu Lee", "Tiziana Refice", "Krishna Somandepalli", "Alexandre Fréchette", "Dan Bahir", "John Karro", "Keith Rush", "Sarah Perrin", "Bill Rosgen", "Xiaomeng Yang", "Clara Huiyi Hu", "Mahmoud Alnahlawi", "Justin Mao-Jones", "Roopal Garg", "Hoang Nguyen", "Bat-Orgil Batsaikhan", "Iñaki Iturrate", "Anselm Levskaya", "Avi Singh", "Ashyana Kachra", "Tony Lu", "Denis Petek", "Zheng Xu", "Mark Graham", "Lukas Zilka", "Yael Karov", "Marija Kostelac", "Fangyu Liu", "Yaohui Guo", "Weiyue Wang", "Bernd Bohnet", "Emily Pitler", "Tony Bruguier", "Keisuke Kinoshita", "Chrysovalantis Anastasiou", "Nilpa Jha", "Ting Liu", "Jerome Connor", "Phil Wallis", "Philip Pham", "Eric Bailey", "Shixin Li", "Heng-Tze Cheng", "Sally Ma", "Haiqiong Li", "Akanksha Maurya", "Kate Olszewska", "Manfred Warmuth", "Christy Koh", "Dominik Paulus", "Siddhartha Reddy Jonnalagadda", "Enrique Piqueras", "Ali Elqursh", "Geoff Brown", "Hadar Shemtov", "Loren Maggiore", "Fei Xia", "Ryan Foley", "Beka Westberg", "George van den Driessche", "Livio Baldini Soares", "Arjun Kar", "Michael Quinn", "Siqi Zuo", "Jialin Wu", "Kyle Kastner", "Anna Bortsova", "Aijun Bai", "Ales Mikhalap", "Luowei Zhou", "Jennifer Brennan", "Vinay Ramasesh", "Honglei Zhuang", "John Maggs", "Johan Schalkwyk", "Yuntao Xu", "Hui Huang", "Andrew Howard", "Sasha Brown", "Linting Xue", "Gloria Shen", "Brian Albert", "Neha Jha", "Daniel Zheng", "Varvara Krayvanova", "Spurthi Amba Hombaiah", "Olivier Lacombe", "Gautam Vasudevan", "Dan Graur", "Tian Xie", "Meet Gandhi", "Bangju Wang", "Dustin Zelle", "Harman Singh", "Dahun Kim", "Sébastien Cevey", "Victor Ungureanu", "Natasha Noy", "Fei Liu", "Annie Xie", "Fangxiaoyu Feng", "Katerina Tsihlas", "Daniel Formoso", "Neera Vats", "Quentin Wellens", "Yinan Wang", "Niket Kumar Bhumihar", "Samrat Ghosh", "Matt Hoffman", "Tom Lieber", "Oran Lang", "Kush Bhatia", "Tom Paine", "Aroonalok Pyne", "Ronny Votel", "Madeleine Clare Elish", "Benoit Schillings", "Alex Panagopoulos", "Haichuan Yang", "Adam Raveret", "Zohar Yahav", "Shuang Liu", "Warren Chen", "Dalia El Badawy", "Nishant Agrawal", "Mohammed Badawi", "Mahdi Mirzazadeh", "Carla Bromberg", "Fan Ye", "Chang Liu", "Tatiana Sholokhova", "George-Cristian Muraru", "Gargi Balasubramaniam", "Jonathan Malmaud", "Alen Carin", "Danilo Martins", "Irina Jurenka", "Pankil Botadra", "Dave Lacey", "Richa Singh", "Mariano Schain", "Dan Zheng", "Isabelle Guyon", "Victor Lavrenko", "Seungji Lee", "Xiang Zhou", "Demis Hassabis", "Jeshwanth Challagundla", "Derek Cheng", "Nikhil Mehta", "Matthew Mauger", "Michela Paganini", "Pushkar Mishra", "Kate Lee", "Zhang Li", "Lexi Baugher", "Ondrej Skopek", "Max Chang", "Amir Zait", "Gaurav Menghani", "Lizzetth Bellot", "Guangxing Han", "Jean-Michel Sarr", "Sharat Chikkerur", "Himanshu Sahni", "Rohan Anil", "Arun Narayanan", "Chandu Thekkath", "Daniele Pighin", "Hana Strejček", "Marko Velic", "Fred Bertsch", "Manuel Tragut", "Keran Rong", "Alicia Parrish", "Kai Bailey", "Jiho Park", "Isabela Albuquerque", "Abhishek Bapna", "Rajesh Venkataraman", "Alec Kosik", "Johannes Griesser", "Zhiwei Deng", "Alek Andreev", "Qingyun Dou", "Kevin Hui", "Fanny Wei", "Xiaobin Yu", "Lei Shu", "Avia Aharon", "David Barker", "Badih Ghazi", "Sebastian Flennerhag", "Chris Breaux", "Yuchuan Liu", "Matthew Bilotti", "Josh Woodward", "Uri Alon", "Stephanie Winkler", "Tzu-Kuo Huang", "Kostas Andriopoulos", "João Gabriel Oliveira", "Penporn Koanantakool", "Berkin Akin", "Michael Wunder", "Cicero Nogueira dos Santos", "Mohammad Hossein Bateni", "Lin Yang", "Dan Horgan", "Beer Changpinyo", "Keyvan Amiri", "Min Ma", "Dayeong Lee", "Lihao Liang", "Anirudh Baddepudi", "Tejasi Latkar", "Raia Hadsell", "Jun Xu", "Hairong Mu", "Michael Han", "Aedan Pope", "Snchit Grover", "Frank Kim", "Ankit Bhagatwala", "Guan Sun", "Yamini Bansal", "Amir Globerson", "Alireza Nazari", "Samira Daruki", "Hagen Soltau", "Jane Labanowski", "Laurent El Shafey", "Matt Harvey", "Yanif Ahmad", "Elan Rosenfeld", "William Kong", "Etienne Pot", "Yi-Xuan Tan", "Aurora Wei", "Victoria Langston", "Marcel Prasetya", "Petar Veličković", "Richard Killam", "Robin Strudel", "Darren Ni", "Zhenhai Zhu", "Aaron Archer", "Kavya Kopparapu", "Lynn Nguyen", "Emilio Parisotto", "Hussain Masoom", "Sravanti Addepalli", "Jordan Grimstad", "Hexiang Hu", "Joss Moore", "Avinatan Hassidim", "Le Hou", "Mukund Raghavachari", "Jared Lichtarge", "Adam R. Brown", "Hilal Dib", "Natalia Ponomareva", "Justin Fu", "Yujing Zhang", "Altaf Rahman", "Joana Iljazi", "Edouard Leurent", "Gabriel Dulac-Arnold", "Cosmo Du", "Chulayuth Asawaroengchai", "Larry Jin", "Ela Gruzewska", "Ziwei Ji", "Benigno Uria", "Daniel De Freitas", "Paul Barham", "Lauren Beltrone", "Víctor Campos", "Jun Yan", "Neel Kovelamudi", "Arthur Nguyen", "Elinor Davies", "Zhichun Wu", "Zoltan Egyed", "Kristina Toutanova", "Nithya Attaluri", "Hongliang Fei", "Peter Stys", "Siddhartha Brahma", "Martin Izzard", "Siva Velusamy", "Scott Lundberg", "Vincent Zhuang", "Kevin Sequeira", "Adam Santoro", "Ehsan Amid", "Ophir Aharoni", "Shuai Ye", "Mukund Sundararajan", "Lijun Yu", "Yu-Cheng Ling", "Stephen Spencer", "Hugo Song", "Josip Djolonga", "Christo Kirov", "Sonal Gupta", "Alessandro Bissacco", "Clemens Meyer", "Mukul Bhutani", "Andrew Dai", "Weiyi Wang", "Siqi Liu", "Ashwin Sreevatsa", "Qijun Tan", "Maria Wang", "Lucy Kim", "Yicheng Wang", "Alex Irpan", "Yang Xiao", "Stanislav Fort", "Yifan He", "Alex Gurney", "Bryan Gale", "Yue Ma", "Monica Roy", "Viorica Patraucean", "Taylan Bilal", "Golnaz Ghiasi", "Anahita Hosseini", "Melvin Johnson", "Zhuowan Li", "Yi Tay", "Benjamin Beyret", "Katie Millican", "Josef Broder", "Mayank Lunayach", "Danny Swisher", "Eugen Vušak", "David Parkinson", "MH Tessler", "Adi Mayrav Gilady", "Richard Song", "Allan Dafoe", "Yves Raimond", "Masa Yamaguchi", "Itay Karo", "Elizabeth Nielsen", "Kevin Kilgour", "Mike Dusenberry", "Rajiv Mathews", "Jiho Choi", "Siyuan Qiao", "Harsh Mehta", "Sahitya Potluri", "Chris Knutsen", "Jialu Liu", "Tat Tan", "Kuntal Sengupta", "Keerthana Gopalakrishnan", "Abodunrinwa Toki", "Mencher Chiang", "Mike Burrows", "Grace Vesom", "Zafarali Ahmed", "Ilia Labzovsky", "Siddharth Vashishtha", "Preeti Singh", "Ankur Sharma", "Ada Ma", "Jinyu Xie", "Pranav Talluri", "Hannah Forbes-Pollard", "Aarush Selvan", "Joel Wee", "Loic Matthey", "Tom Funkhouser", "Parthasarathy Gopavarapu", "Lev Proleev", "Cheng Li", "Matt Thomas", "Kashyap Kolipaka", "Zhipeng Jia", "Ashwin Kakarla", "Srinivas Sunkara", "Joan Puigcerver", "Suraj Satishkumar Sheth", "Emily Graves", "Chen Wang", "Sadh MNM Khan", "Kai Kang", "Shyamal Buch", "Fred Zhang", "Omkar Savant", "David Soergel", "Kevin Lee", "Linda Friso", "Xuanyi Dong", "Rahul Arya", "Shreyas Chandrakaladharan", "Connor Schenck", "Greg Billock", "Tejas Iyer", "Anton Bakalov", "Leslie Baker", "Alex Ruiz", "Angad Chandorkar", "Trieu Trinh", "Matt Miecnikowski", "Yanqi Zhou", "Yangsibo Huang", "Jiazhong Nie", "Ali Shah", "Ashish Thapliyal", "Sam Haves", "Lun Wang", "Uri Shaham", "Patrick Morris-Suzuki", "Soroush Radpour", "Leonard Berrada", "Thomas Strohmann", "Chaochao Yan", "Jingwei Shen", "Sonam Goenka", "Tris Warkentin", "Petar Dević", "Dan Belov", "Albert Webson", "Madhavi Yenugula", "Puranjay Datta", "Jerry Chang", "Nimesh Ghelani", "Aviral Kumar", "Vincent Perot", "Jessica Lo", "Yang Song", "Herman Schmit", "Jianmin Chen", "Vasilisa Bashlovkina", "Xiaoyue Pan", "Diana Mincu", "Paul Roit", "Isabel Edkins", "Andy Davis", "Yujia Li", "Ben Horn", "Xinjian Li", "Pradeep Kumar S", "Eric Doi", "Wanzheng Zhu", "Sri Gayatri Sundara Padmanabhan", "Siddharth Verma", "Jasmine Liu", "Heng Chen", "Mihajlo Velimirović", "Malcolm Reynolds", "Priyanka Agrawal", "Nick Sukhanov", "Abhinit Modi", "Siddharth Goyal", "John Palowitch", "Nima Khajehnouri", "Wing Lowe", "David Klinghoffer", "Sharon Silver", "Vinh Tran", "Candice Schumann", "Francesco Piccinno", "Xi Liu", "Mario Lučić", "Xiaochen Yang", "Sandeep Kumar", "Ajay Kannan", "Ragha Kotikalapudi", "Mudit Bansal", "Fabian Fuchs", "Javad Hosseini", "Abdelrahman Abdelhamed", "Dawn Bloxwich", "Tianhe Yu", "Ruoxin Sang", "Gregory Thornton", "Karan Gill", "Yuchi Liu", "Virat Shejwalkar", "Jason Lin", "Zhipeng Yan", "Kehang Han", "Thomas Buschmann", "Michael Pliskin", "Zhi Xing", "Susheel Tatineni", "Junlin Zhang", "Sissie Hsiao", "Gavin Buttimore", "Marcus Wu", "Zefei Li", "Geza Kovacs", "Legg Yeung", "Tao Huang", "Aaron Cohen", "Bethanie Brownfield", "Averi Nowak", "Mikel Rodriguez", "Tianze Shi", "Hado van Hasselt", "Kevin Cen", "Deepanway Ghoshal", "Kushal Majmundar", "Weiren Yu", "Warren", "Chen", "Danila Sinopalnikov", "Hao Zhang", "Vlado Galić", "Di Lu", "Zeyu Zheng", "Maggie Song", "Gary Wang", "Gui Citovsky", "Swapnil Gawde", "Isaac Galatzer-Levy", "David Silver", "Ivana Balazevic", "Dipanjan Das", "Kingshuk Majumder", "Yale Cong", "Praneet Dutta", "Dustin Tran", "Hui Wan", "Junwei Yuan", "Daniel Eppens", "Alanna Walton", "Been Kim", "Harry Ragan", "James Cobon-Kerr", "Lu Liu", "Weijun Wang", "Bryce Petrini", "Jack Rae", "Rakesh Shivanna", "Yan Xiong", "Chace Lee", "Pauline Coquinot", "Yiming Gu", "Lisa Patel", "Blake Hechtman", "Aviel Boag", "Orion Jankowski", "Alex Wertheim", "Alex Lee", "Paul Covington", "Hila Noga", "Sam Sobell", "Shanthal Vasanth", "William Bono", "Chirag Nagpal", "Wei Fan", "Xavier Garcia", "Kedar Soparkar", "Aybuke Turker", "Nathan Howard", "Sachit Menon", "Yuankai Chen", "Vikas Verma", "Vladimir Pchelin", "Harish Rajamani", "Valentin Dalibard", "Ana Ramalho", "Yang Guo", "Kartikeya Badola", "Seojin Bang", "Nathalie Rauschmayr", "Julia Proskurnia", "Sudeep Dasari", "Xinyun Chen", "Mikhail Sushkov", "Anja Hauth", "Pauline Sho", "Abhinav Singh", "Bilva Chandra", "Allie Culp", "Max Dylla", "Olivier Bachem", "James Besley", "Heri Zhao", "Timothy Lillicrap", "Wei Wei", "Wael Al Jishi", "Ning Niu", "Alban Rrustemi", "Raphaël Lopez Kaufman", "Ryan Poplin", "Jewel Zhao", "Minh Truong", "Shikhar Bharadwaj", "Ester Hlavnova", "Eli Stickgold", "Cordelia Schmid", "Georgi Stephanov", "Zhaoqi Leng", "Frederick Liu", "Léonard Hussenot", "Shenil Dodhia", "Juliana Vicente Franco", "Lesley Katzen", "Abhanshu Sharma", "Sarah Cogan", "Zuguang Yang", "Aniket Ray", "Sergi Caelles", "Shen Yan", "Ravin Kumar", "Daniel Gillick", "Renee Wong", "Joshua Ainslie", "Jonathan Hoech", "Séb Arnold", "Dan Abolafia", "Anca Dragan", "Ben Hora", "Grace Hu", "Alexey Guseynov", "Yang Lu", "Chas Leichner", "Jinmeng Rao", "Abhimanyu Goyal", "Nagabhushan Baddi", "Daniel Hernandez Diaz", "Tim McConnell", "Max Bain", "Jake Abernethy", "Qiqi Yan", "Rylan Schaeffer", "Paul Vicol", "Will Thompson", "Montse Gonzalez Arenas", "Mathias Bellaiche", "Pablo Barrio", "Stefan Zinke", "Riccardo Patana", "Pulkit Mehta", "JK Kearns", "Avraham Ruderman", "Scott Pollom", "David D'Ambrosio", "Cath Hope", "Yang Yu", "Andrea Gesmundo", "Kuang-Huei Lee", "Aviv Rosenberg", "Yiqian Zhou", "Yaoyiran Li", "Drew Garmon", "Yonghui Wu", "Safeen Huda", "Gil Fidel", "Martin Baeuml", "Jian Li", "Phoebe Kirk", "Rhys May", "Tao Tu", "Sara Mc Carthy", "Toshiyuki Fukuzawa", "Miranda Aperghis", "Chih-Kuan Yeh", "Toshihiro Yoshino", "Bo Li", "Austin Myers", "Kaisheng Yao", "Ben Limonchik", "Changwan Ryu", "Rohun Saxena", "Alex Goldin", "Ruizhe Zhao", "Rocky Rhodes", "Tao Zhu", "Divya Tyam", "Heidi Howard", "Nathan Byrd", "Hongxu Ma", "Yan Wu", "Ryan Mullins", "Qingze Wang", "Aida Amini", "Sebastien Baur", "Yiran Mao", "Subhashini Venugopalan", "Will Song", "Wen Ding", "Paul Collins", "Sashank Reddi", "Megan Shum", "Andrei Rusu", "Luisa Zintgraf", "Kelvin Chan", "Sheela Goenka", "Mathieu Blondel", "Michael Collins", "Renke Pan", "Marissa Giustina", "Nikolai Chinaev", "Christian Schuler", "Ce Zheng", "Jonas Valfridsson", "Alyssa Loo", "Alex Yakubovich", "Jamie Smith", "Tao Jiang", "Rich Munoz", "Gabriel Barcik", "Rishabh Bansal", "Mingyao Yang", "Yilun Du", "Pablo Duque", "Mary Phuong", "Alexandra Belias", "Kunal Lad", "Zeyu Liu", "Tal Schuster", "Karthik Duddu", "Jieru Hu", "Paige Kunkle", "Matthew Watson", "Jackson Tolins", "Josh Smith", "Denis Teplyashin", "Garrett Bingham", "Marvin Ritter", "Marco Andreetto", "Divya Pitta", "Mohak Patel", "Shashank Viswanadha", "Trevor Strohman", "Catalin Ionescu", "Jincheng Luo", "Yogesh Kalley", "Jeremy Wiesner", "Dan Deutsch", "Derek Lockhart", "Peter Choy", "Rumen Dangovski", "Chawin Sitawarin", "Cat Graves", "Tanya Lando", "Joost van Amersfoort", "Ndidi Elue", "Zhouyuan Huo", "Pooya Moradi", "Jean Tarbouriech", "Henryk Michalewski", "Wenting Ye", "Eunyoung Kim", "Alex Druinsky", "Florent Altché", "Xinyi Chen", "Artur Dwornik", "Da-Cheng Juan", "Rivka Moroshko", "Horia Toma", "Jarrod Kahn", "Hai Qian", "Maximilian Sieb", "Irene Cai", "Roman Goldenberg", "Praneeth Netrapalli", "Sindhu Raghuram", "Yuan Gong", "Lijie Fan", "Evan Palmer", "Yossi Matias", "Valentin Gabeur", "Shreya Pathak", "Tom Ouyang", "Don Metzler", "Geoff Bacon", "Srinivasan Venkatachary", "Sridhar Thiagarajan", "Alex Cullum", "Eran Ofek", "Vytenis Sakenas", "Mohamed Hammad", "Cesar Magalhaes", "Mayank Daswani", "Oscar Chang", "Ashok Popat", "Ruichao Li", "Komal Jalan", "Yanhan Hou", "Josh Lipschultz", "Antoine He", "Wenhao Jia", "Pier Giuseppe Sessa", "Prateek Kolhar", "William Wong", "Sumeet Singh", "Lukas Haas", "Jay Whang", "Hanna Klimczak-Plucińska", "Georges Rotival", "Grace Chung", "Yiqing Hua", "Anfal Siddiqui", "Nicolas Serrano", "Dongkai Chen", "Billy Porter", "Libin Bai", "Keshav Shivam", "Sho Arora", "Partha Talukdar", "Tom Cobley", "Sangnie Bhardwaj", "Evgeny Gladchenko", "Simon Green", "Kelvin Guu", "Felix Fischer", "Xiao Wu", "Eric Wang", "Achintya Singhal", "Tatiana Matejovicova", "James Martens", "Hongji Li", "Roma Patel", "Elizabeth Kemp", "Jiaqi Pan", "Lily Wang", "Blake JianHang Chen", "Jean-Baptiste Alayrac", "Navneet Potti", "Erika Gemzer", "Eugene Ie", "Kay McKinney", "Takaaki Saeki", "Edward Chou", "Pascal Lamblin", "SQ Mah", "Zach Fisher", "Martin Chadwick", "Jon Stritar", "Obaid Sarvana", "Andrew Hogue", "Artem Shtefan", "Hadi Hashemi", "Yang Xu", "Jindong Gu", "Sharad Vikram", "Chung-Ching Chang", "Sabela Ramos", "Logan Kilpatrick", "Weijuan Xi", "Jenny Brennan", "Yinghao Sun", "Abhishek Jindal", "Ionel Gog", "Dawn Chen", "Felix Wu", "Jason Lee", "Sudhindra Kopalle", "Srinadh Bhojanapalli", "Oriol Vinyals", "Natan Potikha", "Burcu Karagol Ayan", "Yuan Yuan", "Michael Riley", "Piotr Stanczyk", "Sergey Kishchenko", "Bing Wang", "Dan Garrette", "Antoine Yang", "Vlad Feinberg", "CJ Carey", "Javad Azizi", "Viral Shah", "Erica Moreira", "Chongyang Shi", "Josh Feldman", "Elizabeth Salesky", "Thomas Lampe", "Aneesh Pappu", "Duhyeon Kim", "Jonas Adler", "Avi Caciularu", "Brian Walker", "Yunhan Xu", "Yochai Blau", "Dylan Scandinaro", "Terry Huang", "Sam El-Husseini", "Abhishek Sinha", "Lijie Ren", "Taylor Tobin", "Patrik Sundberg", "Tim Sohn", "Vikas Yadav", "Mimi Ly", "Emily Xue", "Jing Xiong", "Afzal Shama Soudagar", "Sneha Mondal", "Nikhil Khadke", "Qingchun Ren", "Ben Vargas", "Stan Bileschi", "Sarah Chakera", "Cindy Wang", "Boyu Wang", "Yoni Halpern", "Joe Jiang", "Vikas Sindhwani", "Petre Petrov", "Pranavaraj Ponnuramu", "Sanket Vaibhav Mehta", "Yu Watanabe", "Betty Chan", "Matheus Wisniewski", "Trang Pham", "Jingwei Zhang", "Conglong Li", "Dario de Cesare", "Art Khurshudov", "Alex Vasiloff", "Melissa Tan", "Zoe Ashwood", "Bobak Shahriari", "Maryam Majzoubi", "Garrett Tanzer", "Olga Kozlova", "Robin Alazard", "James Lee-Thorp", "Nguyet Minh Phu", "Isaac Tian", "Junwhan Ahn", "Andy Crawford", "Lauren Lax", "Yuan", "Shangguan", "Iftekhar Naim", "David Ross", "Oleksandr Ferludin", "Tongfei Guo", "Andrea Banino", "Hubert Soyer", "Xiaoen Ju", "Dominika Rogozińska", "Ishaan Malhi", "Marcella Valentine", "Daniel Balle", "Apoorv Kulshreshtha", "Maciej Kula", "Yiwen Song", "Sophia Austin", "John Schultz", "Roy Hirsch", "Arthur Douillard", "Apoorv Reddy", "Michael Fink", "Summer Yue", "Khyatti Gupta", "Adam Zhang", "Norman Rink", "Daniel McDuff", "Lei Meng", "András György", "Yasaman Razeghi", "Ricky Liang", "Kazuki Osawa", "Aviel Atias", "Matan Eyal", "Tyrone Hill", "Nikolai Grigorev", "Zhengdong Wang", "Nitish Kulkarni", "Rachel Soh", "Ivan Lobov", "Zachary Charles", "Sid Lall", "Kazuma Hashimoto", "Ido Kessler", "Victor Gomes", "Zelda Mariet", "Danny Driess", "Alessandro Agostini", "Canfer Akbulut", "Jingcao Hu", "Marissa Ikonomidis", "Emily Caveness", "Kartik Audhkhasi", "Saurabh Agrawal", "Ioana Bica", "Evan Senter", "Jayaram Mudigonda", "Kelly Chen", "Jingchen Ye", "Xuanhui Wang", "James Svensson", "Philipp Fränken", "Josh Newlan", "Li Lao", "Eva Schnider", "Sami Alabed", "Joseph Kready", "Jesse Emond", "Afief Halumi", "Tim Zaman", "Chengxi Ye", "Naina Raisinghani", "Vilobh Meshram", "Bo Chang", "Ankit Singh Rawat", "Axel Stjerngren", "Sergey Levi", "Rui Wang", "Xiangzhu Long", "Mitchelle Rasquinha", "Steven Hand", "Aditi Mavalankar", "Lauren Agubuzu", "Sudeshna Roy", "Junquan Chen", "Jarek Wilkiewicz", "Hao Zhou", "Michal Jastrzebski", "Qiong Hu", "Agustin Dal Lago", "Ramya Sree Boppana", "Wei-Jen Ko", "Jennifer Prendki", "Yao Su", "Zhi Li", "Eliza Rutherford", "Girish Ramchandra Rao", "Ramona Comanescu", "Adrià Puigdomènech", "Qihang Chen", "Dessie Petrova", "Christine Chan", "Vedrana Milutinovic", "Felipe Tiengo Ferreira", "Chin-Yi Cheng", "Ming Zhang", "Tapomay Dey", "Sherry Yang", "Ramesh Sampath", "Quoc Le", "Howard Zhou", "Chu-Cheng Lin", "Hoi Lam", "Christine Kaeser-Chen", "Kai Hui", "Dean Hirsch", "Tom Eccles", "Basil Mustafa", "Shruti Rijhwani", "Morgane Rivière", "Yuanzhong Xu", "Junjie Wang", "Xinyang Geng", "Xiance Si", "Arjun Khare", "Cheolmin Kim", "Vahab Mirrokni", "Kamyu Lee", "Khuslen Baatarsukh", "Nathaniel Braun", "Lisa Wang", "Pallavi LV", "Richard Tanburn", "Yuvein", "Zhu", "Fangda Li", "Setareh Ariafar", "Dan Goldberg", "Ken Burke", "Daniil Mirylenka", "Meiqi Guo", "Olaf Ronneberger", "Hadas Natalie Vogel", "Liqun Cheng", "Nishita Shetty", "Johnson Jia", "Thomas Jimma", "Corey Fry", "Ted Xiao", "Martin Sundermeyer", "Ryan Burnell", "Yannis Assael", "Mario Pinto", "JD Chen", "Rohit Sathyanarayana", "Donghyun Cho", "Jing Lu", "Rishabh Agarwal", "Sugato Basu", "Lucas Gonzalez", "Dhruv Shah", "Meng Wei", "Dre Mahaarachchi", "Rohan Agrawal", "Tero Rissa", "Yani Donchev", "Ramiro Leal-Cavazos", "Adrian Hutter", "Markus Mircea", "Alon Jacovi", "Faruk Ahmed", "Jiageng Zhang", "Shuguang Hu", "Bo-Juen Chen", "Jonni Kanerva", "Guillaume Desjardins", "Andrew Lee", "Nikos Parotsidis", "Asier Mujika", "Tobias Weyand", "Jasper Snoek", "Jo Chick", "Kai Chen", "Paul Chang", "Ethan Mahintorabi", "Zi Wang", "Tolly Powell", "Orgad Keller", "Abhirut Gupta", "Claire Sha", "Kanav Garg", "Nicolas Heess", "Ágoston Weisz", "Cassidy Hardin", "Bartek Wydrowski", "Ben Coleman", "Karina Zainullina", "Pankaj Joshi", "Alessandro Epasto", "Terry Spitz", "Binbin Xiong", "Kai Zhao", "Arseniy Klimovskiy", "Ivy Zheng", "Johan Ferret", "Itay Yona", "Waleed Khawaja", "Jean-Baptiste Lespiau", "Maxim Krikun", "Siamak Shakeri", "Timothee Cour", "Bonnie Li", "Igor Krivokon", "Dan Suh", "Alex Hofer", "Jad Al Abdallah", "Nikita Putikhin", "Oscar Akerlund", "Silvio Lattanzi", "Anurag Kumar", "Shane Settle", "Himanshu Srivastava", "Folawiyo Campbell-Ajala", "Edouard Rosseel", "Mihai Dorin Istin", "Nishanth Dikkala", "Anand Rao", "Nick Young", "Kate Lin", "Dhruva Bhaswar", "Yiming Wang", "Jaume Sanchez Elias", "Kritika Muralidharan", "James Keeling", "Dayou Du", "Siddharth Gopal", "Gregory Dibb", "Charles Blundell", "Manolis Delakis", "Jacky Liang", "Marco Tulio Ribeiro", "Georgi Karadzhov", "Guillermo Garrido", "Ankur Bapna", "Jiawei Cao", "Adam Sadovsky", "Pouya Tafti", "Arthur Guez", "Coline Devin", "Yixian Di", "Jinwei Xing", "Chuqiao", "Xu", "Hanzhao Lin", "Chun-Te Chu", "Sameera Ponda", "Wesley Helmholz", "Fan Yang", "Yue Gao", "Sara Javanmardi", "Wael Farhan", "Alex Ramirez", "Ricardo Figueira", "Khe Chai Sim", "Yuval Bahat", "Ashwin Vaswani", "Liangzhe Yuan", "Gufeng Zhang", "Leland Rechis", "Hanjun Dai", "Tayo Oguntebi", "Alexandra Cordell", "Eugénie Rives", "Kaan Tekelioglu", "Naveen Kumar", "Bing Zhang", "Aurick Zhou", "Nikolay Savinov", "Andrew Leach", "Alex Tudor", "Sanjay Ganapathy", "Yanyan Zheng", "Mirko Rossini", "Vera Axelrod", "Arnaud Autef", "Yukun Zhu", "Zheng Zheng", "Mingda Zhang", "Baochen Sun", "Jie Ren", "Nenad Tomasev", "Nithish Kannan", "Amer Sinha", "Charles Chen", "Louis O'Bryan", "Alex Pak", "Aditya Kusupati", "Weel Yang", "Deepak Ramachandran", "Patrick Griffin", "Seokhwan Kim", "Philipp Neubeck", "Craig Schiff", "Tammo Spalink", "Mingyang Ling", "Arun Nair", "Ga-Young Joung", "Linda Deng", "Avishkar Bhoopchand", "Lora Aroyo", "Tom Duerig", "Jordan Griffith", "Gabe Barth-Maron", "Jake Ades", "Alex Haig", "Ankur Taly", "Yunting Song", "Paul Michel", "Dave Orr", "Dean Weesner", "Corentin Tallec", "Carrie Grimes Bostock", "Paul Niemczyk", "Andy Twigg", "Mudit Verma", "Rohith Vallu", "Henry Wang", "Marco Gelmi", "Kiranbir Sodhia", "Aleksandr Chuklin", "Omer Goldman", "Jasmine George", "Liang Bai", "Kelvin Zhang", "Petar Sirkovic", "Efrat Nehoran", "Golan Pundak", "Jiaqi Mu", "Alice Chen", "Alex Greve", "Paulo Zacchello", "David Amos", "Heming Ge", "Eric Noland", "Colton Bishop", "Jeffrey Dudek", "Youhei Namiki", "Elena Buchatskaya", "Jing Li", "Dorsa Sadigh", "Masha Samsikova", "Dan Malkin", "Damien Vincent", "Robert David", "Rob Willoughby", "Phoenix Meadowlark", "Shawn Gao", "Yan Li", "Raj Apte", "Amit Jhindal", "Stein Xudong Lin", "Alex Polozov", "Zhicheng Wang", "Tomas Mery", "Anirudh GP", "Varun Yerram", "Sage Stevens", "Tianqi Liu", "Noah Fiedel", "Charles Sutton", "Matthew Johnson", "Xiaodan Song", "Kate Baumli", "Nir Shabat", "Muqthar Mohammad", "Hao Liu", "Marco Selvi", "Yichao Zhou", "Mehdi Hafezi Manshadi", "Chu-ling Ko", "Anthony Chen", "Michael Bendersky", "Jorge Gonzalez Mendez", "Nisarg Kothari", "Amir Zandieh", "Yiling Huang", "Daniel Andor", "Ellie Pavlick", "Idan Brusilovsky", "Jitendra Harlalka", "Sally Goldman", "Andrew Lampinen", "Guowang Li", "Asahi Ushio", "Somit Gupta", "Lei Zhang", "Chuyuan Kelly Fu", "Madhavi Sewak", "Timo Denk", "Jed Borovik", "Brendan Jou", "Avital Zipori", "Prateek Jain", "Junwen Bai", "Thang Luong", "Jonathan Tompson", "Alice Li", "Li Liu", "George Powell", "Jiajun Shen", "Alex Feng", "Grishma Chole", "Da Yu", "Yinlam Chow", "Tongxin Yin", "Eric Malmi", "Kefan Xiao", "Yash Pande", "Shachi Paul", "Niccolò Dal Santo", "Adil Dostmohamed", "Sergio Guadarrama", "Aaron Phillips", "Thanumalayan Sankaranarayana Pillai", "Gal Yona", "Amin Ghafouri", "Preethi Lahoti", "Benjamin Lee", "Dhruv Madeka", "Eren Sezener", "Simon Tokumine", "Adrian Collister", "Nicola De Cao", "Richard Shin", "Uday Kalra", "Parker Beak", "Emily Nottage", "Ryo Nakashima", "Ivan Jurin", "Vikash Sehwag", "Meenu Gaba", "Junhao Zeng", "Kevin R. McKee", "Fernando Pereira", "Tamar Yakar", "Amayika Panda", "Arka Dhar", "Peilin Zhong", "Daniel Sohn", "Mark Brand", "Lars Lowe Sjoesund", "Viral Carpenter", "Sharon Lin", "Shantanu Thakoor", "Marcus Wainwright", "Ashwin Chaugule", "Pranesh Srinivasan", "Muye Zhu", "Bernett Orlando", "Jack Weber", "Ayzaan Wahid", "Gilles Baechler", "Apurv Suman", "Jovana Mitrović", "Gabe Taubman", "Honglin Yu", "Helen King", "Josh Dillon", "Cathy Yip", "Dhriti Varma", "Tomas Izo", "Levent Bolelli", "Borja De Balle Pigem", "Julia Di Trapani", "Fotis Iliopoulos", "Adam Paszke", "Nishant Ranka", "Joe Zou", "Francesco Pongetti", "Jed McGiffin", "Alex Siegman", "Rich Galt", "Ross Hemsley", "Goran Žužić", "Victor Carbune", "Tao Li", "Myle Ott", "Félix de Chaumont Quitry", "David Vilar Torres", "Yuri Chervonyi", "Tomy Tsai", "Prem Eruvbetine", "Samuel Yang", "Matthew Denton", "Jake Walker", "Slavica Andačić", "Idan Heimlich Shtacher", "Vittal Premachandran", "Harshal Tushar Lehri", "Cip Baetu", "Damion Yates", "Lampros Lamprou", "Mariko Iinuma", "Ioana Mihailescu", "Ben Albrecht", "Shachi Dave", "Susie Sargsyan", "Bryan Perozzi", "Lucas Manning", "Chiyuan Zhang", "Denis Vnukov", "Igor Mordatch", "Raia Hadsell Wolfgang Macherey", "Ryan Kappedal", "Jim Stephan", "Aditya Tripathi", "Klaus Macherey", "Jun Qian", "Abhishek Bhowmick", "Shekoofeh Azizi", "Rémi Leblond", "Shiva Mohan Reddy Garlapati", "Timothy Knight", "Matthew Wiethoff", "Wei-Chih Hung", "Anelia Angelova", "Georgios Evangelopoulos", "Pawel Janus", "Dimitris Paparas", "Matthew Rahtz", "Ken Caluwaerts", "Vivek Sampathkumar", "Daniel Jarrett", "Shadi Noghabi", "Antoine Miech", "Chak Yeung", "Geoff Clark", "Henry Prior", "Fei Zheng", "Jean Pouget-Abadie", "Indro Bhattacharya", "Kalpesh Krishna", "Will Bishop", "Zhe Yuan", "Yunxiao Deng", "Ashutosh Sathe", "Kacper Krasowiak", "Ciprian Chelba", "Cho-Jui Hsieh", "Kiran Vodrahalli", "Buhuang Liu", "Thomas Köppe", "Amr Khalifa", "Lubo Litchev", "Pichi Charoenpanit", "Reed Roberts", "Sachin Yadav", "Yasumasa Onoe", "Desi Ivanov", "Megha Mohabey", "Vighnesh Birodkar", "Nemanja Rakićević", "Pierre Sermanet", "Vaibhav Mehta", "Krishan Subudhi", "Travis Choma", "Will Ng", "Luheng He", "Kathie Wang", "Tasos Kementsietsidis", "Shane Gu", "Mansi Gupta", "Andrew Nystrom", "Mehran Kazemi", "Timothy Chung", "Nacho Cano", "Nikhil Dhawan", "Yufei Wang", "Jiawei Xia", "Trevor Yacovone", "Eric Jia", "Mingqing Chen", "Simeon Ivanov", "Ashrith Sheshan", "Sid Dalmia", "Paweł Stradomski", "Pengcheng Yin", "Salem Haykal", "Congchao Wang", "Dennis Duan", "Neslihan Bulut", "Greg Kochanski", "Liam MacDermed", "Namrata Godbole", "Shitao Weng", "Jingjing Chen", "Rachana Fellinger", "Ramin Mehran", "Daniel Suo", "Hisham Husain", "Tong He", "Kaushal Patel", "Joshua Howland", "Randall Parker", "Kelvin Nguyen", "Sharath Maddineni", "Chris Rawles", "Mina Khan", "Shlomi Cohen-Ganor", "Amol Mandhane", "Xinyi Wu", "Chenkai Kuang", "Iulia Comşa", "Ramya Ganeshan", "Hanie Sedghi", "Adam Bloniarz", "Nuo Wang Pierse", "Anton Briukhov", "Petr Mitrichev", "Anita Gergely", "Serena Zhan", "Allan Zhou", "Nikita Saxena", "Eva Lu", "Josef Dean", "Ashish Gupta", "Nicolas Perez-Nieves", "Renjie Wu", "Cory McLean", "Wei Liang", "Disha Jindal", "Anton Tsitsulin", "Wenhao Yu", "Kaiz Alarakyia", "Tom Schaul", "Piyush Patil", "Peter Sung", "Elijah Peake", "Hongkun Yu", "Feryal Behbahani", "JD Co-Reyes", "Alan Ansell", "Sean Sun", "Clara Barbu", "Jonathan Lee", "Seb Noury", "James Allingham", "Bilal Piot", "Mohit Sharma", "Christopher Yew", "Ivan Korotkov", "Bibo Xu", "Demetra Brady", "Goran Petrovic", "Shibl Mourad", "Claire Cui", "Aditya Gupta", "Parker Schuh", "Saarthak Khanna", "Anna Goldie", "Abhinav Arora", "Vadim Zubov", "Amy Stuart", "Mark Epstein", "Yun Zhu", "Jianqiao Liu", "Yury Stuken", "Ziyue Wang", "Karolis Misiunas", "Dee Guo", "Ashleah Gill", "Ale Hartman", "Zaid Nabulsi", "Aurko Roy", "Aleksandra Faust", "Jason Riesa", "Ben Withbroe", "Mengchao Wang", "Marco Tagliasacchi", "Andreea Marzoca", "James Noraky", "Serge Toropov", "Malika Mehrotra", "Bahram Raad", "Sanja Deur", "Steve Xu", "Marianne Monteiro", "Zhongru Wu", "Yi Luan", "Sam Ritter", "Nick Li", "Håvard Garnes", "Yanzhang He", "Martin Zlocha", "Jifan Zhu", "Matteo Hessel", "Will Wu", "Spandana Raj Babbula", "Chizu Kawamoto", "Yuanzhen Li", "Mehadi Hassen", "Yan Wang", "Brian Wieder", "James Freedman", "Yin Zhang", "Xinyi Bai", "Tianli Yu", "David Reitter", "XiangHai Sheng", "Mateo Wirth", "Aditya Kini", "Dima Damen", "Mingcen Gao", "Rachel Hornung", "Michael Voznesensky", "Brian Roark", "Adhi Kuncoro", "Yuxiang Zhou", "Rushin Shah", "Anthony Brohan", "Kuangyuan Chen", "James Wendt", "David Rim", "Paul Kishan Rubenstein", "Jonathan Halcrow", "Michelle Liu", "Ty Geri", "Yunhsuan Sung", "Jane Shapiro", "Shaan Bijwadia", "Chris Duvarney", "Christina Sorokin", "Paul Natsev", "Reeve Ingle", "Pramod Gupta", "Young Maeng", "Ndaba Ndebele", "Kexin Zhu", "Valentin Anklin", "Katherine Lee", "Yuan Liu", "Yaroslav Akulov", "Shaleen Gupta", "Guolong Su", "Flavien Prost", "Tianlin Liu", "Vitaly Kovalev", "Pol Moreno", "Martin Scholz", "Sam Redmond", "Zongwei Zhou", "Alex Castro-Ros", "André Susano Pinto", "Dia Kharrat", "Michal Yarom", "Rachel Saputro", "Jannis Bulian", "Ben Caine", "Ji Liu", "Abbas Abdolmaleki", "Shariq Iqbal", "Tautvydas Misiunas", "Mikhail Sirotenko", "Shefali Garg", "Guy Bensky", "Huan Gui", "Xuezhi Wang", "Raphael Koster", "Mike Bernico", "Da Huang", "Romal Thoppilan", "Trevor Cohn", "Ben Golan", "Wenlei Zhou", "Andrew Rosenberg", "Markus Freitag", "Tynan Gangwani", "Vincent Tsang", "Anand Shukla", "Xiaoqi Ren", "Minh Giang", "Chi Zou", "Andre Elisseeff", "Charline Le Lan", "Dheeru Dua", "Shuba Lall", "Pranav Shyam", "Frankie Garcia", "Sarah Nguyen", "Michael Guzman", "AJ Maschinot", "Marcello Maggioni", "Ming-Wei Chang", "Karol Gregor", "Lotte Weerts", "Kumaran Venkatesan", "Bogdan Damoc", "Leon Liu", "Jan Wassenberg", "Lewis Ho", "Becca Roelofs", "Majid Hadian", "François-Xavier Aubet", "Yu Liang", "Sami Lachgar", "Danny Karmon", "Yong Cheng", "Amelio Vázquez-Reina", "Angie Chen", "Zhuyun Dai", "Andy Brock", "Shubham Agrawal", "Chenxi Pang", "Peter Garst", "Mariella Sanchez-Vargas", "Ivor Rendulic", "Aditya Ayyar", "Andrija Ražnatović", "Olivia Ma", "Roopali Vij", "Neha Sharma", "Ashwin Balakrishna", "Bingyuan Liu", "Ian Mackinnon", "Sorin Baltateanu", "Petra Poklukar", "Gabriel Ibagon", "Colin Ji", "Hongyang Jiao", "Isaac Noble", "Wojciech Stokowiec", "Zhihao Li", "Jeff Dean", "David Lindner", "Mark Omernick", "Kristen Chiafullo", "Mason Dimarco", "Vitor Rodrigues", "Vittorio Selo", "Garrett Honke", "Xintian", "Wu", "Wei He", "Adam Hillier", "Anhad Mohananey", "Vihari Piratla", "Chang Ye", "Chase Malik", "Sebastian Riedel", "Samuel Albanie", "Zi Yang", "Kenny Vassigh", "Maria Bauza", "Sheng Li", "Yiqing Tao", "Nevan Wichers", "Andrii Maksai", "Abe Ittycheriah", "Ross Mcilroy", "Bryan Seybold", "Noah Goodman", "Romina Datta", "Steven M. Hernandez", "Tian Shi", "Yony Kochinski", "Anna Bulanova", "Ken Franko", "Mikita Sazanovich", "Nicholas FitzGerald", "Praneeth Kacham", "Shubha Srinivas Raghvendra", "Vincent Hellendoorn", "Alexander Grushetsky", "Julian Salazar", "Angeliki Lazaridou", "Jason Chang", "Jan-Thorsten Peter", "Sushant Kafle", "Yann Dauphin", "Abhishek Rao", "Filippo Graziano", "Izhak Shafran", "Yuguo Liao", "Tianli Ding", "Geng Yan", "Grace Chu", "Zhao Fu", "Vincent Roulet", "Gabriel Rasskin", "Duncan Williams", "Shahar Drath", "Alex Mossin", "Raphael Hoffmann", "Jordi Orbay", "Francesco Bertolini", "Hila Sheftel", "Justin Chiu", "Siyang Xue", "Yuheng Kuang", "Ferjad Naeem", "Swaroop Nath", "Nana Nti", "Phil Culliton", "Kashyap Krishnakumar", "Michael Isard", "Pei Sun", "Ayan Chakrabarti", "Nathan Clement", "Regev Cohen", "Arissa Wongpanich", "GS Oh", "Ashwin Murthy", "Hao Zheng", "Jessica Hamrick", "Oskar Bunyan", "Suhas Ganesh", "Nitish Gupta", "Roy Frostig", "John Wieting", "Yury Malkov", "Pierre Marcenac", "Zhixin", "Lai", "Xiaodan Tang", "Mohammad Saleh", "Fedir Zubach", "Chinmay Kulkarni", "Huanjie Zhou", "Vicky Zayats", "Nan Ding", "Anshuman Tripathi", "Arijit Pramanik", "Patrik Zochbauer", "Harish Ganapathy", "Vedant Misra", "Zach Behrman", "Hugo Vallet", "Mingyang Zhang", "Mukund Sridhar", "Ye Jin", "Mohammad Babaeizadeh", "Siim Põder", "Megha Goel", "Divya Jain", "Tajwar Nasir", "Shubham Mittal", "Tim Dozat", "Diego Ardila", "Aliaksei Severyn", "Fabio Pardo", "Sammy Jerome", "Siyang Qin", "Louis Rouillard", "Amir Yazdanbakhsh", "Zizhao Zhang", "Shivani Agrawal", "Kaushik Shivakumar", "Caden Lu", "Praveen Kallakuri", "Rachita Chhaparia", "Kanishka Rao", "Charles Kwong", "Asya Fadeeva", "Shitij Nigam", "Yan Virin", "Yuan Zhang", "Balaji Venkatraman", "Beliz Gunel", "Marc Wilson", "Huiyu Wang", "Abhinav Gupta", "Xiaowei Xu", "Adrien Ali Taïga", "Kareem Mohamed", "Doug Fritz", "Daniel Rodriguez", "Zoubin Ghahramani", "Harry Askham", "Lior Belenki", "James Zhao", "Rahul Gupta", "Krzysztof Jastrzębski", "Takahiro Kosakai", "Kaan Katircioglu", "Jon Schneider", "Rina Panigrahy", "Konstantinos Bousmalis", "Peter Grabowski", "Prajit Ramachandran", "Chaitra Hegde", "Mihaela Rosca", "Angelo Scorza Scarpati", "Kyriakos Axiotis", "Ying Xu", "Zach Gleicher", "Assaf Hurwitz Michaely", "Mandar Sharma", "Sanil Jain", "Christoph Hirnschall", "Tal Marian", "Xuhui Jia", "Kevin Mather", "Kilol Gupta", "Linhai Qiu", "Nigamaa Nayakanti", "Lucian Ionita", "Steven Zheng", "Lucia Loher", "Kurt Shuster", "Igor Petrovski", "Roshan Sharma", "Rahma Chaabouni", "Angel Yeh", "James An", "Arushi Gupta", "Steven Schwarcz", "Seher Ellis", "Sam Conway-Rahman", "Javier Snaider", "Alex Zhai", "James Atwood", "Daniel Golovin", "Liqian Peng", "Te I", "Vivian Xia", "Salvatore Scellato", "Mahan Malihi", "Arthur Bražinskas", "Vlad-Doru Ion", "Younghoon Jun", "James Swirhun", "Soroosh Mariooryad", "Jiao Sun", "Steve Chien", "Rey Coaguila", "Ariel Brand", "Yi Gao", "Tom Kwiatkowski", "Roee Aharoni", "Cheng-Chun Lee", "Mislav Žanić", "Yichi Zhang", "Dan Ethier", "Vitaly Nikolaev", "Pranav Nair", "Yoav Ben Shalom", "Hen Fitoussi", "Jai Gupta", "Hongbin Liu", "Dee Cattle", "Tolga Bolukbasi", "Ben Murdoch", "Fantine Huot", "Yin Li", "Chris Hahn"], "categories": ["cs.CL", "cs.AI"], "comment": "72 pages, 17 figures", "summary": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and\nGemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite\nmodels. Gemini 2.5 Pro is our most capable model yet, achieving SoTA\nperformance on frontier coding and reasoning benchmarks. In addition to its\nincredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that\nexcels at multimodal understanding and it is now able to process up to 3 hours\nof video content. Its unique combination of long context, multimodal and\nreasoning capabilities can be combined to unlock new agentic workflows. Gemini\n2.5 Flash provides excellent reasoning abilities at a fraction of the compute\nand latency requirements and Gemini 2.0 Flash and Flash-Lite provide high\nperformance at low latency and cost. Taken together, the Gemini 2.X model\ngeneration spans the full Pareto frontier of model capability vs cost, allowing\nusers to explore the boundaries of what is possible with complex agentic\nproblem solving.", "AI": {"tldr": "Introduction of the Gemini 2.5 model family, highlighting its capabilities and applications in multimodal understanding and reasoning.", "motivation": "To present advanced AI models capable of high-performance reasoning and coding, especially in multimodal contexts.", "method": "Introduction of Gemini 2.5 Pro and Flash models with a focus on their capabilities and performance metrics.", "result": "Gemini 2.5 Pro achieves state-of-the-art performance on coding and reasoning tasks, while Gemini 2.5 Flash provides strong reasoning at lower compute costs.", "conclusion": "The Gemini 2.X model generation offers varied capabilities to suit different user needs for agentic problem-solving tasks.", "key_contributions": ["Introduction of Gemini 2.5 Pro with state-of-the-art performance.", "Enhanced multimodal understanding and reasoning capabilities.", "Cost-effective models for high performance and low latency."], "limitations": "", "keywords": ["Gemini 2.5 Pro", "reasoning", "multimodal understanding", "AI models", "agentic workflows"], "importance_score": 7, "read_time_minutes": 30}}
{"id": "2507.06669", "pdf": "https://arxiv.org/pdf/2507.06669.pdf", "abs": "https://arxiv.org/abs/2507.06669", "title": "Smartphone Exergames with Real-Time Markerless Motion Capture: Challenges and Trade-offs", "authors": ["Mathieu Phosanarack", "Laura Wallard", "Sophie Lepreux", "Christophe Kolski", "Eugénie Avril"], "categories": ["cs.HC"], "comment": "CHI '25 Workshop on Envisioning the Future of Interactive Health, Apr\n  2025, Yokohama, Japan", "summary": "Markerless Motion Capture (MoCap) using smartphone cameras is a promising\napproach to making exergames more accessible and cost-effective for health and\nrehabilitation. Unlike traditional systems requiring specialized hardware,\nrecent advancements in AI-powered pose estimation enable movement tracking\nusing only a mobile device. For an upcoming study, a mobile application with\nreal-time exergames including markerless motion capture is being developed.\nHowever, implementing such technology introduces key challenges, including\nbalancing accuracy and real-time responsiveness, ensuring proper user\ninteraction. Future research should explore optimizing AI models for realtime\nperformance, integrating adaptive gamification, and refining user-centered\ndesign principles. By overcoming these challenges, smartphone-based exergames\ncould become powerful tools for engaging users in physical activity and\nrehabilitation, extending their benefits to a broader audience.", "AI": {"tldr": "This paper discusses the use of smartphone-based markerless motion capture for exergames in health and rehabilitation, highlighting challenges and future research directions.", "motivation": "To enhance accessibility and cost-effectiveness of exergames for health and rehabilitation through smartphone technology.", "method": "Development of a mobile application that utilizes AI-powered pose estimation for real-time exergames with markerless motion capture.", "result": "Identification of challenges in achieving accuracy, responsiveness, and user interaction in markerless motion capture using mobile devices.", "conclusion": "Overcoming existing challenges has the potential to make smartphone-based exergames valuable for promoting physical activity and rehabilitation in diverse populations.", "key_contributions": ["Use of smartphone cameras for motion capture in exergames", "Real-time AI-powered pose estimation", "Focus on user-centered design and adaptive gamification"], "limitations": "Challenges in balancing accuracy and responsiveness, and ensuring effective user interaction need to be addressed.", "keywords": ["Markerless Motion Capture", "Exergames", "AI-powered Pose Estimation", "Health", "Rehabilitation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.06306", "pdf": "https://arxiv.org/pdf/2507.06306.pdf", "abs": "https://arxiv.org/abs/2507.06306", "title": "Humans overrely on overconfident language models, across languages", "authors": ["Neil Rathi", "Dan Jurafsky", "Kaitlyn Zhou"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "10 pages main text, to appear at COLM 2025", "summary": "As large language models (LLMs) are deployed globally, it is crucial that\ntheir responses are calibrated across languages to accurately convey\nuncertainty and limitations. Previous work has shown that LLMs are\nlinguistically overconfident in English, leading users to overrely on confident\ngenerations. However, the usage and interpretation of epistemic markers (e.g.,\n'It's definitely,' 'I think') can differ sharply across languages. Here, we\nstudy the risks of multilingual linguistic (mis)calibration, overconfidence,\nand overreliance across five languages to evaluate the safety of LLMs in a\nglobal context.\n  We find that overreliance risks are high across all languages. We first\nanalyze the distribution of LLM-generated epistemic markers, and observe that\nwhile LLMs are cross-linguistically overconfident, they are also sensitive to\ndocumented linguistic variation. For example, models generate the most markers\nof uncertainty in Japanese and the most markers of certainty in German and\nMandarin. We then measure human reliance rates across languages, finding that\nwhile users strongly rely on confident LLM generations in all languages,\nreliance behaviors differ cross-linguistically: for example, users rely\nsignificantly more on expressions of uncertainty in Japanese than in English.\nTaken together, these results indicate high risk of reliance on overconfident\nmodel generations across languages. Our findings highlight the challenges of\nmultilingual linguistic calibration and stress the importance of culturally and\nlinguistically contextualized model safety evaluations.", "AI": {"tldr": "This paper evaluates the calibration of large language models (LLMs) across five languages, highlighting risks of overconfidence and varying reliance on model outputs based on linguistic differences.", "motivation": "The goal is to ensure LLM responses are calibrated across languages to accurately convey uncertainty and limitations, as previous findings show LLMs often display overconfidence in English.", "method": "The study analyzes the distribution of epistemic markers generated by LLMs and measures human reliance rates on these generations across five different languages.", "result": "The analysis reveals that LLMs are overconfident across languages but vary in the use of uncertainty markers; for example, they generate more markers of uncertainty in Japanese compared to certainty markers in German and Mandarin.", "conclusion": "There is a high risk of overreliance on overly confident LLM outputs across languages, which necessitates an understanding of linguistic and cultural variations for better model safety evaluations.", "key_contributions": ["Analysis of multilingual linguistic calibration of LLMs", "Identification of cultural differences in users' reliance on LLM outputs", "Highlighting the need for context-specific safety evaluations for LLMs"], "limitations": "", "keywords": ["Large Language Models", "Multilingual Calibration", "Epistemic Markers", "User Reliance", "Model Safety"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.06691", "pdf": "https://arxiv.org/pdf/2507.06691.pdf", "abs": "https://arxiv.org/abs/2507.06691", "title": "Effects of task difficulty and music expertise in virtual reality: Observations of cognitive load and task accuracy in a rhythm exergame", "authors": ["Kyla Ellahiyoun", "Emma Jane Pretty", "Renan Guarese", "Marcel Takac", "Haytham Fayek", "Fabio Zambetta"], "categories": ["cs.HC"], "comment": "Submitted to VRST'25", "summary": "This study explores the relationship between musical training, cognitive load\n(CL), and task accuracy within the virtual reality (VR) exergame Beat Saber\nacross increasing levels of difficulty. Participants (N=32) completed a series\nof post-task questionnaires after playing the game under three task difficulty\nlevels while having their physiological data measured by an Emotibit. Using\nregression analyses, we found that task difficulty and gaming experience\nsignificantly predicted subjective CL, whereas musical training did not.\nHowever, musical training significantly predicted higher task accuracy, along\nwith lower subjective CL, increased gaming experience, and greater\nphysiological arousal. These results suggest that musical training enhances\ntask-specific performance but does not directly reduce subjective CL. Future\nresearch should consider alternative methods of grouping musical expertise and\nthe additional predictability of flow and self-efficacy.", "AI": {"tldr": "This study investigates the impact of musical training on cognitive load and task accuracy in the VR exergame Beat Saber.", "motivation": "To examine how musical training affects cognitive load and task performance in a virtual reality gaming context.", "method": "Participants (N=32) played Beat Saber at varying difficulty levels while their physiological data were collected. Post-task questionnaires assessed subjective cognitive load and task accuracy.", "result": "Musical training significantly predicted higher task accuracy; however, it did not directly reduce subjective cognitive load. Difficulty and gaming experience were key predictors of cognitive load.", "conclusion": "Musical training contributes to improved performance in tasks but does not necessarily alleviate cognitive load. Future studies should explore different categorizations of musical expertise and examine the concepts of flow and self-efficacy.", "key_contributions": ["Demonstrated the influence of musical training on task accuracy in VR exergame settings.", "Identified the relationship between gaming experience and subjective cognitive load.", "Suggested avenues for future research on musical expertise and its effects on cognitive processes."], "limitations": "", "keywords": ["musical training", "cognitive load", "task accuracy", "virtual reality", "exergame"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.06313", "pdf": "https://arxiv.org/pdf/2507.06313.pdf", "abs": "https://arxiv.org/abs/2507.06313", "title": "ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time", "authors": ["Kiarash Zahirnia", "Zahra Golpayegani", "Walid Ahmad", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based Language Models' computation and memory overhead increase\nquadratically as a function of sequence length. The quadratic cost poses\nchallenges when employing LLMs for processing long sequences. In this work, we\nintroduce \\ourmodelacronym~(Extend at Test-Time), method for extending the\ncontext length of short context Transformer-based LLMs, with constant memory\nrequirement and linear computation overhead. ETT enable the extension of the\ncontext length at test-time by efficient fine-tuning the model's parameters on\nthe input context, chunked into overlapping small subsequences. We evaluate ETT\non LongBench by extending the context length of GPT-Large and Phi-2 up to 32\ntimes, increasing from 1k to 32k tokens. This results in up to a 30 percent\nimprovement in the model's accuracy. We also study how context can be stored in\nLLM's weights effectively and efficiently. Through a detailed ablation study,\nwe examine which Transformer modules are most beneficial to fine-tune at\ntest-time. Interestingly, we find that fine-tuning the second layer of the FFNs\nis more effective than full fine-tuning, leading to a further improvement in\nthe models' accuracy.", "AI": {"tldr": "This paper introduces Extend at Test-Time (ETT), a method that enables the extension of context length in Transformer-based LLMs with efficient memory usage and improved accuracy.", "motivation": "To address the challenges posed by the quadratic computation and memory overhead of Transformer-based Language Models when processing long sequences.", "method": "ETT fine-tunes model parameters on input contexts chunked into overlapping subsequences, allowing for context length extension at test-time with constant memory requirements and linear computation overhead.", "result": "ETT successfully extends the context length of models like GPT-Large and Phi-2 from 1k to 32k tokens, achieving up to a 30% improvement in model accuracy.", "conclusion": "The approach demonstrates that specific fine-tuning strategies can significantly enhance the performance of LLMs when handling long sequences.", "key_contributions": ["Introduction of a method for extending context length in LLMs with constant memory and linear computation overhead.", "Demonstration of improved accuracy on long sequences.", "Insights on effective fine-tuning of Transformer modules at test-time."], "limitations": "", "keywords": ["Transformer", "Language Models", "Context Length Extension", "Fine-tuning", "Model Accuracy"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.06734", "pdf": "https://arxiv.org/pdf/2507.06734.pdf", "abs": "https://arxiv.org/abs/2507.06734", "title": "Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted Classification in an Open-Source Telegram Monitoring Tool", "authors": ["Milena Pustet", "Elisabeth Steffen", "Helena Mihaljević", "Grischa Stanjek", "Yannis Illies"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "The role of civil society organizations (CSOs) in monitoring harmful online\ncontent is increasingly crucial, especially as platform providers reduce their\ninvestment in content moderation. AI tools can assist in detecting and\nmonitoring harmful content at scale. However, few open-source tools offer\nseamless integration of AI models and social media monitoring infrastructures.\nGiven their thematic expertise and contextual understanding of harmful content,\nCSOs should be active partners in co-developing technological tools, providing\nfeedback, helping to improve models, and ensuring alignment with stakeholder\nneeds and values, rather than as passive 'consumers'. However, collaborations\nbetween the open source community, academia, and civil society remain rare, and\nresearch on harmful content seldom translates into practical tools usable by\ncivil society actors. This work in progress explores how CSOs can be\nmeaningfully involved in an AI-assisted open-source monitoring tool of\nanti-democratic movements on Telegram, which we are currently developing in\ncollaboration with CSO stakeholders.", "AI": {"tldr": "This paper discusses the involvement of civil society organizations (CSOs) in developing AI-assisted tools for monitoring harmful online content, particularly on Telegram.", "motivation": "The paper highlights the importance of CSOs in content moderation as platform providers reduce their investments in this area, emphasizing the need for effective collaboration to ensure tools meet stakeholder needs.", "method": "The authors explore the co-development of an AI-assisted open-source monitoring tool for anti-democratic movements on Telegram, in collaboration with CSO stakeholders.", "result": "The paper outlines preliminary findings on integrating thematic expertise of CSOs with technological solutions to improve harmful content monitoring.", "conclusion": "The study asserts the need for more collaboration between CSOs, academia, and the open-source community to create effective monitoring tools accessible to civil society.", "key_contributions": ["Proposes a framework for CSO involvement in AI tool development", "Identifies gaps in current open-source tools for social media monitoring", "Highlights the collaborative potential between CSOs and tech developers"], "limitations": "Limited scope as the project is still in development, requiring further validation and testing of the proposed tools.", "keywords": ["civil society organizations", "AI-assisted monitoring", "harmful online content", "open-source tools", "Telegram"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.06335", "pdf": "https://arxiv.org/pdf/2507.06335.pdf", "abs": "https://arxiv.org/abs/2507.06335", "title": "Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?", "authors": ["Casey Kennington", "David Schlangen"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "Formal, Distributional, and Grounded theories of computational semantics each\nhave their uses and their drawbacks. There has been a shift to ground models of\nlanguage by adding visual knowledge, and there has been a call to enrich models\nof language with symbolic methods to gain the benefits from formal,\ndistributional, and grounded theories. In this paper, we attempt to make the\ncase that one potential path forward in unifying all three semantic fields is\npaved with the words-as-classifier model, a model of word-level grounded\nsemantics that has been incorporated into formalisms and distributional\nlanguage models in the literature, and it has been well-tested within\ninteractive dialogue settings. We review that literature, motivate the\nwords-as-classifiers model with an appeal to recent work in cognitive science,\nand describe a small experiment. Finally, we sketch a model of semantics\nunified through words-as-classifiers.", "AI": {"tldr": "This paper explores the unification of formal, distributional, and grounded theories of computational semantics through the words-as-classifier model, emphasizing its application in interactive dialogue settings.", "motivation": "There is a growing need to integrate formal, distributional, and grounded approaches to computational semantics to leverage their strengths while addressing their weaknesses.", "method": "The paper reviews existing literature on the words-as-classifier model and presents a small experiment to demonstrate its efficacy.", "result": "The words-as-classifiers model has been shown to be effective in integrating visual knowledge into language models, particularly in interactive dialogue applications.", "conclusion": "A unified semantic model leveraging words-as-classifiers could enhance our understanding and application of computational semantics in AI systems.", "key_contributions": ["Proposal of the words-as-classifiers model as a unifying framework for semantics", "Review of prior literature demonstrating the model's applicability", "Initial experimental validation of the model in dialogue settings"], "limitations": "", "keywords": ["computational semantics", "words-as-classifiers", "interactive dialogue", "grounded semantics", "language models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.06751", "pdf": "https://arxiv.org/pdf/2507.06751.pdf", "abs": "https://arxiv.org/abs/2507.06751", "title": "Combining Human-centred Explainability and Explainable AI", "authors": ["Janin Koch", "Vitor Fortes Rey"], "categories": ["cs.HC"], "comment": null, "summary": "This position paper looks at differences between the current understandings\nof human-centered explainability and explainability AI. We discuss current\nideas in both fields, as well as the differences and opportunities we\ndiscovered. As an example of combining both, we will present preliminary work\non a new algebraic machine learning approach. We are excited to continue\ndiscussing design opportunities for human-centered explainability (HCx) and xAI\nwith the broader HCxAI community.", "AI": {"tldr": "This position paper explores the distinctions and synergies between human-centered explainability and explainable AI, presenting preliminary work on a new algebraic machine learning method.", "motivation": "To address the differences between human-centered explainability and explainable AI, and to identify opportunities for combining insights from both fields.", "method": "Presentation of preliminary work on a new algebraic machine learning approach that integrates concepts from both explainability domains.", "result": "The paper provides an overview of current understandings in human-centered explainability and explainable AI, highlighting differences and design opportunities.", "conclusion": "The authors emphasize the need for ongoing discussions and collaborations within the HCxAI community to advance the field of human-centered explainability.", "key_contributions": ["Identification of differences between human-centered explainability and explainable AI.", "Presentation of a new algebraic machine learning approach.", "Encouragement for dialogue within the HCxAI community."], "limitations": "", "keywords": ["human-centered explainability", "explainable AI", "algebraic machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06378", "pdf": "https://arxiv.org/pdf/2507.06378.pdf", "abs": "https://arxiv.org/abs/2507.06378", "title": "Evaluating Morphological Alignment of Tokenizers in 70 Languages", "authors": ["Catherine Arnett", "Marisa Hudspeth", "Brendan O'Connor"], "categories": ["cs.CL"], "comment": "6 pages, 3 figures. Accepted to the Tokenization Workshop at ICML\n  2025", "summary": "While tokenization is a key step in language modeling, with effects on model\ntraining and performance, it remains unclear how to effectively evaluate\ntokenizer quality. One proposed dimension of tokenizer quality is the extent to\nwhich tokenizers preserve linguistically meaningful subwords, aligning token\nboundaries with morphological boundaries within a word. We expand MorphScore\n(Arnett & Bergen, 2025), which previously covered 22 languages, to support a\ntotal of 70 languages. The updated MorphScore offers more flexibility in\nevaluation and addresses some of the limitations of the original version. We\nthen correlate our alignment scores with downstream task performance for five\npre-trained languages models on seven tasks, with at least one task in each of\nthe languages in our sample. We find that morphological alignment does not\nexplain very much variance in model performance, suggesting that morphological\nalignment alone does not measure dimensions of tokenization quality relevant to\nmodel performance.", "AI": {"tldr": "This paper evaluates the quality of tokenizers by expanding the MorphScore metric to 70 languages and examining its correlation with the performance of language models on various tasks.", "motivation": "To address the unclear evaluation of tokenizer quality, specifically through the lens of morphological alignment and its impact on model training and performance.", "method": "We expanded MorphScore to support 70 languages and correlated alignment scores with the performance of five pre-trained language models on seven tasks.", "result": "The study reveals that morphological alignment does not significantly correlate with model performance, indicating it does not fully capture important aspects of tokenizer quality.", "conclusion": "Morphological alignment alone is insufficient for measuring tokenizer quality in relation to model performance in language tasks.", "key_contributions": ["Expansion of MorphScore to include 70 languages", "Correlation analysis of alignment scores with downstream model performance", "Insights into the limitations of morphological alignment as a quality measure"], "limitations": "The findings suggest that while MorphScore is a useful metric, it may not encompass all dimensions of tokenizer quality that affect model performance.", "keywords": ["tokenization", "morphological alignment", "language models", "MorphScore", "evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.06779", "pdf": "https://arxiv.org/pdf/2507.06779.pdf", "abs": "https://arxiv.org/abs/2507.06779", "title": "Tailoring deep learning for real-time brain-computer interfaces: From offline models to calibration-free online decoding", "authors": ["Martin Wimpff", "Jan Zerfowski", "Bin Yang"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Despite the growing success of deep learning (DL) in offline brain-computer\ninterfaces (BCIs), its adoption in real-time applications remains limited due\nto three primary challenges. First, most DL solutions are designed for offline\ndecoding, making the transition to online decoding unclear. Second, the use of\nsliding windows in online decoding substantially increases computational\ncomplexity. Third, DL models typically require large amounts of training data,\nwhich are often scarce in BCI applications. To address these challenges and\nenable real-time, cross-subject decoding without subject-specific calibration,\nwe introduce realtime adaptive pooling (RAP), a novel parameter-free method.\nRAP seamlessly modifies the pooling layers of existing offline DL models to\nmeet online decoding requirements. It also reduces computational complexity\nduring training by jointly decoding consecutive sliding windows. To further\nalleviate data requirements, our method leverages source-free domain\nadaptation, enabling privacy-preserving adaptation across varying amounts of\ntarget data. Our results demonstrate that RAP provides a robust and efficient\nframework for real-time BCI applications. It preserves privacy, reduces\ncalibration demands, and supports co-adaptive BCI systems, paving the way for\nbroader adoption of DL in online BCIs. These findings lay a strong foundation\nfor developing user-centered, high-performance BCIs that facilitate immediate\nfeedback and user learning.", "AI": {"tldr": "Introduction of realtime adaptive pooling (RAP) for real-time brain-computer interfaces (BCIs) addressing challenges of offline deep learning applications.", "motivation": "To overcome limitations in offline deep learning applications for real-time brain-computer interface decoding, particularly pertaining to computational complexity and data scarcity.", "method": "The paper introduces a novel parameter-free method called realtime adaptive pooling (RAP) that modifies pooling layers in offline deep learning models for online decoding. It decodes sliding windows efficiently and uses source-free domain adaptation to reduce data needs.", "result": "RAP framework shows improved robustness and efficiency for real-time BCI applications, preserving privacy while minimizing calibration needs.", "conclusion": "RAP paves the way for broader deep learning adoption in online BCIs, facilitating user-centered, high-performance systems that offer immediate feedback and aid user learning.", "key_contributions": ["Development of a parameter-free method for real-time decoding in BCIs", "Reduction in computational complexity for online applications", "Improvement in adaptability to scarce data through source-free domain adaptation"], "limitations": "", "keywords": ["brain-computer interfaces", "deep learning", "real-time decoding", "adaptive pooling", "domain adaptation"], "importance_score": 5, "read_time_minutes": 7}}
{"id": "2507.06393", "pdf": "https://arxiv.org/pdf/2507.06393.pdf", "abs": "https://arxiv.org/abs/2507.06393", "title": "Hypermagmas and Colored Operads: Heads, Phases, and Theta Roles", "authors": ["Matilde Marcolli", "Riny Huijbregts", "Richard K. Larson"], "categories": ["cs.CL", "math.QA", "math.RA", "91F20, 18M60, 18M80, 16T05, 68Q70"], "comment": "LaTeX, 48 pages", "summary": "We show that head functions on syntactic objects extend the magma structure\nto a hypermagma, with the c-command relation compatible with the magma\noperation and the m-command relation with the hypermagma. We then show that the\nstructure of head and complement and specifier, additional modifier positions,\nand the structure of phases in the Extended Projection can be formulated as a\nbud generating system of a colored operad, in a form similar to the structure\nof theta roles. We also show that, due to the special form of the colored\noperad generators, the filtering of freely generated syntactic objects by these\ncoloring rules can be equivalently formulated as a filtering in the course of\nstructure formation via a colored Merge, which can in turn be related to the\nhypermagma structure. The rules on movement by Internal Merge with respect to\nphases, the Extended Projection Principle, Empty Category Principle, and Phase\nImpenetrability Condition are all subsumed into the form of the colored operad\ngenerators. Movement compatibilities between the phase structure and the theta\nroles assignments can then be formulated in terms of the respective colored\noperads and a transduction of colored operads.", "AI": {"tldr": "This paper introduces a framework that extends syntactic head functions to hypermagma structures, integrating operations and relations in syntactic theory via colored operads.", "motivation": "To explore the extensions of magma structures in syntax and how these can be expressed within the framework of colored operads.", "method": "The authors develop a mathematical framework relating head functions to hypermagma structures and colored operads, demonstrating connection with syntactic rules and phases using formal operations like Merge.", "result": "The paper presents relationships between syntactic structures and operators, proposing that movement and compatibility in phases can be modeled through colored operads, subsuming existing syntactic rules.", "conclusion": "The findings establish a unified model for various syntactic principles, structured through the lens of colored operads and hypermagma.", "key_contributions": ["Introduction of hypermagma structure in syntax", "Formulation of head and complement structures using colored operads", "Unification of syntactic movement principles under operad theory"], "limitations": "", "keywords": ["hypermagma", "colored operad", "syntactic structures", "Internal Merge", "theta roles"], "importance_score": 2, "read_time_minutes": 48}}
{"id": "2507.06864", "pdf": "https://arxiv.org/pdf/2507.06864.pdf", "abs": "https://arxiv.org/abs/2507.06864", "title": "Toward Neurodivergent-Aware Productivity: A Systems and AI-Based Human-in-the-Loop Framework for ADHD-Affected Professionals", "authors": ["Raghavendra Deshmukh"], "categories": ["cs.HC"], "comment": null, "summary": "Digital work environments in IT and knowledge-based sectors demand high\nlevels of attention management, task juggling, and self-regulation. For adults\nwith ADHD, these settings often amplify challenges such as time blindness,\ndigital distraction, emotional reactivity, and executive dysfunction. These\nindividuals prefer low-touch, easy-to-use interventions for daily tasks.\nConventional productivity tools often fail to support the cognitive variability\nand overload experienced by neurodivergent professionals. This paper presents a\nframework that blends Systems Thinking, Human-in-the-Loop design, AI/ML, and\nprivacy-first adaptive agents to support ADHD-affected users. The assistant\nsenses tab usage, application focus, and inactivity using on-device ML. These\ncues are used to infer attention states and deliver nudges, reflective prompts,\nor accountability-based presence (body doubling) that aid regulation without\ndisruption. Technically grounded in AI, the approach views attention as shaped\nby dynamic feedback loops. The result is a replicable model for adaptive,\ninclusive support tools in high-distraction work environments.", "AI": {"tldr": "The paper presents a framework for developing adaptive support tools targeting adults with ADHD in digital work environments, leveraging AI/ML and human-in-the-loop design to enhance attention management without disruption.", "motivation": "To address the challenges faced by adults with ADHD in high-distraction, digital work environments, where conventional productivity tools often fall short in supporting neurodivergent professionals.", "method": "The proposed framework combines Systems Thinking, Human-in-the-Loop design, and AI/ML to create a privacy-first adaptive assistant that detects user attention states through on-device machine learning, providing tailored nudges and prompts.", "result": "The framework results in a replicable model for adaptive tools that provide inclusive support based on real-time attention state inference, aimed at enhancing regulation and productivity.", "conclusion": "The model showcases the potential of adaptive, privacy-focused AI tools to assist neurodivergent individuals in managing attention and productivity in complex work settings.", "key_contributions": ["Development of a framework integrating Systems Thinking and Human-in-the-Loop design for ADHD support.", "Implementation of on-device ML to infer attention states from user interaction.", "Provision of privacy-first adaptive nudges that enhance executive function without disruption."], "limitations": "The framework may require extensive testing with real users to refine its effectiveness and ensure it meets the diverse needs of neurodivergent individuals.", "keywords": ["ADHD", "Human-Computer Interaction", "AI/ML", "attention management", "adaptive tools"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06415", "pdf": "https://arxiv.org/pdf/2507.06415.pdf", "abs": "https://arxiv.org/abs/2507.06415", "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning", "authors": ["Zeming Chen", "Angelika Romanou", "Gail Weiss", "Antoine Bosselut"], "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 7 figures", "summary": "Long-context reasoning requires accurately identifying relevant information\nin extensive, noisy input contexts. Previous research shows that using\ntest-time learning to encode context directly into model parameters can\neffectively enable reasoning over noisy information. However, meta-learning\nmethods for enabling test-time learning are prohibitively memory-intensive,\npreventing their application to long context settings. In this work, we propose\nPERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for\nlearning to encode long input contexts using gradient updates to a lightweight\nmodel adapter at test time. Specifically, PERK employs two nested optimization\nloops in a meta-training phase. The inner loop rapidly encodes contexts into a\nlow-rank adapter (LoRA) that serves as a parameter-efficient memory module for\nthe base model. Concurrently, the outer loop learns to use the updated adapter\nto accurately recall and reason over relevant information from the encoded long\ncontext. Our evaluations on several long-context reasoning tasks show that PERK\nsignificantly outperforms the standard prompt-based long-context baseline,\nachieving average absolute performance gains of up to 90% for smaller models\n(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In\ngeneral, PERK is more robust to reasoning complexity, length extrapolation, and\nthe locations of relevant information in contexts. Finally, we show that while\nPERK is memory-intensive during training, it scales more efficiently at\ninference time than prompt-based long-context inference.", "AI": {"tldr": "This paper introduces PERK, a method that improves long-context reasoning by efficiently encoding contexts using a lightweight model adapter.", "motivation": "To address the challenges of accurately reasoning over extensive, noisy inputs and the limitations of memory-intensive meta-learning methods in long-context settings.", "method": "PERK employs two nested optimization loops: an inner loop that rapidly encodes contexts into a low-rank adapter, and an outer loop that uses the updated adapter for reasoning over the encoded context.", "result": "PERK achieves significant performance improvements on long-context reasoning tasks, with up to 90% gains in smaller models and 27% in larger models compared to prompt-based baselines.", "conclusion": "While PERK requires more memory during training, it is more efficient during inference, outperforming standard approaches in various complexity scenarios.", "key_contributions": ["Introduction of a lightweight model adapter for efficient long-context reasoning.", "Demonstration of significant performance gains over standard prompt-based methods.", "Scalability of inference despite higher training memory requirements."], "limitations": "", "keywords": ["long-context reasoning", "parameter-efficient memory module", "meta-learning", "gradient updates", "model adapter"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.06419", "pdf": "https://arxiv.org/pdf/2507.06419.pdf", "abs": "https://arxiv.org/abs/2507.06419", "title": "Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling", "authors": ["Pankayaraj Pathmanathan", "Furong Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Reward modeling (RM), which captures human preferences to align large\nlanguage models (LLMs), is increasingly employed in tasks such as model\nfinetuning, response filtering, and ranking. However, due to the inherent\ncomplexity of human preferences and the limited coverage of available datasets,\nreward models often fail under distributional shifts or adversarial\nperturbations. Existing approaches for identifying such failure modes typically\nrely on prior knowledge about preference distributions or failure attributes,\nlimiting their practicality in real-world settings where such information is\nunavailable. In this work, we propose a tractable, preference-distribution\nagnostic method for discovering reward model failure modes via reward guided\ncontrolled decoding. Building on this, we introduce REFORM, a self-improving\nreward modeling framework that enhances robustness by using the reward model\nitself to guide the generation of falsely scored responses. These adversarial\nexamples are then used to augment the training data and patch the reward\nmodel's misaligned behavior. We evaluate REFORM on two widely used preference\ndatasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate\nthat it significantly improves robustness without sacrificing reward quality.\nNotably, REFORM preserves performance both in direct evaluation and in\ndownstream policy training, and further improves alignment quality by removing\nspurious correlations.", "AI": {"tldr": "This paper introduces REFORM, a method for improving the robustness of reward modeling in large language models by identifying and correcting failure modes through adversarial training.", "motivation": "The work addresses the limitations of existing reward models that struggle with human preference complexity and distributional shifts, proposing a practical solution for enhanced robustness.", "method": "The method involves reward guided controlled decoding to discover failure modes, using adversarial examples to augment training data, thereby improving the reward model's performance.", "result": "REFORM was evaluated on two preference datasets showing significant robustness improvements and better alignment quality without compromising reward quality.", "conclusion": "The proposed framework enhances the ability of reward models to handle complex human preferences and improves overall model performance and robustness.", "key_contributions": ["Introduces REFORM, a self-improving reward modeling framework", "Implements a method to discover reward model failure modes", "Demonstrates significant improvements on preference datasets"], "limitations": "", "keywords": ["reward modeling", "large language models", "human preferences", "robustness", "adversarial training"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.06306", "pdf": "https://arxiv.org/pdf/2507.06306.pdf", "abs": "https://arxiv.org/abs/2507.06306", "title": "Humans overrely on overconfident language models, across languages", "authors": ["Neil Rathi", "Dan Jurafsky", "Kaitlyn Zhou"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "10 pages main text, to appear at COLM 2025", "summary": "As large language models (LLMs) are deployed globally, it is crucial that\ntheir responses are calibrated across languages to accurately convey\nuncertainty and limitations. Previous work has shown that LLMs are\nlinguistically overconfident in English, leading users to overrely on confident\ngenerations. However, the usage and interpretation of epistemic markers (e.g.,\n'It's definitely,' 'I think') can differ sharply across languages. Here, we\nstudy the risks of multilingual linguistic (mis)calibration, overconfidence,\nand overreliance across five languages to evaluate the safety of LLMs in a\nglobal context.\n  We find that overreliance risks are high across all languages. We first\nanalyze the distribution of LLM-generated epistemic markers, and observe that\nwhile LLMs are cross-linguistically overconfident, they are also sensitive to\ndocumented linguistic variation. For example, models generate the most markers\nof uncertainty in Japanese and the most markers of certainty in German and\nMandarin. We then measure human reliance rates across languages, finding that\nwhile users strongly rely on confident LLM generations in all languages,\nreliance behaviors differ cross-linguistically: for example, users rely\nsignificantly more on expressions of uncertainty in Japanese than in English.\nTaken together, these results indicate high risk of reliance on overconfident\nmodel generations across languages. Our findings highlight the challenges of\nmultilingual linguistic calibration and stress the importance of culturally and\nlinguistically contextualized model safety evaluations.", "AI": {"tldr": "The paper investigates the risks of multilingual linguistic (mis)calibration in large language models (LLMs), highlighting the overconfidence in their outputs across different languages and the implications for user reliance on these outputs.", "motivation": "With the global deployment of LLMs, it is essential to ensure their responses are calibrated in various languages to accurately represent uncertainty and limitations, as previous studies revealed a tendency towards overconfidence in English. The study aims to address these issues to evaluate the safety of LLMs in a multilingual context.", "method": "The research involved analyzing the distribution of epistemic markers generated by LLMs across five languages, measuring human reliance on these markers, and identifying cross-linguistic variations in user behavior related to confidence and uncertainty in LLM outputs.", "result": "The study found that LLMs exhibit cross-linguistic overconfidence, with significant variation in the usage of epistemic markers. Users display varying reliance on these markers, relying more on expressions of uncertainty in some languages like Japanese compared to English.", "conclusion": "The findings emphasize the need for linguistically and culturally contextualized evaluations of model safety to mitigate the risks of overreliance on LLMs in different languages.", "key_contributions": ["Investigated multilingual linguistic calibration in LLMs.", "Analyzed user reliance on LLM-generated epistemic markers across five languages.", "Highlighted the significance of cultural context in LLM safety evaluations."], "limitations": "The study focuses on only five languages, which may not capture the full spectrum of multilingual risks associated with LLMs.", "keywords": ["large language models", "multilingual calibration", "epistemic markers", "user reliance", "cross-linguistic variation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.06427", "pdf": "https://arxiv.org/pdf/2507.06427.pdf", "abs": "https://arxiv.org/abs/2507.06427", "title": "Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders", "authors": ["Shun Wang", "Tyler Loakman", "Youbo Lei", "Yi Liu", "Bohao Yang", "Yuting Zhao", "Dong Yang", "Chenghua Lin"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are traditionally viewed as black-box\nalgorithms, therefore reducing trustworthiness and obscuring potential\napproaches to increasing performance on downstream tasks. In this work, we\napply an effective LLM decomposition method using a dictionary-learning\napproach with sparse autoencoders. This helps extract monosemantic features\nfrom polysemantic LLM neurons. Remarkably, our work identifies model-internal\nmisunderstanding, allowing the automatic reformulation of the prompts with\nadditional annotations to improve the interpretation by LLMs. Moreover, this\napproach demonstrates a significant performance improvement in downstream\ntasks, such as mathematical reasoning and metaphor detection.", "AI": {"tldr": "This paper presents a method to improve Large Language Models (LLMs) by decomposing them using dictionary-learning and sparse autoencoders, addressing internal misunderstandings and enhancing performance in downstream tasks.", "motivation": "The research seeks to enhance the trustworthiness and performance of LLMs, which are often viewed as black-box algorithms, by illuminating their internal workings.", "method": "Applying dictionary-learning and sparse autoencoders to decompose and extract monosemantic features from polysemantic LLM neurons.", "result": "The method identifies internal misunderstandings in models and enables automatic reformulation of prompts, leading to improved performance in tasks like mathematical reasoning and metaphor detection.", "conclusion": "The proposed approach significantly enhances LLM performance on various downstream tasks by refining prompt interpretation through model decomposition.", "key_contributions": ["Introduction of a dictionary-learning approach for LLM decomposition", "Identification of internal model misunderstandings", "Demonstration of improved performance in specific tasks through enhanced prompt formulation"], "limitations": "", "keywords": ["Large Language Models", "dictionary-learning", "sparse autoencoders", "prompt reformulation", "downstream tasks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06435", "pdf": "https://arxiv.org/pdf/2507.06435.pdf", "abs": "https://arxiv.org/abs/2507.06435", "title": "Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling", "authors": ["Rafiu Adekoya Badekale", "Adewale Akinfaderin"], "categories": ["cs.CL"], "comment": "10 pages, 7 figures. Code and data available at\n  https://github.com/AdeTheBade/TACPD.git", "summary": "Understanding how policy language evolves over time is critical for assessing\nglobal responses to complex challenges such as climate change. Temporal\nanalysis helps stakeholders, including policymakers and researchers, to\nevaluate past priorities, identify emerging themes, design governance\nstrategies, and develop mitigation measures. Traditional approaches, such as\nmanual thematic coding, are time-consuming and limited in capturing the\ncomplex, interconnected nature of global policy discourse. With the increasing\nrelevance of unsupervised machine learning, these limitations can be addressed,\nparticularly under high-volume, complex, and high-dimensional data conditions.\nIn this work, we explore a novel approach that applies the dynamic embedded\ntopic model (DETM) to analyze the evolution of global climate policy discourse.\nA probabilistic model designed to capture the temporal dynamics of topics over\ntime. We collected a corpus of United Nations Framework Convention on Climate\nChange (UNFCCC) policy decisions from 1995 to 2023, excluding 2020 due to the\npostponement of COP26 as a result of the COVID-19 pandemic. The model reveals\nshifts from early emphases on greenhouse gases and international conventions to\nrecent focuses on implementation, technical collaboration, capacity building,\nfinance, and global agreements. Section 3 presents the modeling pipeline,\nincluding preprocessing, model training, and visualization of temporal word\ndistributions. Our results show that DETM is a scalable and effective tool for\nanalyzing the evolution of global policy discourse. Section 4 discusses the\nimplications of these findings and we concluded with future directions and\nrefinements to extend this approach to other policy domains.", "AI": {"tldr": "The paper explores the use of the dynamic embedded topic model (DETM) to analyze the evolution of global climate policy discourse over time, revealing shifts in focus from greenhouse gases to implementation and finance.", "motivation": "To assess global responses to complex challenges like climate change and evaluate past priorities, while addressing the limitations of traditional manual coding techniques.", "method": "The dynamic embedded topic model (DETM) is applied to a corpus of UNFCCC policy decisions from 1995 to 2023 to analyze temporal dynamics in climate policy themes.", "result": "The DETM model effectively reveals shifts in climate policy focus from early international conventions to recent themes of implementation and financial strategies.", "conclusion": "DETM is a scalable tool for analyzing global policy discourse, with implications for future research and applications to other policy domains.", "key_contributions": ["Application of DETM for analyzing climate policy discourse", "Identification of temporal shifts in policy themes", "Demonstrated scalability and effectiveness of the modeling approach"], "limitations": "Limited to UNFCCC policy decisions and may not generalize to other contexts without further adaptation.", "keywords": ["dynamic embedded topic model", "climate policy", "UNFCCC", "temporal analysis", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.06448", "pdf": "https://arxiv.org/pdf/2507.06448.pdf", "abs": "https://arxiv.org/abs/2507.06448", "title": "Perception-Aware Policy Optimization for Multimodal Reasoning", "authors": ["Zhenhailong Wang", "Xuehang Guo", "Sofia Stoica", "Haiyang Xu", "Hongru Wang", "Hyeonjeong Ha", "Xiusi Chen", "Yangyi Chen", "Ming Yan", "Fei Huang", "Heng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.", "AI": {"tldr": "This paper presents Perception-Aware Policy Optimization (PAPO), which enhances multimodal reasoning in LLMs by improving visual perception and reducing perception errors through internal supervision signals.", "motivation": "The current reinforcement learning techniques for LLMs are optimized for textual tasks, leading to poor performance in multimodal reasoning, particularly in perceiving visual inputs.", "method": "PAPO extends the Generalized Recurrent Policy Optimization (GRPO) by adding an Implicit Perception Loss using a KL divergence term, promoting the model's learning of perception along with reasoning.", "result": "PAPO yields significant improvements of 4.4% on various multimodal benchmarks and up to 8.0% on vision-dependent tasks, along with a substantial reduction in perception errors by 30.5%.", "conclusion": "The work establishes a new RL framework that integrates perception-aware supervision into RLVR, supporting better visually grounded reasoning.", "key_contributions": ["Introduction of Perception-Aware Policy Optimization (PAPO) for multimodal reasoning", "Significant improvement in perception and reasoning tasks without relying on additional data", "Development of Double Entropy Loss to mitigate loss hacking issues"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Multimodal Reasoning", "Perception-Aware Policy Optimization", "Implicit Perception Loss"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06450", "pdf": "https://arxiv.org/pdf/2507.06450.pdf", "abs": "https://arxiv.org/abs/2507.06450", "title": "A Semantic Parsing Framework for End-to-End Time Normalization", "authors": ["Xin Su", "Sungduk Yu", "Phillip Howard", "Steven Bethard"], "categories": ["cs.CL"], "comment": null, "summary": "Time normalization is the task of converting natural language temporal\nexpressions into machine-readable representations. It underpins many downstream\napplications in information retrieval, question answering, and clinical\ndecision-making. Traditional systems based on the ISO-TimeML schema limit\nexpressivity and struggle with complex constructs such as compositional,\nevent-relative, and multi-span time expressions. In this work, we introduce a\nnovel formulation of time normalization as a code generation task grounded in\nthe SCATE framework, which defines temporal semantics through symbolic and\ncompositional operators. We implement a fully executable SCATE Python library\nand demonstrate that large language models (LLMs) can generate executable SCATE\ncode. Leveraging this capability, we develop an automatic data augmentation\npipeline using LLMs to synthesize large-scale annotated data with code-level\nvalidation. Our experiments show that small, locally deployable models trained\non this augmented data can achieve strong performance, outperforming even their\nLLM parents and enabling practical, accurate, and interpretable time\nnormalization.", "AI": {"tldr": "This paper introduces a novel approach to time normalization by formulating it as a code generation task using the SCATE framework, enhancing its expressivity and performance.", "motivation": "Time normalization is crucial for converting temporal expressions into formats usable by machines, impacting fields like information retrieval and clinical decision-making. Traditional methods are limited in handling complex temporal constructs.", "method": "The authors propose a time normalization formulation grounded in the SCATE framework, implementing a Python library that generates executable SCATE code, and employing LLMs for automatic data augmentation.", "result": "Experiments demonstrate that models trained on LLM-augmented data outperform existing methods, providing strong accuracy and interpretability in time normalization tasks.", "conclusion": "The proposed method significantly enhances time normalization capabilities, enabling effective deployment of smaller models while maintaining performance.", "key_contributions": ["Introduction of a new formulation for time normalization as a code generation task.", "Implementation of a SCATE framework to define temporal semantics accurately.", "Creation of an automatic data augmentation pipeline using LLMs for synthesizing annotated data."], "limitations": "The performance might vary with different contexts or the complexity of the temporal expressions beyond the tested scenarios.", "keywords": ["time normalization", "natural language processing", "large language models", "data augmentation", "SCATE framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06457", "pdf": "https://arxiv.org/pdf/2507.06457.pdf", "abs": "https://arxiv.org/abs/2507.06457", "title": "A Systematic Analysis of Hybrid Linear Attention", "authors": ["Dustin Wang", "Rui-Jie Zhu", "Steven Abreu", "Yong Shan", "Taylor Kergan", "Yuqi Pan", "Yuhong Chou", "Zheng Li", "Ge Zhang", "Wenhao Huang", "Jason Eshraghian"], "categories": ["cs.CL"], "comment": null, "summary": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.", "AI": {"tldr": "This paper evaluates various linear attention mechanisms in hybrid architectures to address the limitations of Transformers with long sequences, presenting new models and benchmarks for language modeling and recall tasks.", "motivation": "To address the quadratic complexity and memory issues of Transformers when processing long sequences, the adoption of linear attention mechanisms and their hybridization is crucial.", "method": "The study systematically evaluates various linear attention models (including vector recurrences and gating mechanisms) across generations, analyzing both standalone and hybridized configurations using 72 models trained at different parameter sizes and token counts.", "result": "The study finds that while standalone linear models can perform well, they do not always excel when hybridized with full attention. Recall performance improves with more full attention layers, especially under a 3:1 linear-to-full ratio.", "conclusion": "The research emphasizes the importance of selective gating, hierarchical recurrence, and controlled forgetting in hybrid models and recommends specific architectures for optimal performance.", "key_contributions": ["Evaluation of various linear attention models in hybrid architectures", "Open-sourced 72 trained models for future research", "Identification of key mechanisms for effective hybrid models"], "limitations": "", "keywords": ["Linear Attention", "Hybrid Models", "Transformers", "Language Modeling", "Recall Tasks"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.06489", "pdf": "https://arxiv.org/pdf/2507.06489.pdf", "abs": "https://arxiv.org/abs/2507.06489", "title": "On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks", "authors": ["Stephen Obadinma", "Xiaodan Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Robust verbal confidence generated by large language models (LLMs) is crucial\nfor the deployment of LLMs to ensure transparency, trust, and safety in\nhuman-AI interactions across many high-stakes applications. In this paper, we\npresent the first comprehensive study on the robustness of verbal confidence\nunder adversarial attacks. We introduce a novel framework for attacking verbal\nconfidence scores through both perturbation and jailbreak-based methods, and\nshow that these attacks can significantly jeopardize verbal confidence\nestimates and lead to frequent answer changes. We examine a variety of\nprompting strategies, model sizes, and application domains, revealing that\ncurrent confidence elicitation methods are vulnerable and that commonly used\ndefence techniques are largely ineffective or counterproductive. Our findings\nunderscore the urgent need to design more robust mechanisms for confidence\nexpression in LLMs, as even subtle semantic-preserving modifications can lead\nto misleading confidence in responses.", "AI": {"tldr": "This study investigates the robustness of verbal confidence in large language models (LLMs) under adversarial attacks and introduces a novel framework for testing this robustness.", "motivation": "To ensure transparency, trust, and safety in human-AI interactions, particularly in high-stakes applications, it's vital to assess the robustness of verbal confidence generated by LLMs.", "method": "The paper introduces a framework for attacking verbal confidence scores using both perturbation and jailbreak-based methods, examining various prompting strategies, model sizes, and application domains.", "result": "The study finds that current confidence elicitation methods are highly vulnerable to attacks, resulting in significant changes in confidence estimates that can mislead users.", "conclusion": "The findings indicate a pressing need for developing more robust mechanisms for confidence expression in LLMs, as existing methods are insufficient against subtle adversarial modifications.", "key_contributions": ["First comprehensive study on robustness of verbal confidence in LLMs", "Introduction of a novel framework for adversarial attacks on verbal confidence", "Identification of vulnerabilities in current confidence elicitation methods"], "limitations": "The study primarily focuses on certain types of attacks and may not cover all potential adversarial techniques.", "keywords": ["verbal confidence", "large language models", "adversarial attacks", "confidence elicitation", "human-AI interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.06506", "pdf": "https://arxiv.org/pdf/2507.06506.pdf", "abs": "https://arxiv.org/abs/2507.06506", "title": "Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings", "authors": ["Russell Taylor", "Benjamin Herbert", "Michael Sana"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain", "summary": "Translating wordplay across languages presents unique challenges that have\nlong confounded both professional human translators and machine translation\nsystems. This research proposes a novel approach for translating puns from\nEnglish to French by combining state-of-the-art large language models with\nspecialized techniques for wordplay generation.\n  Our methodology employs a three-stage approach. First, we establish a\nbaseline using multiple frontier large language models with feedback based on a\nnew contrastive learning dataset. Second, we implement a guided\nchain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we\nimplement a multi-agent generator-discriminator framework for evaluating and\nregenerating puns with feedback.\n  Moving beyond the limitations of literal translation, our methodology's\nprimary objective is to capture the linguistic creativity and humor of the\nsource text wordplay, rather than simply duplicating its vocabulary. Our best\nruns earned first and second place in the CLEF JOKER 2025 Task 2 competition\nwhere they were evaluated manually by expert native French speakers.\n  This research addresses a gap between translation studies and computational\nlinguistics by implementing linguistically-informed techniques for wordplay\ntranslation, advancing our understanding of how language models can be\nleveraged to handle the complex interplay between semantic ambiguity, phonetic\nsimilarity, and the implicit cultural and linguistic awareness needed for\nsuccessful humor.", "AI": {"tldr": "The paper presents a novel method for translating puns from English to French, combining large language models with specialized techniques to capture linguistic creativity and humor.", "motivation": "Translating puns across languages is challenging due to the need to maintain humor and linguistic creativity, which traditional methods often fail to achieve.", "method": "The methodology involves a three-stage approach: 1) establishing a baseline with large language models and a contrastive learning dataset, 2) employing a guided chain-of-thought pipeline with phonetic-semantic embeddings, and 3) using a multi-agent generator-discriminator framework for pun evaluation and regeneration.", "result": "The proposed approach won first and second place in the CLEF JOKER 2025 Task 2 competition, receiving positive evaluations from expert native French speakers.", "conclusion": "The research offers a significant contribution to the intersection of translation studies and computational linguistics, enhancing the understanding of language models in handling semantic ambiguity and cultural awareness in humor.", "key_contributions": ["Development of a novel approach combining large language models for pun translation", "Establishment of a contrastive learning dataset for feedback", "Implementation of a multi-agent framework for pun evaluation"], "limitations": "", "keywords": ["machine translation", "wordplay", "humor", "large language models", "contrastive learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2409.12538", "pdf": "https://arxiv.org/pdf/2409.12538.pdf", "abs": "https://arxiv.org/abs/2409.12538", "title": "PersonaFlow: Designing LLM-Simulated Expert Perspectives for Enhanced Research Ideation", "authors": ["Yiren Liu", "Pranav Sharma", "Mehul Jitendra Oswal", "Haijun Xia", "Yun Huang"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to DIS2025", "summary": "Generating interdisciplinary research ideas requires diverse domain\nexpertise, but access to timely feedback is often limited by the availability\nof experts. In this paper, we introduce PersonaFlow, a novel system designed to\nprovide multiple perspectives by using LLMs to simulate domain-specific\nexperts. Our user studies showed that the new design 1) increased the perceived\nrelevance and creativity of ideated research directions, and 2) promoted users'\ncritical thinking activities (e.g., interpretation, analysis, evaluation,\ninference, and self-regulation), without increasing their perceived cognitive\nload. Moreover, users' ability to customize expert profiles significantly\nimproved their sense of agency, which can potentially mitigate their\nover-reliance on AI. This work contributes to the design of intelligent systems\nthat augment creativity and collaboration, and provides design implications of\nusing customizable AI-simulated personas in domains within and beyond research\nideation.", "AI": {"tldr": "PersonaFlow is a system that uses LLMs to simulate domain-specific experts, enhancing the creativity and relevance of interdisciplinary research ideas while supporting critical thinking and a sense of agency among users.", "motivation": "Generating interdisciplinary research ideas often requires diverse expertise, which can be limited by the availability of domain experts for timely feedback.", "method": "User studies were conducted to evaluate the effectiveness of PersonaFlow in simulating experts, assessing its impact on perceived relevance, creativity, and critical thinking activities.", "result": "The user studies indicated that PersonaFlow increased perceived relevance and creativity of the ideated research directions and supported critical thinking without raising cognitive load.", "conclusion": "PersonaFlow demonstrates that customizable AI-simulated personas can effectively enhance research ideation, creativity, and collaboration while providing users a greater sense of control over the process.", "key_contributions": ["Introduction of PersonaFlow for simulating domain-specific experts using LLMs", "Improvement in users' critical thinking activities", "Customization of expert profiles increasing users' sense of agency"], "limitations": "", "keywords": ["PersonaFlow", "LLM", "interdisciplinary research", "AI-simulated experts", "creativity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06517", "pdf": "https://arxiv.org/pdf/2507.06517.pdf", "abs": "https://arxiv.org/abs/2507.06517", "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers", "authors": ["Zicong Tang", "Shi Luohe", "Zuchao Li", "Baoyuan Qi", "Guoming Liu", "Lefei Zhang", "Ping Wang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.", "AI": {"tldr": "SpindleKV is a novel KV cache reduction method for Large Language Models that improves memory efficiency and model performance by utilizing distinct approaches for shallow and deep layers.", "motivation": "The increasing memory consumption of KV cache during inference poses significant challenges for Large Language Models, necessitating effective reduction methods that maintain performance.", "method": "SpindleKV implements an attention weight based eviction method for deep layers and a codebook based replacement approach for shallow layers, guided by similarity and merging policies.", "result": "Experiments demonstrated that SpindleKV achieves better KV cache reduction compared to baseline methods while preserving similar or improved model performance on two common benchmarks across three LLMs.", "conclusion": "SpindleKV effectively addresses the challenges associated with KV cache in LLMs, reducing memory consumption while maintaining or enhancing inference performance.", "key_contributions": ["Introduction of SpindleKV for efficient KV cache reduction", "Utilization of distinct methods for shallow and deep layers", "Improvement in model performance paired with cache efficiency"], "limitations": "", "keywords": ["Large Language Models", "KV cache reduction", "attention mechanisms", "memory efficiency", "machine learning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2410.14879", "pdf": "https://arxiv.org/pdf/2410.14879.pdf", "abs": "https://arxiv.org/abs/2410.14879", "title": "Vital Insight: Assisting Experts' Context-Driven Sensemaking of Multi-modal Personal Tracking Data Using Visualization and Human-In-The-Loop LLM", "authors": ["Jiachen Li", "Xiwen Li", "Justin Steinberg", "Akshat Choube", "Bingsheng Yao", "Xuhai Xu", "Dakuo Wang", "Elizabeth Mynatt", "Varun Mishra"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Passive tracking methods, such as phone and wearable sensing, have become\ndominant in monitoring human behaviors in modern ubiquitous computing studies.\nWhile there have been significant advances in machine-learning approaches to\ntranslate periods of raw sensor data to model momentary behaviors, (e.g.,\nphysical activity recognition), there still remains a significant gap in the\ntranslation of these sensing streams into meaningful, high-level, context-aware\ninsights that are required for various applications (e.g., summarizing an\nindividual's daily routine). To bridge this gap, experts often need to employ a\ncontext-driven sensemaking process in real-world studies to derive insights.\nThis process often requires manual effort and can be challenging even for\nexperienced researchers due to the complexity of human behaviors.\n  We conducted three rounds of user studies with 21 experts to explore\nsolutions to address challenges with sensemaking. We follow a human-centered\ndesign process to identify needs and design, iterate, build, and evaluate Vital\nInsight (VI), a novel, LLM-assisted, prototype system to enable\nhuman-in-the-loop inference (sensemaking) and visualizations of multi-modal\npassive sensing data from smartphones and wearables. Using the prototype as a\ntechnology probe, we observe experts' interactions with it and develop an\nexpert sensemaking model that explains how experts move between direct data\nrepresentations and AI-supported inferences to explore, question, and validate\ninsights. Through this iterative process, we also synthesize and discuss a list\nof design implications for the design of future AI-augmented visualization\nsystems to better assist experts' sensemaking processes in multi-modal health\nsensing data.", "AI": {"tldr": "This paper presents Vital Insight (VI), a novel LLM-assisted prototype system that enhances human-centered sensemaking of multi-modal passive sensing data for health applications, addressing challenges faced by experts.", "motivation": "To bridge the gap in deriving meaningful insights from passive sensing streams and aid experts in the complex sensemaking process.", "method": "Conducted three rounds of user studies with 21 experts following a human-centered design process to develop and evaluate Vital Insight, a prototype system for human-in-the-loop inference and visualizations of sensing data.", "result": "Experts interacted with the Vital Insight prototype, leading to the development of an expert sensemaking model and identification of design implications for future AI-augmented visualization systems.", "conclusion": "The research highlights the importance of supporting experts in their sensemaking processes through AI-assisted tools, particularly for multi-modal health sensing data.", "key_contributions": ["Development of the Vital Insight (VI) prototype system for sensemaking", "Creation of an expert sensemaking model based on user interactions", "Identification of design implications for AI-augmented visualization systems"], "limitations": "", "keywords": ["Human-Computer Interaction", "Machine Learning", "Health Informatics", "Sensemaking", "AI-augmented visualizations"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.06528", "pdf": "https://arxiv.org/pdf/2507.06528.pdf", "abs": "https://arxiv.org/abs/2507.06528", "title": "InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior", "authors": ["Huisheng Wang", "Zhuoshi Pan", "Hangjing Zhang", "Mingxiao Liu", "Hanqing Gao", "H. Vicky Zhao"], "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG"], "comment": null, "summary": "Aligning Large Language Models (LLMs) with investor decision-making processes\nunder herd behavior is a critical challenge in behavioral finance, which\ngrapples with a fundamental limitation: the scarcity of real-user data needed\nfor Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM\noutputs and human behavioral patterns, its reliance on massive authentic data\nimposes substantial collection costs and privacy risks. We propose InvestAlign,\na novel framework that constructs high-quality SFT datasets by leveraging\ntheoretical solutions to similar and simple optimal investment problems rather\nthan complex scenarios. Our theoretical analysis demonstrates that training\nLLMs with InvestAlign-generated data achieves faster parameter convergence than\nusing real-user data, suggesting superior learning efficiency. Furthermore, we\ndevelop InvestAgent, an LLM agent fine-tuned with InvestAlign, which\ndemonstrates significantly closer alignment to real-user data than pre-SFT\nmodels in both simple and complex investment problems. This highlights our\nproposed InvestAlign as a promising approach with the potential to address\ncomplex optimal investment problems and align LLMs with investor\ndecision-making processes under herd behavior. Our code is publicly available\nat https://github.com/thu-social-network-research-group/InvestAlign.", "AI": {"tldr": "InvestAlign framework constructs high-quality SFT datasets from theoretical investment solutions, improving LLM training efficiency and alignment with investor behaviors.", "motivation": "Aligning LLMs with investor decision-making and addressing the scarcity of real-user data for effective supervised fine-tuning in behavioral finance.", "method": "The InvestAlign framework generates SFT datasets from simpler theoretical investment problems rather than complex scenarios, enabling LLMs to learn efficiently.", "result": "LLMs trained with InvestAlign data show faster parameter convergence and greater alignment with real-user data compared to pre-fine-tuning models, especially in both simple and complex investment problems.", "conclusion": "InvestAlign presents a promising method for enhancing LLM performance and alignment in behavioral finance contexts involving herd behavior.", "key_contributions": ["Development of the InvestAlign framework for SFT dataset generation.", "Demonstration of improved learning efficiency with theoretical investment solutions.", "Creation of InvestAgent, an LLM agent showing superior alignment with real-user data."], "limitations": "", "keywords": ["Large Language Models", "Behavioral Finance", "Supervised Fine-Tuning", "Herd Behavior", "Investment Decision-Making"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06539", "pdf": "https://arxiv.org/pdf/2507.06539.pdf", "abs": "https://arxiv.org/abs/2507.06539", "title": "Large Language Model for Extracting Complex Contract Information in Industrial Scenes", "authors": ["Yunyang Cao", "Yanjun Li", "Silong Dai"], "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a high-quality dataset construction method for complex\ncontract information extraction tasks in industrial scenarios and fine-tunes a\nlarge language model based on this dataset. Firstly, cluster analysis is\nperformed on industrial contract texts, and GPT-4 and GPT-3.5 are used to\nextract key information from the original contract data, obtaining high-quality\ndata annotations. Secondly, data augmentation is achieved by constructing new\ntexts, and GPT-3.5 generates unstructured contract texts from randomly combined\nkeywords, improving model robustness. Finally, the large language model is\nfine-tuned based on the high-quality dataset. Experimental results show that\nthe model achieves excellent overall performance while ensuring high field\nrecall and precision and considering parsing efficiency. LoRA, data balancing,\nand data augmentation effectively enhance model accuracy and robustness. The\nproposed method provides a novel and efficient solution for industrial contract\ninformation extraction tasks.", "AI": {"tldr": "The paper presents a method for constructing a high-quality dataset for complex contract information extraction, utilizing GPT models for annotation and data augmentation, and demonstrating improved model performance.", "motivation": "To address challenges in extracting information from industrial contracts and enhance the performance of language models in this domain.", "method": "Cluster analysis on industrial contract texts, followed by using GPT-4 and GPT-3.5 for extracting annotations and generating unstructured texts through data augmentation.", "result": "The fine-tuned large language model exhibits excellent overall performance with high recall and precision in information extraction tasks.", "conclusion": "The proposed method offers a novel and efficient approach for industrial contract information extraction, significantly improving model accuracy and robustness.", "key_contributions": ["Development of a high-quality dataset construction method for contract information extraction", "Utilization of GPT models for effective data annotation and augmentation", "Demonstration of improved model performance and robustness through experimental results"], "limitations": "", "keywords": ["contract information extraction", "dataset construction", "large language models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.06565", "pdf": "https://arxiv.org/pdf/2507.06565.pdf", "abs": "https://arxiv.org/abs/2507.06565", "title": "The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production", "authors": ["Juan B. Gutiérrez"], "categories": ["cs.CL", "cs.LG", "68T01, 60J10, 91D30, 05C82, 68T50, 68W20, 94A15", "I.2.7; I.2.11; G.3"], "comment": "27 pages, 3 figures, 4 tables, 1 algorithm, 28 references", "summary": "Large-language models turn writing into a live exchange between humans and\nsoftware. We capture this new medium with a discursive-network model that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. Broadening the focus from isolated hallucinations, we define\ninvalidation (any factual, logical, or structural breach) and show it follows\nfour hazards: drift from truth, self-repair, fresh fabrication, and external\ndetection. A general mathematical model of discursive networks is developed to\nprovide valuable insights: A network governed only by drift and self-repair\nstabilizes at a modest error rate; adding fabrication reproduces the high rates\nseen in current LLMs. Giving each false claim even a small chance of peer\nreview shifts the system to a truth-dominant state. We operationalize peer\nreview with the open-source \\emph{Flaws-of-Others (FOO) algorithm}: a\nconfigurable loop in which any set of agents critique one another while a\nharmoniser merges their verdicts. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nwiring imperfect ones into networks that keep each other honest.", "AI": {"tldr": "This paper presents a discursive-network model for understanding the interactions between humans and large-language models (LLMs) and explores the concept of 'invalidation' along with hazards that impact the reliability of these interactions.", "motivation": "To explore how large-language models function in a live exchange with users and to investigate factors that compromise the reliability of such exchanges.", "method": "Development of a mathematical model for discursive networks that examines the dynamics between humans and LLMs, focusing on the concept of invalidation and its associated hazards.", "result": "The study finds that a network of LLMs governed by certain conditions can stabilize at low error rates, while adding fabrication increases error rates similar to existing LLM behaviors. Peer review mechanisms can shift the network towards a truth-dominant state.", "conclusion": "Reliability in interactions between humans and LLMs is achieved not by perfect models but through interconnected networks of imperfect models that critically evaluate each other.", "key_contributions": ["Introduction of a discursive-network model for LLM interactions", "Identification of four hazards affecting truth in LLM exchanges", "Development of the Flaws-of-Others (FOO) algorithm for peer review mechanisms"], "limitations": "The model may not account for all real-world complexities in human-LLM interactions and is based on theoretical assumptions.", "keywords": ["large-language models", "discursive networks", "invalidation", "peer review", "reliability"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2507.06571", "pdf": "https://arxiv.org/pdf/2507.06571.pdf", "abs": "https://arxiv.org/abs/2507.06571", "title": "Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis", "authors": ["Srihari K B", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a unified food-domain QA framework that combines a large-scale\nmultimodal knowledge graph (MMKG) with generative AI. Our MMKG links 13,000\nrecipes, 3,000 ingredients, 140,000 relations, and 14,000 images. We generate\n40,000 QA pairs using 40 templates and LLaVA/DeepSeek augmentation. Joint\nfine-tuning of Meta LLaMA 3.1-8B and Stable Diffusion 3.5-Large improves\nBERTScore by 16.2\\%, reduces FID by 37.8\\%, and boosts CLIP alignment by\n31.1\\%. Diagnostic analyses-CLIP-based mismatch detection (35.2\\% to 7.3\\%) and\nLLaVA-driven hallucination checks-ensure factual and visual fidelity. A hybrid\nretrieval-generation strategy achieves 94.1\\% accurate image reuse and 85\\%\nadequacy in synthesis. Our results demonstrate that structured knowledge and\nmultimodal generation together enhance reliability and diversity in food QA.", "AI": {"tldr": "A framework combining multimodal knowledge graphs and generative AI for food-domain QA improves accuracy and reliability of food information retrieval.", "motivation": "To enhance the reliability and diversity in food-related question answering through the integration of structured knowledge and AI generation techniques.", "method": "The authors developed a large-scale multimodal knowledge graph linking recipes, ingredients, and images. They generated QA pairs and fine-tuned models (Meta LLaMA and Stable Diffusion) to improve various performance metrics.", "result": "The framework demonstrated significant improvements: BERTScore increased by 16.2%, FID reduced by 37.8%, and CLIP alignment boosted by 31.1%. Diagnostic analyses showed a drastic reduction in mismatch and hallucination rates.", "conclusion": "The use of structured knowledge combined with generative techniques effectively enhances QA performance in the food domain, ensuring both factual and visual accuracy.", "key_contributions": ["Development of a large-scale multimodal knowledge graph for food QA", "Joint fine-tuning of generative models resulting in improved metrics", "Hybrid retrieval-generation strategy achieving high accuracy in image reuse"], "limitations": "", "keywords": ["multimodal knowledge graph", "generative AI", "food-domain QA", "LLaMA", "Stable Diffusion"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.06607", "pdf": "https://arxiv.org/pdf/2507.06607.pdf", "abs": "https://arxiv.org/abs/2507.06607", "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation", "authors": ["Liliang Ren", "Congcong Chen", "Haoran Xu", "Young Jin Kim", "Adam Atkinson", "Zheng Zhan", "Jiankai Sun", "Baolin Peng", "Liyuan Liu", "Shuohang Wang", "Hao Cheng", "Jianfeng Gao", "Weizhu Chen", "Yelong Shen"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in language modeling have demonstrated the effectiveness of\nState Space Models (SSMs) for efficient sequence modeling. While hybrid\narchitectures such as Samba and the decoder-decoder architecture, YOCO, have\nshown promising performance gains over Transformers, prior works have not\ninvestigated the efficiency potential of representation sharing between SSM\nlayers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet\neffective mechanism for efficient memory sharing across layers. We apply it to\ncreate SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in\nthe cross-decoder to share memory readout states from a Samba-based\nself-decoder. SambaY significantly enhances decoding efficiency, preserves\nlinear pre-filling time complexity, and boosts long-context performance, all\nwhile eliminating the need for explicit positional encoding. Through extensive\nscaling experiments, we demonstrate that our model exhibits a significantly\nlower irreducible loss compared to a strong YOCO baseline, indicating superior\nperformance scalability under large-scale compute regimes. Our largest model\nenhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves\nsignificantly better performance than Phi4-mini-Reasoning on reasoning tasks\nsuch as Math500, AIME24/25, and GPQA Diamond without any reinforcement\nlearning, while delivering up to 10x higher decoding throughput on 2K-length\nprompts with 32K generation length under the vLLM inference framework. We\nrelease our training codebase on open-source data at\nhttps://github.com/microsoft/ArchScale.", "AI": {"tldr": "Introduction of Gated Memory Unit (GMU) for efficient memory sharing in language modeling with the SambaY architecture, enhancing decoding efficiency and long-context performance.", "motivation": "Investigate the efficiency potential of representation sharing in State Space Models (SSMs) for language modeling.", "method": "Propose the Gated Memory Unit (GMU) and apply it to the SambaY architecture, which integrates GMUs in a cross-decoder setup.", "result": "SambaY achieves lower irreducible loss compared to YOCO, enhancing scalability and 10x higher decoding throughput on 2K-length prompts.", "conclusion": "The model shows significant performance improvements in reasoning tasks while maintaining efficiency, with all code released for open-source use.", "key_contributions": ["Introduction of Gated Memory Unit (GMU) for memory sharing", "Development of SambaY architecture", "Demonstrated significant performance scalability and efficiency improvements"], "limitations": "", "keywords": ["language modeling", "State Space Models", "Gated Memory Unit", "SambaY architecture", "decoding efficiency"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.06622", "pdf": "https://arxiv.org/pdf/2507.06622.pdf", "abs": "https://arxiv.org/abs/2507.06622", "title": "FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation", "authors": ["Boshko Koloski", "Senja Pollak", "Roberto Navigli", "Blaž Škrlj"], "categories": ["cs.CL"], "comment": null, "summary": "Building on the success of Large Language Models (LLMs), LLM-based\nrepresentations have dominated the document representation landscape, achieving\ngreat performance on the document embedding benchmarks. However, the\nhigh-dimensional, computationally expensive embeddings from LLMs tend to be\neither too generic or inefficient for domain-specific applications. To address\nthese limitations, we introduce FuDoBa a Bayesian optimisation-based method\nthat integrates LLM-based embeddings with domain-specific structured knowledge,\nsourced both locally and from external repositories like WikiData. This fusion\nproduces low-dimensional, task-relevant representations while reducing training\ncomplexity and yielding interpretable early-fusion weights for enhanced\nclassification performance. We demonstrate the effectiveness of our approach on\nsix datasets in two domains, showing that when paired with robust AutoML-based\nclassifiers, our proposed representation learning approach performs on par\nwith, or surpasses, those produced solely by the proprietary LLM-based\nembedding baselines.", "AI": {"tldr": "This paper presents FuDoBa, a method that integrates LLM-based embeddings with structured domain-specific knowledge to create efficient, low-dimensional document representations for improved classification performance.", "motivation": "To overcome the limitations of high-dimensional and generic embeddings from Large Language Models for domain-specific applications.", "method": "FuDoBa employs Bayesian optimization to fuse LLM-based embeddings with structured knowledge from local and external sources like WikiData.", "result": "FuDoBa produced low-dimensional, task-relevant representations that reduce training complexity and yield interpretable weights, performing comparably or better than proprietary LLM-based embedding methods across six datasets from two domains.", "conclusion": "The proposed approach enhances classification performance and makes the process more interpretable by leveraging both LLM embeddings and domain-specific structured knowledge.", "key_contributions": ["Introduction of FuDoBa for embedding fusion", "Utilization of Bayesian optimization for improved efficiency", "Benchmarking on six datasets demonstrating superior performance"], "limitations": "", "keywords": ["Large Language Models", "document embeddings", "Bayesian optimization", "classification", "structured knowledge"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.06623", "pdf": "https://arxiv.org/pdf/2507.06623.pdf", "abs": "https://arxiv.org/abs/2507.06623", "title": "Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review", "authors": ["James Stewart-Evans", "Emma Wilson", "Tessa Langley", "Andrew Prayle", "Angela Hands", "Karen Exley", "Jo Leonardi-Bee"], "categories": ["cs.CL", "cs.AI"], "comment": "44 pages, 4 figures", "summary": "The data extraction stages of reviews are resource-intensive, and researchers\nmay seek to expediate data extraction using online (large language models) LLMs\nand review protocols. Claude 3.5 Sonnet was used to trial two approaches that\nused a review protocol to prompt data extraction from 10 evidence sources\nincluded in a case study scoping review. A protocol-based approach was also\nused to review extracted data. Limited performance evaluation was undertaken\nwhich found high accuracy for the two extraction approaches (83.3% and 100%)\nwhen extracting simple, well-defined citation details; accuracy was lower (9.6%\nand 15.8%) when extracting more complex, subjective data items. Considering all\ndata items, both approaches had precision >90% but low recall (<25%) and F1\nscores (<40%). The context of a complex scoping review, open response types and\nmethodological approach likely impacted performance due to missed and\nmisattributed data. LLM feedback considered the baseline extraction accurate\nand suggested minor amendments: four of 15 (26.7%) to citation details and 8 of\n38 (21.1%) to key findings data items were considered to potentially add value.\nHowever, when repeating the process with a dataset featuring deliberate errors,\nonly 2 of 39 (5%) errors were detected. Review-protocol-based methods used for\nexpediency require more robust performance evaluation across a range of LLMs\nand review contexts with comparison to conventional prompt engineering\napproaches. We recommend researchers evaluate and report LLM performance if\nusing them similarly to conduct data extraction or review extracted data. LLM\nfeedback contributed to protocol adaptation and may assist future review\nprotocol drafting.", "AI": {"tldr": "This paper examines the use of LLMs for data extraction in systematic reviews, evaluating their performance with varying complexity of data.", "motivation": "To expedite data extraction in reviews using LLMs under a structured protocol and assess their effectiveness.", "method": "Two approaches with Claude 3.5 Sonnet were trialed to prompt data extraction and review from evidence sources in a scoping review.", "result": "The high accuracy (83.3% to 100%) for simple data contrasted with low accuracy (9.6% to 15.8%) for complex items, highlighting issues with recall and F1 scores.", "conclusion": "LLM-based methods require more thorough evaluation and comparisons to traditional approaches, with suggested improvements based on LLM feedback.", "key_contributions": ["Evaluation of LLM performance in data extraction contexts", "Identification of strengths and weaknesses in LLM data capture", "Recommendations for future protocols using LLMs"], "limitations": "Performance evaluation was limited and may not generalize across diverse contexts or data types.", "keywords": ["data extraction", "LLM", "systematic reviews", "protocols", "health informatics"], "importance_score": 8, "read_time_minutes": 44}}
{"id": "2507.06658", "pdf": "https://arxiv.org/pdf/2507.06658.pdf", "abs": "https://arxiv.org/abs/2507.06658", "title": "Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models", "authors": ["Gennadii Iakovlev"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This project introduces a new measure of elite polarization via actor and\nsubject detection using artificial intelligence. I identify when politicians\nmention one another in parliamentary speeches, note who is speaking and who is\nbeing addressed, and assess the emotional temperature behind these evaluations.\nThis maps how elites evaluate their various out-parties, allowing us to create\nan index of mutual out-party hostility, that is, elite polarization. While I\nanalyzed polarization data over the past four decades for the UK, and two\ndecades for Hungary and Italy, my approach lays the groundwork for a\ntwenty-year, EU-wide time-series dataset on elite polarization. I obtain the\nresults that can be aggregated by party and quarter. The resulting index\ndemonstrates a good face validity: it reacts to events such as electoral\ncampaigns, country- and party-level crises, and to parties losing and assuming\npower.", "AI": {"tldr": "The paper presents a novel measure of elite polarization using AI to analyze political speeches and emotional evaluations among political actors, establishing a new index of mutual out-party hostility.", "motivation": "To create a comprehensive measure of elite polarization that can be analyzed over time and political contexts. This measure helps understand how political elites evaluate one another in terms of hostility.", "method": "The methodology includes analyzing parliamentary speeches to detect actors and subjects, assessing emotional tones, and aggregating data to create an index that captures mutual out-party hostility.", "result": "The developed index effectively reflects levels of elite polarization in several countries over recent decades, responding to significant political events like elections and crises.", "conclusion": "The index serves as a valuable tool for measuring and understanding elite polarization, potentially applicable for a broader EU-wide dataset in the future.", "key_contributions": ["Development of a novel index of elite polarization based on AI analysis of parliamentary speeches.", "Establishment of a methodology to assess and quantify mutual out-party hostility over long time periods.", "Presentation of a dataset that can inform political science research on elite behavior."], "limitations": "The analysis is currently limited to specific countries (UK, Hungary, Italy) and may not capture the full spectrum of political contexts across all EU nations.", "keywords": ["elite polarization", "AI analysis", "political speeches", "mutual hostility index", "time-series dataset"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2507.06715", "pdf": "https://arxiv.org/pdf/2507.06715.pdf", "abs": "https://arxiv.org/abs/2507.06715", "title": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs", "authors": ["Garapati Keerthana", "Manik Gupta"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "12 pages, 4 figures", "summary": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust.", "AI": {"tldr": "Introducing CLI-RAG, a framework for structured clinical text generation using LLMs that addresses the challenges of unstructured patient data and long clinical notes.", "motivation": "To improve the generation of clinical text while overcoming challenges posed by unstructured and dense clinical notes.", "method": "The framework uses a hierarchical chunking strategy and a dual-stage retrieval mechanism to generate structured progress notes from clinical data.", "result": "Achieved an average alignment score of 87.7%, higher than the baseline of 80.7% from real clinician-authored notes, with high consistency across LLMs.", "conclusion": "CLI-RAG demonstrates efficient handling of clinical documents, ensuring temporal and semantic alignment and reinforcing trust in generated outputs.", "key_contributions": ["Introduction of CLI-RAG for structured clinical text generation", "Hierarchical chunking strategy tailored for clinical document structure", "Task-specific dual-stage retrieval mechanism for enhanced relevance"], "limitations": "", "keywords": ["clinical text generation", "large language models", "retrieval-augmented generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.06722", "pdf": "https://arxiv.org/pdf/2507.06722.pdf", "abs": "https://arxiv.org/abs/2507.06722", "title": "On the Effect of Uncertainty on Layer-wise Inference Dynamics", "authors": ["Sunwoo Kim", "Haneul Yoo", "Alice Oh"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to Actionable Interpretability Workshop - ICML 2025", "summary": "Understanding how large language models (LLMs) internally represent and\nprocess their predictions is central to detecting uncertainty and preventing\nhallucinations. While several studies have shown that models encode uncertainty\nin their hidden states, it is underexplored how this affects the way they\nprocess such hidden states. In this work, we demonstrate that the dynamics of\noutput token probabilities across layers for certain and uncertain outputs are\nlargely aligned, revealing that uncertainty does not seem to affect inference\ndynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to\nanalyze the layer-wise probability trajectories of final prediction tokens\nacross 11 datasets and 5 models. Using incorrect predictions as those with\nhigher epistemic uncertainty, our results show aligned trajectories for certain\nand uncertain predictions that both observe abrupt increases in confidence at\nsimilar layers. We balance this finding by showing evidence that more competent\nmodels may learn to process uncertainty differently. Our findings challenge the\nfeasibility of leveraging simplistic methods for detecting uncertainty at\ninference. More broadly, our work demonstrates how interpretability methods may\nbe used to investigate the way uncertainty affects inference.", "AI": {"tldr": "This paper explores how large language models (LLMs) deal with uncertainty in their predictions and shows that this uncertainty does not significantly alter the inference dynamics across different model layers.", "motivation": "Understanding the representation and processing of uncertainty in LLMs is crucial for improving their reliability and performance, particularly for preventing hallucinations in outputs.", "method": "The study employs the Tuned Lens method to analyze output token probability trajectories across various layers for different datasets and models, comparing certain and uncertain predictions.", "result": "The analysis reveals that certain and uncertain outputs exhibit aligned probability trajectories across layers, indicating that uncertainty does not significantly affect the inference dynamics.", "conclusion": "The findings suggest caution in using simplistic methods to detect uncertainty in LLM outputs, highlighting the importance of using interpretability methods to better understand inference processes.", "key_contributions": ["Demonstrates the aligned dynamics of probabilities for certain and uncertain outputs in LLMs.", "Challenges existing notions about detecting uncertainty in model outputs.", "Shows the utility of interpretability methods in analyzing uncertainty effects on inference."], "limitations": "The study focuses on only a limited number of datasets and models, which may affect generalizability.", "keywords": ["Large Language Models", "Uncertainty", "Interpretability", "Inference Dynamics", "Model Competence"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06753", "pdf": "https://arxiv.org/pdf/2507.06753.pdf", "abs": "https://arxiv.org/abs/2507.06753", "title": "KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution", "authors": ["Ye Kyaw Thu", "Thura Aung", "Thazin Myint Oo", "Thepchai Supnithi"], "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6"], "comment": "10 pages, 3 figures, 4 tables", "summary": "This paper presents the first application of Kolmogorov-Arnold Convolution\nfor Text (KAConvText) in sentence classification, addressing three tasks:\nimbalanced binary hate speech detection, balanced multiclass news\nclassification, and imbalanced multiclass ethnic language identification. We\ninvestigate various embedding configurations, comparing random to fastText\nembeddings in both static and fine-tuned settings, with embedding dimensions of\n100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs\nand CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we\ninvestigated KAConvText with different classification heads - MLP and KAN,\nwhere using KAN head supports enhanced interpretability. Results show that\nKAConvText-MLP with fine-tuned fastText embeddings achieves the best\nperformance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,\n92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%\naccuracy (F1-score = 0.9982) for language identification.", "AI": {"tldr": "This paper introduces the KAConvText method for sentence classification, achieving high accuracy in various tasks such as hate speech detection and news classification.", "motivation": "To apply Kolmogorov-Arnold Convolution for text in the context of sentence classification, particularly for imbalanced tasks.", "method": "The paper examines different embedding configurations (static and fine-tuned fastText) and compares them with standard CNNs; employs MLP and KAN classification heads to enhance interpretability.", "result": "KAConvText-MLP with fine-tuned fastText embeddings achieves top accuracies: 91.23% for hate speech detection, 92.66% for news classification, and 99.82% for language identification.", "conclusion": "KAConvText demonstrates promising performance in various text classification tasks and enhances interpretability when using the KAN head.", "key_contributions": ["Introduction of KAConvText for text classification tasks", "Comparison of different embedding strategies in classification systems", "Demonstration of enhanced interpretability with KAN classification head"], "limitations": "Limited to certain types of text classification tasks; further research needed for broader applications.", "keywords": ["Kolmogorov-Arnold Convolution", "text classification", "hate speech detection", "machine learning", "embedding techniques"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.06774", "pdf": "https://arxiv.org/pdf/2507.06774.pdf", "abs": "https://arxiv.org/abs/2507.06774", "title": "Checklist Engineering Empowers Multilingual LLM Judges", "authors": ["Mohammad Ghiasvand Mohammadkhani", "Hamid Beigy"], "categories": ["cs.CL"], "comment": null, "summary": "Automated text evaluation has long been a central issue in Natural Language\nProcessing (NLP). Recently, the field has shifted toward using Large Language\nModels (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While\npromising and easily adaptable across tasks, this approach has seen limited\nexploration in multilingual contexts. Existing multilingual studies often rely\non proprietary models or require extensive training data for fine-tuning,\nraising concerns about cost, time, and efficiency. In this paper, we propose\nChecklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free\nframework that uses checklist intuition for multilingual evaluation with an\nopen-source model. Experiments across multiple languages and three benchmark\ndatasets, under both pointwise and pairwise settings, show that our method\ngenerally surpasses the baselines and performs on par with the GPT-4o model.", "AI": {"tldr": "Proposal of a framework, CE-Judge, for multilingual text evaluation using LLMs without the need for extensive training.", "motivation": "To address the limitations of current multilingual evaluations in NLP that primarily rely on proprietary models and extensive training data.", "method": "Introduction of Checklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free framework for multilingual evaluation utilizing checklist principles.", "result": "CE-Judge outperforms baseline methods and matches the performance of the state-of-the-art GPT-4o model across multiple languages and datasets.", "conclusion": "CE-Judge provides an effective and efficient approach for multilingual evaluation in NLP using open-source models without extensive training.", "key_contributions": ["Introduction of a training-free multilingual evaluation framework.", "Demonstration of superior performance over baseline models.", "Utilization of open-source models for broader accessibility."], "limitations": "", "keywords": ["Natural Language Processing", "Large Language Models", "multilingual evaluation", "checklist engineering", "LLM-as-a-Judge"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.06795", "pdf": "https://arxiv.org/pdf/2507.06795.pdf", "abs": "https://arxiv.org/abs/2507.06795", "title": "Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications", "authors": ["Seonwu Kim", "Yohan Na", "Kihun Kim", "Hanhee Cho", "Geun Lim", "Mintae Kim", "Seongik Park", "Ki Hyun Kim", "Youngsub Han", "Byoung-Ki Jeon"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review", "summary": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.", "AI": {"tldr": "This paper explores the effectiveness of Domain Adaptive Continual Pretraining (DACP) for small LLMs in enterprise applications, demonstrating improvements in domain performance and general capabilities.", "motivation": "With organizations lacking the infrastructure for large-scale LLM deployment, small LLMs are seen as practical alternatives, necessitating continued research on their enhancement through techniques like DACP.", "method": "The study validates DACP across various foundation models and service domains through extensive experiments and real-world evaluations.", "result": "DACP-applied small LLMs show substantial gains in target domain performance while maintaining general capabilities, indicating their viability for enterprise use.", "conclusion": "The findings propose DACP as a cost-effective and scalable solution for enterprises seeking to leverage LLMs without high infrastructure costs.", "key_contributions": ["Validation of DACP for small LLMs in commercial settings", "Demonstration of improved domain adaptation capabilities", "Presentation of a scalable approach for enterprise deployment"], "limitations": "", "keywords": ["Domain Adaptive Continual Pretraining", "small LLMs", "enterprise applications"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.06803", "pdf": "https://arxiv.org/pdf/2507.06803.pdf", "abs": "https://arxiv.org/abs/2507.06803", "title": "Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams", "authors": ["Matthew Anderson Hendricks", "Alice Cicirello"], "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only.", "AI": {"tldr": "This paper presents a method for automating the generation of computational models of dynamical systems using domain knowledge and NLP strategies, particularly focusing on SysML diagrams.", "motivation": "To expedite the design and deployment of engineering dynamical systems by leveraging domain and expert knowledge.", "method": "The approach involves five steps using SysML diagrams to extract information about system components, employing NLP strategies and LLMs to enhance the generation process of diagrams and computational models.", "result": "The method demonstrated improved performance in generating computational models of complex dynamical systems compared to using LLMs alone, illustrated through various case studies including a simple pendulum example.", "conclusion": "The proposed strategy can be applied broadly across various domains and is not confined to specific computational systems or software.", "key_contributions": ["Automated generation of SysML diagrams for dynamical systems", "Integration of NLP and LLM strategies to improve model output", "Demonstration of end-to-end automated model generation with case studies"], "limitations": "", "keywords": ["Dynamical Systems", "SysML", "NLP", "LLM", "Automated Generation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.06829", "pdf": "https://arxiv.org/pdf/2507.06829.pdf", "abs": "https://arxiv.org/abs/2507.06829", "title": "Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework", "authors": ["Zenan Xu", "Zexuan Qiu", "Guanhua Huang", "Kun Li", "Siheng Li", "Chenchen Zhang", "Kejiao Li", "Qi Yi", "Yuhao Jiang", "Bo Zhou", "Fengzong Lian", "Zhanhui Kang"], "categories": ["cs.CL"], "comment": "13 pages, 5 fiures", "summary": "Recent advances in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, with inference-time scaling emerging as\na key technique. Contemporary approaches leverage either sequential reasoning\n(iteratively extending chains of thought) or parallel reasoning (generating\nmultiple solutions simultaneously) to scale inference. However, both paradigms\nface fundamental limitations: sequential scaling typically relies on arbitrary\ntoken budgets for termination, leading to inefficiency or premature cutoff;\nwhile parallel scaling often lacks coordination among parallel branches and\nrequires intrusive fine-tuning to perform effectively. In light of these\nchallenges, we aim to design a flexible test-time collaborative inference\nframework that exploits the complementary strengths of both sequential and\nparallel reasoning paradigms. Towards this goal, the core challenge lies in\ndeveloping an efficient and accurate intrinsic quality metric to assess model\nresponses during collaborative inference, enabling dynamic control and early\ntermination of the reasoning trace. To address this challenge, we introduce\nsemantic entropy (SE), which quantifies the semantic diversity of parallel\nmodel responses and serves as a robust indicator of reasoning quality due to\nits strong negative correlation with accuracy...", "AI": {"tldr": "This paper introduces a collaborative inference framework for large language models that combines sequential and parallel reasoning techniques, addressing their limitations by developing an intrinsic quality metric, semantic entropy, to improve efficiency and accuracy.", "motivation": "The motivation behind this research is to overcome the fundamental limitations of current inference techniques in large language models by creating a framework that can effectively leverage both sequential and parallel reasoning.", "method": "The authors propose a test-time collaborative inference framework which utilizes an intrinsic quality metric, semantic entropy, to assess the quality of model responses dynamically and enable controlled reasoning processes.", "result": "The introduction of semantic entropy shows a strong negative correlation with accuracy, providing a reliable metric for evaluating reasoning quality during collaborative inference, which demonstrates improved efficiency.", "conclusion": "The proposed framework aims to enhance the performance of large language models by effectively combining sequential and parallel reasoning while addressing their individual limitations through the use of semantic entropy as a quality metric.", "key_contributions": ["Development of a collaborative inference framework for LLMs", "Introduction of semantic entropy as a quality assessment metric", "Demonstration of improved efficiency in LLM inference"], "limitations": "The study does not thoroughly explore the practical implementation complexities of the proposed framework in real-world applications.", "keywords": ["large language models", "collaborative inference", "semantic entropy", "reasoning paradigms", "AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.06838", "pdf": "https://arxiv.org/pdf/2507.06838.pdf", "abs": "https://arxiv.org/abs/2507.06838", "title": "Shifting from Ranking to Set Selection for Retrieval Augmented Generation", "authors": ["Dahyun Lee", "Yongrae Jo", "Haeju Park", "Moontae Lee"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted to ACL 2025 Oral", "summary": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR", "AI": {"tldr": "This paper introduces SETR, a set-wise passage selection approach for Retrieval-Augmented Generation that enhances multi-hop question answering by collectively retrieving passages that meet complex information needs.", "motivation": "Existing retrieval methods in RAG fail to collectively satisfy complex information requirements in multi-hop question answering, often only considering individual passage relevance.", "method": "SETR uses Chain-of-Thought reasoning to identify query information needs and selects an optimal set of passages that work together effectively.", "result": "SETR significantly improves answer correctness and retrieval quality compared to both proprietary and open-source rerankers on multi-hop RAG benchmarks.", "conclusion": "SETR offers an efficient alternative to traditional rerankers in RAG systems, enhancing the retrieval process for complex queries.", "key_contributions": ["Introduction of SETR for collective passage selection", "Use of Chain-of-Thought reasoning for identifying query needs", "Outperformance against existing rerankers in multi-hop question answering"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "multi-hop question answering", "Chain-of-Thought reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.06893", "pdf": "https://arxiv.org/pdf/2507.06893.pdf", "abs": "https://arxiv.org/abs/2507.06893", "title": "Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights", "authors": ["Alexandra Abbas", "Celia Waggoner", "Justin Olive"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI evaluations have become critical tools for assessing large language model\ncapabilities and safety. This paper presents practical insights from eight\nmonths of maintaining $inspect\\_evals$, an open-source repository of 70+\ncommunity-contributed AI evaluations. We identify key challenges in\nimplementing and maintaining AI evaluations and develop solutions including:\n(1) a structured cohort management framework for scaling community\ncontributions, (2) statistical methodologies for optimal resampling and\ncross-model comparison with uncertainty quantification, and (3) systematic\nquality control processes for reproducibility. Our analysis reveals that AI\nevaluation requires specialized infrastructure, statistical rigor, and\ncommunity coordination beyond traditional software development practices.", "AI": {"tldr": "The paper discusses challenges and solutions in maintaining an open-source AI evaluation repository, emphasizing community contributions and statistical methodologies.", "motivation": "To improve the assessment of AI models through structured and community-driven evaluation practices.", "method": "The authors developed a management framework, statistical methodologies for evaluation, and quality control processes.", "result": "The analysis highlighted the need for specialized infrastructure and rigorous statistical methods to support AI evaluations.", "conclusion": "AI evaluations require careful management and a structured approach to ensure reliability and scalability.", "key_contributions": ["Development of a cohort management framework for community contributions.", "Introduction of statistical methodologies for evaluation and comparison.", "Establishment of quality control processes for reproducibility."], "limitations": "", "keywords": ["AI evaluation", "large language models", "community contributions", "statistical methodologies", "quality control"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.06895", "pdf": "https://arxiv.org/pdf/2507.06895.pdf", "abs": "https://arxiv.org/abs/2507.06895", "title": "SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN", "authors": ["Luca Mariotti", "Veronica Guidetti", "Federica Mandreoli"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.", "AI": {"tldr": "SCoRE is a modular relation extraction system that enhances knowledge graph enrichment using large language models without the need for fine-tuning.", "motivation": "To meet the demand for efficient knowledge graph enrichment and improve relation extraction performance under low-supervision settings.", "method": "SCoRE combines supervised contrastive learning with a Bayesian k-Nearest Neighbors classifier for multi-label classification, enabling seamless integration with pre-trained language models.", "result": "Experiments demonstrate that SCoRE matches or exceeds state-of-the-art methods while significantly reducing energy consumption, particularly with noisy annotations in distantly supervised corpora.", "conclusion": "SCoRE's modular, efficient, and adaptable design makes it a superior choice for practical relation extraction tasks.", "key_contributions": ["Introduction of a modular RE system (SCoRE)", "Development of new evaluation metrics (CSD and P@R)", "Release of the Wiki20d benchmark dataset for RE research"], "limitations": "", "keywords": ["relation extraction", "knowledge graph", "large language models", "distant supervision", "benchmark dataset"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.06899", "pdf": "https://arxiv.org/pdf/2507.06899.pdf", "abs": "https://arxiv.org/abs/2507.06899", "title": "VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation", "authors": ["Ziang Ye", "Yang Zhang", "Wentao Shi", "Xiaoyu You", "Fuli Feng", "Tat-Seng Chua"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents powered by Large Vision-Language Models\n(LVLMs) have emerged as a revolutionary approach to automating human-machine\ninteractions, capable of autonomously operating personal devices (e.g., mobile\nphones) or applications within the device to perform complex real-world tasks\nin a human-like manner. However, their close integration with personal devices\nraises significant security concerns, with many threats, including backdoor\nattacks, remaining largely unexplored. This work reveals that the visual\ngrounding of GUI agent-mapping textual plans to GUI elements-can introduce\nvulnerabilities, enabling new types of backdoor attacks. With backdoor attack\ntargeting visual grounding, the agent's behavior can be compromised even when\ngiven correct task-solving plans. To validate this vulnerability, we propose\nVisualTrap, a method that can hijack the grounding by misleading the agent to\nlocate textual plans to trigger locations instead of the intended targets.\nVisualTrap uses the common method of injecting poisoned data for attacks, and\ndoes so during the pre-training of visual grounding to ensure practical\nfeasibility of attacking. Empirical results show that VisualTrap can\neffectively hijack visual grounding with as little as 5% poisoned data and\nhighly stealthy visual triggers (invisible to the human eye); and the attack\ncan be generalized to downstream tasks, even after clean fine-tuning. Moreover,\nthe injected trigger can remain effective across different GUI environments,\ne.g., being trained on mobile/web and generalizing to desktop environments.\nThese findings underscore the urgent need for further research on backdoor\nattack risks in GUI agents.", "AI": {"tldr": "This paper reveals security vulnerabilities in GUI agents powered by Large Vision-Language Models (LVLMs), specifically focusing on backdoor attacks that can hijack visual grounding of agents used for human-machine interaction.", "motivation": "The emergence of GUI agents using LVLMs presents significant security risks, especially with regard to backdoor attacks that undermine their functionality and trustworthiness.", "method": "The paper introduces VisualTrap, a method that hijacks the visual grounding of GUI agents by injecting poisoned data during pre-training to mislead the agent's localization of textual plans.", "result": "VisualTrap can effectively compromise the visual grounding with as little as 5% poisoned data, using stealthy visual triggers applicable across various GUI environments.", "conclusion": "The findings highlight the need for increased research into the security vulnerabilities of GUI agents, particularly regarding backdoor attack risks and their implications for safe operation.", "key_contributions": ["Identification of vulnerabilities in visual grounding of GUI agents", "Development of VisualTrap as a method for hijacking visual localization", "Demonstration of effective stealthy attacks with minimal poisoned data"], "limitations": "", "keywords": ["GUI agents", "LVLM", "backdoor attacks", "visual grounding", "VisualTrap"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.06908", "pdf": "https://arxiv.org/pdf/2507.06908.pdf", "abs": "https://arxiv.org/abs/2507.06908", "title": "MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection", "authors": ["Ziyan Liu", "Chunxiao Fan", "Haoran Lou", "Yuexin Wu", "Kaiwei Deng"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "The rapid expansion of memes on social media has highlighted the urgent need\nfor effective approaches to detect harmful content. However, traditional\ndata-driven approaches struggle to detect new memes due to their evolving\nnature and the lack of up-to-date annotated data. To address this issue, we\npropose MIND, a multi-agent framework for zero-shot harmful meme detection that\ndoes not rely on annotated data. MIND implements three key strategies: 1) We\nretrieve similar memes from an unannotated reference set to provide contextual\ninformation. 2) We propose a bi-directional insight derivation mechanism to\nextract a comprehensive understanding of similar memes. 3) We then employ a\nmulti-agent debate mechanism to ensure robust decision-making through reasoned\narbitration. Extensive experiments on three meme datasets demonstrate that our\nproposed framework not only outperforms existing zero-shot approaches but also\nshows strong generalization across different model architectures and parameter\nscales, providing a scalable solution for harmful meme detection. The code is\navailable at https://github.com/destroy-lonely/MIND.", "AI": {"tldr": "Proposes MIND, a multi-agent framework for zero-shot detection of harmful memes without annotated data.", "motivation": "The need for effective detection of harmful content in rapidly evolving memes on social media.", "method": "The MIND framework retrieves similar memes from an unannotated reference set for contextual information, uses a bi-directional insight derivation mechanism for meme understanding, and applies a multi-agent debate mechanism for decision-making.", "result": "Extensive experiments show MIND outperforms existing zero-shot approaches and generalizes well across different model architectures and parameter scales.", "conclusion": "MIND provides a scalable solution for detecting harmful memes, potentially improving content moderation on social media.", "key_contributions": ["Introduction of a multi-agent framework for zero-shot harmful meme detection.", "Development of a bi-directional insight derivation mechanism for meme understanding.", "Implementation of a multi-agent debate mechanism for robust decision-making."], "limitations": "", "keywords": ["Harmful content detection", "Memes", "Zero-shot learning", "Multi-agent systems", "Natural language processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.06909", "pdf": "https://arxiv.org/pdf/2507.06909.pdf", "abs": "https://arxiv.org/abs/2507.06909", "title": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction", "authors": ["Xiao Wang", "Jiahuan Pei", "Diancheng Shui", "Zhiguang Han", "Xin Sun", "Dawei Zhu", "Xiaoyu Shen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by NLPCC 2025", "summary": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.", "AI": {"tldr": "This paper introduces the Multi-Person Multi-Charge Prediction (MPMCP) dataset and evaluates legal judgment prediction across various scenarios using legal large language models (LLMs).", "motivation": "There is a need to explore how multiple defendants and charges should be treated in legal judgment prediction (LJP).", "method": "The paper evaluates several prevailing legal LLMs on four practical legal judgment scenarios, including charge prediction and penalty term prediction, using a newly created dataset (MPMCP).", "result": "The evaluation found that the scenario with multiple defendants and multiple charges (S4) presents the greatest challenges, with significant variations in model performance across different scenarios.", "conclusion": "The performance of legal LLMs is heavily influenced by the complexity of the legal judgment scenarios, highlighting the need for tailored approaches in LJP.", "key_contributions": ["Introduction of the MPMCP dataset for legal judgment prediction.", "Evaluation of multiple LLMs across different legal judgment scenarios.", "Insights into model performance variations in complex legal settings."], "limitations": "The study may not generalize to all legal systems or jurisdictions, and further research is needed to refine model performance in real-world applications.", "keywords": ["legal judgment prediction", "large language models", "multi-person multi-charge", "data evaluation", "model performance"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.06910", "pdf": "https://arxiv.org/pdf/2507.06910.pdf", "abs": "https://arxiv.org/abs/2507.06910", "title": "Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues", "authors": ["Fareya Ikram", "Alexander Scarlatos", "Andrew Lan"], "categories": ["cs.CL", "cs.CY"], "comment": "Published in BEA 2025: 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications", "summary": "Tutoring dialogues have gained significant attention in recent years, given\nthe prominence of online learning and the emerging tutoring abilities of\nartificial intelligence (AI) agents powered by large language models (LLMs).\nRecent studies have shown that the strategies used by tutors can have\nsignificant effects on student outcomes, necessitating methods to predict how\ntutors will behave and how their actions impact students. However, few works\nhave studied predicting tutor strategy in dialogues. Therefore, in this work we\ninvestigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to\npredict both future tutor moves and student outcomes in dialogues, using two\nmath tutoring dialogue datasets. We find that even state-of-the-art LLMs\nstruggle to predict future tutor strategy while tutor strategy is highly\nindicative of student outcomes, outlining a need for more powerful methods to\napproach this task.", "AI": {"tldr": "This paper explores the prediction of tutor strategies in dialogues using advanced LLMs like Llama 3 and GPT-4o, highlighting their limitations despite the significance of tutor strategies on student outcomes.", "motivation": "The need to understand tutor strategies in dialogues arises from their impact on student learning outcomes, especially in the context of increasing online learning and AI-driven tutoring systems.", "method": "The study employed two math tutoring dialogue datasets to assess the prediction capabilities of Llama 3 and GPT-4o concerning tutor moves and their effects on student outcomes.", "result": "The findings reveal that state-of-the-art LLMs often fail to accurately predict future tutor strategies, even though these strategies are strongly correlated with student success.", "conclusion": "The research concludes that existing LLMs require enhancement for more effective tutor strategy prediction and emphasizes the necessity for advanced methods in this area.", "key_contributions": ["Investigation of tutor strategy prediction in dialogues using LLMs.", "Analysis of the correlation between tutor strategies and student outcomes.", "Identification of the limitations of current LLMs in predicting pedagogical moves."], "limitations": "Current LLMs are inadequate for accurately predicting tutor strategies despite their relevance to student outcomes.", "keywords": ["Tutoring Dialogues", "Large Language Models", "Tutor Strategy Prediction", "Student Outcomes", "AI in Education"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06920", "pdf": "https://arxiv.org/pdf/2507.06920.pdf", "abs": "https://arxiv.org/abs/2507.06920", "title": "Rethinking Verification for LLM Code Generation: From Generation to Testing", "authors": ["Zihan Ma", "Taolin Zhang", "Maosong Cao", "Wenwei Zhang", "Minnan Luo", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.", "AI": {"tldr": "This paper presents a novel method for improving test-case generation for code evaluation in large language models (LLMs) by introducing the SAGA approach and developing TCGBench.", "motivation": "To address the shortcomings in existing code evaluation benchmarks that miss subtle faults due to their limited and homogeneous test cases, thus inflating performance metrics and hindering accurate reward estimation in reinforcement learning settings.", "method": "The authors propose a new methodology that involves multi-dimensional metrics to quantify test-suite thoroughness and introduce SAGA, a collaborative method that combines human programming expertise with LLM reasoning.", "result": "SAGA achieves a detection rate of 90.62% and a verifier accuracy of 32.58% on the newly developed TCGBench, outperforming existing benchmarks.", "conclusion": "The work contributes to reliable LLM code evaluation, advancements in reinforcement learning with verifiable rewards, and lays groundwork for automatic adversarial test synthesis.", "key_contributions": ["Introduction of multi-dimensional metrics for test-suite thoroughness", "Development of SAGA, which combines human and LLM strengths for better test-case generation", "Creation of TCGBench for systematic study and evaluation of test-case generation methods"], "limitations": "", "keywords": ["large language models", "test-case generation", "code evaluation", "reinforcement learning", "adversarial test synthesis"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.06956", "pdf": "https://arxiv.org/pdf/2507.06956.pdf", "abs": "https://arxiv.org/abs/2507.06956", "title": "Investigating the Robustness of Retrieval-Augmented Generation at the Query Level", "authors": ["Sezen Perçin", "Xin Su", "Qutub Sha Syed", "Phillip Howard", "Aleksei Kuvshinov", "Leo Schwinn", "Kay-Ulrich Scholl"], "categories": ["cs.CL"], "comment": "Accepted to Generation, Evaluation & Metrics (GEM) Workshop at ACL\n  2025", "summary": "Large language models (LLMs) are very costly and inefficient to update with\nnew information. To address this limitation, retrieval-augmented generation\n(RAG) has been proposed as a solution that dynamically incorporates external\nknowledge during inference, improving factual consistency and reducing\nhallucinations. Despite its promise, RAG systems face practical challenges-most\nnotably, a strong dependence on the quality of the input query for accurate\nretrieval. In this paper, we investigate the sensitivity of different\ncomponents in the RAG pipeline to various types of query perturbations. Our\nanalysis reveals that the performance of commonly used retrievers can degrade\nsignificantly even under minor query variations. We study each module in\nisolation as well as their combined effect in an end-to-end question answering\nsetting, using both general-domain and domain-specific datasets. Additionally,\nwe propose an evaluation framework to systematically assess the query-level\nrobustness of RAG pipelines and offer actionable recommendations for\npractitioners based on the results of more than 1092 experiments we performed.", "AI": {"tldr": "This paper investigates the sensitivity of retrieval-augmented generation (RAG) systems to query perturbations and proposes an evaluation framework for assessing query-level robustness.", "motivation": "The paper addresses the inefficiencies in updating large language models (LLMs) and examines the impact of input query quality on RAG performance, aiming to enhance factual consistency and reduce hallucinations.", "method": "The authors analyze the performance of various retrievers under different types of query perturbations in an end-to-end question answering setting, using both general-domain and domain-specific datasets. They conducted over 1092 experiments to understand these effects.", "result": "The findings demonstrate significant performance degradation in commonly used retrievers due to minor query variations, highlighting the critical need for robust input querying in RAG systems.", "conclusion": "The authors propose an evaluation framework for practitioners to systematically assess the query-level robustness of RAG pipelines, offering actionable recommendations based on their findings.", "key_contributions": ["Identification of sensitivity in RAG systems to query perturbations", "Development of an evaluation framework for query-level robustness", "Actionable recommendations for practitioners based on extensive empirical analysis"], "limitations": "", "keywords": ["Retrieval-augmented generation", "Query perturbations", "Robustness evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.06974", "pdf": "https://arxiv.org/pdf/2507.06974.pdf", "abs": "https://arxiv.org/abs/2507.06974", "title": "FRaN-X: FRaming and Narratives-eXplorer", "authors": ["Artur Muratov", "Hana Fatima Shaikh", "Vanshikaa Jani", "Tarek Mahmoud", "Zhuohan Xie", "Daniil Orel", "Aaryamonvikram Singh", "Yuxia Wang", "Aadi Joshi", "Hasan Iqbal", "Ming Shan Hee", "Dhruv Sahnan", "Nikolaos Nikolaidis", "Purificação Silvano", "Dimitar Dimitrov", "Roman Yangarber", "Ricardo Campos", "Alípio Jorge", "Nuno Guimarães", "Elisa Sartori", "Nicolas Stefanovitch", "Giovanni Da San Martino", "Jakub Piskorski", "Preslav Nakov"], "categories": ["cs.CL"], "comment": "19 pages, 13 figures, submitted to EMNLP 2025 - Demo Track", "summary": "We present FRaN-X, a Framing and Narratives Explorer that automatically\ndetects entity mentions and classifies their narrative roles directly from raw\ntext. FRaN-X comprises a two-stage system that combines sequence labeling with\nfine-grained role classification to reveal how entities are portrayed as\nprotagonists, antagonists, or innocents, using a unique taxonomy of 22\nfine-grained roles nested under these three main categories. The system\nsupports five languages (Bulgarian, English, Hindi, Russian, and Portuguese)\nand two domains (the Russia-Ukraine Conflict and Climate Change). It provides\nan interactive web interface for media analysts to explore and compare framing\nacross different sources, tackling the challenge of automatically detecting and\nlabeling how entities are framed. Our system allows end users to focus on a\nsingle article as well as analyze up to four articles simultaneously. We\nprovide aggregate level analysis including an intuitive graph visualization\nthat highlights the narrative a group of articles are pushing. Our system\nincludes a search feature for users to look up entities of interest, along with\na timeline view that allows analysts to track an entity's role transitions\nacross different contexts within the article. The FRaN-X system and the trained\nmodels are licensed under an MIT License. FRaN-X is publicly accessible at\nhttps://fran-x.streamlit.app/ and a video demonstration is available at\nhttps://youtu.be/VZVi-1B6yYk.", "AI": {"tldr": "FRaN-X is an automated tool for detecting and classifying narrative roles of entities in text, focusing on media analysis.", "motivation": "To tackle the challenge of analyzing how entities are framed in narrative contexts, enabling more effective media analysis.", "method": "FRaN-X employs a two-stage system combining sequence labeling and fine-grained role classification, assessing entities as protagonists, antagonists, or innocents using a taxonomy of 22 roles.", "result": "The system provides an interactive web interface for comparing framing across articles and supports multi-language analysis, offering visualization of narrative trends.", "conclusion": "FRaN-X aids media analysts in understanding entity roles and transitions in narratives, enhancing the analysis of media framing.", "key_contributions": ["Introduction of a unique taxonomy of 22 narrative roles", "Multi-language support for analysis", "Interactive features for media analysts, including graph visualizations"], "limitations": "", "keywords": ["Framing", "Narratives", "Entity mentions", "Media analysis", "Language processing"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.07024", "pdf": "https://arxiv.org/pdf/2507.07024.pdf", "abs": "https://arxiv.org/abs/2507.07024", "title": "FlexOlmo: Open Language Models for Flexible Data Use", "authors": ["Weijia Shi", "Akshita Bhagia", "Kevin Farhat", "Niklas Muennighoff", "Pete Walsh", "Jacob Morrison", "Dustin Schwenk", "Shayne Longpre", "Jake Poznanski", "Allyson Ettinger", "Daogao Liu", "Margaret Li", "Dirk Groeneveld", "Mike Lewis", "Wen-tau Yih", "Luca Soldaini", "Kyle Lo", "Noah A. Smith", "Luke Zettlemoyer", "Pang Wei Koh", "Hannaneh Hajishirzi", "Ali Farhadi", "Sewon Min"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.", "AI": {"tldr": "FlexOlmo is a novel language model that enables distributed training on closed datasets and flexible data inclusion/exclusion for inference through a mixture-of-experts architecture, significantly improving performance while respecting data owners' preferences.", "motivation": "To address the challenges of distributed training and inference with sensitive data in regulated industries, enabling collaboration while preserving data privacy.", "method": "FlexOlmo utilizes a mixture-of-experts architecture where individual experts are trained on closed datasets and can be integrated through domain-informed routing without joint training.", "result": "FlexOlmo achieves an average 41% relative improvement in performance on 31 diverse downstream tasks compared to general experts and surpasses prior model merging methods by 10.1% while maintaining the same training FLOPs.", "conclusion": "FlexOlmo provides a solution for benefiting from closed data in regulated environments, balancing data utilization and privacy concerns effectively.", "key_contributions": ["Introduces a new architecture for distributed training on closed datasets", "Enables flexible inference with improved performance", "Surpasses existing model merging techniques and standard MoE approaches."], "limitations": "", "keywords": ["language models", "distributed training", "mixture-of-experts", "data privacy", "inference flexibility"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.07030", "pdf": "https://arxiv.org/pdf/2507.07030.pdf", "abs": "https://arxiv.org/abs/2507.07030", "title": "UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations", "authors": ["Fengran Mo", "Yifan Gao", "Chuan Meng", "Xin Liu", "Zhuofeng Wu", "Kelong Mao", "Zhengyang Wang", "Pei Chen", "Zheng Li", "Xian Li", "Bing Yin", "Meng Jiang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (main)", "summary": "The rapid advancement of conversational search systems revolutionizes how\ninformation is accessed by enabling the multi-turn interaction between the user\nand the system. Existing conversational search systems are usually built with\ntwo different models. This separation restricts the system from leveraging the\nintrinsic knowledge of the models simultaneously, which cannot ensure the\neffectiveness of retrieval benefiting the generation. The existing studies for\ndeveloping unified models cannot fully address the aspects of understanding\nconversational context, managing retrieval independently, and generating\nresponses. In this paper, we explore how to unify dense retrieval and response\ngeneration for large language models in conversation. We conduct joint\nfine-tuning with different objectives and design two mechanisms to reduce the\ninconsistency risks while mitigating data discrepancy. The evaluations on five\nconversational search datasets demonstrate that our unified model can mutually\nimprove both tasks and outperform the existing baselines.", "AI": {"tldr": "This paper presents a unified model for conversational search systems that integrates dense retrieval and response generation for large language models, improving performance across multiple conversational datasets.", "motivation": "To enhance conversational search systems by overcoming the limitations of separate models that fail to leverage combined knowledge effectively.", "method": "Joint fine-tuning of retrieval and generation mechanisms, along with two design mechanisms to minimize inconsistency and data discrepancy.", "result": "The unified model significantly improves both retrieval and generation tasks, outperforming existing baselines on five datasets.", "conclusion": "The proposed unified approach demonstrates mutual improvements in conversation tasks, suggesting a more effective framework for conversational AI.", "key_contributions": ["Unified approach to conversational search", "Joint fine-tuning strategy", "Evaluation on multiple datasets showcasing performance improvements"], "limitations": "", "keywords": ["Conversational Search", "Large Language Models", "Joint Fine-Tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.07050", "pdf": "https://arxiv.org/pdf/2507.07050.pdf", "abs": "https://arxiv.org/abs/2507.07050", "title": "Discrete Diffusion Models for Language Generation", "authors": ["Ashen Weligalle"], "categories": ["cs.CL", "cs.LG", "stat.ML", "68T50 (Primary) 68Q32, 60J27 (Secondary)", "G.3"], "comment": "pdfLaTeX, 69 pages with 21 figures, Licentiate Thesis", "summary": "Diffusion models have emerged as a powerful class of generative models,\nachieving state-of-the-art results in continuous data domains such as image and\nvideo generation. Their core mechanism involves a forward diffusion process\nthat gradually transforms structured data into a Gaussian-like distribution,\nfollowed by a learned reverse process to reconstruct the data. While successful\nin continuous modalities, applying this framework to discrete data-particularly\nnatural language-remains challenging due to token dependency complexities and\nthe lack of a defined generation order.This thesis investigates the feasibility\nand performance of discrete diffusion models for natural language generation.\nSpecifically, we evaluate the Discrete Denoising Diffusion Probabilistic Model\n(D3PM) and compare it with traditional autoregressive (AR) language models. To\nassess generative performance, we use Bits Per Token (BPT), Negative\nLog-Likelihood (NLL), Perplexity (PPL), and Batch Processing Speed.\n  Results show the best-performing D3PM model achieves a BPT of 5.72, with a\nmean of 8.05. The AR model outperforms in compression with a lower mean BPT of\n4.59, but D3PM achieves higher processing speed, reaching up to 3.97 batches\nper sec., indicating potential for parallel generation.All evaluations were\nconducted under consistent conditions-generating 100,000 tokens per model with\na fixed batch size of four-for fair comparison. This research presents a\ndetailed analysis of diffusion-based vs. autoregressive models, highlighting\ntrade-offs in generative quality and efficiency. Findings emphasize both the\npromise and limitations of diffusion models for discrete data, supporting\nfuture work in non-autoregressive language generation.", "AI": {"tldr": "This thesis investigates discrete diffusion models for natural language generation, comparing them with autoregressive models in terms of generative performance and processing speed.", "motivation": "The study addresses the challenges of applying diffusion models to discrete data, such as natural language, which involve token dependencies and lack of generation order.", "method": "The research evaluates the Discrete Denoising Diffusion Probabilistic Model (D3PM) and compares its performance to traditional autoregressive (AR) models using metrics like Bits Per Token (BPT), Negative Log-Likelihood (NLL), and Batch Processing Speed.", "result": "The D3PM model achieved a BPT of 5.72 and a processing speed of 3.97 batches per second, while the AR model had a lower mean BPT of 4.59 but outperformed in compression.", "conclusion": "The findings present a comparative analysis of diffusion models versus autoregressive models and indicate both the potential and limitations of diffusion for discrete generation tasks, suggesting directions for future research.", "key_contributions": ["Detailed analysis of discrete diffusion models for NLP", "Comparison of D3PM with traditional autoregressive models", "Evaluation metrics and performance benchmarks"], "limitations": "The study is limited to the evaluation of a single diffusion model and requires further exploration of additional architectures for comprehensive insights.", "keywords": ["Discrete Diffusion Models", "Natural Language Generation", "Autoregressive Models"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2507.06235", "pdf": "https://arxiv.org/pdf/2507.06235.pdf", "abs": "https://arxiv.org/abs/2507.06235", "title": "Super Kawaii Vocalics: Amplifying the \"Cute\" Factor in Computer Voice", "authors": ["Yuto Mandai", "Katie Seaborn", "Tomoyasu Nakano", "Xin Sun", "Yijia Wang", "Jun Kato"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI '25", "summary": "\"Kawaii\" is the Japanese concept of cute, which carries sociocultural\nconnotations related to social identities and emotional responses. Yet,\nvirtually all work to date has focused on the visual side of kawaii, including\nin studies of computer agents and social robots. In pursuit of formalizing the\nnew science of kawaii vocalics, we explored what elements of voice relate to\nkawaii and how they might be manipulated, manually and automatically. We\nconducted a four-phase study (grand N = 512) with two varieties of computer\nvoices: text-to-speech (TTS) and game character voices. We found kawaii \"sweet\nspots\" through manipulation of fundamental and formant frequencies, but only\nfor certain voices and to a certain extent. Findings also suggest a ceiling\neffect for the kawaii vocalics of certain voices. We offer empirical validation\nof the preliminary kawaii vocalics model and an elementary method for\nmanipulating kawaii perceptions of computer voice.", "AI": {"tldr": "The study explores the vocal elements of 'kawaii' in computer voices and provides methods for manipulating perceptions of cute voice characteristics.", "motivation": "To formalize the new science of kawaii vocalics, which had been largely overlooked in favor of visual representations of kawaii.", "method": "A four-phase study with a grand N of 512 participants, examining manipulation of text-to-speech (TTS) and game character voices through fundamental and formant frequency adjustments.", "result": "Identified 'sweet spots' for kawaii vocalics in certain voices, with limitations on effectiveness and a ceiling effect for some voice types.", "conclusion": "Validated the kawaii vocalics model and proposed a method for influencing perceptions of cute in computer-generated voices.", "key_contributions": ["Exploration of kawaii vocalics in computer voices", "Identification of manipulative sweet spots for kawaii", "Empirical validation of a kawaii vocalics model"], "limitations": "Effectiveness is limited to specific voices and may reach a ceiling effect.", "keywords": ["kawaii", "vocalics", "computer voices", "text-to-speech", "emotional response"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.06483", "pdf": "https://arxiv.org/pdf/2507.06483.pdf", "abs": "https://arxiv.org/abs/2507.06483", "title": "Learning Japanese with Jouzu: Interaction Outcomes with Stylized Dialogue Fictional Agents", "authors": ["Zackary Rackauckas", "Julia Hirschberg"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "This study investigates how stylized, voiced agents shape user interaction in\na multimodal language learning environment. We conducted a mixed-methods\nevaluation of 54 participants interacting with anime-inspired characters\npowered by large language models and expressive text-to-speech synthesis. These\nagents responded in Japanese character language, offering users asynchronous,\nsemi-structured conversation in varying speech styles and emotional tones. We\nanalyzed user engagement patterns, perceived usability, emotional responses,\nand learning behaviors, with particular attention to how agent stylization\ninfluenced interaction across language proficiency levels and cultural\nbackgrounds. Our findings reveal that agent design, especially voice, persona,\nand linguistic style, substantially affected user experience, motivation, and\nstrategy. This work contributes to the understanding of affective, culturally\nstylized agents in human-agent interaction and offers guidance for designing\nmore engaging, socially responsive systems.", "AI": {"tldr": "The study explores how stylized, voiced agents impact user interaction in language learning, revealing the effects of agent design on experience and engagement.", "motivation": "To investigate the role of stylized, voiced agents in multimodal language learning environments and their effect on user engagement and learning behaviors.", "method": "A mixed-methods evaluation involving 54 participants interacting with anime-inspired characters powered by large language models and expressive text-to-speech.", "result": "Users showed varying levels of engagement and emotional responses depending on the agent's voice, persona, and linguistic style, which influenced their motivation and learning strategies.", "conclusion": "Agent design is crucial for enhancing user experience in language learning settings, with potential applications to create socially responsive learning systems.", "key_contributions": ["Investigation of agent stylization impact on interaction across proficiency levels", "Insights into emotional responses and engagement in language learning", "Guidelines for designing socially responsive agents"], "limitations": "", "keywords": ["human-agent interaction", "language learning", "emotional design", "user engagement", "artificial intelligence"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06734", "pdf": "https://arxiv.org/pdf/2507.06734.pdf", "abs": "https://arxiv.org/abs/2507.06734", "title": "Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted Classification in an Open-Source Telegram Monitoring Tool", "authors": ["Milena Pustet", "Elisabeth Steffen", "Helena Mihaljević", "Grischa Stanjek", "Yannis Illies"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "The role of civil society organizations (CSOs) in monitoring harmful online\ncontent is increasingly crucial, especially as platform providers reduce their\ninvestment in content moderation. AI tools can assist in detecting and\nmonitoring harmful content at scale. However, few open-source tools offer\nseamless integration of AI models and social media monitoring infrastructures.\nGiven their thematic expertise and contextual understanding of harmful content,\nCSOs should be active partners in co-developing technological tools, providing\nfeedback, helping to improve models, and ensuring alignment with stakeholder\nneeds and values, rather than as passive 'consumers'. However, collaborations\nbetween the open source community, academia, and civil society remain rare, and\nresearch on harmful content seldom translates into practical tools usable by\ncivil society actors. This work in progress explores how CSOs can be\nmeaningfully involved in an AI-assisted open-source monitoring tool of\nanti-democratic movements on Telegram, which we are currently developing in\ncollaboration with CSO stakeholders.", "AI": {"tldr": "This paper explores the involvement of civil society organizations (CSOs) in co-developing AI-assisted monitoring tools for detecting harmful online content, particularly focusing on anti-democratic movements on Telegram.", "motivation": "The need for effective monitoring of harmful online content as platform providers reduce investment in content moderation.", "method": "Investigating the collaborative development of an open-source monitoring tool with CSOs to leverage their expertise in identifying harmful content.", "result": "CSOs can significantly enhance the detection of harmful content through their thematic expertise, yet current collaborations and practical implementations are limited.", "conclusion": "Active involvement of CSOs in tool development is critical for creating effective AI solutions for monitoring harmful content.", "key_contributions": ["Proposed collaboration model between CSOs and AI tool developers.", "Identification of gaps in current open-source monitoring solutions.", "Emphasis on the active role of CSOs rather than passive consumption."], "limitations": "Limited research on the practical applications of tools developed in collaboration with civil society actors.", "keywords": ["civil society organizations", "harmful content", "AI monitoring", "open-source tools", "anti-democratic movements"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2401.17196", "pdf": "https://arxiv.org/pdf/2401.17196.pdf", "abs": "https://arxiv.org/abs/2401.17196", "title": "Single Word Change is All You Need: Designing Attacks and Defenses for Text Classifiers", "authors": ["Lei Xu", "Sarah Alnegheimish", "Laure Berti-Equille", "Alfredo Cuesta-Infante", "Kalyan Veeramachaneni"], "categories": ["cs.CL"], "comment": null, "summary": "In text classification, creating an adversarial example means subtly\nperturbing a few words in a sentence without changing its meaning, causing it\nto be misclassified by a classifier. A concerning observation is that a\nsignificant portion of adversarial examples generated by existing methods\nchange only one word. This single-word perturbation vulnerability represents a\nsignificant weakness in classifiers, which malicious users can exploit to\nefficiently create a multitude of adversarial examples. This paper studies this\nproblem and makes the following key contributions: (1) We introduce a novel\nmetric \\r{ho} to quantitatively assess a classifier's robustness against\nsingle-word perturbation. (2) We present the SP-Attack, designed to exploit the\nsingle-word perturbation vulnerability, achieving a higher attack success rate,\nbetter preserving sentence meaning, while reducing computation costs compared\nto state-of-the-art adversarial methods. (3) We propose SP-Defense, which aims\nto improve \\r{ho} by applying data augmentation in learning. Experimental\nresults on 4 datasets and BERT and distilBERT classifiers show that SP-Defense\nimproves \\r{ho} by 14.6% and 13.9% and decreases the attack success rate of\nSP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the\nattack success rate of existing attack methods that involve multiple-word\nperturbations.", "AI": {"tldr": "This paper investigates the vulnerability of text classifiers to single-word adversarial perturbations, introducing metrics and methodologies to assess and defend against this weakness.", "motivation": "To address the significant weaknesses in classifiers due to the vulnerability to single-word perturbations that can be exploited by malicious users.", "method": "The paper introduces a novel metric to assess classifier robustness against single-word perturbations, presents the SP-Attack to generate adversarial examples effectively, and proposes SP-Defense for improving robustness through data augmentation.", "result": "Experimental results demonstrate that SP-Defense improves robustness metrics and significantly decreases the attack success rate of both SP-Attack and other existing methods.", "conclusion": "The proposed SP-Attack and SP-Defense contribute to better understanding and countering adversarial attacks in text classification systems.", "key_contributions": ["A novel metric for assessing robustness against single-word perturbations.", "The introduction of SP-Attack for generating adversarial examples efficiently.", "The development of SP-Defense to enhance classifier robustness via data augmentation."], "limitations": "", "keywords": ["adversarial examples", "text classification", "robustness metrics"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2406.05615", "pdf": "https://arxiv.org/pdf/2406.05615.pdf", "abs": "https://arxiv.org/abs/2406.05615", "title": "Video-Language Understanding: A Survey from Model Architecture, Model Training, and Data Perspectives", "authors": ["Thong Nguyen", "Yi Bin", "Junbin Xiao", "Leigang Qu", "Yicong Li", "Jay Zhangjie Wu", "Cong-Duy Nguyen", "See-Kiong Ng", "Luu Anh Tuan"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2024 (Findings)", "summary": "Humans use multiple senses to comprehend the environment. Vision and language\nare two of the most vital senses since they allow us to easily communicate our\nthoughts and perceive the world around us. There has been a lot of interest in\ncreating video-language understanding systems with human-like senses since a\nvideo-language pair can mimic both our linguistic medium and visual environment\nwith temporal dynamics. In this survey, we review the key tasks of these\nsystems and highlight the associated challenges. Based on the challenges, we\nsummarize their methods from model architecture, model training, and data\nperspectives. We also conduct performance comparison among the methods, and\ndiscuss promising directions for future research.", "AI": {"tldr": "This survey reviews video-language understanding systems, key tasks, challenges, methods, and future research directions.", "motivation": "To explore the integration of vision and language for enhanced human-like video understanding systems.", "method": "Survey of existing models, tasks, challenges, and performance evaluation of video-language systems.", "result": "Comparison of methods highlights current challenges and effective strategies in video-language understanding.", "conclusion": "Future work should focus on overcoming identified challenges and improving model architectures and training methods.", "key_contributions": ["Comprehensive review of video-language understanding tasks", "Performance comparison among existing methodologies", "Identification of key challenges and future research directions"], "limitations": "", "keywords": ["video-language understanding", "human-like senses", "model architecture"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2406.13217", "pdf": "https://arxiv.org/pdf/2406.13217.pdf", "abs": "https://arxiv.org/abs/2406.13217", "title": "Automating IRAC Analysis in Malaysian Contract Law using a Semi-Structured Knowledge Base", "authors": ["Xiaoxi Kang", "Lizhen Qu", "Lay-Ki Soon", "Zhuang Li", "Adnan Trakic"], "categories": ["cs.CL"], "comment": null, "summary": "The effectiveness of Large Language Models (LLMs) in legal reasoning is often\nlimited due to the unique legal terminologies and the necessity for highly\nspecialized knowledge. These limitations highlight the need for high-quality\ndata tailored for complex legal reasoning tasks. This paper introduces\nLegalSemi, a benchmark specifically curated for legal scenario analysis.\nLegalSemi comprises 54 legal scenarios, each rigorously annotated by legal\nexperts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion)\nframework from Malaysian Contract Law. In addition, LegalSemi is accompanied by\na structured knowledge base (SKE). A series of experiments were conducted to\nassess the usefulness of LegalSemi for IRAC analysis. The experimental results\ndemonstrate the effectiveness of incorporating the SKE for issue\nidentification, rule retrieval, application and conclusion generation using\nfour different LLMs.", "AI": {"tldr": "LegalSemi is a benchmark for legal scenario analysis, consisting of 54 rigorously annotated legal scenarios based on Malaysian Contract Law, aimed at enhancing the effectiveness of LLMs in legal reasoning tasks.", "motivation": "The paper addresses the limitations of LLMs in legal reasoning due to unique terminologies and specialized knowledge, emphasizing the need for tailored data in complex legal scenarios.", "method": "LegalSemi consists of 54 legal scenarios annotated by legal experts, and is based on the IRAC framework from Malaysian Contract Law, accompanied by a structured knowledge base (SKE) for better analysis.", "result": "Experiments show that incorporating the SKE significantly improves issue identification, rule retrieval, and conclusion generation using four different LLMs.", "conclusion": "LegalSemi demonstrates that a well-structured knowledge base can enhance LLM performance in legal reasoning tasks by providing context and clarity in complex scenarios.", "key_contributions": ["Introduction of the LegalSemi benchmark for legal scenario analysis.", "Annotation of scenarios based on the IRAC framework by legal experts.", "Demonstration of SKE's effectiveness in improving LLM functions in legal contexts."], "limitations": "", "keywords": ["Large Language Models", "Legal reasoning", "IRAC", "Malaysian Contract Law", "Structured knowledge base"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2410.01735", "pdf": "https://arxiv.org/pdf/2410.01735.pdf", "abs": "https://arxiv.org/abs/2410.01735", "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits", "authors": ["Duy Nguyen", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "categories": ["cs.CL", "cs.LG"], "comment": "28 pages; First two authors contributed equally. Code:\n  https://github.com/duykhuongnguyen/LASeR-MAB", "summary": "Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling.", "AI": {"tldr": "LASeR (Learning to Adaptively Select Rewards) improves LLM training by dynamically selecting reward models (RMs) for various tasks, resulting in higher accuracy and efficiency.", "motivation": "The paper addresses the limitations of using a single reward model for training large language models, highlighting the need for a strategy that efficiently selects the most suitable RM for different tasks.", "method": "LASeR formulates the task of RM selection as a multi-armed bandit problem, allowing iterative training of LLMs with optimal RM choice for each instance.", "result": "LASeR improves the accuracy of Llama-3-8B by 2.67% across commonsense and math reasoning tasks, achieves a 72.69% win rate on WildChat, and enhances performance on long-context generation tasks.", "conclusion": "By adaptively selecting the best RM, LASeR not only boosts accuracy but also significantly speeds up LLM training, demonstrating its effectiveness in various tasks.", "key_contributions": ["Introduces a novel framework for RM selection using a multi-armed bandit approach", "Demonstrates improved LLM training efficiency and accuracy over traditional RM ensemble methods", "Achieves superior performance in both commonsense reasoning and math tasks."], "limitations": "", "keywords": ["Reward Models", "Large Language Models", "Multi-armed Bandit", "Human-Computer Interaction", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.12513", "pdf": "https://arxiv.org/pdf/2410.12513.pdf", "abs": "https://arxiv.org/abs/2410.12513", "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency Reduction", "authors": ["Akriti Jain", "Saransh Sharma", "Koyel Mukherjee", "Soumyabrata Pal"], "categories": ["cs.CL"], "comment": null, "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.", "AI": {"tldr": "FiRST is an adaptive algorithm for reducing inference latency in auto-regressive LLMs by selecting transformer layers based on input sequences, maintaining quality and compatibility with KV caching.", "motivation": "To address computation and latency challenges in LLMs, particularly in constrained environments like mobile and edge devices, which are exacerbated by existing layer skipping strategies.", "method": "FiRST uses layer-specific routers to dynamically choose which transformer layers to skip during inference, based on the input prompt, while remaining compatible with KV caching.", "result": "FiRST significantly reduces latency and outperforms existing layer selection methods in quality metrics, occasionally even improving upon the base model's performance.", "conclusion": "FiRST presents an efficient and model-agnostic solution for improving LLM inference in low-resource environments, demonstrating the importance of input adaptivity.", "key_contributions": ["Introduction of FiRST for adaptive layer skipping in LLMs", "Demonstration of input-specific layer importance", "Compatibility with KV caching for improved inference speed"], "limitations": "", "keywords": ["Large Language Models", "Latent Inference", "Layer Skipping", "KV Caching", "Adaptive Algorithms"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.09073", "pdf": "https://arxiv.org/pdf/2411.09073.pdf", "abs": "https://arxiv.org/abs/2411.09073", "title": "CHAI for LLMs: Improving Code-Mixed Translation in Large Language Models through Reinforcement Learning with AI Feedback", "authors": ["Wenbo Zhang", "Aditya Majumdar", "Amulya Yadav"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "full draft v2: 8 pages, 3 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks but struggle with code-mixed (or code-switched) language\nunderstanding. For example, prior work benchmarking the performance of\nmultilingual LLMs on code-mixed translation tasks has demonstrated that current\nstate-of-the-art multilingual LLMs are ineffective in dealing with code-mixed\nlanguages. However, the question of how to improve the capability of\nmultilingual LLMs to handle code-mixed language has not received any attention\nto date. In this paper, we tackle this research gap by proposing CHAI, a novel\ngeneral-purpose framework for improving the ability of multilingual LLMs to\nhandle code-mixed languages. CHAI relies on three novel contributions made in\nthis paper. First, we explore the ability of LLMs to provide accurate\nannotations for code-mixed translation tasks. Second, we leverage this ability\nof LLMs as annotators to generate preference data for code-mixed translation\ntasks at scale, which are then used within a reinforcement learning from AI\nfeedback (RLAIF) procedure to improve LLMs' capability on code-mixed tasks.\nThird, we conduct a rigorous experimental evaluation across various real-world\ndatasets and settings. Our analysis shows that CHAI-powered LLMs outperform\nstate-of-the-art open-source LLMs by 25.66% (in terms of win rate adjudicated\nby human annotators) in code-mixed translation tasks. This work represents a\nfirst step towards developing more inclusive code-mixed LLMs.", "AI": {"tldr": "The paper presents CHAI, a framework for enhancing multilingual LLMs' abilities in code-mixed language understanding, resulting in significant performance gains in translation tasks.", "motivation": "Despite the proven capabilities of LLMs, their effectiveness in handling code-mixed languages is lacking, prompting the need for improvement methods.", "method": "The CHAI framework utilizes LLMs to generate annotations and preference data for code-mixed translation tasks, followed by a reinforcement learning from AI feedback (RLAIF) approach for model enhancement.", "result": "CHAI-powered LLMs achieved a 25.66% improvement in performance over state-of-the-art models on code-mixed translation tasks, based on expert adjudication.", "conclusion": "The development of CHAI is a pioneering effort towards creating more effective multilingual LLMs for inclusive code-mixed language applications.", "key_contributions": ["Introduction of the CHAI framework", "Use of LLMs for annotation generation", "Demonstration of significant performance improvement in code-mixed tasks"], "limitations": "", "keywords": ["Large Language Models", "code-mixed languages", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2412.08268", "pdf": "https://arxiv.org/pdf/2412.08268.pdf", "abs": "https://arxiv.org/abs/2412.08268", "title": "LCFO: Long Context and Long Form Output Dataset and Benchmarking", "authors": ["Marta R. Costa-jussà", "Pierre Andrews", "Mariano Coria Meglioli", "Joy Chen", "Joe Chuang", "David Dale", "Christophe Ropers", "Alexandre Mourachko", "Eduardo Sánchez", "Holger Schwenk", "Tuan Tran", "Arina Turkatenko", "Carleigh Wood"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6).", "AI": {"tldr": "This paper introduces the LCFO benchmark for evaluating summarization and summary expansion capabilities.", "motivation": "The motivation is to create a controllable framework for generating long texts from shorter inputs through summary expansion, providing structured evaluation metrics.", "method": "The method includes the design of the LCFO benchmark, featuring long documents paired with multiple summaries and QA elements, along with human evaluation and LLM performance analysis.", "result": "GPT-4o-mini outperforms other models and humans in some summarization tasks, marking advancements in automatic summarization metrics.", "conclusion": "The study concludes that automatic metrics correlate poorly with human evaluations, highlighting challenges in effectively measuring summarization quality.", "key_contributions": ["Introduction of the LCFO benchmark for diverse summary evaluation", "Human evaluation scores alongside state-of-the-art LLM results", "Insights into the correlation between automatic metrics and human evaluations"], "limitations": "Automatic metrics show low correlations with human scores, indicating limited reliability in evaluation of summarization quality.", "keywords": ["summarization", "summary expansion", "long context", "evaluation framework", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.16412", "pdf": "https://arxiv.org/pdf/2412.16412.pdf", "abs": "https://arxiv.org/abs/2412.16412", "title": "InfoTech Assistant: A Multimodal Conversational Agent for InfoTechnology Web Portal Queries", "authors": ["Sai Surya Gadiraju", "Duoduo Liao", "Akhila Kudupudi", "Santosh Kasula", "Charitha Chalasani"], "categories": ["cs.CL"], "comment": "Accepted by IEEE Big Data 2024", "summary": "This pilot study presents the development of the InfoTech Assistant, a\ndomain-specific, multimodal chatbot engineered to address queries in bridge\nevaluation and infrastructure technology. By integrating web data scraping,\nlarge language models (LLMs), and Retrieval-Augmented Generation (RAG), the\nInfoTech Assistant provides accurate and contextually relevant responses. Data,\nincluding textual descriptions and images, are sourced from publicly available\ndocuments on the InfoTechnology website and organized in JSON format to\nfacilitate efficient querying. The architecture of the system includes an\nHTML-based interface and a Flask back end connected to the Llama 3.1 model via\nLLM Studio. Evaluation results show approximately 95 percent accuracy on\ndomain-specific tasks, with high similarity scores confirming the quality of\nresponse matching. This RAG-enhanced setup enables the InfoTech Assistant to\nhandle complex, multimodal queries, offering both textual and visual\ninformation in its responses. The InfoTech Assistant demonstrates strong\npotential as a dependable tool for infrastructure professionals, delivering\nhigh accuracy and relevance in its domain-specific outputs.", "AI": {"tldr": "Development of the InfoTech Assistant, a multimodal chatbot for bridge evaluation using LLMs and RAG.", "motivation": "To create a domain-specific chatbot that provides accurate responses in bridge evaluation and infrastructure technology.", "method": "Integration of web data scraping, LLMs, and RAG to process queries and deliver contextual answers.", "result": "Achieved about 95% accuracy on domain-specific tasks with high similarity scores in response matching.", "conclusion": "The InfoTech Assistant shows promise as a reliable tool for infrastructure professionals by providing accurate, multimodal information.", "key_contributions": ["Development of a multimodal chatbot for infrastructure", "Integration of RAG with LLMs for enhanced querying", "High accuracy and relevance in domain-specific outputs."], "limitations": "", "keywords": ["multimodal chatbot", "infrastructure technology", "RAG", "LLMs", "data scraping"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2412.18497", "pdf": "https://arxiv.org/pdf/2412.18497.pdf", "abs": "https://arxiv.org/abs/2412.18497", "title": "Neuron-Level Differentiation of Memorization and Generalization in Large Language Models", "authors": ["Ko-Wei Huang", "Yi-Fu Fu", "Ching-Yu Tsai", "Yu-Chieh Tu", "Tzu-Ling Cheng", "Cheng-Yu Lin", "Yi-Ting Yang", "Heng-Yi Liu", "Keng-Te Liao", "Da-Cheng Juan", "Shou-De Lin"], "categories": ["cs.CL"], "comment": null, "summary": "We investigate how Large Language Models (LLMs) distinguish between\nmemorization and generalization at the neuron level. Through carefully designed\ntasks, we identify distinct neuron subsets responsible for each behavior.\nExperiments on both a GPT-2 model trained from scratch and a pretrained\nLLaMA-3.2 model fine-tuned with LoRA show consistent neuron-level\nspecialization. We further demonstrate that inference-time interventions on\nthese neurons can steer the model's behavior toward memorization or\ngeneralization. To assess robustness, we evaluate intra-task and inter-task\nconsistency, confirming that these neuron-behavior associations reflect\ngeneralizable patterns rather than dataset-specific artifacts. Our findings\nreveal modular structure in LLMs and enable controlling memorization and\ngeneralization behaviors at inference time.", "AI": {"tldr": "The paper explores neuron-level distinctions in Large Language Models (LLMs) between memorization and generalization, identifying specific neuron subsets and demonstrating interventions can influence behaviors at inference time.", "motivation": "To understand how LLMs differentiate between memorization and generalization at the neuron level, and to control these behaviors during inference.", "method": "The study involved experiments on GPT-2 and LLaMA-3.2 models, using designed tasks to identify neuron subsets linked to memorization and generalization, followed by interventions at inference time to steer model behavior.", "result": "We found distinct neuron subsets responsible for memorization and generalization, with interventions steering model behavior consistently. Evaluations confirmed neuron-behavior associations are generalizable across tasks.", "conclusion": "Our findings reveal a modular structure in LLMs, enabling control over memorization and generalization behaviors during inference.", "key_contributions": ["Identification of neuron subsets linked to memorization and generalization", "Demonstration of inference-time interventions on neuron behavior", "Confirmation of generalizable neuron-behavior associations across tasks"], "limitations": "", "keywords": ["Large Language Models", "Memorization", "Generalization", "Neurons", "Inference"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2501.10487", "pdf": "https://arxiv.org/pdf/2501.10487.pdf", "abs": "https://arxiv.org/abs/2501.10487", "title": "Theme-Explanation Structure for Table Summarization using Large Language Models: A Case Study on Korean Tabular Data", "authors": ["TaeYoon Kwack", "Jisoo Kim", "Ki Yong Jung", "DongGeon Lee", "Heesun Park"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to TRL@ACL 2025", "summary": "Tables are a primary medium for conveying critical information in\nadministrative domains, yet their complexity hinders utilization by Large\nLanguage Models (LLMs). This paper introduces the Theme-Explanation\nStructure-based Table Summarization (Tabular-TX) pipeline, a novel approach\ndesigned to generate highly interpretable summaries from tabular data, with a\nspecific focus on Korean administrative documents. Current table summarization\nmethods often neglect the crucial aspect of human-friendly output. Tabular-TX\naddresses this by first employing a multi-step reasoning process to ensure deep\ntable comprehension by LLMs, followed by a journalist persona prompting\nstrategy for clear sentence generation. Crucially, it then structures the\noutput into a Theme Part (an adverbial phrase) and an Explanation Part (a\npredicative clause), significantly enhancing readability. Our approach\nleverages in-context learning, obviating the need for extensive fine-tuning and\nassociated labeled data or computational resources. Experimental results show\nthat Tabular-TX effectively processes complex table structures and metadata,\noffering a robust and efficient solution for generating human-centric table\nsummaries, especially in low-resource scenarios.", "AI": {"tldr": "Introducing Tabular-TX, a new pipeline for generating interpretable summaries from complex tables in Korean administrative documents.", "motivation": "To improve the interpretability and usability of table summarization for LLMs in administrative contexts, specifically for Korean documents.", "method": "A multi-step reasoning process combined with a journalist persona prompting strategy, outputting summaries in a structured format with a Theme Part and an Explanation Part.", "result": "Experimental results demonstrate that Tabular-TX effectively processes complex table structures and generates clear, human-friendly summaries.", "conclusion": "Tabular-TX offers a robust solution to table summarization in low-resource settings, enhancing the clarity of information conveyed through tables.", "key_contributions": ["Developed a novel Tabular-TX pipeline for table summarization.", "Introduced a structured output format (Theme Part and Explanation Part).", "Utilized in-context learning to minimize reliance on labeled data and resources."], "limitations": "", "keywords": ["table summarization", "human-computer interaction", "Korean administrative documents"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2502.05167", "pdf": "https://arxiv.org/pdf/2502.05167.pdf", "abs": "https://arxiv.org/abs/2502.05167", "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching", "authors": ["Ali Modarressi", "Hanieh Deilamsalehy", "Franck Dernoncourt", "Trung Bui", "Ryan A. Rossi", "Seunghyun Yoon", "Hinrich Schütze"], "categories": ["cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 13 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 11 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.\nEven models enhanced with reasoning capabilities or CoT prompting struggle to\nmaintain performance in long contexts. We publicly release the dataset and\nevaluation code at https://github.com/adobe-research/NoLiMa.", "AI": {"tldr": "This paper introduces NoLiMa, a new benchmark for evaluating large language models (LLMs) on their ability to retrieve relevant information from long contexts, without relying on lexical matching.", "motivation": "To better evaluate the capabilities of LLMs in extracting relevant information from long contexts, addressing the weaknesses of existing benchmarks like the needle-in-a-haystack test.", "method": "NoLiMa benchmark utilizes a needle set designed with minimal lexical overlap to evaluate the performance of LLMs when identifying relevant information in lengthy contexts.", "result": "Evaluation of 13 popular LLMs revealed a significant drop in performance as context length increased, highlighting challenges in attention mechanisms under minimal lexical overlap conditions.", "conclusion": "Models struggle to maintain retrieval accuracy in longer contexts, even when enhanced with reasoning techniques. The dataset and code will be publicly accessible for further research.", "key_contributions": ["Introduction of NoLiMa benchmark for LLM evaluation", "Analysis of model performance decline with longer contexts", "Public release of dataset and evaluation code"], "limitations": "The results indicate limitations in LLMs' performance in real-world applications where long context retrieval is critical.", "keywords": ["large language models", "benchmarks", "long contexts", "needle-in-a-haystack", "information retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11703", "pdf": "https://arxiv.org/pdf/2502.11703.pdf", "abs": "https://arxiv.org/abs/2502.11703", "title": "CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models in Medical Quality Control Indicator Calculation", "authors": ["Guangya Yu", "Yanhao Li", "Zongying Jiang", "Yuxiong Jin", "Li Dai", "Yupian Lin", "Ruihui Hou", "Weiyan Zhang", "Yongqi Fan", "Qi Ye", "Jingping Liu", "Tong Ruan"], "categories": ["cs.CL"], "comment": "2025 ACL Findings", "summary": "Medical quality control indicators are essential to assess the qualifications\nof healthcare institutions for medical services. With the impressive\nperformance of large language models (LLMs) like GPT-4 in the medical field,\nleveraging these technologies for the Medical Quality Control Indicator\nCalculation (MQCIC) presents a promising approach. In this work, (1) we\nintroduce a real-world task MQCIC and propose an open-source Chinese electronic\nmedical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances\nand 76 indicators. (2) We propose a semi-automatic method to enhance the rule\nrepresentation. Then we propose the Clinical Facts-based Inferential Rule\n(CF-IR) method that disentangles the clinical fact verification and inferential\nrule reasoning actions. (3) We conduct comprehensive experiments on 20\nrepresentative LLMs, covering general and medical models. Our findings reveal\nthat CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct\nan error analysis and investigate the capabilities of clinical fact\nverification and inferential rule reasoning, providing insights to improve\nperformance in the MQCIC further. The dataset and code is available in this\nrepository https://github.com/YuY-2001/C-MQCIC.", "AI": {"tldr": "This paper introduces a dataset and a method using LLMs for medical quality control indicator calculations, demonstrating improved performance over existing methods.", "motivation": "To improve the assessment of healthcare institutions' qualifications for medical services using LLMs.", "method": "A semi-automatic method enhances rule representation and introduces the CF-IR method to separate clinical fact verification from inferential rule reasoning.", "result": "CF-IR outperforms Chain-of-Thought methods in MQCIC tasks across 20 representative LLMs.", "conclusion": "The study provides insights to improve MQCIC performance and offers accessible datasets and code for future research.", "key_contributions": ["Introduction of the CMQCIC-Bench dataset with 785 instances and 76 indicators.", "Development of the CF-IR method for improved clinical fact verification and reasoning.", "Error analysis to enhance understanding of MQCIC performance."], "limitations": "", "keywords": ["Medical Quality Control", "Large Language Models", "Clinical Facts", "Dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.12022", "pdf": "https://arxiv.org/pdf/2502.12022.pdf", "abs": "https://arxiv.org/abs/2502.12022", "title": "Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving", "authors": ["Xin Xu", "Yan Xu", "Tianhao Chen", "Yuchen Yan", "Chengwu Liu", "Zaoyu Chen", "Yufei Wang", "Yichun Yin", "Yasheng Wang", "Lifeng Shang", "Qun Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages", "summary": "Existing approaches to mathematical reasoning with large language models\n(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated\nReasoning (TIR) for precise computation. While efforts have been made to\ncombine these methods, they primarily rely on post-selection or predefined\nstrategies, leaving an open question: whether LLMs can autonomously adapt their\nreasoning strategy based on their inherent capabilities. In this work, we\npropose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework\nthat enables LLMs to personalize their reasoning strategy spontaneously,\naligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware\ndata selection during supervised fine-tuning (SFT) to tailor training data to\nthe model's unique abilities. This approach equips LLMs to autonomously\ndetermine and apply the appropriate reasoning strategy at test time. We\nevaluate TATA through extensive experiments on six mathematical reasoning\nbenchmarks, using both general-purpose and math-specialized LLMs. Empirical\nresults demonstrate that TATA effectively combines the complementary strengths\nof CoT and TIR, achieving superior or comparable performance with improved\ninference efficiency compared to TIR alone. Further analysis underscores the\ncritical role of aptitude-aware data selection in enabling LLMs to make\neffective and adaptive reasoning decisions and align reasoning strategies with\nmodel capabilities.", "AI": {"tldr": "The paper presents TATA, a framework enabling large language models (LLMs) to autonomously adapt their reasoning strategies based on their capabilities, enhancing efficiency and performance in mathematical reasoning tasks.", "motivation": "Existing methods for mathematical reasoning with LLMs have limitations in their adaptability and reliance on predefined strategies.", "method": "TATA integrates base-LLM-aware data selection during supervised fine-tuning to personalize reasoning strategies for LLMs based on their intrinsic aptitude.", "result": "TATA was evaluated on six mathematical reasoning benchmarks, showing improved performance and efficiency by effectively combining Chain-of-Thought and Tool-Integrated Reasoning approaches.", "conclusion": "The results indicate that aptitude-aware data selection is crucial for LLMs to autonomously determine and apply suitable reasoning strategies, leading to better performance.", "key_contributions": ["Introduction of the TATA framework for adaptive reasoning in LLMs", "Demonstration of improved performance in mathematical reasoning tasks", "Empirical validation of the importance of aptitude-aware data selection."], "limitations": "", "keywords": ["large language models", "mathematical reasoning", "adaptive reasoning", "supervised fine-tuning", "aptitude-aware data selection"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.12446", "pdf": "https://arxiv.org/pdf/2502.12446.pdf", "abs": "https://arxiv.org/abs/2502.12446", "title": "Multi-Attribute Steering of Language Models via Targeted Intervention", "authors": ["Duy Nguyen", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 camera-ready, code link:\n  https://github.com/duykhuongnguyen/MAT-Steer", "summary": "Inference-time intervention (ITI) has emerged as a promising method for\nsteering large language model (LLM) behavior in a particular direction (e.g.,\nimproving helpfulness) by intervening on token representations without costly\nupdates to the LLM's parameters. However, existing ITI approaches fail to scale\nto multi-attribute settings with conflicts, such as enhancing helpfulness while\nalso reducing toxicity. To address this, we introduce Multi-Attribute Targeted\nSteering (MAT-Steer), a novel steering framework designed for selective\ntoken-level intervention across multiple attributes. MAT-Steer learns steering\nvectors using an alignment objective that shifts the model's internal\nrepresentations of undesirable outputs closer to those of desirable ones while\nenforcing sparsity and orthogonality among vectors for different attributes,\nthereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two\ndistinct settings: (i) on question answering (QA) tasks where we balance\nattributes like truthfulness, bias, and toxicity; (ii) on generative tasks\nwhere we simultaneously improve attributes like helpfulness, correctness, and\ncoherence. MAT-Steer outperforms existing ITI and parameter-efficient\nfine-tuning approaches across both task types (e.g., 3% average accuracy gain\nacross QA tasks and 55.82% win rate against the best ITI baseline).", "AI": {"tldr": "Introduction of MAT-Steer, a novel framework for targeted steering of LLM behaviors across multiple attributes without costly parameter updates.", "motivation": "Current ITI methods struggle with multi-attribute conflicts in LLM behavior, needing a solution that can balance difficult trade-offs like helpfulness and toxicity.", "method": "MAT-Steer employs selective token-level interventions using steering vectors optimized for sparsity and orthogonality to manage multiple attributes effectively.", "result": "MAT-Steer achieves significant performance gains, showing a 3% average accuracy increase across QA tasks and a 55.82% win rate against the best existing ITI approaches.", "conclusion": "MAT-Steer demonstrates the potential for effective multi-attribute steering in LLMs, paving the way for safer and more useful AI applications.", "key_contributions": ["Development of MAT-Steer for selective interventions", "Optimization of steering vectors for multi-attribute balance", "Empirical validation on QA and generative tasks"], "limitations": "", "keywords": ["multi-attribute steering", "LLM behavior", "token-level intervention"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.14541", "pdf": "https://arxiv.org/pdf/2502.14541.pdf", "abs": "https://arxiv.org/abs/2502.14541", "title": "LLM-based User Profile Management for Recommender System", "authors": ["Seunghwan Bang", "Hwanjun Song"], "categories": ["cs.CL"], "comment": "Accepted GENNEXT@SIGIR'25 Workshop", "summary": "The rapid advancement of Large Language Models (LLMs) has opened new\nopportunities in recommender systems by enabling zero-shot recommendation\nwithout conventional training. Despite their potential, most existing works\nrely solely on users' purchase histories, leaving significant room for\nimprovement by incorporating user-generated textual data, such as reviews and\nproduct descriptions. Addressing this gap, we propose PURE, a novel LLM-based\nrecommendation framework that builds and maintains evolving user profiles by\nsystematically extracting and summarizing key information from user reviews.\nPURE consists of three core components: a Review Extractor for identifying user\npreferences and key product features, a Profile Updater for refining and\nupdating user profiles, and a Recommender for generating personalized\nrecommendations using the most current profile. To evaluate PURE, we introduce\na continuous sequential recommendation task that reflects real-world scenarios\nby adding reviews over time and updating predictions incrementally. Our\nexperimental results on Amazon datasets demonstrate that PURE outperforms\nexisting LLM-based methods, effectively leveraging long-term user information\nwhile managing token limitations.", "AI": {"tldr": "The paper introduces PURE, a novel LLM-based recommendation framework that enhances user profiles by utilizing user-generated textual data for more effective recommendations.", "motivation": "Existing recommendation systems mainly rely on purchase histories and often overlook user-generated textual data, limiting their effectiveness.", "method": "PURE utilizes three components: a Review Extractor for extracting user preferences, a Profile Updater for maintaining user profiles, and a Recommender for providing personalized recommendations based on updated profiles.", "result": "Experimental results show that PURE outperforms current LLM-based recommendation systems by effectively leveraging long-term user information and adapting to new data over time.", "conclusion": "PURE presents significant improvements in the recommendation process by integrating evolving user reviews, demonstrating its strength in real-world applications through continuous updating of user profiles.", "key_contributions": ["Introduction of a framework that combines LLMs with user-generated data for recommendations", "Development of a continuous sequential recommendation task for evaluation", "Demonstrated superior performance on Amazon datasets compared to existing methods"], "limitations": "The study focuses on Amazon datasets, which may limit generalizability to other platforms or domains.", "keywords": ["Large Language Models", "recommender systems", "user profiles", "user reviews", "personalized recommendations"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.16903", "pdf": "https://arxiv.org/pdf/2502.16903.pdf", "abs": "https://arxiv.org/abs/2502.16903", "title": "GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of In-the-wild LLM Jailbreak Methods", "authors": ["Ruixuan Huang", "Xunguang Wang", "Zongjie Li", "Daoyuan Wu", "Shuai Wang"], "categories": ["cs.CL", "cs.CR"], "comment": "Homepage: https://sproutnan.github.io/AI-Safety_Benchmark/", "summary": "Despite the growing interest in jailbreak methods as an effective red-teaming\ntool for building safe and responsible large language models (LLMs), flawed\nevaluation system designs have led to significant discrepancies in their\neffectiveness assessments. We conduct a systematic measurement study based on\n37 jailbreak studies since 2022, focusing on both the methods and the\nevaluation systems they employ. We find that existing evaluation systems lack\ncase-specific criteria, resulting in misleading conclusions about their\neffectiveness and safety implications. This paper advocates a shift to a more\nnuanced, case-by-case evaluation paradigm. We introduce GuidedBench, a novel\nbenchmark comprising a curated harmful question dataset, detailed case-by-case\nevaluation guidelines and an evaluation system integrated with these guidelines\n-- GuidedEval. Experiments demonstrate that GuidedBench offers more accurate\nmeasurements of jailbreak performance, enabling meaningful comparisons across\nmethods and uncovering new insights overlooked in previous evaluations.\nGuidedEval reduces inter-evaluator variance by at least 76.03\\%. Furthermore,\nwe observe that incorporating guidelines can enhance the effectiveness of\njailbreak methods themselves, offering new insights into both attack strategies\nand evaluation paradigms.", "AI": {"tldr": "This paper critiques current jailbreak evaluation methods for large language models and introduces GuidedBench, a new benchmark for improved assessment.", "motivation": "To address significant discrepancies in the effectiveness assessments of jailbreak methods due to flawed evaluation systems.", "method": "A systematic measurement study of 37 jailbreak studies, introducing GuidedBench with a curated dataset and evaluation guidelines.", "result": "GuidedBench provides more accurate jailbreak performance measurements and reduces inter-evaluator variance by at least 76.03%.", "conclusion": "A nuanced evaluation paradigm can enhance both the effectiveness of jailbreak methods and the reliability of assessments.", "key_contributions": ["Introduction of GuidedBench as a new evaluation benchmark for jailbreak methods.", "Development of detailed case-specific evaluation guidelines through GuidedEval.", "Demonstration of improved performance measurement and inter-evaluator consistency."], "limitations": "", "keywords": ["jailbreak methods", "evaluation systems", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.18890", "pdf": "https://arxiv.org/pdf/2502.18890.pdf", "abs": "https://arxiv.org/abs/2502.18890", "title": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation", "authors": ["Tong Wu", "Junzhe Shen", "Zixia Jia", "Yuxuan Wang", "Zilong Zheng"], "categories": ["cs.CL"], "comment": "Accepted By ICML25", "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.", "AI": {"tldr": "TOKENSWIFT is a framework that accelerates the generation of ultra-long sequences with large language models by addressing key challenges and achieving significant speedup.", "motivation": "As generating sequences up to 100K tokens with large language models is time-intensive, there is a need for efficient generation methods.", "method": "The paper introduces TOKENSWIFT, which tackles issues like frequent model reloading, dynamic key-value management, and repetitive generation to speed up the generation process.", "result": "TOKENSWIFT achieves over 3 times speedup across various models and architectures, resulting in considerable time savings for ultra-long sequence generation.", "conclusion": "TOKENSWIFT is a scalable and effective solution for accelerating ultra-long sequence generation while maintaining model quality.", "key_contributions": ["Introduction of the TOKENSWIFT framework for ultra-long sequence generation", "Identification of challenges in existing generation methods", "Demonstration of significant speedup across different model scales"], "limitations": "", "keywords": ["large language models", "sequence generation", "TOKENSWIFT"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2503.19328", "pdf": "https://arxiv.org/pdf/2503.19328.pdf", "abs": "https://arxiv.org/abs/2503.19328", "title": "Substance over Style: Evaluating Proactive Conversational Coaching Agents", "authors": ["Vidya Srinivas", "Xuhai Xu", "Xin Liu", "Kumar Ayush", "Isaac Galatzer-Levy", "Shwetak Patel", "Daniel McDuff", "Tim Althoff"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025", "summary": "While NLP research has made strides in conversational tasks, many approaches\nfocus on single-turn responses with well-defined objectives or evaluation\ncriteria. In contrast, coaching presents unique challenges with initially\nundefined goals that evolve through multi-turn interactions, subjective\nevaluation criteria, mixed-initiative dialogue. In this work, we describe and\nimplement five multi-turn coaching agents that exhibit distinct conversational\nstyles, and evaluate them through a user study, collecting first-person\nfeedback on 155 conversations. We find that users highly value core\nfunctionality, and that stylistic components in absence of core components are\nviewed negatively. By comparing user feedback with third-person evaluations\nfrom health experts and an LM, we reveal significant misalignment across\nevaluation approaches. Our findings provide insights into design and evaluation\nof conversational coaching agents and contribute toward improving\nhuman-centered NLP applications.", "AI": {"tldr": "The paper investigates the challenges of designing multi-turn coaching agents in conversational NLP, revealing user preferences and evaluation misalignments.", "motivation": "To address the unique challenges in coaching that involve evolving goals and subjective evaluations in multi-turn interactions.", "method": "The authors implement five coaching agents with different conversational styles and evaluate them through a user study involving feedback on 155 conversations.", "result": "Users prioritize core functionalities over stylistic components, and there is a significant misalignment between user feedback and expert evaluations.", "conclusion": "The study offers insights into the design and evaluation of conversational coaching agents, aiming to enhance human-centered NLP applications.", "key_contributions": ["Development of five distinct multi-turn coaching agents", "User study with 155 conversations to gather feedback", "Identification of misalignments between user and expert evaluations"], "limitations": "", "keywords": ["Conversational Agents", "User Study", "Health Informatics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.04204", "pdf": "https://arxiv.org/pdf/2504.04204.pdf", "abs": "https://arxiv.org/abs/2504.04204", "title": "Adaptive Elicitation of Latent Information Using Natural Language", "authors": ["Jimmy Wang", "Thomas Zollo", "Richard Zemel", "Hongseok Namkoong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Eliciting information to reduce uncertainty about a latent entity is a\ncritical task in many application domains, e.g., assessing individual student\nlearning outcomes, diagnosing underlying diseases, or learning user\npreferences. Though natural language is a powerful medium for this purpose,\nlarge language models (LLMs) and existing fine-tuning algorithms lack\nmechanisms for strategically gathering information to refine their own\nunderstanding of the latent entity. To harness the generalization power and\nworld knowledge of LLMs in developing effective information-gathering\nstrategies, we propose an adaptive elicitation framework that actively reduces\nuncertainty on the latent entity. Since probabilistic modeling of an abstract\nlatent entity is difficult, our framework adopts a predictive view of\nuncertainty, using a meta-learned language model to simulate future\nobservations and enable scalable uncertainty quantification over complex\nnatural language. Through autoregressive forward simulation, our model\nquantifies how new questions reduce epistemic uncertainty, enabling the\ndevelopment of sophisticated information-gathering strategies to choose the\nmost informative next queries. In experiments on the 20 questions game, dynamic\nopinion polling, and adaptive student assessment, our method consistently\noutperforms baselines in identifying critical unknowns and improving downstream\npredictions, illustrating the promise of strategic information gathering in\nnatural language settings.", "AI": {"tldr": "This paper proposes an adaptive elicitation framework using large language models (LLMs) to actively gather information and reduce uncertainty regarding latent entities.", "motivation": "The need to reduce uncertainty about latent entities in various applications, such as education or healthcare, and the limitations of existing LLM fine-tuning methods in strategic information gathering.", "method": "The framework employs a meta-learned language model to predict future observations, enabling scalable uncertainty quantification and developing sophisticated querying strategies.", "result": "The proposed model outperforms baseline methods in identifying critical unknowns and improving predictions across several experimental scenarios, including the 20 questions game and adaptive student assessment.", "conclusion": "The study demonstrates that strategic information gathering can significantly enhance the performance of LLMs in natural language processing tasks.", "key_contributions": ["Development of an adaptive elicitation framework leveraging LLMs", "Implementation of autoregressive forward simulation for uncertainty quantification", "Empirical validation showing improvement over baseline approaches"], "limitations": "", "keywords": ["Large Language Models", "Uncertainty Reduction", "Information Gathering", "Adaptive Elicitation", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.06036", "pdf": "https://arxiv.org/pdf/2504.06036.pdf", "abs": "https://arxiv.org/abs/2504.06036", "title": "Multi-Sense Embeddings for Language Models and Knowledge Distillation", "authors": ["Qitong Wang", "Mohammed J. Zaki", "Georgios Kollias", "Vasileios Kalantzis"], "categories": ["cs.CL"], "comment": "16 pages, 4 figures", "summary": "Transformer-based large language models (LLMs) rely on contextual embeddings\nwhich generate different (continuous) representations for the same token\ndepending on its surrounding context. Nonetheless, words and tokens typically\nhave a limited number of senses (or meanings). We propose multi-sense\nembeddings as a drop-in replacement for each token in order to capture the\nrange of their uses in a language. To construct a sense embedding dictionary,\nwe apply a clustering algorithm to embeddings generated by an LLM and consider\nthe cluster centers as representative sense embeddings. In addition, we propose\na novel knowledge distillation method that leverages the sense dictionary to\nlearn a smaller student model that mimics the senses from the much larger base\nLLM model, offering significant space and inference time savings, while\nmaintaining competitive performance. Via thorough experiments on various\nbenchmarks, we showcase the effectiveness of our sense embeddings and knowledge\ndistillation approach. We share our code at\nhttps://github.com/Qitong-Wang/SenseDict", "AI": {"tldr": "This paper proposes multi-sense embeddings for transformer-based LLMs to effectively capture multiple meanings of tokens and introduces a knowledge distillation method to create a smaller model that retains performance while being efficient.", "motivation": "To address the limitation of transformer-based LLMs that generate context-dependent representations for tokens, which do not fully capture the multiple meanings or senses that words can embody.", "method": "The authors apply a clustering algorithm to LLM-generated embeddings to create a sense embedding dictionary, representing the various senses of tokens. They also propose a knowledge distillation technique to train a smaller student model using this sense dictionary.", "result": "Experiments demonstrate that the multi-sense embeddings and the knowledge distillation method yield significant improvements in efficiency without compromising performance on various benchmarks.", "conclusion": "The proposed approach effectively reduces space and inference time while maintaining competitive results, showcasing the potential of multi-sense embeddings alongside knowledge distillation in LLM applications.", "key_contributions": ["Introduction of multi-sense embeddings as a drop-in replacement for LLMs.", "Development of a knowledge distillation method leveraging sense embeddings.", "Demonstration of improved efficiency and performance on benchmark datasets."], "limitations": "The approach may be limited by the clustering algorithm's ability to accurately represent all meanings of tokens and may require extensive tuning to achieve optimal results across various contexts.", "keywords": ["multi-sense embeddings", "knowledge distillation", "transformer models", "language models", "NLP"], "importance_score": 8, "read_time_minutes": 16}}
