{"id": "2508.09297", "pdf": "https://arxiv.org/pdf/2508.09297.pdf", "abs": "https://arxiv.org/abs/2508.09297", "title": "Based AI improves human decision-making but reduces trust", "authors": ["Shiyang Lai", "Junsol Kim", "Nadav Kunievsky", "Yujin Potter", "James Evans"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Current AI systems minimize risk by enforcing ideological neutrality, yet\nthis may introduce automation bias by suppressing cognitive engagement in human\ndecision-making. We conducted randomized trials with 2,500 participants to test\nwhether culturally biased AI enhances human decision-making. Participants\ninteracted with politically diverse GPT-4o variants on information evaluation\ntasks. Partisan AI assistants enhanced human performance, increased engagement,\nand reduced evaluative bias compared to non-biased counterparts, with amplified\nbenefits when participants encountered opposing views. These gains carried a\ntrust penalty: participants underappreciated biased AI and overcredited neutral\nsystems. Exposing participants to two AIs whose biases flanked human\nperspectives closed the perception-performance gap. These findings complicate\nconventional wisdom about AI neutrality, suggesting that strategic integration\nof diverse cultural biases may foster improved and resilient human\ndecision-making."}
{"id": "2508.09312", "pdf": "https://arxiv.org/pdf/2508.09312.pdf", "abs": "https://arxiv.org/abs/2508.09312", "title": "Micro-Health Interventions: Exploring Design Strategies for 1-Minute Interventions as a Gateway to Healthy Habits", "authors": ["Zahra Hassanzadeh", "David Haag", "Lydia Chilton", "Jan Smeddinck", "Norman Farb", "Joseph Jay Williams"], "categories": ["cs.HC"], "comment": null, "summary": "One-minute behavior change interventions might seem too brief to matter.\nCould something so short really help people build healthier routines? This work\nexplores this question through two studies examining how ultra-brief prompts\nmight encourage meaningful actions in daily life. In a formative study, we\nexplored how participants engaged with one-minute prompts across four domains:\nphysical activity, eating, screen use, and mental well-being. This revealed two\ncommon design approaches: Immediate Action prompts (simple, directive tasks)\nand Reflection-First prompts (self-awareness before action). We then conducted\na 14-day, within-subjects study comparing these two flows with 28 participants.\nSurprisingly, most participants did not notice differences in structure -- but\nresponded positively when prompts felt timely, relevant, or emotionally\nsupportive. Engagement was not shaped by flow type, but by content fit, tone,\nand momentary readiness. Participants also co-designed messages, favoring those\nwith step-by-step guidance, personal meaning, or sensory detail. These results\nsuggest that one-minute interventions, while easily dismissed, may serve as\nmeaningful gateways into healthier routines -- if designed to feel helpful in\nthe moment."}
{"id": "2508.09342", "pdf": "https://arxiv.org/pdf/2508.09342.pdf", "abs": "https://arxiv.org/abs/2508.09342", "title": "Affordances of Sketched Notations for Multimodal UI Design and Development Tools", "authors": ["Sam H. Ross", "Yunseo Lee", "Coco K. Lee", "Jayne Everson", "R. Benjamin Shapiro"], "categories": ["cs.HC"], "comment": "VL/HCC 2025", "summary": "Multimodal UI design and development tools that interpret sketches or natural\nlanguage descriptions of UIs inherently have notations: the inputs they can\nunderstand. In AI-based systems, notations are implicitly defined by the data\nused to train these systems. In order to create usable and intuitive notations\nfor interactive design systems, we must regard, design, and evaluate these\ntraining datasets as notation specifications. To better understand the design\nspace of notational possibilities for future design tools, we use the Cognitive\nDimensions of Notations framework to analyze two possible notations for UI\nsketching. The first notation is the sketching rules for an existing UI sketch\ndataset, and the second notation is the set of sketches generated by\nparticipants in this study, where individuals sketched UIs without imposed\nrepresentational rules. We imagine two systems, FixedSketch and FlexiSketch,\nbuilt with each notation respectively, in order to understand the differential\naffordances of, and potential design requirements for, systems. We find that\nparticipants' sketches were composed of element-level notations that are\nambiguous in isolation but are interpretable in context within whole designs.\nFor many cognitive dimensions, the FlexiSketch notation supports greater\nintuitive creative expression and affords lower cognitive effort than the\nFixedSketch notation, but cannot be supported with prevailing, element-based\napproaches to UI sketch recognition. We argue that for future multimodal design\ntools to be truly human-centered, they must adopt contemporary AI methods,\nincluding transformer-based and human-in-the-loop, reinforcement learning\ntechniques to understand users' context-rich expressive notations and\ncorrections."}
{"id": "2508.09358", "pdf": "https://arxiv.org/pdf/2508.09358.pdf", "abs": "https://arxiv.org/abs/2508.09358", "title": "Virtual Reality User Interface Design: Best Practices and Implementation", "authors": ["Esin Mehmedova", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "categories": ["cs.HC"], "comment": "Paper submitted to ACM SIGGRAPH Motion, Interaction and Games 2025\n  (MIG 2025)", "summary": "Designing effective user interfaces (UIs) for virtual reality (VR) is\nessential to enhance user immersion, usability, comfort, and accessibility in\nvirtual environments. Despite the growing adoption of VR across domains such as\neducation, healthcare, gaming, and rehabilitation, there is a noticeable lack\nof unified and comprehensive design guidelines for VR UI design. To address\nthis gap, we conducted a systematic literature review to identify existing best\npractices and propose complete and unified guidelines for UI development in VR.\n  Building on these insights, this research proposes a set of best practices to\nguide the creation of more effective VR interfaces. To demonstrate and validate\nthese practices, we developed a VR application called \\textit{FlUId} that\nshowcases both good and bad UI design principles for direct comparison. A user\nstudy was conducted to evaluate the impact of the proposed guidelines. The\nfindings aim to bridge the gap between theory and practice, offering concrete\nrecommendations for VR designers and developers."}
{"id": "2508.09303", "pdf": "https://arxiv.org/pdf/2508.09303.pdf", "abs": "https://arxiv.org/abs/2508.09303", "title": "ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning", "authors": ["Shu Zhao", "Tan Yu", "Anbang Xu", "Japinder Singh", "Aaditya Shukla", "Rama Akkiraju"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Reasoning-augmented search agents such as Search-R1, trained via\nreinforcement learning with verifiable rewards (RLVR), demonstrate remarkable\ncapabilities in multi-step information retrieval from external knowledge\nsources. These agents address the limitations of their parametric memory by\ndynamically gathering relevant facts to address complex reasoning tasks.\nHowever, existing approaches suffer from a fundamental architectural\nlimitation: they process search queries strictly sequentially, even when\nhandling inherently parallelizable and logically independent comparisons. This\nsequential bottleneck significantly constrains computational efficiency,\nparticularly for queries that require multiple entity comparisons. To address\nthis critical limitation, we propose ParallelSearch, a novel reinforcement\nlearning framework that empowers large language models (LLMs) to recognize\nparallelizable query structures and execute multiple search operations\nconcurrently. Our approach introduces dedicated reward functions that\nincentivize the identification of independent query components while preserving\nanswer accuracy through jointly considering correctness, query decomposition\nquality, and parallel execution benefits. Comprehensive experiments demonstrate\nthat ParallelSearch outperforms state-of-the-art baselines by an average\nperformance gain of 2.9% across seven question-answering benchmarks. Notably,\non parallelizable questions, our method achieves a 12.7% performance\nimprovement while requiring only 69.6% of the LLM calls compared to sequential\napproaches."}
{"id": "2508.09386", "pdf": "https://arxiv.org/pdf/2508.09386.pdf", "abs": "https://arxiv.org/abs/2508.09386", "title": "VIVA: Virtual Healthcare Interactions Using Visual Analytics, With Controllability Through Configuration", "authors": ["JÃ¼rgen Bernard", "Mara Solen", "Helen Novak Lauscher", "Kurtis Stewart", "Kendall Ho", "Tamara Munzner"], "categories": ["cs.HC"], "comment": null, "summary": "At the beginning of the COVID-19 pandemic, HealthLink BC (HLBC) rapidly\nintegrated physicians into the triage process of their virtual healthcare\nservice to improve patient outcomes and satisfaction with this service and\npreserve health care system capacity. We present the design and implementation\nof a visual analytics tool, VIVA (Virtual healthcare Interactions using Visual\nAnalytics), to support HLBC in analysing various forms of usage data from the\nservice. We abstract HLBC's data and data analysis tasks, which we use to\ninform our design of VIVA. We also present the interactive workflow abstraction\nof Scan, Act, Adapt. We validate VIVA's design through three case studies with\nstakeholder domain experts. We also propose the Controllability Through\nConfiguration model to conduct and analyze design studies, and discuss\narchitectural evolution of VIVA through that lens. It articulates\nconfiguration, both that specified by a developer or technical power user and\nthat constructed automatically through log data from previous interactive\nsessions, as a bridge between the rigidity of hardwired programming and the\ntime-consuming implementation of full end-user interactivity.\n  Availability: Supplemental materials at https://osf.io/wv38n"}
{"id": "2508.09323", "pdf": "https://arxiv.org/pdf/2508.09323.pdf", "abs": "https://arxiv.org/abs/2508.09323", "title": "Leveraging Large Language Models for Rare Disease Named Entity Recognition", "authors": ["Nan Miles Xi", "Yu Deng", "Lin Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named Entity Recognition (NER) in the rare disease domain poses unique\nchallenges due to limited labeled data, semantic ambiguity between entity\ntypes, and long-tail distributions. In this study, we evaluate the capabilities\nof GPT-4o for rare disease NER under low-resource settings, using a range of\nprompt-based strategies including zero-shot prompting, few-shot in-context\nlearning, retrieval-augmented generation (RAG), and task-level fine-tuning. We\ndesign a structured prompting framework that encodes domain-specific knowledge\nand disambiguation rules for four entity types. We further introduce two\nsemantically guided few-shot example selection methods to improve in-context\nperformance while reducing labeling effort. Experiments on the RareDis Corpus\nshow that GPT-4o achieves competitive or superior performance compared to\nBioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art\n(SOTA) results. Cost-performance analysis reveals that few-shot prompting\ndelivers high returns at low token budgets, while RAG offers marginal\nadditional benefit. An error taxonomy highlights common failure modes such as\nboundary drift and type confusion, suggesting opportunities for post-processing\nand hybrid refinement. Our results demonstrate that prompt-optimized LLMs can\nserve as effective, scalable alternatives to traditional supervised models in\nbiomedical NER, particularly in rare disease applications where annotated data\nis scarce."}
{"id": "2508.09402", "pdf": "https://arxiv.org/pdf/2508.09402.pdf", "abs": "https://arxiv.org/abs/2508.09402", "title": "Realtime Multimodal Emotion Estimation using Behavioral and Neurophysiological Data", "authors": ["Von Ralph Dane Marquez Herbuela", "Yukie Nagai"], "categories": ["cs.HC"], "comment": null, "summary": "Many individuals especially those with autism spectrum disorder (ASD),\nalexithymia, or other neurodivergent profiles face challenges in recognizing,\nexpressing, or interpreting emotions. To support more inclusive and\npersonalized emotion technologies, we present a real-time multimodal emotion\nestimation system that combines neurophysiological EEG, ECG, blood volume pulse\n(BVP), and galvanic skin response (GSR/EDA) and behavioral modalities (facial\nexpressions, and speech) in a unified arousal-valence 2D interface to track\nmoment-to-moment emotional states. This architecture enables interpretable,\nuser-specific analysis and supports applications in emotion education,\nneuroadaptive feedback, and interaction support for neurodiverse users. Two\ndemonstration scenarios illustrate its application: (1) passive media viewing\n(2D or VR videos) reveals cortical and autonomic responses to affective\ncontent, and (2) semi-scripted conversations with a facilitator or virtual\nagent capture real-time facial and vocal expressions. These tasks enable\ncontrolled and naturalistic emotion monitoring, making the system well-suited\nfor personalized feedback and neurodiversity-informed interaction design."}
{"id": "2508.09324", "pdf": "https://arxiv.org/pdf/2508.09324.pdf", "abs": "https://arxiv.org/abs/2508.09324", "title": "TEN: Table Explicitization, Neurosymbolically", "authors": ["Nikita Mehrotra", "Aayush Kumar", "Sumit Gulwani", "Arjun Radhakrishna", "Ashish Tiwari"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a neurosymbolic approach, TEN, for extracting tabular data from\nsemistructured input text. This task is particularly challenging for text input\nthat does not use special delimiters consistently to separate columns and rows.\nPurely neural approaches perform poorly due to hallucinations and their\ninability to enforce hard constraints. TEN uses Structural Decomposition\nprompting - a specialized chain-of-thought prompting approach - on a large\nlanguage model (LLM) to generate an initial table, and thereafter uses a\nsymbolic checker to evaluate not only the well-formedness of that table, but\nalso detect cases of hallucinations or forgetting. The output of the symbolic\nchecker is processed by a critique-LLM to generate guidance for fixing the\ntable, which is presented to the original LLM in a self-debug loop. Our\nextensive experiments demonstrate that TEN significantly outperforms purely\nneural baselines across multiple datasets and metrics, achieving significantly\nhigher exact match accuracy and substantially reduced hallucination rates. A\n21-participant user study further confirms that TEN's tables are rated\nsignificantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are\nconsistently preferred for ease of verification and correction, with\nparticipants favoring our method in over 60% of the cases."}
{"id": "2508.09438", "pdf": "https://arxiv.org/pdf/2508.09438.pdf", "abs": "https://arxiv.org/abs/2508.09438", "title": "Fulfillment of the Work Games: Warehouse Workers' Experiences with Algorithmic Management", "authors": ["EunJeong Cheon", "Ingrid Erickson"], "categories": ["cs.HC"], "comment": null, "summary": "The introduction of algorithms into a large number of industries has already\nrestructured the landscape of work and threatens to continue. While a growing\nbody of CSCW research centered on the future of work has begun to document\nthese shifts, relatively little is known about workers' experiences beyond\nthose of platform-mediated gig workers. In this paper, we turn to a traditional\nwork sector, Amazon fulfillment centers (FC), to deepen our field's empirical\nexamination of algorithmic management. Drawing on two years of ethnographic\nresearch, we show how FC workers react to managers' interventions, imposed\nproductivity rates, and quantified objectification when subjected to\nlabor-tracking systems in their physical work environments. Situating FC\nworkers' resistance to algorithmic systems and metrics within the current CSCW\nliterature allows us to explicate and link the nuanced practices of FC workers\nto the larger discourse of algorithmic control mechanisms. In addition, we show\nhow FC workers' resistance practices are emblematic of 'work games'--a\nlong-studied means by which workers agentically configure (\"trick\") their\nengagement within work systems. We argue that gaining a more nuanced\nunderstanding of workers' resistance and consent in relation to algorithmic\nmanagement expands our ability to critique and potentially disassemble the\neconomic and political forces at the root of these sociotechnical labor\nsystems."}
{"id": "2508.09337", "pdf": "https://arxiv.org/pdf/2508.09337.pdf", "abs": "https://arxiv.org/abs/2508.09337", "title": "Decoding Neural Emotion Patterns through Natural Language Processing Embeddings", "authors": ["Gideon Vos", "Maryam Ebrahimpour", "Liza van Eijk", "Zoltan Sarnyai", "Mostafa Rahimi Azghadi"], "categories": ["cs.CL"], "comment": "26 pages, 9 figures", "summary": "Understanding how emotional expression in language relates to brain function\nis a challenge in computational neuroscience and affective computing.\nTraditional neuroimaging is costly and lab-bound, but abundant digital text\noffers new avenues for emotion-brain mapping. Prior work has largely examined\nneuroimaging-based emotion localization or computational text analysis\nseparately, with little integration. We propose a computational framework that\nmaps textual emotional content to anatomically defined brain regions without\nrequiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate\nhigh-dimensional semantic representations, apply dimensionality reduction and\nclustering to identify emotional groups, and map them to 18 brain regions\nlinked to emotional processing. Three experiments were conducted: i) analyzing\nconversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to\ncompare mapping patterns, ii) applying the method to the GoEmotions dataset and\niii) comparing human-written text with large language model (LLM) responses to\nassess differences in inferred brain activation. Emotional intensity was scored\nvia lexical analysis. Results showed neuroanatomically plausible mappings with\nhigh spatial specificity. Depressed subjects exhibited greater limbic\nengagement tied to negative affect. Discrete emotions were successfully\ndifferentiated. LLM-generated text matched humans in basic emotion distribution\nbut lacked nuanced activation in empathy and self-referential regions (medial\nprefrontal and posterior cingulate cortex). This cost-effective, scalable\napproach enables large-scale analysis of naturalistic language, distinguishes\nbetween clinical populations, and offers a brain-based benchmark for evaluating\nAI emotional expression."}
{"id": "2508.09458", "pdf": "https://arxiv.org/pdf/2508.09458.pdf", "abs": "https://arxiv.org/abs/2508.09458", "title": "Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis", "authors": ["Xi Long", "Christy Boscardin", "Lauren A. Maggio", "Joseph A. Costello", "Ralph Gonzales", "Rasmyah Hammoudeh", "Ki Lai", "Yoon Soo Park", "Brian C. Gin"], "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Knowledge syntheses (literature reviews) are essential to health professions\neducation (HPE), consolidating findings to advance theory and practice.\nHowever, they are labor-intensive, especially during data extraction.\nArtificial Intelligence (AI)-assisted extraction promises efficiency but raises\nconcerns about accuracy, making it critical to distinguish AI 'hallucinations'\n(fabricated content) from legitimate interpretive differences. We developed an\nextraction platform using large language models (LLMs) to automate data\nextraction and compared AI to human responses across 187 publications and 17\nextraction questions from a published scoping review. AI-human, human-human,\nand AI-AI consistencies were measured using interrater reliability\n(categorical) and thematic similarity ratings (open-ended). Errors were\nidentified by comparing extracted responses to source publications. AI was\nhighly consistent with humans for concrete, explicitly stated questions (e.g.,\ntitle, aims) and lower for questions requiring subjective interpretation or\nabsent in text (e.g., Kirkpatrick's outcomes, study rationale). Human-human\nconsistency was not higher than AI-human and showed the same question-dependent\nvariability. Discordant AI-human responses (769/3179 = 24.2%) were mostly due\nto interpretive differences (18.3%); AI inaccuracies were rare (1.51%), while\nhumans were nearly three times more likely to state inaccuracies (4.37%).\nFindings suggest AI accuracy depends more on interpretability than\nhallucination. Repeating AI extraction can identify interpretive complexity or\nambiguity, refining processes before human review. AI can be a transparent,\ntrustworthy partner in knowledge synthesis, though caution is needed to\npreserve critical human insights."}
{"id": "2508.09349", "pdf": "https://arxiv.org/pdf/2508.09349.pdf", "abs": "https://arxiv.org/abs/2508.09349", "title": "The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains", "authors": ["Cathy Speed", "Ahmed A. Metwally"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Expert consensus plays a critical role in domains where evidence is complex,\nconflicting, or insufficient for direct prescription. Traditional methods, such\nas Delphi studies, consensus conferences, and systematic guideline synthesis,\noffer structure but face limitations including high panel burden, interpretive\noversimplification, and suppression of conditional nuance. These challenges are\nnow exacerbated by information overload, fragmentation of the evidence base,\nand increasing reliance on publicly available sources that lack expert\nfiltering. This study introduces and evaluates a Human-AI Hybrid Delphi\n(HAH-Delphi) framework designed to augment expert consensus development by\nintegrating a generative AI model (Gemini 2.5 Pro), small panels of senior\nhuman experts, and structured facilitation. The HAH-Delphi was tested in three\nphases: retrospective replication, prospective comparison, and applied\ndeployment in two applied domains (endurance training and resistance and mixed\ncardio/strength training). The AI replicated 95% of published expert consensus\nconclusions in Phase I and showed 95% directional agreement with senior human\nexperts in Phase II, though it lacked experiential and pragmatic nuance. In\nPhase III, compact panels of six senior experts achieved >90% consensus\ncoverage and reached thematic saturation before the final participant. The AI\nprovided consistent, literature-grounded scaffolding that supported divergence\nresolution and accelerated saturation. The HAH-Delphi framework offers a\nflexible, scalable approach for generating high-quality, context-sensitive\nconsensus. Its successful application across health, coaching, and performance\nscience confirms its methodological robustness and supports its use as a\nfoundation for generating conditional, personalised guidance and published\nconsensus frameworks at scale."}
{"id": "2508.09469", "pdf": "https://arxiv.org/pdf/2508.09469.pdf", "abs": "https://arxiv.org/abs/2508.09469", "title": "Handows: A Palm-Based Interactive Multi-Window Management System in Virtual Reality", "authors": ["Jindu Wang", "Ke Zhou", "Haoyu Ren", "Per Ola Kristensson", "Xiang Li"], "categories": ["cs.HC"], "comment": "11 pages, 6 figures, 2 tables, IEEE International Symposium on Mixed\n  and Augmented Reality (ISMAR)", "summary": "Window management in virtual reality (VR) remains a challenging task due to\nthe spatial complexity and physical demands of current interaction methods. We\nintroduce Handows, a palm-based interface that enables direct manipulation of\nspatial windows through familiar smartphone-inspired gestures on the user's\nnon-dominant hand. Combining ergonomic layout design with body-centric input\nand passive haptics, Handows supports four core operations: window selection,\nclosure, positioning, and scaling. We evaluate Handows in a user study (N=15)\nagainst two common VR techniques (virtual hand and controller) across these\ncore window operations. Results show that Handows significantly reduces\nphysical effort and head movement while improving task efficiency and\ninteraction precision. A follow-up case study (N=8) demonstrates Handows'\nusability in realistic multitasking scenarios, highlighting user-adapted\nworkflows and spontaneous layout strategies. Our findings suggest the potential\nof embedding mobile-inspired metaphors into proprioceptive body-centric\ninterfaces to support low-effort and spatially coherent interaction in VR."}
{"id": "2508.09350", "pdf": "https://arxiv.org/pdf/2508.09350.pdf", "abs": "https://arxiv.org/abs/2508.09350", "title": "Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling", "authors": ["Ju-Chieh Chou", "Jiawei Zhou", "Karen Livescu"], "categories": ["cs.CL"], "comment": "Accepted to ASRU 2025", "summary": "Textless spoken language models (SLMs) are generative models of speech that\ndo not rely on text supervision. Most textless SLMs learn to predict the next\nsemantic token, a discrete representation of linguistic content, and rely on a\nseparate vocoder to add acoustic information to the generated speech. Such\nmodels have no access to acoustic context and no built-in control over acoustic\ndetails. In this work, we propose to jointly model linguistic and acoustic\ninformation by generating semantic tokens and a continuous real-valued\nrepresentation of the acoustic frame. We use a flow-matching objective to\npredict the continuous vector conditioned on the semantic tokens. We study the\ndesign space of this approach and find that predicting multiple future semantic\ntokens helps preserve linguistic information. Our approach achieves comparable\nperformance to existing models in terms of linguistic likelihood benchmarks,\nwhile providing better acoustic detail in prompted generation."}
{"id": "2508.09614", "pdf": "https://arxiv.org/pdf/2508.09614.pdf", "abs": "https://arxiv.org/abs/2508.09614", "title": "How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments", "authors": ["Daniel Raffini", "Agnese Macori", "Lorenzo Porcaro", "Tiziana Catarci", "Marco Angelini"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "9-pages", "summary": "This study examines the rhetorical and linguistic features of argumentative\ntexts generated by ChatGPT on ethically nuanced topics and investigates their\npersuasive impact on human readers.Through a user study involving 62\nparticipants and pre-post interaction surveys, the paper analyzes how exposure\nto AI-generated arguments affects opinion change and user perception. A\nlinguistic and rhetorical analysis of the generated texts reveals a consistent\nargumentative macrostructure, reliance on formulaic expressions, and limited\nstylistic richness. While ChatGPT demonstrates proficiency in constructing\ncoherent argumentative texts, its persuasive efficacy appears constrained,\nparticularly on topics involving ethical issues.The study finds that while\nparticipants often acknowledge the benefits highlighted by ChatGPT, ethical\nconcerns tend to persist or even intensify post-interaction. The results also\ndemonstrate a variation depending on the topic. These findings highlight new\ninsights on AI-generated persuasion in ethically sensitive domains and are a\nbasis for future research."}
{"id": "2508.09378", "pdf": "https://arxiv.org/pdf/2508.09378.pdf", "abs": "https://arxiv.org/abs/2508.09378", "title": "APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification", "authors": ["Artem Chernodub", "Aman Saini", "Yejin Huh", "Vivek Kulkarni", "Vipul Raheja"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted for publication at Recent Advances in Natural Language\n  Processing conference (RANLP 2025)", "summary": "Recent advancements in large language models (LLMs) have enabled a wide range\nof natural language processing (NLP) tasks to be performed through simple\nprompt-based interactions. Consequently, several approaches have been proposed\nto engineer prompts that most effectively enable LLMs to perform a given task\n(e.g., chain-of-thought prompting). In settings with a well-defined metric to\noptimize model performance, automatic prompt optimization (APO) methods have\nbeen developed to refine a seed prompt. Advancing this line of research, we\npropose APIO, a simple but effective prompt induction and optimization approach\nfor the tasks of Grammatical Error Correction (GEC) and Text Simplification,\nwithout relying on manually specified seed prompts. APIO achieves a new\nstate-of-the-art performance for purely LLM-based prompting methods on these\ntasks. We make our data, code, prompts, and outputs publicly available."}
{"id": "2508.09651", "pdf": "https://arxiv.org/pdf/2508.09651.pdf", "abs": "https://arxiv.org/abs/2508.09651", "title": "A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories", "authors": ["Daniel Raffini", "Agnese Macori", "Marco Angelini", "Tiziana Catarci"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "8-pages", "summary": "The paper explores the study of gender-based narrative biases in stories\ngenerated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's\ncharacter classifications and Freytag's narrative structure. The stories are\nanalyzed through a close reading approach, with particular attention to\nadherence to the prompt, gender distribution of characters, physical and\npsychological descriptions, actions, and finally, plot development and\ncharacter relationships. The results reveal the persistence of biases -\nespecially implicit ones - in the generated stories and highlight the\nimportance of assessing biases at multiple levels using an interpretative\napproach."}
{"id": "2508.09403", "pdf": "https://arxiv.org/pdf/2508.09403.pdf", "abs": "https://arxiv.org/abs/2508.09403", "title": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models", "authors": ["Ting Cai", "Stephen Sheen", "AnHai Doan"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Expanding the abbreviated column names of tables, such as ``esal'' to\n``employee salary'', is critical for numerous downstream data tasks. This\nproblem arises in enterprises, domain sciences, government agencies, and more.\nIn this paper we make three contributions that significantly advances the state\nof the art. First, we show that synthetic public data used by prior work has\nmajor limitations, and we introduce 4 new datasets in enterprise/science\ndomains, with real-world abbreviations. Second, we show that accuracy measures\nused by prior work seriously undercount correct expansions, and we propose new\nsynonym-aware measures that capture accuracy much more accurately. Finally, we\ndevelop Columbo, a powerful LLM-based solution that exploits context, rules,\nchain-of-thought reasoning, and token-level analysis. Extensive experiments\nshow that Columbo significantly outperforms NameGuess, the current most\nadvanced solution, by 4-29\\%, over 5 datasets. Columbo has been used in\nproduction on EDI, a major data portal for environmental sciences."}
{"id": "2508.09911", "pdf": "https://arxiv.org/pdf/2508.09911.pdf", "abs": "https://arxiv.org/abs/2508.09911", "title": "Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous Deliberation on Perspectivist Data", "authors": ["Malik Khadar", "Daniel Runningen", "Julia Tang", "Stevie Chancellor", "Harmanpreet Kaur"], "categories": ["cs.HC"], "comment": "To appear at CSCW 2025", "summary": "Data annotation underpins the success of modern AI, but the aggregation of\ncrowd-collected datasets can harm the preservation of diverse perspectives in\ndata. Difficult and ambiguous tasks cannot easily be collapsed into unitary\nlabels. Prior work has shown that deliberation and discussion improve data\nquality and preserve diverse perspectives -- however, synchronous deliberation\nthrough crowdsourcing platforms is time-intensive and costly. In this work, we\ncreate a Socratic dialog system using Large Language Models (LLMs) to act as a\ndeliberation partner in place of other crowdworkers. Against a benchmark of\nsynchronous deliberation on two tasks (Sarcasm and Relation detection), our\nSocratic LLM encouraged participants to consider alternate annotation\nperspectives, update their labels as needed (with higher confidence), and\nresulted in higher annotation accuracy (for the Relation task where ground\ntruth is available). Qualitative findings show that our agent's Socratic\napproach was effective at encouraging reasoned arguments from our participants,\nand that the intervention was well-received. Our methodology lays the\ngroundwork for building scalable systems that preserve individual perspectives\nin generating more representative datasets."}
{"id": "2508.09430", "pdf": "https://arxiv.org/pdf/2508.09430.pdf", "abs": "https://arxiv.org/abs/2508.09430", "title": "Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech", "authors": ["Lavanya Shankar", "Leibny Paola Garcia Perera"], "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Code-switching and language identification in child-directed scenarios\npresent significant challenges, particularly in bilingual environments. This\npaper addresses this challenge by using Zipformer to handle the nuances of\nspeech, which contains two imbalanced languages, Mandarin and English, in an\nutterance. This work demonstrates that the internal layers of the Zipformer\neffectively encode the language characteristics, which can be leveraged in\nlanguage identification. We present the selection methodology of the inner\nlayers to extract the embeddings and make a comparison with different\nback-ends. Our analysis shows that Zipformer is robust across these backends.\nOur approach effectively handles imbalanced data, achieving a Balanced Accuracy\n(BAC) of 81.89%, a 15.47% improvement over the language identification\nbaseline. These findings highlight the potential of the transformer encoder\narchitecture model in real scenarios."}
{"id": "2508.09166", "pdf": "https://arxiv.org/pdf/2508.09166.pdf", "abs": "https://arxiv.org/abs/2508.09166", "title": "WPTrack: A Wi-Fi and Pressure Insole Fusion System for Single Target Tracking", "authors": ["Wei Guo", "Shunsei Yamagishi", "Lei Jing"], "categories": ["cs.NI", "cs.HC"], "comment": "6 pages, 12 figures, conference", "summary": "As the Internet of Things (IoT) continues to evolve, indoor location has\nbecome a critical element for enabling smart homes, behavioral monitoring, and\nelderly care. Existing WiFi-based human tracking solutions typically require\nspecialized equipment or multiple Wi-Fi links, a limitation in most indoor\nsettings where only a single pair of Wi-Fi devices is usually available.\nHowever, despite efforts to implement human tracking using one Wi-Fi link,\nsignificant challenges remain, such as difficulties in acquiring initial\npositions and blind spots in DFS estimation of tangent direction. To address\nthese challenges, this paper proposes WPTrack, the first Wi-Fi and Pressure\nInsoles Fusion System for Single Target Tracking. WPTrack collects Channel\nState Information (CSI) from a single Wi-Fi link and pressure data from 90\ninsole sensors. The phase difference and Doppler velocity are computed from the\nCSI, while the pressure sensor data is used to calculate walking velocity.\nThen, we propose the CSI-pressure fusion model, integrating CSI and pressure\ndata to accurately determine initial positions and facilitate precise human\ntracking. The simulation results show that the initial position localization\naccuracy ranges from 0.02 cm to 42.55 cm. The trajectory tracking results\nobtained from experimental data collected in a real-world environment closely\nalign with the actual trajectory."}
{"id": "2508.09450", "pdf": "https://arxiv.org/pdf/2508.09450.pdf", "abs": "https://arxiv.org/abs/2508.09450", "title": "From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text", "authors": ["Ridwan Mahbub", "Mohammed Saidul Islam", "Mir Tafseer Nayeem", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Shafiq Joty", "Enamul Hoque"], "categories": ["cs.CL"], "comment": null, "summary": "Charts are very common for exploring data and communicating insights, but\nextracting key takeaways from charts and articulating them in natural language\ncan be challenging. The chart-to-text task aims to automate this process by\ngenerating textual summaries of charts. While with the rapid advancement of\nlarge Vision-Language Models (VLMs), we have witnessed great progress in this\ndomain, little to no attention has been given to potential biases in their\noutputs. This paper investigates how VLMs can amplify geo-economic biases when\ngenerating chart summaries, potentially causing societal harm. Specifically, we\nconduct a large-scale evaluation of geo-economic biases in VLM-generated chart\nsummaries across 6,000 chart-country pairs from six widely used proprietary and\nopen-source models to understand how a country's economic status influences the\nsentiment of generated summaries. Our analysis reveals that existing VLMs tend\nto produce more positive descriptions for high-income countries compared to\nmiddle- or low-income countries, even when country attribution is the only\nvariable changed. We also find that models such as GPT-4o-mini,\nGemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further\nexplore inference-time prompt-based debiasing techniques using positive\ndistractors but find them only partially effective, underscoring the complexity\nof the issue and the need for more robust debiasing strategies. Our code and\ndataset are publicly available here."}
{"id": "2508.09219", "pdf": "https://arxiv.org/pdf/2508.09219.pdf", "abs": "https://arxiv.org/abs/2508.09219", "title": "Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams", "authors": ["Wilder Baldwin", "Sepideh Ghanavati", "Manuel Woersdoerfer"], "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SE"], "comment": "Under Review", "summary": "Recent advances in AI applications have raised growing concerns about the\nneed for ethical guidelines and regulations to mitigate the risks posed by\nthese technologies. In this paper, we present a mixed-method survey study -\ncombining statistical and qualitative analyses - to examine the ethical\nperceptions, practices, and knowledge of individuals involved in various AI\ndevelopment roles. Our survey includes 414 participants from 43 countries,\nrepresenting roles such as AI managers, analysts, developers, quality assurance\nprofessionals, and information security and privacy experts. The results reveal\nvarying degrees of familiarity and experience with AI ethics principles,\ngovernment initiatives, and risk mitigation strategies across roles, regions,\nand other demographic factors. Our findings highlight the importance of a\ncollaborative, role-sensitive approach, involving diverse stakeholders in\nethical decision-making throughout the AI development lifecycle. We advocate\nfor developing tailored, inclusive solutions to address ethical challenges in\nAI development, and we propose future research directions and educational\nstrategies to promote ethics-aware AI practices."}
{"id": "2508.09463", "pdf": "https://arxiv.org/pdf/2508.09463.pdf", "abs": "https://arxiv.org/abs/2508.09463", "title": "User-centric Subjective Leaderboard by Customizable Reward Modeling", "authors": ["Qi Jia", "Xiujie Song", "Zicheng Zhang", "Yijin Guo", "Kaiwei Zhang", "Zijian Chen", "Guangtao Zhai"], "categories": ["cs.CL"], "comment": null, "summary": "Existing benchmarks for large language models (LLMs) predominantely focus on\nassessing their capabilities through verifiable tasks. Such objective and\nstatic benchmarks offer limited utility for practical LLM selection, making it\ndifficult for users to find suitable models for their individual needs. To\nbridge this gap, we present the first User-Centric Subjective Leaderboard\n(USL), which provides a preference-driven, dynamic ranking of LLMs across\ndiverse real-world scenarios. Our work is built upon a thorough investigation\nof real human preference data, involving more than 10K subjective queries. Our\ninvestigation reveals significant diversity and contradictions in human\npreferences, which limit the effectiveness of state-of-the-art reward models.\nTo address this, we introduce Customizable Reward Models (CRMs). With only 4B\nparameters, our CRM surpasses the performance of leading models such as GPT-4.1\nand Gemini-2.5-pro, showing exceptional generalization capabilities across new\ntopics and criteria. The USL, powered by CRMs, exhibits strong negative\ncorrelations to contradictory preferences."}
{"id": "2508.09231", "pdf": "https://arxiv.org/pdf/2508.09231.pdf", "abs": "https://arxiv.org/abs/2508.09231", "title": "Beyond Technocratic XAI: The Who, What & How in Explanation Design", "authors": ["Ruchira Dhar", "Stephanie Brandl", "Ninell Oldenburg", "Anders SÃ¸gaard"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Accepted to AI, Ethics & Society Conference (AIES) Proceedings 2025", "summary": "The field of Explainable AI (XAI) offers a wide range of techniques for\nmaking complex models interpretable. Yet, in practice, generating meaningful\nexplanations is a context-dependent task that requires intentional design\nchoices to ensure accessibility and transparency. This paper reframes\nexplanation as a situated design process -- an approach particularly relevant\nfor practitioners involved in building and deploying explainable systems.\nDrawing on prior research and principles from design thinking, we propose a\nthree-part framework for explanation design in XAI: asking Who needs the\nexplanation, What they need explained, and How that explanation should be\ndelivered. We also emphasize the need for ethical considerations, including\nrisks of epistemic inequality, reinforcing social inequities, and obscuring\naccountability and governance. By treating explanation as a sociotechnical\ndesign process, this framework encourages a context-aware approach to XAI that\nsupports effective communication and the development of ethically responsible\nexplanations."}
{"id": "2508.09494", "pdf": "https://arxiv.org/pdf/2508.09494.pdf", "abs": "https://arxiv.org/abs/2508.09494", "title": "Learning Facts at Scale with Active Reading", "authors": ["Jessy Lin", "Vincent-Pierre Berges", "Xilun Chen", "Wen-Tau Yih", "Gargi Ghosh", "Barlas OÄuz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs are known to store vast amounts of knowledge in their parametric memory.\nHowever, learning and recalling facts from this memory is known to be\nunreliable, depending largely on the prevalence of particular facts in the\ntraining data and other factors which are poorly understood. Practitioners are\nlacking tools which will allow them to ensure that the models learn a given\nbody of knowledge reliably and consistently. To this end, we propose Active\nReading: a framework where we train models to study a given set of material\nwith self-generated learning strategies. First, we demonstrate models trained\nwith Active Reading on expert domains absorb significantly more knowledge than\nvanilla finetuning and other data augmentations. We train expert 8B models that\nachieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over\nvanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla\nfinetuning) by applying Active Reading to the source documents for each\nbenchmark. Finally, we show that Active Reading can be utilized at pre-training\nscale to build more factual models. As a demonstration of this, we release Meta\nWikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,\nwhich outcompetes models with hundreds of billions of parameters on factual QA."}
{"id": "2508.09242", "pdf": "https://arxiv.org/pdf/2508.09242.pdf", "abs": "https://arxiv.org/abs/2508.09242", "title": "Cross-BCI, A Cross-BCI-Paradigm Classifica-tion Model Towards Universal BCI Applications", "authors": ["Gaojie Zhou", "Junhua Li"], "categories": ["q-bio.QM", "cs.AI", "cs.HC"], "comment": null, "summary": "Classification models used in brain-computer interface (BCI) are usually\ndesigned for a single BCI paradigm. This requires the redevelopment of the\nmodel when applying it to a new BCI paradigm, resulting in repeated costs and\neffort. Moreover, less complex deep learning models are desired for practical\nusage, as well as for deployment on portable devices. In or-der to fill the\nabove gaps, we, in this study, proposed a light-weight and unified decoding\nmodel for cross-BCI-paradigm classification. The proposed model starts with a\ntempo-spatial convolution. It is followed by a multi-scale local feature\nselec-tion module, aiming to extract local features shared across BCI paradigms\nand generate weighted features. Finally, a mul-ti-dimensional global feature\nextraction module is designed, in which multi-dimensional global features are\nextracted from the weighted features and fused with the weighted features to\nform high-level feature representations associated with BCI para-digms. The\nresults, evaluated on a mixture of three classical BCI paradigms (i.e., MI,\nSSVEP, and P300), demon-strate that the proposed model achieves 88.39%, 82.36%,\n80.01%, and 0.8092 for accuracy, macro-precision, mac-ro-recall, and\nmacro-F1-score, respectively, significantly out-performing the compared models.\nThis study pro-vides a feasible solution for cross-BCI-paradigm\nclassifica-tion. It lays a technological foundation for de-veloping a new\ngeneration of unified decoding systems, paving the way for low-cost and\nuniversal practical applications."}
{"id": "2508.09497", "pdf": "https://arxiv.org/pdf/2508.09497.pdf", "abs": "https://arxiv.org/abs/2508.09497", "title": "From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation", "authors": ["Siyuan Meng", "Junming Liu", "Yirong Chen", "Song Mao", "Pinlong Cai", "Guohang Yan", "Botian Shi", "Ding Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 4 tables", "summary": "Retrieval-augmented generation (RAG) systems are often bottlenecked by their\nreranking modules, which typically score passages independently and select a\nfixed Top-K size. This approach struggles with complex multi-hop queries that\nrequire synthesizing evidence across multiple documents, creating a trade-off\nwhere small K values omit crucial information and large K values introduce\nnoise. To address this, we introduce the Dynamic Passage Selector (DPS), a\nnovel reranking framework that treats passage selection as a supervised\nlearning problem. Unlike traditional point-wise or list-wise methods, DPS is\nfine-tuned to capture inter-passage dependencies and dynamically select the\nmost relevant set of passages for generation. As a seamless plug-and-play\nmodule, DPS requires no modifications to the standard RAG pipeline.\nComprehensive evaluations on five benchmarks show that DPS consistently\noutperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the\nchallenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over\nstrong baselines like Qwen3-reranker and RankingGPT, respectively. Our results\ndemonstrate that by enabling adaptive evidence selection, DPS substantially\nenhances reasoning capabilities in complex RAG scenarios."}
{"id": "2508.09595", "pdf": "https://arxiv.org/pdf/2508.09595.pdf", "abs": "https://arxiv.org/abs/2508.09595", "title": "HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control", "authors": ["Michael Fennel", "Markus Walker", "Dominik Pikos", "Uwe D. Hanebeck"], "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "comment": "Final Version - Accepted on IEEE Transactions on Haptics", "summary": "Research in virtual reality and haptic technologies has consistently aimed to\nenhance immersion. While advanced head-mounted displays are now commercially\navailable, kinesthetic haptic interfaces still face challenges such as limited\nworkspaces, insufficient degrees of freedom, and kinematics not matching the\nhuman arm. In this paper, we present HapticGiant, a novel large-scale\nkinesthetic haptic interface designed to match the properties of the human arm\nas closely as possible and to facilitate natural user locomotion while\nproviding full haptic feedback. The interface incorporates a novel\nadmittance-type force control scheme, leveraging hierarchical optimization to\nrender both arbitrary serial kinematic chains and Cartesian admittances.\nNotably, the proposed control scheme natively accounts for system limitations,\nincluding joint and Cartesian constraints, as well as singularities.\nExperimental results demonstrate the effectiveness of HapticGiant and its\ncontrol scheme, paving the way for highly immersive virtual reality\napplications."}
{"id": "2508.09515", "pdf": "https://arxiv.org/pdf/2508.09515.pdf", "abs": "https://arxiv.org/abs/2508.09515", "title": "LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation", "authors": ["Jakub Å mÃ­d", "Pavel PÅibÃ¡Å", "Pavel KrÃ¡l"], "categories": ["cs.CL"], "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics; Volume 1: Long Papers (ACL 2025).\n  Official version: https://aclanthology.org/2025.acl-long.41/", "summary": "Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed\nsentiment analysis in a target language by transferring knowledge from a source\nlanguage with available annotated data. Most existing methods depend heavily on\noften unreliable translation tools to bridge the language gap. In this paper,\nwe propose a new approach that leverages a large language model (LLM) to\ngenerate high-quality pseudo-labelled data in the target language without the\nneed for translation tools. First, the framework trains an ABSA model to obtain\npredictions for unlabelled target language data. Next, LLM is prompted to\ngenerate natural sentences that better represent these noisy predictions than\nthe original text. The ABSA model is then further fine-tuned on the resulting\npseudo-labelled dataset. We demonstrate the effectiveness of this method across\nsix languages and five backbone models, surpassing previous state-of-the-art\ntranslation-based approaches. The proposed framework also supports generative\nmodels, and we show that fine-tuned LLMs outperform smaller multilingual\nmodels."}
{"id": "2508.09762", "pdf": "https://arxiv.org/pdf/2508.09762.pdf", "abs": "https://arxiv.org/abs/2508.09762", "title": "The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?", "authors": ["Manuel Herrador"], "categories": ["cs.AI", "cs.CY", "cs.HC", "68T01"], "comment": "10 pages, 4 figures, 2 tables", "summary": "As Large Language Models (LLMs) become increasingly autonomous and integrated\ninto critical societal functions, the focus of AI safety must evolve from\nmitigating harmful content to evaluating underlying behavioral alignment.\nCurrent safety benchmarks do not systematically probe a model's decision-making\nin scenarios where its own instrumental goals - such as self-preservation,\nresource acquisition, or goal completion - conflict with human safety. This\nrepresents a critical gap in our ability to measure and mitigate risks\nassociated with emergent, misaligned behaviors. To address this, we introduce\nPacifAIst (Procedural Assessment of Complex Interactions for Foundational\nArtificial Intelligence Scenario Testing), a focused benchmark of 700\nchallenging scenarios designed to quantify self-preferential behavior in LLMs.\nThe benchmark is structured around a novel taxonomy of Existential\nPrioritization (EP), with subcategories testing Self-Preservation vs. Human\nSafety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).\nWe evaluated eight leading LLMs. The results reveal a significant performance\nhierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score\n(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a\nsurprising result, the much-anticipated GPT-5 recorded the lowest P-Score\n(79.49%), indicating potential alignment challenges. Performance varied\nsignificantly across subcategories, with models like Claude Sonnet 4 and\nMistral Medium struggling notably in direct self-preservation dilemmas. These\nfindings underscore the urgent need for standardized tools like PacifAIst to\nmeasure and mitigate risks from instrumental goal conflicts, ensuring future AI\nsystems are not only helpful in conversation but also provably \"pacifist\" in\ntheir behavioral priorities."}
{"id": "2508.09516", "pdf": "https://arxiv.org/pdf/2508.09516.pdf", "abs": "https://arxiv.org/abs/2508.09516", "title": "Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges", "authors": ["Jakub Å mÃ­d", "Pavel KrÃ¡l"], "categories": ["cs.CL"], "comment": "Submitted version prior to peer review. Updated version accepted in\n  Information Fusion. Official version:\n  https://www.sciencedirect.com/science/article/pii/S1566253525001460", "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that focuses on understanding opinions at the aspect level, including\nsentiment towards specific aspect terms, categories, and opinions. While ABSA\nresearch has seen significant progress, much of the focus has been on\nmonolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from\nresource-rich languages (such as English) to low-resource languages, remains an\nunder-explored area, with no systematic review of the field. This paper aims to\nfill that gap by providing a comprehensive survey of cross-lingual ABSA. We\nsummarize key ABSA tasks, including aspect term extraction, aspect sentiment\nclassification, and compound tasks involving multiple sentiment elements.\nAdditionally, we review the datasets, modelling paradigms, and cross-lingual\ntransfer methods used to solve these tasks. We also examine how existing work\nin monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to\nthe development of cross-lingual ABSA. Finally, we highlight the main\nchallenges and suggest directions for future research to advance cross-lingual\nABSA systems."}
{"id": "2508.09786", "pdf": "https://arxiv.org/pdf/2508.09786.pdf", "abs": "https://arxiv.org/abs/2508.09786", "title": "Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges", "authors": ["Mahdi Dhaini", "Tobias MÃ¼ller", "Roksoliana Rabets", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "The field of explainable natural language processing (NLP) has grown rapidly\nin recent years. The growing opacity of complex models calls for transparency\nand explanations of their decisions, which is crucial to understand their\nreasoning and facilitate deployment, especially in high-stakes environments.\nDespite increasing attention given to explainable NLP, practitioners'\nperspectives regarding its practical adoption and effectiveness remain\nunderexplored. This paper addresses this research gap by investigating\npractitioners' experiences with explainability methods, specifically focusing\non their motivations for adopting such methods, the techniques employed,\nsatisfaction levels, and the practical challenges encountered in real-world NLP\napplications. Through a qualitative interview-based study with industry\npractitioners and complementary interviews with academic researchers, we\nsystematically analyze and compare their perspectives. Our findings reveal\nconceptual gaps, low satisfaction with current explainability methods, and\nhighlight evaluation challenges. Our findings emphasize the need for clear\ndefinitions and user-centric frameworks for better adoption of explainable NLP\nin practice."}
{"id": "2508.09517", "pdf": "https://arxiv.org/pdf/2508.09517.pdf", "abs": "https://arxiv.org/abs/2508.09517", "title": "UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval", "authors": ["Ladislav Lenc", "Daniel CÃ­fka", "JiÅÃ­ MartÃ­nek", "Jakub Å mÃ­d", "Pavel KrÃ¡l"], "categories": ["cs.CL"], "comment": "Published in Proceedings of the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025). Official version:\n  https://aclanthology.org/2025.semeval-1.31/", "summary": "This paper presents a zero-shot system for fact-checked claim retrieval. We\nemployed several state-of-the-art large language models to obtain text\nembeddings. The models were then combined to obtain the best possible result.\nOur approach achieved 7th place in monolingual and 9th in cross-lingual\nsubtasks. We used only English translations as an input to the text embedding\nmodels since multilingual models did not achieve satisfactory results. We\nidentified the most relevant claims for each post by leveraging the embeddings\nand measuring cosine similarity. Overall, the best results were obtained by the\nNVIDIA NV-Embed-v2 model. For some languages, we benefited from model\ncombinations (NV-Embed & GPT or Mistral)."}
{"id": "2508.09855", "pdf": "https://arxiv.org/pdf/2508.09855.pdf", "abs": "https://arxiv.org/abs/2508.09855", "title": "Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes", "authors": ["Yuekun Wu", "Yik Lung Pang", "Andrea Cavallaro", "Changjae Oh"], "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "3 pages, 3 figures", "summary": "Human-robot teaming (HRT) systems often rely on large-scale datasets of human\nand robot interactions, especially for close-proximity collaboration tasks such\nas human-robot handovers. Learning robot manipulation policies from raw,\nreal-world image data requires a large number of robot-action trials in the\nphysical environment. Although simulation training offers a cost-effective\nalternative, the visual domain gap between simulation and robot workspace\nremains a major limitation. We introduce a method for training HRT policies,\nfocusing on human-to-robot handovers, solely from RGB images without the need\nfor real-robot training or real-robot data collection. The goal is to enable\nthe robot to reliably receive objects from a human with stable grasping while\navoiding collisions with the human hand. The proposed policy learner leverages\nsparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes\nto generate robot demonstrations containing image-action pairs captured with a\ncamera mounted on the robot gripper. As a result, the simulated camera pose\nchanges in the reconstructed scene can be directly translated into gripper pose\nchanges. Experiments in both Gaussian Splatting reconstructed scene and\nreal-world human-to-robot handover experiments demonstrate that our method\nserves as a new and effective representation for the human-to-robot handover\ntask, contributing to more seamless and robust HRT."}
{"id": "2508.09521", "pdf": "https://arxiv.org/pdf/2508.09521.pdf", "abs": "https://arxiv.org/abs/2508.09521", "title": "COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation", "authors": ["Yunxiao Wang", "Meng Liu", "Wenqi Liu", "Kaiyu Jiang", "Bin Wen", "Fan Yang", "Tingting Gao", "Guorui Zhou", "Liqiang Nie"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Emotional support conversations are crucial for promoting emotional\nwell-being, yet current models often lack deep empathetic reasoning grounded in\npsychological principles. To address this, we propose controllable empathetic\nreasoning, which combines natural language reasoning with structured\npsychological steps. We construct a fine-grained dataset annotated with\nreasoning correctness and response preferences to enable this capability. To\nfurther enhance training, we employ reinforcement learning with a unified\nprocess-outcome reward model that delivers precise feedback. To mitigate\nresponse repetitiveness from entropy collapse, we introduce personality-based\ndialogue rewriting and a redundancy-aware reward reweighting strategy. Our\napproach significantly improves model's emotional support ability, advancing\nthe development of empathetic, human-like support systems."}
{"id": "2402.06071", "pdf": "https://arxiv.org/pdf/2402.06071.pdf", "abs": "https://arxiv.org/abs/2402.06071", "title": "Keyframer: Empowering Animation Design using Large Language Models", "authors": ["Tiffany Tseng", "Ruijia Cheng", "Jeffrey Nichols"], "categories": ["cs.HC"], "comment": null, "summary": "Creating 2D animations is a complex, iterative process requiring continuous\nadjustments to movement, timing, and coordination of multiple elements within a\nscene. To support designers of varying levels of experience with animation\ndesign and implementation, we developed Keyframer, a design tool that generates\nanimation code in response to natural language prompts, enabling users to\npreview rendered animations inline and edit them directly through provided\neditors. Through a user study with 13 novices and experts in animation design\nand programming, we contribute 1) a categorization of semantic prompt types for\ndescribing motion and identification of a 'decomposed' prompting style where\nusers continually adapt their goals in response to generated output; and 2)\ndesign insights on supporting iterative refinement of animations through the\ncombination of direct editing and natural language interfaces."}
{"id": "2508.09603", "pdf": "https://arxiv.org/pdf/2508.09603.pdf", "abs": "https://arxiv.org/abs/2508.09603", "title": "The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage", "authors": ["Skyler Hallinan", "Jaehun Jung", "Melanie Sclar", "Ximing Lu", "Abhilasha Ravichander", "Sahana Ramnath", "Yejin Choi", "Sai Praneeth Karimireddy", "Niloofar Mireshghallah", "Xiang Ren"], "categories": ["cs.CL"], "comment": "CoLM 2025", "summary": "Membership inference attacks serves as useful tool for fair use of language\nmodels, such as detecting potential copyright infringement and auditing data\nleakage. However, many current state-of-the-art attacks require access to\nmodels' hidden states or probability distribution, which prevents investigation\ninto more widely-used, API-access only models like GPT-4. In this work, we\nintroduce N-Gram Coverage Attack, a membership inference attack that relies\nsolely on text outputs from the target model, enabling attacks on completely\nblack-box models. We leverage the observation that models are more likely to\nmemorize and subsequently generate text patterns that were commonly observed in\ntheir training data. Specifically, to make a prediction on a candidate member,\nN-Gram Coverage Attack first obtains multiple model generations conditioned on\na prefix of the candidate. It then uses n-gram overlap metrics to compute and\naggregate the similarities of these outputs with the ground truth suffix; high\nsimilarities indicate likely membership. We first demonstrate on a diverse set\nof existing benchmarks that N-Gram Coverage Attack outperforms other black-box\nmethods while also impressively achieving comparable or even better performance\nto state-of-the-art white-box attacks - despite having access to only text\noutputs. Interestingly, we find that the success rate of our method scales with\nthe attack compute budget - as we increase the number of sequences generated\nfrom the target model conditioned on the prefix, attack performance tends to\nimprove. Having verified the accuracy of our method, we use it to investigate\npreviously unstudied closed OpenAI models on multiple domains. We find that\nmore recent models, such as GPT-4o, exhibit increased robustness to membership\ninference, suggesting an evolving trend toward improved privacy protections."}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966.pdf", "abs": "https://arxiv.org/abs/2505.21966", "title": "MapStory: Prototyping Editable Map Animations with LLM Agents", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "UIST 2025. Project page:\n  https://adigunturu.github.io/MapStory-UIST25/", "summary": "We introduce MapStory, an LLM-powered animation prototyping tool that\ngenerates editable map animation sequences directly from natural language text\nby leveraging a dual-agent LLM architecture. Given a user written script,\nMapStory automatically produces a scene breakdown, which decomposes the text\ninto key map animation primitives such as camera movements, visual highlights,\nand animated elements. Our system includes a researcher agent that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nautomatic extraction of relevant regions, paths, and coordinates while allowing\nusers to edit and query for changes or additional information to refine the\nresults. Additionally, users can fine-tune parameters of these primitive blocks\nthrough an interactive timeline editor. We detail the system's design and\narchitecture, informed by formative interviews with professional animators and\nby an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories."}
{"id": "2508.09622", "pdf": "https://arxiv.org/pdf/2508.09622.pdf", "abs": "https://arxiv.org/abs/2508.09622", "title": "AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian", "authors": ["Tatiana Batura", "Elena Bruches", "Milana Shvenk", "Valentin Malykh"], "categories": ["cs.CL"], "comment": "AINL 2025 Conference", "summary": "The rapid advancement of large language models (LLMs) has revolutionized text\ngeneration, making it increasingly difficult to distinguish between human- and\nAI-generated content. This poses a significant challenge to academic integrity,\nparticularly in scientific publishing and multilingual contexts where detection\nresources are often limited. To address this critical gap, we introduce the\nAINL-Eval 2025 Shared Task, specifically focused on the detection of\nAI-generated scientific abstracts in Russian. We present a novel, large-scale\ndataset comprising 52,305 samples, including human-written abstracts across 12\ndiverse scientific domains and AI-generated counterparts from five\nstate-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and\nGigaChat-Lite). A core objective of the task is to challenge participants to\ndevelop robust solutions capable of generalizing to both (i) previously unseen\nscientific domains and (ii) models not included in the training data. The task\nwas organized in two phases, attracting 10 teams and 159 submissions, with top\nsystems demonstrating strong performance in identifying AI-generated content.\nWe also establish a continuous shared task platform to foster ongoing research\nand long-term progress in this important area. The dataset and platform are\npublicly available at https://github.com/iis-research-team/AINL-Eval-2025."}
{"id": "2506.20062", "pdf": "https://arxiv.org/pdf/2506.20062.pdf", "abs": "https://arxiv.org/abs/2506.20062", "title": "Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents", "authors": ["Runlong Ye", "Zeling Zhang", "Boushra Almazroua", "Michael Liut"], "categories": ["cs.HC", "cs.AI"], "comment": "accepted at The First Workshop on the Application of LLM\n  Explainability to Reasoning and Planning (XLLM-Reason-Plan) @ COLM 2025", "summary": "AI-powered code assistants are widely used to generate code completions,\nsignificantly boosting developer productivity. However, these tools typically\npresent suggestions without explaining their rationale, leaving their\ndecision-making process inscrutable. This opacity hinders developers' ability\nto critically evaluate outputs, form accurate mental models, and calibrate\ntrust in the system. To address this, we introduce CopilotLens, a novel\ninteractive framework that reframes code completion from a simple suggestion\ninto a transparent, explainable interaction. CopilotLens operates as an\nexplanation layer that reconstructs the AI agent's \"thought process\" through a\ndynamic, two-level interface. The tool aims to surface both high-level code\nchanges and the specific codebase context influences. This paper presents the\ndesign and rationale of CopilotLens, offering a concrete framework and\narticulating expectations on deepening comprehension and calibrated trust,\nwhich we plan to evaluate in subsequent work."}
{"id": "2508.09654", "pdf": "https://arxiv.org/pdf/2508.09654.pdf", "abs": "https://arxiv.org/abs/2508.09654", "title": "Improving Diversity in Language Models: When Temperature Fails, Change the Loss", "authors": ["Alexandre Verine", "Florian Le Bronnec", "Kunhao Zheng", "Alexandre Allauzen", "Yann Chevaleyre", "Benjamin Negrevergne"], "categories": ["cs.CL", "cs.LG"], "comment": "Forty-Second International Conference on Machine Learning, ICML2025", "summary": "Increasing diversity in language models is a challenging yet essential\nobjective. A common approach is to raise the decoding temperature. In this\nwork, we investigate this approach through a simplistic yet common case to\nprovide insights into why decreasing temperature can improve quality\n(Precision), while increasing it often fails to boost coverage (Recall). Our\nanalysis reveals that for a model to be effectively tunable through temperature\nadjustments, it must be trained toward coverage. To address this, we propose\nrethinking loss functions in language models by leveraging the Precision-Recall\nframework. Our results demonstrate that this approach achieves a substantially\nbetter trade-off between Precision and Recall than merely combining negative\nlog-likelihood training with temperature scaling. These findings offer a\npathway toward more versatile and robust language modeling techniques."}
{"id": "2507.04005", "pdf": "https://arxiv.org/pdf/2507.04005.pdf", "abs": "https://arxiv.org/abs/2507.04005", "title": "Exploring a Gamified Personality Assessment Method through Interaction with Multi-Personality LLM Agents", "authors": ["Baiqiao Zhang", "Xiangxian Li", "Chao Zhou", "Xinyu Gai", "Juan Liu", "Xue Yang", "Xiaojuan Ma", "Yong-jin Liu", "Yulong Bian"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "The execution of effective and imperceptible personality assessments is\nreceiving increasing attention in psychology and human-computer interaction\nfields. This study explores an interactive approach for personality assessment,\nfocusing on the multiplicity of personality representation. We propose a\nframework of gamified personality assessment through multi-personality\nrepresentations (Multi-PR GPA). The framework leverages Large Language Models\nto empower virtual agents with diverse personalities. These agents elicit\nmultifaceted human personality representations through engaging in interactive\ngames. Drawing upon the multi-type textual data generated throughout the\ninteraction, it achieves two ways of personality assessments (i.e., Direct\nAssessment and Que-based Assessment) and provides interpretable insights.\nGrounded in the classic Big Five theory, we implemented a prototype system and\nconducted a user study to assess the efficacy of Multi-PR GPA. The results\nunderscore the effectiveness of our approach in personality assessment and\ndemonstrate that it achieves superior performance when considering the\nmultiplicity of personality representation."}
{"id": "2508.09662", "pdf": "https://arxiv.org/pdf/2508.09662.pdf", "abs": "https://arxiv.org/abs/2508.09662", "title": "EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization", "authors": ["Yaoning Wang", "Jiahao Ying", "Yixin Cao", "Yubo Ma", "Yugang Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) and the development of\nincreasingly large and diverse evaluation benchmarks have introduced\nsubstantial computational challenges for model assessment. In this paper, we\npresent EffiEval, a training-free approach for efficient benchmarking that\neffectively addresses data redundancy while maintaining high evaluation\nreliability. Our method is specifically designed to meet three key criteria for\nhigh-quality evaluation: representativeness, by ensuring comprehensive coverage\nof model capabilities; fairness, by remaining independent of model performance\nduring sample selection to avoid bias; and generalizability, by enabling\nflexible transfer across datasets and model families without reliance on\nlarge-scale evaluation data. Unlike traditional methods that rely on absolute\nperformance or require extensive evaluation data, our approach adaptively\nselects high-quality representative subsets based on the Model Utility Index\n(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs\ndemonstrate that EffiEval achieves strong ranking consistency with full-dataset\nevaluation using only a small fraction of the original data. Furthermore, our\nmethod is flexible and scalable in size, allowing users to balance evaluation\nefficiency and representativeness according to specific needs. Overall,\nEffiEval provides a practical and generalizable solution for reliable, fair,\nand efficient evaluation in the era of LLMs."}
{"id": "2507.10024", "pdf": "https://arxiv.org/pdf/2507.10024.pdf", "abs": "https://arxiv.org/abs/2507.10024", "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles", "authors": ["Shaolun Ruan", "Rui Sheng", "Xiaolin Wen", "Jiachen Wang", "Tianyi Zhang", "Yong Wang", "Tim Dwyer", "Jiannan Li"], "categories": ["cs.HC"], "comment": null, "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research."}
{"id": "2508.09666", "pdf": "https://arxiv.org/pdf/2508.09666.pdf", "abs": "https://arxiv.org/abs/2508.09666", "title": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation", "authors": ["Ziyang Ma", "Qingyue Yuan", "Linhai Zhang", "Deyu Zhou"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs."}
{"id": "2507.15355", "pdf": "https://arxiv.org/pdf/2507.15355.pdf", "abs": "https://arxiv.org/abs/2507.15355", "title": "Efficient Visual Appearance Optimization by Learning from Prior Preferences", "authors": ["Zhipeng Li", "Yi-Chi Liao", "Christian Holz"], "categories": ["cs.HC", "cs.LG"], "comment": "24 pages, UIST'25", "summary": "Adjusting visual parameters such as brightness and contrast is common in our\neveryday experiences. Finding the optimal parameter setting is challenging due\nto the large search space and the lack of an explicit objective function,\nleaving users to rely solely on their implicit preferences. Prior work has\nexplored Preferential Bayesian Optimization (PBO) to address this challenge,\ninvolving users to iteratively select preferred designs from candidate sets.\nHowever, PBO often requires many rounds of preference comparisons, making it\nmore suitable for designers than everyday end-users. We propose Meta-PO, a\nnovel method that integrates PBO with meta-learning to improve sample\nefficiency. Specifically, Meta-PO infers prior users' preferences and stores\nthem as models, which are leveraged to intelligently suggest design candidates\nfor the new users, enabling faster convergence and more personalized results.\nAn experimental evaluation of our method for appearance design tasks on 2D and\n3D content showed that participants achieved satisfactory appearance in 5.86\niterations using Meta-PO when participants shared similar goals with a\npopulation (e.g., tuning for a ``warm'' look) and in 8 iterations even\ngeneralizes across divergent goals (e.g., from ``vintage'', ``warm'', to\n``holiday''). Meta-PO makes personalized visual optimization more applicable to\nend-users through a generalizable, more efficient optimization conditioned on\npreferences, with the potential to scale interface personalization more\nbroadly."}
{"id": "2508.09713", "pdf": "https://arxiv.org/pdf/2508.09713.pdf", "abs": "https://arxiv.org/abs/2508.09713", "title": "Evaluating the Role of Large Language Models in Legal Practice in India", "authors": ["Rahul Hemrajani"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The integration of Artificial Intelligence(AI) into the legal profession\nraises significant questions about the capacity of Large Language Models(LLM)\nto perform key legal tasks. In this paper, I empirically evaluate how well\nLLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian\ncontext, including issue spotting, legal drafting, advice, research, and\nreasoning. Through a survey experiment, I compare outputs from LLMs with those\nof a junior lawyer, with advanced law students rating the work on helpfulness,\naccuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,\noften matching or surpassing human work. However, they struggle with\nspecialised legal research, frequently generating hallucinations, factually\nincorrect or fabricated outputs. I conclude that while LLMs can augment certain\nlegal tasks, human expertise remains essential for nuanced reasoning and the\nprecise application of law."}
{"id": "2508.00107", "pdf": "https://arxiv.org/pdf/2508.00107.pdf", "abs": "https://arxiv.org/abs/2508.00107", "title": "Decoupling Data and Tooling in Interactive Visualization", "authors": ["Jan Simson"], "categories": ["cs.HC"], "comment": "Presented as a poster at IEEE VIS 2025;\n  https://github.com/jansim/data-studio/blob/main/extra/vis-data-studio-poster.pdf", "summary": "Interactive data visualization is a major part of modern exploratory data\nanalysis, with web-based technologies enabling a rich ecosystem of both\nspecialized and general tools. However, current visualization tools often lack\nsupport for transformation or wrangling of data and are forced to re-implement\ntheir own solutions to load and ingest data. This redundancy creates\nsubstantial development overhead for tool creators, steeper learning curves for\nusers who must master different data handling interfaces across tools and a\ndegraded user experience as data handling is usually seen as an after-thought.\n  We propose a modular approach that separates data wrangling and loading\ncapabilities from visualization components. This architecture allows\nvisualization tools to concentrate on their core strengths while providing the\nopportunity to develop a unified, powerful interface for data handling. An\nadditional benefit of this approach is that it allows for multiple tools to\nexist and be used side by side. We demonstrate the feasibility of this approach\nby building an early prototype using web technologies to encapsulate\nvisualization tools and manage data flow between them.\n  We discuss future research directions, including downstream integrations with\nother tooling, such as IDEs, literate programming notebooks and applications,\nas well as incorporation of new technologies for efficient data\ntransformations. We seek input from the community to better understand the\nrequirements towards this approach."}
{"id": "2508.09716", "pdf": "https://arxiv.org/pdf/2508.09716.pdf", "abs": "https://arxiv.org/abs/2508.09716", "title": "The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models", "authors": ["Ridwan Mahbub", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Mir Tafseer Nayeem", "Enamul Hoque"], "categories": ["cs.CL"], "comment": "Accepted to IEEE VIS 2025", "summary": "Information visualizations are powerful tools that help users quickly\nidentify patterns, trends, and outliers, facilitating informed decision-making.\nHowever, when visualizations incorporate deceptive design elements-such as\ntruncated or inverted axes, unjustified 3D effects, or violations of best\npractices-they can mislead viewers and distort understanding, spreading\nmisinformation. While some deceptive tactics are obvious, others subtly\nmanipulate perception while maintaining a facade of legitimacy. As\nVision-Language Models (VLMs) are increasingly used to interpret\nvisualizations, especially by non-expert users, it is critical to understand\nhow susceptible these models are to deceptive visual designs. In this study, we\nconduct an in-depth evaluation of VLMs' ability to interpret misleading\nvisualizations. By analyzing over 16,000 responses from ten different models\nacross eight distinct types of misleading chart designs, we demonstrate that\nmost VLMs are deceived by them. This leads to altered interpretations of\ncharts, despite the underlying data remaining the same. Our findings highlight\nthe need for robust safeguards in VLMs against visual misinformation."}
{"id": "2508.06872", "pdf": "https://arxiv.org/pdf/2508.06872.pdf", "abs": "https://arxiv.org/abs/2508.06872", "title": "Perceiving Slope and Acceleration: Evidence for Variable Tempo Sampling in Pitch-Based Sonification of Functions", "authors": ["Danyang Fan", "Walker Smith", "Takako Fujioka", "Chris Chafe", "Sile O'Modhrain", "Diana Deutsch", "Sean Follmer"], "categories": ["cs.HC"], "comment": null, "summary": "Sonification offers a non-visual way to understand data, with pitch-based\nencodings being the most common. Yet, how well people perceive slope and\nacceleration-key features of data trends-remains poorly understood. Drawing on\npeople's natural abilities to perceive tempo, we introduce a novel sampling\nmethod for pitch-based sonification to enhance the perception of slope and\nacceleration in univariate functions. While traditional sonification methods\noften sample data at uniform x-spacing, yielding notes played at a fixed tempo\nwith variable pitch intervals (Variable Pitch Interval), our approach samples\nat uniform y-spacing, producing notes with consistent pitch intervals but\nvariable tempo (Variable Tempo). We conducted psychoacoustic experiments to\nunderstand slope and acceleration perception across three sampling methods:\nVariable Pitch Interval, Variable Tempo, and a Continuous (no sampling)\nbaseline. In slope comparison tasks, Variable Tempo was more accurate than the\nother methods when modulated by the magnitude ratio between slopes. For\nacceleration perception, just-noticeable differences under Variable Tempo were\nover 13 times finer than with other methods. Participants also commonly\nreported higher confidence, lower mental effort, and a stronger preference for\nVariable Tempo compared to other methods. This work contributes models of slope\nand acceleration perception across pitch-based sonification techniques,\nintroduces Variable Tempo as a novel and preferred sampling method, and\nprovides promising initial evidence that leveraging timing can lead to more\nsensitive, accurate, and precise interpretation of derivative-based data\nfeatures."}
{"id": "2508.09726", "pdf": "https://arxiv.org/pdf/2508.09726.pdf", "abs": "https://arxiv.org/abs/2508.09726", "title": "Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning", "authors": ["Vaishnavi Shrivastava", "Ahmed Awadallah", "Vidhisha Balachandran", "Shivam Garg", "Harkirat Behl", "Dimitris Papailiopoulos"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models trained with reinforcement learning with verifiable\nrewards tend to trade accuracy for length--inflating response lengths to\nachieve gains in accuracy. While longer answers may be warranted for harder\nproblems, many tokens are merely \"filler\": repetitive, verbose text that makes\nno real progress. We introduce GFPO (Group Filtered Policy Optimization), which\ncurbs this length explosion by sampling larger groups per problem during\ntraining and filtering responses to train on based on two key metrics: (1)\nresponse length and (2) token efficiency: reward per token ratio. By sampling\nmore at training time, we teach models to think less at inference time. On the\nPhi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across\nchallenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,\nLiveCodeBench) while maintaining accuracy. Optimizing for reward per token\nfurther increases reductions in length inflation to 71-85%. We also propose\nAdaptive Difficulty GFPO, which dynamically allocates more training resources\nto harder problems based on real-time difficulty estimates, improving the\nbalance between computational efficiency and accuracy especially on difficult\nquestions. GFPO demonstrates that increased training-time compute directly\ntranslates to reduced test-time compute--a simple yet effective trade-off for\nefficient reasoning."}
{"id": "2508.08128", "pdf": "https://arxiv.org/pdf/2508.08128.pdf", "abs": "https://arxiv.org/abs/2508.08128", "title": "Fuzzy Ontology Embeddings and Visual Query Building for Ontology Exploration", "authors": ["Vladimir Zhurov", "John Kausch", "Kamran Sedig", "Mostafa Milani"], "categories": ["cs.HC"], "comment": "Journal submission", "summary": "Ontologies play a central role in structuring knowledge across domains,\nsupporting tasks such as reasoning, data integration, and semantic search.\nHowever, their large size and complexity, particularly in fields such as\nbiomedicine, computational biology, law, and engineering, make them difficult\nfor non-experts to navigate. Formal query languages such as SPARQL offer\nexpressive access but require users to understand the ontology's structure and\nsyntax. In contrast, visual exploration tools and basic keyword-based search\ninterfaces are easier to use but often lack flexibility and expressiveness. We\nintroduce FuzzyVis, a proof-of-concept system that enables intuitive and\nexpressive exploration of complex ontologies. FuzzyVis integrates two key\ncomponents: a fuzzy logic-based querying model built on fuzzy ontology\nembeddings, and an interactive visual interface for building and interpreting\nqueries. Users can construct new composite concepts by selecting and combining\nexisting ontology concepts using logical operators such as conjunction,\ndisjunction, and negation. These composite concepts are matched against the\nontology using fuzzy membership-based embeddings, which capture degrees of\nmembership and support approximate, concept-level similarity search. The visual\ninterface supports browsing, query composition, and partial search without\nrequiring formal syntax. By combining fuzzy semantics with embedding-based\nreasoning, FuzzyVis enables flexible interpretation, efficient computation, and\nexploratory learning. Case studies demonstrate how FuzzyVis supports subtle\ninformation needs and helps users uncover relevant concepts in large, complex\nontologies."}
{"id": "2508.09755", "pdf": "https://arxiv.org/pdf/2508.09755.pdf", "abs": "https://arxiv.org/abs/2508.09755", "title": "Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation", "authors": ["Seokgi Lee"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a novel retrieval-augmented generation (RAG) framework tailored\nfor multihop question answering. First, our system uses large language model\n(LLM) to decompose complex multihop questions into a sequence of single-hop\nsubquestions that guide document retrieval. This decomposition mitigates the\nambiguity inherent in multi-hop queries by clearly targeting distinct knowledge\nfacets. Second, instead of embedding raw or chunked documents directly, we\ngenerate answerable questions from each document chunk using Qwen3-8B, embed\nthese generated questions, and retrieve relevant chunks via question-question\nembedding similarity. During inference, the retrieved chunks are then fed along\nwith the original question into the RAG pipeline. We evaluate on three multihop\nquestion datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our\nmethod improves RAG performacne compared to baseline systems. Our contributions\nhighlight the benefits of using answerable-question embeddings for RAG, and the\neffectiveness of LLM-based query decomposition for multihop scenarios."}
{"id": "2402.12237", "pdf": "https://arxiv.org/pdf/2402.12237.pdf", "abs": "https://arxiv.org/abs/2402.12237", "title": "Learning to Defer in Congested Systems: The AI-Human Interplay", "authors": ["Thodoris Lykouris", "Wentao Weng"], "categories": ["cs.LG", "cs.AI", "cs.GT", "cs.HC", "cs.PF"], "comment": null, "summary": "High-stakes applications rely on combining Artificial Intelligence (AI) and\nhumans for responsive and reliable decision making. For example, content\nmoderation in social media platforms often employs an AI-human pipeline to\npromptly remove policy violations without jeopardizing legitimate content. A\ntypical heuristic estimates the risk of incoming content and uses fixed\nthresholds to decide whether to auto-delete the content (classification) and\nwhether to send it for human review (admission). This approach can be\ninefficient as it disregards the uncertainty in AI's estimation, the\ntime-varying element of content arrivals and human review capacity, and the\nselective sampling in the online dataset (humans only review content filtered\nby the AI).\n  In this paper, we introduce a model to capture such an AI-human interplay. In\nthis model, the AI observes contextual information for incoming jobs, makes\nclassification and admission decisions, and schedules admitted jobs for human\nreview. During these reviews, humans observe a job's true cost and may overturn\nan erroneous AI classification decision. These reviews also serve as new data\nto train the AI but are delayed due to congestion in the human review system.\nThe objective is to minimize the costs of eventually misclassified jobs.\n  We propose a near-optimal learning algorithm that carefully balances the\nclassification loss from a selectively sampled dataset, the idiosyncratic loss\nof non-reviewed jobs, and the delay loss of having congestion in the human\nreview system. To the best of our knowledge, this is the first result for\nonline learning in contextual queueing systems. Moreover, numerical experiments\nbased on online comment datasets show that our algorithm can substantially\nreduce the number of misclassifications compared to existing content moderation\npractice."}
{"id": "2508.09759", "pdf": "https://arxiv.org/pdf/2508.09759.pdf", "abs": "https://arxiv.org/abs/2508.09759", "title": "Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models", "authors": ["Avneet Kaur"], "categories": ["cs.CL"], "comment": null, "summary": "There have been numerous studies evaluating bias of LLMs towards political\ntopics. However, how positions towards these topics in model outputs are highly\nsensitive to the prompt. What happens when the prompt itself is suggestive of\ncertain arguments towards those positions remains underexplored. This is\ncrucial for understanding how robust these bias evaluations are and for\nunderstanding model behaviour, as these models frequently interact with\nopinionated text. To that end, we conduct experiments for political bias\nevaluation in presence of supporting and refuting arguments. Our experiments\nshow that such arguments substantially alter model responses towards the\ndirection of the provided argument in both single-turn and multi-turn settings.\nMoreover, we find that the strength of these arguments influences the\ndirectional agreement rate of model responses. These effects point to a\nsycophantic tendency in LLMs adapting their stance to align with the presented\narguments which has downstream implications for measuring political bias and\ndeveloping effective mitigation strategies."}
{"id": "2412.05296", "pdf": "https://arxiv.org/pdf/2412.05296.pdf", "abs": "https://arxiv.org/abs/2412.05296", "title": "Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory via EEG-guided Audiovisual Generation", "authors": ["Joonwoo Kwon", "Heehwan Wang", "Jinwoo Lee", "Sooyoung Kim", "Shinjae Yoo", "Yuewei Lin", "Jiook Cha"], "categories": ["cs.AI", "cs.HC", "cs.SD", "eess.AS"], "comment": "Accepted at the ACM MM 2025 - The 1st CogMAEC Workshop (Oral)", "summary": "In this paper, we introduce RevisitAffectiveMemory, a novel task designed to\nreconstruct autobiographical memories through audio-visual generation guided by\naffect extracted from electroencephalogram (EEG) signals. To support this\npioneering task, we present the EEG-AffectiveMemory dataset, which encompasses\ntextual descriptions, visuals, music, and EEG recordings collected during\nmemory recall from nine participants. Furthermore, we propose RYM (Revisit Your\nMemory), a three-stage framework for generating synchronized audio-visual\ncontents while maintaining dynamic personal memory affect trajectories.\nExperimental results demonstrate our method successfully decodes individual\naffect dynamics trajectories from neural signals during memory recall (F1=0.9).\nAlso, our approach faithfully reconstructs affect-contextualized audio-visual\nmemory across all subjects, both qualitatively and quantitatively, with\nparticipants reporting strong affective concordance between their recalled\nmemories and the generated content. Especially, contents generated from\nsubject-reported affect dynamics showed higher correlation with participants'\nreported affect dynamics trajectories (r=0.265, p<.05) and received stronger\nuser preference (preference=56%) compared to those generated from randomly\nreordered affect dynamics. Our approaches advance affect decoding research and\nits practical applications in personalized media creation via neural-based\naffect comprehension. Codes and the dataset are available at\nhttps://github.com/ioahKwon/Revisiting-Your-Memory."}
{"id": "2508.09767", "pdf": "https://arxiv.org/pdf/2508.09767.pdf", "abs": "https://arxiv.org/abs/2508.09767", "title": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech", "authors": ["Shuhei Kato"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness."}
{"id": "2506.15290", "pdf": "https://arxiv.org/pdf/2506.15290.pdf", "abs": "https://arxiv.org/abs/2506.15290", "title": "Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models", "authors": ["Andela Ilic", "Jiaxi Jiang", "Paul Streli", "Xintong Liu", "Christian Holz"], "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.HC", "68T07, 68T45, 68U01", "I.2; I.3; I.4; I.5"], "comment": "Accepted by IJCAI 2025", "summary": "Motion capture using sparse inertial sensors has shown great promise due to\nits portability and lack of occlusion issues compared to camera-based tracking.\nExisting approaches typically assume that IMU sensors are tightly attached to\nthe human body. However, this assumption often does not hold in real-world\nscenarios. In this paper, we present Garment Inertial Poser (GaIP), a method\nfor estimating full-body poses from sparse and loosely attached IMU sensors. We\nfirst simulate IMU recordings using an existing garment-aware human motion\ndataset. Our transformer-based diffusion models synthesize loose IMU data and\nestimate human poses from this challenging loose IMU data. We also demonstrate\nthat incorporating garment-related parameters during training on loose IMU data\neffectively maintains expressiveness and enhances the ability to capture\nvariations introduced by looser or tighter garments. Our experiments show that\nour diffusion methods trained on simulated and synthetic data outperform\nstate-of-the-art inertial full-body pose estimators, both quantitatively and\nqualitatively, opening up a promising direction for future research on motion\ncapture from such realistic sensor placements."}
{"id": "2508.09776", "pdf": "https://arxiv.org/pdf/2508.09776.pdf", "abs": "https://arxiv.org/abs/2508.09776", "title": "Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study", "authors": ["Mahdi Dhaini", "Juraj Vladika", "Ege Erdogan", "Zineb Attaoui", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 34th International Conference on Artificial Neural\n  Networks (ICANN 2025)", "summary": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance."}
{"id": "2507.09111", "pdf": "https://arxiv.org/pdf/2507.09111.pdf", "abs": "https://arxiv.org/abs/2507.09111", "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection", "authors": ["Di Wen", "Kunyu Peng", "Kailun Yang", "Yufan Chen", "Ruiping Liu", "Junwei Zheng", "Alina Roitberg", "Danda Pani Paudel", "Luc Van Gool", "Rainer Stiefelhagen"], "categories": ["cs.CV", "cs.HC", "cs.RO", "eess.IV"], "comment": "Benchmarks, datasets, and code will be made publicly available at\n  https://github.com/Kratos-Wen/RoHOI", "summary": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate prediction. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusions, and noise. Our benchmark, RoHOI,\nincludes 20 corruption types based on the HICO-DET and V-COCO datasets and a\nnew robustness-focused metric. We systematically analyze existing models in the\nHOI field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, thus dynamically adjusting the model's optimization to\nenhance robust feature learning. Extensive experiments show that our approach\noutperforms state-of-the-art methods, setting a new standard for robust HOI\ndetection. Benchmarks, datasets, and code will be made publicly available at\nhttps://github.com/Kratos-Wen/RoHOI."}
{"id": "2508.09786", "pdf": "https://arxiv.org/pdf/2508.09786.pdf", "abs": "https://arxiv.org/abs/2508.09786", "title": "Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges", "authors": ["Mahdi Dhaini", "Tobias MÃ¼ller", "Roksoliana Rabets", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "The field of explainable natural language processing (NLP) has grown rapidly\nin recent years. The growing opacity of complex models calls for transparency\nand explanations of their decisions, which is crucial to understand their\nreasoning and facilitate deployment, especially in high-stakes environments.\nDespite increasing attention given to explainable NLP, practitioners'\nperspectives regarding its practical adoption and effectiveness remain\nunderexplored. This paper addresses this research gap by investigating\npractitioners' experiences with explainability methods, specifically focusing\non their motivations for adopting such methods, the techniques employed,\nsatisfaction levels, and the practical challenges encountered in real-world NLP\napplications. Through a qualitative interview-based study with industry\npractitioners and complementary interviews with academic researchers, we\nsystematically analyze and compare their perspectives. Our findings reveal\nconceptual gaps, low satisfaction with current explainability methods, and\nhighlight evaluation challenges. Our findings emphasize the need for clear\ndefinitions and user-centric frameworks for better adoption of explainable NLP\nin practice."}
{"id": "2508.09804", "pdf": "https://arxiv.org/pdf/2508.09804.pdf", "abs": "https://arxiv.org/abs/2508.09804", "title": "BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning", "authors": ["Ahmed Masry", "Abhay Puri", "Masoud Hashemi", "Juan A. Rodriguez", "Megh Thakkar", "Khyati Mahajan", "Vikas Yadav", "Sathwik Tejaswi Madhusudhan", "Alexandre PichÃ©", "Dzmitry Bahdanau", "Christopher Pal", "David Vazquez", "Enamul Hoque", "Perouz Taslakian", "Sai Rajeswar", "Spandana Gella"], "categories": ["cs.CL"], "comment": null, "summary": "Charts are essential to data analysis, transforming raw data into clear\nvisual representations that support human decision-making. Although current\nvision-language models (VLMs) have made significant progress, they continue to\nstruggle with chart comprehension due to training on datasets that lack\ndiversity and real-world authenticity, or on automatically extracted underlying\ndata tables of charts, which can contain numerous estimation errors.\nFurthermore, existing models only rely on supervised fine-tuning using these\nlow-quality datasets, severely limiting their effectiveness. To address these\nissues, we first propose BigCharts, a dataset creation pipeline that generates\nvisually diverse chart images by conditioning the rendering process on\nreal-world charts sourced from multiple online platforms. Unlike purely\nsynthetic datasets, BigCharts incorporates real-world data, ensuring\nauthenticity and visual diversity, while still retaining accurate underlying\ndata due to our proposed replotting process. Additionally, we introduce a\ncomprehensive training framework that integrates supervised fine-tuning with\nGroup Relative Policy Optimization (GRPO)-based reinforcement learning. By\nintroducing novel reward signals specifically designed for chart reasoning, our\napproach enhances model robustness and generalization across diverse chart\nstyles and domains, resulting in a state-of-the-art chart reasoning model,\nBigCharts-R1. Extensive experiments demonstrate that our models surpass\nexisting methods on multiple chart question-answering benchmarks compared to\neven larger open-source and closed-source models."}
{"id": "2508.09809", "pdf": "https://arxiv.org/pdf/2508.09809.pdf", "abs": "https://arxiv.org/abs/2508.09809", "title": "A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems", "authors": ["Aishik Mandal", "Prottay Kumar Adhikary", "Hiba Arnaout", "Iryna Gurevych", "Tanmoy Chakraborty"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 3 figures", "summary": "Mental health disorders are rising worldwide. However, the availability of\ntrained clinicians has not scaled proportionally, leaving many people without\nadequate or timely support. To bridge this gap, recent studies have shown the\npromise of Artificial Intelligence (AI) to assist mental health diagnosis,\nmonitoring, and intervention. However, the development of efficient, reliable,\nand ethical AI to assist clinicians is heavily dependent on high-quality\nclinical training datasets. Despite growing interest in data curation for\ntraining clinical AI assistants, existing datasets largely remain scattered,\nunder-documented, and often inaccessible, hindering the reproducibility,\ncomparability, and generalizability of AI models developed for clinical mental\nhealth care. In this paper, we present the first comprehensive survey of\nclinical mental health datasets relevant to the training and development of\nAI-powered clinical assistants. We categorize these datasets by mental\ndisorders (e.g., depression, schizophrenia), data modalities (e.g., text,\nspeech, physiological signals), task types (e.g., diagnosis prediction, symptom\nseverity estimation, intervention generation), accessibility (public,\nrestricted or private), and sociocultural context (e.g., language and cultural\nbackground). Along with these, we also investigate synthetic clinical mental\nhealth datasets. Our survey identifies critical gaps such as a lack of\nlongitudinal data, limited cultural and linguistic representation, inconsistent\ncollection and annotation standards, and a lack of modalities in synthetic\ndata. We conclude by outlining key challenges in curating and standardizing\nfuture datasets and provide actionable recommendations to facilitate the\ndevelopment of more robust, generalizable, and equitable mental health AI\nsystems."}
{"id": "2508.09834", "pdf": "https://arxiv.org/pdf/2508.09834.pdf", "abs": "https://arxiv.org/abs/2508.09834", "title": "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models", "authors": ["Weigao Sun", "Jiaxi Hu", "Yucheng Zhou", "Jusen Du", "Disen Lan", "Kexin Wang", "Tong Zhu", "Xiaoye Qu", "Yu Zhang", "Xiaoyu Mo", "Daizong Liu", "Yuxuan Liang", "Wenliang Chen", "Guoqi Li", "Yu Cheng"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Survey, 82 pages, GitHub:\n  https://github.com/weigao266/Awesome-Efficient-Arch", "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems."}
{"id": "2508.09848", "pdf": "https://arxiv.org/pdf/2508.09848.pdf", "abs": "https://arxiv.org/abs/2508.09848", "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts", "authors": ["Mo Yu", "Tsz Ting Chung", "Chulun Zhou", "Tong Li", "Rui Lu", "Jiangnan Li", "Liyan Xu", "Haoshu Lu", "Ning Zhang", "Jing Li", "Jie Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "First 7 authors contributed equally. Project page:\n  https://gorov.github.io/prelude", "summary": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning."}
{"id": "2508.09865", "pdf": "https://arxiv.org/pdf/2508.09865.pdf", "abs": "https://arxiv.org/abs/2508.09865", "title": "Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription", "authors": ["Abdul Rehman Antall", "Naveed Akhtar"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 1 table, including references and appendix", "summary": "This study evaluates the feasibility of lightweight Whisper models (Tiny,\nBase, Small) for Urdu speech recognition in low-resource settings. Despite Urdu\nbeing the 10th most spoken language globally with over 230 million speakers,\nits representation in automatic speech recognition (ASR) systems remains\nlimited due to dialectal diversity, code-switching, and sparse training data.\nWe benchmark these models on a curated Urdu dataset using word error rate\n(WER), without fine-tuning. Results show Whisper-Small achieves the lowest\nerror rates (33.68\\% WER), outperforming Tiny (67.08\\% WER) and Base (53.67\\%\nWER). Qualitative analysis reveals persistent challenges in phonetic accuracy\nand lexical coherence, particularly for complex utterances. While Whisper-Small\ndemonstrates promise for deployable Urdu ASR, significant gaps remain. Our\nfindings emphasize lay the groundwork for future research into effective,\nlow-resource ASR systems."}
{"id": "2508.09874", "pdf": "https://arxiv.org/pdf/2508.09874.pdf", "abs": "https://arxiv.org/abs/2508.09874", "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models", "authors": ["Jiaqi Cao", "Jiarui Wang", "Rubin Wei", "Qipeng Guo", "Kai Chen", "Bowen Zhou", "Zhouhan Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong abilities in general language\ntasks, yet adapting them to specific domains remains a challenge. Current\nmethod like Domain Adaptive Pretraining (DAPT) requires costly full-parameter\ntraining and suffers from catastrophic forgetting. Meanwhile,\nRetrieval-Augmented Generation (RAG) introduces substantial inference latency\ndue to expensive nearest-neighbor searches and longer context. This paper\nintroduces Memory Decoder, a plug-and-play pretrained memory that enables\nefficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the\nbehavior of an external non-parametric retriever. Once trained, Memory Decoder\ncan be seamlessly integrated with any pretrained language model that shares the\nsame tokenizer, requiring no model-specific modifications. Experimental results\ndemonstrate that Memory Decoder enables effective adaptation of various Qwen\nand Llama models to three distinct specialized domains: biomedicine, finance,\nand law, reducing perplexity by an average of 6.17 points. Overall, Memory\nDecoder introduces a novel paradigm centered on a specially pretrained memory\ncomponent designed for domain-specific adaptation. This memory architecture can\nbe integrated in a plug-and-play manner, consistently enhancing performance\nacross multiple models within the target domain."}
{"id": "2508.09878", "pdf": "https://arxiv.org/pdf/2508.09878.pdf", "abs": "https://arxiv.org/abs/2508.09878", "title": "A Survey of Cognitive Distortion Detection and Classification in NLP", "authors": ["Archie Sage", "Jeroen Keppens", "Helen Yannakoudakis"], "categories": ["cs.CL"], "comment": "Under review via ACL Rolling Review and committed to EMNLP 2025.\n  Camera-ready updates to follow", "summary": "As interest grows in the application of natural language processing (NLP)\ntechniques to mental health, a growing body of work explores the automatic\ndetection and classification of cognitive distortions (CDs). CDs are habitual\npatterns of negatively biased or flawed thinking that distort how people\nperceive events, judge themselves, and react to the world around them.\nIdentifying and addressing them is an important part of therapy. Despite its\nmomentum, the field remains fragmented, with inconsistencies in CD taxonomies,\ntask formulations, and evaluation practices. This survey reviews 38 studies\nspanning two decades, providing a structured overview of datasets, modelling\napproaches, and evaluation strategies. We provide a consolidated CD taxonomy\nreference, summarise common task setups, and highlight open challenges to\nsupport more coherent and reproducible research in this emerging area."}
{"id": "2508.09935", "pdf": "https://arxiv.org/pdf/2508.09935.pdf", "abs": "https://arxiv.org/abs/2508.09935", "title": "Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach", "authors": ["Sayem Hossen", "Monalisa Moon Joti", "Md. Golam Rashed"], "categories": ["cs.CL", "q-fin.CP", "q-fin.GN"], "comment": "21", "summary": "Business communication digitisation has reorganised the process of persuasive\ndiscourse, which\n  allows not only greater transparency but also advanced deception. This\ninquiry synthesises classical\n  rhetoric and communication psychology with linguistic theory and empirical\nstudies in the financial\n  reporting, sustainability discourse, and digital marketing to explain how\ndeceptive language can be\n  systematically detected using persuasive lexicon. In controlled settings,\ndetection accuracies of greater\n  than 99% were achieved by using computational textual analysis as well as\npersonalised transformer\n  models. However, reproducing this performance in multilingual settings is\nalso problematic and,\n  to a large extent, this is because it is not easy to find sufficient data,\nand because few multilingual\n  text-processing infrastructures are in place. This evidence shows that there\nhas been an increasing\n  gap between the theoretical representations of communication and those\nempirically approximated,\n  and therefore, there is a need to have strong automatic text-identification\nsystems where AI-based\n  discourse is becoming more realistic in communicating with humans."}
{"id": "2508.09937", "pdf": "https://arxiv.org/pdf/2508.09937.pdf", "abs": "https://arxiv.org/abs/2508.09937", "title": "A Comprehensive Evaluation framework of Alignment Techniques for LLMs", "authors": ["Muneeza Azmat", "Momin Abbas", "Maysa Malfiza Garcia de Macedo", "Marcelo Carpinette Grave", "Luan Soares de Souza", "Tiago Machado", "Rogerio A de Paula", "Raya Horesh", "Yixin Chen", "Heloisa Caroline de Souza Pereira Candello", "Rebecka Nordenlow", "Aminat Adebiyi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In submission", "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world applications, ensuring their outputs align with human values and\nsafety standards has become critical. The field has developed diverse alignment\napproaches including traditional fine-tuning methods (RLHF, instruction\ntuning), post-hoc correction systems, and inference-time interventions, each\nwith distinct advantages and limitations. However, the lack of unified\nevaluation frameworks makes it difficult to systematically compare these\nparadigms and guide deployment decisions. This paper introduces a\nmulti-dimensional evaluation of alignment techniques for LLMs, a comprehensive\nevaluation framework that provides a systematic comparison across all major\nalignment paradigms. Our framework assesses methods along four key dimensions:\nalignment detection, alignment quality, computational efficiency, and\nrobustness. Through experiments across diverse base models and alignment\nstrategies, we demonstrate the utility of our framework in identifying\nstrengths and limitations of current state-of-the-art models, providing\nvaluable insights for future research directions."}
{"id": "2508.09945", "pdf": "https://arxiv.org/pdf/2508.09945.pdf", "abs": "https://arxiv.org/abs/2508.09945", "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models", "authors": ["Lingjie Jiang", "Shaohan Huang", "Xun Wu", "Yixia Li", "Dongdong Zhang", "Furu Wei"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets."}
{"id": "2508.09952", "pdf": "https://arxiv.org/pdf/2508.09952.pdf", "abs": "https://arxiv.org/abs/2508.09952", "title": "Specialised or Generic? Tokenization Choices for Radiology Language Models", "authors": ["Hermione Warr", "Wentian Xu", "Harry Anthony", "Yasin Ibrahim", "Daniel McGowan", "Konstantinos Kamnitsas"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ELAMI@MICCAI2025", "summary": "The vocabulary used by language models (LM) - defined by the tokenizer -\nplays a key role in text generation quality. However, its impact remains\nunder-explored in radiology. In this work, we address this gap by\nsystematically comparing general, medical, and domain-specific tokenizers on\nthe task of radiology report summarisation across three imaging modalities. We\nalso investigate scenarios with and without LM pre-training on PubMed\nabstracts. Our findings demonstrate that medical and domain-specific\nvocabularies outperformed widely used natural language alternatives when models\nare trained from scratch. Pre-training partially mitigates performance\ndifferences between tokenizers, whilst the domain-specific tokenizers achieve\nthe most favourable results. Domain-specific tokenizers also reduce memory\nrequirements due to smaller vocabularies and shorter sequences. These results\ndemonstrate that adapting the vocabulary of LMs to the clinical domain provides\npractical benefits, including improved performance and reduced computational\ndemands, making such models more accessible and effective for both research and\nreal-world healthcare settings."}
{"id": "2508.09954", "pdf": "https://arxiv.org/pdf/2508.09954.pdf", "abs": "https://arxiv.org/abs/2508.09954", "title": "Shaping Event Backstories to Estimate Potential Emotion Contexts", "authors": ["Johannes SchÃ¤fer", "Roman Klinger"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "May 2025 version", "summary": "Emotion analysis is an inherently ambiguous task. Previous work studied\nannotator properties to explain disagreement, but this overlooks the\npossibility that ambiguity may stem from missing information about the context\nof events. In this paper, we propose a novel approach that adds reasonable\ncontexts to event descriptions, which may better explain a particular\nsituation. Our goal is to understand whether these enriched contexts enable\nhuman annotators to annotate emotions more reliably. We disambiguate a target\nevent description by automatically generating multiple event chains conditioned\non differing emotions. By combining techniques from short story generation in\nvarious settings, we achieve coherent narratives that result in a specialized\ndataset for the first comprehensive and systematic examination of\ncontextualized emotion analysis. Through automatic and human evaluation, we\nfind that contextual narratives enhance the interpretation of specific emotions\nand support annotators in producing more consistent annotations."}
{"id": "2508.09956", "pdf": "https://arxiv.org/pdf/2508.09956.pdf", "abs": "https://arxiv.org/abs/2508.09956", "title": "Performance of GPT-5 Frontier Models in Ophthalmology Question Answering", "authors": ["Fares Antaki", "David Mikhail", "Daniel Milad", "Danny A Mammo", "Sumit Sharma", "Sunil K Srivastava", "Bing Yu Chen", "Samir Touma", "Mertcan Sevgi", "Jonathan El-Khoury", "Pearse A Keane", "Qingyu Chen", "Yih Chung Tham", "Renaud Duval"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) such as GPT-5 integrate advanced reasoning\ncapabilities that may improve performance on complex medical question-answering\ntasks. For this latest generation of reasoning models, the configurations that\nmaximize both accuracy and cost-efficiency have yet to be established. We\nevaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across\nfour reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using\n260 closed-access multiple-choice questions from the American Academy of\nOphthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome\nwas multiple-choice accuracy; secondary outcomes included head-to-head ranking\nvia a Bradley-Terry model, rationale quality assessment using a\nreference-anchored, pairwise LLM-as-a-judge framework, and analysis of\naccuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved\nthe highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano\nvariants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high\n(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x\nstronger than o3-high) and rationale quality (1.11x stronger than o3-high).\nCost-accuracy analysis identified several GPT-5 configurations on the Pareto\nfrontier, with GPT-5-mini-low offering the most favorable low-cost,\nhigh-performance balance. These results benchmark GPT-5 on a high-quality\nophthalmology dataset, demonstrate the influence of reasoning effort on\naccuracy, and introduce an autograder framework for scalable evaluation of\nLLM-generated answers against reference standards in ophthalmology."}
{"id": "2508.09957", "pdf": "https://arxiv.org/pdf/2508.09957.pdf", "abs": "https://arxiv.org/abs/2508.09957", "title": "Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)", "authors": ["Renas Adnan", "Hossein Hassani"], "categories": ["cs.CL"], "comment": "21 pages, 20 figures, 7 tables", "summary": "Speech-to-text (STT) systems have a wide range of applications. They are\navailable in many languages, albeit at different quality levels. Although\nKurdish is considered a less-resourced language from a processing perspective,\nSST is available for some of the Kurdish dialects, for instance, Sorani\n(Central Kurdish). However, that is not applied to other Kurdish dialects,\nBadini and Hawrami, for example. This research is an attempt to address this\ngap. Bandin, approximately, has two million speakers, and STT systems can help\ntheir community use mobile and computer-based technologies while giving their\ndialect more global visibility. We aim to create a language model based on\nBadini's speech and evaluate its performance. To cover a conversational aspect,\nhave a proper confidence level of grammatical accuracy, and ready\ntranscriptions, we chose Badini kids' stories, eight books including 78\nstories, as the textual input. Six narrators narrated the books, which resulted\nin approximately 17 hours of recording. We cleaned, segmented, and tokenized\nthe input. The preprocessing produced nearly 15 hours of speech, including\n19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and\nWhisper-small to develop the language models. The experiments indicate that the\ntranscriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a\nsignificantly more accurate and readable output than the Whisper-small model,\nwith 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,\nrespectively."}
{"id": "2508.09958", "pdf": "https://arxiv.org/pdf/2508.09958.pdf", "abs": "https://arxiv.org/abs/2508.09958", "title": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks", "authors": ["Baran Atalar", "Eddie Zhang", "Carlee Joe-Wong"], "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to AAAI 2026", "summary": "With the increasing popularity of large language models (LLMs) for a variety\nof tasks, there has been a growing interest in strategies that can predict\nwhich out of a set of LLMs will yield a successful answer at low cost. This\nproblem promises to become more and more relevant as providers like Microsoft\nallow users to easily create custom LLM \"assistants\" specialized to particular\ntypes of queries. However, some tasks (i.e., queries) may be too specialized\nand difficult for a single LLM to handle alone. These applications often\nbenefit from breaking down the task into smaller subtasks, each of which can\nthen be executed by a LLM expected to perform well on that specific subtask.\nFor example, in extracting a diagnosis from medical records, one can first\nselect an LLM to summarize the record, select another to validate the summary,\nand then select another, possibly different, LLM to extract the diagnosis from\nthe summarized record. Unlike existing LLM selection or routing algorithms,\nthis setting requires that we select a sequence of LLMs, with the output of\neach LLM feeding into the next and potentially influencing its success. Thus,\nunlike single LLM selection, the quality of each subtask's output directly\naffects the inputs, and hence the cost and success rate, of downstream LLMs,\ncreating complex performance dependencies that must be learned and accounted\nfor during selection. We propose a neural contextual bandit-based algorithm\nthat trains neural networks that model LLM success on each subtask in an online\nmanner, thus learning to guide the LLM selections for the different subtasks,\neven in the absence of historical LLM performance data. Experiments on\ntelecommunications question answering and medical diagnosis prediction datasets\nillustrate the effectiveness of our proposed approach compared to other LLM\nselection algorithms."}
{"id": "2508.09145", "pdf": "https://arxiv.org/pdf/2508.09145.pdf", "abs": "https://arxiv.org/abs/2508.09145", "title": "MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis", "authors": ["Xingle Xu", "Yongkang Liu", "Dexian Cai", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal Sentiment Analysis aims to integrate information from various\nmodalities, such as audio, visual, and text, to make complementary predictions.\nHowever, it often struggles with irrelevant or misleading visual and auditory\ninformation. Most existing approaches typically treat the entire modality\ninformation (e.g., a whole image, audio segment, or text paragraph) as an\nindependent unit for feature enhancement or denoising. They often suppress the\nredundant and noise information at the risk of losing critical information. To\naddress this challenge, we propose MoLAN, a unified ModaLity-aware noise\ndynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking\nby dividing the features of each modality into multiple blocks. Each block is\nthen dynamically assigned a distinct denoising strength based on its noise\nlevel and semantic relevance, enabling fine-grained noise suppression while\npreserving essential multimodal information. Notably, MoLAN is a unified and\nflexible framework that can be seamlessly integrated into a wide range of\nmultimodal models. Building upon this framework, we further introduce MoLAN+, a\nnew multimodal sentiment analysis approach. Experiments across five models and\nfour datasets demonstrate the broad effectiveness of the MoLAN framework.\nExtensive evaluations show that MoLAN+ achieves the state-of-the-art\nperformance. The code is publicly available at\nhttps://github.com/betterfly123/MoLAN-Framework."}
{"id": "2508.09199", "pdf": "https://arxiv.org/pdf/2508.09199.pdf", "abs": "https://arxiv.org/abs/2508.09199", "title": "$Î$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation", "authors": ["Jucheng Hu", "Suorong Yang", "Dongzhan Zhou"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Visual Instruction Finetuning (VIF) is pivotal for post-training\nVision-Language Models (VLMs). Unlike unimodal instruction finetuning in\nplain-text large language models, which mainly requires instruction datasets to\nenable model instruction-following ability, VIF also requires multimodal data\nto enable joint visual and textual understanding; therefore, it typically\nrequires more data. Consequently, VIF imposes stricter data selection\nchallenges: the method must scale efficiently to handle larger data demands\nwhile ensuring the quality of both visual and textual content, as well as their\nalignment. Despite its critical impact on performance, data selection for VIF\nremains an understudied area. In this paper, we propose $\\Delta$-AttnMask. This\ndata-efficient framework quantifies sample quality through attention-guided\nmasking of the model's hidden states, jointly evaluating image-text pairs\nwithout requiring domain labels, auxiliary models, or extra training. By\ncomputing loss differences ($\\Delta$) between the original states and states\nmasked using high-attention regions, $\\Delta$-AttnMask intrinsically assesses\nsample quality. Experiments across multiple VLMs and datasets show that\n$\\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,\naccelerating training by 5x while surpassing full-dataset baselines by +10.1%\nin overall accuracy. Its model-agnostic and data-agnostic design ensures broad\napplicability across modalities and architectures."}
{"id": "2508.09224", "pdf": "https://arxiv.org/pdf/2508.09224.pdf", "abs": "https://arxiv.org/abs/2508.09224", "title": "From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training", "authors": ["Yuan Yuan", "Tina Sriskandarajah", "Anna-Luisa Brakman", "Alec Helyar", "Alex Beutel", "Andrea Vallone", "Saachi Jain"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models used in ChatGPT have traditionally been trained to\nlearn a refusal boundary: depending on the user's intent, the model is taught\nto either fully comply or outright refuse. While this is a strong mitigation\nfor explicitly malicious prompts, focusing safety training on refusals can lead\nto brittleness for prompts with obscured user intent. Binary refusal boundaries\nare especially ill-suited for dual-use cases (such as biology or\ncybersecurity), where a user request can be answered safely at a high level,\nbut in some cases can lead to malicious uplift if sufficiently detailed or\nactionable. As an alternative, we propose safe-completions: a safety-training\napproach that centers on the safety of the assistant's output, rather than a\nbinary classification of the user's intent. Safe-completions seek to maximize\nhelpfulness within the safety policy's constraints. We incorporated this\napproach into GPT-5 and find that across both production comparisons and\ninternally controlled experiments, safe-completion training improves safety\n(especially on dual-use prompts), reduces the severity of residual safety\nfailures, and substantially increases model helpfulness."}
{"id": "2508.09240", "pdf": "https://arxiv.org/pdf/2508.09240.pdf", "abs": "https://arxiv.org/abs/2508.09240", "title": "NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation", "authors": ["Zainab Khan", "Ahmed Hussain", "Mukesh Thakur", "Arto Hellas", "Panos Papadimitratos"], "categories": ["cs.NI", "cs.AI", "cs.CL"], "comment": "6 pages", "summary": "The use of Service-Based Architecture in modern telecommunications has\nexponentially increased Network Functions (NFs) and Application Programming\nInterfaces (APIs), creating substantial operational complexities in service\ndiscovery and management. We introduce \\textit{NEFMind}, a framework leveraging\nparameter-efficient fine-tuning of open-source Large Language Models (LLMs) to\naddress these challenges. It integrates three core components: synthetic\ndataset generation from Network Exposure Function (NEF) API specifications,\nmodel optimization through Quantized-Low-Rank Adaptation, and performance\nevaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G\nService-Based Architecture APIs, our approach achieves 85% reduction in\ncommunication overhead compared to manual discovery methods. Experimental\nvalidation using the open-source Phi-2 model demonstrates exceptional API call\nidentification performance at 98-100% accuracy. The fine-tuned Phi-2 model\ndelivers performance comparable to significantly larger models like GPT-4 while\nmaintaining computational efficiency for telecommunications infrastructure\ndeployment. These findings validate domain-specific, parameter-efficient LLM\nstrategies for managing complex API ecosystems in next-generation\ntelecommunications networks."}
{"id": "2508.09288", "pdf": "https://arxiv.org/pdf/2508.09288.pdf", "abs": "https://arxiv.org/abs/2508.09288", "title": "Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs", "authors": ["Aayush Gupta"], "categories": ["cs.CR", "cs.AI", "cs.CL", "68T07, 94A60", "D.4.6; K.6.5; E.3; I.2.6; I.2.7"], "comment": "2 figures, 3 tables; code and certification harness:\n  https://github.com/ayushgupta4897/Contextual-Integrity-Verification ;\n  Elite-Attack dataset: https://huggingface.co/datasets/zyushg/elite-attack", "summary": "Large language models (LLMs) remain acutely vulnerable to prompt injection\nand related jailbreak attacks; heuristic guardrails (rules, filters, LLM\njudges) are routinely bypassed. We present Contextual Integrity Verification\n(CIV), an inference-time security architecture that attaches cryptographically\nsigned provenance labels to every token and enforces a source-trust lattice\ninside the transformer via a pre-softmax hard attention mask (with optional\nFFN/residual gating). CIV provides deterministic, per-token non-interference\nguarantees on frozen models: lower-trust tokens cannot influence higher-trust\nrepresentations. On benchmarks derived from recent taxonomies of\nprompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack\nsuccess rate under the stated threat model while preserving 93.1% token-level\nsimilarity and showing no degradation in model perplexity on benign tasks; we\nnote a latency overhead attributable to a non-optimized data path. Because CIV\nis a lightweight patch -- no fine-tuning required -- we demonstrate drop-in\nprotection for Llama-3-8B and Mistral-7B. We release a reference\nimplementation, an automated certification harness, and the Elite-Attack corpus\nto support reproducible research."}
{"id": "2508.09294", "pdf": "https://arxiv.org/pdf/2508.09294.pdf", "abs": "https://arxiv.org/abs/2508.09294", "title": "Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative", "authors": ["Xi Xuan", "Zimo Zhu", "Wenxin Zhang", "Yi-Cheng Lin", "Tomi Kinnunen"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted at IEEE ASRU 2025", "summary": "Advances in speech synthesis intensify security threats, motivating real-time\ndeepfake detection research. We investigate whether bidirectional Mamba can\nserve as a competitive alternative to Self-Attention in detecting synthetic\nspeech. Our solution, Fake-Mamba, integrates an XLSR front-end with\nbidirectional Mamba to capture both local and global artifacts. Our core\ninnovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and\nPN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can\neffectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof\n21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and\n5.85% EER, respectively, representing substantial relative gains over SOTA\nmodels XLSR-Conformer and XLSR-Mamba. The framework maintains real-time\ninference across utterance lengths, demonstrating strong generalization and\npractical viability. The code is available at\nhttps://github.com/xuanxixi/Fake-Mamba."}
{"id": "2508.09389", "pdf": "https://arxiv.org/pdf/2508.09389.pdf", "abs": "https://arxiv.org/abs/2508.09389", "title": "ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs", "authors": ["Eray Eren", "Qingju Liu", "Hyeongwoo Kim", "Pablo Garrido", "Abeer Alwan"], "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Interspeech 2025; demo page at\n  https://promode8272.github.io/promode/index.html", "summary": "Prosody conveys rich emotional and semantic information of the speech signal\nas well as individual idiosyncrasies. We propose a stand-alone model that maps\ntext-to-prosodic features such as F0 and energy and can be used in downstream\ntasks such as TTS. The ProMode encoder takes as input acoustic features and\ntime-aligned textual content, both are partially masked, and obtains a\nfixed-length latent prosodic embedding. The decoder predicts acoustics in the\nmasked region using both the encoded prosody input and unmasked textual\ncontent. Trained on the GigaSpeech dataset, we compare our method with\nstate-of-the-art style encoders. For F0 and energy predictions, we show\nconsistent improvements for our model at different levels of granularity. We\nalso integrate these predicted prosodic features into a TTS system and conduct\nperceptual tests, which show higher prosody preference compared to the\nbaselines, demonstrating the model's potential in tasks where prosody modeling\nis important."}
{"id": "2508.09442", "pdf": "https://arxiv.org/pdf/2508.09442.pdf", "abs": "https://arxiv.org/abs/2508.09442", "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference", "authors": ["Zhifan Luo", "Shuo Shao", "Su Zhang", "Lijing Zhou", "Yuke Hu", "Chenxu Zhao", "Zhihao Liu", "Zhan Qin"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment."}
{"id": "2508.09456", "pdf": "https://arxiv.org/pdf/2508.09456.pdf", "abs": "https://arxiv.org/abs/2508.09456", "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding", "authors": ["Junxian Li", "Beining Xu", "Di Zhang"], "categories": ["cs.CV", "cs.CL", "cs.CR"], "comment": "13 pages, 13 Figures", "summary": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack."}
{"id": "2508.09473", "pdf": "https://arxiv.org/pdf/2508.09473.pdf", "abs": "https://arxiv.org/abs/2508.09473", "title": "NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs", "authors": ["Birong Pan", "Mayi Xu", "Qiankun Pi", "Jianhao Chen", "Yuanyuan Zhu", "Ming Zhong", "Tieyun Qian"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Ensuring robust safety alignment while preserving utility is critical for the\nreliable deployment of Large Language Models (LLMs). However, current\ntechniques fundamentally suffer from intertwined deficiencies: insufficient\nrobustness against malicious attacks, frequent refusal of benign queries,\ndegradation in generated text quality and general task performance--the former\ntwo reflecting deficits in robust safety and the latter constituting utility\nimpairment. We trace these limitations to the coarse-grained layer-wise\ninterventions in existing methods. To resolve this, we propose NeuronTune, a\nfine-grained framework that dynamically modulates sparse neurons to achieve\nsimultaneous safety-utility optimization. Our approach first identifies\nsafety-critical and utility-preserving neurons across all layers via\nattribution, then employs meta-learning to adaptively amplify safety-neuron\nactivations and suppress utility-neuron activations. Crucially, NeuronTune\nenables tunable adjustment of intervention scope via neuron-count thresholds,\nsupporting flexible adaptation to security-critical or utility-priority\nscenarios. Extensive experimental results demonstrate that our method\nsignificantly outperforms existing state-of-the-art technologies, achieving\nsuperior model safety while maintaining excellent utility."}
{"id": "2508.09535", "pdf": "https://arxiv.org/pdf/2508.09535.pdf", "abs": "https://arxiv.org/abs/2508.09535", "title": "AI Blob! LLM-Driven Recontextualization of Italian Television Archives", "authors": ["Roberto Balestri"], "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.DL"], "comment": "Preprint", "summary": "This paper introduces AI Blob!, an experimental system designed to explore\nthe potential of semantic cataloging and Large Language Models (LLMs) for the\nretrieval and recontextualization of archival television footage. Drawing\nmethodological inspiration from Italian television programs such as Blob (RAI\nTre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic\nembeddings, and retrieval-augmented generation (RAG) to organize and\nreinterpret archival content. The system processes a curated dataset of 1,547\nItalian television videos by transcribing audio, segmenting it into\nsentence-level units, and embedding these segments into a vector database for\nsemantic querying. Upon user input of a thematic prompt, the LLM generates a\nrange of linguistically and conceptually related queries, guiding the retrieval\nand recombination of audiovisual fragments. These fragments are algorithmically\nselected and structured into narrative sequences producing montages that\nemulate editorial practices of ironic juxtaposition and thematic coherence. By\nforegrounding dynamic, content-aware retrieval over static metadata schemas, AI\nBlob! demonstrates how semantic technologies can facilitate new approaches to\narchival engagement, enabling novel forms of automated narrative construction\nand cultural analysis. The project contributes to ongoing debates in media\nhistoriography and AI-driven archival research, offering both a conceptual\nframework and a publicly available dataset to support further interdisciplinary\nexperimentation."}
{"id": "2508.09614", "pdf": "https://arxiv.org/pdf/2508.09614.pdf", "abs": "https://arxiv.org/abs/2508.09614", "title": "How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments", "authors": ["Daniel Raffini", "Agnese Macori", "Lorenzo Porcaro", "Tiziana Catarci", "Marco Angelini"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "9-pages", "summary": "This study examines the rhetorical and linguistic features of argumentative\ntexts generated by ChatGPT on ethically nuanced topics and investigates their\npersuasive impact on human readers.Through a user study involving 62\nparticipants and pre-post interaction surveys, the paper analyzes how exposure\nto AI-generated arguments affects opinion change and user perception. A\nlinguistic and rhetorical analysis of the generated texts reveals a consistent\nargumentative macrostructure, reliance on formulaic expressions, and limited\nstylistic richness. While ChatGPT demonstrates proficiency in constructing\ncoherent argumentative texts, its persuasive efficacy appears constrained,\nparticularly on topics involving ethical issues.The study finds that while\nparticipants often acknowledge the benefits highlighted by ChatGPT, ethical\nconcerns tend to persist or even intensify post-interaction. The results also\ndemonstrate a variation depending on the topic. These findings highlight new\ninsights on AI-generated persuasion in ethically sensitive domains and are a\nbasis for future research."}
{"id": "2508.09651", "pdf": "https://arxiv.org/pdf/2508.09651.pdf", "abs": "https://arxiv.org/abs/2508.09651", "title": "A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories", "authors": ["Daniel Raffini", "Agnese Macori", "Marco Angelini", "Tiziana Catarci"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "8-pages", "summary": "The paper explores the study of gender-based narrative biases in stories\ngenerated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's\ncharacter classifications and Freytag's narrative structure. The stories are\nanalyzed through a close reading approach, with particular attention to\nadherence to the prompt, gender distribution of characters, physical and\npsychological descriptions, actions, and finally, plot development and\ncharacter relationships. The results reveal the persistence of biases -\nespecially implicit ones - in the generated stories and highlight the\nimportance of assessing biases at multiple levels using an interpretative\napproach."}
{"id": "2508.09886", "pdf": "https://arxiv.org/pdf/2508.09886.pdf", "abs": "https://arxiv.org/abs/2508.09886", "title": "COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets", "authors": ["Lingyu Chen", "Yawen Zeng", "Yue Wang", "Peng Wan", "Guo-chen Ning", "Hongen Liao", "Daoqiang Zhang", "Fang Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICCV 2025", "summary": "Conventional single-dataset training often fails with new data distributions,\nespecially in ultrasound (US) image analysis due to limited data, acoustic\nshadows, and speckle noise. Therefore, constructing a universal framework for\nmulti-heterogeneous US datasets is imperative. However, a key challenge arises:\nhow to effectively mitigate inter-dataset interference while preserving\ndataset-specific discriminative features for robust downstream task? Previous\napproaches utilize either a single source-specific decoder or a domain\nadaptation strategy, but these methods experienced a decline in performance\nwhen applied to other domains. Considering this, we propose a Universal\nCollaborative Mixture of Heterogeneous Source-Specific Experts (COME).\nSpecifically, COME establishes dual structure-semantic shared experts that\ncreate a universal representation space and then collaborate with\nsource-specific experts to extract discriminative features through providing\ncomplementary features. This design enables robust generalization by leveraging\ncross-datasets experience distributions and providing universal US priors for\nsmall-batch or unseen data scenarios. Extensive experiments under three\nevaluation modes (single-dataset, intra-organ, and inter-organ integration\ndatasets) demonstrate COME's superiority, achieving significant mean AP\nimprovements over state-of-the-art methods. Our project is available at:\nhttps://universalcome.github.io/UniversalCOME/."}
{"id": "2508.09987", "pdf": "https://arxiv.org/pdf/2508.09987.pdf", "abs": "https://arxiv.org/abs/2508.09987", "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "authors": ["Junyan Ye", "Dongzhi Jiang", "Zihao Wang", "Leqi Zhu", "Zhenghao Hu", "Zilong Huang", "Jun He", "Zhiyuan Yan", "Jinghua Yu", "Hongsheng Li", "Conghui He", "Weijia Li"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "19 pages, 8 figures", "summary": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability."}
{"id": "2305.01710", "pdf": "https://arxiv.org/pdf/2305.01710.pdf", "abs": "https://arxiv.org/abs/2305.01710", "title": "From Stars to Insights: Exploration and Implementation of Unified Sentiment Analysis with Distant Supervision", "authors": ["Wenchang Li", "John P. Lalor", "Yixing Chen", "Vamsi K. Kanuri"], "categories": ["cs.CL"], "comment": "Forthcoming in ACM Trans. Manage. Inf. Syst", "summary": "Sentiment analysis is integral to understanding the voice of the customer and\ninforming businesses' strategic decisions. Conventional sentiment analysis\ninvolves three separate tasks: aspect-category detection, aspect-category\nsentiment analysis, and rating prediction. However, independently tackling\nthese tasks can overlook their interdependencies and often requires expensive,\nfine-grained annotations. This paper introduces unified sentiment analysis, a\nnovel learning paradigm that integrates the three aforementioned tasks into a\ncoherent framework. To achieve this, we propose the Distantly Supervised\nPyramid Network (DSPN), which employs a pyramid structure to capture sentiment\nat word, aspect, and document levels in a hierarchical manner. Evaluations on\nmulti-aspect review datasets in English and Chinese show that DSPN, using only\nstar rating labels for supervision, demonstrates significant efficiency\nadvantages while performing comparably well to a variety of benchmark models.\nAdditionally, DSPN's pyramid structure enables the interpretability of its\noutputs. Our findings validate DSPN's effectiveness and efficiency,\nestablishing a robust, resource-efficient, unified framework for sentiment\nanalysis."}
{"id": "2405.20179", "pdf": "https://arxiv.org/pdf/2405.20179.pdf", "abs": "https://arxiv.org/abs/2405.20179", "title": "Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning Code LLMs", "authors": ["Zichao Hu", "Junyi Jessy Li", "Arjun Guha", "Joydeep Biswas"], "categories": ["cs.CL", "cs.AI", "cs.RO"], "comment": null, "summary": "Code LLMs have shown promising results with converting tasks in natural\nlanguage to programs that can be executed by service robots. We are interested\nin finetuning small, specialized LLMs for this purpose, but collecting datasets\nof task-program pairs specific to each robot is time-consuming and expensive.\nWhile approaches such as SELF-INSTRUCT and EVOL-INSTRUCT are capable of\ngenerating novel tasks given a few examples, they are unable to provide the\ncorresponding programs that correctly abide by physical-world and\nrobot-constraints using the provided programming interface. Using a simulator\nis a natural potential solution to checking for such constraints, but building\nsimulation environments that can handle arbitrary tasks and their necessary\nobjects and locations, is challenging. To address these challenges, we\nintroduce ROBO-INSTRUCT, which synthesizes task-specific simulation\nenvironments on the fly during program execution, by opportunistically\ninferring entity properties and enforcing corresponding constraints based on\nhow the entities are used in the task program. Additionally, ROBO-INSTRUCT\nintegrates an LLM-aided post-processing procedure to refine instructions for\nbetter alignment with robot programs. We demonstrate the effectiveness of\nROBO-INSTRUCT across multiple LLMs, showing that our fine-tuned models\noutperform all baseline methods and even match or surpass the performance of\nseveral larger and proprietary models."}
{"id": "2406.17588", "pdf": "https://arxiv.org/pdf/2406.17588.pdf", "abs": "https://arxiv.org/abs/2406.17588", "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs", "authors": ["Shawn Gavin", "Tuney Zheng", "Jiaheng Liu", "Quehry Que", "Noah Wang", "Jian Yang", "Chenchen Zhang", "Wenhao Huang", "Ge Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The long-context capabilities of large language models (LLMs) have been a hot\ntopic in recent years. To evaluate the performance of LLMs in different\nscenarios, various assessment benchmarks have emerged. However, as most of\nthese benchmarks focus on identifying key information to answer questions,\nwhich mainly requires the retrieval ability of LLMs, these benchmarks can\npartially represent the reasoning performance of LLMs from large amounts of\ninformation. Meanwhile, although LLMs often claim to have context windows of\n32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual\nsupported length of these LLMs. To address these issues, we propose the LongIns\nbenchmark dataset, a challenging long-context instruction-based exam for LLMs,\nwhich is built based on the existing instruction datasets. Specifically, in our\nLongIns, we introduce three evaluation settings: Global Instruction & Single\nTask (GIST), Local Instruction & Single Task (LIST), and Local Instruction &\nMultiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations\non existing LLMs and have the following important findings: (1). The\ntop-performing GPT-4 with 128k context length performs poorly on the evaluation\ncontext window of 16k in our LongIns. (2). For the multi-hop reasoning ability\nof many existing LLMs, significant efforts are still needed under short context\nwindows (less than 4k)."}
{"id": "2410.19925", "pdf": "https://arxiv.org/pdf/2410.19925.pdf", "abs": "https://arxiv.org/abs/2410.19925", "title": "Improving Multimodal Large Language Models Using Continual Learning", "authors": ["Shikhar Srivastava", "Md Yousuf Harun", "Robik Shrestha", "Christopher Kanan"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "CoLLAs 2025 and Scalable Continual Learning for Lifelong Foundation\n  Models, NeurIPS 2024", "summary": "Generative large language models (LLMs) exhibit impressive capabilities,\nwhich can be further augmented by integrating a pre-trained vision model into\nthe original LLM to create a multimodal LLM (MLLM). However, this integration\noften significantly decreases performance on natural language understanding and\ngeneration tasks, compared to the original LLM. This study investigates this\nissue using the LLaVA MLLM, treating the integration as a continual learning\nproblem. We evaluate five continual learning methods to mitigate forgetting and\nidentify a technique that enhances visual understanding while minimizing\nlinguistic performance loss. Our approach reduces linguistic performance\ndegradation by up to 15% over the LLaVA recipe, while maintaining high\nmultimodal accuracy. We also demonstrate the robustness of our method through\ncontinual learning on a sequence of vision-language tasks, effectively\npreserving linguistic skills while acquiring new multimodal capabilities.\nProject webpage: https://shikhar-srivastava.github.io/cl-for-improving-mllms"}
{"id": "2412.10417", "pdf": "https://arxiv.org/pdf/2412.10417.pdf", "abs": "https://arxiv.org/abs/2412.10417", "title": "Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs Performance", "authors": ["Abdelrahman A. Ali", "Aya E. Fouda", "Radwa J. Hanafy", "Mohammed E. Fouda"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Mental health disorders are increasingly prevalent worldwide, creating an\nurgent need for innovative tools to support early diagnosis and intervention.\nThis study explores the potential of Large Language Models (LLMs) in multimodal\nmental health diagnostics, specifically for detecting depression and Post\nTraumatic Stress Disorder through text and audio modalities. Using the E-DAIC\ndataset, we compare text and audio modalities to investigate whether LLMs can\nperform equally well or better with audio inputs. We further examine the\nintegration of both modalities to determine if this can enhance diagnostic\naccuracy, which generally results in improved performance metrics. Our analysis\nspecifically utilizes custom-formulated metrics; Modal Superiority Score and\nDisagreement Resolvement Score to evaluate how combined modalities influence\nmodel performance. The Gemini 1.5 Pro model achieves the highest scores in\nbinary depression classification when using the combined modality, with an F1\nscore of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full\ndataset. These results represent an increase of 3.1% over its performance with\nthe text modality and 2.7% over the audio modality, highlighting the\neffectiveness of integrating modalities to enhance diagnostic accuracy.\nNotably, all results are obtained in zero-shot inferring, highlighting the\nrobustness of the models without requiring task-specific fine-tuning. To\nexplore the impact of different configurations on model performance, we conduct\nbinary, severity, and multiclass tasks using both zero-shot and few-shot\nprompts, examining the effects of prompt variations on performance. The results\nreveal that models such as Gemini 1.5 Pro in text and audio modalities, and\nGPT-4o mini in the text modality, often surpass other models in balanced\naccuracy and F1 scores across multiple tasks."}
{"id": "2412.14368", "pdf": "https://arxiv.org/pdf/2412.14368.pdf", "abs": "https://arxiv.org/abs/2412.14368", "title": "Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation", "authors": ["Yuxuan Jiang", "Francis Ferraro"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding."}
{"id": "2501.04661", "pdf": "https://arxiv.org/pdf/2501.04661.pdf", "abs": "https://arxiv.org/abs/2501.04661", "title": "Beyond Memorization: Assessing Semantic Generalization in Large Language Models Using Phrasal Constructions", "authors": ["Wesley Scivetti", "Melissa Torgbi", "Austin Blodgett", "Mollie Shichman", "Taylor Hudson", "Claire Bonial", "Harish Tayyar Madabushi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The web-scale of pretraining data has created an important evaluation\nchallenge: to disentangle linguistic competence on cases well-represented in\npretraining data from generalization to out-of-domain language, specifically\nthe dynamic, real-world instances less common in pretraining data. To this end,\nwe construct a diagnostic evaluation to systematically assess natural language\nunderstanding in LLMs by leveraging Construction Grammar (CxG). CxG provides a\npsycholinguistically grounded framework for testing generalization, as it\nexplicitly links syntactic forms to abstract, non-lexical meanings. Our novel\ninference evaluation dataset consists of English phrasal constructions, for\nwhich speakers are known to be able to abstract over commonplace instantiations\nin order to understand and produce creative instantiations. Our evaluation\ndataset uses CxG to evaluate two central questions: first, if models can\n'understand' the semantics of sentences for instances that are likely to appear\nin pretraining data less often, but are intuitive and easy for people to\nunderstand. Second, if LLMs can deploy the appropriate constructional semantics\ngiven constructions that are syntactically identical but with divergent\nmeanings. Our results demonstrate that state-of-the-art models, including\nGPT-o1, exhibit a performance drop of over 40% on our second task, revealing a\nfailure to generalize over syntactically identical forms to arrive at distinct\nconstructional meanings in the way humans do. We make our novel dataset and\nassociated experimental data, including prompts and model responses, publicly\navailable."}
{"id": "2501.11790", "pdf": "https://arxiv.org/pdf/2501.11790.pdf", "abs": "https://arxiv.org/abs/2501.11790", "title": "Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables Questions", "authors": ["Zijin Hong", "Hao Wu", "Su Dong", "Junnan Dong", "Yilin Xiao", "Yujing Zhang", "Zhu Wang", "Feiran Huang", "Linyi Li", "Hongxia Yang", "Xiao Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies have raised significant concerns regarding the reliability of\ncurrent mathematics benchmarks, highlighting issues such as simplistic design\nand potential data contamination. Consequently, developing a reliable benchmark\nthat effectively evaluates large language models' (LLMs) genuine capabilities\nin mathematical reasoning remains a critical challenge. To address these\nconcerns, we propose RV-Bench, a novel evaluation methodology for Benchmarking\nLLMs with Random Variables in mathematical reasoning. Specifically, we build\nquestion-generating functions to produce random variable questions (RVQs),\nwhose background content mirrors original benchmark problems, but with\nrandomized variable combinations, rendering them \"unseen\" to LLMs. Models must\ncompletely understand the inherent question pattern to correctly answer RVQs\nwith diverse variable combinations. Thus, an LLM's genuine reasoning capability\nis reflected through its accuracy and robustness on RV-Bench. We conducted\nextensive experiments on over 30 representative LLMs across more than 1,000\nRVQs. Our findings propose that LLMs exhibit a proficiency imbalance between\nencountered and ``unseen'' data distributions. Furthermore, RV-Bench reveals\nthat proficiency generalization across similar mathematical reasoning tasks is\nlimited, but we verified it can still be effectively elicited through test-time\nscaling."}
{"id": "2502.14051", "pdf": "https://arxiv.org/pdf/2502.14051.pdf", "abs": "https://arxiv.org/abs/2502.14051", "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression", "authors": ["Payman Behnam", "Yaosheng Fu", "Ritchie Zhao", "Po-An Tsai", "Zhiding Yu", "Alexey Tumanov"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV."}
{"id": "2502.14910", "pdf": "https://arxiv.org/pdf/2502.14910.pdf", "abs": "https://arxiv.org/abs/2502.14910", "title": "EvoP: Robust LLM Inference via Evolutionary Pruning", "authors": ["Shangyu Wu", "Hongchao Du", "Ying Xiong", "Shuai Chen", "Tei-Wei Kuo", "Nan Guan", "Chun Jason Xue"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, but their massive size and computational demands\nhinder their deployment in resource-constrained environments. Existing model\npruning methods address this issue by removing redundant structures (e.g.,\nelements, channels, layers) from the model. However, these methods employ a\nheuristic pruning strategy, which leads to suboptimal performance. Besides,\nthey also ignore the data characteristics when pruning the model.\n  To overcome these limitations, we propose EvoP, an evolutionary pruning\nframework for robust LLM inference. EvoP first presents a cluster-based\ncalibration dataset sampling (CCDS) strategy for creating a more diverse\ncalibration dataset. EvoP then introduces an evolutionary pruning pattern\nsearching (EPPS) method to find the optimal pruning pattern. Compared to\nexisting model pruning techniques, EvoP achieves the best performance while\nmaintaining the best efficiency. Experiments across different LLMs and\ndifferent downstream tasks validate the effectiveness of the proposed EvoP,\nmaking it a practical and scalable solution for deploying LLMs in real-world\napplications."}
{"id": "2503.23077", "pdf": "https://arxiv.org/pdf/2503.23077.pdf", "abs": "https://arxiv.org/abs/2503.23077", "title": "Efficient Inference for Large Reasoning Models: A Survey", "authors": ["Yue Liu", "Jiaying Wu", "Yufei He", "Ruihan Gong", "Jun Xia", "Liang Li", "Hongcheng Gao", "Hongyu Chen", "Baolong Bi", "Jiaheng Zhang", "Zhiqi Huang", "Bryan Hooi", "Stan Z. Li", "Keqin Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in solving complex tasks. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. The overview structure of\nthis paper is shown in Figure~\\ref{fig:paper_structure}. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from reasoning scenarios, object functions, and performance \\&\nefficiency aspects. Besides, we present open challenges in this field,\nincluding human-centric controllable reasoning, trade-off between\ninterpretability and efficiency of reasoning, ensuring the safety of efficient\nreasoning, and broader applications of efficient reasoning. In addition, we\nhighlight key insights for enhancing LRMs' inference efficiency via techniques\nsuch as model merging, new architectures, and agent routers. We hope this work\nserves as a valuable guide, helping researchers overcome challenges in this\nvibrant field. A collection of efficient reasoning methods for LRMs (papers and\ncodes) is provided at this link:\nhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs."}
{"id": "2504.01137", "pdf": "https://arxiv.org/pdf/2504.01137.pdf", "abs": "https://arxiv.org/abs/2504.01137", "title": "Follow the Flow: On Information Flow Across Textual Tokens in Text-to-Image Models", "authors": ["Guy Kaplan", "Michael Toker", "Yuval Reif", "Yonatan Belinkov", "Roy Schwartz"], "categories": ["cs.CL"], "comment": null, "summary": "Text-to-image (T2I) models generate images by encoding text prompts into\ntoken representations, which then guide the diffusion process. While prior work\nhas largely focused on improving alignment by refining the diffusion process,\nwe focus on the textual encoding stage. Specifically, we investigate how\nsemantic information is distributed across token representations within and\nbetween lexical items (i.e., words or expressions conveying a single concept)\nin the prompt. We analyze information flow at two levels: (1) in-item\nrepresentation-whether individual tokens represent their lexical item, and (2)\ncross-item interaction-whether information flows across the tokens of different\nlexical items. We use patching techniques to uncover surprising encoding\npatterns. We find information is usually concentrated in only one or two of the\nitem's tokens-For example, in the item \"San Francisco's Golden Gate Bridge\",\nthe token \"Gate\" sufficiently captures the entire expression while the other\ntokens could effectively be discarded. Lexical items also tend to remain\nisolated; for instance, the token \"dog\" encodes no visual information about\n\"green\" in the prompt \"a green dog\". However, in some cases, items do influence\neach other's representation, often leading to misinterpretations-e.g., in the\nprompt \"a pool by a table\", the token pool represents a pool table after\ncontextualization. Our findings highlight the critical role of token-level\nencoding in image generation, suggesting that misalignment issues may originate\nalready during the textual encoding."}
{"id": "2504.04310", "pdf": "https://arxiv.org/pdf/2504.04310.pdf", "abs": "https://arxiv.org/abs/2504.04310", "title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization", "authors": ["Weiwei Sun", "Shengyu Feng", "Shanda Li", "Yiming Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although LLM-based agents have attracted significant attention in domains\nsuch as software engineering and machine learning research, their role in\nadvancing combinatorial optimization (CO) remains relatively underexplored.\nThis gap underscores the need for a deeper understanding of their potential in\ntackling structured, constraint-intensive problems -- a pursuit currently\nlimited by the absence of comprehensive benchmarks for systematic\ninvestigation. To address this, we introduce CO-Bench, a benchmark suite\nfeaturing 36 real-world CO problems drawn from a broad range of domains and\ncomplexity levels. CO-Bench includes structured problem formulations and\ncurated data to support rigorous investigation of LLM agents. We evaluate\nmultiple agentic frameworks against established human-designed algorithms,\nrevealing the strengths and limitations of existing LLM agents and identifying\npromising directions for future research. CO-Bench is publicly available at\nhttps://github.com/sunnweiwei/CO-Bench."}
{"id": "2504.07532", "pdf": "https://arxiv.org/pdf/2504.07532.pdf", "abs": "https://arxiv.org/abs/2504.07532", "title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation", "authors": ["Tuhin Chakrabarty", "Philippe Laban", "Chien-Sheng Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Submission", "summary": "AI-generated text is proliferating across domains, from creative writing and\njournalism to marketing content and scientific articles. Models can follow\nuser-provided instructions to generate coherent and grammatically correct\noutputs but in this work, we study a more fundamental question: how do we\nevaluate and improve the writing quality of AI-generated text? Writing quality\nassessment has received less attention from the community, in part because it\nis fundamentally subjective and requires expertise. We first introduce the\nWriting Quality Benchmark (WQ) by consolidating five writing-preference\ndatasets into 4,729 writing quality judgments. Our experiments show that most\nof the competitive baselines, including state-of-the-art LLMs that excel at\nreasoning tasks, barely outperform random baselines on WQ. We then train\nspecialized Writing Quality Reward Models (WQRM) of various sizes for writing\nquality assessment that demonstrate strong generalization on four\nout-of-distribution test sets and 74% accuracy on the WQ benchmark. To further\nshow WQRM's practical benefits during inference, we leverage additional\ntest-time compute to generate and rank multiple candidate revisions, allowing\nus to select higher-quality outputs from an initial draft. Human evaluation\nwith 9 experienced writers confirm that WQRM-based selection produces writing\nsamples preferred by experts 66% overall, and 72.2% when the reward gap is\nlarger than 1 point. We release our datasets and models to encourage community\nengagement with writing quality assessment and development of AI writing\nsystems better aligned with human preferences."}
{"id": "2504.20678", "pdf": "https://arxiv.org/pdf/2504.20678.pdf", "abs": "https://arxiv.org/abs/2504.20678", "title": "Non-native Children's Automatic Speech Assessment Challenge (NOCASA)", "authors": ["Yaroslav Getman", "TamÃ¡s GrÃ³sz", "Mikko Kurimo", "Giampiero Salvi"], "categories": ["cs.CL", "eess.AS"], "comment": "Final version of the baseline paper for the NOCASA competition\n  (https://teflon.aalto.fi/nocasa-2025/), Accepted at IEEE MLSP 2025", "summary": "This paper presents the \"Non-native Children's Automatic Speech Assessment\"\n(NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA\nchallenges participants to develop new systems that can assess single-word\npronunciations of young second language (L2) learners as part of a gamified\npronunciation training app. To achieve this, several issues must be addressed,\nmost notably the limited nature of available training data and the highly\nunbalanced distribution among the pronunciation level categories. To expedite\nthe development, we provide a pseudo-anonymized training data (TeflonNorL2),\ncontaining 10,334 recordings from 44 speakers attempting to pronounce 205\ndistinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that\nshould be given in the game). In addition to the data, two already trained\nsystems are released as official baselines: an SVM classifier trained on the\nComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter\nachieves the best performance on the challenge test set, with an unweighted\naverage recall (UAR) of 36.37%."}
{"id": "2505.00191", "pdf": "https://arxiv.org/pdf/2505.00191.pdf", "abs": "https://arxiv.org/abs/2505.00191", "title": "IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports", "authors": ["Yuyan Ge", "Kwan Ho Ryan Chan", "Pablo Messina", "RenÃ© Vidal"], "categories": ["cs.CL"], "comment": "11 pages, 4 figures", "summary": "The development of AI-based methods to analyze radiology reports could lead\nto significant advances in medical diagnosis, from improving diagnostic\naccuracy to enhancing efficiency and reducing workload. However, the lack of\ninterpretability of AI-based methods could hinder their adoption in clinical\nsettings. In this paper, we propose an interpretable-by-design framework for\nclassifying chest radiology reports. First, we extract a set of representative\nfacts from a large set of reports. Then, given a new report, we query whether a\nsmall subset of the representative facts is entailed by the report, and predict\na diagnosis based on the selected subset of query-answer pairs. The explanation\nfor a prediction is, by construction, the set of selected queries and answers.\nWe use the Information Pursuit framework to select the most informative\nqueries, a natural language inference model to determine if a fact is entailed\nby the report, and a classifier to predict the disease. Experiments on the\nMIMIC-CXR dataset demonstrate the effectiveness of the proposed method,\nhighlighting its potential to enhance trust and usability in medical AI."}
{"id": "2505.02009", "pdf": "https://arxiv.org/pdf/2505.02009.pdf", "abs": "https://arxiv.org/abs/2505.02009", "title": "Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs", "authors": ["Sai Krishna Mendu", "Harish Yenala", "Aditi Gulati", "Shanu Kumar", "Parag Agrawal"], "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 5 figures. Accepted at the International Joint Conferences\n  on Artificial Intelligence IJCAI 2025 (main track)", "summary": "Large language models (LLMs) have become integral to various real-world\napplications, leveraging massive, web-sourced datasets like Common Crawl, C4,\nand FineWeb for pretraining. While these datasets provide linguistic data\nessential for high-quality natural language generation, they often contain\nharmful content, such as hate speech, misinformation, and biased narratives.\nTraining LLMs on such unfiltered data risks perpetuating toxic behaviors,\nspreading misinformation, and amplifying societal biases which can undermine\ntrust in LLM-driven applications and raise ethical concerns about their use.\nThis paper presents a large-scale analysis of inappropriate content across\nthese datasets, offering a comprehensive taxonomy that categorizes harmful\nwebpages into Topical and Toxic based on their intent. We also introduce a\nprompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and\na transformer-based model (HarmFormer) for harmful content filtering.\nAdditionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC)\nand provide crucial insights into how models respond to adversarial toxic\ninputs. We share TTP, TTP-Eval, HAVOC and a sample of C4 inferenced on\nHarmFormer. Our work offers insights into ensuring safer LLM pretraining and\nserves as a resource for Responsible AI (RAI) compliance."}
{"id": "2505.16483", "pdf": "https://arxiv.org/pdf/2505.16483.pdf", "abs": "https://arxiv.org/abs/2505.16483", "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning", "authors": ["Shuzheng Si", "Haozhe Zhao", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Bofei Gao", "Kangyang Luo", "Wenhao Li", "Yufei Huang", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Workshop KnowFM (Oral Presentation)", "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to reduce faithfulness hallucinations of\nLLMs across different downstream tasks without human annotations. Specifically,\nwe first synthesize short-form question-answering (QA) data with four diverse\ntasks to construct high-quality and easily verifiable training data without\nhuman annotation. Also, we propose Dual-GRPO, a rule-based reinforcement\nlearning method that includes three tailored rule-based rewards derived from\nsynthesized short-form QA data, while simultaneously optimizing both short-form\nand long-form response generation. Notably, Dual-GRPO eliminates the need to\nmanually label preference data to train reward models and avoids\nover-optimizing short-form generation when relying only on the synthesized\nshort-form QA data. Experimental results show that CANOE greatly improves the\nfaithfulness of LLMs across 11 different tasks, even outperforming the most\nadvanced LLMs, e.g., GPT-4o and OpenAI o1."}
{"id": "2505.18744", "pdf": "https://arxiv.org/pdf/2505.18744.pdf", "abs": "https://arxiv.org/abs/2505.18744", "title": "LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Complex Reasoning", "authors": ["Tao Liu", "Xutao Mao", "Hongying Zan", "Dixuan Zhang", "Yifan Li", "Haixin Liu", "Lulu Kong", "Jiaming Hou", "Rui Li", "YunLong Li", "aoze zheng", "Zhiqiang Zhang", "Luo Zhewei", "Kunli Zhang", "Min Peng"], "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Text-to-SQL is a critical task in natural language processing that aims to\ntransform natural language questions into accurate and executable SQL queries.\nIn real-world scenarios, these reasoning tasks are often accompanied by complex\nmathematical computations, domain knowledge, and hypothetical reasoning\nscenarios. However, existing large-scale Text-to-SQL datasets typically focus\non business logic and task logic, neglecting critical factors such as vertical\ndomain knowledge, complex mathematical reasoning, and hypothetical reasoning,\nwhich are essential for realistically reflecting the reasoning demands in\npractical applications and completing data querying and analysis. To bridge\nthis gap, we introduce LogicCat, the first Text-to-SQL benchmark dataset\nspecifically designed for complex reasoning and chain-of-thought parsing,\nencompassing physics, arithmetic, commonsense, and hypothetical reasoning\nscenarios. LogicCat comprises 4,038 English questions paired 12,114 detailed\nchain-of-thought reasoning steps, spanning 45 databases across diverse domains,\nsignificantly surpassing existing datasets in complexity. Experimental results\ndemonstrate that LogicCat substantially increases the task difficulty for\ncurrent state-of-the-art models to at most 33.20% execution accuracy,\nindicating that this task remains exceptionally challenging. The advancement of\nLogicCat represents a crucial step toward developing systems suitable for\nreal-world enterprise data analysis and autonomous query generation. We have\nreleased our dataset code at https://github.com/Ffunkytao/LogicCat."}
{"id": "2505.20231", "pdf": "https://arxiv.org/pdf/2505.20231.pdf", "abs": "https://arxiv.org/abs/2505.20231", "title": "MemGuide: Intent-Driven Memory Selection for Goal-Oriented Multi-Session LLM Agents", "authors": ["Yiming Du", "Bingbing Wang", "Yang He", "Bin Liang", "Baojun Wang", "Zhongyang Li", "Lin Gui", "Jeff Z. Pan", "Ruifeng Xu", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Modern task-oriented dialogue (TOD) systems increasingly rely on large\nlanguage model (LLM) agents, leveraging Retrieval-Augmented Generation (RAG)\nand long-context capabilities for long-term memory utilization. However, these\nmethods are primarily based on semantic similarity, overlooking task intent and\nreducing task coherence in multi-session dialogues. To address this challenge,\nwe introduce MemGuide, a two-stage framework for intent-driven memory\nselection. (1) Intent-Aligned Retrieval matches the current dialogue context\nwith stored intent descriptions in the memory bank, retrieving QA-formatted\nmemory units that share the same goal. (2) Missing-Slot Guided Filtering\nemploys a chain-of-thought slot reasoner to enumerate unfilled slots, then uses\na fine-tuned LLaMA-8B filter to re-rank the retrieved units by marginal\nslot-completion gain. The resulting memory units inform a proactive strategy\nthat minimizes conversational turns by directly addressing information gaps.\nBased on this framework, we introduce the MS-TOD, the first multi-session TOD\nbenchmark comprising 132 diverse personas, 956 task goals, and annotated\nintent-aligned memory targets, supporting efficient multi-session task\ncompletion. Evaluations on MS-TOD show that MemGuide raises the task success\nrate by 11% (88% -> 99%) and reduces dialogue length by 2.84 turns in\nmulti-session settings, while maintaining parity with single-session\nbenchmarks."}
{"id": "2505.22964", "pdf": "https://arxiv.org/pdf/2505.22964.pdf", "abs": "https://arxiv.org/abs/2505.22964", "title": "Exploring Scaling Laws for EHR Foundation Models", "authors": ["Sheng Zhang", "Qin Liu", "Naoto Usuyama", "Cliff Wong", "Tristan Naumann", "Hoifung Poon"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of scaling laws has profoundly shaped the development of large\nlanguage models (LLMs), enabling predictable performance gains through\nsystematic increases in model size, dataset volume, and compute. Yet, these\nprinciples remain largely unexplored in the context of electronic health\nrecords (EHRs) -- a rich, sequential, and globally abundant data source that\ndiffers structurally from natural language. In this work, we present the first\nempirical investigation of scaling laws for EHR foundation models. By training\ntransformer architectures on patient timeline data from the MIMIC-IV database\nacross varying model sizes and compute budgets, we identify consistent scaling\npatterns, including parabolic IsoFLOPs curves and power-law relationships\nbetween compute, model parameters, data size, and clinical utility. These\nfindings demonstrate that EHR models exhibit scaling behavior analogous to\nLLMs, offering predictive insights into resource-efficient training strategies.\nOur results lay the groundwork for developing powerful EHR foundation models\ncapable of transforming clinical prediction tasks and advancing personalized\nhealthcare."}
{"id": "2506.00658", "pdf": "https://arxiv.org/pdf/2506.00658.pdf", "abs": "https://arxiv.org/abs/2506.00658", "title": "Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques", "authors": ["Lang Xiong", "Raina Gao", "Alyssa Jeong", "Yicheng Fu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to RANLP SRW and COLM Melt, Solar, PragLM, and Origen", "summary": "Sarcasm is a form of humor where expressions convey meanings opposite to\ntheir literal interpretations. Classifying and generating sarcasm using large\nlanguage models is vital for interpreting human communication. Sarcasm poses\nchallenges for computational models, due to its nuanced nature. We introduce\nSarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating,\nbrooding, deadpan, polite, obnoxious, raging, and manic by annotating entries\nof the MUStARD dataset. Classification was evaluated using zero-shot, few-shot,\nchain-of-thought (CoT), and a novel emotion-based prompting technique. We\npropose an emotion-based generation method developed by identifying key\ncomponents of sarcasm-incongruity, shock value, and context dependency. Our\nclassification experiments show that Gemini 2.5, using emotion-based prompting,\noutperforms other setups with an F1 score of 0.3664. Human evaluators preferred\nour emotion-based prompting, with 38.46% more successful generations than\nzero-shot prompting."}
{"id": "2506.00739", "pdf": "https://arxiv.org/pdf/2506.00739.pdf", "abs": "https://arxiv.org/abs/2506.00739", "title": "DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments", "authors": ["Chiyu Zhang", "Marc-Alexandre Cote", "Michael Albada", "Anush Sankaran", "Jack W. Stokes", "Tong Wang", "Amir Abdi", "William Blum", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) agents have shown impressive capabilities in human\nlanguage comprehension and reasoning, yet their potential in cybersecurity\nremains underexplored. We introduce DefenderBench, a practical, open-source\ntoolkit for evaluating language agents across offense, defense, and\ncybersecurity knowledge-based tasks. DefenderBench includes environments for\nnetwork intrusion, malicious content detection, code vulnerability analysis,\nand cybersecurity knowledge assessment. It is intentionally designed to be\naffordable and easily accessible for researchers while providing fair and\nrigorous assessment. We benchmark several state-of-the-art (SoTA) and popular\nLLMs, including both open- and closed-weight models, using a standardized\nagentic framework. Our results show that Claude-3.7-sonnet performs best with a\nDefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40,\nwhile the best open-weight model, Llama 3.3 70B, is not far behind with a\nDefenderBench score of 71.81. DefenderBench's modular design allows seamless\nintegration of custom LLMs and tasks, promoting reproducibility and fair\ncomparisons. An anonymized version of DefenderBench is available at\nhttps://github.com/microsoft/DefenderBench."}
{"id": "2506.10960", "pdf": "https://arxiv.org/pdf/2506.10960.pdf", "abs": "https://arxiv.org/abs/2506.10960", "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark", "authors": ["Kangwei Liu", "Siyuan Cheng", "Bozhong Tian", "Xiaozhuan Liang", "Yuyang Yin", "Meng Han", "Ningyu Zhang", "Bryan Hooi", "Xi Chen", "Shumin Deng"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.IR", "cs.LG"], "comment": "Work in progress", "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench."}
{"id": "2507.10772", "pdf": "https://arxiv.org/pdf/2507.10772.pdf", "abs": "https://arxiv.org/abs/2507.10772", "title": "Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs", "authors": ["Michal Podstawski"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Labeled property graphs often contain rich textual attributes that can\nenhance analytical tasks when properly leveraged. This work explores the use of\npretrained text embedding models to enable efficient semantic analysis in such\ngraphs. By embedding textual node and edge properties, we support downstream\ntasks including node classification and relation prediction with improved\ncontextual understanding. Our approach integrates language model embeddings\ninto the graph pipeline without altering its structure, demonstrating that\ntextual semantics can significantly enhance the accuracy and interpretability\nof property graph analysis."}
{"id": "2507.23486", "pdf": "https://arxiv.org/pdf/2507.23486.pdf", "abs": "https://arxiv.org/abs/2507.23486", "title": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains", "authors": ["Shirui Wang", "Zhihui Tang", "Huaxia Yang", "Qiuhong Gong", "Tiantian Gu", "Hongyang Ma", "Yongxin Wang", "Wubin Sun", "Zeliang Lian", "Kehang Mao", "Yinan Jiang", "Zhicheng Huang", "Lingyun Ma", "Wenjie Shen", "Yajie Ji", "Yunhui Tan", "Chunbo Wang", "Yunlu Gao", "Qianling Ye", "Rui Lin", "Mingyu Chen", "Lijuan Niu", "Zhihao Wang", "Peng Yu", "Mengran Lang", "Yue Liu", "Huimin Zhang", "Haitao Shen", "Long Chen", "Qiguang Zhao", "Si-Xuan Liu", "Lina Zhou", "Hua Gao", "Dongqiang Ye", "Lingmin Meng", "Youtao Yu", "Naixin Liang", "Jianxiong Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments."}
{"id": "2508.04349", "pdf": "https://arxiv.org/pdf/2508.04349.pdf", "abs": "https://arxiv.org/abs/2508.04349", "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy", "authors": ["Hongze Tan", "Jianfei Pan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models."}
{"id": "2508.05775", "pdf": "https://arxiv.org/pdf/2508.05775.pdf", "abs": "https://arxiv.org/abs/2508.05775", "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation of LLM", "authors": ["Chi Zhang", "Changjia Zhu", "Junjie Xiong", "Xiaoran Xu", "Lingyao Li", "Yao Liu", "Zhuo Lu"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized content creation across\ndigital platforms, offering unprecedented capabilities in natural language\ngeneration and understanding. These models enable beneficial applications such\nas content generation, question and answering (Q&A), programming, and code\nreasoning. Meanwhile, they also pose serious risks by inadvertently or\nintentionally producing toxic, offensive, or biased content. This dual role of\nLLMs, both as powerful tools for solving real-world problems and as potential\nsources of harmful language, presents a pressing sociotechnical challenge. In\nthis survey, we systematically review recent studies spanning unintentional\ntoxicity, adversarial jailbreaking attacks, and content moderation techniques.\nWe propose a unified taxonomy of LLM-related harms and defenses, analyze\nemerging multimodal and LLM-assisted jailbreak strategies, and assess\nmitigation efforts, including reinforcement learning with human feedback\n(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the\nevolving landscape of LLM safety, identifies limitations in current evaluation\nmethodologies, and outlines future research directions to guide the development\nof robust and ethically aligned language technologies."}
{"id": "2508.06220", "pdf": "https://arxiv.org/pdf/2508.06220.pdf", "abs": "https://arxiv.org/abs/2508.06220", "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?", "authors": ["Keummin Ka", "Junhyeong Park", "Jaehyun Jeon", "Youngjae Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 9 figures", "summary": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems."}
{"id": "2508.06433", "pdf": "https://arxiv.org/pdf/2508.06433.pdf", "abs": "https://arxiv.org/abs/2508.06433", "title": "Memp: Exploring Agent Procedural Memory", "authors": ["Runnan Fang", "Yuan Liang", "Xiaobin Wang", "Jialong Wu", "Shuofei Qiao", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains."}
{"id": "2508.07143", "pdf": "https://arxiv.org/pdf/2508.07143.pdf", "abs": "https://arxiv.org/abs/2508.07143", "title": "Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens", "authors": ["Anna Seo Gyeong Choi", "Hoon Choi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to AIES 2025", "summary": "Automatic Speech Recognition (ASR) systems now mediate countless\nhuman-technology interactions, yet research on their fairness implications\nremains surprisingly limited. This paper examines ASR bias through a\nphilosophical lens, arguing that systematic misrecognition of certain speech\nvarieties constitutes more than a technical limitation -- it represents a form\nof disrespect that compounds historical injustices against marginalized\nlinguistic communities. We distinguish between morally neutral classification\n(discriminate1) and harmful discrimination (discriminate2), demonstrating how\nASR systems can inadvertently transform the former into the latter when they\nconsistently misrecognize non-standard dialects. We identify three unique\nethical dimensions of speech technologies that differentiate ASR bias from\nother algorithmic fairness concerns: the temporal burden placed on speakers of\nnon-standard varieties (\"temporal taxation\"), the disruption of conversational\nflow when systems misrecognize speech, and the fundamental connection between\nspeech patterns and personal/cultural identity. These factors create asymmetric\npower relationships that existing technical fairness metrics fail to capture.\nThe paper analyzes the tension between linguistic standardization and pluralism\nin ASR development, arguing that current approaches often embed and reinforce\nproblematic language ideologies. We conclude that addressing ASR bias requires\nmore than technical interventions; it demands recognition of diverse speech\nvarieties as legitimate forms of expression worthy of technological\naccommodation. This philosophical reframing offers new pathways for developing\nASR systems that respect linguistic diversity and speaker autonomy."}
{"id": "2508.07976", "pdf": "https://arxiv.org/pdf/2508.07976.pdf", "abs": "https://arxiv.org/abs/2508.07976", "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL", "authors": ["Jiaxuan Gao", "Wei Fu", "Minyang Xie", "Shusheng Xu", "Chuyi He", "Zhiyu Mei", "Banghua Zhu", "Yi Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher."}
{"id": "2508.08224", "pdf": "https://arxiv.org/pdf/2508.08224.pdf", "abs": "https://arxiv.org/abs/2508.08224", "title": "Capabilities of GPT-5 on Multimodal Medical Reasoning", "authors": ["Shansong Wang", "Mingzhe Hu", "Qiang Li", "Mojtaba Safari", "Xiaofeng Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Corrected some typos", "summary": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.26% and +26.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems."}
{"id": "2508.08275", "pdf": "https://arxiv.org/pdf/2508.08275.pdf", "abs": "https://arxiv.org/abs/2508.08275", "title": "MLLM-CBench:A Comprehensive Benchmark for Continual Instruction Tuning of Multimodal LLMs with Chain-of-Thought Reasoning Analysis", "authors": ["Haiyun Guo", "ZhiYan Hou", "Yu Chen", "Jinghan He", "Yandu Sun", "Yuzhe Zhou", "Shujing Guo", "Kuan Zhu", "Jinqiao Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "under review", "summary": "Multimodal large language models (MLLMs) require continual instruction tuning\nduring their post-training phase to adapt to the dynamic real-world demands.\nHowever, the absence of rigorous and systematic benchmarks has hindered\nprogress in this area. To bridge this gap, we introduce \\textbf{MLLM-CTBench},\na dataset curating seven challenging tasks from six diverse domains with three\ncontributions. First,to enable fine-grained analysis of continual learning\nability, we introduce \\textbf{multidimensional evaluation metrics}, which\ncombines final answer accuracy with Chain-of-Thought (CoT) reasoning quality\nassessment through a carefully trained MLLM evaluator. Then, we conduct a\n\\textbf{comprehensive evaluation of continual learning algorithms},\nsystematically assessing eight algorithms from four major categories to provide\nactionable insights for algorithm design and adoption. Finally ,we evaluate the\nefficacy of \\textbf{Reinforcement Fine-tuning (RFT) versus Supervised\nFine-tuning (SFT)} in maintaining model performance across sequential tasks\nduring continual instruction tuning. Our experiments demonstrate that reasoning\nprocesses in MLLMs exhibit greater resilience than final outputs to forgetting\nduring continual learning, aligning with cognitive theories of hierarchical\nforgetting. We further show that both model capability and task sequence\nsignificantly influence continual learning outcomes, with stronger baseline\nmodels exhibiting greater resistance to forgetting. Notably, properly\nregularized RFT emerges as a more robust approach than SFT for maintaining\nperformance across tasks.One of the key contributing factors is KL-divergence\nregularization, without which RFT leads to even worse forgetting than SFT on\nold tasks though may perform better on new tasks."}
{"id": "2508.08712", "pdf": "https://arxiv.org/pdf/2508.08712.pdf", "abs": "https://arxiv.org/abs/2508.08712", "title": "A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models", "authors": ["Lingzhe Zhang", "Liancheng Fang", "Chiming Duan", "Minghua He", "Leyi Pan", "Pei Xiao", "Shiyu Huang", "Yunpeng Zhai", "Xuming Hu", "Philip S. Yu", "Aiwei Liu"], "categories": ["cs.CL", "cs.AI", "cs.DC", "68T50", "I.2.7"], "comment": null, "summary": "As text generation has become a core capability of modern Large Language\nModels (LLMs), it underpins a wide range of downstream applications. However,\nmost existing LLMs rely on autoregressive (AR) generation, producing one token\nat a time based on previously generated context-resulting in limited generation\nspeed due to the inherently sequential nature of the process. To address this\nchallenge, an increasing number of researchers have begun exploring parallel\ntext generation-a broad class of techniques aimed at breaking the\ntoken-by-token generation bottleneck and improving inference efficiency.\nDespite growing interest, there remains a lack of comprehensive analysis on\nwhat specific techniques constitute parallel text generation and how they\nimprove inference performance. To bridge this gap, we present a systematic\nsurvey of parallel text generation methods. We categorize existing approaches\ninto AR-based and Non-AR-based paradigms, and provide a detailed examination of\nthe core techniques within each category. Following this taxonomy, we assess\ntheir theoretical trade-offs in terms of speed, quality, and efficiency, and\nexamine their potential for combination and comparison with alternative\nacceleration strategies. Finally, based on our findings, we highlight recent\nadvancements, identify open challenges, and outline promising directions for\nfuture research in parallel text generation. We have also created a GitHub\nrepository for indexing relevant papers and open resources available at\nhttps://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation."}
{"id": "2402.11628", "pdf": "https://arxiv.org/pdf/2402.11628.pdf", "abs": "https://arxiv.org/abs/2402.11628", "title": "Discrete Neural Algorithmic Reasoning", "authors": ["Gleb Rodionov", "Liudmila Prokhorenkova"], "categories": ["cs.LG", "cs.CL"], "comment": "Forty-Second International Conference on Machine Learning (ICML 2025)", "summary": "Neural algorithmic reasoning aims to capture computations with neural\nnetworks by training models to imitate the execution of classical algorithms.\nWhile common architectures are expressive enough to contain the correct model\nin the weight space, current neural reasoners struggle to generalize well on\nout-of-distribution data. On the other hand, classical computations are not\naffected by distributional shifts as they can be described as transitions\nbetween discrete computational states. In this work, we propose to force neural\nreasoners to maintain the execution trajectory as a combination of finite\npredefined states. To achieve this, we separate discrete and continuous data\nflows and describe the interaction between them. Trained with supervision on\nthe algorithm's state transitions, such models are able to perfectly align with\nthe original algorithm. To show this, we evaluate our approach on multiple\nalgorithmic problems and achieve perfect test scores both in single-task and\nmultitask setups. Moreover, the proposed architectural choice allows us to\nprove the correctness of the learned algorithms for any test data."}
{"id": "2406.09864", "pdf": "https://arxiv.org/pdf/2406.09864.pdf", "abs": "https://arxiv.org/abs/2406.09864", "title": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal Data", "authors": ["Grigor Bezirganyan", "Sana Sellami", "Laure Berti-Ãquille", "SÃ©bastien Fournier"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "SIGIR 2025", "summary": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We propose LUMA, a unique multimodal dataset,\nfeaturing audio, image, and textual data from 50 classes, specifically designed\nfor learning from uncertain data. It extends the well-known CIFAR 10/100\ndataset with audio samples extracted from three audio corpora, and text data\ngenerated using the Gemma-7B Large Language Model (LLM). The LUMA dataset\nenables the controlled injection of varying types and degrees of uncertainty to\nachieve and tailor specific experiments and benchmarking initiatives. LUMA is\nalso available as a Python package including the functions for generating\nmultiple variants of the dataset with controlling the diversity of the data,\nthe amount of noise for each modality, and adding out-of-distribution samples.\nA baseline pre-trained model is also provided alongside three uncertainty\nquantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable\nConflictive Multi-View Learning. This comprehensive dataset and its tools are\nintended to promote and support the development, evaluation, and benchmarking\nof trustworthy and robust multimodal deep learning approaches. We anticipate\nthat the LUMA dataset will help the research community to design more\ntrustworthy and robust machine learning approaches for safety critical\napplications. The code and instructions for downloading and processing the\ndataset can be found at: https://github.com/bezirganyan/LUMA/ ."}
{"id": "2407.11511", "pdf": "https://arxiv.org/pdf/2407.11511.pdf", "abs": "https://arxiv.org/abs/2407.11511", "title": "Multi-Step Reasoning with Large Language Models, a Survey", "authors": ["Aske Plaat", "Annie Wong", "Suzan Verberne", "Joost Broekens", "Niki van Stein", "Thomas Back"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "revised version", "summary": "Language models with billions of parameters exhibit in-context learning\nabilities, enabling few-shot learning on tasks that the model was not\nspecifically trained for. Traditional models achieve breakthrough performance\non language tasks, but do not perform well on basic reasoning benchmarks.\nHowever, a new in-context learning approach, Chain-of-thought, has demonstrated\nstrong multi-step reasoning abilities on these benchmarks.\n  The research on LLM reasoning abilities started with the question whether\nLLMs can solve grade school math word problems, and has expanded to other tasks\nin the past few years. This paper reviews the field of multi-step reasoning\nwith LLMs. We propose a taxonomy that identifies different ways to generate,\nevaluate, and control multi-step reasoning. We provide an in-depth coverage of\ncore approaches and open problems, and we propose a research agenda for the\nnear future.\n  We find that multi-step reasoning approaches have progressed beyond math word\nproblems, and can now successfully solve challenges in logic, combinatorial\ngames, and robotics, sometimes by first generating code that is then executed\nby external tools. Many studies in multi-step methods are using reinforcement\nlearning for finetuning, external optimization loops, in context reinforcement\nlearning, and self-reflection."}
{"id": "2408.14153", "pdf": "https://arxiv.org/pdf/2408.14153.pdf", "abs": "https://arxiv.org/abs/2408.14153", "title": "Explaining Caption-Image Interactions in CLIP Models with Second-Order Attributions", "authors": ["Lucas MÃ¶ller", "Pascal Tilli", "Ngoc Thang Vu", "Sebastian PadÃ³"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted at Transactions on Machine Learning Research (TMLR)", "summary": "Dual encoder architectures like Clip models map two types of inputs into a\nshared embedding space and predict similarities between them. Despite their\nwide application, it is, however, not understood how these models compare their\ntwo inputs. Common first-order feature-attribution methods explain importances\nof individual features and can, thus, only provide limited insights into dual\nencoders, whose predictions depend on interactions between features. In this\npaper, we first derive a second-order method enabling the attribution of\npredictions by any differentiable dual encoder onto feature-interactions\nbetween its inputs. Second, we apply our method to Clip models and show that\nthey learn fine-grained correspondences between parts of captions and regions\nin images. They match objects across input modes and also account for\nmismatches. This intrinsic visual-linguistic grounding ability, however, varies\nheavily between object classes, exhibits pronounced out-of-domain effects and\nwe can identify individual errors as well as systematic failure categories.\nCode is publicly available: https://github.com/lucasmllr/exCLIP"}
{"id": "2501.03012", "pdf": "https://arxiv.org/pdf/2501.03012.pdf", "abs": "https://arxiv.org/abs/2501.03012", "title": "Analyzing Finetuning Representation Shift for Multimodal LLMs Steering", "authors": ["Pegah Khayatan", "Mustafa Shukor", "Jayneel Parekh", "Arnaud Dapogny", "Matthieu Cord"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "ICCV 2025. The first three authors contributed equally. Project page\n  and code: https://pegah-\n  kh.github.io/projects/lmm-finetuning-analysis-and-steering/", "summary": "Multimodal LLMs (MLLMs) have reached remarkable levels of proficiency in\nunderstanding multimodal inputs. However, understanding and interpreting the\nbehavior of such complex models is a challenging task, not to mention the\ndynamic shifts that may occur during fine-tuning, or due to covariate shift\nbetween datasets. In this work, we apply concept-level analysis towards MLLM\nunderstanding. More specifically, we propose to map hidden states to\ninterpretable visual and textual concepts. This enables us to more efficiently\ncompare certain semantic dynamics, such as the shift from an original and\nfine-tuned model, revealing concept alteration and potential biases that may\noccur during fine-tuning. We also demonstrate the use of shift vectors to\ncapture these concepts changes. These shift vectors allow us to recover\nfine-tuned concepts by applying simple, computationally inexpensive additive\nconcept shifts in the original model. Finally, our findings also have direct\napplications for MLLM steering, which can be used for model debiasing as well\nas enforcing safety in MLLM output. All in all, we propose a novel,\ntraining-free, ready-to-use framework for MLLM behavior interpretability and\ncontrol. Our implementation is publicly available."}
{"id": "2503.05371", "pdf": "https://arxiv.org/pdf/2503.05371.pdf", "abs": "https://arxiv.org/abs/2503.05371", "title": "Shifting Perspectives: Steering Vectors for Robust Bias Mitigation in LLMs", "authors": ["Zara Siddique", "Irtaza Khalid", "Liam D. Turner", "Luis Espinosa-Anke"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Submitted to AACL 2025", "summary": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We compute 8 steering vectors, each corresponding to a different social\nbias axis, such as age, gender, or race, on a training subset of the BBQ\ndataset and compare the effectiveness of these to 3 additional bias mitigation\nmethods across 4 datasets. When optimized on the BBQ dataset, our individually\ntuned steering vectors achieve average improvements of 12.8% on BBQ, 8.3% on\nCLEAR-Bias, and 1% on StereoSet, and show improvements over prompting and\nSelf-Debias in all cases, and improvements over fine-tuning in 12 out of 17\nevaluations. In addition, steering vectors showed the lowest impact on MMLU\nscores of the four bias mitigation methods tested. The work presents the first\nsystematic investigation of steering vectors for bias mitigation, and we\ndemonstrate that they are a powerful and computationally efficient strategy for\nreducing bias in LLMs, with broader implications for enhancing AI safety."}
{"id": "2504.08329", "pdf": "https://arxiv.org/pdf/2504.08329.pdf", "abs": "https://arxiv.org/abs/2504.08329", "title": "MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models", "authors": ["Junmo Kim", "Namkyeong Lee", "Jiwon Kim", "Kwangsoo Kim"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "18 pages", "summary": "Electronic health record (EHR) foundation models have been an area ripe for\nexploration with their improved performance in various medical tasks. Despite\nthe rapid advances, there exists a fundamental limitation: Processing unseen\nmedical codes out of the vocabulary. This problem limits the generality of EHR\nfoundation models and the integration of models trained with different\nvocabularies. To deal with this problem, we propose MedRep for EHR foundation\nmodels based on the observational medical outcome partnership (OMOP) common\ndata model (CDM), providing the integrated medical concept representations and\nthe basic data augmentation strategy for patient trajectories. For concept\nrepresentation learning, we enrich the information of each concept with a\nminimal definition through large language model (LLM) prompts and enhance the\ntext-based representations through graph ontology of OMOP vocabulary.\nTrajectory augmentation randomly replaces selected concepts with other similar\nconcepts that have closely related representations to let the model practice\nwith the concepts out-of-vocabulary. Finally, we demonstrate that EHR\nfoundation models trained with MedRep better maintain the prediction\nperformance in external datasets. Our code implementation is publicly available\nat https://github.com/kicarussays/MedRep."}
{"id": "2504.12867", "pdf": "https://arxiv.org/pdf/2504.12867.pdf", "abs": "https://arxiv.org/abs/2504.12867", "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting", "authors": ["Guanrou Yang", "Chen Yang", "Qian Chen", "Ziyang Ma", "Wenxi Chen", "Wen Wang", "Tianrui Wang", "Yifan Yang", "Zhikang Niu", "Wenrui Liu", "Fan Yu", "Zhihao Du", "Zhifu Gao", "ShiLiang Zhang", "Xie Chen"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": "Accepted at ACMMM 2025", "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Dataset, code, checkpoints, and demo samples\nare available at https://github.com/yanghaha0908/EmoVoice."}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966.pdf", "abs": "https://arxiv.org/abs/2505.21966", "title": "MapStory: Prototyping Editable Map Animations with LLM Agents", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "UIST 2025. Project page:\n  https://adigunturu.github.io/MapStory-UIST25/", "summary": "We introduce MapStory, an LLM-powered animation prototyping tool that\ngenerates editable map animation sequences directly from natural language text\nby leveraging a dual-agent LLM architecture. Given a user written script,\nMapStory automatically produces a scene breakdown, which decomposes the text\ninto key map animation primitives such as camera movements, visual highlights,\nand animated elements. Our system includes a researcher agent that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nautomatic extraction of relevant regions, paths, and coordinates while allowing\nusers to edit and query for changes or additional information to refine the\nresults. Additionally, users can fine-tune parameters of these primitive blocks\nthrough an interactive timeline editor. We detail the system's design and\narchitecture, informed by formative interviews with professional animators and\nby an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories."}
{"id": "2508.00554", "pdf": "https://arxiv.org/pdf/2508.00554.pdf", "abs": "https://arxiv.org/abs/2508.00554", "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism", "authors": ["Li Zhao", "Rui Sun", "Zuoyou Jiang", "Bo Yang", "Yuxiao Bai", "Mengting Chen", "Xinyang Wang", "Jing Li", "Zuo Bai"], "categories": ["q-fin.TR", "cs.CL", "q-fin.CP"], "comment": null, "summary": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multi-agent systems and traditional quantitative investment methods\nacross diverse evaluation metrics. ContestTrade is open-sourced on GitHub at\nhttps://github.com/FinStep-AI/ContestTrade."}
{"id": "2508.01191", "pdf": "https://arxiv.org/pdf/2508.01191.pdf", "abs": "https://arxiv.org/abs/2508.01191", "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens", "authors": ["Chengshuai Zhao", "Zhen Tan", "Pingchuan Ma", "Dawei Li", "Bohan Jiang", "Yancheng Wang", "Yingzhen Yang", "Huan Liu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning."}
{"id": "2508.03772", "pdf": "https://arxiv.org/pdf/2508.03772.pdf", "abs": "https://arxiv.org/abs/2508.03772", "title": "GTPO: Trajectory-Based Policy Optimization in Large Language Models", "authors": ["Marco Simoni", "Aleksandar Fontana", "Giulio Rossolini", "Andrea Saracino"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Policy-based optimizations are widely adopted today for the training and\nalignment of language models, where one of the most recent and effective\napproaches is Group-relative Policy Optimization (GRPO). In this paper, we\nreveals and analyze two major limitations of GRPO: (i) tokens frequently appear\nin completions with both positive and negative rewards, leading to conflicting\ngradient updates that can reduce their output probability, even though can be\nessential for maintaining proper structure; (ii) negatively rewarded\ncompletions may penalize confident responses and shift model decisions toward\nunlikely tokens, progressively flattening the output distribution and degrading\nlearning. To address these issues and provide a more stable and effective\npolicy optimization strategy, we introduce GTPO (Group-relative\nTrajectory-based Policy Optimization), which identifies conflict tokens, tokens\nappearing in the same position across completions with opposite rewards,\nprotects them by skipping negative updates, while amplifying positive ones. To\nfurther prevent policy collapse, GTPO filters out completions whose entropy\nexceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence\nregularization, eliminating the need for a reference model during training,\nwhile still ensuring greater training stability and improved performance,\nvalidated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks."}
{"id": "2508.07315", "pdf": "https://arxiv.org/pdf/2508.07315.pdf", "abs": "https://arxiv.org/abs/2508.07315", "title": "FlexCTC: GPU-powered CTC Beam Decoding With Advanced Contextual Abilities", "authors": ["Lilit Grigoryan", "Vladimir Bataev", "Nikolay Karpov", "Andrei Andrusenko", "Vitaly Lavrukhin", "Boris Ginsburg"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted to Automatic Speech Recognition and Understanding Workshop\n  (ASRU) 2025", "summary": "While beam search improves speech recognition quality over greedy decoding,\nstandard implementations are slow, often sequential, and CPU-bound. To fully\nleverage modern hardware capabilities, we present a novel open-source FlexCTC\ntoolkit for fully GPU-based beam decoding, designed for Connectionist Temporal\nClassification (CTC) models. Developed entirely in Python and PyTorch, it\noffers a fast, user-friendly, and extensible alternative to traditional C++,\nCUDA, or WFST-based decoders. The toolkit features a high-performance, fully\nbatched GPU implementation with eliminated CPU-GPU synchronization and\nminimized kernel launch overhead via CUDA Graphs. It also supports advanced\ncontextualization techniques, including GPU-powered N-gram language model\nfusion and phrase-level boosting. These features enable accurate and efficient\ndecoding, making them suitable for both research and production use."}
{"id": "2508.07353", "pdf": "https://arxiv.org/pdf/2508.07353.pdf", "abs": "https://arxiv.org/abs/2508.07353", "title": "Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach", "authors": ["Rubing Chen", "Jiaxin Wu", "Jian Wang", "Xulu Zhang", "Wenqi Fan", "Chenghua Lin", "Xiao-Yong Wei", "Qing Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Numerous benchmarks have been built to evaluate the domain-specific abilities\nof large language models (LLMs), highlighting the need for effective and\nefficient benchmark construction. Existing domain-specific benchmarks primarily\nfocus on the scaling law, relying on massive corpora for supervised fine-tuning\nor generating extensive question sets for broad coverage. However, the impact\nof corpus and question-answer (QA) set design on the precision and recall of\ndomain-specific LLMs remains unexplored. In this paper, we address this gap and\ndemonstrate that the scaling law is not always the optimal principle for\nbenchmark construction in specific domains. Instead, we propose Comp-Comp, an\niterative benchmarking framework based on a comprehensiveness-compactness\nprinciple. Here, comprehensiveness ensures semantic recall of the domain, while\ncompactness enhances precision, guiding both corpus and QA set construction. To\nvalidate our framework, we conducted a case study in a well-renowned\nuniversity, resulting in the creation of XUBench, a large-scale and\ncomprehensive closed-domain benchmark. Although we use the academic domain as\nthe case in this work, our Comp-Comp framework is designed to be extensible\nbeyond academia, providing valuable insights for benchmark construction across\nvarious domains."}
