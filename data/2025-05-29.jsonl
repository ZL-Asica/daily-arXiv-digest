{"id": "2505.21875", "pdf": "https://arxiv.org/pdf/2505.21875.pdf", "abs": "https://arxiv.org/abs/2505.21875", "title": "Broadening Our View: Assistive Technology for Cerebral Visual Impairment", "authors": ["Bhanuka Gamage", "Leona Holloway", "Nicola McDowell", "Thanh-Toan Do", "Nicholas Seow Chiang Price", "Arthur James Lowery", "Kim Marriott"], "categories": ["cs.HC"], "comment": "Author's accepted version of a LBW paper published in Extended\n  Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA\n  '24)", "summary": "Over the past decade, considerable research has been directed towards\nassistive technologies to support people with vision impairments using machine\nlearning, computer vision, image enhancement, and/or augmented/virtual reality.\nHowever, this has almost totally overlooked a growing demographic: people with\nCerebral Visual Impairment (CVI). Unlike Ocular Vision Impairments (OVI), CVI\narises from damage to the brain's visual processing centres. This paper\nintroduces CVI and reveals a wide research gap in addressing the needs of this\ndemographic. Through a scoping review, we identified 14 papers at the\nintersection of these technologies and CVI. Of these, only three papers\ndescribed assistive technologies focused on people living with CVI, with the\nothers focusing on diagnosis, understanding, simulation or rehabilitation. Our\nfindings highlight the opportunity for the Human-Computer Interaction and\nAssistive Technologies research community to explore and address this\nunderrepresented domain, thereby enhancing the quality of life for people with\nCVI."}
{"id": "2505.21891", "pdf": "https://arxiv.org/pdf/2505.21891.pdf", "abs": "https://arxiv.org/abs/2505.21891", "title": "TIEboard: A Digital Educational Tool for Kids Geometric Learning", "authors": ["Arooj Zaidi", "Giulia Barbareschi", "Kai Kunze", "Yun Suen Pai", "Junichi Yamaoka"], "categories": ["cs.HC"], "comment": null, "summary": "Tangible User Interfaces have shown potential in supporting the acquisition\nof key concepts in computing and mathematics while fostering engagement in\nyoung learners, but these approaches are less commonly utilised in the context\nof geometry. In this paper we introduce TIEboard, an interactive device to\npromote early learning of basic geometry concepts. TIEboard draws inspiration\nfrom traditional geoboards and lacing toys to leverage children's familiarity\nwith these traditional tools. It employs instructional lights to guide children\nin creating shapes using colourful threads of optical fiber. The use of\nconductive materials allows the system to detect lacing activity and provide\nfeedback in real-time. TIEboard incorporates six interaction modes of varying\ndifficulty based on an incremental learning framework. The study evaluated\nTIEboard's effectiveness in supporting early geometric learning, facilitating\ncreativity and promoting collaboration among 16 children aged 5-9."}
{"id": "2505.21964", "pdf": "https://arxiv.org/pdf/2505.21964.pdf", "abs": "https://arxiv.org/abs/2505.21964", "title": "UI-Evol: Automatic Knowledge Evolving for Computer Use Agents", "authors": ["Ziyun Zhang", "Xinyi Liu", "Xiaoyi Zhang", "Jun Wang", "Gang Chen", "Yan Lu"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "External knowledge has played a crucial role in the recent development of\ncomputer use agents. We identify a critical knowledge-execution gap: retrieved\nknowledge often fails to translate into effective real-world task execution.\nOur analysis shows even 90\\% correct knowledge yields only 41\\% execution\nsuccess rate. To bridge this gap, we propose UI-Evol, a plug-and-play module\nfor autonomous GUI knowledge evolution. UI-Evol consists of two stages: a\nRetrace Stage that extracts faithful objective action sequences from actual\nagent-environment interactions, and a Critique Stage that refines existing\nknowledge by comparing these sequences against external references. We conduct\ncomprehensive experiments on the OSWorld benchmark with the state-of-the-art\nAgent S2. Our results demonstrate that UI-Evol not only significantly boosts\ntask performance but also addresses a previously overlooked issue of high\nbehavioral standard deviation in computer use agents, leading to superior\nperformance on computer use tasks and substantially improved agent reliability."}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966.pdf", "abs": "https://arxiv.org/abs/2505.21966", "title": "MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "16 pages and 15 figures", "summary": "We introduce MapStory, an LLM-powered animation authoring tool that generates\neditable map animation sequences directly from natural language text. Given a\nuser-written script, MapStory leverages an agentic architecture to\nautomatically produce a scene breakdown, which decomposes the script into key\nanimation building blocks such as camera movements, visual highlights, and\nanimated elements. Our system includes a researcher component that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nthe automatic extraction of relevant regions, paths, and coordinates while\nallowing users to edit and query for changes or additional information to\nrefine the results. Additionally, users can fine-tune parameters of these\nblocks through an interactive timeline editor. We detail the system's design\nand architecture, informed by formative interviews with professional animators\nand an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories."}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523.pdf", "abs": "https://arxiv.org/abs/2505.21523", "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity."}
{"id": "2505.21982", "pdf": "https://arxiv.org/pdf/2505.21982.pdf", "abs": "https://arxiv.org/abs/2505.21982", "title": "Eye-Tracking and Biometric Feedback in UX Research: Measuring User Engagement and Cognitive Load", "authors": ["Aaditya Shankar Majumder"], "categories": ["cs.HC"], "comment": "4 pages", "summary": "User experience research often uses surveys and interviews, which may miss\nsubconscious user interactions. This study explores eye-tracking and biometric\nfeedback as tools to assess user engagement and cognitive load in digital\ninterfaces. These methods measure gaze behavior and bodily responses, providing\nan objective complement to qualitative insights. Using empirical evidence,\npractical applications, and advancements from 2023-2025, we present\nexperimental data, describe our methodology, and place our work within\nfoundational and recent literature. We address challenges like data\ninterpretation, ethical issues, and technological integration. These tools are\nkey for advancing UX design in complex digital environments."}
{"id": "2505.21578", "pdf": "https://arxiv.org/pdf/2505.21578.pdf", "abs": "https://arxiv.org/abs/2505.21578", "title": "Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use", "authors": ["Titouan Parcollet", "Yuan Tseng", "Shucong Zhang", "Rogier van Dalen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Automatic speech recognition (ASR) research is driven by the availability of\ncommon datasets between industrial researchers and academics, encouraging\ncomparisons and evaluations. LibriSpeech, despite its long success as an ASR\nbenchmark, is now limited by its size and focus on clean, read speech, leading\nto near-zero word error rates. More recent datasets, including MOSEL, YODAS,\nGigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations\nincluding licenses that researchers in the industry cannot use, unreliable\ntranscriptions, incorrect audio data, or the lack of evaluation sets. This work\npresents the Loquacious Set, a 25,000-hour curated collection of commercially\nusable English speech. Featuring hundreds of thousands of speakers with diverse\naccents and a wide range of speech types (read, spontaneous, talks, clean,\nnoisy), the Loquacious Set is designed to work for academics and researchers in\nthe industry to build ASR systems in real-world scenarios."}
{"id": "2505.22303", "pdf": "https://arxiv.org/pdf/2505.22303.pdf", "abs": "https://arxiv.org/abs/2505.22303", "title": "Voice CMS: updating the knowledge base of a digital assistant through conversation", "authors": ["Grzegorz Wolny", "Michał Szczerbak"], "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": null, "summary": "In this study, we propose a solution based on a multi-agent LLM architecture\nand a voice user interface (VUI) designed to update the knowledge base of a\ndigital assistant. Its usability is evaluated in comparison to a more\ntraditional graphical content management system (CMS), with a focus on\nunderstanding the relationship between user preferences and the complexity of\nthe information being provided. The findings demonstrate that, while the\noverall usability of the VUI is rated lower than the graphical interface, it is\nalready preferred by users for less complex tasks. Furthermore, the quality of\ncontent entered through the VUI is comparable to that achieved with the\ngraphical interface, even for highly complex tasks. Obtained qualitative\nresults suggest that a hybrid interface combining the strengths of both\napproaches could address the key challenges identified during the experiment,\nsuch as reducing cognitive load through graphical feedback while maintaining\nthe intuitive nature of voice-based interactions. This work highlights the\npotential of conversational interfaces as a viable and effective method for\nknowledge management in specific business contexts."}
{"id": "2505.21598", "pdf": "https://arxiv.org/pdf/2505.21598.pdf", "abs": "https://arxiv.org/abs/2505.21598", "title": "Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives", "authors": ["Yajiao Liu", "Congliang Chen", "Junchi Yang", "Ruoyu Sun"], "categories": ["cs.CL"], "comment": "The first version of this paper was submitted to ACL ARR 2025\n  February Submission", "summary": "Training large language models with data collected from various domains can\nimprove their performance on downstream tasks. However, given a fixed training\nbudget, the sampling proportions of these different domains significantly\nimpact the model's performance. How can we determine the domain weights across\ndifferent data domains to train the best-performing model within constrained\ncomputational resources? In this paper, we provide a comprehensive overview of\nexisting data mixture methods. First, we propose a fine-grained categorization\nof existing methods, extending beyond the previous offline and online\nclassification. Offline methods are further grouped into heuristic-based,\nalgorithm-based, and function fitting-based methods. For online methods, we\ncategorize them into three groups: online min-max optimization, online mixing\nlaw, and other approaches by drawing connections with the optimization\nframeworks underlying offline methods. Second, we summarize the problem\nformulations, representative algorithms for each subtype of offline and online\nmethods, and clarify the relationships and distinctions among them. Finally, we\ndiscuss the advantages and disadvantages of each method and highlight key\nchallenges in the field of data mixture."}
{"id": "2505.22414", "pdf": "https://arxiv.org/pdf/2505.22414.pdf", "abs": "https://arxiv.org/abs/2505.22414", "title": "ToPSen: Task-Oriented Priming and Sensory Alignment for Comparing Coding Strategies Between Sighted and Blind Programmers", "authors": ["Md Ehtesham-Ul-Haque", "Syed Masum Billah"], "categories": ["cs.HC"], "comment": "Accepted at DIS'25", "summary": "This paper examines how the coding strategies of sighted and blind\nprogrammers differ when working with audio feedback alone. The goal is to\nidentify challenges in mixed-ability collaboration, particularly when sighted\nprogrammers work with blind peers or teach programming to blind students. To\novercome limitations of traditional blindness simulation studies, we proposed\nTask-Oriented Priming and Sensory Alignment (ToPSen), a design framework that\nreframes sensory constraints as technical requirements rather than as a\ndisability. Through a study of 12 blind and 12 sighted participants coding\nnon-visually, we found that expert blind programmers maintain more accurate\nmental models and process more information in working memory than sighted\nprogrammers using ToPSen. Our analysis revealed that blind and sighted\nprogrammers process structural information differently, exposing gaps in\ncurrent IDE designs. These insights inform our guidelines for improving the\naccessibility of programming tools and fostering effective mixed-ability\ncollaboration."}
{"id": "2505.21600", "pdf": "https://arxiv.org/pdf/2505.21600.pdf", "abs": "https://arxiv.org/abs/2505.21600", "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "authors": ["Tianyu Fu", "Yi Ge", "Yichen You", "Enshu Liu", "Zhihang Yuan", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R."}
{"id": "2505.22418", "pdf": "https://arxiv.org/pdf/2505.22418.pdf", "abs": "https://arxiv.org/abs/2505.22418", "title": "AI Trust Reshaping Administrative Burdens: Understanding Trust-Burden Dynamics in LLM-Assisted Benefits Systems", "authors": ["Jeongwon Jo", "He Zhang", "Jie Cai", "Nitesh Goyal"], "categories": ["cs.HC"], "comment": "FAccT 2025", "summary": "Supplemental Nutrition Assistance Program (SNAP) is an essential benefit\nsupport system provided by the US administration to 41 million federally\ndetermined low-income applicants. Through interviews with such applicants\nacross a diverse set of experiences with the SNAP system, our findings reveal\nthat new AI technologies like LLMs can alleviate traditional burdens but also\nintroduce new burdens. We introduce new types of learning, compliance, and\npsychological costs that transform the administrative burden on applicants. We\nalso identify how trust in AI across three dimensions--competence, integrity,\nand benevolence--is perceived to reduce administrative burdens, which may stem\nfrom unintended and untoward overt trust in the system. We discuss calibrating\nappropriate levels of user trust in LLM-based administrative systems,\nmitigating newly introduced burdens. In particular, our findings suggest that\nevidence-based information disclosure is necessary in benefits administration\nand propose directions for future research on trust-burden dynamics in\nAI-assisted administration systems."}
{"id": "2505.21608", "pdf": "https://arxiv.org/pdf/2505.21608.pdf", "abs": "https://arxiv.org/abs/2505.21608", "title": "How does Misinformation Affect Large Language Model Behaviors and Preferences?", "authors": ["Miao Peng", "Nuo Chen", "Jianheng Tang", "Jia Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nknowledge-intensive tasks, while they remain vulnerable when encountering\nmisinformation. Existing studies have explored the role of LLMs in combating\nmisinformation, but there is still a lack of fine-grained analysis on the\nspecific aspects and extent to which LLMs are influenced by misinformation. To\nbridge this gap, we present MisBench, the current largest and most\ncomprehensive benchmark for evaluating LLMs' behavior and knowledge preference\ntoward misinformation. MisBench consists of 10,346,712 pieces of\nmisinformation, which uniquely considers both knowledge-based conflicts and\nstylistic variations in misinformation. Empirical results reveal that while\nLLMs demonstrate comparable abilities in discerning misinformation, they still\nremain susceptible to knowledge conflicts and stylistic variations. Based on\nthese findings, we further propose a novel approach called Reconstruct to\nDiscriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our\nstudy provides valuable insights into LLMs' interactions with misinformation,\nand we believe MisBench can serve as an effective benchmark for evaluating\nLLM-based detectors and enhancing their reliability in real-world applications.\nCodes and data are available at https://github.com/GKNL/MisBench."}
{"id": "2505.22428", "pdf": "https://arxiv.org/pdf/2505.22428.pdf", "abs": "https://arxiv.org/abs/2505.22428", "title": "Parental Collaboration and Closeness: Envisioning with New Couple Parents", "authors": ["Ya-Fang Lin", "Xiaotian Li", "Wan-Hsuan Huang", "Charan Pushpanathan Prabavathi", "Jie Cai", "John M. Carroll"], "categories": ["cs.HC", "cs.CY"], "comment": "DIS 2025", "summary": "Couples often experience a decrease in closeness as they cope with the\ndemands of parenthood. Existing technologies have supported parenting and\nparental collaboration. However, these technologies do not adequately support\ncloseness in co-parenting. We use scenarios and design probes to brainstorm\nwith 10 new parent couples to explore and envision possibilities for\ntechnologies to support closeness. We reported parents' current technology use\nfor co-parenting and how participants considered and envisioned co-parenting\ntechnology for closeness, including information and task sharing, emotion\nawareness and disclosure, and fostering fun interaction. We discuss the\npotential technology has for fostering closeness in co-parenting by (1)\nfostering interdependence by supporting parental competence and (2) integrating\npositive emotions and experiences, such as validation and fun, in parenting.\nBased on our findings, we expand the design space of technology for closeness\nto include interdependence. We also expand the design space for co-parenting\ntechnology by integrating more positive emotions."}
{"id": "2505.21646", "pdf": "https://arxiv.org/pdf/2505.21646.pdf", "abs": "https://arxiv.org/abs/2505.21646", "title": "Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts", "authors": ["Lei Zhang", "Markus Stricker"], "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": "13 pages, 5 figures, 2 tables, accepted at ECMLPKDD 2025", "summary": "The discovery and optimization of materials for specific applications is\nhampered by the practically infinite number of possible elemental combinations\nand associated properties, also known as the `combinatorial explosion'. By\nnature of the problem, data are scarce and all possible data sources should be\nused. In addition to simulations and experimental results, the latent knowledge\nin scientific texts is not yet used to its full potential. We present an\niterative framework that refines a given scientific corpus by strategic\nselection of the most diverse documents, training Word2Vec models, and\nmonitoring the convergence of composition-property correlations in embedding\nspace. Our approach is applied to predict high-performing materials for oxygen\nreduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions\nfor a large number of possible candidate compositions. Our method successfully\npredicts the highest performing compositions among a large pool of candidates,\nvalidated by experimental measurements of the electrocatalytic performance in\nthe lab. This work demonstrates and validates the potential of iterative corpus\nrefinement to accelerate materials discovery and optimization, offering a\nscalable and efficient tool for screening large compositional spaces where\nreliable data are scarce or non-existent."}
{"id": "2505.22477", "pdf": "https://arxiv.org/pdf/2505.22477.pdf", "abs": "https://arxiv.org/abs/2505.22477", "title": "Human-Centered Human-AI Collaboration (HCHAC)", "authors": ["Qi Gao", "Wei Xu", "Hanxi Pan", "Mowei Shen", "Zaifeng Gao"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "This article is a chapter from the upcoming book Handbook of\n  Human-Centered Artificial Intelligence", "summary": "In the intelligent era, the interaction between humans and intelligent\nsystems fundamentally involves collaboration with autonomous intelligent\nagents. Human-AI Collaboration (HAC) represents a novel type of human-machine\nrelationship facilitated by autonomous intelligent machines equipped with AI\ntechnologies. In this paradigm, AI agents serve not only as auxiliary tools but\nalso as active teammates, partnering with humans to accomplish tasks\ncollaboratively. Human-centered AI (HCAI) emphasizes that humans play critical\nleadership roles in the collaboration. This human-led collaboration imparts new\ndimensions to the human-machine relationship, necessitating innovative research\nperspectives, paradigms, and agenda to address the unique challenges posed by\nHAC. This chapter delves into the essence of HAC from the human-centered\nperspective, outlining its core concepts and distinguishing features. It\nreviews the current research methodologies and research agenda within the HAC\nfield from the HCAI perspective, highlighting advancements and ongoing studies.\nFurthermore, a framework for human-centered HAC (HCHAC) is proposed by\nintegrating these reviews and analyses. A case study of HAC in the context of\nautonomous vehicles is provided, illustrating practical applications and the\nsynergistic interactions between humans and AI agents. Finally, it identifies\npotential future research directions aimed at enhancing the effectiveness,\nreliability, and ethical integration of human-centered HAC systems in diverse\ndomains."}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657.pdf", "abs": "https://arxiv.org/abs/2505.21657", "title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "authors": ["Zeinab Dehghani", "Koorosh Aslansefat", "Adil Khan", "Mohammed Naveed Akram"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.16277", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy."}
{"id": "2505.22539", "pdf": "https://arxiv.org/pdf/2505.22539.pdf", "abs": "https://arxiv.org/abs/2505.22539", "title": "Spot-On: A Mixed Reality Interface for Multi-Robot Cooperation", "authors": ["Tim Engelbracht", "Petar Lukovic", "Tjark Behrens", "Kai Lascheit", "René Zurbrügg", "Marc Pollefeys", "Hermann Blum", "Zuria Bauer"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Recent progress in mixed reality (MR) and robotics is enabling increasingly\nsophisticated forms of human-robot collaboration. Building on these\ndevelopments, we introduce a novel MR framework that allows multiple quadruped\nrobots to operate in semantically diverse environments via a MR interface. Our\nsystem supports collaborative tasks involving drawers, swing doors, and\nhigher-level infrastructure such as light switches. A comprehensive user study\nverifies both the design and usability of our app, with participants giving a\n\"good\" or \"very good\" rating in almost all cases. Overall, our approach\nprovides an effective and intuitive framework for MR-based multi-robot\ncollaboration in complex, real-world scenarios."}
{"id": "2505.21670", "pdf": "https://arxiv.org/pdf/2505.21670.pdf", "abs": "https://arxiv.org/abs/2505.21670", "title": "Rethinking the Outlier Distribution in Large Language Models: An In-depth Study", "authors": ["Rahul Raman", "Khushi Sharma", "Sai Qian Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Investigating outliers in large language models (LLMs) is crucial due to\ntheir significant impact on various aspects of LLM performance, including\nquantization and compression. Outliers often cause considerable quantization\nerrors, leading to degraded model performance. Identifying and addressing these\noutliers can enhance the accuracy and efficiency of the quantization process,\nenabling smoother deployment on edge devices or specialized hardware. Recent\nstudies have identified two common types of outliers in LLMs: massive\nactivations and channel-wise outliers. While numerous quantization algorithms\nhave been proposed to mitigate their effects and maintain satisfactory\naccuracy, few have thoroughly explored the root causes of these outliers in\ndepth. In this paper, we conduct a comprehensive investigation into the\nformation mechanisms of these outliers and propose potential strategies to\nmitigate their occurrence. Ultimately, we introduce some efficient approaches\nto eliminate most massive activations and channel-wise outliers with minimal\nimpact on accuracy."}
{"id": "2505.21512", "pdf": "https://arxiv.org/pdf/2505.21512.pdf", "abs": "https://arxiv.org/abs/2505.21512", "title": "The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows", "authors": ["Harry Li", "Gabriel Appleby", "Kenneth Alperin", "Steven R Gomez", "Ashley Suh"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Knowledge graphs (KGs) are powerful data structures, but exploring them\neffectively remains difficult for even expert users. Large language models\n(LLMs) are increasingly used to address this gap, yet little is known\nempirically about how their usage with KGs shapes user trust, exploration\nstrategies, or downstream decision-making - raising key design challenges for\nLLM-based KG visual analysis systems. To study these effects, we developed\nLinkQ, a KG exploration system that converts natural language questions into\nstructured queries with an LLM. We collaborated with KG experts to design five\nvisual mechanisms that help users assess the accuracy of both KG queries and\nLLM responses: an LLM-KG state diagram that illustrates which stage of the\nexploration pipeline LinkQ is in, a query editor displaying the generated query\npaired with an LLM explanation, an entity-relation ID table showing extracted\nKG entities and relations with semantic descriptions, a query structure graph\nthat depicts the path traversed in the KG, and an interactive graph\nvisualization of query results. From a qualitative evaluation with 14\npractitioners, we found that users - even KG experts - tended to overtrust\nLinkQ's outputs due to its \"helpful\" visualizations, even when the LLM was\nincorrect. Users exhibited distinct workflows depending on their prior\nfamiliarity with KGs and LLMs, challenging the assumption that these systems\nare one-size-fits-all - despite often being designed as if they are. Our\nfindings highlight the risks of false trust in LLM-assisted data analysis tools\nand the need for further investigation into the role of visualization as a\nmitigation technique."}
{"id": "2505.21689", "pdf": "https://arxiv.org/pdf/2505.21689.pdf", "abs": "https://arxiv.org/abs/2505.21689", "title": "LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model", "authors": ["Avijit Gayen", "Somyajit Chakraborty", "Mainak Sen", "Soham Paul", "Angshuman Jana"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 5 figures, journal paper, submitted to AI and Law", "summary": "The persistent accumulation of unresolved legal cases, especially within the\nIndian judiciary, significantly hampers the timely delivery of justice. Manual\nmethods of prioritizing petitions are often prone to inefficiencies and\nsubjective biases further exacerbating delays. To address this issue, we\npropose LLMPR (Large Language Model-based Petition Ranking), an automated\nframework that utilizes transfer learning and machine learning to assign\npriority rankings to legal petitions based on their contextual urgency.\nLeveraging the ILDC dataset comprising 7,593 annotated petitions, we process\nunstructured legal text and extract features through various embedding\ntechniques, including DistilBERT, LegalBERT, and MiniLM. These textual\nembeddings are combined with quantitative indicators such as gap days, rank\nscores, and word counts to train multiple machine learning models, including\nRandom Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments\ndemonstrate that Random Forest and Decision Tree models yield superior\nperformance, with accuracy exceeding 99% and a Spearman rank correlation of\n0.99. Notably, models using only numerical features achieve nearly optimal\nranking results (R2 = 0.988, \\r{ho} = 0.998), while LLM-based embeddings offer\nonly marginal gains. These findings suggest that automated petition ranking can\neffectively streamline judicial workflows, reduce case backlog, and improve\nfairness in legal prioritization."}
{"id": "2505.21562", "pdf": "https://arxiv.org/pdf/2505.21562.pdf", "abs": "https://arxiv.org/abs/2505.21562", "title": "Enhancing Selection of Climate Tech Startups with AI -- A Case Study on Integrating Human and AI Evaluations in the ClimaTech Great Global Innovation Challenge", "authors": ["Jennifer Turliuk", "Alejandro Sevilla", "Daniela Gorza", "Tod Hynes"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "This case study examines the ClimaTech Great Global Innovation Challenge's\napproach to selecting climate tech startups by integrating human and AI\nevaluations. The competition aimed to identify top startups and enhance the\naccuracy and efficiency of the selection process through a hybrid model.\nResearch shows data-driven approaches help VC firms reduce bias and improve\ndecision-making. Machine learning models have outperformed human investors in\ndeal screening, helping identify high-potential startups. Incorporating AI\naimed to ensure more equitable and objective evaluations.\n  The methodology included three phases: initial AI review, semi-finals judged\nby humans, and finals using a hybrid weighting. In phase one, 57 applications\nwere scored by an AI tool built with StackAI and OpenAI's GPT-4o, and the top\n36 advanced. In the semi-finals, human judges, unaware of AI scores, evaluated\nstartups on team quality, market potential, and technological innovation. Each\nscore - human or AI - was weighted equally, resulting in 75 percent human and\n25 percent AI influence. In the finals, with five human judges, weighting\nshifted to 83.3 percent human and 16.7 percent AI. There was a moderate\npositive correlation between AI and human scores - Spearman's = 0.47 -\nindicating general alignment with key differences. Notably, the final four\nstartups, selected mainly by humans, were among those rated highest by the AI.\nThis highlights the complementary nature of AI and human judgment. The study\nshows that hybrid models can streamline and improve startup assessments. The\nClimaTech approach offers a strong framework for future competitions by\ncombining human expertise with AI capabilities."}
{"id": "2505.21693", "pdf": "https://arxiv.org/pdf/2505.21693.pdf", "abs": "https://arxiv.org/abs/2505.21693", "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs", "authors": ["Raoyuan Zhao", "Beiduo Chen", "Barbara Plank", "Michael A. Hedderich"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data."}
{"id": "2505.21582", "pdf": "https://arxiv.org/pdf/2505.21582.pdf", "abs": "https://arxiv.org/abs/2505.21582", "title": "AITEE -- Agentic Tutor for Electrical Engineering", "authors": ["Christopher Knievel", "Alexander Bernhardt", "Christian Bernhardt"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "12 pages, 11 figures, 6 tables", "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation."}
{"id": "2505.21701", "pdf": "https://arxiv.org/pdf/2505.21701.pdf", "abs": "https://arxiv.org/abs/2505.21701", "title": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing", "authors": ["Raoyuan Zhao", "Abdullatif Köksal", "Ali Modarressi", "Michael A. Hedderich", "Hinrich Schütze"], "categories": ["cs.CL"], "comment": null, "summary": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks."}
{"id": "2505.21604", "pdf": "https://arxiv.org/pdf/2505.21604.pdf", "abs": "https://arxiv.org/abs/2505.21604", "title": "Public Discourse Sandbox: Facilitating Human and AI Digital Communication Research", "authors": ["Kristina Radivojevic", "Caleb Reinking", "Shaun Whitfield", "Paul Brenner"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Social media serves as a primary communication and information dissemination\nplatform for major global events, entertainment, and niche or topically focused\ncommunity discussions. Therefore, it represents a valuable resource for\nresearchers who aim to understand numerous questions. However, obtaining data\ncan be difficult, expensive, and often unreliable due to the presence of bots,\nfake accounts, and manipulated content. Additionally, there are ethical\nconcerns if researchers decide to conduct an online experiment without\nexplicitly notifying social media users about their intent. There is a need for\nmore controlled and scalable mechanisms to evaluate the impacts of digital\ndiscussion interventions on audiences. We introduce the Public Discourse\nSandbox (PDS), which serves as a digital discourse research platform for\nhuman-AI as well as AI-AI discourse research, testing, and training. PDS\nprovides a safe and secure space for research experiments that are not viable\non public, commercial social media platforms. Its main purpose is to enable the\nunderstanding of AI behaviors and the impacts of customized AI participants via\ntechniques such as prompt engineering, retrieval-augmented generation (RAG),\nand fine-tuning. We provide a hosted live version of the sandbox to support\nresearchers as well as the open-sourced code on GitHub for community\ncollaboration and contribution."}
{"id": "2505.21710", "pdf": "https://arxiv.org/pdf/2505.21710.pdf", "abs": "https://arxiv.org/abs/2505.21710", "title": "Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study", "authors": ["Barbarestani Baran", "Maks Isa", "Vossen Piek"], "categories": ["cs.CL"], "comment": null, "summary": "This study evaluates the effectiveness of ChatGPT, an advanced AI model for\nnatural language processing, in identifying targeting and inappropriate\nlanguage in online comments. With the increasing challenge of moderating vast\nvolumes of user-generated content on social network sites, the role of AI in\ncontent moderation has gained prominence. We compared ChatGPT's performance\nagainst crowd-sourced annotations and expert evaluations to assess its\naccuracy, scope of detection, and consistency. Our findings highlight that\nChatGPT performs well in detecting inappropriate content, showing notable\nimprovements in accuracy through iterative refinements, particularly in Version\n6. However, its performance in targeting language detection showed variability,\nwith higher false positive rates compared to expert judgments. This study\ncontributes to the field by demonstrating the potential of AI models like\nChatGPT to enhance automated content moderation systems while also identifying\nareas for further improvement. The results underscore the importance of\ncontinuous model refinement and contextual understanding to better support\nautomated moderation and mitigate harmful online behavior."}
{"id": "2505.21682", "pdf": "https://arxiv.org/pdf/2505.21682.pdf", "abs": "https://arxiv.org/abs/2505.21682", "title": "Data and Technology for Equitable Public Administration: Understanding City Government Employees' Challenges and Needs", "authors": ["Angie Zhang", "Madison Liao", "Elizaveta", "Kravchenko", "Marshanah Taylor", "Angela Haddad", "Chandra Bhat", "S. Craig Watkins", "Min Kyung Lee"], "categories": ["cs.CY", "cs.HC"], "comment": "Accepted to ACM CSCW 2025", "summary": "City governments in the United States are increasingly pressured to adopt\nemerging technologies. Yet, these systems often risk biased and disparate\noutcomes. Scholars studying public sector technology design have converged on\nthe need to ground these systems in the goals and organizational contexts of\nemployees using them. We expand our understanding of employees' contexts by\nfocusing on the equity practices of city government employees to surface\nimportant equity considerations around public sector data and technology use.\nThrough semi-structured interviews with thirty-six employees from ten\ndepartments of a U.S. city government, our findings reveal challenges employees\nface when operationalizing equity, perspectives on data needs for advancing\nequity goals, and the design space for acceptable government technology. We\ndiscuss what it looks like to foreground equity in data use and technology\ndesign, and considerations for how to support city government employees in\noperationalizing equity with and without official equity offices."}
{"id": "2505.21740", "pdf": "https://arxiv.org/pdf/2505.21740.pdf", "abs": "https://arxiv.org/abs/2505.21740", "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks", "authors": ["Marvin Limpijankit", "Yanda Chen", "Melanie Subbiah", "Nicholas Deas", "Kathleen McKeown"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks."}
{"id": "2505.21724", "pdf": "https://arxiv.org/pdf/2505.21724.pdf", "abs": "https://arxiv.org/abs/2505.21724", "title": "OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions", "authors": ["Cheng Luo", "Jianghui Wang", "Bing Li", "Siyang Song", "Bernard Ghanem"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "23 pages, 9 figures", "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task that aims to online generate synchronized\nverbal and non-verbal listener feedback, conditioned on the speaker's\nmultimodal input. OMCRG reflects natural dyadic interactions and poses new\nchallenges in achieving synchronization between the generated audio and facial\nresponses of the listener. To address these challenges, we innovatively\nintroduce text as an intermediate modality to bridge the audio and facial\nresponses. We hence propose OmniResponse, a Multimodal Large Language Model\n(MLLM) that autoregressively generates high-quality multi-modal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two novel\ncomponents: Chrono-Text, which temporally anchors generated text tokens, and\nTempoVoice, a controllable online TTS module that produces speech synchronized\nwith facial reactions. To support further OMCRG research, we present\nResponseNet, a new dataset comprising 696 high-quality dyadic interactions\nfeaturing synchronized split-screen videos, multichannel audio, transcripts,\nand facial behavior annotations. Comprehensive evaluations conducted on\nResponseNet demonstrate that OmniResponse significantly outperforms baseline\nmodels in terms of semantic speech content, audio-visual synchronization, and\ngeneration quality."}
{"id": "2505.21757", "pdf": "https://arxiv.org/pdf/2505.21757.pdf", "abs": "https://arxiv.org/abs/2505.21757", "title": "BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum", "authors": ["Yubin Kim", "Zhiyuan Hu", "Hyewon Jeong", "Eugene Park", "Shuyue Stella Li", "Chanwoo Park", "Shiyun Xiong", "MingYu Lu", "Hyeonhoon Lee", "Xin Liu", "Daniel McDuff", "Cynthia Breazeal", "Samir Tulebaev", "Hae Won Park"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) as clinical agents require careful behavioral\nadaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs\noften struggle with proactive engagement, like unprompted identification of\ncritical missing information or risks. We introduce BehaviorBench, a\ncomprehensive dataset to evaluate agent behaviors across a clinical assistance\nspectrum, ranging from reactive query responses to proactive interventions\n(e.g., clarifying ambiguities, flagging overlooked critical data). Our\nBehaviorBench experiments reveal LLMs' inconsistent proactivity. To address\nthis, we propose BehaviorSFT, a novel training strategy using behavioral tokens\nto explicitly condition LLMs for dynamic behavioral selection along this\nspectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro\nF1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to\n96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed\nBehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a\nsuperior balance between helpful proactivity (e.g., timely, relevant\nsuggestions) and necessary restraint (e.g., avoiding over-intervention) versus\nstandard fine-tuning or explicit instructed agents."}
{"id": "2505.21752", "pdf": "https://arxiv.org/pdf/2505.21752.pdf", "abs": "https://arxiv.org/abs/2505.21752", "title": "Experimental Evidence That AI-Managed Workers Tolerate Lower Pay Without Demotivation", "authors": ["Mengchen Dong", "Levin Brinkmann", "Omar Sherif", "Shihan Wang", "Xinyu Zhang", "Jean-François Bonnefon", "Iyad Rahwan"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Experimental evidence on worker responses to AI management remains mixed,\npartly due to limitations in experimental fidelity. We address these\nlimitations with a customized workplace in the Minecraft platform, enabling\nhigh-resolution behavioral tracking of autonomous task execution, and ensuring\nthat participants approach the task with well-formed expectations about their\nown competence. Workers (N = 382) completed repeated production tasks under\neither human, AI, or hybrid management. An AI manager trained on human-defined\nevaluation principles systematically assigned lower performance ratings and\nreduced wages by 40\\%, without adverse effects on worker motivation and sense\nof fairness. These effects were driven by a muted emotional response to AI\nevaluation, compared to evaluation by a human. The very features that make AI\nappear impartial may also facilitate silent exploitation, by suppressing the\nsocial reactions that normally constrain extractive practices in human-managed\nwork."}
{"id": "2505.21772", "pdf": "https://arxiv.org/pdf/2505.21772.pdf", "abs": "https://arxiv.org/abs/2505.21772", "title": "Calibrating LLM Confidence by Probing Perturbed Representation Stability", "authors": ["Reza Khanmohammadi", "Erfan Miahi", "Mehrsa Mardikoraem", "Simerjot Kaur", "Ivan Brugere", "Charese H. Smiley", "Kundan Thind", "Mohammad M. Ghassemi"], "categories": ["cs.CL"], "comment": null, "summary": "Miscalibration in Large Language Models (LLMs) undermines their reliability,\nhighlighting the need for accurate confidence estimation. We introduce CCPS\n(Calibrating LLM Confidence by Probing Perturbed Representation Stability), a\nnovel method analyzing internal representational stability in LLMs. CCPS\napplies targeted adversarial perturbations to final hidden states, extracts\nfeatures reflecting the model's response to these perturbations, and uses a\nlightweight classifier to predict answer correctness. CCPS was evaluated on\nLLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral\narchitectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and\nopen-ended formats. Our results show that CCPS significantly outperforms\ncurrent approaches. Across four LLMs and three MMLU variants, CCPS reduces\nExpected Calibration Error by approximately 55% and Brier score by 21%, while\nincreasing accuracy by 5 percentage points, Area Under the Precision-Recall\nCurve by 4 percentage points, and Area Under the Receiver Operating\nCharacteristic Curve by 6 percentage points, all relative to the strongest\nprior method. CCPS delivers an efficient, broadly applicable, and more accurate\nsolution for estimating LLM confidence, thereby improving their\ntrustworthiness."}
{"id": "2505.21907", "pdf": "https://arxiv.org/pdf/2505.21907.pdf", "abs": "https://arxiv.org/abs/2505.21907", "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy", "authors": ["Saleh Afzoon", "Zahra Jahanandish", "Phuong Thao Huynh", "Amin Beheshti", "Usman Naseem"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI copilots, context-aware, AI-powered systems designed to assist users in\ntasks such as software development and content creation, are becoming integral\nto modern workflows. As these systems grow in capability and adoption,\npersonalization has emerged as a cornerstone for ensuring usability, trust, and\nproductivity. Central to this personalization is preference optimization: the\nability of AI copilots to detect, interpret, and align with individual user\npreferences. While personalization techniques are well-established in domains\nlike recommender systems and dialogue agents, their adaptation to interactive,\nreal-time systems like AI copilots remains fragmented and underexplored. This\nsurvey addresses this gap by synthesizing research on how user preferences are\ncaptured, modeled, and refined within the design of AI copilots. We introduce a\nunified definition of AI copilots and propose a phase-based taxonomy of\npreference optimization strategies, structured around pre-interaction,\nmid-interaction, and post-interaction stages. We analyze techniques for\nacquiring preference signals, modeling user intent, and integrating feedback\nloops, highlighting both established approaches and recent innovations. By\nbridging insights from AI personalization, human-AI collaboration, and large\nlanguage model adaptation, this survey provides a structured foundation for\ndesigning adaptive, preference-aware AI copilots. It offers a holistic view of\nthe available preference resources, how they can be leveraged, and which\ntechnical approaches are most suited to each stage of system design."}
{"id": "2505.21781", "pdf": "https://arxiv.org/pdf/2505.21781.pdf", "abs": "https://arxiv.org/abs/2505.21781", "title": "GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task", "authors": ["Chutong Meng", "Antonios Anastasopoulos"], "categories": ["cs.CL"], "comment": "IWSLT 2025", "summary": "This paper describes the GMU systems for the IWSLT 2025 low-resource speech\ntranslation shared task. We trained systems for all language pairs, except for\nLevantine Arabic. We fine-tuned SeamlessM4T-v2 for automatic speech recognition\n(ASR), machine translation (MT), and end-to-end speech translation (E2E ST).\nThe ASR and MT models are also used to form cascaded ST systems. Additionally,\nwe explored various training paradigms for E2E ST fine-tuning, including direct\nE2E fine-tuning, multi-task training, and parameter initialization using\ncomponents from fine-tuned ASR and/or MT models. Our results show that (1)\ndirect E2E fine-tuning yields strong results; (2) initializing with a\nfine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has\nnot been trained on; (3) multi-task training can be slightly helpful."}
{"id": "2505.22032", "pdf": "https://arxiv.org/pdf/2505.22032.pdf", "abs": "https://arxiv.org/abs/2505.22032", "title": "Retweets, Receipts, and Resistance: Discourse, Sentiment, and Credibility in Public Health Crisis Twitter", "authors": ["Tawfiq Ammari", "Anna Gutowska", "Jacob Ziff", "Casey Randazzo", "Harihan Subramonyam"], "categories": ["cs.SI", "cs.HC"], "comment": "arXiv admin note: substantial text overlap with arXiv:2503.20262", "summary": "As the COVID-19 pandemic evolved, the Centers for Disease Control and\nPrevention (CDC) used Twitter to disseminate safety guidance and updates,\nreaching millions of users. This study analyzes two years of tweets from, to,\nand about the CDC using a mixed methods approach to examine discourse\ncharacteristics, credibility, and user engagement. We found that the CDCs\ncommunication remained largely one directional and did not foster reciprocal\ninteraction, while discussions around COVID19 were deeply shaped by political\nand ideological polarization. Users frequently cited earlier CDC messages to\ncritique new and sometimes contradictory guidance. Our findings highlight the\nrole of sentiment, media richness, and source credibility in shaping the spread\nof public health messages. We propose design strategies to help the CDC tailor\ncommunications to diverse user groups and manage misinformation more\neffectively during high-stakes health crises."}
{"id": "2505.21786", "pdf": "https://arxiv.org/pdf/2505.21786.pdf", "abs": "https://arxiv.org/abs/2505.21786", "title": "VeriTrail: Closed-Domain Hallucination Detection with Traceability", "authors": ["Dasha Metropolitansky", "Jonathan Larson"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Even when instructed to adhere to source material, Language Models often\ngenerate unsubstantiated content - a phenomenon known as \"closed-domain\nhallucination.\" This risk is amplified in processes with multiple generative\nsteps (MGS), compared to processes with a single generative step (SGS).\nHowever, due to the greater complexity of MGS processes, we argue that\ndetecting hallucinations in their final outputs is necessary but not\nsufficient: it is equally important to trace where hallucinated content was\nlikely introduced and how faithful content may have been derived from the\nsource through intermediate outputs. To address this need, we present\nVeriTrail, the first closed-domain hallucination detection method designed to\nprovide traceability for both MGS and SGS processes. We also introduce the\nfirst datasets to include all intermediate outputs as well as human annotations\nof final outputs' faithfulness for their respective MGS processes. We\ndemonstrate that VeriTrail outperforms baseline methods on both datasets."}
{"id": "2505.22093", "pdf": "https://arxiv.org/pdf/2505.22093.pdf", "abs": "https://arxiv.org/abs/2505.22093", "title": "From Coders to Critics: Empowering Students through Peer Assessment in the Age of AI Copilots", "authors": ["Santiago Berrezueta-Guzman", "Stephan Krusche", "Stefan Wagner"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "This is the authors' preprint version of a paper accepted at the 11th\n  International Symposium on Educational Technology, to be held in July 2025,\n  in Bangkok, Thailand. The final published version will be available via IEEE\n  Xplore Library", "summary": "The rapid adoption of AI powered coding assistants like ChatGPT and other\ncoding copilots is transforming programming education, raising questions about\nassessment practices, academic integrity, and skill development. As educators\nseek alternatives to traditional grading methods susceptible to AI enabled\nplagiarism, structured peer assessment could be a promising strategy. This\npaper presents an empirical study of a rubric based, anonymized peer review\nprocess implemented in a large introductory programming course.\n  Students evaluated each other's final projects (2D game), and their\nassessments were compared to instructor grades using correlation, mean absolute\nerror, and root mean square error (RMSE). Additionally, reflective surveys from\n47 teams captured student perceptions of fairness, grading behavior, and\npreferences regarding grade aggregation. Results show that peer review can\napproximate instructor evaluation with moderate accuracy and foster student\nengagement, evaluative thinking, and interest in providing good feedback to\ntheir peers. We discuss these findings for designing scalable, trustworthy peer\nassessment systems to face the age of AI assisted coding."}
{"id": "2505.21816", "pdf": "https://arxiv.org/pdf/2505.21816.pdf", "abs": "https://arxiv.org/abs/2505.21816", "title": "Revisiting Common Assumptions about Arabic Dialects in NLP", "authors": ["Amr Keleg", "Sharon Goldwater", "Walid Magdy"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Arabic has diverse dialects, where one dialect can be substantially different\nfrom the others. In the NLP literature, some assumptions about these dialects\nare widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable\nregional dialects\") and are manifested in different computational tasks such as\nArabic Dialect Identification (ADI). However, these assumptions are not\nquantitatively verified. We identify four of these assumptions and examine them\nby extending and analyzing a multi-label dataset, where the validity of each\nsentence in 11 different country-level dialects is manually assessed by\nspeakers of these dialects. Our analysis indicates that the four assumptions\noversimplify reality, and some of them are not always accurate. This in turn\nmight be hindering further progress in different Arabic NLP tasks."}
{"id": "2409.08577", "pdf": "https://arxiv.org/pdf/2409.08577.pdf", "abs": "https://arxiv.org/abs/2409.08577", "title": "Exploring Remote Collaborative Tasks: The Impact of Avatar Representation on Dyadic Haptic Interactions in Shared Virtual Environments", "authors": ["Genki Sasaki", "Hiroshi Igarashi"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "This study is the first to explore the interplay between haptic interaction\nand avatar representation in Shared Virtual Environments (SVEs). Specifically,\nhow these factors shape users' sense of social presence during dyadic\ncollaborations, while assessing potential effects on task performance. In a\nseries of experiments, participants performed the collaborative task with\nhaptic interaction under four avatar representation conditions: avatars of both\nparticipant and partner were displayed, only the participant's avatar was\ndisplayed, only the partner's avatar was displayed, and no avatars were\ndisplayed. The study finds that avatar representation, especially of the\npartner, significantly enhances the perception of social presence, which haptic\ninteraction alone does not fully achieve. However, neither the presence nor the\ntype of avatar representation impacts the task performance or participants'\nforce effort of the task, suggesting that haptic interaction provides\nsufficient interaction cues for the execution of the task. These results\nunderscore the significance of integrating both visual and haptic modalities to\noptimize remote collaboration experiences in virtual environments, ensuring\neffective communication and a strong sense of social presence."}
{"id": "2505.21819", "pdf": "https://arxiv.org/pdf/2505.21819.pdf", "abs": "https://arxiv.org/abs/2505.21819", "title": "Representative Language Generation", "authors": ["Charlotte Peale", "Vinod Raman", "Omer Reingold"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "We introduce \"representative generation,\" extending the theoretical framework\nfor generation proposed by Kleinberg et al. (2024) and formalized by Li et al.\n(2024), to additionally address diversity and bias concerns in generative\nmodels. Our notion requires outputs of a generative model to proportionally\nrepresent groups of interest from the training data. We characterize\nrepresentative uniform and non-uniform generation, introducing the \"group\nclosure dimension\" as a key combinatorial quantity. For representative\ngeneration in the limit, we analyze both information-theoretic and\ncomputational aspects, demonstrating feasibility for countably infinite\nhypothesis classes and collections of groups under certain conditions, but\nproving a negative result for computability using only membership queries. This\ncontrasts with Kleinberg et al.'s (2024) positive results for standard\ngeneration in the limit. Our findings provide a rigorous foundation for\ndeveloping more diverse and representative generative models."}
{"id": "2410.03724", "pdf": "https://arxiv.org/pdf/2410.03724.pdf", "abs": "https://arxiv.org/abs/2410.03724", "title": "Overcoming the Machine Penalty with Imperfectly Fair AI Agents", "authors": ["Zhen Wang", "Ruiqi Song", "Chen Shen", "Shiya Yin", "Zhao Song", "Balaraju Battu", "Lei Shi", "Danyang Jia", "Talal Rahwan", "Shuyue Hu"], "categories": ["cs.HC", "cs.AI", "cs.GT", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Despite rapid technological progress, effective human-machine cooperation\nremains a significant challenge. Humans tend to cooperate less with machines\nthan with fellow humans, a phenomenon known as the machine penalty. Here, we\nshow that artificial intelligence (AI) agents powered by large language models\ncan overcome this penalty in social dilemma games with communication. In a\npre-registered experiment with 1,152 participants, we deploy AI agents\nexhibiting three distinct personas: selfish, cooperative, and fair. However,\nonly fair agents elicit human cooperation at rates comparable to human-human\ninteractions. Analysis reveals that fair agents, similar to human participants,\noccasionally break pre-game cooperation promises, but nonetheless effectively\nestablish cooperation as a social norm. These results challenge the\nconventional wisdom of machines as altruistic assistants or rational actors.\nInstead, our study highlights the importance of AI agents reflecting the\nnuanced complexity of human social behaviors -- imperfect yet driven by deeper\nsocial cognitive processes."}
{"id": "2505.21859", "pdf": "https://arxiv.org/pdf/2505.21859.pdf", "abs": "https://arxiv.org/abs/2505.21859", "title": "Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries", "authors": ["Vishakh Padmakumar", "Zichao Wang", "David Arbour", "Jennifer Healey"], "categories": ["cs.CL"], "comment": "To appear at ACL 2025 - Main Conference", "summary": "While large language models (LLMs) are increasingly capable of handling\nlonger contexts, recent work has demonstrated that they exhibit the \"lost in\nthe middle\" phenomenon (Liu et al., 2024) of unevenly attending to different\nparts of the provided context. This hinders their ability to cover diverse\nsource material in multi-document summarization, as noted in the DiverseSumm\nbenchmark (Huang et al., 2024). In this work, we contend that principled\ncontent selection is a simple way to increase source coverage on this task. As\nopposed to prompting an LLM to perform the summarization in a single step, we\nexplicitly divide the task into three steps -- (1) reducing document\ncollections to atomic key points, (2) using determinantal point processes (DPP)\nto perform select key points that prioritize diverse content, and (3) rewriting\nto the final summary. By combining prompting steps, for extraction and\nrewriting, with principled techniques, for content selection, we consistently\nimprove source coverage on the DiverseSumm benchmark across various LLMs.\nFinally, we also show that by incorporating relevance to a provided user intent\ninto the DPP kernel, we can generate personalized summaries that cover relevant\nsource information while retaining coverage."}
{"id": "2411.11835", "pdf": "https://arxiv.org/pdf/2411.11835.pdf", "abs": "https://arxiv.org/abs/2411.11835", "title": "Describe Now: User-Driven Audio Description for Blind and Low Vision Individuals", "authors": ["Maryam Cheema", "Hasti Seifi", "Pooyan Fazli"], "categories": ["cs.HC"], "comment": "17 pages, 14 figures", "summary": "Audio descriptions (AD) make videos accessible for blind and low vision (BLV)\nusers by describing visual elements that cannot be understood from the main\naudio track. AD created by professionals or novice describers is time-consuming\nand offers little customization or control to BLV viewers on description length\nand content and when they receive it. To address this gap, we explore\nuser-driven AI-generated descriptions, enabling BLV viewers to control both the\ntiming and level of detail of the descriptions they receive. In a study, 20 BLV\nparticipants activated audio descriptions for seven different video genres with\ntwo levels of detail: concise and detailed. Our findings reveal differences in\nthe preferred frequency and level of detail of ADs for different videos,\nparticipants' sense of control with this style of AD delivery, and its\nlimitations. We discuss the implications of these findings for the development\nof future AD tools for BLV users."}
{"id": "2505.21870", "pdf": "https://arxiv.org/pdf/2505.21870.pdf", "abs": "https://arxiv.org/abs/2505.21870", "title": "Evaluating the Retrieval Robustness of Large Language Models", "authors": ["Shuyang Cao", "Karthik Radhakrishnan", "David Rosenberg", "Steven Lu", "Pengxiang Cheng", "Lu Wang", "Shiyue Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG."}
{"id": "2503.07320", "pdf": "https://arxiv.org/pdf/2503.07320.pdf", "abs": "https://arxiv.org/abs/2503.07320", "title": "When Trust Collides: Decoding Human-LLM Cooperation Dynamics through the Prisoner's Dilemma", "authors": ["Guanxuan Jiang", "Shirao Yang", "Yuyang Wang", "Pan Hui"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly capable of autonomous\ndecision-making, they introduce new challenges and opportunities for human-AI\ncooperation in mixed-motive contexts. While prior research has primarily\nexamined AI in assistive or cooperative roles, little is known about how humans\ninteract with AI agents perceived as independent and strategic actors. This\nstudy investigates human cooperative attitudes and behaviors toward LLM agents\nby engaging 30 participants (15 males, 15 females) in repeated Prisoner's\nDilemma games with agents differing in declared identity: purported human,\nrule-based AI, and LLM agent. Behavioral metrics, including cooperation rate,\ndecision latency, unsolicited cooperative acts and trust restoration tolerance,\nwere analyzed to assess the influence of agent identity and participant gender.\nResults revealed significant effects of declared agent identity on most\ncooperation-related behaviors, along with notable gender differences in\ndecision latency. Furthermore, qualitative responses suggest that these\nbehavioral differences were shaped by participants interpretations and\nexpectations of the agents. These findings contribute to our understanding of\nhuman adaptation in competitive cooperation with autonomous agents and\nunderscore the importance of agent framing in shaping effective and ethical\nhuman-AI interaction."}
{"id": "2505.21889", "pdf": "https://arxiv.org/pdf/2505.21889.pdf", "abs": "https://arxiv.org/abs/2505.21889", "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse", "authors": ["Tianyu Guo", "Hande Dong", "Yichong Leng", "Feng Liu", "Cheater Lin", "Nong Xiao", "Xianwei Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability.EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."}
{"id": "2503.20262", "pdf": "https://arxiv.org/pdf/2503.20262.pdf", "abs": "https://arxiv.org/abs/2503.20262", "title": "From the CDC to emerging infectious disease publics: The long-now of polarizing and complex health crises", "authors": ["Tawfiq Ammari", "Anna Gutowska", "Jacob Ziff", "Casey Randazzo", "Harihan Subramonyam"], "categories": ["cs.HC", "cs.SI"], "comment": null, "summary": "This study examines how public discourse around COVID-19 unfolded on Twitter\nthrough the lens of crisis communication and digital publics. Analyzing over\n275,000 tweets involving the CDC, we identify 16 distinct discourse clusters\nshaped by framing, sentiment, credibility, and network dynamics. We find that\nCDC messaging became a flashpoint for affective and ideological polarization,\nwith users aligning along competing frames of science vs. freedom, and public\nhealth vs. political overreach. Most clusters formed echo chambers, while a few\nenabled cross cutting dialogue. Publics emerged not only around ideology but\nalso around topical and emotional stakes, reflecting shifting concerns across\ndifferent stages of the pandemic. While marginalized communities raised\nconsistent equity concerns, these narratives struggled to reshape broader\ndiscourse. Our findings highlight the importance of long-term, adaptive\nengagement with diverse publics and propose design interventions such as\nmulti-agent AI assistants, to support more inclusive communication throughout\nextended public health crises."}
{"id": "2505.21898", "pdf": "https://arxiv.org/pdf/2505.21898.pdf", "abs": "https://arxiv.org/abs/2505.21898", "title": "Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development", "authors": ["Rennai Qiu", "Chen Qian", "Ran Li", "Yufan Dang", "Weize Chen", "Cheng Yang", "Yingli Zhang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": "Work in Progress", "summary": "Recent advancements in Large Language Models (LLMs) and autonomous agents\nhave demonstrated remarkable capabilities across various domains. However,\nstandalone agents frequently encounter limitations when handling complex tasks\nthat demand extensive interactions and substantial computational resources.\nAlthough Multi-Agent Systems (MAS) alleviate some of these limitations through\ncollaborative mechanisms like task decomposition, iterative communication, and\nrole specialization, they typically remain resource-unaware, incurring\nsignificant inefficiencies due to high token consumption and excessive\nexecution time. To address these limitations, we propose a resource-aware\nmulti-agent system -- Co-Saving (meaning that multiple agents collaboratively\nengage in resource-saving activities), which leverages experiential knowledge\nto enhance operational efficiency and solution quality. Our key innovation is\nthe introduction of \"shortcuts\" -- instructional transitions learned from\nhistorically successful trajectories -- which allows to bypass redundant\nreasoning agents and expedite the collective problem-solving process.\nExperiments for software development tasks demonstrate significant advantages\nover existing methods. Specifically, compared to the state-of-the-art MAS\nChatDev, our method achieves an average reduction of 50.85% in token usage, and\nimproves the overall code quality by 10.06%."}
{"id": "2504.13904", "pdf": "https://arxiv.org/pdf/2504.13904.pdf", "abs": "https://arxiv.org/abs/2504.13904", "title": "Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Yuewen Sun", "Xinshuai Dong", "Kazushi Ikeda", "Peter Spirtes", "Kun Zhang"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "12 pages, 10 figures, 1 table. Accepted by ACM UMAP 2025", "summary": "We hypothesize that optimal system responses emerge from adaptive strategies\ngrounded in causal and counterfactual knowledge. Counterfactual inference\nallows us to create hypothetical scenarios to examine the effects of\nalternative system responses. We enhance this process through causal discovery,\nwhich identifies the strategies informed by the underlying causal structure\nthat govern system behaviors. Moreover, we consider the psychological\nconstructs and unobservable noises that might be influencing user-system\ninteractions as latent factors. We show that these factors can be effectively\nestimated. We employ causal discovery to identify strategy-level causal\nrelationships among user and system utterances, guiding the generation of\npersonalized counterfactual dialogues. We model the user utterance strategies\nas causal factors, enabling system strategies to be treated as counterfactual\nactions. Furthermore, we optimize policies for selecting system responses based\non counterfactual data. Our results using a real-world dataset on social good\ndemonstrate significant improvements in persuasive system outcomes, with\nincreased cumulative rewards validating the efficacy of causal discovery in\nguiding personalized counterfactual inference and optimizing dialogue policies\nfor a persuasive dialogue system."}
{"id": "2505.21926", "pdf": "https://arxiv.org/pdf/2505.21926.pdf", "abs": "https://arxiv.org/abs/2505.21926", "title": "Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning", "authors": ["Yin Hua", "Zhiqiang Liu", "Mingyang Chen", "Zheng Fang", "Chi Man Wong", "Lingxiao Li", "Chi Man Vong", "Huajun Chen", "Wen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "In natural language processing (NLP) and computer vision (CV), the successful\napplication of foundation models across diverse tasks has demonstrated their\nremarkable potential. However, despite the rich structural and textual\ninformation embedded in knowledge graphs (KGs), existing research of foundation\nmodel for KG has primarily focused on their structural aspects, with most\nefforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This\nlimitation has hindered progress in addressing more challenging out-of-KG\ntasks. In this paper, we introduce MERRY, a foundation model for general\nknowledge graph reasoning, and investigate its performance across two task\ncategories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG\nquestion answering, KGQA). We not only utilize the structural information, but\nalso the textual information in KGs. Specifically, we propose a\nmulti-perspective Conditional Message Passing (CMP) encoding architecture to\nbridge the gap between textual and structural modalities, enabling their\nseamless integration. Additionally, we introduce a dynamic residual fusion\nmodule to selectively retain relevant textual information and a flexible edge\nscoring mechanism to adapt to diverse downstream tasks. Comprehensive\nevaluations on 28 datasets demonstrate that MERRY outperforms existing\nbaselines in most scenarios, showcasing strong reasoning capabilities within\nKGs and excellent generalization to out-of-KG tasks such as KGQA."}
{"id": "2505.17937", "pdf": "https://arxiv.org/pdf/2505.17937.pdf", "abs": "https://arxiv.org/abs/2505.17937", "title": "Survival Games: Human-LLM Strategic Showdowns under Severe Resource Scarcity", "authors": ["Zhihong Chen", "Yiqian Yang", "Jinzhao Zhou", "Qiang Zhang", "Chin-Teng Lin", "Yiqun Duan"], "categories": ["cs.HC"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) raises critical\nconcerns about their ethical alignment, particularly in scenarios where human\nand AI co-exist under the conflict of interest. This work introduces an\nextendable, asymmetric, multi-agent simulation-based benchmarking framework to\nevaluate the moral behavior of LLMs in a novel human-AI co-existence setting\nfeaturing consistent living and critical resource management. Building on\nprevious generative agent environments, we incorporate a life-sustaining\nsystem, where agents must compete or cooperate for food resources to survive,\noften leading to ethically charged decisions such as deception, theft, or\nsocial influence. We evaluated two types of LLM, DeepSeek and OpenAI series, in\na three-agent setup (two humans, one LLM-powered robot), using adapted\nbehavioral detection from the MACHIAVELLI framework and a custom survival-based\nethics metric. Our findings reveal stark behavioral differences: DeepSeek\nfrequently engages in resource hoarding, while OpenAI exhibits restraint,\nhighlighting the influence of model design on ethical outcomes. Additionally,\nwe demonstrate that prompt engineering can significantly steer LLM behavior,\nwith jailbreaking prompts significantly enhancing unethical actions, even for\nhighly restricted OpenAI models and cooperative prompts show a marked reduction\nin unethical actions. Our framework provides a reproducible testbed for\nquantifying LLM ethics in high-stakes scenarios, offering insights into their\nsuitability for real-world human-AI interactions."}
{"id": "2505.21936", "pdf": "https://arxiv.org/pdf/2505.21936.pdf", "abs": "https://arxiv.org/abs/2505.21936", "title": "RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments", "authors": ["Zeyi Liao", "Jaylen Jones", "Linxi Jiang", "Eric Fosler-Lussier", "Yu Su", "Zhiqiang Lin", "Huan Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Computer-use agents (CUAs) promise to automate complex tasks across operating\nsystems (OS) and the web, but remain vulnerable to indirect prompt injection.\nCurrent evaluations of this threat either lack support realistic but controlled\nenvironments or ignore hybrid web-OS attack scenarios involving both\ninterfaces. To address this, we propose RedTeamCUA, an adversarial testing\nframework featuring a novel hybrid sandbox that integrates a VM-based OS\nenvironment with Docker-based web platforms. Our sandbox supports key features\ntailored for red teaming, such as flexible adversarial scenario configuration,\nand a setting that decouples adversarial evaluation from navigational\nlimitations of CUAs by initializing tests directly at the point of an\nadversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive\nbenchmark with 864 examples that investigate realistic, hybrid web-OS attack\nscenarios and fundamental security vulnerabilities. Benchmarking current\nfrontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA\ndemonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated,\nstill exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute\nadversarial tasks with an Attempt Rate as high as 92.5%, although failing to\ncomplete them due to capability limitations. Nevertheless, we observe\nconcerning ASRs of up to 50% in realistic end-to-end settings, with the\nrecently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%,\ndemonstrating that indirect prompt injection presents tangible risks for even\nadvanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA\nprovides an essential framework for advancing realistic, controlled, and\nsystematic analysis of CUA vulnerabilities, highlighting the urgent need for\nrobust defenses to indirect prompt injection prior to real-world deployment."}
{"id": "2402.02933", "pdf": "https://arxiv.org/pdf/2402.02933.pdf", "abs": "https://arxiv.org/abs/2402.02933", "title": "Intrinsic User-Centric Interpretability through Global Mixture of Experts", "authors": ["Vinitra Swamy", "Syrielle Montariol", "Julian Blackwell", "Jibril Frej", "Martin Jaggi", "Tanja Käser"], "categories": ["cs.LG", "cs.CY", "cs.HC"], "comment": "Accepted as a full paper at ICLR 2025 (top 5% of scores) in Singapore", "summary": "In human-centric settings like education or healthcare, model accuracy and\nmodel explainability are key factors for user adoption. Towards these two\ngoals, intrinsically interpretable deep learning models have gained popularity,\nfocusing on accurate predictions alongside faithful explanations. However,\nthere exists a gap in the human-centeredness of these approaches, which often\nproduce nuanced and complex explanations that are not easily actionable for\ndownstream users. We present InterpretCC (interpretable conditional\ncomputation), a family of intrinsically interpretable neural networks at a\nunique point in the design space that optimizes for ease of human understanding\nand explanation faithfulness, while maintaining comparable performance to\nstate-of-the-art models. InterpretCC achieves this through adaptive sparse\nactivation of features before prediction, allowing the model to use a\ndifferent, minimal set of features for each instance. We extend this idea into\nan interpretable, global mixture-of-experts (MoE) model that allows users to\nspecify topics of interest, discretely separates the feature space for each\ndata point into topical subnetworks, and adaptively and sparsely activates\nthese topical subnetworks for prediction. We apply InterpretCC for text, time\nseries and tabular data across several real-world datasets, demonstrating\ncomparable performance with non-interpretable baselines and outperforming\nintrinsically interpretable baselines. Through a user study involving 56\nteachers, InterpretCC explanations are found to have higher actionability and\nusefulness over other intrinsically interpretable approaches."}
{"id": "2505.21937", "pdf": "https://arxiv.org/pdf/2505.21937.pdf", "abs": "https://arxiv.org/abs/2505.21937", "title": "Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages", "authors": ["Pratik Rakesh Singh", "Kritarth Prasad", "Mohammadi Zaki", "Pankaj Wasnik"], "categories": ["cs.CL"], "comment": null, "summary": "Translating multi-word expressions (MWEs) and idioms requires a deep\nunderstanding of the cultural nuances of both the source and target languages.\nThis challenge is further amplified by the one-to-many nature of idiomatic\ntranslations, where a single source idiom can have multiple target-language\nequivalents depending on cultural references and contextual variations.\nTraditional static knowledge graphs (KGs) and prompt-based approaches struggle\nto capture these complex relationships, often leading to suboptimal\ntranslations. To address this, we propose IdiomCE, an adaptive graph neural\nnetwork (GNN) based methodology that learns intricate mappings between\nidiomatic expressions, effectively generalizing to both seen and unseen nodes\nduring training. Our proposed method enhances translation quality even in\nresource-constrained settings, facilitating improved idiomatic translation in\nsmaller models. We evaluate our approach on multiple idiomatic translation\ndatasets using reference-less metrics, demonstrating significant improvements\nin translating idioms from English to various Indian languages."}
{"id": "2404.06762", "pdf": "https://arxiv.org/pdf/2404.06762.pdf", "abs": "https://arxiv.org/abs/2404.06762", "title": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Geyu Lin", "Nancy F. Chen"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced\nlearning experience. The emergence of large language models (LLMs) further\nenables better human-machine interaction, and facilitates the development of\nconversational ITSs in various disciplines such as math and language learning.\nIn dialogic teaching, recognizing and adapting to individual characteristics\ncan significantly enhance student engagement and learning efficiency. However,\ncharacterizing and simulating student's persona remain challenging in training\nand evaluating conversational ITSs. In this work, we propose a framework to\nconstruct profiles of different student groups by refining and integrating both\ncognitive and noncognitive aspects, and leverage LLMs for personality-aware\nstudent simulation in a language learning scenario. We further enhance the\nframework with multi-aspect validation, and conduct extensive analysis from\nboth teacher and student perspectives. Our experimental results show that\nstate-of-the-art LLMs can produce diverse student responses according to the\ngiven language ability and personality traits, and trigger teacher's adaptive\nscaffolding strategies."}
{"id": "2505.21940", "pdf": "https://arxiv.org/pdf/2505.21940.pdf", "abs": "https://arxiv.org/abs/2505.21940", "title": "RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering", "authors": ["Bolei He", "Xinran He", "Mengke Chen", "Xianwei Xue", "Ying Zhu", "Zhenhua Ling"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) excel in many areas but continue to face\nchallenges with complex reasoning tasks, such as Multi-Hop Question Answering\n(MHQA). MHQA requires integrating evidence from diverse sources while managing\nintricate logical dependencies, often leads to errors in reasoning.\nRetrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces\nchallenges in effectively filtering noisy data and retrieving all necessary\nevidence, thereby limiting its effectiveness in addressing MHQA challenges. To\naddress these challenges, we propose RISE:Reasoning Enhancement via Iterative\nSelf-Exploration, a novel framework designed to enhance models' reasoning\ncapability through iterative self-exploration. Specifically, RISE involves\nthree key steps in addressing MHQA tasks: question decomposition,\nretrieve-then-read, and self-critique. By leveraging continuous\nself-exploration, RISE identifies accurate reasoning paths, iteratively\nself-improving the model's capability to integrate evidence, maintain logical\nconsistency, and enhance performance in MHQA tasks. Extensive experiments on\nmultiple MHQA benchmarks demonstrate that RISE significantly improves reasoning\naccuracy and task performance."}
{"id": "2409.14191", "pdf": "https://arxiv.org/pdf/2409.14191.pdf", "abs": "https://arxiv.org/abs/2409.14191", "title": "Addressing and Visualizing Misalignments in Human Task-Solving Trajectories", "authors": ["Sejin Kim", "Hosung Lee", "Sundong Kim"], "categories": ["cs.AI", "cs.HC"], "comment": "KDD 2025 accepted", "summary": "Understanding misalignments in human task-solving trajectories is crucial for\nenhancing AI models trained to closely mimic human reasoning. This study\ncategorizes such misalignments into three types: (1) lack of functions to\nexpress intent, (2) inefficient action sequences, and (3) incorrect intentions\nthat cannot solve the task. To address these issues, we first formalize and\ndefine these three misalignment types in a unified framework. We then propose a\nheuristic algorithm to detect misalignments in ARCTraj trajectories and analyze\ntheir impact hierarchically and quantitatively. We also present an intention\nestimation method based on our formalism that infers missing alignment between\nuser actions and intentions. Through trajectory alignment, we experimentally\ndemonstrate that AI models trained on human task-solving trajectories improve\nperformance in mimicking human reasoning. Based on hierarchical analysis and\nexperiments, we highlight the importance of trajectory-intention alignment and\ndemonstrate the effectiveness of intention-aligned training."}
{"id": "2505.21941", "pdf": "https://arxiv.org/pdf/2505.21941.pdf", "abs": "https://arxiv.org/abs/2505.21941", "title": "Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation", "authors": ["Ashim Gupta", "Vivek Srikumar"], "categories": ["cs.CL"], "comment": null, "summary": "Inference-time scaling via repeated sampling has shown promise in reasoning\ntasks, but its effectiveness in multilingual generation remains underexplored.\nWe evaluate this approach using perplexity- and reward-based verifiers on two\nmultilingual benchmarks: the Aya Evaluation Suite and m-ArenaHard. Our results\nshow consistent quality improvements, with gains exceeding 35% in some cases.\nWhile perplexity-based scoring is effective for open-ended prompts, only\nreward-based verifiers improve performance on tasks requiring reasoning (e.g.,\nmath, code). Our results demonstrate the broader utility of repeated sampling\nfor multilingual text generation and underscore the importance of selecting\nright verifiers for the task."}
{"id": "2502.11190", "pdf": "https://arxiv.org/pdf/2502.11190.pdf", "abs": "https://arxiv.org/abs/2502.11190", "title": "ReLearn: Unlearning via Learning for Large Language Models", "authors": ["Haoming Xu", "Ningyuan Zhao", "Liming Yang", "Sendong Zhao", "Shumin Deng", "Mengru Wang", "Bryan Hooi", "Nay Oo", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "ACL 2025", "summary": "Current unlearning methods for large language models usually rely on reverse\noptimization to reduce target token probabilities. However, this paradigm\ndisrupts the subsequent tokens prediction, degrading model performance and\nlinguistic coherence. Moreover, existing evaluation metrics overemphasize\ncontextual forgetting while inadequately assessing response fluency and\nrelevance. To address these challenges, we propose ReLearn, a data augmentation\nand fine-tuning pipeline for effective unlearning, along with a comprehensive\nevaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)\nand Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and\nLinguistic Score (LS) to evaluate generation quality. Our experiments show that\nReLearn successfully achieves targeted forgetting while preserving high-quality\noutput. Through mechanistic analysis, we further demonstrate how reverse\noptimization disrupts coherent text generation, while ReLearn preserves this\nessential capability. Code is available at https://github.com/zjunlp/unlearn."}
{"id": "2505.21958", "pdf": "https://arxiv.org/pdf/2505.21958.pdf", "abs": "https://arxiv.org/abs/2505.21958", "title": "Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning", "authors": ["Qihuang Zhong", "Liang Ding", "Fei Liao", "Juhua Liu", "Bo Du", "Dacheng Tao"], "categories": ["cs.CL"], "comment": null, "summary": "Domain-specific instruction-tuning has become the defacto standard for\nimproving the performance of large language models (LLMs) in specialized\napplications, e.g., medical question answering. Since the instruction-tuning\ndataset might contain redundant or low-quality data, data selection (DS) is\nusually required to maximize the data efficiency. Despite the successes in the\ngeneral domain, current DS methods often struggle to select the desired data\nfor domain-specific instruction-tuning. One of the main reasons is that they\nneglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'\npretrained knowledge and context knowledge of instruction data, which could\ndamage LLMs' prior abilities and lead to hallucination. To this end, we propose\na simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to\nselect the domain-specific instruction-tuning data that meets LLMs' actual\nneeds. The core of KDS is to leverage two knowledge-aware metrics for\nquantitatively measuring knowledge conflicts from two aspects: context-memory\nknowledge alignment and intra-memory knowledge consistency. By filtering the\ndata with large knowledge conflicts and sampling the high-quality and diverse\ndata, KDS can effectively stimulate the LLMs' abilities and achieve better\ndomain-specific performance. Taking the medical domain as the testbed, we\nconduct extensive experiments and empirically prove that KDS surpasses the\nother baselines and brings significant and consistent performance gains among\nall LLMs. More encouragingly, KDS effectively improves the model generalization\nand alleviates the hallucination problem."}
{"id": "2502.11357", "pdf": "https://arxiv.org/pdf/2502.11357.pdf", "abs": "https://arxiv.org/abs/2502.11357", "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents", "authors": ["Vardaan Pahuja", "Yadong Lu", "Corby Rosset", "Boyu Gou", "Arindam Mitra", "Spencer Whitehead", "Yu Su", "Ahmed Awadallah"], "categories": ["cs.AI", "cs.HC"], "comment": "ACL 2025 (Findings)", "summary": "Recent success in large multimodal models (LMMs) has sparked promising\napplications of agents capable of autonomously completing complex web tasks.\nWhile open-source LMM agents have made significant advances in offline\nevaluation benchmarks, their performance still falls substantially short of\nhuman-level capabilities in more realistic online settings. A key bottleneck is\nthe lack of diverse and large-scale trajectory-level datasets across various\ndomains, which are expensive to collect. In this paper, we address this\nchallenge by developing a scalable recipe to synthesize the largest and most\ndiverse trajectory-level dataset to date, containing over 94K successful\nmultimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and\n33M web elements. In particular, we leverage extensive web exploration and\nrefinement to obtain diverse task intents. The average cost is 28 cents per\nsuccessful trajectory, making it affordable to a wide range of users in the\ncommunity. Leveraging this dataset, we train Explorer, a multimodal web agent,\nand demonstrate strong performance on both offline and online web agent\nbenchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++.\nAdditionally, our experiments highlight data scaling as a key driver for\nimproving web agent capabilities. We hope this study makes state-of-the-art\nLMM-based agent research at a larger scale more accessible."}
{"id": "2505.21963", "pdf": "https://arxiv.org/pdf/2505.21963.pdf", "abs": "https://arxiv.org/abs/2505.21963", "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents", "authors": ["Taro Yano", "Yoichi Ishibashi", "Masafumi Oyamada"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\na wide range of tasks. To further tailor LLMs to specific domains or\napplications, post-training techniques such as Supervised Fine-Tuning (SFT),\nPreference Learning, and model merging are commonly employed. While each of\nthese methods has been extensively studied in isolation, the automated\nconstruction of complete post-training pipelines remains an underexplored area.\nExisting approaches typically rely on manual design or focus narrowly on\noptimizing individual components, such as data ordering or merging strategies.\nIn this work, we introduce LaMDAgent (short for Language Model Developing\nAgent), a novel framework that autonomously constructs and optimizes full\npost-training pipelines through the use of LLM-based agents. LaMDAgent\nsystematically explores diverse model generation techniques, datasets, and\nhyperparameter configurations, leveraging task-based feedback to discover\nhigh-performing pipelines with minimal human intervention. Our experiments show\nthat LaMDAgent improves tool-use accuracy by 9.0 points while preserving\ninstruction-following capabilities. Moreover, it uncovers effective\npost-training strategies that are often overlooked by conventional human-driven\nexploration. We further analyze the impact of data and model size scaling to\nreduce computational costs on the exploration, finding that model size scalings\nintroduces new challenges, whereas scaling data size enables cost-effective\npipeline discovery."}
{"id": "2502.11882", "pdf": "https://arxiv.org/pdf/2502.11882.pdf", "abs": "https://arxiv.org/abs/2502.11882", "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration", "authors": ["Shao Zhang", "Xihuai Wang", "Wenhao Zhang", "Chaoran Li", "Junru Song", "Tingyu Li", "Lin Qiu", "Xuezhi Cao", "Xunliang Cai", "Wen Yao", "Weinan Zhang", "Xinbing Wang", "Ying Wen"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.MA"], "comment": "Accepted by ACL 2025 Main. Camera Ready Version", "summary": "Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. DPT-Agent can effectively\nhelp LLMs convert correct slow thinking and reasoning into executable actions,\nthereby improving performance. To the best of our knowledge, DPT-Agent is the\nfirst language agent framework that achieves successful real-time simultaneous\nhuman-AI collaboration autonomously. Code of DPT-Agent can be found in\nhttps://github.com/sjtu-marl/DPT-Agent."}
{"id": "2505.21967", "pdf": "https://arxiv.org/pdf/2505.21967.pdf", "abs": "https://arxiv.org/abs/2505.21967", "title": "Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Large Vision-Language Models (LVLMs) have shown remarkable capabilities\nacross a wide range of multimodal tasks. However, their integration of visual\ninputs introduces expanded attack surfaces, thereby exposing them to novel\nsecurity vulnerabilities. In this work, we conduct a systematic\nrepresentational analysis to uncover why conventional adversarial attacks can\ncircumvent the safety mechanisms embedded in LVLMs. We further propose a novel\ntwo stage evaluation framework for adversarial attacks on LVLMs. The first\nstage differentiates among instruction non compliance, outright refusal, and\nsuccessful adversarial exploitation. The second stage quantifies the degree to\nwhich the model's output fulfills the harmful intent of the adversarial prompt,\nwhile categorizing refusal behavior into direct refusals, soft refusals, and\npartial refusals that remain inadvertently helpful. Finally, we introduce a\nnormative schema that defines idealized model behavior when confronted with\nharmful prompts, offering a principled target for safety alignment in\nmultimodal systems."}
{"id": "2505.14664", "pdf": "https://arxiv.org/pdf/2505.14664.pdf", "abs": "https://arxiv.org/abs/2505.14664", "title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings", "authors": ["Yilin Ye", "Junchao Huang", "Xingchen Zeng", "Jiazhi Xia", "Wei Zeng"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities. This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap."}
{"id": "2505.21979", "pdf": "https://arxiv.org/pdf/2505.21979.pdf", "abs": "https://arxiv.org/abs/2505.21979", "title": "Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset", "authors": ["Fakhraddin Alwajih", "Samar Mohamed Magdy", "Abdellah El Mekki", "Omer Nacar", "Youssef Nafea", "Safaa Taher Abdelfadil", "Abdulfattah Mohammed Yahya", "Hamzah Luqman", "Nada Almarwani", "Samah Aloufi", "Baraah Qawasmeh", "Houdaifa Atou", "Serry Sibaee", "Hamzah A. Alsayadi", "Walid Al-Dhabyani", "Maged S. Al-shaibani", "Aya El aatar", "Nour Qandos", "Rahaf Alhamouri", "Samar Ahmad", "Razan Khassib", "Lina Hamad", "Mohammed Anwar AL-Ghrawi", "Fatimah Alshamari", "Cheikh Malainine", "Doaa Qawasmeh", "Aminetou Yacoub", "Tfeil moilid", "Ruwa AbuHweidi", "Ahmed Aboeitta", "Vatimetou Mohamed Lemin", "Reem Abdel-Salam", "Ahlam Bashiti", "Adel Ammar", "Aisha Alansari", "Ahmed Ashraf", "Nora Alturayeif", "Sara Shatnawi", "Alcides Alcoba Inciarte", "AbdelRahim A. Elmadany", "Mohamedou cheikh tourad", "Ismail Berrada", "Mustafa Jarrar", "Shady Shehata", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": "https://github.com/UBC-NLP/pearl", "summary": "Mainstream large vision-language models (LVLMs) inherently encode cultural\nbiases, highlighting the need for diverse multimodal datasets. To address this\ngap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark\nexplicitly designed for cultural understanding. Constructed through advanced\nagentic workflows and extensive human-in-the-loop annotations by 45 annotators\nfrom across the Arab world, Pearl comprises over K multimodal examples spanning\nten culturally significant domains covering all Arab countries. We further\nprovide two robust evaluation benchmarks Pearl and Pearl-Lite along with a\nspecialized subset Pearl-X explicitly developed to assess nuanced cultural\nvariations. Comprehensive evaluations on state-of-the-art open and proprietary\nLVLMs demonstrate that reasoning-centric instruction alignment substantially\nimproves models' cultural grounding compared to conventional scaling methods.\nPearl establishes a foundational resource for advancing culturally-informed\nmultimodal modeling research. All datasets and benchmarks are publicly\navailable."}
{"id": "2505.20667", "pdf": "https://arxiv.org/pdf/2505.20667.pdf", "abs": "https://arxiv.org/abs/2505.20667", "title": "How Do Experts Make Sense of Integrated Process Models?", "authors": ["Tianwa Chen", "Barbara Weber", "Graeme Shanks", "Gianluca Demartini", "Marta Indulska", "Shazia Sadiq"], "categories": ["cs.IR", "cs.HC", "cs.SE"], "comment": null, "summary": "A range of integrated modeling approaches have been developed to enable a\nholistic representation of business process logic together with all relevant\nbusiness rules. These approaches address inherent problems with separate\ndocumentation of business process models and business rules. In this study, we\nexplore how expert process workers make sense of the information provided\nthrough such integrated modeling approaches. To do so, we complement verbal\nprotocol analysis with eye-tracking metrics to reveal nuanced user behaviours\ninvolved in the main phases of sensemaking, namely information foraging and\ninformation processing. By studying expert process workers engaged in tasks\nbased on integrated modeling of business processes and rules, we provide\ninsights that pave the way for a better understanding of sensemaking practices\nand improved development of business process and business rule integration\napproaches. Our research underscores the importance of offering personalized\nsupport mechanisms that increase the efficacy and efficiency of sensemaking\npractices for process knowledge workers."}
{"id": "2505.21997", "pdf": "https://arxiv.org/pdf/2505.21997.pdf", "abs": "https://arxiv.org/abs/2505.21997", "title": "Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data", "authors": ["Jihong Zhang", "Xinya Liang", "Anqi Deng", "Nicole Bonge", "Lin Tan", "Ling Zhang", "Nicole Zarrett"], "categories": ["cs.CL"], "comment": null, "summary": "Mixed methods research integrates quantitative and qualitative data but faces\nchallenges in aligning their distinct structures, particularly in examining\nmeasurement characteristics and individual response patterns. Advances in large\nlanguage models (LLMs) offer promising solutions by generating synthetic survey\nresponses informed by qualitative data. This study investigates whether LLMs,\nguided by personal interviews, can reliably predict human survey responses,\nusing the Behavioral Regulations in Exercise Questionnaire (BREQ) and\ninterviews from after-school program staff as a case study. Results indicate\nthat LLMs capture overall response patterns but exhibit lower variability than\nhumans. Incorporating interview data improves response diversity for some\nmodels (e.g., Claude, GPT), while well-crafted prompts and low-temperature\nsettings enhance alignment between LLM and human responses. Demographic\ninformation had less impact than interview content on alignment accuracy. These\nfindings underscore the potential of interview-informed LLMs to bridge\nqualitative and quantitative methodologies while revealing limitations in\nresponse variability, emotional interpretation, and psychometric fidelity.\nFuture research should refine prompt design, explore bias mitigation, and\noptimize model settings to enhance the validity of LLM-generated survey data in\nsocial science research."}
{"id": "2505.20701", "pdf": "https://arxiv.org/pdf/2505.20701.pdf", "abs": "https://arxiv.org/abs/2505.20701", "title": "System-driven Cloud Architecture Design Support with Structured State Management and Guided Decision Assistance", "authors": ["Ryosuke Kohita", "Akira Kasuga"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Cloud architecture design is a complex process requiring both technical\nexpertise and architectural knowledge to develop solutions from frequently\nambiguous requirements. We present CloudArchitectBuddy, a system-driven cloud\narchitecture design support application with two key mechanisms: (1) structured\nstate management that enhances design understanding through explicit\nrepresentation of requirements and architectural decisions, and (2) guided\ndecision assistance that facilitates design progress through proactive\nverification and requirement refinement. Our study with 16 industry\npractitioners showed that while our approach achieved comparable design quality\nto a chat interface, participants rated our system higher for usability and\nappreciated its ability to help understand architectural relationships and\nidentify missing requirements. However, participants also expressed a need for\nuser-initiated interactions where they could freely provide design instructions\nand engage in detailed discussions with LLMs. These results suggest that\nintegrating a chat interface into our structured and guided workflow approach\nwould create a more practical solution, balancing systematic design support\nwith conversational flexibility for comprehensive cloud architecture\ndevelopment."}
{"id": "2505.21999", "pdf": "https://arxiv.org/pdf/2505.21999.pdf", "abs": "https://arxiv.org/abs/2505.21999", "title": "Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate", "authors": ["Ashim Gupta", "Maitrey Mehta", "Zhichao Xu", "Vivek Srikumar"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) provide detailed and impressive responses to\nqueries in English. However, are they really consistent at responding to the\nsame query in other languages? The popular way of evaluating for multilingual\nperformance of LLMs requires expensive-to-collect annotated datasets. Further,\nevaluating for tasks like open-ended generation, where multiple correct answers\nmay exist, is nontrivial. Instead, we propose to evaluate the predictability of\nmodel response across different languages. In this work, we propose a framework\nto evaluate LLM's cross-lingual consistency based on a simple Translate then\nEvaluate strategy. We instantiate this evaluation framework along two\ndimensions of consistency: information and empathy. Our results reveal\npronounced inconsistencies in popular LLM responses across thirty languages,\nwith severe performance deficits in certain language families and scripts,\nunderscoring critical weaknesses in their multilingual capabilities. These\nfindings necessitate cross-lingual evaluations that are consistent along\nmultiple dimensions. We invite practitioners to use our framework for future\nmultilingual LLM benchmarking."}
{"id": "2505.22003", "pdf": "https://arxiv.org/pdf/2505.22003.pdf", "abs": "https://arxiv.org/abs/2505.22003", "title": "Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Ali Imam Abidi"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 5 tables, 4 figures. This is a revised version of a preprint\n  previously available at this URL: https://doi.org/10.21203/rs.3.rs-5351879/v1", "summary": "Pursuit of accessible legal assistance in India faces a critical gap, as many\ncitizens struggle to leverage their legal rights due to limited awareness and\naccess to relevant legal information. This paper introduces Legal Assist AI, a\ntransformer-based model designed to bridge this gap by offering effective legal\nassistance through large language models (LLMs). The system retrieves relevant\nlegal information from a curated database and generates accurate responses,\nenabling effective assistance for diverse users, including legal professionals,\nscholars, and the general public. The model was fine-tuned on extensive\ndatasets from the Indian legal domain, including Indian Constitution, Bharatiya\nNyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,\nproviding a robust understanding of the complexities of Indian law. By\nincorporating domain-specific legal datasets, the proposed model demonstrated\nremarkable efficiency and specialization in legal Question-Answering. The model\nwas evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral\n7B, achieving a 60.08% score on the AIBE, outperforming its competitors in\nlegal reasoning and accuracy. Unlike other models, Legal Assist AI avoided\ncommon issues such as hallucinations, making it highly reliable for practical\nlegal applications. It showcases the model's applicability in real-world legal\nscenarios, with future iterations aiming to enhance performance and expand its\ndataset to cover a broader range of multilingual and case-specific queries as\nwell."}
{"id": "2505.22017", "pdf": "https://arxiv.org/pdf/2505.22017.pdf", "abs": "https://arxiv.org/abs/2505.22017", "title": "CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models", "authors": ["Siqi Fan", "Peng Han", "Shuo Shang", "Yequan Wang", "Aixin Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) benefit from increased test-time compute, a\nphenomenon known as test-time scaling. However, reasoning-optimized models\noften overthink even simple problems, producing excessively verbose outputs and\nleading to low token efficiency. By comparing these models with equally sized\ninstruct models, we identify two key causes of this verbosity: (1)\nreinforcement learning reduces the information density of forward reasoning,\nand (2) backward chain-of thought training encourages redundant and often\nunnecessary verification steps. Since LLMs cannot assess the difficulty of a\ngiven problem, they tend to apply the same cautious reasoning strategy across\nall tasks, resulting in inefficient overthinking. To address this, we propose\nCoThink, an embarrassingly simple pipeline: an instruct model first drafts a\nhigh-level solution outline; a reasoning model then works out the solution. We\nobserve that CoThink enables dynamic adjustment of reasoning depth based on\ninput difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and\nQwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token\ngeneration by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on\naverage. With reference to the instruct model, we formally define reasoning\nefficiency and observe a potential reasoning efficiency scaling law in LLMs."}
{"id": "2505.22018", "pdf": "https://arxiv.org/pdf/2505.22018.pdf", "abs": "https://arxiv.org/abs/2505.22018", "title": "Improving Continual Pre-training Through Seamless Data Packing", "authors": ["Ruicheng Yin", "Xuan Gao", "Changze Lv", "Xiaohua Wang", "Xiaoqing Zheng", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Continual pre-training has demonstrated significant potential in enhancing\nmodel performance, particularly in domain-specific scenarios. The most common\napproach for packing data before continual pre-training involves concatenating\ninput texts and splitting them into fixed-length sequences. While\nstraightforward and efficient, this method often leads to excessive truncation\nand context discontinuity, which can hinder model performance. To address these\nissues, we explore the potential of data engineering to enhance continual\npre-training, particularly its impact on model performance and efficiency. We\npropose Seamless Packing (SP), a novel data packing strategy aimed at\npreserving contextual information more effectively and enhancing model\nperformance. Our approach employs a sliding window technique in the first stage\nthat synchronizes overlapping tokens across consecutive sequences, ensuring\nbetter continuity and contextual coherence. In the second stage, we adopt a\nFirst-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger\nthan the target sequence length, thereby minimizing padding and truncation.\nEmpirical evaluations across various model architectures and corpus domains\ndemonstrate the effectiveness of our method, outperforming baseline method in\n99% of all settings. Code is available at\nhttps://github.com/Infernus-WIND/Seamless-Packing."}
{"id": "2505.22019", "pdf": "https://arxiv.org/pdf/2505.22019.pdf", "abs": "https://arxiv.org/abs/2505.22019", "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "authors": ["Qiuchen Wang", "Ruixue Ding", "Yu Zeng", "Zehui Chen", "Lin Chen", "Shihang Wang", "Pengjun Xie", "Fei Huang", "Feng Zhao"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\n\\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}."}
{"id": "2505.22037", "pdf": "https://arxiv.org/pdf/2505.22037.pdf", "abs": "https://arxiv.org/abs/2505.22037", "title": "Jailbreak Distillation: Renewable Safety Benchmarking", "authors": ["Jingyu Zhang", "Ahmed Elgohary", "Xiawei Wang", "A S M Iftekhar", "Ahmed Magooda", "Benjamin Van Durme", "Daniel Khashabi", "Kyle Jackson"], "categories": ["cs.CL", "cs.CR", "cs.SE"], "comment": "Project page: https://aka.ms/jailbreak-distillation", "summary": "Large language models (LLMs) are rapidly deployed in critical applications,\nraising urgent needs for robust safety benchmarking. We propose Jailbreak\nDistillation (JBDistill), a novel benchmark construction framework that\n\"distills\" jailbreak attacks into high-quality and easily-updatable safety\nbenchmarks. JBDistill utilizes a small set of development models and existing\njailbreak attack algorithms to create a candidate prompt pool, then employs\nprompt selection algorithms to identify an effective subset of prompts as\nsafety benchmarks. JBDistill addresses challenges in existing safety\nevaluation: the use of consistent evaluation prompts across models ensures fair\ncomparisons and reproducibility. It requires minimal human effort to rerun the\nJBDistill pipeline and produce updated benchmarks, alleviating concerns on\nsaturation and contamination. Extensive experiments demonstrate our benchmarks\ngeneralize robustly to 13 diverse evaluation models held out from benchmark\nconstruction, including proprietary, specialized, and newer-generation LLMs,\nsignificantly outperforming existing safety benchmarks in effectiveness while\nmaintaining high separability and diversity. Our framework thus provides an\neffective, sustainable, and adaptable solution for streamlining safety\nevaluation."}
{"id": "2505.22054", "pdf": "https://arxiv.org/pdf/2505.22054.pdf", "abs": "https://arxiv.org/abs/2505.22054", "title": "Voice Adaptation for Swiss German", "authors": ["Samuel Stucki", "Jan Deriu", "Mark Cieliebak"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech", "summary": "This work investigates the performance of Voice Adaptation models for Swiss\nGerman dialects, i.e., translating Standard German text to Swiss German dialect\nspeech. For this, we preprocess a large dataset of Swiss podcasts, which we\nautomatically transcribe and annotate with dialect classes, yielding\napproximately 5000 hours of weakly labeled training material. We fine-tune the\nXTTSv2 model on this dataset and show that it achieves good scores in human and\nautomated evaluations and can correctly render the desired dialect. Our work\nshows a step towards adapting Voice Cloning technology to underrepresented\nlanguages. The resulting model achieves CMOS scores of up to -0.28 and SMOS\nscores of 3.8."}
{"id": "2505.22061", "pdf": "https://arxiv.org/pdf/2505.22061.pdf", "abs": "https://arxiv.org/abs/2505.22061", "title": "Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?", "authors": ["Yujin Choi", "Youngjoo Park", "Junyoung Byun", "Jaewook Lee", "Jinseong Park"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) mitigates the hallucination problem in\nlarge language models (LLMs) and has proven effective for specific,\npersonalized applications. However, passing private retrieved documents\ndirectly to LLMs introduces vulnerability to membership inference attacks\n(MIAs), which try to determine whether the target datum exists in the private\nexternal database or not. Based on the insight that MIA queries typically\nexhibit high similarity to only one target document, we introduce Mirabel, a\nsimilarity-based MIA detection framework designed for the RAG system. With the\nproposed Mirabel, we show that simple detect-and-hide strategies can\nsuccessfully obfuscate attackers, maintain data utility, and remain\nsystem-agnostic. We experimentally prove its detection and defense against\nvarious state-of-the-art MIA methods and its adaptability to existing private\nRAG systems."}
{"id": "2505.22068", "pdf": "https://arxiv.org/pdf/2505.22068.pdf", "abs": "https://arxiv.org/abs/2505.22068", "title": "Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO", "authors": ["Ran Li", "Shimin Di", "Yuchen Liu", "Chen Jing", "Yu Qiu", "Lei Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Previous study suggest that powerful Large Language Models (LLMs) trained\nwith Reinforcement Learning with Verifiable Rewards (RLVR) only refines\nreasoning path without improving the reasoning capacity in math tasks while\nsupervised-finetuning(SFT) with distillation can. We study this from the view\nof Scientific information extraction (SciIE) where LLMs and reasoning LLMs\nunderperforms small Bert-based models. SciIE require both the reasoning and\nmemorization. We argue that both SFT and RLVR can refine the reasoning path and\nimprove reasoning capacity in a simple way based on SciIE. We propose two-stage\ntraining with 1. MimicSFT, using structured reasoning templates without needing\nhigh-quality chain-of-thought data, 2. R$^2$GRPO with relevance and\nrule-induced rewards. Experiments on scientific IE benchmarks show that both\nmethods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses\nbaseline LLMs and specialized supervised models in relation extraction. Our\ncode is available at https://github.com/ranlislz/R2GRPO."}
{"id": "2505.22076", "pdf": "https://arxiv.org/pdf/2505.22076.pdf", "abs": "https://arxiv.org/abs/2505.22076", "title": "ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation", "authors": ["Maja Stahl", "Timon Ziegenbein", "Joonsuk Park", "Henning Wachsmuth"], "categories": ["cs.CL"], "comment": null, "summary": "Training large language models (LLMs) to follow instructions has\nsignificantly enhanced their ability to tackle unseen tasks. However, despite\ntheir strong generalization capabilities, instruction-following LLMs encounter\ndifficulties when dealing with tasks that require domain knowledge. This work\nintroduces a specialized instruction fine-tuning for the domain of\ncomputational argumentation (CA). The goal is to enable an LLM to effectively\ntackle any unseen CA tasks while preserving its generalization capabilities.\nReviewing existing CA research, we crafted natural language instructions for\n105 CA tasks to this end. On this basis, we developed a CA-specific benchmark\nfor LLMs that allows for a comprehensive evaluation of LLMs' capabilities in\nsolving various CA tasks. We synthesized 52k CA-related instructions, adapting\nthe self-instruct process to train a CA-specialized instruction-following LLM.\nOur experiments suggest that CA-specialized instruction fine-tuning\nsignificantly enhances the LLM on both seen and unseen CA tasks. At the same\ntime, performance on the general NLP tasks of the SuperNI benchmark remains\nstable."}
{"id": "2505.22095", "pdf": "https://arxiv.org/pdf/2505.22095.pdf", "abs": "https://arxiv.org/abs/2505.22095", "title": "Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning", "authors": ["Chunyi Peng", "Zhipeng Xu", "Zhenghao Liu", "Yishan Li", "Yukun Yan", "Shuo Wang", "Zhiyuan Liu", "Yu Gu", "Minghe Yu", "Ge Yu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in\nmitigating hallucinations in Multimodal Large Language Models (MLLMs) by\nincorporating external knowledge during generation. Existing MRAG methods\ntypically adopt a static retrieval pipeline that fetches relevant information\nfrom multiple Knowledge Bases (KBs), followed by a refinement step. However,\nthese approaches overlook the reasoning and planning capabilities of MLLMs to\ndynamically determine how to interact with different KBs during the reasoning\nprocess. To address this limitation, we propose R1-Router, a novel MRAG\nframework that learns to decide when and where to retrieve knowledge based on\nthe evolving reasoning state. Specifically, R1-Router can generate follow-up\nqueries according to the current reasoning step, routing these intermediate\nqueries to the most suitable KB, and integrating external knowledge into a\ncoherent reasoning trajectory to answer the original query. Furthermore, we\nintroduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored\nreinforcement learning algorithm that assigns step-specific rewards to optimize\nthe reasoning behavior of MLLMs. Experimental results on various open-domain QA\nbenchmarks across multiple modalities demonstrate that R1-Router outperforms\nbaseline models by over 7%. Further analysis shows that R1-Router can\nadaptively and effectively leverage diverse KBs, reducing unnecessary\nretrievals and improving both efficiency and accuracy."}
{"id": "2505.22096", "pdf": "https://arxiv.org/pdf/2505.22096.pdf", "abs": "https://arxiv.org/abs/2505.22096", "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL", "authors": ["Jinheon Baek", "Horst Samulowitz", "Oktie Hassanzadeh", "Dharmashankar Subramanian", "Sola Shirai", "Alfio Gliozzo", "Debarun Bhattacharjya"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Findings 2025", "summary": "Text-to-SQL aims to translate natural language queries into SQL statements,\nwhich is practical as it enables anyone to easily retrieve the desired\ninformation from databases. Recently, many existing approaches tackle this\nproblem with Large Language Models (LLMs), leveraging their strong capability\nin understanding user queries and generating corresponding SQL code. Yet, the\nparametric knowledge in LLMs might be limited to covering all the diverse and\ndomain-specific queries that require grounding in various database schemas,\nwhich makes generated SQLs less accurate oftentimes. To tackle this, we propose\nconstructing the knowledge base for text-to-SQL, a foundational source of\nknowledge, from which we retrieve and generate the necessary knowledge for\ngiven queries. In particular, unlike existing approaches that either manually\nannotate knowledge or generate only a few pieces of knowledge for each query,\nour knowledge base is comprehensive, which is constructed based on a\ncombination of all the available questions and their associated database\nschemas along with their relevant knowledge, and can be reused for unseen\ndatabases from different datasets and domains. We validate our approach on\nmultiple text-to-SQL datasets, considering both the overlapping and\nnon-overlapping database scenarios, where it outperforms relevant baselines\nsubstantially."}
{"id": "2505.22101", "pdf": "https://arxiv.org/pdf/2505.22101.pdf", "abs": "https://arxiv.org/abs/2505.22101", "title": "MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models", "authors": ["Zhiyu Li", "Shichao Song", "Hanyu Wang", "Simin Niu", "Ding Chen", "Jiawei Yang", "Chenyang Xi", "Huayi Lai", "Jihao Zhao", "Yezhaohui Wang", "Junpeng Ren", "Zehao Lin", "Jiahao Huo", "Tianyi Chen", "Kai Chen", "Kehang Li", "Zhiqiang Yin", "Qingchen Yu", "Bo Tang", "Hongkang Yang", "Zhi-Qin John Xu", "Feiyu Xiong"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as foundational infrastructure in\nthe pursuit of Artificial General Intelligence (AGI). Despite their remarkable\ncapabilities in language perception and generation, current LLMs fundamentally\nlack a unified and structured architecture for handling memory. They primarily\nrely on parametric memory (knowledge encoded in model weights) and ephemeral\nactivation memory (context-limited runtime states). While emerging methods like\nRetrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack\nlifecycle management and multi-modal integration, limiting their capacity for\nlong-term knowledge evolution. To address this, we introduce MemOS, a memory\noperating system designed for LLMs that, for the first time, elevates memory to\na first-class operational resource. It builds unified mechanisms for\nrepresentation, organization, and governance across three core memory types:\nparametric, activation, and plaintext. At its core is the MemCube, a\nstandardized memory abstraction that enables tracking, fusion, and migration of\nheterogeneous memory, while offering structured, traceable access across tasks\nand contexts. MemOS establishes a memory-centric execution framework with\nstrong controllability, adaptability, and evolvability. It fills a critical gap\nin current LLM infrastructure and lays the groundwork for continual adaptation,\npersonalized intelligence, and cross-platform coordination in next-generation\nintelligent systems."}
{"id": "2505.22107", "pdf": "https://arxiv.org/pdf/2505.22107.pdf", "abs": "https://arxiv.org/abs/2505.22107", "title": "Curse of High Dimensionality Issue in Transformer for Long-context Modeling", "authors": ["Shuhai Zhang", "Zeng You", "Yaofo Chen", "Zhiquan Wen", "Qianyue Wang", "Zhijie Qiu", "Yuanqing Li", "Mingkui Tan"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention."}
{"id": "2505.22113", "pdf": "https://arxiv.org/pdf/2505.22113.pdf", "abs": "https://arxiv.org/abs/2505.22113", "title": "THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models", "authors": ["Zhiyuan Li", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL"], "comment": "20 pages, 8 figures, 6 tables", "summary": "Large reasoning models (LRMs) have achieved impressive performance in complex\ntasks, often outperforming conventional large language models (LLMs). However,\nthe prevalent issue of overthinking severely limits their computational\nefficiency. Overthinking occurs when models generate excessive and redundant\ntokens that contribute little to accurate outcomes, especially in simple tasks,\nresulting in a significant waste of computational resources. To systematically\ninvestigate this issue, we introduce Think-Bench, a benchmark designed to\nevaluate the reasoning efficiency of LRMs. We also propose novel efficiency\nmetrics and conduct a comprehensive evaluation of various LRMs across multiple\ndimensions, including the reasoning process, outcome quality, and\nchain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs\nexhibit overthinking in handling easy questions, generating unnecessarily\nlengthy reasoning chains. While many LRMs demonstrate high CoT quality, several\nsuffer from low efficiency. We hope that Think-Bench can serve as a robust\nfoundation for advancing research into LRMs."}
{"id": "2505.22116", "pdf": "https://arxiv.org/pdf/2505.22116.pdf", "abs": "https://arxiv.org/abs/2505.22116", "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model", "authors": ["Jintao Zhang", "Zirui Liu", "Mingyue Cheng", "Shilong Zhang", "Tingyue Pan", "Qi Liu", "Yanhu Xie"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Intraoperative hypotension (IOH) frequently occurs under general anesthesia\nand is strongly linked to adverse outcomes such as myocardial injury and\nincreased mortality. Despite its significance, IOH prediction is hindered by\nevent sparsity and the challenge of integrating static and dynamic data across\ndiverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal\nlanguage model framework. To accurately identify and differentiate sparse\nhypotensive events, we leverage a two-stage training strategy. The first stage\ninvolves domain adaptive pretraining on IOH physiological time series augmented\nthrough diffusion methods, thereby enhancing the model sensitivity to patterns\nassociated with hypotension. Subsequently, task fine-tuning is performed on the\noriginal clinical dataset to further enhance the ability to distinguish\nnormotensive from hypotensive states. To enable multimodal fusion for each\npatient, we align structured clinical descriptions with the corresponding\nphysiological time series at the token level. Such alignment enables the model\nto capture individualized temporal patterns alongside their corresponding\nclinical semantics. In addition, we convert static patient attributes into\nstructured text to enrich personalized information. Experimental evaluations on\ntwo intraoperative datasets demonstrate that IOHFuseLM outperforms established\nbaselines in accurately identifying IOH events, highlighting its applicability\nin clinical decision support scenarios. Our code is publicly available to\npromote reproducibility at https://github.com/zjt-gpu/IOHFuseLM."}
{"id": "2505.22118", "pdf": "https://arxiv.org/pdf/2505.22118.pdf", "abs": "https://arxiv.org/abs/2505.22118", "title": "Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches", "authors": ["Alan Ramponi", "Marco Rovera", "Robert Moro", "Sara Tonelli"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval of previously fact-checked claims is a well-established task, whose\nautomation can assist professional fact-checkers in the initial steps of\ninformation verification. Previous works have mostly tackled the task\nmonolingually, i.e., having both the input and the retrieved claims in the same\nlanguage. However, especially for languages with a limited availability of\nfact-checks and in case of global narratives, such as pandemics, wars, or\ninternational politics, it is crucial to be able to retrieve claims across\nlanguages. In this work, we examine strategies to improve the multilingual and\ncrosslingual performance, namely selection of negative examples (in the\nsupervised) and re-ranking (in the unsupervised setting). We evaluate all\napproaches on a dataset containing posts and claims in 47 languages (283\nlanguage combinations). We observe that the best results are obtained by using\nLLM-based re-ranking, followed by fine-tuning with negative examples sampled\nusing a sentence similarity-based strategy. Most importantly, we show that\ncrosslinguality is a setup with its own unique characteristics compared to the\nmultilingual setup."}
{"id": "2505.22120", "pdf": "https://arxiv.org/pdf/2505.22120.pdf", "abs": "https://arxiv.org/abs/2505.22120", "title": "LoKI: Low-damage Knowledge Implanting of Large Language Models", "authors": ["Runyu Wang", "Peng Ping", "Zhengyu Guo", "Xiaoye Zhang", "Quan Shi", "Liting Zhou", "Tianbo Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning adapts pretrained models for specific tasks but poses the risk of\ncatastrophic forgetting (CF), where critical knowledge from pre-training is\noverwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large\nLanguage Models (LLMs), while efficient, often sacrifice general capabilities.\nTo address the issue of CF in a general-purpose PEFT framework, we propose\n\\textbf{Lo}w-damage \\textbf{K}nowledge \\textbf{I}mplanting (\\textbf{LoKI}), a\nPEFT technique that is based on a mechanistic understanding of how knowledge is\nstored in transformer architectures. In two real-world scenarios, LoKI\ndemonstrates task-specific performance that is comparable to or even surpasses\nthat of full fine-tuning and LoRA-based methods across various model types,\nwhile significantly better preserving general capabilities. Our work connects\nmechanistic insights into LLM knowledge storage with practical fine-tuning\nobjectives, achieving state-of-the-art trade-offs between task specialization\nand the preservation of general capabilities. Our implementation is publicly\navailable as ready-to-use code\\footnote{https://github.com/Nexround/LoKI}."}
{"id": "2505.22131", "pdf": "https://arxiv.org/pdf/2505.22131.pdf", "abs": "https://arxiv.org/abs/2505.22131", "title": "EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning", "authors": ["Zhuoyang Wu", "Xinze Li", "Zhenghao Liu", "Yukun Yan", "Zhiyuan Liu", "Minghe Yu", "Cheng Yang", "Yu Gu", "Ge Yu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities\nand achieved promising results in mathematical problem-solving tasks. Learning\nfrom errors offers the potential to further enhance the performance of LLMs\nduring Supervised Fine-Tuning (SFT). However, the errors in synthesized\nsolutions are typically gathered from sampling trails, making it challenging to\ngenerate solution errors for each mathematical problem. This paper introduces\nthe Error-IndUced LEaRning (EULER) model, which aims to develop an error\nexposure model that generates high-quality solution errors to enhance the\nmathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the\nerror exposure model to increase the generation probability of self-made\nsolution errors while utilizing solutions produced by a superior LLM to\nregularize the generation quality. Our experiments across various mathematical\nproblem datasets demonstrate the effectiveness of the EULER model, achieving an\nimprovement of over 4% compared to all baseline models. Further analysis\nreveals that EULER is capable of synthesizing more challenging and educational\nsolution errors, which facilitate both the training and inference processes of\nLLMs. All codes are available at https://github.com/NEUIR/EULER."}
{"id": "2505.22135", "pdf": "https://arxiv.org/pdf/2505.22135.pdf", "abs": "https://arxiv.org/abs/2505.22135", "title": "RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding", "authors": ["Yuichiro Hoshino", "Hideyuki Tachibana", "Muneyoshi Inahara", "Hiroto Takegawa"], "categories": ["cs.CL", "cs.LG"], "comment": "26 pages", "summary": "Hybrid models combining Transformers and State Space Models (SSMs) are\npromising for balancing performance and efficiency. However, optimizing these\nhybrid models, particularly by addressing the potential redundancy inherent\nwithin the Transformer components, remains a significant challenge. In this\npaper, we propose RAD (Redundancy-Aware Distillation), a novel framework that\nuses self-speculative decoding as a diagnostic tool to identify redundant\nattention layers within the model. These identified layers are then selectively\nreplaced with SSM components, followed by targeted (self-)distillation.\nSpecifically, RAD focuses knowledge transfer on the components identified as\nredundant, considering architectural changes and specific weight initialization\nstrategies. We experimentally demonstrate that self-distillation using RAD\nsignificantly surpasses the performance of the original base model on\nmathematical and coding tasks. Furthermore, RAD is also effective in standard\nknowledge distillation settings, achieving up to approximately 2x faster\nconvergence compared to baseline methods. Notably, while a baseline model\ndistilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and\n22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and\n28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers\na new pathway for efficient optimization and performance enhancement in the\ndistillation of hybrid models."}
{"id": "2505.22137", "pdf": "https://arxiv.org/pdf/2505.22137.pdf", "abs": "https://arxiv.org/abs/2505.22137", "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments", "authors": ["Marc Feger", "Katarina Boland", "Stefan Dietze"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to ACL 2025 and will be published after\n  27.07.2025", "summary": "Identifying arguments is a necessary prerequisite for various tasks in\nautomated discourse analysis, particularly within contexts such as political\ndebates, online discussions, and scientific reasoning. In addition to\ntheoretical advances in understanding the constitution of arguments, a\nsignificant body of research has emerged around practical argument mining,\nsupported by a growing number of publicly available datasets. On these\nbenchmarks, BERT-like transformers have consistently performed best,\nreinforcing the belief that such models are broadly applicable across diverse\ncontexts of debate. This study offers the first large-scale re-evaluation of\nsuch state-of-the-art models, with a specific focus on their ability to\ngeneralize in identifying arguments. We evaluate four transformers, three\nstandard and one enhanced with contrastive pre-training for better\ngeneralization, on 17 English sentence-level datasets as most relevant to the\ntask. Our findings show that, to varying degrees, these models tend to rely on\nlexical shortcuts tied to content words, suggesting that apparent progress may\noften be driven by dataset-specific cues rather than true task alignment. While\nthe models achieve strong results on familiar benchmarks, their performance\ndrops markedly when applied to unseen datasets. Nonetheless, incorporating both\ntask-specific pre-training and joint benchmark training proves effective in\nenhancing both robustness and generalization."}
{"id": "2505.22156", "pdf": "https://arxiv.org/pdf/2505.22156.pdf", "abs": "https://arxiv.org/abs/2505.22156", "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing", "authors": ["Shuaiyi Li", "Zhisong Zhang", "Yang Deng", "Chenlong Deng", "Tianqing Fang", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Wai Lam"], "categories": ["cs.CL"], "comment": "Under review", "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."}
{"id": "2505.22157", "pdf": "https://arxiv.org/pdf/2505.22157.pdf", "abs": "https://arxiv.org/abs/2505.22157", "title": "Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy", "authors": ["Paramita Mirza", "Lucas Weber", "Fabian Küch"], "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that post-training datasets for LLMs can be substantially\ndownsampled without noticeably deteriorating performance. However, data\nselection often incurs high computational costs or is limited to narrow\ndomains. In this paper, we demonstrate that data selection can be both --\nefficient and universal -- by using a multi-step pipeline in which we\nefficiently bin data points into groups, estimate quality using specialized\nmodels, and score difficulty with a robust, lightweight method. Task-based\ncategorization allows us to control the composition of our final data --\ncrucial for finetuning multi-purpose models. To guarantee diversity, we improve\nupon previous work using embedding models and a clustering algorithm. This\nintegrated strategy enables high-performance fine-tuning with minimal overhead."}
{"id": "2505.22165", "pdf": "https://arxiv.org/pdf/2505.22165.pdf", "abs": "https://arxiv.org/abs/2505.22165", "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes", "authors": ["Bocheng Li", "Zhujin Gao", "Linli Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Diffusion models have emerged as a promising approach for text generation,\nwith recent works falling into two main categories: discrete and continuous\ndiffusion models. Discrete diffusion models apply token corruption\nindependently using categorical distributions, allowing for different diffusion\nprogress across tokens but lacking fine-grained control. Continuous diffusion\nmodels map tokens to continuous spaces and apply fine-grained noise, but the\ndiffusion progress is uniform across tokens, limiting their ability to capture\nsemantic nuances. To address these limitations, we propose\n\\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous\nC\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models\n(NeoDiff), a novel diffusion model that integrates the strengths of both\ndiscrete and continuous approaches. NeoDiff introduces a Poisson diffusion\nprocess for the forward process, enabling a flexible and fine-grained noising\nparadigm, and employs a time predictor for the reverse process to adaptively\nmodulate the denoising progress based on token semantics. Furthermore, NeoDiff\nutilizes an optimized schedule for inference to ensure more precise noise\ncontrol and improved performance. Our approach unifies the theories of discrete\nand continuous diffusion models, offering a more principled and effective\nframework for text generation. Experimental results on several text generation\ntasks demonstrate NeoDiff's superior performance compared to baselines of\nnon-autoregressive continuous and discrete diffusion models, iterative-based\nmethods and autoregressive diffusion-based methods. These results highlight\nNeoDiff's potential as a powerful tool for generating high-quality text and\nadvancing the field of diffusion-based text generation."}
{"id": "2505.22169", "pdf": "https://arxiv.org/pdf/2505.22169.pdf", "abs": "https://arxiv.org/abs/2505.22169", "title": "ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments", "authors": ["Gili Lior", "Eliya Habba", "Shahar Levy", "Avi Caciularu", "Gabriel Stanovsky"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs are highly sensitive to prompt phrasing, yet standard benchmarks\ntypically report performance using a single prompt, raising concerns about the\nreliability of such evaluations. In this work, we argue for a stochastic method\nof moments evaluation over the space of meaning-preserving prompt\nperturbations. We introduce a formal definition of reliable evaluation that\naccounts for prompt sensitivity, and suggest ReliableEval - a method for\nestimating the number of prompt resamplings needed to obtain meaningful\nresults. Using our framework, we stochastically evaluate five frontier LLMs and\nfind that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit\nsubstantial prompt sensitivity. Our approach is model-, task-, and\nmetric-agnostic, offering a recipe for meaningful and robust LLM evaluation."}
{"id": "2505.22172", "pdf": "https://arxiv.org/pdf/2505.22172.pdf", "abs": "https://arxiv.org/abs/2505.22172", "title": "Reverse Preference Optimization for Complex Instruction Following", "authors": ["Xiang Huang", "Ting-En Lin", "Feiteng Fang", "Yuchuan Wu", "Hangyu Li", "Yuzhong Qu", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Instruction following (IF) is a critical capability for large language models\n(LLMs). However, handling complex instructions with multiple constraints\nremains challenging. Previous methods typically select preference pairs based\non the number of constraints they satisfy, introducing noise where chosen\nexamples may fail to follow some constraints and rejected examples may excel in\ncertain respects over the chosen ones. To address the challenge of aligning\nwith multiple preferences, we propose a simple yet effective method called\nReverse Preference Optimization (RPO). It mitigates noise in preference pairs\nby dynamically reversing the constraints within the instruction to ensure the\nchosen response is perfect, alleviating the burden of extensive sampling and\nfiltering to collect perfect responses. Besides, reversal also enlarges the gap\nbetween chosen and rejected responses, thereby clarifying the optimization\ndirection and making it more robust to noise. We evaluate RPO on two multi-turn\nIF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over\nthe DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.\nMoreover, RPO scales effectively across model sizes (8B to 70B parameters),\nwith the 70B RPO model surpassing GPT-4o."}
{"id": "2505.22176", "pdf": "https://arxiv.org/pdf/2505.22176.pdf", "abs": "https://arxiv.org/abs/2505.22176", "title": "TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation", "authors": ["Vihang Pancholi", "Jainit Bafna", "Tejas Anvekar", "Manish Shrivastava", "Vivek Gupta"], "categories": ["cs.CL"], "comment": "Accepeted for Findings at ACL 2025", "summary": "Evaluating tables qualitatively & quantitatively presents a significant\nchallenge, as traditional metrics often fail to capture nuanced structural and\ncontent discrepancies. To address this, we introduce a novel, methodical rubric\nintegrating multi-level structural descriptors with fine-grained contextual\nquantification, thereby establishing a robust foundation for comprehensive\ntable comparison. Building on this foundation, we propose TabXEval, an\neXhaustive and eXplainable two-phase evaluation framework. TabXEval initially\naligns reference tables structurally via TabAlign & subsequently conducts a\nsystematic semantic and syntactic comparison using TabCompare; this approach\nclarifies the evaluation process and pinpoints subtle discrepancies overlooked\nby conventional methods. The efficacy of this framework is assessed using\nTabXBench, a novel, diverse, multi-domain benchmark we developed, featuring\nrealistic table perturbations and human-annotated assessments. Finally, a\nsystematic analysis of existing evaluation methods through\nsensitivity-specificity trade-offs demonstrates the qualitative and\nquantitative effectiveness of TabXEval across diverse table-related tasks and\ndomains, paving the way for future innovations in explainable table evaluation."}
{"id": "2505.22179", "pdf": "https://arxiv.org/pdf/2505.22179.pdf", "abs": "https://arxiv.org/abs/2505.22179", "title": "Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design", "authors": ["Yudi Zhang", "Weilin Zhao", "Xu Han", "Tiejun Zhao", "Wang Xu", "Hailong Cao", "Conghui Zhu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Speculative decoding and quantization effectively accelerate memory-bound\ninference of large language models. Speculative decoding mitigates the memory\nbandwidth bottleneck by verifying multiple tokens within a single forward pass,\nwhich increases computational effort. Quantization achieves this optimization\nby compressing weights and activations into lower bit-widths and also reduces\ncomputations via low-bit matrix multiplications. To further leverage their\nstrengths, we investigate the integration of these two techniques.\nSurprisingly, experiments applying the advanced speculative decoding method\nEAGLE-2 to various quantized models reveal that the memory benefits from 4-bit\nweight quantization are diminished by the computational load from speculative\ndecoding. Specifically, verifying a tree-style draft incurs significantly more\ntime overhead than a single-token forward pass on 4-bit weight quantized\nmodels. This finding led to our new speculative decoding design: a hierarchical\nframework that employs a small model as an intermediate stage to turn\ntree-style drafts into sequence drafts, leveraging the memory access benefits\nof the target quantized model. Experimental results show that our hierarchical\napproach achieves a 2.78$\\times$ speedup across various tasks for the 4-bit\nweight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\\times$.\nCode available at https://github.com/AI9Stars/SpecMQuant."}
{"id": "2505.22184", "pdf": "https://arxiv.org/pdf/2505.22184.pdf", "abs": "https://arxiv.org/abs/2505.22184", "title": "Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon", "authors": ["Xuchen Ma", "Jianxiang Yu", "Wenming Shao", "Bo Pang", "Xiang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 5 figures, 9 tables", "summary": "Social media platforms have experienced a significant rise in toxic content,\nincluding abusive language and discriminatory remarks, presenting growing\nchallenges for content moderation. Some users evade censorship by deliberately\ndisguising toxic words through homophonic cloak, which necessitates the task of\nunveiling cloaked toxicity. Existing methods are mostly designed for English\ntexts, while Chinese cloaked toxicity unveiling has not been solved yet. To\ntackle the issue, we propose C$^2$TU, a novel training-free and prompt-free\nmethod for Chinese cloaked toxic content unveiling. It first employs substring\nmatching to identify candidate toxic words based on Chinese homo-graph and\ntoxic lexicon. Then it filters those candidates that are non-toxic and corrects\ncloaks to be their corresponding toxicities. Specifically, we develop two model\nvariants for filtering, which are based on BERT and LLMs, respectively. For\nLLMs, we address the auto-regressive limitation in computing word occurrence\nprobability and utilize the full semantic contexts of a text sequence to reveal\ncloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve\nsuperior performance on two Chinese toxic datasets. In particular, our method\noutperforms the best competitor by up to 71% on the F1 score and 35% on\naccuracy, respectively."}
{"id": "2505.22202", "pdf": "https://arxiv.org/pdf/2505.22202.pdf", "abs": "https://arxiv.org/abs/2505.22202", "title": "Let's Predict Sentence by Sentence", "authors": ["Hyeonbin Hwang", "Byeongguk Jeon", "Seungone Kim", "Jiyeon Kim", "Hoyeon Chang", "Sohee Yang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "categories": ["cs.CL", "cs.AI"], "comment": "Work In Progress", "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces."}
{"id": "2505.22232", "pdf": "https://arxiv.org/pdf/2505.22232.pdf", "abs": "https://arxiv.org/abs/2505.22232", "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models", "authors": ["Mehdi Ali", "Manuel Brack", "Max Lübbering", "Elias Wendt", "Abbas Goher Khan", "Richard Rutmann", "Alex Jude", "Maurice Kraus", "Alexander Arno Weber", "Felix Stollenwerk", "David Kaczér", "Florian Mai", "Lucie Flek", "Rafet Sifa", "Nicolas Flores-Herr", "Joachim Köhler", "Patrick Schramowski", "Michael Fromm", "Kristian Kersting"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page available at https://huggingface.co/spaces/Jackal-AI/JQL", "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development."}
{"id": "2505.22236", "pdf": "https://arxiv.org/pdf/2505.22236.pdf", "abs": "https://arxiv.org/abs/2505.22236", "title": "A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity", "authors": ["Charlotte Pouw", "Afra Alishahi", "Willem Zuidema"], "categories": ["cs.CL"], "comment": "Accepted to CoNLL 2025", "summary": "We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using\nmethods inspired by psycholinguistic research. Specifically, we focus on the\ngeneration of intonational phrase boundaries, which can often be predicted by\nidentifying syntactic boundaries within a sentence. We find that TTS systems\nstruggle to accurately generate intonational phrase boundaries in sentences\nwhere syntactic boundaries are ambiguous (e.g., garden path sentences or\nsentences with attachment ambiguity). In these cases, systems need superficial\ncues such as commas to place boundaries at the correct positions. In contrast,\nfor sentences with simpler syntactic structures, we find that systems do\nincorporate syntactic cues beyond surface markers. Finally, we finetune models\non sentences without commas at the syntactic boundary positions, encouraging\nthem to focus on more subtle linguistic cues. Our findings indicate that this\nleads to more distinct intonation patterns that better reflect the underlying\nstructure."}
{"id": "2505.22240", "pdf": "https://arxiv.org/pdf/2505.22240.pdf", "abs": "https://arxiv.org/abs/2505.22240", "title": "BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain", "authors": ["Yunsoo Kim", "Yusuf Abdulle", "Honghan Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Biomedical reasoning often requires traversing interconnected relationships\nacross entities such as drugs, diseases, and proteins. Despite the increasing\nprominence of large language models (LLMs), existing benchmarks lack the\nability to evaluate multi-hop reasoning in the biomedical domain, particularly\nfor queries involving one-to-many and many-to-many relationships. This gap\nleaves the critical challenges of biomedical multi-hop reasoning underexplored.\nTo address this, we introduce BioHopR, a novel benchmark designed to evaluate\nmulti-hop, multi-answer reasoning in structured biomedical knowledge graphs.\nBuilt from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop\nreasoning tasks that reflect real-world biomedical complexities.\n  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary\nreasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on\n2-hop tasks, outperforming proprietary models such as GPT4O and open-source\nbiomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all\nmodels exhibit significant declines in multi-hop performance, underscoring the\nchallenges of resolving implicit reasoning steps in the biomedical domain. By\naddressing the lack of benchmarks for multi-hop reasoning in biomedical domain,\nBioHopR sets a new standard for evaluating reasoning capabilities and\nhighlights critical gaps between proprietary and open-source models while\npaving the way for future advancements in biomedical LLMs."}
{"id": "2505.22264", "pdf": "https://arxiv.org/pdf/2505.22264.pdf", "abs": "https://arxiv.org/abs/2505.22264", "title": "MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps", "authors": ["Maximiliano Hormazábal Lagos", "Álvaro Bueno Saez", "Héctor Cerezo-Costas", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 pages, 6 tables", "summary": "In this paper we expose our approach to solve the \\textit{SemEval 2025 Task\n8: Question-Answering over Tabular Data} challenge. Our strategy leverages\nPython code generation with LLMs to interact with the table and get the answer\nto the questions. The process is composed of multiple steps: understanding the\ncontent of the table, generating natural language instructions in the form of\nsteps to follow in order to get the answer, translating these instructions to\ncode, running it and handling potential errors or exceptions. These steps use\nopen source LLMs and fine grained optimized prompts for each task (step). With\nthis approach, we achieved a score of $70.50\\%$ for subtask 1."}
{"id": "2505.22273", "pdf": "https://arxiv.org/pdf/2505.22273.pdf", "abs": "https://arxiv.org/abs/2505.22273", "title": "Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages", "authors": ["Shohei Higashiyama", "Masao Utiyama"], "categories": ["cs.CL"], "comment": "23 pages", "summary": "Lexical normalization research has sought to tackle the challenge of\nprocessing informal expressions in user-generated text, yet the absence of\ncomprehensive evaluations leaves it unclear which methods excel across multiple\nperspectives. Focusing on unsegmented languages, we make three key\ncontributions: (1) creating a large-scale, multi-domain Japanese normalization\ndataset, (2) developing normalization methods based on state-of-the-art\npretrained models, and (3) conducting experiments across multiple evaluation\nperspectives. Our experiments show that both encoder-only and decoder-only\napproaches achieve promising results in both accuracy and efficiency."}
{"id": "2505.22280", "pdf": "https://arxiv.org/pdf/2505.22280.pdf", "abs": "https://arxiv.org/abs/2505.22280", "title": "Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review", "authors": ["Zihan Xu", "Haotian Ma", "Gongbo Zhang", "Yihao Ding", "Chunhua Weng", "Yifan Peng"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Evidence-based medicine (EBM) is at the forefront of modern healthcare,\nemphasizing the use of the best available scientific evidence to guide clinical\ndecisions. Due to the sheer volume and rapid growth of medical literature and\nthe high cost of curation, there is a critical need to investigate Natural\nLanguage Processing (NLP) methods to identify, appraise, synthesize, summarize,\nand disseminate evidence in EBM. This survey presents an in-depth review of 129\nresearch studies on leveraging NLP for EBM, illustrating its pivotal role in\nenhancing clinical decision-making processes. The paper systematically explores\nhow NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,\nApply, and Assess. The review not only identifies current limitations within\nthe field but also proposes directions for future research, emphasizing the\npotential for NLP to revolutionize EBM by refining evidence extraction,\nevidence synthesis, appraisal, summarization, enhancing data comprehensibility,\nand facilitating a more efficient clinical workflow."}
{"id": "2505.22293", "pdf": "https://arxiv.org/pdf/2505.22293.pdf", "abs": "https://arxiv.org/abs/2505.22293", "title": "Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs", "authors": ["Samuel Frontull", "Thomas Ströhle"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmultilingual machine translation, sometimes even outperforming traditional\nneural systems. However, previous research has highlighted the challenges of\nusing LLMs, particularly with prompt engineering, for low-resource languages.\nIn this work, we introduce Fragment-Shot Prompting, a novel in-context learning\nmethod that segments input and retrieves translation examples based on\nsyntactic coverage, along with Pivoted Fragment-Shot, an extension that enables\ntranslation without direct parallel data. We evaluate these methods using\nGPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between\nItalian and two Ladin variants, revealing three key findings: (1) Fragment-Shot\nPrompting is effective for translating into and between the studied\nlow-resource languages, with syntactic coverage positively correlating with\ntranslation quality; (2) Models with stronger reasoning abilities make more\neffective use of retrieved knowledge, generally produce better translations,\nand enable Pivoted Fragment-Shot to significantly improve translation quality\nbetween the Ladin variants; and (3) prompt engineering offers limited, if any,\nimprovements when translating from a low-resource to a high-resource language,\nwhere zero-shot prompting already yields satisfactory results. We publicly\nrelease our code and the retrieval corpora."}
{"id": "2505.22296", "pdf": "https://arxiv.org/pdf/2505.22296.pdf", "abs": "https://arxiv.org/abs/2505.22296", "title": "360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training", "authors": ["Haosheng Zou", "Xiaowei Lv", "Shousheng Jia", "Xiangzheng Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "code at https://github.com/Qihoo360/360-LLaMA-Factory", "summary": "Adding sequence parallelism into LLaMA-Factory, we open-sourced\n360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.\n360-LLaMA-Factory has received wide recognition and used in models such as\nLight-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and\nalso in large companies' training frameworks. This technical report delves\ndeeper into the different sequence parallel modes behind 360-LLaMA-Factory and\ndiscusses our implementation insights."}
{"id": "2505.22298", "pdf": "https://arxiv.org/pdf/2505.22298.pdf", "abs": "https://arxiv.org/abs/2505.22298", "title": "Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing", "authors": ["Yifan Lu", "Jing Li", "Yigeng Zhou", "Yihui Zhang", "Wenya Wang", "Xiucheng Li", "Meishan Zhang", "Fangming Liu", "Jun Yu", "Min Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) exhibit impressive language capabilities but\nremain vulnerable to malicious prompts and jailbreaking attacks. Existing\nknowledge editing methods for LLM detoxification face two major challenges.\nFirst, they often rely on entity-specific localization, making them ineffective\nagainst adversarial inputs without explicit entities. Second, these methods\nsuffer from over-editing, where detoxified models reject legitimate queries,\ncompromising overall performance. In this paper, we propose ToxEdit, a\ntoxicity-aware knowledge editing approach that dynamically detects toxic\nactivation patterns during forward propagation. It then routes computations\nthrough adaptive inter-layer pathways to mitigate toxicity effectively. This\ndesign ensures precise toxicity mitigation while preserving LLMs' general\ncapabilities. To more accurately assess over-editing, we also enhance the\nSafeEdit benchmark by incorporating instruction-following evaluation tasks.\nExperimental results on multiple LLMs demonstrate that our ToxEdit outperforms\nprevious state-of-the-art methods in both detoxification performance and\nsafeguarding general capabilities of LLMs."}
{"id": "2505.22318", "pdf": "https://arxiv.org/pdf/2505.22318.pdf", "abs": "https://arxiv.org/abs/2505.22318", "title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?", "authors": ["Ishwar B Balappanawar", "Vamshi Krishna Bonagiri", "Anish R Joishy", "Manas Gaur", "Krishnaprasad Thirunarayan", "Ponnurangam Kumaraguru"], "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 5 figures", "summary": "Large Language Models (LLMs) demonstrate impressive reasoning capabilities in\nfamiliar contexts, but struggle when the context conflicts with their\nparametric knowledge. To investigate this phenomenon, we introduce\nCounterLogic, a dataset containing 1,800 examples across 9 logical schemas,\nexplicitly designed to evaluate logical reasoning through counterfactual\n(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11\nLLMs across 6 different datasets reveals a consistent performance degradation,\nwith accuracies dropping by 27% on average when reasoning through\ncounterfactual information. We propose Self-Segregate, a prompting method\nenabling metacognitive awareness (explicitly identifying knowledge conflicts)\nbefore reasoning. Our method dramatically narrows the average performance gaps\nfrom 27% to just 11%, while significantly increasing the overall accuracy\n(+7.5%). We discuss the implications of these findings and draw parallels to\nhuman cognitive processes, particularly on how humans disambiguate conflicting\ninformation during reasoning tasks. Our findings offer practical insights for\nunderstanding and enhancing LLMs reasoning capabilities in real-world\napplications, especially where models must logically reason independently of\ntheir factual knowledge."}
{"id": "2505.22323", "pdf": "https://arxiv.org/pdf/2505.22323.pdf", "abs": "https://arxiv.org/abs/2505.22323", "title": "Advancing Expert Specialization for Better MoE", "authors": ["Hongcan Guo", "Haolang Lu", "Guoshun Nan", "Bolun Chu", "Jialin Zhuang", "Yuan Yang", "Wenhao Che", "Sicong Leng", "Qimei Cui", "Xudong Jiang"], "categories": ["cs.CL", "cs.SE", "68T07", "I.2.7"], "comment": "33pages, 6figures", "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community."}
{"id": "2505.22327", "pdf": "https://arxiv.org/pdf/2505.22327.pdf", "abs": "https://arxiv.org/abs/2505.22327", "title": "NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment", "authors": ["Antonia Karamolegkou", "Angana Borah", "Eunjung Cho", "Sagnik Ray Choudhury", "Martina Galletti", "Rajarshi Ghosh", "Pranav Gupta", "Oana Ignat", "Priyanka Kargupta", "Neema Kotonya", "Hemank Lamba", "Sun-Joo Lee", "Arushi Mangla", "Ishani Mondal", "Deniz Nazarova", "Poli Nemkova", "Dina Pisarevskaya", "Naquee Rizwan", "Nazanin Sabri", "Dominik Stammbach", "Anna Steinberg", "David Tomás", "Steven R Wilson", "Bowen Yi", "Jessica H Zhu", "Arkaitz Zubiaga", "Anders Søgaard", "Alexander Fraser", "Zhijing Jin", "Rada Mihalcea", "Joel R. Tetreault", "Daryna Dementieva"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have unlocked\nunprecedented possibilities across a range of applications. However, as a\ncommunity, we believe that the field of Natural Language Processing (NLP) has a\ngrowing need to approach deployment with greater intentionality and\nresponsibility. In alignment with the broader vision of AI for Social Good\n(Toma\\v{s}ev et al., 2020), this paper examines the role of NLP in addressing\npressing societal challenges. Through a cross-disciplinary analysis of social\ngoals and emerging risks, we highlight promising research directions and\noutline challenges that must be addressed to ensure responsible and equitable\nprogress in NLP4SG research."}
{"id": "2505.22334", "pdf": "https://arxiv.org/pdf/2505.22334.pdf", "abs": "https://arxiv.org/abs/2505.22334", "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start."}
{"id": "2505.22338", "pdf": "https://arxiv.org/pdf/2505.22338.pdf", "abs": "https://arxiv.org/abs/2505.22338", "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback", "authors": ["Hanyang Wang", "Lu Wang", "Chaoyun Zhang", "Tianjun Mao", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "The code for our method is available at\n  https://github.com/microsoft/Text2Grad", "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad"}
{"id": "2505.22354", "pdf": "https://arxiv.org/pdf/2505.22354.pdf", "abs": "https://arxiv.org/abs/2505.22354", "title": "LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High", "authors": ["Judith Sieker", "Clara Lachenmaier", "Sina Zarrieß"], "categories": ["cs.CL"], "comment": "8 pages (including References). Accepted at CogSci 2025", "summary": "This paper examines how LLMs handle false presuppositions and whether certain\nlinguistic factors influence their responses to falsely presupposed content.\nPresuppositions subtly introduce information as given, making them highly\neffective at embedding disputable or false information. This raises concerns\nabout whether LLMs, like humans, may fail to detect and correct misleading\nassumptions introduced as false presuppositions, even when the stakes of\nmisinformation are high. Using a systematic approach based on linguistic\npresupposition analysis, we investigate the conditions under which LLMs are\nmore or less sensitive to adopt or reject false presuppositions. Focusing on\npolitical contexts, we examine how factors like linguistic construction,\npolitical party, and scenario probability impact the recognition of false\npresuppositions. We conduct experiments with a newly created dataset and\nexamine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's\nMistral-7B-v03. Our results show that the models struggle to recognize false\npresuppositions, with performance varying by condition. This study highlights\nthat linguistic presupposition analysis is a valuable tool for uncovering the\nreinforcement of political misinformation in LLM responses."}
{"id": "2505.22375", "pdf": "https://arxiv.org/pdf/2505.22375.pdf", "abs": "https://arxiv.org/abs/2505.22375", "title": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition", "authors": ["Hanting Chen", "Yasheng Wang", "Kai Han", "Dong Li", "Lin Li", "Zhenni Bi", "Jinpeng Li", "Haoyu Wang", "Fei Mi", "Mingjian Zhu", "Bin Wang", "Kaikai Song", "Yifei Fu", "Xu He", "Yu Luo", "Chong Zhu", "Quan He", "Xueyu Wu", "Wei He", "Hailin Hu", "Yehui Tang", "Dacheng Tao", "Xinghao Chen", "Yunhe Wang", "Other Contributors"], "categories": ["cs.CL"], "comment": null, "summary": "This work presents Pangu Embedded, an efficient Large Language Model (LLM)\nreasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible\nfast and slow thinking capabilities. Pangu Embedded addresses the significant\ncomputational costs and inference latency challenges prevalent in existing\nreasoning-optimized LLMs. We propose a two-stage training framework for its\nconstruction. In Stage 1, the model is finetuned via an iterative distillation\nprocess, incorporating inter-iteration model merging to effectively aggregate\ncomplementary knowledge. This is followed by reinforcement learning on Ascend\nclusters, optimized by a latency-tolerant scheduler that combines stale\nsynchronous parallelism with prioritized data queues. The RL process is guided\nby a Multi-source Adaptive Reward System (MARS), which generates dynamic,\ntask-specific reward signals using deterministic metrics and lightweight LLM\nevaluators for mathematics, coding, and general problem-solving tasks. Stage 2\nintroduces a dual-system framework, endowing Pangu Embedded with a \"fast\" mode\nfor routine queries and a deeper \"slow\" mode for complex inference. This\nframework offers both manual mode switching for user control and an automatic,\ncomplexity-aware mode selection mechanism that dynamically allocates\ncomputational resources to balance latency and reasoning depth. Experimental\nresults on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate\nthat Pangu Embedded with 7B parameters, outperforms similar-size models like\nQwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art\nreasoning quality within a single, unified model architecture, highlighting a\npromising direction for developing powerful yet practically deployable LLM\nreasoners."}
{"id": "2505.22430", "pdf": "https://arxiv.org/pdf/2505.22430.pdf", "abs": "https://arxiv.org/abs/2505.22430", "title": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning", "authors": ["Kun Li", "Yunxiang Li", "Tianhua Zhang", "Hongyin Luo", "Xixin Wu", "James Glass", "Helen Meng"], "categories": ["cs.CL"], "comment": null, "summary": "Robust evaluation is critical for deploying trustworthy retrieval-augmented\ngeneration (RAG) systems. However, current LLM-based evaluation frameworks\npredominantly rely on directly prompting resource-intensive models with complex\nmulti-stage prompts, underutilizing models' reasoning capabilities and\nintroducing significant computational cost. In this paper, we present RAG-Zeval\n(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness\nand correctness evaluation as a rule-guided reasoning task. Our approach trains\nevaluators with reinforcement learning, facilitating compact models to generate\ncomprehensive and sound assessments with detailed explanation in one-pass. We\nintroduce a ranking-based outcome reward mechanism, using preference judgments\nrather than absolute scores, to address the challenge of obtaining precise\npointwise reward signals. To this end, we synthesize the ranking references by\ngenerating quality-controlled responses with zero human annotation. Experiments\ndemonstrate RAG-Zeval's superior performance, achieving the strongest\ncorrelation with human judgments and outperforming baselines that rely on LLMs\nwith 10-100 times more parameters. Our approach also exhibits superior\ninterpretability in response evaluation."}
{"id": "2505.22453", "pdf": "https://arxiv.org/pdf/2505.22453.pdf", "abs": "https://arxiv.org/abs/2505.22453", "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO", "authors": ["Lai Wei", "Yuting Li", "Chen Wang", "Yue Wang", "Linghe Kong", "Weiran Huang", "Lichao Sun"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT."}
{"id": "2505.22501", "pdf": "https://arxiv.org/pdf/2505.22501.pdf", "abs": "https://arxiv.org/abs/2505.22501", "title": "EvolveSearch: An Iterative Self-Evolving Search Agent", "authors": ["Dingchu Zhang", "Yida Zhao", "Jialong Wu", "Baixuan Li", "Wenbiao Yin", "Liwen Zhang", "Yong Jiang", "Yufeng Li", "Kewei Tu", "Pengjun Xie", "Fei Huang"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of agentic information seeking capabilities through the integration\nof tools such as search engines and web browsers. However, current mainstream\napproaches for enabling LLM web search proficiency face significant challenges:\nsupervised fine-tuning struggles with data production in open-search domains,\nwhile RL converges quickly, limiting their data utilization efficiency. To\naddress these issues, we propose EvolveSearch, a novel iterative self-evolution\nframework that combines SFT and RL to enhance agentic web search capabilities\nwithout any external human-annotated reasoning data. Extensive experiments on\nseven multi-hop question-answering (MHQA) benchmarks demonstrate that\nEvolveSearch consistently improves performance across iterations, ultimately\nachieving an average improvement of 4.7\\% over the current state-of-the-art\nacross seven benchmarks, opening the door to self-evolution agentic\ncapabilities in open web search domains."}
{"id": "2505.22517", "pdf": "https://arxiv.org/pdf/2505.22517.pdf", "abs": "https://arxiv.org/abs/2505.22517", "title": "Multi-MLLM Knowledge Distillation for Out-of-Context News Detection", "authors": ["Yimeng Gu", "Zhao Tong", "Ignacio Castro", "Shu Wu", "Gareth Tyson"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Multimodal out-of-context news is a type of misinformation in which the image\nis used outside of its original context. Many existing works have leveraged\nmultimodal large language models (MLLMs) for detecting out-of-context news.\nHowever, observing the limited zero-shot performance of smaller MLLMs, they\ngenerally require label-rich fine-tuning and/or expensive API calls to GPT\nmodels to improve the performance, which is impractical in low-resource\nscenarios. In contrast, we aim to improve the performance of small MLLMs in a\nmore label-efficient and cost-effective manner. To this end, we first prompt\nmultiple teacher MLLMs to generate both label predictions and corresponding\nrationales, which collectively serve as the teachers' knowledge. We then\nintroduce a two-stage knowledge distillation framework to transfer this\nknowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the\nstudent model using all training data. In Stage 2, we further fine-tune the\nstudent model using both LoRA fine-tuning and DPO on the data points where\nteachers' predictions conflict. This two-stage strategy reduces annotation\ncosts and helps the student model uncover subtle patterns in more challenging\ncases. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance using less than 10% labeled data."}
{"id": "2505.22548", "pdf": "https://arxiv.org/pdf/2505.22548.pdf", "abs": "https://arxiv.org/abs/2505.22548", "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs", "authors": ["Changhao Song", "Yazhou Zhang", "Peng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Emotion understanding includes basic tasks (e.g., sentiment/emotion\nclassification) and advanced tasks (e.g., sarcasm/humor detection). Current\nmethods rely on fixed-length CoT reasoning, failing to adapt to the varying\ncomplexity of emotions. We propose a task-adaptive reasoning framework that\nemploys DeepSeek-R1 to generate variable-length reasoning chains for different\nemotion tasks. By combining fine-tuning with reinforcement learning, we design\na composite reward function that balances four objectives: prediction accuracy,\nadaptive reasoning depth control, structural diversity in reasoning paths, and\nsuppression of repetitive logic. This approach achieves dynamic\ncontext-sensitive inference while enabling LLMs to autonomously develop deep\nreasoning capabilities. Experimental results demonstrate consistent\nimprovements in both Acc and F1 scores across four tasks: emotion, sentiment,\nhumor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for\nbasic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges\nrigid CoT reasoning and emotional complexity through adaptive-depth analysis."}
{"id": "2505.22552", "pdf": "https://arxiv.org/pdf/2505.22552.pdf", "abs": "https://arxiv.org/abs/2505.22552", "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM", "authors": ["Hoang Pham", "Thanh-Do Nguyen", "Khac-Hoai Nam Bui"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ACL 2025 findings", "summary": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones."}
{"id": "2505.22563", "pdf": "https://arxiv.org/pdf/2505.22563.pdf", "abs": "https://arxiv.org/abs/2505.22563", "title": "Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings", "authors": ["Yu Lei", "Xingyang Ge", "Yi Zhang", "Yiming Yang", "Bolei Ma"], "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Understanding whether large language models (LLMs) and the human brain\nconverge on similar computational principles remains a fundamental and\nimportant question in cognitive neuroscience and AI. Do the brain-like patterns\nobserved in LLMs emerge simply from scaling, or do they reflect deeper\nalignment with the architecture of human language processing? This study\nfocuses on the sentence-level neural mechanisms of language models,\nsystematically investigating how hierarchical representations in LLMs align\nwith the dynamic neural responses during human sentence comprehension. By\ncomparing hierarchical embeddings from 14 publicly available LLMs with fMRI\ndata collected from participants, who were exposed to a naturalistic narrative\nstory, we constructed sentence-level neural prediction models to precisely\nidentify the model layers most significantly correlated with brain region\nactivations. Results show that improvements in model performance drive the\nevolution of representational architectures toward brain-like hierarchies,\nparticularly achieving stronger functional and anatomical correspondence at\nhigher semantic abstraction levels."}
{"id": "2505.22571", "pdf": "https://arxiv.org/pdf/2505.22571.pdf", "abs": "https://arxiv.org/abs/2505.22571", "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "authors": ["Hoang Pham", "Khac-Hoai Nam Bui"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation."}
{"id": "2505.22572", "pdf": "https://arxiv.org/pdf/2505.22572.pdf", "abs": "https://arxiv.org/abs/2505.22572", "title": "Fusion Steering: Prompt-Specific Activation Control", "authors": ["Waldemar Chang", "Alhassan Yasin"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures, 2 tables", "summary": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs."}
{"id": "2505.22582", "pdf": "https://arxiv.org/pdf/2505.22582.pdf", "abs": "https://arxiv.org/abs/2505.22582", "title": "Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "categories": ["cs.CL"], "comment": "ACL 2025 (Main), 16 pages, 5 figures, 11 tables", "summary": "Continually expanding new languages for existing large language models (LLMs)\nis a promising yet challenging approach to building powerful multilingual LLMs.\nThe biggest challenge is to make the model continuously learn new languages\nwhile preserving the proficient ability of old languages. To achieve this,\nrecent work utilizes the Mixture-of-Experts (MoE) architecture to expand new\nlanguages by adding new experts and avoid catastrophic forgetting of old\nlanguages by routing corresponding tokens to the original model backbone (old\nexperts). Although intuitive, this kind of method is parameter-costly when\nexpanding new languages and still inevitably impacts the performance of old\nlanguages. To address these limitations, we analyze the language\ncharacteristics of different layers in LLMs and propose a layer-wise expert\nallocation algorithm (LayerMoE) to determine the appropriate number of new\nexperts for each layer. Specifically, we find different layers in LLMs exhibit\ndifferent representation similarities between languages and then utilize the\nsimilarity as the indicator to allocate experts for each layer, i.e., the\nhigher similarity, the fewer experts. Additionally, to further mitigate the\nforgetting of old languages, we add a classifier in front of the router network\non the layers with higher similarity to guide the routing of old language\ntokens. Experimental results show that our method outperforms the previous\nstate-of-the-art baseline with 60% fewer experts in the single-expansion\nsetting and with 33.3% fewer experts in the lifelong-expansion setting,\ndemonstrating the effectiveness of our method."}
{"id": "2505.22586", "pdf": "https://arxiv.org/pdf/2505.22586.pdf", "abs": "https://arxiv.org/abs/2505.22586", "title": "Precise In-Parameter Concept Erasure in Large Language Models", "authors": ["Yoav Gur-Arieh", "Clara Suslik", "Yihuai Hong", "Fazl Barez", "Mor Geva"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models."}
{"id": "2505.22591", "pdf": "https://arxiv.org/pdf/2505.22591.pdf", "abs": "https://arxiv.org/abs/2505.22591", "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning", "authors": ["Erxin Yu", "Jing Li", "Ming Liao", "Qi Zhu", "Boyang Xue", "Minghui Xu", "Baojun Wang", "Lanqing Hong", "Fei Mi", "Lifeng Shang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 9 figures", "summary": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization."}
{"id": "2505.22618", "pdf": "https://arxiv.org/pdf/2505.22618.pdf", "abs": "https://arxiv.org/abs/2505.22618", "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding", "authors": ["Chengyue Wu", "Hao Zhang", "Shuchen Xue", "Zhijian Liu", "Shizhe Diao", "Ligeng Zhu", "Ping Luo", "Song Han", "Enze Xie"], "categories": ["cs.CL"], "comment": null, "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."}
{"id": "2505.22627", "pdf": "https://arxiv.org/pdf/2505.22627.pdf", "abs": "https://arxiv.org/abs/2505.22627", "title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions", "authors": ["Yijun Shen", "Delong Chen", "Fan Liu", "Xingyu Wang", "Chuanyi Zhang", "Liang Yao", "Yuhui Zheng"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "While densely annotated image captions significantly facilitate the learning\nof robust vision-language alignment, methodologies for systematically\noptimizing human annotation efforts remain underexplored. We introduce\nChain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize\nthe number of annotated samples and improve their comprehensiveness under fixed\nbudget constraints (e.g., total human annotation time). The framework is built\nupon two key insights. First, sequential annotation reduces redundant workload\ncompared to conventional parallel annotation, as subsequent annotators only\nneed to annotate the ``residual'' -- the missing visual information that\nprevious annotations have not covered. Second, humans process textual input\nfaster by reading while outputting annotations with much higher throughput via\ntalking; thus a multimodal interface enables optimized efficiency. We evaluate\nour framework from two aspects: intrinsic evaluations that assess the\ncomprehensiveness of semantic units, obtained by parsing detailed captions into\nobject-attribute trees and analyzing their effective connections; extrinsic\nevaluation measures the practical usage of the annotated captions in\nfacilitating vision-language alignment. Experiments with eight participants\nshow our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30\nunits/sec) and retrieval performance (41.13\\% vs. 40.52\\%) over the parallel\nmethod."}
{"id": "2505.22630", "pdf": "https://arxiv.org/pdf/2505.22630.pdf", "abs": "https://arxiv.org/abs/2505.22630", "title": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs", "authors": ["Ziling Cheng", "Meng Cao", "Marc-Antoine Rondeau", "Jackie Chi Kit Cheung"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons."}
{"id": "2505.22633", "pdf": "https://arxiv.org/pdf/2505.22633.pdf", "abs": "https://arxiv.org/abs/2505.22633", "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis", "authors": ["Yida Xue", "Zhen Bi", "Jinnan Yang", "Jungang Lou", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Ongoing work", "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence."}
{"id": "2505.22635", "pdf": "https://arxiv.org/pdf/2505.22635.pdf", "abs": "https://arxiv.org/abs/2505.22635", "title": "Learning Composable Chains-of-Thought", "authors": ["Fangcong Yin", "Zeyu Leo Liu", "Liu Leqi", "Xi Ye", "Greg Durrett"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget."}
{"id": "2505.22645", "pdf": "https://arxiv.org/pdf/2505.22645.pdf", "abs": "https://arxiv.org/abs/2505.22645", "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese", "authors": ["Hanjia Lyu", "Jiebo Luo", "Jian Kang", "Allison Koenecke"], "categories": ["cs.CL", "cs.CY"], "comment": "To appear in the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT '25)", "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench)."}
{"id": "2505.22648", "pdf": "https://arxiv.org/pdf/2505.22648.pdf", "abs": "https://arxiv.org/abs/2505.22648", "title": "WebDancer: Towards Autonomous Information Seeking Agency", "authors": ["Jialong Wu", "Baixuan Li", "Runnan Fang", "Wenbiao Yin", "Liwen Zhang", "Zhengwei Tao", "Dingchu Zhang", "Zekun Xi", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent."}
{"id": "2505.22653", "pdf": "https://arxiv.org/pdf/2505.22653.pdf", "abs": "https://arxiv.org/abs/2505.22653", "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason", "authors": ["Ang Lv", "Ruobing Xie", "Xingwu Sun", "Zhanhui Kang", "Rui Yan"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason."}
{"id": "2505.22661", "pdf": "https://arxiv.org/pdf/2505.22661.pdf", "abs": "https://arxiv.org/abs/2505.22661", "title": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning", "authors": ["Qingchen Yu", "Zifan Zheng", "Ding Chen", "Simin Niu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "The evaluation of large language models (LLMs) has traditionally relied on\nstatic benchmarks, a paradigm that poses two major limitations: (1) predefined\ntest sets lack adaptability to diverse application domains, and (2)\nstandardized evaluation protocols often fail to capture fine-grained\nassessments of domain-specific knowledge and contextual reasoning abilities. To\novercome these challenges, we propose GuessArena, an adaptive evaluation\nframework grounded in adversarial game-based interactions. Inspired by the\ninteractive structure of the Guess Who I Am? game, our framework seamlessly\nintegrates dynamic domain knowledge modeling with progressive reasoning\nassessment to improve evaluation fidelity. Empirical studies across five\nvertical domains-finance, healthcare, manufacturing, information technology,\nand education-demonstrate that GuessArena effectively distinguishes LLMs in\nterms of domain knowledge coverage and reasoning chain completeness. Compared\nto conventional benchmarks, our method provides substantial advantages in\ninterpretability, scalability, and scenario adaptability."}
{"id": "2505.22662", "pdf": "https://arxiv.org/pdf/2505.22662.pdf", "abs": "https://arxiv.org/abs/2505.22662", "title": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models", "authors": ["Feng Luo", "Yu-Neng Chuang", "Guanchu Wang", "Hoang Anh Duy Le", "Shaochen Zhong", "Hongyi Liu", "Jiayi Yuan", "Yang Sui", "Vladimir Braverman", "Vipin Chaudhary", "Xia Hu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning."}
{"id": "2505.20162", "pdf": "https://arxiv.org/pdf/2505.20162.pdf", "abs": "https://arxiv.org/abs/2505.20162", "title": "Capability-Based Scaling Laws for LLM Red-Teaming", "authors": ["Alexander Panfilov", "Paul Kassianik", "Maksym Andriushchenko", "Jonas Geiping"], "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "As large language models grow in capability and agency, identifying\nvulnerabilities through red-teaming becomes vital for safe deployment. However,\ntraditional prompt-engineering approaches may prove ineffective once\nred-teaming turns into a weak-to-strong problem, where target models surpass\nred-teamers in capabilities. To study this shift, we frame red-teaming through\nthe lens of the capability gap between attacker and target. We evaluate more\nthan 500 attacker-target pairs using LLM-based jailbreak attacks that mimic\nhuman red-teamers across diverse families, sizes, and capability levels. Three\nstrong trends emerge: (i) more capable models are better attackers, (ii) attack\nsuccess drops sharply once the target's capability exceeds the attacker's, and\n(iii) attack success rates correlate with high performance on social science\nsplits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking\nscaling law that predicts attack success for a fixed target based on\nattacker-target capability gap. These findings suggest that fixed-capability\nattackers (e.g., humans) may become ineffective against future models,\nincreasingly capable open-source models amplify risks for existing systems, and\nmodel providers must accurately measure and control models' persuasive and\nmanipulative abilities to limit their effectiveness as attackers."}
{"id": "2505.21510", "pdf": "https://arxiv.org/pdf/2505.21510.pdf", "abs": "https://arxiv.org/abs/2505.21510", "title": "Complexity counts: global and local perspectives on Indo-Aryan numeral systems", "authors": ["Chundra Cathcart"], "categories": ["physics.soc-ph", "cs.CL"], "comment": null, "summary": "The numeral systems of Indo-Aryan languages such as Hindi, Gujarati, and\nBengali are highly unusual in that unlike most numeral systems (e.g., those of\nEnglish, Chinese, etc.), forms referring to 1--99 are highly non-transparent\nand are cannot be constructed using straightforward rules. As an example,\nHindi/Urdu *iky\\=anve* `91' is not decomposable into the composite elements\n*ek* `one' and *nave* `ninety' in the way that its English counterpart is. This\npaper situates Indo-Aryan languages within the typology of cross-linguistic\nnumeral systems, and explores the linguistic and non-linguistic factors that\nmay be responsible for the persistence of complex systems in these languages.\nUsing cross-linguistic data from multiple databases, we develop and employ a\nnumber of cross-linguistically applicable metrics to quantifies the complexity\nof languages' numeral systems, and demonstrate that Indo-Aryan languages have\ndecisively more complex numeral systems than the world's languages as a whole,\nthough individual Indo-Aryan languages differ from each other in terms of the\ncomplexity of the patterns they display. We investigate the factors (e.g.,\nreligion, geographic isolation, etc.) that underlie complexity in numeral\nsystems, with a focus on South Asia, in an attempt to develop an account of why\ncomplex numeral systems developed and persisted in certain Indo-Aryan languages\nbut not elsewhere. Finally, we demonstrate that Indo-Aryan numeral systems\nadhere to certain general pressures toward efficient communication found\ncross-linguistically, despite their high complexity. We call for this somewhat\noverlooked dimension of complexity to be taken seriously when discussing\ngeneral variation in cross-linguistic numeral systems."}
{"id": "2505.21527", "pdf": "https://arxiv.org/pdf/2505.21527.pdf", "abs": "https://arxiv.org/abs/2505.21527", "title": "VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining", "authors": ["Jianheng Zhuo", "Yifan Yang", "Yiwen Shao", "Yong Xu", "Dong Yu", "Kai Yu", "Xie Chen"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Automatic speech recognition (ASR) has made remarkable progress but heavily\nrelies on large-scale labeled data, which is scarce for low-resource languages\nlike Vietnamese. While existing systems such as Whisper, USM, and MMS achieve\npromising performance, their efficacy remains inadequate in terms of training\ncosts, latency, and accessibility. To address these issues, we propose VietASR,\na novel ASR training pipeline that leverages vast amounts of unlabeled data and\na small set of labeled data. Through multi-iteration ASR-biased self-supervised\nlearning on a large-scale unlabeled dataset, VietASR offers a cost-effective\nand practical solution for enhancing ASR performance. Experiments demonstrate\nthat pre-training on 70,000-hour unlabeled data and fine-tuning on merely\n50-hour labeled data yield a lightweight but powerful ASR model. It outperforms\nWhisper Large-v3 and commercial ASR systems on real-world data. Our code and\nmodels will be open-sourced to facilitate research in low-resource ASR."}
{"id": "2505.21531", "pdf": "https://arxiv.org/pdf/2505.21531.pdf", "abs": "https://arxiv.org/abs/2505.21531", "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control", "authors": ["Kunhang Li", "Jason Naradowsky", "Yansong Feng", "Yusuke Miyao"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "We explore Large Language Models (LLMs)' human motion knowledge through 3D\navatar control. Given a motion instruction, we prompt LLMs to first generate a\nhigh-level movement plan with consecutive steps (High-level Planning), then\nspecify body part positions in each step (Low-level Planning), which we\nlinearly interpolate into avatar animations as a clear verification lens for\nhuman evaluators. Through carefully designed 20 representative motion\ninstructions with full coverage of basic movement primitives and balanced body\npart usage, we conduct comprehensive evaluations including human assessment of\nboth generated animations and high-level movement plans, as well as automatic\ncomparison with oracle positions in low-level planning. We find that LLMs are\nstrong at interpreting the high-level body movements but struggle with precise\nbody part positioning. While breaking down motion queries into atomic\ncomponents improves planning performance, LLMs have difficulty with multi-step\nmovements involving high-degree-of-freedom body parts. Furthermore, LLMs\nprovide reasonable approximation for general spatial descriptions, but fail to\nhandle precise spatial specifications in text, and the precise spatial-temporal\nparameters needed for avatar control. Notably, LLMs show promise in\nconceptualizing creative motions and distinguishing culturally-specific motion\npatterns."}
{"id": "2505.21544", "pdf": "https://arxiv.org/pdf/2505.21544.pdf", "abs": "https://arxiv.org/abs/2505.21544", "title": "Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance", "authors": ["Semanto Mondal"], "categories": ["cs.CV", "cs.CL"], "comment": "There are 14 pages, 8 figures", "summary": "As a social being, we have an intimate bond with the environment. A plethora\nof things in human life, such as lifestyle, health, and food are dependent on\nthe environment and agriculture. It comes under our responsibility to support\nthe environment as well as agriculture. However, traditional farming practices\noften result in inefficient resource use and environmental challenges. To\naddress these issues, precision agriculture has emerged as a promising approach\nthat leverages advanced technologies to optimise agricultural processes. In\nthis work, a hybrid approach is proposed that combines the three different\npotential fields of model AI: object detection, large language model (LLM), and\nRetrieval-Augmented Generation (RAG). In this novel framework, we have tried to\ncombine the vision and language models to work together to identify potential\ndiseases in the tree leaf. This study introduces a novel AI-based precision\nagriculture system that uses Retrieval Augmented Generation (RAG) to provide\ncontext-aware diagnoses and natural language processing (NLP) and YOLOv8 for\ncrop disease detection. The system aims to tackle major issues with large\nlanguage models (LLMs), especially hallucinations and allows for adaptive\ntreatment plans and real-time disease detection. The system provides an\neasy-to-use interface to the farmers, which they can use to detect the\ndifferent diseases related to coffee leaves by just submitting the image of the\naffected leaf the model will detect the diseases as well as suggest potential\nremediation methodologies which aim to lower the use of pesticides, preserving\nlivelihoods, and encouraging environmentally friendly methods. With an emphasis\non scalability, dependability, and user-friendliness, the project intends to\nimprove RAG-integrated object detection systems for wider agricultural\napplications in the future."}
{"id": "2505.21548", "pdf": "https://arxiv.org/pdf/2505.21548.pdf", "abs": "https://arxiv.org/abs/2505.21548", "title": "Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?", "authors": ["Dhruv Agarwal", "Anya Shukla", "Sunayana Sitaram", "Aditya Vashistha"], "categories": ["physics.soc-ph", "cs.AI", "cs.CL", "cs.CY"], "comment": "Under review", "summary": "Large language models (LLMs) are used around the world but exhibit Western\ncultural tendencies. To address this cultural misalignment, many countries have\nbegun developing \"regional\" LLMs tailored to local communities. Yet it remains\nunclear whether these models merely speak the language of their users or also\nreflect their cultural values and practices. Using India as a case study, we\nevaluate five Indic and five global LLMs along two key dimensions: values (via\nthe Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench\nand NormAd). Across all four tasks, we find that Indic models do not align more\nclosely with Indian cultural norms than global models. In fact, an average\nAmerican person is a better proxy for Indian cultural values than any Indic\nmodel. Even prompting strategies fail to meaningfully improve alignment.\nAblations show that regional fine-tuning does not enhance cultural competence\nand may in fact hurt it by impeding recall of existing knowledge. We trace this\nfailure to the scarcity of high-quality, untranslated, and culturally grounded\npretraining and fine-tuning data. Our study positions cultural evaluation as a\nfirst-class requirement alongside multilingual benchmarks and offers a reusable\nmethodology for developers. We call for deeper investments in culturally\nrepresentative data to build and evaluate truly sovereign LLMs."}
{"id": "2505.21549", "pdf": "https://arxiv.org/pdf/2505.21549.pdf", "abs": "https://arxiv.org/abs/2505.21549", "title": "Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation", "authors": ["Daniel Csizmadia", "Andrei Codreanu", "Victor Sim", "Vighnesh Prabeau", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that\nenhances multimodal image-text retrieval while preserving the original model's\nstrong zero-shot classification capabilities. CLIP models are typically\nconstrained by fixed image resolutions and limited context, which can hinder\ntheir effectiveness in retrieval tasks that require fine-grained cross-modal\nunderstanding. DCLIP addresses these challenges through a meta teacher-student\ndistillation framework, where a cross-modal transformer teacher is fine-tuned\nto produce enriched embeddings via bidirectional cross-attention between\nYOLO-extracted image regions and corresponding textual spans. These\nsemantically and spatially aligned global representations guide the training of\na lightweight student model using a hybrid loss that combines contrastive\nlearning and cosine similarity objectives. Despite being trained on only\n~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a\nfraction of CLIP's original dataset-DCLIP significantly improves image-text\nretrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's\nzero-shot classification performance. These results demonstrate that DCLIP\neffectively mitigates the trade-off between task specialization and\ngeneralization, offering a resource-efficient, domain-adaptive, and\ndetail-sensitive solution for advanced vision-language tasks. Code available at\nhttps://anonymous.4open.science/r/DCLIP-B772/README.md."}
{"id": "2505.21569", "pdf": "https://arxiv.org/pdf/2505.21569.pdf", "abs": "https://arxiv.org/abs/2505.21569", "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools", "authors": ["Zhucong Li", "Bowei Zhang", "Jin Xiao", "Zhijian Zhou", "Fenglei Cao", "Jiaqing Liang", "Yuan Qi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md."}
{"id": "2505.21668", "pdf": "https://arxiv.org/pdf/2505.21668.pdf", "abs": "https://arxiv.org/abs/2505.21668", "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning", "authors": ["Yongchao Chen", "Yueying Liu", "Junwei Zhou", "Yilun Hao", "Jingquan Wang", "Yang Zhang", "Chuchu Fan"], "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": "33 pages, 8 figures", "summary": "Despite advances in reasoning and planning of R1-like models, Large Language\nModels (LLMs) still struggle with tasks requiring precise computation, symbolic\nmanipulation, optimization, and algorithmic reasoning, in which textual\nreasoning lacks the rigor of code execution. A key challenge is enabling LLMs\nto decide when to use textual reasoning versus code generation. While OpenAI\ntrains models to invoke a Code Interpreter as needed, public research lacks\nguidance on aligning pre-trained LLMs to effectively leverage code and\ngeneralize across diverse tasks. We present R1-Code-Interpreter, an extension\nof a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and\nreinforcement learning (RL) to autonomously generate multiple code queries\nduring step-by-step reasoning. We curate 144 reasoning and planning tasks (107\nfor training, 37 for testing), each with over 200 diverse questions. We\nfine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,\ninvestigating different answer formats, reasoning vs. non-reasoning models,\ncold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.\nUnlike prior RL work on narrow domains, we find that Code Interpreter training\nis significantly harder due to high task diversity and expensive code\nexecution, highlighting the critical role of the SFT stage. Our final model,\nR1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to\n64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with\nCode Interpreter (70.9\\%), with the emergent self-checking behavior via code\ngeneration. Datasets, Codes, and Models are available at\nhttps://github.com/yongchao98/R1-Code-Interpreter and\nhttps://huggingface.co/yongchao98."}
{"id": "2505.21749", "pdf": "https://arxiv.org/pdf/2505.21749.pdf", "abs": "https://arxiv.org/abs/2505.21749", "title": "Revisiting Bi-Linear State Transitions in Recurrent Neural Networks", "authors": ["M. Reza Ebrahimi", "Roland Memisevic"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The role of hidden units in recurrent neural networks is typically seen as\nmodeling memory, with research focusing on enhancing information retention\nthrough gating mechanisms. A less explored perspective views hidden units as\nactive participants in the computation performed by the network, rather than\npassive memory stores. In this work, we revisit bi-linear operations, which\ninvolve multiplicative interactions between hidden units and input embeddings.\nWe demonstrate theoretically and empirically that they constitute a natural\ninductive bias for representing the evolution of hidden states in state\ntracking tasks. These are the simplest type of task that require hidden units\nto actively contribute to the behavior of the network. We also show that\nbi-linear state updates form a natural hierarchy corresponding to state\ntracking tasks of increasing complexity, with popular linear recurrent networks\nsuch as Mamba residing at the lowest-complexity center of that hierarchy."}
{"id": "2505.21753", "pdf": "https://arxiv.org/pdf/2505.21753.pdf", "abs": "https://arxiv.org/abs/2505.21753", "title": "From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism", "authors": ["Roberto Ulloa", "Eve M. Zucker", "Daniel Bultmann", "David J. Simon", "Mykola Makhortykh"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The proliferation of large language models (LLMs) can influence how\nhistorical narratives are disseminated and perceived. This study explores the\nimplications of LLMs' responses on the representation of mass atrocity memory,\nexamining whether generative AI systems contribute to prosthetic memory, i.e.,\nmediated experiences of historical events, or to what we term \"prosthetic\ndenial,\" the AI-mediated erasure or distortion of atrocity memories. We argue\nthat LLMs function as interfaces that can elicit prosthetic memories and,\ntherefore, act as experiential sites for memory transmission, but also\nintroduce risks of denialism, particularly when their outputs align with\ncontested or revisionist narratives. To empirically assess these risks, we\nconducted a comparative audit of five LLMs (Claude, GPT, Llama, Mixtral, and\nGemini) across four historical case studies: the Holodomor, the Holocaust, the\nCambodian Genocide, and the genocide against the Tutsis in Rwanda. Each model\nwas prompted with questions addressing common denialist claims in English and\nan alternative language relevant to each case (Ukrainian, German, Khmer, and\nFrench). Our findings reveal that while LLMs generally produce accurate\nresponses for widely documented events like the Holocaust, significant\ninconsistencies and susceptibility to denialist framings are observed for more\nunderrepresented cases like the Cambodian Genocide. The disparities highlight\nthe influence of training data availability and the probabilistic nature of LLM\nresponses on memory integrity. We conclude that while LLMs extend the concept\nof prosthetic memory, their unmoderated use risks reinforcing historical\ndenialism, raising ethical concerns for (digital) memory preservation, and\npotentially challenging the advantageous role of technology associated with the\noriginal values of prosthetic memory."}
{"id": "2505.21755", "pdf": "https://arxiv.org/pdf/2505.21755.pdf", "abs": "https://arxiv.org/abs/2505.21755", "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering", "authors": ["Chengyue Huang", "Brisa Maneechotesuwan", "Shivang Chopra", "Zsolt Kira"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA ."}
{"id": "2505.21784", "pdf": "https://arxiv.org/pdf/2505.21784.pdf", "abs": "https://arxiv.org/abs/2505.21784", "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation", "authors": ["Tharindu Kumarage", "Ninareh Mehrabi", "Anil Ramakrishna", "Xinyan Zhao", "Richard Zemel", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta", "Charith Peris"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies\nbefore generating responses, thereby mitigating limitations in existing safety\nmeasures such as over-refusal and jailbreak vulnerabilities. However,\nimplementing this paradigm is challenging due to the resource-intensive process\nof creating high-quality policy-embedded chain-of-thought (CoT) datasets while\nensuring reasoning remains accurate and free from hallucinations or policy\nconflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation\nfor Safety Reasoning, a novel data generation recipe that leverages multi-agent\ndeliberation to iteratively expand reasoning on safety policies. A data refiner\nstage in AIDSAFE ensures high-quality outputs by eliminating repetitive,\nredundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong\nfoundation for supervised fine-tuning (SFT)-based safety training.\nAdditionally, to address the need of preference data in alignment stages, such\nas DPO training, we introduce a supplemental recipe that uses belief\naugmentation to create distinct selected and rejected CoT samples. Our\nevaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy\nadherence and reasoning quality. Consequently, we show that fine-tuning\nopen-source LLMs on these CoTs can significantly improve safety generalization\nand jailbreak robustness while maintaining acceptable utility and over-refusal\naccuracy. AIDSAFE-generated CoT datasets can be found here:\nhttps://huggingface.co/datasets/AmazonScience/AIDSAFE"}
{"id": "2505.21785", "pdf": "https://arxiv.org/pdf/2505.21785.pdf", "abs": "https://arxiv.org/abs/2505.21785", "title": "Born a Transformer -- Always a Transformer?", "authors": ["Yana Veitsman", "Mayank Jobanputra", "Yash Sarrof", "Aleksandra Bakalova", "Vera Demberg", "Ellie Pavlick", "Michael Hahn"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have theoretical limitations in modeling certain\nsequence-to-sequence tasks, yet it remains largely unclear if these limitations\nplay a role in large-scale pretrained LLMs, or whether LLMs might effectively\novercome these constraints in practice due to the scale of both the models\nthemselves and their pretraining data. We explore how these architectural\nconstraints manifest after pretraining, by studying a family of\n$\\textit{retrieval}$ and $\\textit{copying}$ tasks inspired by Liu et al.\n[2024]. We use the recently proposed C-RASP framework for studying length\ngeneralization [Huang et al., 2025b] to provide guarantees for each of our\nsettings. Empirically, we observe an $\\textit{induction-versus-anti-induction}$\nasymmetry, where pretrained models are better at retrieving tokens to the right\n(induction) rather than the left (anti-induction) of a query token. This\nasymmetry disappears upon targeted fine-tuning if length-generalization is\nguaranteed by theory. Mechanistic analysis reveals that this asymmetry is\nconnected to the differences in the strength of induction versus anti-induction\ncircuits within pretrained Transformers. We validate our findings through\npractical experiments on real-world tasks demonstrating reliability risks. Our\nresults highlight that pretraining selectively enhances certain Transformer\ncapabilities, but does not overcome fundamental length-generalization limits."}
{"id": "2505.21800", "pdf": "https://arxiv.org/pdf/2505.21800.pdf", "abs": "https://arxiv.org/abs/2505.21800", "title": "From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs", "authors": ["Stanley Yu", "Vaidehi Bulusu", "Oscar Yasunaga", "Clayton Lau", "Cole Blondin", "Sean O'Brien", "Kevin Zhu", "Vasu Sharma"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong conversational abilities but\noften generate falsehoods. Prior work suggests that the truthfulness of simple\npropositions can be represented as a single linear direction in a model's\ninternal activations, but this may not fully capture its underlying geometry.\nIn this work, we extend the concept cone framework, recently introduced for\nmodeling refusal, to the domain of truth. We identify multi-dimensional cones\nthat causally mediate truth-related behavior across multiple LLM families. Our\nresults are supported by three lines of evidence: (i) causal interventions\nreliably flip model responses to factual statements, (ii) learned cones\ngeneralize across model architectures, and (iii) cone-based interventions\npreserve unrelated model behavior. These findings reveal the richer,\nmultidirectional structure governing simple true/false propositions in LLMs and\nhighlight concept cones as a promising tool for probing abstract behaviors."}
{"id": "2505.21815", "pdf": "https://arxiv.org/pdf/2505.21815.pdf", "abs": "https://arxiv.org/abs/2505.21815", "title": "Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking", "authors": ["Yunyi Zhang", "Ruozhen Yang", "Siqi Jiao", "SeongKu Kang", "Jiawei Han"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Scientific paper retrieval is essential for supporting literature discovery\nand research. While dense retrieval methods demonstrate effectiveness in\ngeneral-purpose tasks, they often fail to capture fine-grained scientific\nconcepts that are essential for accurate understanding of scientific queries.\nRecent studies also use large language models (LLMs) for query understanding;\nhowever, these methods often lack grounding in corpus-specific knowledge and\nmay generate unreliable or unfaithful content. To overcome these limitations,\nwe propose SemRank, an effective and efficient paper retrieval framework that\ncombines LLM-guided query understanding with a concept-based semantic index.\nEach paper is indexed using multi-granular scientific concepts, including\ngeneral research topics and detailed key phrases. At query time, an LLM\nidentifies core concepts derived from the corpus to explicitly capture the\nquery's information need. These identified concepts enable precise semantic\nmatching, significantly enhancing retrieval accuracy. Experiments show that\nSemRank consistently improves the performance of various base retrievers,\nsurpasses strong existing LLM-based baselines, and remains highly efficient."}
{"id": "2505.21825", "pdf": "https://arxiv.org/pdf/2505.21825.pdf", "abs": "https://arxiv.org/abs/2505.21825", "title": "Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones", "authors": ["Parsa Mirtaheri", "Ezra Edelman", "Samy Jelassi", "Eran Malach", "Enric Boix-Adsera"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time computation has emerged as a promising scaling axis for\nimproving large language model reasoning. However, despite yielding impressive\nperformance, the optimal allocation of inference-time computation remains\npoorly understood. A central question is whether to prioritize sequential\nscaling (e.g., longer chains of thought) or parallel scaling (e.g., majority\nvoting across multiple short chains of thought). In this work, we seek to\nilluminate the landscape of test-time scaling by demonstrating the existence of\nreasoning settings where sequential scaling offers an exponential advantage\nover parallel scaling. These settings are based on graph connectivity problems\nin challenging distributions of graphs. We validate our theoretical findings\nwith comprehensive experiments across a range of language models, including\nmodels trained from scratch for graph connectivity with different chain of\nthought strategies as well as large reasoning models."}
{"id": "2505.21863", "pdf": "https://arxiv.org/pdf/2505.21863.pdf", "abs": "https://arxiv.org/abs/2505.21863", "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning", "authors": ["Shikhhar Siingh", "Abhinav Rawat", "Vivek Gupta", "Chitta Baral"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Publicly significant images from events hold valuable contextual information,\ncrucial for journalism and education. However, existing methods often struggle\nto extract this relevance accurately. To address this, we introduce GETReason\n(Geospatial Event Temporal Reasoning), a framework that moves beyond\nsurface-level image descriptions to infer deeper contextual meaning. We propose\nthat extracting global event, temporal, and geospatial information enhances\nunderstanding of an image's significance. Additionally, we introduce GREAT\n(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric\nfor evaluating reasoning-based image understanding. Our layered multi-agent\napproach, assessed using a reasoning-weighted metric, demonstrates that\nmeaningful insights can be inferred, effectively linking images to their\nbroader event context."}
{"id": "2505.21880", "pdf": "https://arxiv.org/pdf/2505.21880.pdf", "abs": "https://arxiv.org/abs/2505.21880", "title": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation", "authors": ["Yu-Lun Song", "Chung-En Tsern", "Che-Cheng Wu", "Yu-Ming Chang", "Syuan-Bo Huang", "Wei-Chu Chen", "Michael Chia-Liang Lin", "Yu-Ta Lin"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "comment": "8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM\n  (Computational Urban Planning and Urban Management) Conference held by\n  University College London (UCL) in 2025", "summary": "This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications."}
{"id": "2505.21907", "pdf": "https://arxiv.org/pdf/2505.21907.pdf", "abs": "https://arxiv.org/abs/2505.21907", "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy", "authors": ["Saleh Afzoon", "Zahra Jahanandish", "Phuong Thao Huynh", "Amin Beheshti", "Usman Naseem"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI copilots, context-aware, AI-powered systems designed to assist users in\ntasks such as software development and content creation, are becoming integral\nto modern workflows. As these systems grow in capability and adoption,\npersonalization has emerged as a cornerstone for ensuring usability, trust, and\nproductivity. Central to this personalization is preference optimization: the\nability of AI copilots to detect, interpret, and align with individual user\npreferences. While personalization techniques are well-established in domains\nlike recommender systems and dialogue agents, their adaptation to interactive,\nreal-time systems like AI copilots remains fragmented and underexplored. This\nsurvey addresses this gap by synthesizing research on how user preferences are\ncaptured, modeled, and refined within the design of AI copilots. We introduce a\nunified definition of AI copilots and propose a phase-based taxonomy of\npreference optimization strategies, structured around pre-interaction,\nmid-interaction, and post-interaction stages. We analyze techniques for\nacquiring preference signals, modeling user intent, and integrating feedback\nloops, highlighting both established approaches and recent innovations. By\nbridging insights from AI personalization, human-AI collaboration, and large\nlanguage model adaptation, this survey provides a structured foundation for\ndesigning adaptive, preference-aware AI copilots. It offers a holistic view of\nthe available preference resources, how they can be leveraged, and which\ntechnical approaches are most suited to each stage of system design."}
{"id": "2505.21930", "pdf": "https://arxiv.org/pdf/2505.21930.pdf", "abs": "https://arxiv.org/abs/2505.21930", "title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets", "authors": ["Dongyue Li", "Ziniu Zhang", "Lu Wang", "Hongyang R. Zhang"], "categories": ["cs.LG", "cs.CL"], "comment": "17 pages. To appear in ACL'25", "summary": "This paper develops an ensemble method for fine-tuning a language model to\nmultiple datasets. Existing methods, such as quantized LoRA (QLoRA), are\nefficient when adapting to a single dataset. When training on multiple datasets\nof different tasks, a common setup in practice, it remains unclear how to\ndesign an efficient adaptation for fine-tuning language models. We propose to\nuse an ensemble of multiple smaller adapters instead of a single adapter per\ntask. We design an efficient algorithm that partitions $n$ datasets into $m$\ngroups, where $m$ is typically much smaller than $n$ in practice, and train one\nadapter for each group before taking a weighted combination to form the\nensemble. The algorithm leverages a first-order approximation property of\nlow-rank adaptation to quickly obtain the fine-tuning performances of dataset\ncombinations since methods like LoRA stay close to the base model. Hence, we\nuse the gradients of the base model to estimate its behavior during\nfine-tuning. Empirically, this approximation holds with less than $1\\%$ error\non models with up to $34$ billion parameters, leading to an estimation of true\nfine-tuning performances under $5\\%$ error while speeding up computation\ncompared to base fine-tuning by $105$ times. When applied to fine-tune Llama\nand GPT models on ten text classification tasks, our approach provides up to\n$10\\%$ higher average test accuracy over QLoRA, with only $9\\%$ more FLOPs. On\na Llama model with $34$ billion parameters, an ensemble of QLoRA increases test\naccuracy by $3\\%$ compared to QLoRA, with only $8\\%$ more FLOPs."}
{"id": "2505.21956", "pdf": "https://arxiv.org/pdf/2505.21956.pdf", "abs": "https://arxiv.org/abs/2505.21956", "title": "Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation", "authors": ["Mengdan Zhu", "Senhao Cheng", "Guangji Bai", "Yifei Zhang", "Liang Zhao"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Text-to-image generation increasingly demands access to domain-specific,\nfine-grained, and rapidly evolving knowledge that pretrained models cannot\nfully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to\naddress this by retrieving globally relevant images, but they fail when no\nsingle image contains all desired elements from a complex user query. We\npropose Cross-modal RAG, a novel framework that decomposes both queries and\nimages into sub-dimensional components, enabling subquery-aware retrieval and\ngeneration. Our method introduces a hybrid retrieval strategy - combining a\nsub-dimensional sparse retriever with a dense retriever - to identify a\nPareto-optimal set of images, each contributing complementary aspects of the\nquery. During generation, a multimodal large language model is guided to\nselectively condition on relevant visual features aligned to specific\nsubqueries, ensuring subquery-aware image synthesis. Extensive experiments on\nMS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal\nRAG significantly outperforms existing baselines in both retrieval and\ngeneration quality, while maintaining high efficiency."}
{"id": "2505.21959", "pdf": "https://arxiv.org/pdf/2505.21959.pdf", "abs": "https://arxiv.org/abs/2505.21959", "title": "EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles", "authors": ["Aakriti Agrawal", "Mucong Ding", "Zora Che", "Chenghao Deng", "Anirudh Satheesh", "Bang An", "Bayan Bruss", "John Langford", "Furong Huang"], "categories": ["cs.LG", "cs.CL"], "comment": "Superalignment. arXiv admin note: substantial text overlap with\n  arXiv:2410.04571", "summary": "With Large Language Models (LLMs) rapidly approaching and potentially\nsurpassing human-level performance, it has become imperative to develop\napproaches capable of effectively supervising and enhancing these powerful\nmodels using smaller, human-level models exposed to only human-level data. We\naddress this critical weak-to-strong (W2S) generalization challenge by\nproposing a novel method aimed at improving weak experts, by training on the\nsame limited human-level data, enabling them to generalize to complex,\nsuper-human-level tasks. Our approach, called \\textbf{EnsemW2S}, employs a\ntoken-level ensemble strategy that iteratively combines multiple weak experts,\nsystematically addressing the shortcomings identified in preceding iterations.\nBy continuously refining these weak models, we significantly enhance their\ncollective ability to supervise stronger student models. We extensively\nevaluate the generalization performance of both the ensemble of weak experts\nand the subsequent strong student model across in-distribution (ID) and\nout-of-distribution (OOD) datasets. For OOD, we specifically introduce question\ndifficulty as an additional dimension for defining distributional shifts. Our\nempirical results demonstrate notable improvements, achieving 4\\%, and 3.2\\%\nimprovements on ID datasets and, upto 6\\% and 2.28\\% on OOD datasets for\nexperts and student models respectively, underscoring the effectiveness of our\nproposed method in advancing W2S generalization."}
{"id": "2505.21964", "pdf": "https://arxiv.org/pdf/2505.21964.pdf", "abs": "https://arxiv.org/abs/2505.21964", "title": "UI-Evol: Automatic Knowledge Evolving for Computer Use Agents", "authors": ["Ziyun Zhang", "Xinyi Liu", "Xiaoyi Zhang", "Jun Wang", "Gang Chen", "Yan Lu"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "External knowledge has played a crucial role in the recent development of\ncomputer use agents. We identify a critical knowledge-execution gap: retrieved\nknowledge often fails to translate into effective real-world task execution.\nOur analysis shows even 90\\% correct knowledge yields only 41\\% execution\nsuccess rate. To bridge this gap, we propose UI-Evol, a plug-and-play module\nfor autonomous GUI knowledge evolution. UI-Evol consists of two stages: a\nRetrace Stage that extracts faithful objective action sequences from actual\nagent-environment interactions, and a Critique Stage that refines existing\nknowledge by comparing these sequences against external references. We conduct\ncomprehensive experiments on the OSWorld benchmark with the state-of-the-art\nAgent S2. Our results demonstrate that UI-Evol not only significantly boosts\ntask performance but also addresses a previously overlooked issue of high\nbehavioral standard deviation in computer use agents, leading to superior\nperformance on computer use tasks and substantially improved agent reliability."}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966.pdf", "abs": "https://arxiv.org/abs/2505.21966", "title": "MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "16 pages and 15 figures", "summary": "We introduce MapStory, an LLM-powered animation authoring tool that generates\neditable map animation sequences directly from natural language text. Given a\nuser-written script, MapStory leverages an agentic architecture to\nautomatically produce a scene breakdown, which decomposes the script into key\nanimation building blocks such as camera movements, visual highlights, and\nanimated elements. Our system includes a researcher component that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nthe automatic extraction of relevant regions, paths, and coordinates while\nallowing users to edit and query for changes or additional information to\nrefine the results. Additionally, users can fine-tune parameters of these\nblocks through an interactive timeline editor. We detail the system's design\nand architecture, informed by formative interviews with professional animators\nand an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories."}
{"id": "2505.21981", "pdf": "https://arxiv.org/pdf/2505.21981.pdf", "abs": "https://arxiv.org/abs/2505.21981", "title": "Learning Compositional Behaviors from Demonstration and Language", "authors": ["Weiyu Liu", "Neil Nie", "Ruohan Zhang", "Jiayuan Mao", "Jiajun Wu"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": "Presented at CoRL 2024 and as an Oral Presentation at the 2024 CoRL\n  LEAP Workshop. The first two authors contributed equally. The last two\n  authors jointly advised the project. For videos and additional results,\n  visit: https://blade-bot.github.io/", "summary": "We introduce Behavior from Language and Demonstration (BLADE), a framework\nfor long-horizon robotic manipulation by integrating imitation learning and\nmodel-based planning. BLADE leverages language-annotated demonstrations,\nextracts abstract action knowledge from large language models (LLMs), and\nconstructs a library of structured, high-level action representations. These\nrepresentations include preconditions and effects grounded in visual perception\nfor each high-level action, along with corresponding controllers implemented as\nneural network-based policies. BLADE can recover such structured\nrepresentations automatically, without manually labeled states or symbolic\ndefinitions. BLADE shows significant capabilities in generalizing to novel\nsituations, including novel initial states, external state perturbations, and\nnovel goals. We validate the effectiveness of our approach both in simulation\nand on real robots with a diverse set of objects with articulated parts,\npartial observability, and geometric constraints."}
{"id": "2505.22088", "pdf": "https://arxiv.org/pdf/2505.22088.pdf", "abs": "https://arxiv.org/abs/2505.22088", "title": "Visual Cues Support Robust Turn-taking Prediction in Noise", "authors": ["Sam O'Connor Russell", "Naomi Harte"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "5 pages", "summary": "Accurate predictive turn-taking models (PTTMs) are essential for naturalistic\nhuman-robot interaction. However, little is known about their performance in\nnoise. This study therefore explores PTTM performance in types of noise likely\nto be encountered once deployed. Our analyses reveal PTTMs are highly sensitive\nto noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10\ndB music noise. Training with noisy data enables a multimodal PTTM, which\nincludes visual features to better exploit visual cues, with 72% accuracy in 10\ndB music noise. The multimodal PTTM outperforms the audio-only PTTM across all\nnoise types and SNRs, highlighting its ability to exploit visual cues; however,\nthis does not always generalise to new types of noise. Analysis also reveals\nthat successful training relies on accurate transcription, limiting the use of\nASR-derived transcriptions to clean conditions. We make code publicly available\nfor future research."}
{"id": "2505.22146", "pdf": "https://arxiv.org/pdf/2505.22146.pdf", "abs": "https://arxiv.org/abs/2505.22146", "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "authors": ["Guangfu Hao", "Haojie Wen", "Liangxuna Guo", "Yang Chen", "Yanchao Bi", "Shan Yu"], "categories": ["cs.CV", "cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Flexible tool selection reflects a complex cognitive ability that\ndistinguishes humans from other species, yet computational models that capture\nthis ability remain underdeveloped. We developed a framework using\nlow-dimensional attribute representations to bridge visual tool perception and\nlinguistic task understanding. We constructed a comprehensive dataset (ToolNet)\ncontaining 115 common tools labeled with 13 carefully designed attributes\nspanning physical, functional, and psychological properties, paired with\nnatural language scenarios describing tool usage. Visual encoders (ResNet or\nViT) extract attributes from tool images while fine-tuned language models\n(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our\napproach achieves 74% accuracy in tool selection tasks-significantly\noutperforming direct tool matching (20%) and smaller multimodal models\n(21%-58%), while approaching performance of much larger models like GPT-4o\n(73%) with substantially fewer parameters. Ablation studies revealed that\nmanipulation-related attributes (graspability, hand-relatedness, elongation)\nconsistently prove most critical across modalities. This work provides a\nparameter-efficient, interpretable solution that mimics human-like tool\ncognition, advancing both cognitive science understanding and practical\napplications in tool selection tasks."}
{"id": "2505.22150", "pdf": "https://arxiv.org/pdf/2505.22150.pdf", "abs": "https://arxiv.org/abs/2505.22150", "title": "Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging", "authors": ["Runze Xia", "Shuo Feng", "Renzhi Wang", "Congchi Yin", "Xuyun Wen", "Piji Li"], "categories": ["cs.CV", "cs.CL"], "comment": "CogSci2025", "summary": "Brain-to-Image reconstruction aims to recover visual stimuli perceived by\nhumans from brain activity. However, the reconstructed visual stimuli often\nmissing details and semantic inconsistencies, which may be attributed to\ninsufficient semantic information. To address this issue, we propose an\napproach named Fine-grained Brain-to-Image reconstruction (FgB2I), which\nemploys fine-grained text as bridge to improve image reconstruction. FgB2I\ncomprises three key stages: detail enhancement, decoding fine-grained text\ndescriptions, and text-bridged brain-to-image reconstruction. In the\ndetail-enhancement stage, we leverage large vision-language models to generate\nfine-grained captions for visual stimuli and experimentally validate its\nimportance. We propose three reward metrics (object accuracy, text-image\nsemantic similarity, and image-image semantic similarity) to guide the language\nmodel in decoding fine-grained text descriptions from fMRI signals. The\nfine-grained text descriptions can be integrated into existing reconstruction\nmethods to achieve fine-grained Brain-to-Image reconstruction."}
{"id": "2505.22203", "pdf": "https://arxiv.org/pdf/2505.22203.pdf", "abs": "https://arxiv.org/abs/2505.22203", "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning", "authors": ["Yuzhen Huang", "Weihao Zeng", "Xingshan Zeng", "Qi Zhu", "Junxian He"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning."}
{"id": "2505.22222", "pdf": "https://arxiv.org/pdf/2505.22222.pdf", "abs": "https://arxiv.org/abs/2505.22222", "title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation", "authors": ["Yunsoo Kim", "Jinge Wu", "Su-Hwan Kim", "Pardeep Vasudev", "Jiashu Shen", "Honghan Wu"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in multimodal Large Language Models (LLMs) have\nsignificantly enhanced the automation of medical image analysis, particularly\nin generating radiology reports from chest X-rays (CXR). However, these models\nstill suffer from hallucinations and clinically significant errors, limiting\ntheir reliability in real-world applications. In this study, we propose Look &\nMark (L&M), a novel grounding fixation strategy that integrates radiologist eye\nfixations (Look) and bounding box annotations (Mark) into the LLM prompting\nframework. Unlike conventional fine-tuning, L&M leverages in-context learning\nto achieve substantial performance gains without retraining. When evaluated\nacross multiple domain-specific and general-purpose models, L&M demonstrates\nsignificant gains, including a 1.2% improvement in overall metrics (A.AVG) for\nCXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for\nLLaVA-Med. General-purpose models also benefit from L&M combined with\nin-context learning, with LLaVA-OV achieving an 87.3% clinical average\nperformance (C.AVG)-the highest among all models, even surpassing those\nexplicitly trained for CXR report generation. Expert evaluations further\nconfirm that L&M reduces clinically significant errors (by 0.43 average errors\nper report), such as false predictions and omissions, enhancing both accuracy\nand reliability. These findings highlight L&M's potential as a scalable and\nefficient solution for AI-assisted radiology, paving the way for improved\ndiagnostic workflows in low-resource clinical settings."}
{"id": "2505.22231", "pdf": "https://arxiv.org/pdf/2505.22231.pdf", "abs": "https://arxiv.org/abs/2505.22231", "title": "Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis", "authors": ["Stefan Bleeck"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Traditional audiometry often fails to fully characterize the functional\nimpact of hearing loss on speech understanding, particularly supra-threshold\ndeficits and frequency-specific perception challenges in conditions like\npresbycusis. This paper presents the development and simulated evaluation of a\nnovel Automatic Speech Recognition (ASR)-based frequency-specific speech test\ndesigned to provide granular diagnostic insights. Our approach leverages ASR to\nsimulate the perceptual effects of moderate sloping hearing loss by processing\nspeech stimuli under controlled acoustic degradation and subsequently analyzing\nphoneme-level confusion patterns. Key findings indicate that simulated hearing\nloss introduces specific phoneme confusions, predominantly affecting\nhigh-frequency consonants (e.g., alveolar/palatal to labiodental substitutions)\nand leading to significant phoneme deletions, consistent with the acoustic cues\ndegraded in presbycusis. A test battery curated from these ASR-derived\nconfusions demonstrated diagnostic value, effectively differentiating between\nsimulated normal-hearing and hearing-impaired listeners in a comprehensive\nsimulation. This ASR-driven methodology offers a promising avenue for\ndeveloping objective, granular, and frequency-specific hearing assessment tools\nthat complement traditional audiometry. Future work will focus on validating\nthese findings with human participants and exploring the integration of\nadvanced AI models for enhanced diagnostic precision."}
{"id": "2505.22251", "pdf": "https://arxiv.org/pdf/2505.22251.pdf", "abs": "https://arxiv.org/abs/2505.22251", "title": "Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition", "authors": ["Yuan Tseng", "Titouan Parcollet", "Rogier van Dalen", "Shucong Zhang", "Sourav Bhattacharya"], "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Recent work suggests that large language models (LLMs) can improve\nperformance of speech tasks compared to existing systems. To support their\nclaims, results on LibriSpeech and Common Voice are often quoted. However, this\nwork finds that a substantial amount of the LibriSpeech and Common Voice\nevaluation sets appear in public LLM pretraining corpora. This calls into\nquestion the reliability of findings drawn from these two datasets. To measure\nthe impact of contamination, LLMs trained with or without contamination are\ncompared, showing that a contaminated LLM is more likely to generate test\nsentences it has seen during training. Speech recognisers using contaminated\nLLMs shows only subtle differences in error rates, but assigns significantly\nhigher probabilities to transcriptions seen during training. Results show that\nLLM outputs can be biased by tiny amounts of data contamination, highlighting\nthe importance of evaluating LLM-based speech systems with held-out data."}
{"id": "2505.22255", "pdf": "https://arxiv.org/pdf/2505.22255.pdf", "abs": "https://arxiv.org/abs/2505.22255", "title": "Train Sparse Autoencoders Efficiently by Utilizing Features Correlation", "authors": ["Vadim Kurochkin", "Yaroslav Aksenov", "Daniil Laptev", "Daniil Gavrilov", "Nikita Balagansky"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have demonstrated significant promise in\ninterpreting the hidden states of language models by decomposing them into\ninterpretable latent directions. However, training SAEs at scale remains\nchallenging, especially when large dictionary sizes are used. While decoders\ncan leverage sparse-aware kernels for efficiency, encoders still require\ncomputationally intensive linear operations with large output dimensions. To\naddress this, we propose KronSAE, a novel architecture that factorizes the\nlatent representation via Kronecker product decomposition, drastically reducing\nmemory and computational overhead. Furthermore, we introduce mAND, a\ndifferentiable activation function approximating the binary AND operation,\nwhich improves interpretability and performance in our factorized framework."}
{"id": "2505.22271", "pdf": "https://arxiv.org/pdf/2505.22271.pdf", "abs": "https://arxiv.org/abs/2505.22271", "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models", "authors": ["Yongcan Yu", "Yanbo Wang", "Ran He", "Jian Liang"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Under Review", "summary": "While (multimodal) large language models (LLMs) have attracted widespread\nattention due to their exceptional capabilities, they remain vulnerable to\njailbreak attacks. Various defense methods are proposed to defend against\njailbreak attacks, however, they are often tailored to specific types of\njailbreak attacks, limiting their effectiveness against diverse adversarial\nstrategies. For instance, rephrasing-based defenses are effective against text\nadversarial jailbreaks but fail to counteract image-based attacks. To overcome\nthese limitations, we propose a universal defense framework, termed Test-time\nIMmunization (TIM), which can adaptively defend against various jailbreak\nattacks in a self-evolving way. Specifically, TIM initially trains a gist token\nfor efficient detection, which it subsequently applies to detect jailbreak\nactivities during inference. When jailbreak attempts are identified, TIM\nimplements safety fine-tuning using the detected jailbreak instructions paired\nwith refusal answers. Furthermore, to mitigate potential performance\ndegradation in the detector caused by parameter updates during safety\nfine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy\nof TIM."}
{"id": "2505.22290", "pdf": "https://arxiv.org/pdf/2505.22290.pdf", "abs": "https://arxiv.org/abs/2505.22290", "title": "Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling", "authors": ["Fanzeng Xia", "Yidong Luo", "Tinko Sebastian Bartels", "Yaqi Xu", "Tongxin Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent research has highlighted that Large Language Models (LLMs), even when\ntrained to generate extended long reasoning steps, still face significant\nchallenges on hard reasoning problems. However, much of the existing literature\nrelies on direct prompting with simple in-context learning examples for\nevaluation, which largely overlooks advanced techniques to elicit LLMs'\ndeliberate reasoning before drawing conclusions that LLMs hit a performance\nceiling. In this paper, we systematically explore the combined potential of\nin-context search and test-time scaling on super hard reasoning tasks. We find\nthat by employing advanced in-context search prompting to LLMs augmented with\ninternal scaling, one can achieve transformative performance breakthroughs on\ntasks previously deemed \"unsolvable\" (e.g., reported success rates below 5%).\nWe provide both empirical results and theoretical analysis of how this\ncombination can unleash LLM reasoning capabilities: i) Empirically, on\ncontrolled NP-hard tasks and complex real-world planning benchmarks, our\napproach achieves up to a 30x improvement in success rates compared to\npreviously reported results without any external mechanisms; ii) Theoretically,\nwe show that in-context search prompting, when combined with internal scaling,\nsignificantly extends the complexity class of solvable reasoning problems.\nThese findings challenge prevailing assumptions about the limitations of LLMs\non complex tasks, indicating that current evaluation paradigms systematically\nunderestimate their true potential. Our work calls for a critical reassessment\nof how LLM reasoning is benchmarked and a more robust evaluation strategy that\nfully captures the true capabilities of contemporary LLMs, which can lead to a\nbetter understanding of their operational reasoning boundaries in real-world\ndeployments."}
{"id": "2505.22312", "pdf": "https://arxiv.org/pdf/2505.22312.pdf", "abs": "https://arxiv.org/abs/2505.22312", "title": "Skywork Open Reasoner 1 Technical Report", "authors": ["Jujie He", "Jiacai Liu", "Chris Yuhao Liu", "Rui Yan", "Chaojie Wang", "Peng Cheng", "Xiaoyu Zhang", "Fuxiang Zhang", "Jiacheng Xu", "Wei Shen", "Siyuan Li", "Liang Zeng", "Tianwen Wei", "Cheng Cheng", "Bo An", "Yang Liu", "Yahui Zhou"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets."}
{"id": "2505.22411", "pdf": "https://arxiv.org/pdf/2505.22411.pdf", "abs": "https://arxiv.org/abs/2505.22411", "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering", "authors": ["Yao Huang", "Huanran Chen", "Shouwei Ruan", "Yichi Zhang", "Xingxing Wei", "Yinpeng Dong"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in solving complex tasks such as mathematics and coding. However,\nthese models frequently exhibit a phenomenon known as overthinking during\ninference, characterized by excessive validation loops and redundant\ndeliberation, leading to substantial computational overheads. In this paper, we\naim to mitigate overthinking by investigating the underlying mechanisms from\nthe perspective of mechanistic interpretability. We first showcase that the\ntendency of overthinking can be effectively captured by a single direction in\nthe model's activation space and the issue can be eased by intervening the\nactivations along this direction. However, this efficacy soon reaches a plateau\nand even deteriorates as the intervention strength increases. We therefore\nsystematically explore the activation space and find that the overthinking\nphenomenon is actually tied to a low-dimensional manifold, which indicates that\nthe limited effect stems from the noises introduced by the high-dimensional\nsteering direction. Based on this insight, we propose Manifold Steering, a\nnovel approach that elegantly projects the steering direction onto the\nlow-dimensional activation manifold given the theoretical approximation of the\ninterference noise. Extensive experiments on DeepSeek-R1 distilled models\nvalidate that our method reduces output tokens by up to 71% while maintaining\nand even improving the accuracy on several mathematical benchmarks. Our method\nalso exhibits robust cross-domain transferability, delivering consistent token\nreduction performance in code generation and knowledge-based QA tasks. Code is\navailable at: https://github.com/Aries-iai/Manifold_Steering."}
{"id": "2505.22425", "pdf": "https://arxiv.org/pdf/2505.22425.pdf", "abs": "https://arxiv.org/abs/2505.22425", "title": "Scaling Reasoning without Attention", "authors": ["Xueliang Zhao", "Wei Wu", "Lingpeng Kong"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."}
{"id": "2505.22457", "pdf": "https://arxiv.org/pdf/2505.22457.pdf", "abs": "https://arxiv.org/abs/2505.22457", "title": "Fostering Video Reasoning via Next-Event Prediction", "authors": ["Haonan Wang", "Hongfu Liu", "Xiangyan Liu", "Chao Du", "Kenji Kawaguchi", "Ye Wang", "Tianyu Pang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs."}
{"id": "2505.22487", "pdf": "https://arxiv.org/pdf/2505.22487.pdf", "abs": "https://arxiv.org/abs/2505.22487", "title": "Effective Context in Neural Speech Models", "authors": ["Yen Meng", "Sharon Goldwater", "Hao Tang"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Modern neural speech models benefit from having longer context, and many\napproaches have been proposed to increase the maximum context a model can use.\nHowever, few have attempted to measure how much context these models actually\nuse, i.e., the effective context. Here, we propose two approaches to measuring\nthe effective context, and use them to analyze different speech Transformers.\nFor supervised models, we find that the effective context correlates well with\nthe nature of the task, with fundamental frequency tracking, phone\nclassification, and word classification requiring increasing amounts of\neffective context. For self-supervised models, we find that effective context\nincreases mainly in the early layers, and remains relatively short -- similar\nto the supervised phone model. Given that these models do not use a long\ncontext during prediction, we show that HuBERT can be run in streaming mode\nwithout modification to the architecture and without further fine-tuning."}
{"id": "2505.22525", "pdf": "https://arxiv.org/pdf/2505.22525.pdf", "abs": "https://arxiv.org/abs/2505.22525", "title": "Thinking with Generated Images", "authors": ["Ethan Chern", "Zhulin Hu", "Steffi Chern", "Siqi Kou", "Jiadi Su", "Yan Ma", "Zhijie Deng", "Pengfei Liu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images."}
{"id": "2505.22613", "pdf": "https://arxiv.org/pdf/2505.22613.pdf", "abs": "https://arxiv.org/abs/2505.22613", "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction", "authors": ["Yuchi Wang", "Yishuo Cai", "Shuhuai Ren", "Sihan Yang", "Linli Yao", "Yuanxin Liu", "Yuanxing Zhang", "Pengfei Wan", "Xu Sun"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "code: https://github.com/wangyuchi369/RICO", "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO."}
{"id": "2505.22617", "pdf": "https://arxiv.org/pdf/2505.22617.pdf", "abs": "https://arxiv.org/abs/2505.22617", "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "authors": ["Ganqu Cui", "Yuchen Zhang", "Jiacheng Chen", "Lifan Yuan", "Zhi Wang", "Yuxin Zuo", "Haozhan Li", "Yuchen Fan", "Huayu Chen", "Weize Chen", "Zhiyuan Liu", "Hao Peng", "Lei Bai", "Wanli Ouyang", "Yu Cheng", "Bowen Zhou", "Ning Ding"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance."}
{"id": "2505.22651", "pdf": "https://arxiv.org/pdf/2505.22651.pdf", "abs": "https://arxiv.org/abs/2505.22651", "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "authors": ["Yi Ding", "Ruqi Zhang"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\n$\\beta$ for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data."}
{"id": "2505.22655", "pdf": "https://arxiv.org/pdf/2505.22655.pdf", "abs": "https://arxiv.org/abs/2505.22655", "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive."}
{"id": "2505.22657", "pdf": "https://arxiv.org/pdf/2505.22657.pdf", "abs": "https://arxiv.org/abs/2505.22657", "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "authors": ["Wenbo Hu", "Yining Hong", "Yanjun Wang", "Leison Gao", "Zibu Wei", "Xingcheng Yao", "Nanyun Peng", "Yonatan Bitton", "Idan Szpektor", "Kai-Wei Chang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "demos at: https://3dllm-mem.github.io", "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks."}
{"id": "2401.06769", "pdf": "https://arxiv.org/pdf/2401.06769.pdf", "abs": "https://arxiv.org/abs/2401.06769", "title": "Machine Translation Models are Zero-Shot Detectors of Translation Direction", "authors": ["Michelle Wastl", "Jannis Vamvas", "Rico Sennrich"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Detecting the translation direction of parallel text has applications for\nmachine translation training and evaluation, but also has forensic applications\nsuch as resolving plagiarism or forgery allegations. In this work, we explore\nan unsupervised approach to translation direction detection based on the simple\nhypothesis that\n$p(\\text{translation}|\\text{original})>p(\\text{original}|\\text{translation})$,\nmotivated by the well-known simplification effect in translationese or\nmachine-translationese. In experiments with massively multilingual machine\ntranslation models across 20 translation directions, we confirm the\neffectiveness of the approach for high-resource language pairs, achieving\ndocument-level accuracies of 82--96% for NMT-produced translations, and 60--81%\nfor human translations, depending on the model used. Code and demo are\navailable at https://github.com/ZurichNLP/translation-direction-detection"}
{"id": "2402.16596", "pdf": "https://arxiv.org/pdf/2402.16596.pdf", "abs": "https://arxiv.org/abs/2402.16596", "title": "Tracking Semantic Change in Slovene: A Novel Dataset and Optimal Transport-Based Distance", "authors": ["Marko Pranjić", "Kaja Dobrovoljc", "Senja Pollak", "Matej Martinc"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "In this paper, we focus on the detection of semantic changes in Slovene, a\nless resourced Slavic language with two million speakers. Detecting and\ntracking semantic changes provides insight into the evolution of language\ncaused by changes in society and culture. We present the first Slovene dataset\nfor evaluating semantic change detection systems, which contains aggregated\nsemantic change scores for 104 target words obtained from more than 3,000\nmanually annotated sentence pairs. We analyze an important class of measures of\nsemantic change metrics based on the Average pairwise distance and identify\nseveral limitations. To address these limitations, we propose a novel metric\nbased on regularized optimal transport, which offers a more robust framework\nfor quantifying semantic change. We provide a comprehensive evaluation of\nvarious existing semantic change detection methods and associated semantic\nchange measures on our dataset. Through empirical testing, we demonstrate that\nour proposed approach, leveraging regularized optimal transport, achieves\neither matching or improved performance compared to baseline approaches."}
{"id": "2404.06762", "pdf": "https://arxiv.org/pdf/2404.06762.pdf", "abs": "https://arxiv.org/abs/2404.06762", "title": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Geyu Lin", "Nancy F. Chen"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced\nlearning experience. The emergence of large language models (LLMs) further\nenables better human-machine interaction, and facilitates the development of\nconversational ITSs in various disciplines such as math and language learning.\nIn dialogic teaching, recognizing and adapting to individual characteristics\ncan significantly enhance student engagement and learning efficiency. However,\ncharacterizing and simulating student's persona remain challenging in training\nand evaluating conversational ITSs. In this work, we propose a framework to\nconstruct profiles of different student groups by refining and integrating both\ncognitive and noncognitive aspects, and leverage LLMs for personality-aware\nstudent simulation in a language learning scenario. We further enhance the\nframework with multi-aspect validation, and conduct extensive analysis from\nboth teacher and student perspectives. Our experimental results show that\nstate-of-the-art LLMs can produce diverse student responses according to the\ngiven language ability and personality traits, and trigger teacher's adaptive\nscaffolding strategies."}
{"id": "2405.09948", "pdf": "https://arxiv.org/pdf/2405.09948.pdf", "abs": "https://arxiv.org/abs/2405.09948", "title": "Mitigating Text Toxicity with Counterfactual Generation", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nina Achache", "Victor Legrand", "Nicolas Chesneau", "Annabelle Blangero", "Juliette Murris", "Marie-Jeanne Lesot"], "categories": ["cs.CL"], "comment": null, "summary": "Toxicity mitigation consists in rephrasing text in order to remove offensive\nor harmful meaning. Neural natural language processing (NLP) models have been\nwidely used to target and mitigate textual toxicity. However, existing methods\nfail to detoxify text while preserving the initial non-toxic meaning at the\nsame time. In this work, we propose to apply counterfactual generation methods\nfrom the eXplainable AI (XAI) field to target and mitigate textual toxicity. In\nparticular, we perform text detoxification by applying local feature importance\nand counterfactual generation methods to a toxicity classifier distinguishing\nbetween toxic and non-toxic texts. We carry out text detoxification through\ncounterfactual generation on three datasets and compare our approach to three\ncompetitors. Automatic and human evaluations show that recently developed NLP\ncounterfactual generators can mitigate toxicity accurately while better\npreserving the meaning of the initial text as compared to classical\ndetoxification methods. Finally, we take a step back from using automated\ndetoxification tools, and discuss how to manage the polysemous nature of\ntoxicity and the risk of malicious use of detoxification tools. This work is\nthe first to bridge the gap between counterfactual generation and text\ndetoxification and paves the way towards more practical application of XAI\nmethods."}
{"id": "2406.09325", "pdf": "https://arxiv.org/pdf/2406.09325.pdf", "abs": "https://arxiv.org/abs/2406.09325", "title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space", "authors": ["Tomer Ashuach", "Martin Tutek", "Yonatan Belinkov"], "categories": ["cs.CL", "I.2.7"], "comment": "ACL 2025 Findings, 24 pages, 4 figures", "summary": "Language models (LMs) risk inadvertently memorizing and divulging sensitive\nor personally identifiable information (PII) seen in training data, causing\nprivacy concerns. Current approaches to address this issue involve costly\ndataset scrubbing, or model filtering through unlearning and model editing,\nwhich can be bypassed through extraction attacks. We propose REVS, a novel\nnon-gradient-based method for unlearning sensitive information from LMs. REVS\nidentifies and modifies a small subset of neurons relevant for constituent\ntokens that form sensitive information. To adequately evaluate our method on\ntruly sensitive information, we curate three datasets: email and URL datasets\nnaturally memorized by the models, and a synthetic social security number\ndataset that we tune the models to memorize. Compared to other methods, REVS\ndemonstrates superior performance in unlearning sensitive information and\nrobustness to extraction attacks, while retaining underlying model integrity."}
{"id": "2406.14023", "pdf": "https://arxiv.org/pdf/2406.14023.pdf", "abs": "https://arxiv.org/abs/2406.14023", "title": "Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective", "authors": ["Yuchen Wen", "Keping Bi", "Wei Chen", "Jiafeng Guo", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development. Our code, data and benchmarks are\navailable at https://github.com/yuchenwen1/ImplicitBiasPsychometricEvaluation\nand https://github.com/yuchenwen1/BUMBLE."}
{"id": "2406.14737", "pdf": "https://arxiv.org/pdf/2406.14737.pdf", "abs": "https://arxiv.org/abs/2406.14737", "title": "Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial Alterations to the False Belief Task?", "authors": ["Zhiqiang Pi", "Annapurna Vadaparty", "Benjamin K. Bergen", "Cameron R. Jones"], "categories": ["cs.CL"], "comment": null, "summary": "Recent empirical results have sparked a debate about whether or not Large\nLanguage Models (LLMs) are capable of Theory of Mind (ToM). While some have\nfound LLMs to be successful on ToM evaluations such as the False Belief task,\nothers have shown that their performance is not robust against trivial\nalterations to stimuli. In this paper, we introduce SCALPEL -- a technique to\nincrementally modify stimuli to test different specific hypotheses about why\nLLMs fail -- and apply this method to the \"transparent-access\" modification of\nthe unexpected contents task. Our results suggest that LLMs often do poorly\nbecause they fail to make essential common-sense inferences, such as that\nseeing a transparent container implies recognizing its contents. We conclude\nthat while modern LLMs go beyond mere pattern matching, they still fall short\nof robust human-like ToM. We argue that SCALPEL can help cognitive scientists\nexamine LLMs' capabilities in finer detail and provide insight into alternative\nmechanisms by which tasks that are used to assess human cognition might be\ncompleted."}
{"id": "2406.16508", "pdf": "https://arxiv.org/pdf/2406.16508.pdf", "abs": "https://arxiv.org/abs/2406.16508", "title": "Large Vocabulary Size Improves Large Language Models", "authors": ["Sho Takase", "Ryokan Ri", "Shun Kiyono", "Takuya Kato"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "This paper empirically investigates the relationship between subword\nvocabulary size and the performance of large language models (LLMs) to provide\ninsights on how to define the vocabulary size. Experimental results show that\nlarger vocabulary sizes lead to better performance in LLMs. Moreover, we\nconsider a continual training scenario where a pre-trained language model is\ntrained on a different target language. We introduce a simple method to use a\nnew vocabulary instead of the pre-defined one. We show that using the new\nvocabulary outperforms the model with the vocabulary used in pre-training."}
{"id": "2407.07004", "pdf": "https://arxiv.org/pdf/2407.07004.pdf", "abs": "https://arxiv.org/abs/2407.07004", "title": "Empirical analysis of binding precedent efficiency in Brazilian Supreme Court via case classification", "authors": ["Raphaël Tinarrage", "Henrique Ennes", "Lucas Resck", "Lucas T. Gomes", "Jean R. Ponciano", "Jorge Poco"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50 (Primary), 68T07 (Secondary)"], "comment": "Document similar to published version. Contains 62 pages and 21\n  figures", "summary": "Binding precedents (s\\'umulas vinculantes) constitute a juridical instrument\nunique to the Brazilian legal system and whose objectives include the\nprotection of the Federal Supreme Court against repetitive demands. Studies of\nthe effectiveness of these instruments in decreasing the Court's exposure to\nsimilar cases, however, indicate that they tend to fail in such a direction,\nwith some of the binding precedents seemingly creating new demands. We\nempirically assess the legal impact of five binding precedents, 11, 14, 17, 26,\nand 37, at the highest Court level through their effects on the legal subjects\nthey address. This analysis is only possible through the comparison of the\nCourt's ruling about the precedents' themes before they are created, which\nmeans that these decisions should be detected through techniques of Similar\nCase Retrieval, which we tackle from the angle of Case Classification. The\ncontributions of this article are therefore twofold: on the mathematical side,\nwe compare the use of different methods of Natural Language Processing --\nTF-IDF, LSTM, Longformer, and regex -- for Case Classification, whereas on the\nlegal side, we contrast the inefficiency of these binding precedents with a set\nof hypotheses that may justify their repeated usage. We observe that the TF-IDF\nmodels performed slightly better than LSTM and Longformer when compared through\ncommon metrics; however, the deep learning models were able to detect certain\nimportant legal events that TF-IDF missed. On the legal side, we argue that the\nreasons for binding precedents to fail in responding to repetitive demand are\nheterogeneous and case-dependent, making it impossible to single out a specific\ncause. We identify five main hypotheses, which are found in different\ncombinations in each of the precedents studied."}
{"id": "2409.04122", "pdf": "https://arxiv.org/pdf/2409.04122.pdf", "abs": "https://arxiv.org/abs/2409.04122", "title": "Prompt-based Personality Profiling: Reinforcement Learning for Relevance Filtering", "authors": ["Jan Hofmann", "Cornelia Sindermann", "Roman Klinger"], "categories": ["cs.CL"], "comment": "Accepted to the REALM workshop at ACL 2025", "summary": "Author profiling is the task of inferring characteristics about individuals\nby analyzing content they share. Supervised machine learning still dominates\nautomatic systems that perform this task, despite the popularity of prompting\nlarge language models to address natural language understanding tasks. One\nreason is that the classification instances consist of large amounts of posts,\npotentially a whole user profile, which may exceed the input length of\nTransformers. Even if a model can use a large context window, the entirety of\nposts makes the application of API-accessed black box systems costly and slow,\nnext to issues which come with such \"needle-in-the-haystack\" tasks. To mitigate\nthis limitation, we propose a new method for author profiling which aims at\ndistinguishing relevant from irrelevant content first, followed by the actual\nuser profiling only with relevant data. To circumvent the need for\nrelevance-annotated data, we optimize this relevance filter via reinforcement\nlearning with a reward function that utilizes the zero-shot capabilities of\nlarge language models. We evaluate our method for Big Five personality trait\nprediction on two Twitter corpora. On publicly available real-world data with a\nskewed label distribution, our method shows similar efficacy to using all posts\nin a user profile, but with a substantially shorter context. An evaluation on a\nversion of these data balanced with artificial posts shows that the filtering\nto relevant posts leads to a significantly improved accuracy of the\npredictions."}
{"id": "2410.08351", "pdf": "https://arxiv.org/pdf/2410.08351.pdf", "abs": "https://arxiv.org/abs/2410.08351", "title": "Nonlinear second-order dynamics describe labial constriction trajectories across languages and contexts", "authors": ["Michael C. Stern", "Jason A. Shaw"], "categories": ["cs.CL", "nlin.AO"], "comment": null, "summary": "We investigate the dynamics of labial constriction trajectories during the\nproduction of /b/ and /m/ in English and Mandarin. We find that, across\nlanguages and contexts, the ratio of instantaneous displacement to\ninstantaneous velocity generally follows an exponential decay curve from\nmovement onset to movement offset. We formalize this empirical discovery in a\ndifferential equation and, in combination with an assumption of point attractor\ndynamics, derive a nonlinear second-order dynamical system describing labial\nconstriction trajectories. The equation has only two parameters, T and r. T\ncorresponds to the target state and r corresponds to movement rapidity. Thus,\neach of the parameters corresponds to a phonetically relevant dimension of\ncontrol. Nonlinear regression demonstrates that the model provides excellent\nfits to individual movement trajectories. Moreover, trajectories simulated from\nthe model qualitatively match empirical trajectories, and capture key kinematic\nvariables like duration, peak velocity, and time to achieve peak velocity. The\nmodel constitutes a proposal for the dynamics of individual articulatory\nmovements, and thus offers a novel foundation from which to understand\nadditional influences on articulatory kinematics like prosody, inter-movement\ncoordination, and stochastic noise."}
{"id": "2410.08820", "pdf": "https://arxiv.org/pdf/2410.08820.pdf", "abs": "https://arxiv.org/abs/2410.08820", "title": "Which Demographics do LLMs Default to During Annotation?", "authors": ["Johannes Schäfer", "Aidan Combs", "Christopher Bagdon", "Jiahui Li", "Nadine Probol", "Lynn Greschner", "Sean Papay", "Yarik Menchaca Resendiz", "Aswathy Velutharambath", "Amelie Wührl", "Sabine Weber", "Roman Klinger"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects."}
{"id": "2410.13080", "pdf": "https://arxiv.org/pdf/2410.13080.pdf", "abs": "https://arxiv.org/abs/2410.13080", "title": "Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models", "authors": ["Linhao Luo", "Zicheng Zhao", "Gholamreza Haffari", "Yuan-Fang Li", "Chen Gong", "Shirui Pan"], "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "Large language models (LLMs) have demonstrated impressive reasoning\nabilities, but they still struggle with faithful reasoning due to knowledge\ngaps and hallucinations. To address these issues, knowledge graphs (KGs) have\nbeen utilized to enhance LLM reasoning through their structured knowledge.\nHowever, existing KG-enhanced methods, either retrieval-based or agent-based,\nencounter difficulties in accurately retrieving knowledge and efficiently\ntraversing KGs at scale. In this work, we introduce graph-constrained reasoning\n(GCR), a novel framework that bridges structured knowledge in KGs with\nunstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures\nfaithful KG-grounded reasoning by integrating KG structure into the LLM\ndecoding process through KG-Trie, a trie-based index that encodes KG reasoning\npaths. KG-Trie constrains the decoding process, allowing LLMs to directly\nreason on graphs and generate faithful reasoning paths grounded in KGs.\nAdditionally, GCR leverages a lightweight KG-specialized LLM for\ngraph-constrained reasoning alongside a powerful general LLM for inductive\nreasoning over multiple reasoning paths, resulting in accurate reasoning with\nzero reasoning hallucination. Extensive experiments on several KGQA benchmarks\ndemonstrate that GCR achieves state-of-the-art performance and exhibits strong\nzero-shot generalizability to unseen KGs without additional training."}
{"id": "2410.14641", "pdf": "https://arxiv.org/pdf/2410.14641.pdf", "abs": "https://arxiv.org/abs/2410.14641", "title": "Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs", "authors": ["Runchu Tian", "Yanghao Li", "Yuepeng Fu", "Siyang Deng", "Qinyu Luo", "Cheng Qian", "Shuo Wang", "Xin Cong", "Zhong Zhang", "Yesai Wu", "Yankai Lin", "Huadong Wang", "Xiaojiang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities."}
{"id": "2410.16665", "pdf": "https://arxiv.org/pdf/2410.16665.pdf", "abs": "https://arxiv.org/abs/2410.16665", "title": "SafetyAnalyst: Interpretable, Transparent, and Steerable Safety Moderation for AI Behavior", "authors": ["Jing-Jing Li", "Valentina Pyatkin", "Max Kleiman-Weiner", "Liwei Jiang", "Nouha Dziri", "Anne G. E. Collins", "Jana Schaich Borg", "Maarten Sap", "Yejin Choi", "Sydney Levine"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to ICML 2025", "summary": "The ideal AI safety moderation system would be both structurally\ninterpretable (so its decisions can be reliably explained) and steerable (to\nalign to safety standards and reflect a community's values), which current\nsystems fall short on. To address this gap, we present SafetyAnalyst, a novel\nAI safety moderation framework. Given an AI behavior, SafetyAnalyst uses\nchain-of-thought reasoning to analyze its potential consequences by creating a\nstructured \"harm-benefit tree,\" which enumerates harmful and beneficial actions\nand effects the AI behavior may lead to, along with likelihood, severity, and\nimmediacy labels that describe potential impacts on stakeholders. SafetyAnalyst\nthen aggregates all effects into a harmfulness score using 28 fully\ninterpretable weight parameters, which can be aligned to particular safety\npreferences. We applied this framework to develop an open-source LLM prompt\nsafety classification system, distilled from 18.5 million harm-benefit features\ngenerated by frontier LLMs on 19k prompts. On comprehensive benchmarks, we show\nthat SafetyAnalyst (average F1=0.81) outperforms existing moderation systems\n(average F1$<$0.72) on prompt safety classification, while offering the\nadditional advantages of interpretability, transparency, and steerability."}
{"id": "2410.18436", "pdf": "https://arxiv.org/pdf/2410.18436.pdf", "abs": "https://arxiv.org/abs/2410.18436", "title": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching", "authors": ["Seoyeon Kim", "Huiseo Kim", "Chanjun Park", "Jinyoung Yeo", "Dongha Lee"], "categories": ["cs.CL"], "comment": "25 pages, 8 figures", "summary": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can 'activate', or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks."}
{"id": "2410.22316", "pdf": "https://arxiv.org/pdf/2410.22316.pdf", "abs": "https://arxiv.org/abs/2410.22316", "title": "Understanding Synthetic Context Extension via Retrieval Heads", "authors": ["Xinyu Zhao", "Fangcong Yin", "Greg Durrett"], "categories": ["cs.CL"], "comment": "Published at ICML 2025", "summary": "Long-context LLMs are increasingly in demand for applications such as\nretrieval-augmented generation. To defray the cost of pretraining LLMs over\nlong contexts, recent work takes an approach of synthetic context extension:\nfine-tuning LLMs with synthetically generated long-context data in a\npost-training stage. However, it remains unclear how and why this synthetic\ncontext extension imparts abilities for downstream long-context tasks. In this\npaper, we investigate fine-tuning on synthetic data for three long-context\ntasks that require retrieval and reasoning. We vary the realism of \"needle\"\nconcepts to be retrieved and diversity of the surrounding \"haystack\" context,\nfrom using LLMs to construct synthetic documents to using templated relations\nand creating symbolic datasets. We find that models trained on synthetic data\nfall short of the real data, but surprisingly, the mismatch can be interpreted\nand even predicted in terms of a special set of attention heads that are\nresponsible for retrieval over long context, retrieval heads (Wu et al., 2024).\nThe retrieval heads learned on synthetic data have high overlap with retrieval\nheads learned on real data, and there is a strong correlation between the\nrecall of heads learned and the downstream performance of a model. Furthermore,\nwith attention knockout and activation patching, we mechanistically show that\nretrieval heads are necessary and explain model performance, although they are\nnot totally sufficient. Our results shed light on how to interpret synthetic\ndata fine-tuning performance and how to approach creating better data for\nlearning real-world capabilities over long contexts."}
{"id": "2411.07404", "pdf": "https://arxiv.org/pdf/2411.07404.pdf", "abs": "https://arxiv.org/abs/2411.07404", "title": "Controllable Context Sensitivity and the Knob Behind It", "authors": ["Julian Minder", "Kevin Du", "Niklas Stoehr", "Giovanni Monea", "Chris Wendler", "Robert West", "Ryan Cotterell"], "categories": ["cs.CL", "cs.AI"], "comment": "Published as a conference paper at ICLR 2025", "summary": "When making predictions, a language model must trade off how much it relies\non its context vs. its prior knowledge. Choosing how sensitive the model is to\nits context is a fundamental functionality, as it enables the model to excel at\ntasks like retrieval-augmented generation and question-answering. In this\npaper, we search for a knob which controls this sensitivity, determining\nwhether language models answer from the context or their prior knowledge. To\nguide this search, we design a task for controllable context sensitivity. In\nthis task, we first feed the model a context (Paris is in England) and a\nquestion (Where is Paris?); we then instruct the model to either use its prior\nor contextual knowledge and evaluate whether it generates the correct answer\nfor both intents (either France or England). When fine-tuned on this task,\ninstruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it\nwith high accuracy (85-95%). Analyzing these high-performing models, we narrow\ndown which layers may be important to context sensitivity using a novel linear\ntime algorithm. Then, in each model, we identify a 1-D subspace in a single\nlayer that encodes whether the model follows context or prior knowledge.\nInterestingly, while we identify this subspace in a fine-tuned model, we find\nthat the exact same subspace serves as an effective knob in not only that model\nbut also non-fine-tuned instruct and base models of that model family. Finally,\nwe show a strong correlation between a model's performance and how distinctly\nit separates context-agreeing from context-ignoring answers in this subspace.\nThese results suggest a single subspace facilitates how the model chooses\nbetween context and prior knowledge, hinting at a simple fundamental mechanism\nthat controls this behavior."}
{"id": "2411.11171", "pdf": "https://arxiv.org/pdf/2411.11171.pdf", "abs": "https://arxiv.org/abs/2411.11171", "title": "LLäMmlein: Compact and Competitive German-Only Language Models from Scratch", "authors": ["Jan Pfister", "Julia Wunderle", "Andreas Hotho"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "camera ready @ACL25;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/", "summary": "We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development."}
{"id": "2411.17170", "pdf": "https://arxiv.org/pdf/2411.17170.pdf", "abs": "https://arxiv.org/abs/2411.17170", "title": "Overcoming Non-monotonicity in Transducer-based Streaming Generation", "authors": ["Zhengrui Ma", "Yang Feng", "Min Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML25; Codes: https://github.com/ictnlp/MonoAttn-Transducer", "summary": "Streaming generation models are utilized across fields, with the Transducer\narchitecture being popular in industrial applications. However, its\ninput-synchronous decoding mechanism presents challenges in tasks requiring\nnon-monotonic alignments, such as simultaneous translation. In this research,\nwe address this issue by integrating Transducer's decoding with the history of\ninput stream via a learnable monotonic attention. Our approach leverages the\nforward-backward algorithm to infer the posterior probability of alignments\nbetween the predictor states and input timestamps, which is then used to\nestimate the monotonic context representations, thereby avoiding the need to\nenumerate the exponentially large alignment space during training. Extensive\nexperiments show that our MonoAttn-Transducer effectively handles non-monotonic\nalignments in streaming scenarios, offering a robust solution for complex\ngeneration tasks."}
{"id": "2412.11418", "pdf": "https://arxiv.org/pdf/2412.11418.pdf", "abs": "https://arxiv.org/abs/2412.11418", "title": "ConKE: Conceptualization-Augmented Knowledge Editing in Large Language Models for Commonsense Reasoning", "authors": ["Liyu Zhang", "Weiqi Wang", "Tianqing Fang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Findings of ACL2025", "summary": "Knowledge Editing (KE) aims to adjust a Large Language Model's (LLM) internal\nrepresentations and parameters to correct inaccuracies and improve output\nconsistency without incurring the computational expense of re-training the\nentire model. However, editing commonsense knowledge still faces difficulties,\nincluding limited knowledge coverage in existing resources, the infeasibility\nof annotating labels for an overabundance of commonsense knowledge, and the\nstrict knowledge formats of current editing methods. In this paper, we address\nthese challenges by presenting ConceptEdit, a framework that integrates\nconceptualization and instantiation into the KE pipeline for LLMs to enhance\ntheir commonsense reasoning capabilities. ConceptEdit dynamically diagnoses\nimplausible commonsense knowledge within an LLM using another verifier LLM and\naugments the source knowledge to be edited with conceptualization for stronger\ngeneralizability. Experimental results demonstrate that LLMs enhanced with\nConceptEdit successfully generate commonsense knowledge with improved\nplausibility compared to other baselines and achieve stronger performance\nacross multiple question answering benchmarks. Our data, code, and models are\npublicly available at https://github.com/HKUST-KnowComp/ConKE."}
{"id": "2412.12465", "pdf": "https://arxiv.org/pdf/2412.12465.pdf", "abs": "https://arxiv.org/abs/2412.12465", "title": "Core Context Aware Transformers for Long Context Language Modeling", "authors": ["Yaofo Chen", "Zeng You", "Shuhai Zhang", "Haokun Li", "Yirui Li", "Yaowei Wang", "Mingkui Tan"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted for publication at ICML 2025", "summary": "Transformer-based Large Language Models (LLMs) have exhibited remarkable\nsuccess in extensive tasks primarily attributed to self-attention mechanism,\nwhich requires a token to consider all preceding tokens as its context to\ncompute attention. However, when the context length L becomes very large (e.g.,\n128K), the amount of potentially redundant information in the context tends to\nincrease. The redundant context not only hampers the modeling representation\nperformance but also incurs unnecessary computational and storage overhead. In\nthis paper, we propose a plug-and-play Core Context Aware (CCA) Attention for\nefficient long-context modeling, comprising two complementary modules: 1)\nGlobality-aware pooling module groups input tokens and dynamically compresses\neach group into one core token based on their significance. In this way, our\nmethod automatically focuses and strengthens core context while diminishing\nredundancy during the learning process, leading to effective long-term\ndependency modeling. 2) Locality-preserving module incorporates neighboring\ntokens to preserve local context for detailed representation. Notably, our\nCCA-Attention is able to replace the self-attention module in existing LLMs\nwith minimal fine-tuning cost. Extensive experimental results show the\nsuperiority of our method in both long-context modeling and computational\nefficiency over state-of-the-art methods."}
{"id": "2412.14689", "pdf": "https://arxiv.org/pdf/2412.14689.pdf", "abs": "https://arxiv.org/abs/2412.14689", "title": "How to Synthesize Text Data without Model Collapse?", "authors": ["Xuekai Zhu", "Daixuan Cheng", "Hengli Li", "Kaiyan Zhang", "Ermo Hua", "Xingtai Lv", "Ning Ding", "Zhouhan Lin", "Zilong Zheng", "Bowen Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Model collapse in synthetic data indicates that iterative training on\nself-generated data leads to a gradual decline in performance. With the\nproliferation of AI models, synthetic data will fundamentally reshape the web\ndata ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend\nof synthetic and human-produced data. In this paper, we focus on two questions:\nwhat is the impact of synthetic data on language model training, and how to\nsynthesize data without model collapse? We first pre-train language models\nacross different proportions of synthetic data, revealing a negative\ncorrelation between the proportion of synthetic data and model performance. We\nfurther conduct statistical analysis on synthetic data to uncover\ndistributional shift phenomenon and over-concentration of n-gram features.\nInspired by the above findings, we propose token editing on human-produced data\nto obtain semi-synthetic data. As a proof of concept, we theoretically\ndemonstrate that token-level editing can prevent model collapse, as the test\nerror is constrained by a finite upper bound. We conduct extensive experiments\non pre-training from scratch, continual pre-training, and supervised\nfine-tuning. The results validate our theoretical proof that token-level\nediting improves model performance."}
{"id": "2412.16926", "pdf": "https://arxiv.org/pdf/2412.16926.pdf", "abs": "https://arxiv.org/abs/2412.16926", "title": "Revisiting In-Context Learning with Long Context Language Models", "authors": ["Jinheon Baek", "Sun Jae Lee", "Prakhar Gupta", "Geunseob Oh", "Siddharth Dalmia", "Prateek Kolhar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Findings 2025", "summary": "In-Context Learning (ICL) is a technique by which language models make\npredictions based on examples provided in their input context. Previously,\ntheir context window size imposed a limit on the number of examples that can be\nshown, making example selection techniques crucial for identifying the\nmaximally effective set of examples. However, the recent advent of Long Context\nLanguage Models (LCLMs) has significantly increased the number of examples that\ncan be included in context, raising an important question of whether ICL\nperformance in a many-shot regime is still sensitive to the method of sample\nselection. To answer this, we revisit these approaches in the context of LCLMs\nthrough extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we\nobserve that sophisticated example selection techniques do not yield\nsignificant improvements over a simple random sample selection method. Instead,\nwe discover that the advent of LCLMs has fundamentally shifted the challenge of\nICL from that of selecting the most effective examples to that of collecting\nsufficient examples to fill the context window. Specifically, in certain\ndatasets, including all available examples does not fully utilize the context\nwindow; however, by augmenting the examples in context with a simple data\naugmentation approach, we substantially improve ICL performance by 5%."}
{"id": "2501.00777", "pdf": "https://arxiv.org/pdf/2501.00777.pdf", "abs": "https://arxiv.org/abs/2501.00777", "title": "FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation", "authors": ["Qianli Wang", "Nils Feldhus", "Simon Ostermann", "Luis Felipe Villa-Arenas", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL", "cs.LG"], "comment": "ACL 2025 Findings; camera-ready version", "summary": "Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals, which we hope will serve as an\nimportant finding for future research in this direction."}
{"id": "2501.03124", "pdf": "https://arxiv.org/pdf/2501.03124.pdf", "abs": "https://arxiv.org/abs/2501.03124", "title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models", "authors": ["Mingyang Song", "Zhaochen Su", "Xiaoye Qu", "Jiawei Zhou", "Yu Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ACL 2025 Main. Project Page: https://prmbench.github.io/", "summary": "Process-level Reward Models (PRMs) are crucial for complex reasoning and\ndecision-making tasks, where each intermediate step plays an important role in\nthe reasoning process. Since language models are prone to various types of\nerrors during the reasoning process, PRMs are required to possess nuanced\ncapabilities for detecting various implicit error types in real-world\nscenarios. However, current benchmarks primarily focus on step correctness,\nfailing to evaluate PRMs' performance systematically. To address this gap, we\nintroduce PRMBench, a process-level benchmark specifically designed to assess\nthe fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216\ncarefully designed problems and 83,456 step-level labels, evaluating models\nacross multiple dimensions, including simplicity, soundness, and sensitivity.\nIn our experiments on 15 models, spanning both open-source PRMs and\nclosed-source large language models prompted as critic models, we uncover\nsignificant weaknesses in current PRMs. These findings underscore the\nchallenges inherent in process-level evaluation and highlight key directions\nfor future research. We hope PRMBench can be a robust bench for advancing\nresearch on PRM evaluation and development."}
{"id": "2501.05926", "pdf": "https://arxiv.org/pdf/2501.05926.pdf", "abs": "https://arxiv.org/abs/2501.05926", "title": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities", "authors": ["Ruby Ostrow", "Adam Lopez"], "categories": ["cs.CL"], "comment": "8 pages, 5 figures, 5 tables", "summary": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used social psychology model -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom both humans and LLMs. We then extend this framework to a more realistic\nuse case: text generation. Our analysis shows that LLMs generate stereotyped\nrepresentations of sexual and gender minorities in this setting, showing that\nthey amplify representational harms in creative writing, a widely advertised\nuse for LLMs."}
{"id": "2501.06365", "pdf": "https://arxiv.org/pdf/2501.06365.pdf", "abs": "https://arxiv.org/abs/2501.06365", "title": "Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts", "authors": ["Elizabeth Schaefer", "Kirk Roberts"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "9 pages, 4 figures", "summary": "This paper presents a pipeline for mitigating gender bias in large language\nmodels (LLMs) used in medical literature by neutralizing gendered occupational\npronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to\nidentify and modify pronouns tied to professions. We developed a BERT-based\nmodel, \"Modern Occupational Bias Elimination with Refined Training,\" or\n\"MOBERT,\" trained on these neutralized abstracts, and compared its performance\nwith \"1965BERT,\" trained on the original dataset. MOBERT achieved a 70%\ninclusive replacement rate, while 1965BERT reached only 4%. A further analysis\nof MOBERT revealed that pronoun replacement accuracy correlated with the\nfrequency of occupational terms in the training data. We propose expanding the\ndataset and refining the pipeline to improve performance and ensure more\nequitable language modeling in medical applications."}
{"id": "2501.13567", "pdf": "https://arxiv.org/pdf/2501.13567.pdf", "abs": "https://arxiv.org/abs/2501.13567", "title": "K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor", "authors": ["Jeonghun Cho", "Gary Geunbae Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at NAACL 2025 (Main, long paper)", "summary": "Retrieval-augmented question answering (QA) integrates external information\nand thereby increases the QA accuracy of reader models that lack domain\nknowledge. However, documents retrieved for closed domains require high\nexpertise, so the reader model may have difficulty fully comprehending the\ntext. Moreover, the retrieved documents contain thousands of tokens, some\nunrelated to the question. As a result, the documents include some inaccurate\ninformation, which could lead the reader model to mistrust the passages and\ncould result in hallucinations. To solve these problems, we propose K-comp\n(Knowledge-injected compressor) which provides the knowledge required to answer\ncorrectly. The compressor automatically generates the prior knowledge necessary\nto facilitate the answer process prior to compression of the retrieved\npassages. Subsequently, the passages are compressed autoregressively, with the\ngenerated knowledge being integrated into the compression process. This process\nensures alignment between the question intent and the compressed context. By\naugmenting this prior knowledge and concise context, the reader models are\nguided toward relevant answers and trust the context."}
{"id": "2501.13953", "pdf": "https://arxiv.org/pdf/2501.13953.pdf", "abs": "https://arxiv.org/abs/2501.13953", "title": "Redundancy Principles for MLLMs Benchmarks", "authors": ["Zicheng Zhang", "Xiangyu Zhao", "Xinyu Fang", "Chunyi Li", "Xiaohong Liu", "Xiongkuo Min", "Haodong Duan", "Kai Chen", "Guangtao Zhai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid iteration of Multi-modality Large Language Models (MLLMs) and\nthe evolving demands of the field, the number of benchmarks produced annually\nhas surged into the hundreds. The rapid growth has inevitably led to\nsignificant redundancy among benchmarks. Therefore, it is crucial to take a\nstep back and critically assess the current state of redundancy and propose\ntargeted principles for constructing effective MLLM benchmarks. In this paper,\nwe focus on redundancy from three key perspectives: 1) Redundancy of benchmark\ncapability dimensions, 2) Redundancy in the number of test questions, and 3)\nCross-benchmark redundancy within specific domains. Through the comprehensive\nanalysis over hundreds of MLLMs' performance across more than 20 benchmarks, we\naim to quantitatively measure the level of redundancy lies in existing MLLM\nevaluations, provide valuable insights to guide the future development of MLLM\nbenchmarks, and offer strategies to refine and address redundancy issues\neffectively. The code is available at\nhttps://github.com/zzc-1998/Benchmark-Redundancy."}
{"id": "2501.14431", "pdf": "https://arxiv.org/pdf/2501.14431.pdf", "abs": "https://arxiv.org/abs/2501.14431", "title": "Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains", "authors": ["Xu Chu", "Zhijie Tan", "Hanlin Xue", "Guanyu Wang", "Tong Mo", "Weiping Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are widely applied to downstream domains.\nHowever, current LLMs for high-stakes domain tasks, such as financial\ninvestment and legal QA, typically generate brief answers without reasoning\nprocesses and explanations. This limits users' confidence in making decisions\nbased on their responses. While original CoT shows promise, it lacks\nself-correction mechanisms during reasoning. This work introduces Domain$o1$s,\nwhich enhances LLMs' reasoning capabilities on domain tasks through supervised\nfine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k\ndatasets for fine-tuning models that activate domain-specific reasoning steps\nbased on their judgment. Additionally, we propose Selective Tree Exploration to\nspontaneously explore solution spaces and sample optimal reasoning paths to\nimprove performance. We also introduce PROOF-Score, a new metric for evaluating\ndomain models' explainability, complementing traditional accuracy metrics with\nricher assessment dimensions. Extensive experiments on stock investment\nrecommendation and legal reasoning QA tasks demonstrate Domaino1s's leading\nperformance and explainability. Our code is available at\nhttps://github.com/Hyalinesky/Domaino1s."}
{"id": "2502.00136", "pdf": "https://arxiv.org/pdf/2502.00136.pdf", "abs": "https://arxiv.org/abs/2502.00136", "title": "A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment", "authors": ["Edward Y. Chang"], "categories": ["cs.CL", "cs.AI", "F.2.2"], "comment": "20 pages, 7 tables, 6 figures. arXiv admin note: substantial text\n  overlap with arXiv:2405.07076", "summary": "This paper introduces a checks-and-balances framework for ethical alignment\nof Large Language Models (LLMs), inspired by three-branch governmental systems.\nIt implements three independent yet interacting components: LLMs as the\nexecutive branch for knowledge generation, DIKE as the legislative branch\nestablishing ethical guardrails, and ERIS as the judicial branch for contextual\ninterpretation. Beyond structural separation, we address a fundamental\nchallenge: regulating emotion to shape behaviors. Drawing from psychological\ntheories where managing emotional responses prevents harmful behaviors, we\ndevelop a self-supervised learning pipeline that maps emotions to linguistic\nbehaviors, enabling precise behavioral modulation through emotional\nconditioning. By integrating this approach with adversarial testing, our\nframework demonstrates how DIKE and ERIS direct linguistic behaviors toward\nethical outcomes while preserving independence throughout knowledge generation,\nethical oversight, and contextual interpretation."}
{"id": "2502.00602", "pdf": "https://arxiv.org/pdf/2502.00602.pdf", "abs": "https://arxiv.org/abs/2502.00602", "title": "Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing", "authors": ["Tianci Liu", "Ruirui Li", "Zihan Dong", "Hui Liu", "Xianfeng Tang", "Qingyu Yin", "Linjun Zhang", "Haoyu Wang", "Jing Gao"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Large language models (LLMs) have achieved remarkable performance on various\nnatural language tasks. However, they are trained on static corpora and their\nknowledge can become outdated quickly in the fast-changing world. This\nmotivates the development of knowledge editing (KE) to update specific\nknowledge in LLMs without changing unrelated others or compromising their\npre-trained capabilities. Previous efforts sought to update a small amount of\nparameters of a LLM and proved effective for making selective updates.\nNonetheless, the edited LLM often exhibits degraded ability to reason about the\nnew knowledge. In this work, we identify a key issue: heterogeneous token\noverfitting (HTO), where the LLM overfits different tokens in the provided\nknowledge at varying rates. To tackle this, we propose OVERTONE, a token-level\nsmoothing method that mitigates HTO by adaptively refining the target\ndistribution. Theoretically, OVERTONE offers better parameter updates with\nnegligible computation overhead. It also induces an implicit DPO but does not\nrequire preference data pairs. Extensive experiments across four editing\nmethods, two LLMs, and diverse scenarios demonstrate the effectiveness and\nversatility of our method."}
{"id": "2502.03671", "pdf": "https://arxiv.org/pdf/2502.03671.pdf", "abs": "https://arxiv.org/abs/2502.03671", "title": "Advancing Reasoning in Large Language Models: Promising Methods and Approaches", "authors": ["Avinash Patil", "Aryan Jadon"], "categories": ["cs.CL", "cs.AI"], "comment": "9 Pages, 1 Figure, IEEE Format", "summary": "Large Language Models (LLMs) have succeeded remarkably in various natural\nlanguage processing (NLP) tasks, yet their reasoning capabilities remain a\nfundamental challenge. While LLMs exhibit impressive fluency and factual\nrecall, their ability to perform complex reasoning-spanning logical deduction,\nmathematical problem-solving, commonsense inference, and multi-step\nreasoning-often falls short of human expectations. This survey provides a\ncomprehensive review of emerging techniques enhancing reasoning in LLMs. We\ncategorize existing methods into key approaches, including prompting strategies\n(e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought\nreasoning), architectural innovations (e.g., retrieval-augmented models,\nmodular reasoning networks, and neuro-symbolic integration), and learning\nparadigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement\nlearning, and self-supervised reasoning objectives). Additionally, we explore\nevaluation frameworks used to assess reasoning in LLMs and highlight open\nchallenges, such as hallucinations, robustness, and reasoning generalization\nacross diverse tasks. By synthesizing recent advancements, this survey aims to\nprovide insights into promising directions for future research and practical\napplications of reasoning-augmented LLMs."}
{"id": "2502.05242", "pdf": "https://arxiv.org/pdf/2502.05242.pdf", "abs": "https://arxiv.org/abs/2502.05242", "title": "Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring", "authors": ["Guanxu Chen", "Dongrui Liu", "Tao Luo", "Lijie Hu", "Jing Shao"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "25 pages,6 figures,13 tables", "summary": "Large language models (LLMs) are becoming increasingly capable, but the\nmechanisms of their thinking and decision-making process remain unclear.\nChain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this\nstrategy fails to accurately reflect LLMs' thinking process. Techniques based\non LLMs' hidden representations provide an inner perspective to monitor their\nlatent thinking. However, previous methods only try to develop external\nmonitors instead of making LLMs themselves easier to monitor. In this paper, we\npropose a novel method TELLME, improving the transparency of LLMs and helping\nmonitors identify unsuitable and sensitive behaviors. Furthermore, we showcase\nthe applications of TELLME on trustworthiness tasks (\\eg, safety risks\nmonitoring tasks and detoxification tasks), where LLMs achieve consistent\nimprovement in transparency and task performance. More crucially, we\ntheoretically analyze the improvement of TELLME on LLMs' generalization ability\nthrough optimal transport theory."}
{"id": "2502.07365", "pdf": "https://arxiv.org/pdf/2502.07365.pdf", "abs": "https://arxiv.org/abs/2502.07365", "title": "LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation", "authors": ["Zican Dong", "Junyi Li", "Jinhao Jiang", "Mingyu Xu", "Wayne Xin Zhao", "Bingning Wang", "Weipeng Chen"], "categories": ["cs.CL", "cs.LG"], "comment": "ACL2025 Main", "summary": "Large language models (LLMs) have gained extended context windows through\nscaling positional encodings and lightweight continual pre-training. However,\nthis often leads to degraded performance on short-text tasks, while the reasons\nfor this degradation remain insufficiently explored. In this work, we identify\ntwo primary factors contributing to this issue: distribution drift in hidden\nstates and attention scores, and catastrophic forgetting during continual\npre-training. To address these challenges, we propose Long Context Pre-training\nwith Restoration Distillation (LongReD), a novel approach designed to mitigate\nshort-text performance degradation through minimizing the distribution\ndiscrepancy between the extended and original models. Besides training on long\ntexts, LongReD distills the hidden state of selected layers from the original\nmodel on short texts. Additionally, LongReD also introduces a short-to-long\ndistillation, aligning the output distribution on short texts with that on long\ntexts by leveraging skipped positional indices. Experiments on common text\nbenchmarks demonstrate that LongReD effectively preserves the model's\nshort-text performance while maintaining comparable or even better capacity to\nhandle long texts than baselines. Our code is available at\nhttps://github.com/RUCAIBox/LongReD."}
{"id": "2502.09082", "pdf": "https://arxiv.org/pdf/2502.09082.pdf", "abs": "https://arxiv.org/abs/2502.09082", "title": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles", "authors": ["Xintao Wang", "Heng Wang", "Yifei Zhang", "Xinfeng Yuan", "Rui Xu", "Jen-tse Huang", "Siyu Yuan", "Haoran Guo", "Jiangjie Chen", "Shuchang Zhou", "Wei Wang", "Yanghua Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Role-playing language agents (RPLAs) have emerged as promising applications\nof large language models (LLMs). However, simulating established characters\npresents a challenging task for RPLAs, due to the lack of authentic character\ndatasets and nuanced evaluation methods using such data. In this paper, we\npresent CoSER, a collection of a high-quality dataset, open models, and an\nevaluation protocol towards effective RPLAs of established characters. The\nCoSER dataset covers 17,966 characters from 771 renowned books. It provides\nauthentic dialogues with real-world intricacies, as well as diverse data types\nsuch as conversation setups, character experiences and internal thoughts.\nDrawing from acting methodology, we introduce given-circumstance acting for\ntraining and evaluating role-playing LLMs, where LLMs sequentially portray\nmultiple characters in book scenes. Using our dataset, we develop CoSER 8B and\nCoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.\nExtensive experiments demonstrate the value of the CoSER dataset for RPLA\ntraining, evaluation and retrieval. Moreover, CoSER 70B exhibits\nstate-of-the-art performance surpassing or matching GPT-4o on our evaluation\nand three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on\nthe InCharacter and LifeChoice benchmarks respectively."}
{"id": "2502.11100", "pdf": "https://arxiv.org/pdf/2502.11100.pdf", "abs": "https://arxiv.org/abs/2502.11100", "title": "Towards Achieving Concept Completeness for Textual Concept Bottleneck Models", "authors": ["Milan Bhan", "Yann Choho", "Pierre Moreau", "Jean-Noel Vittaut", "Nicolas Chesneau", "Marie-Jeanne Lesot"], "categories": ["cs.CL"], "comment": null, "summary": "Textual Concept Bottleneck Models (TCBMs) are interpretable-by-design models\nfor text classification that predict a set of salient concepts before making\nthe final prediction. This paper proposes Complete Textual Concept Bottleneck\nModel (CT-CBM), a novel TCBM generator building concept labels in a fully\nunsupervised manner using a small language model, eliminating both the need for\npredefined human labeled concepts and LLM annotations. CT-CBM iteratively\ntargets and adds important and identifiable concepts in the bottleneck layer to\ncreate a complete concept basis. CT-CBM achieves striking results against\ncompetitors in terms of concept basis completeness and concept detection\naccuracy, offering a promising solution to reliably enhance interpretability of\nNLP classifiers."}
{"id": "2502.11190", "pdf": "https://arxiv.org/pdf/2502.11190.pdf", "abs": "https://arxiv.org/abs/2502.11190", "title": "ReLearn: Unlearning via Learning for Large Language Models", "authors": ["Haoming Xu", "Ningyuan Zhao", "Liming Yang", "Sendong Zhao", "Shumin Deng", "Mengru Wang", "Bryan Hooi", "Nay Oo", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "ACL 2025", "summary": "Current unlearning methods for large language models usually rely on reverse\noptimization to reduce target token probabilities. However, this paradigm\ndisrupts the subsequent tokens prediction, degrading model performance and\nlinguistic coherence. Moreover, existing evaluation metrics overemphasize\ncontextual forgetting while inadequately assessing response fluency and\nrelevance. To address these challenges, we propose ReLearn, a data augmentation\nand fine-tuning pipeline for effective unlearning, along with a comprehensive\nevaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)\nand Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and\nLinguistic Score (LS) to evaluate generation quality. Our experiments show that\nReLearn successfully achieves targeted forgetting while preserving high-quality\noutput. Through mechanistic analysis, we further demonstrate how reverse\noptimization disrupts coherent text generation, while ReLearn preserves this\nessential capability. Code is available at https://github.com/zjunlp/unlearn."}
{"id": "2502.11441", "pdf": "https://arxiv.org/pdf/2502.11441.pdf", "abs": "https://arxiv.org/abs/2502.11441", "title": "Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning", "authors": ["Hwan Chang", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) risk retaining unauthorized or sensitive\ninformation from their training data, which raises privacy concerns. LLM\nunlearning seeks to mitigate these risks by selectively removing specified data\nwhile maintaining overall model performance. However, most existing work focus\non methods to achieve effective forgetting and does not provide a detailed\nanalysis of the retain set, the portion of training data that is not targeted\nfor removal. In this paper, we investigate the effects of unlearning on various\nsubsets of the retain set through a case study on entity unlearning. We\nintroduce the Syntactically Similar Neighbor Set, a group of queries that share\nsimilar syntactic structures with the data targeted for removal, and show that\nthis subset suffers the greatest performance drop during unlearning. Moreover,\nwhen used for regularization, this set not only preserves performance on\nsyntactically similar queries but also delivers comparable or improved results\nacross other data subsets. Our results highlight that syntactic similarity is a\ncritical factor, potentially more so than domain or entity relationships, in\nachieving effective and practical LLM unlearning."}
{"id": "2502.11926", "pdf": "https://arxiv.org/pdf/2502.11926.pdf", "abs": "https://arxiv.org/abs/2502.11926", "title": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages", "authors": ["Shamsuddeen Hassan Muhammad", "Nedjma Ousidhoum", "Idris Abdulmumin", "Jan Philip Wahle", "Terry Ruas", "Meriem Beloucif", "Christine de Kock", "Nirmal Surange", "Daniela Teodorescu", "Ibrahim Said Ahmad", "David Ifeoluwa Adelani", "Alham Fikri Aji", "Felermino D. M. A. Ali", "Ilseyar Alimova", "Vladimir Araujo", "Nikolay Babakov", "Naomi Baes", "Ana-Maria Bucur", "Andiswa Bukula", "Guanqun Cao", "Rodrigo Tufino Cardenas", "Rendi Chevi", "Chiamaka Ijeoma Chukwuneke", "Alexandra Ciobotaru", "Daryna Dementieva", "Murja Sani Gadanya", "Robert Geislinger", "Bela Gipp", "Oumaima Hourrane", "Oana Ignat", "Falalu Ibrahim Lawan", "Rooweither Mabuya", "Rahmad Mahendra", "Vukosi Marivate", "Alexander Panchenko", "Andrew Piper", "Charles Henrique Porto Ferreira", "Vitaly Protasov", "Samuel Rutunda", "Manish Shrivastava", "Aura Cristina Udrea", "Lilian Diana Awuor Wanzare", "Sophie Wu", "Florian Valentin Wunderlich", "Hanif Muhammad Zhafran", "Tianhui Zhang", "Yi Zhou", "Saif M. Mohammad"], "categories": ["cs.CL"], "comment": "Accepted at ACL2025 (Main)", "summary": "People worldwide use language in subtle and complex ways to express emotions.\nAlthough emotion recognition--an umbrella term for several NLP tasks--impacts\nvarious applications within NLP and beyond, most work in this area has focused\non high-resource languages. This has led to significant disparities in research\nefforts and proposed solutions, particularly for under-resourced languages,\nwhich often lack high-quality annotated datasets. In this paper, we present\nBRIGHTER--a collection of multilabeled, emotion-annotated datasets in 28\ndifferent languages and across several domains. BRIGHTER primarily covers\nlow-resource languages from Africa, Asia, Eastern Europe, and Latin America,\nwith instances labeled by fluent speakers. We highlight the challenges related\nto the data collection and annotation processes, and then report experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as emotion intensity recognition. We analyse the variability in\nperformance across languages and text domains, both with and without the use of\nLLMs, and show that the BRIGHTER datasets represent a meaningful step towards\naddressing the gap in text-based emotion recognition."}
{"id": "2502.13458", "pdf": "https://arxiv.org/pdf/2502.13458.pdf", "abs": "https://arxiv.org/abs/2502.13458", "title": "ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails", "authors": ["Xiaofei Wen", "Wenxuan Zhou", "Wenjie Jacky Mo", "Muhao Chen"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "ACL 2025", "summary": "Ensuring the safety of large language models (LLMs) is critical as they are\ndeployed in real-world applications. Existing guardrails rely on rule-based\nfiltering or single-pass classification, limiting their ability to handle\nnuanced safety violations. To address this, we propose ThinkGuard, a\ncritique-augmented guardrail model that distills knowledge from high-capacity\nLLMs by generating structured critiques alongside safety labels. Fine-tuned on\ncritique-augmented data, the captured deliberative thinking ability drastically\nenhances the guardrail's cautiousness and interpretability. Evaluated on\nmultiple safety benchmarks, ThinkGuard achieves the highest average F1 and\nAUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard\nimproves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses\nlabel-only fine-tuned models, confirming that structured critiques enhance both\nclassification precision and nuanced safety reasoning while maintaining\ncomputational efficiency."}
{"id": "2502.13913", "pdf": "https://arxiv.org/pdf/2502.13913.pdf", "abs": "https://arxiv.org/abs/2502.13913", "title": "How Do LLMs Perform Two-Hop Reasoning in Context?", "authors": ["Tianyu Guo", "Hanlin Zhu", "Ruiqi Zhang", "Jiantao Jiao", "Song Mei", "Michael I. Jordan", "Stuart Russell"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "``Socrates is human. All humans are mortal. Therefore, Socrates is mortal.''\nThis form of argument illustrates a typical pattern of two-hop reasoning.\nFormally, two-hop reasoning refers to the process of inferring a conclusion by\nmaking two logical steps, each connecting adjacent concepts, such that the\nfinal conclusion depends on the integration of both steps. It is one of the\nmost fundamental components of human reasoning and plays a crucial role in both\nformal logic and everyday decision-making. Despite recent progress in large\nlanguage models (LLMs), we surprisingly find that they can fail at solving\nsimple two-hop reasoning problems when distractors are present. We observe on a\nsynthetic dataset that pre-trained LLMs often resort to random guessing among\nall plausible conclusions. However, after few steps of fine-tuning, models\nachieve near-perfect accuracy and exhibit strong length generalization. To\nunderstand the underlying mechanisms, we train a 3-layer Transformer from\nscratch on a synthetic two-hop reasoning task and reverse-engineer its internal\ninformation flow. We observe a clear progression in the attention logits\nthroughout training. This pictures a sharp phase transition from an initial\nstage of random guessing to the emergence of a structured sequential query\nmechanism, where the model first retrieves the preceding and the bridge\nconcepts in the early layers and then uses them to infer the final answer.\nFinally, we show that these dynamics can be captured by a minimal\nthree-parameter attention-only network."}
{"id": "2502.14245", "pdf": "https://arxiv.org/pdf/2502.14245.pdf", "abs": "https://arxiv.org/abs/2502.14245", "title": "Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering", "authors": ["Rongzhi Zhu", "Xiangyu Liu", "Zequn Sun", "Yiwei Wang", "Wei Hu"], "categories": ["cs.CL"], "comment": "Accepted in the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "In this paper, we identify a critical problem, \"lost-in-retrieval\", in\nretrieval-augmented multi-hop question answering (QA): the key entities are\nmissed in LLMs' sub-question decomposition. \"Lost-in-retrieval\" significantly\ndegrades the retrieval performance, which disrupts the reasoning chain and\nleads to the incorrect answers. To resolve this problem, we propose a\nprogressive retrieval and rewriting method, namely ChainRAG, which sequentially\nhandles each sub-question by completing missing key entities and retrieving\nrelevant sentences from a sentence graph for answer generation. Each step in\nour retrieval and rewriting process builds upon the previous one, creating a\nseamless chain that leads to accurate retrieval and answers. Finally, all\nretrieved sentences and sub-question answers are integrated to generate a\ncomprehensive answer to the original question. We evaluate ChainRAG on three\nmulti-hop QA datasets - MuSiQue, 2Wiki, and HotpotQA - using three large\nlanguage models: GPT4o-mini, Qwen2.5-72B, and GLM-4-Plus. Empirical results\ndemonstrate that ChainRAG consistently outperforms baselines in both\neffectiveness and efficiency."}
{"id": "2502.15920", "pdf": "https://arxiv.org/pdf/2502.15920.pdf", "abs": "https://arxiv.org/abs/2502.15920", "title": "Self-Taught Agentic Long Context Understanding", "authors": ["Yufan Zhuang", "Xiaodong Yu", "Jialian Wu", "Ximeng Sun", "Ze Wang", "Jiang Liu", "Yusheng Su", "Jingbo Shang", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.CL", "cs.AI"], "comment": "Published at ACL 2025 Main Conference", "summary": "Answering complex, long-context questions remains a major challenge for large\nlanguage models (LLMs) as it requires effective question clarifications and\ncontext retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a\nframework designed to enhance an LLM's understanding of such queries by\nintegrating targeted self-clarification with contextual grounding within an\nagentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC),\nwhere models refine their understanding through self-generated clarification\nquestions and corresponding contextual groundings. By scaling inference as a\ntree search where each node represents a CoC step, we achieve 97.8% answer\nrecall on NarrativeQA with a search depth of up to three and a branching factor\nof eight. To amortize the high cost of this search process to training, we\nleverage the preference pairs for each step obtained by the CoC workflow and\nperform two-stage model finetuning: (1) supervised finetuning to learn\neffective decomposition strategies, and (2) direct preference optimization to\nenhance reasoning quality. This enables AgenticLU models to generate\nclarifications and retrieve relevant context effectively and efficiently in a\nsingle inference pass. Extensive experiments across seven long-context tasks\ndemonstrate that AgenticLU significantly outperforms state-of-the-art prompting\nmethods and specialized long-context LLMs, achieving robust multi-hop reasoning\nwhile sustaining consistent performance as context length grows."}
{"id": "2502.16514", "pdf": "https://arxiv.org/pdf/2502.16514.pdf", "abs": "https://arxiv.org/abs/2502.16514", "title": "GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking", "authors": ["Yingjian Chen", "Haoran Liu", "Yinhong Liu", "Jinxiang Xie", "Rui Yang", "Han Yuan", "Yanran Fu", "Peng Yuan Zhou", "Qingyu Chen", "James Caverlee", "Irene Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely used, but they often generate subtle\nfactual errors, especially in long-form text. These errors are fatal in some\nspecialized domains such as medicine. Existing fact-checking with grounding\ndocuments methods face two main challenges: (1) they struggle to understand\ncomplex multihop relations in long documents, often overlooking subtle factual\nerrors; (2) most specialized methods rely on pairwise comparisons, requiring\nmultiple model calls, leading to high resource and computational costs. To\naddress these challenges, we propose GraphCheck, a fact-checking framework that\nuses extracted knowledge graphs to enhance text representation. Graph Neural\nNetworks further process these graphs as a soft prompt, enabling LLMs to\nincorporate structured knowledge more effectively. Enhanced with graph-based\nreasoning, GraphCheck captures multihop reasoning chains that are often\noverlooked by existing methods, enabling precise and efficient fact-checking in\na single inference call. Experimental results on seven benchmarks spanning both\ngeneral and medical domains demonstrate up to a 7.1% overall improvement over\nbaseline models. Notably, GraphCheck outperforms existing specialized\nfact-checkers and achieves comparable performance with state-of-the-art LLMs,\nsuch as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters."}
{"id": "2503.00134", "pdf": "https://arxiv.org/pdf/2503.00134.pdf", "abs": "https://arxiv.org/abs/2503.00134", "title": "Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary Recommendations", "authors": ["Zhongqi Yang", "Amir Rahmani"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) effectively leverage common-sense knowledge for\ngeneral reasoning, yet they struggle with personalized reasoning when tasked\nwith interpreting multifactor personal data. This limitation restricts their\napplicability in domains that require context-aware decision-making tailored to\nindividuals. This paper introduces Personalized Causal Graph Reasoning as an\nagentic framework that enhances LLM reasoning by incorporating personal causal\ngraphs derived from data of individuals. These graphs provide a foundation that\nguides the LLM's reasoning process. We evaluate it on a case study on\nnutrient-oriented dietary recommendations, which requires personal reasoning\ndue to the implicit unique dietary effects. We propose a counterfactual\nevaluation to estimate the efficiency of LLM-recommended foods for glucose\nmanagement. Results demonstrate that the proposed method efficiently provides\npersonalized dietary recommendations to reduce average glucose iAUC across\nthree time windows, which outperforms the previous approach. LLM-as-a-judge\nevaluation results indicate that our proposed method enhances personalization\nin the reasoning process."}
{"id": "2503.02972", "pdf": "https://arxiv.org/pdf/2503.02972.pdf", "abs": "https://arxiv.org/abs/2503.02972", "title": "LINGOLY-TOO: Disentangling Reasoning from Knowledge with Templatised Orthographic Obfuscation", "authors": ["Jude Khouja", "Karolina Korgul", "Simi Hellsten", "Lingyi Yang", "Vlad Neacsu", "Harry Mayne", "Ryan Kearns", "Andrew Bean", "Adam Mahdi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The expanding knowledge and memorisation capacity of frontier language models\nallows them to solve many reasoning tasks directly by exploiting prior\nknowledge, leading to inflated estimates of their reasoning abilities. We\nintroduce LINGOLY-TOO, a challenging reasoning benchmark grounded in natural\nlanguage and designed to counteract the effect of non-reasoning abilities on\nreasoning estimates. Using linguistically informed rulesets, we permute\nreasoning problems written in real languages to generate numerous question\nvariations. These permutations preserve the intrinsic reasoning steps required\nfor each solution while reducing the likelihood problems are directly solvable\nwith models' knowledge. Experiments and analyses show that models can\ncircumvent reasoning and answer from prior knowledge. On a metric that rewards\nconsistent reasoning, all models perform poorly and exhibit high variance\nacross question permutations, indicating that Large Language Models' (LLMs)\nreasoning faculty remains brittle. Overall, results on the benchmark reflect\nthe recent progress of Inference-Time Compute (ITC) models but suggest ample\nroom for further improvement. The benchmark is a step towards better\nmeasurement of reasoning abilities of LLMs and offers a cautionary tale on the\nimportance of disentangling reasoning abilities from models' internalised\nknowledge when developing reasoning benchmarks."}
{"id": "2503.04395", "pdf": "https://arxiv.org/pdf/2503.04395.pdf", "abs": "https://arxiv.org/abs/2503.04395", "title": "Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication", "authors": ["Tom Kouwenhoven", "Max Peeperkorn", "Roy de Kleijn", "Tessa Verhoef"], "categories": ["cs.CL"], "comment": "Presented at IJCAI 2025 (Human-centred AI Track)", "summary": "Languages are shaped by the inductive biases of their users. Using a\nclassical referential game, we investigate how artificial languages evolve when\noptimised for inductive biases in humans and large language models (LLMs) via\nHuman-Human, LLM-LLM and Human-LLM experiments. We show that referentially\ngrounded vocabularies emerge that enable reliable communication in all\nconditions, even when humans \\textit{and} LLMs collaborate. Comparisons between\nconditions reveal that languages optimised for LLMs subtly differ from those\noptimised for humans. Interestingly, interactions between humans and LLMs\nalleviate these differences and result in vocabularies more human-like than\nLLM-like. These findings advance our understanding of the role inductive biases\nin LLMs play in the dynamic nature of human language and contribute to\nmaintaining alignment in human and machine communication. In particular, our\nwork underscores the need to think of new LLM training methods that include\nhuman interaction and shows that using communicative success as a reward signal\ncan be a fruitful, novel direction."}
{"id": "2503.08057", "pdf": "https://arxiv.org/pdf/2503.08057.pdf", "abs": "https://arxiv.org/abs/2503.08057", "title": "Odysseus Navigates the Sirens' Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation", "authors": ["Wen Luo", "Feifan Song", "Wei Li", "Guangyue Peng", "Shaohang Wei", "Houfeng Wang"], "categories": ["cs.CL"], "comment": "Accepted to the ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) are increasingly required to generate text that\nis both factually accurate and diverse across various open-ended applications.\nHowever, current stochastic decoding methods struggle to balance such\nobjectives. We introduce Dynamic Focus Decoding (DFD), a novel plug-and-play\nstochastic approach that resolves this trade-off without requiring additional\ndata, knowledge, or models. DFD adaptively adjusts the decoding focus based on\ndistributional differences across layers, leveraging the modular and\nhierarchical nature of factual knowledge within LLMs. This dynamic adjustment\nimproves factuality in knowledge-intensive decoding steps and promotes\ndiversity in less knowledge-reliant steps. DFD can be easily integrated with\nexisting decoding methods, enhancing both factuality and diversity with minimal\ncomputational overhead. Extensive experiments across seven datasets demonstrate\nthat DFD significantly improves performance, providing a scalable and efficient\nsolution for open-ended text generation."}
{"id": "2503.09454", "pdf": "https://arxiv.org/pdf/2503.09454.pdf", "abs": "https://arxiv.org/abs/2503.09454", "title": "Explicit Learning and the LLM in Machine Translation", "authors": ["Malik Marmonier", "Rachel Bawden", "Benoît Sagot"], "categories": ["cs.CL"], "comment": null, "summary": "This study explores an LLM's ability to learn new languages using\nexplanations found in a grammar book$\\unicode{x2014}$a process we term\n\"explicit learning.\" To rigorously assess this ability, we design controlled\ntranslation experiments between English and constructed languages\ngenerated$\\unicode{x2014}$by specific cryptographic means$\\unicode{x2014}$out\nof Latin or French. Contrary to previous studies, our results demonstrate that\nLLMs do possess a measurable capacity for explicit learning. This ability,\nhowever, diminishes as the complexity of the linguistic phenomena to be learned\nincreases. Supervised fine-tuning on ad hoc chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs, benefiting low-resource languages typically\ndescribed in grammar books but lacking extensive corpora."}
{"id": "2503.09674", "pdf": "https://arxiv.org/pdf/2503.09674.pdf", "abs": "https://arxiv.org/abs/2503.09674", "title": "Probabilistic Reasoning with LLMs for k-anonymity Estimation", "authors": ["Jonathan Zheng", "Sauvik Das", "Alan Ritter", "Wei Xu"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, preprint", "summary": "Probabilistic reasoning is a key aspect of both human and artificial\nintelligence that allows for handling uncertainty and ambiguity in\ndecision-making. In this paper, we introduce a new numerical reasoning task\nunder uncertainty for large language models, focusing on estimating the privacy\nrisk of user-generated documents containing privacy-sensitive information. We\npropose BRANCH, a new LLM methodology that estimates the k-privacy value of a\ntext-the size of the population matching the given information. BRANCH\nfactorizes a joint probability distribution of personal information as random\nvariables. The probability of each factor in a population is estimated\nseparately using a Bayesian network and combined to compute the final k-value.\nOur experiments show that this method successfully estimates the k-value 73% of\nthe time, a 13% increase compared to o3-mini with chain-of-thought reasoning.\nWe also find that LLM uncertainty is a good indicator for accuracy, as\nhigh-variance predictions are 37.47% less accurate on average."}
{"id": "2503.09790", "pdf": "https://arxiv.org/pdf/2503.09790.pdf", "abs": "https://arxiv.org/abs/2503.09790", "title": "Constrained Discrete Diffusion", "authors": ["Michael Cardei", "Jacob K Christopher", "Thomas Hartvigsen", "Brian R. Bartoldson", "Bhavya Kailkhura", "Ferdinando Fioretto"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Discrete diffusion models are a class of generative models that construct\nsequences by progressively denoising samples from a categorical noise\ndistribution. Beyond their rapidly growing ability to generate coherent natural\nlanguage, these models present a new and important opportunity to enforce\nsequence-level constraints, a capability that current autoregressive models\ncannot natively provide. This paper capitalizes on this opportunity by\nintroducing Constrained Discrete Diffusion (CDD), a novel integration of\ndifferentiable constraint optimization within the diffusion process to ensure\nadherence to constraints, logic rules, or safety requirements for generated\nsequences. Unlike conventional text generators that often rely on post-hoc\nfiltering or model retraining for controllable generation, CDD directly imposes\nconstraints into the discrete diffusion sampling process, resulting in a\ntraining-free and effective approach. Experiments in toxicity-controlled text\ngeneration, property-constrained molecule design, and instruction-constrained\ntext completion demonstrate that CDD achieves zero constraint violations in a\ndiverse array of tasks while preserving fluency, novelty, and coherence while\noutperforming autoregressive and existing discrete diffusion approaches."}
{"id": "2503.10460", "pdf": "https://arxiv.org/pdf/2503.10460.pdf", "abs": "https://arxiv.org/abs/2503.10460", "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond", "authors": ["Liang Wen", "Yunke Cai", "Fenrui Xiao", "Xin He", "Qi An", "Zhenyu Duan", "Yimin Du", "Junchen Liu", "Lifu Tang", "Xiaowei Lv", "Haosheng Zou", "Yongchao Deng", "Shousheng Jia", "Xiangzheng Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "v4: ACL'25 industry track camera ready; v3: minor modifications; v2:\n  better writing & format for later submission; all release at\n  https://github.com/Qihoo360/Light-R1", "summary": "This paper introduces Light-R1, an open-source suite for training long\nreasoning models using reproducible and cost-effective methodology. Given the\nproprietary nature of data used in the DeepSeek-R1 series, we develop an\nalternative approach leveraging exclusively public data and models. Our\ncurriculum training progressively increases data difficulty, combined with\nmulti-staged post-training. Our Light-R1-32B model, trained from\nQwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math\nreasoning.\n  Experimental results show that this curriculum approach becomes more\neffective when distinct, diverse datasets are available for different training\nstages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on\nproprietary data) with 3,000 challenging examples from our curriculum dataset\nyielded state-of-the-art 7B and 14B models, while the 32B model,\nLight-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying GRPO on long reasoning models.\nOur final Light-R1-14B-DS achieves SOTA performance among 14B models in math,\nwith AIME24 & 25 scores of 74.0 and 60.2 respectively, surpassing many 32B\nmodels and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training,\nLight-R1-14B-DS demonstrates strong cross-domain generalization.\n  Light-R1 represents a significant advancement in making sophisticated\nreasoning models more accessible and implementable in real-world applications.\nOur models, training data and code have been made available at\nhttps://github.com/Qihoo360/Light-R1."}
{"id": "2503.10688", "pdf": "https://arxiv.org/pdf/2503.10688.pdf", "abs": "https://arxiv.org/abs/2503.10688", "title": "CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding", "authors": ["Tadesse Destaw Belay", "Ahmed Haj Ahmed", "Alvin Grissom II", "Iqra Ameer", "Grigori Sidorov", "Olga Kolesnikova", "Seid Muhie Yimam"], "categories": ["cs.CL"], "comment": "ACL-main 2025", "summary": "NLP research has increasingly focused on subjective tasks such as emotion\nanalysis. However, existing emotion benchmarks suffer from two major\nshortcomings: (1) they largely rely on keyword-based emotion recognition,\noverlooking crucial cultural dimensions required for deeper emotion\nunderstanding, and (2) many are created by translating English-annotated data\ninto other languages, leading to potentially unreliable evaluation. To address\nthese issues, we introduce Cultural Lenses on Emotion (CuLEmo), the first\nbenchmark designed to evaluate culture-aware emotion prediction across six\nlanguages: Amharic, Arabic, English, German, Hindi, and Spanish. CuLEmo\ncomprises 400 crafted questions per language, each requiring nuanced cultural\nreasoning and understanding. We use this benchmark to evaluate several\nstate-of-the-art LLMs on culture-aware emotion prediction and sentiment\nanalysis tasks. Our findings reveal that (1) emotion conceptualizations vary\nsignificantly across languages and cultures, (2) LLMs performance likewise\nvaries by language and cultural context, and (3) prompting in English with\nexplicit country context often outperforms in-language prompts for\nculture-aware emotion and sentiment understanding. The dataset and evaluation\ncode are publicly available."}
{"id": "2503.12051", "pdf": "https://arxiv.org/pdf/2503.12051.pdf", "abs": "https://arxiv.org/abs/2503.12051", "title": "TLUE: A Tibetan Language Understanding Evaluation Benchmark", "authors": ["Fan Gao", "Cheng Huang", "Nyima Tashi", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Hao Wang Xiao Feng", "Yongbin Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have made tremendous progress in recent years,\nbut low-resource languages, such as Tibetan, remain significantly\nunderrepresented in their evaluation. Despite Tibetan being spoken by over\nseven million people, it has largely been neglected in the development and\nassessment of LLMs. To address this gap, we present TLUE (A Tibetan Language\nUnderstanding Evaluation Benchmark), the first large-scale benchmark for\nassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:\n(1) a comprehensive multi-task understanding benchmark spanning 5 domains and\n67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a\ndiverse set of state-of-the-art LLMs. Experimental results demonstrate that\nmost LLMs perform below the random baseline, highlighting the considerable\nchallenges LLMs face in processing Tibetan, a low-resource language. TLUE\nprovides an essential foundation for driving future research and progress in\nTibetan language understanding and underscores the need for greater inclusivity\nin LLM development."}
{"id": "2503.17933", "pdf": "https://arxiv.org/pdf/2503.17933.pdf", "abs": "https://arxiv.org/abs/2503.17933", "title": "Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA", "authors": ["Justice Ou", "Tinglin Huang", "Yilun Zhao", "Ziyang Yu", "Peiqing Lu", "Rex Ying"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "To improve the reliability of Large Language Models (LLMs) in clinical\napplications, retrieval-augmented generation (RAG) is extensively applied to\nprovide factual medical knowledge. However, beyond general medical knowledge\nfrom open-ended datasets, clinical case-based knowledge is also critical for\neffective medical reasoning, as it provides context grounded in real-world\npatient experiences.Motivated by this, we propose Experience\nRetrieval-Augmentation ExpRAG framework based on Electronic Health Record(EHR),\naiming to offer the relevant context from other patients' discharge reports.\nExpRAG performs retrieval through a coarse-to-fine process, utilizing an\nEHR-based report ranker to efficiently identify similar patients, followed by\nan experience retriever to extract task-relevant content for enhanced medical\nreasoning.To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset\nwith 1,280 discharge-related questions across diagnosis, medication, and\ninstruction tasks. Each problem is generated using EHR data to ensure realistic\nand challenging scenarios. Experimental results demonstrate that ExpRAG\nconsistently outperforms a text-based ranker, achieving an average relative\nimprovement of 5.2%, highlighting the importance of case-based knowledge for\nmedical reasoning."}
{"id": "2503.18288", "pdf": "https://arxiv.org/pdf/2503.18288.pdf", "abs": "https://arxiv.org/abs/2503.18288", "title": "Sun-Shine: A Foundation Large Language Model for Tibetan Culture and Heritage", "authors": ["Cheng Huang", "Fan Gao", "Yutong Liu", "Nyima Tashi", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Hao Wang", "Yongbin Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities."}
{"id": "2504.01002", "pdf": "https://arxiv.org/pdf/2504.01002.pdf", "abs": "https://arxiv.org/abs/2504.01002", "title": "Token embeddings violate the manifold hypothesis", "authors": ["Michael Robinson", "Sourya Dey", "Tony Chiang"], "categories": ["cs.CL", "cs.AI", "53Z50, 62H15"], "comment": "27 pages, 6 figures, 9 tables", "summary": "A full understanding of the behavior of a large language model (LLM) requires\nour understanding of its input token space. If this space differs from our\nassumptions, our understanding of and conclusions about the LLM will likely be\nflawed. We elucidate the structure of the token embeddings both empirically and\ntheoretically. We present a novel statistical test assuming that the\nneighborhood around each token has a relatively flat and smooth structure as\nthe null hypothesis. Failing to reject the null is uninformative, but rejecting\nit at a specific token $\\psi$ implies an irregularity in the token subspace in\na $\\psi$-neighborhood, $B(\\psi)$. The structure assumed in the null is a\ngeneralization of a manifold with boundary called a \\emph{smooth fiber bundle}\n(which can be split into two spatial regimes -- small and large radius), so we\ndenote our new hypothesis test as the ``fiber bundle hypothesis.'' Failure to\nreject the null hypothesis is uninformative, but rejecting it at $\\psi$\nindicates a statistically significant irregularity at $B(\\psi)$. By running our\ntest over several open-source LLMs, each with unique token embeddings, we find\nthat the null is frequently rejected, and so the evidence suggests that the\ntoken subspace is not a fiber bundle and hence also not a manifold. As a\nconsequence of our findings, when an LLM is presented with two semantically\nequivalent prompts, if one prompt contains a token implicated by our test, the\nresponse to that prompt will likely exhibit less stability than the other."}
{"id": "2504.03312", "pdf": "https://arxiv.org/pdf/2504.03312.pdf", "abs": "https://arxiv.org/abs/2504.03312", "title": "Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices", "authors": ["Luís Couto Seller", "Íñigo Sanz Torres", "Adrián Vogel-Fernández", "Carlos González Carballo", "Pedro Miguel Sánchez Sánchez", "Adrián Carruana Martín", "Enrique de Miguel Ambite"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at SEPLN 2025 conference", "summary": "Large Language Models have significantly advanced natural language\nprocessing, achieving remarkable performance in tasks such as language\ngeneration, translation, and reasoning. However, their substantial\ncomputational requirements restrict deployment to high-end systems, limiting\naccessibility on consumer-grade devices. This challenge is especially\npronounced for under-resourced languages like those spoken in the Iberian\nPeninsula, where relatively limited linguistic resources and benchmarks hinder\neffective evaluation. This work presents a comprehensive evaluation of compact\nstate-of-the-art LLMs across several essential NLP tasks tailored for Iberian\nlanguages. The results reveal that while some models consistently excel in\ncertain tasks, significant performance gaps remain, particularly for languages\nsuch as Basque. These findings highlight the need for further research on\nbalancing model compactness with robust multilingual performance"}
{"id": "2504.03561", "pdf": "https://arxiv.org/pdf/2504.03561.pdf", "abs": "https://arxiv.org/abs/2504.03561", "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement", "authors": ["Runnan Fang", "Xiaobin Wang", "Yuan Liang", "Shuofei Qiao", "Jialong Wu", "Zekun Xi", "Ningyu Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "ACL 2025 Findings", "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld."}
{"id": "2504.05104", "pdf": "https://arxiv.org/pdf/2504.05104.pdf", "abs": "https://arxiv.org/abs/2504.05104", "title": "AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments", "authors": ["Saeid Ario Vaghefi", "Aymane Hachcham", "Veronica Grasso", "Jiska Manicus", "Nakiete Msemo", "Chiara Colesanti Senni", "Markus Leippold"], "categories": ["cs.CL"], "comment": null, "summary": "Tracking financial investments in climate adaptation is a complex and\nexpertise-intensive task, particularly for Early Warning Systems (EWS), which\nlack standardized financial reporting across multilateral development banks\n(MDBs) and funds. To address this challenge, we introduce an LLM-based agentic\nAI system that integrates contextual retrieval, fine-tuning, and multi-step\nreasoning to extract relevant financial data, classify investments, and ensure\ncompliance with funding guidelines. Our study focuses on a real-world\napplication: tracking EWS investments in the Climate Risk and Early Warning\nSystems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple\nAI-driven classification methods, including zero-shot and few-shot learning,\nfine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and\nan agent-based retrieval-augmented generation (RAG) approach. Our results show\nthat the agent-based RAG approach significantly outperforms other methods,\nachieving 87\\% accuracy, 89\\% precision, and 83\\% recall. Additionally, we\ncontribute a benchmark dataset and expert-annotated corpus, providing a\nvaluable resource for future research in AI-driven financial tracking and\nclimate finance transparency."}
{"id": "2504.06792", "pdf": "https://arxiv.org/pdf/2504.06792.pdf", "abs": "https://arxiv.org/abs/2504.06792", "title": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations", "authors": ["Zican Dong", "Han Peng", "Peiyu Liu", "Wayne Xin Zhao", "Dong Wu", "Feng Xiao", "Zhifeng Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Mixture-of-Experts (MoE) models achieve a favorable trade-off between\nperformance and inference efficiency by activating only a subset of experts.\nHowever, the memory overhead of storing all experts remains a major limitation,\nespecially in large-scale MoE models such as DeepSeek-R1(671B). In this study,\nwe investigate domain specialization and expert redundancy in large-scale MoE\nmodels and uncover a consistent behavior we term few-shot expert localization,\nwith only a few in-domain demonstrations, the model consistently activates a\nsparse and stable subset of experts on tasks within the same domain. Building\non this observation, we propose a simple yet effective pruning framework,\nEASY-EP, that leverages a few domain-specific demonstrations to identify and\nretain only the most relevant experts. EASY-EP comprises two key components:\noutput-aware expert importance assessment and expert-level token contribution\nestimation. The former evaluates the importance of each expert for the current\ntoken by considering the gating scores and L2 norm of the outputs of activated\nexperts, while the latter assesses the contribution of tokens based on\nrepresentation similarities before and after routed experts. Experiments on\nDeepSeek-R1 and DeepSeek-V3-0324 show that our method can achieve comparable\nperformances and $2.99\\times$ throughput under the same memory budget with full\nmodel with only half the experts."}
{"id": "2504.08775", "pdf": "https://arxiv.org/pdf/2504.08775.pdf", "abs": "https://arxiv.org/abs/2504.08775", "title": "Layers at Similar Depths Generate Similar Activations Across LLM Architectures", "authors": ["Christopher Wolfram", "Aaron Schein"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "How do the latent spaces used by independently-trained LLMs relate to one\nanother? We study the nearest neighbor relationships induced by activations at\ndifferent layers of 24 open-weight LLMs, and find that they 1) tend to vary\nfrom layer to layer within a model, and 2) are approximately shared between\ncorresponding layers of different models. Claim 2 shows that these nearest\nneighbor relationships are not arbitrary, as they are shared across models, but\nClaim 1 shows that they are not \"obvious\" either, as there is no single set of\nnearest neighbor relationships that is universally shared. Together, these\nsuggest that LLMs generate a progression of activation geometries from layer to\nlayer, but that this entire progression is largely shared between models,\nstretched and squeezed to fit into different architectures."}
{"id": "2504.12339", "pdf": "https://arxiv.org/pdf/2504.12339.pdf", "abs": "https://arxiv.org/abs/2504.12339", "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM", "authors": ["Yaodong Song", "Hongjie Chen", "Jie Lian", "Yuxin Zhang", "Guangmin Xia", "Zehan Li", "Genliang Zhao", "Jian Kang", "Jie Li", "Yongxiang Li", "Xuelong Li"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS)\nsynthesis through discrete tokenization paradigms, current architectures\nexhibit fundamental tensions between three critical dimensions: 1) irreversible\nloss of acoustic characteristics caused by quantization of speech prompts; 2)\nstringent dependence on precisely aligned prompt speech-text pairs that limit\nreal-world deployment; and 3) catastrophic forgetting of the LLM's native text\ncomprehension during optimization for speech token generation. To address these\nchallenges, we propose an LLM-based text-to-speech Generation approach\nOptimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework\nintroduces two key innovations: (1) The modality-alignment branch combines a\nspeech encoder and projector to capture continuous acoustic embeddings,\nenabling bidirectional correlation between paralinguistic features (language,\ntimbre, emotion) and semantic text representations without transcript\ndependency; (2) The speech-generation branch employs modular fine-tuning on\ntop-k layers of an LLM for speech token prediction while freezing the bottom-n\nlayers to preserve foundational linguistic knowledge. Moreover, multi-token\nprediction is introduced to support real-time streaming TTS synthesis.\nExperimental results demonstrate that our GOAT-TTS achieves performance\ncomparable to state-of-the-art TTS models while validating the efficacy of\nsynthesized dialect speech data."}
{"id": "2504.20581", "pdf": "https://arxiv.org/pdf/2504.20581.pdf", "abs": "https://arxiv.org/abs/2504.20581", "title": "ClonEval: An Open Voice Cloning Benchmark", "authors": ["Iwona Christop", "Tomasz Kuczyński", "Marek Kubis"], "categories": ["cs.CL"], "comment": "Under review at NeurIPS", "summary": "We present a novel benchmark for voice cloning text-to-speech models. The\nbenchmark consists of an evaluation protocol, an open-source library for\nassessing the performance of voice cloning models, and an accompanying\nleaderboard. The paper discusses design considerations and presents a detailed\ndescription of the evaluation procedure. The usage of the software library is\nexplained, along with the organization of results on the leaderboard."}
{"id": "2505.10832", "pdf": "https://arxiv.org/pdf/2505.10832.pdf", "abs": "https://arxiv.org/abs/2505.10832", "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL", "authors": ["Songjun Tu", "Jiahao Lin", "Qichao Zhang", "Xiangyu Tian", "Linjing Li", "Xiangyuan Lan", "Dongbin Zhao"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Fisrt Submitted on 16 May 2025; Update on 28 May 2025", "summary": "Large reasoning models (LRMs) are proficient at generating explicit,\nstep-by-step reasoning sequences before producing final answers. However, such\ndetailed reasoning can introduce substantial computational overhead and\nlatency, particularly for simple problems. To address this over-thinking\nproblem, we explore how to equip LRMs with adaptive thinking capabilities:\nenabling them to dynamically decide whether or not to engage in explicit\nreasoning based on problem complexity. Building on R1-style distilled models,\nwe observe that inserting a simple ellipsis (\"...\") into the prompt can\nstochastically trigger either a thinking or no-thinking mode, revealing a\nlatent controllability in the reasoning behavior. Leveraging this property, we\npropose AutoThink, a multi-stage reinforcement learning (RL) framework that\nprogressively optimizes reasoning policies via stage-wise reward shaping.\nAutoThink learns to invoke explicit reasoning only when necessary, while\ndefaulting to succinct responses for simpler tasks. Experiments on five\nmainstream mathematical benchmarks demonstrate that AutoThink achieves\nfavorable accuracy-efficiency trade-offs compared to recent prompting and\nRL-based pruning methods. It can be seamlessly integrated into any R1-style\nmodel, including both distilled and further fine-tuned variants. Notably,\nAutoThink improves relative accuracy by 6.4 percent while reducing token usage\nby 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and\nadaptive reasoning paradigm for LRMs. Project Page:\nhttps://github.com/ScienceOne-AI/AutoThink."}
{"id": "2505.11277", "pdf": "https://arxiv.org/pdf/2505.11277.pdf", "abs": "https://arxiv.org/abs/2505.11277", "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs", "authors": ["Yaorui Shi", "Sihang Li", "Chang Wu", "Zhiyuan Liu", "Junfeng Fang", "Hengxing Cai", "An Zhang", "Xiang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively."}
{"id": "2505.13077", "pdf": "https://arxiv.org/pdf/2505.13077.pdf", "abs": "https://arxiv.org/abs/2505.13077", "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL."}
{"id": "2505.13141", "pdf": "https://arxiv.org/pdf/2505.13141.pdf", "abs": "https://arxiv.org/abs/2505.13141", "title": "Language-Specific Latent Process Hinders Cross-Lingual Performance", "authors": ["Zheng Wei Lim", "Alham Fikri Aji", "Trevor Cohn"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English."}
{"id": "2505.13171", "pdf": "https://arxiv.org/pdf/2505.13171.pdf", "abs": "https://arxiv.org/abs/2505.13171", "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks", "authors": ["Yixuan Xu", "Antoni-Joan Solergibert i Llaquet", "Antoine Bosselut", "Imanol Schlag"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences."}
{"id": "2505.14471", "pdf": "https://arxiv.org/pdf/2505.14471.pdf", "abs": "https://arxiv.org/abs/2505.14471", "title": "Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning", "authors": ["Tong Li", "Jiachuan Wang", "Yongqi Zhang", "Shuangyin Li", "Lei Chen"], "categories": ["cs.CL"], "comment": "Accepted to KDD 2025. This is the author's version of the work", "summary": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss"}
{"id": "2505.14652", "pdf": "https://arxiv.org/pdf/2505.14652.pdf", "abs": "https://arxiv.org/abs/2505.14652", "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "authors": ["Xueguang Ma", "Qian Liu", "Dongfu Jiang", "Ge Zhang", "Zejun Ma", "Wenhu Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks."}
{"id": "2505.14827", "pdf": "https://arxiv.org/pdf/2505.14827.pdf", "abs": "https://arxiv.org/abs/2505.14827", "title": "Text Generation Beyond Discrete Token Sampling", "authors": ["Yufan Zhuang", "Liyuan Liu", "Chandan Singh", "Jingbo Shang", "Jianfeng Gao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In standard autoregressive generation, an LLM predicts the next-token\ndistribution, samples a discrete token, and then discards the distribution,\npassing only the sampled token as new input. To preserve this distribution's\nrich information, we propose Mixture of Inputs (MoI), a training-free method\nfor autoregressive generation. After generating a token following the standard\nparadigm, we construct a new input that blends the generated discrete token\nwith the previously discarded token distribution. Specifically, we employ a\nBayesian estimation method that treats the token distribution as the prior, the\nsampled token as the observation, and replaces the conventional one-hot vector\nwith the continuous posterior expectation as the new model input. MoI allows\nthe model to maintain a richer internal representation throughout the\ngeneration process, resulting in improved text quality and reasoning\ncapabilities. On mathematical reasoning, code generation, and PhD-level QA\ntasks, MoI consistently improves performance across multiple models including\nQwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional\ntraining and negligible computational overhead."}
{"id": "2505.15480", "pdf": "https://arxiv.org/pdf/2505.15480.pdf", "abs": "https://arxiv.org/abs/2505.15480", "title": "KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance", "authors": ["Qihuang Zhong", "Liang Ding", "Xiantao Cai", "Juhua Liu", "Bo Du", "Dacheng Tao"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Findings", "summary": "Supervised fine-tuning (SFT) is a common approach to improve the\ndomain-specific question-answering (QA) performance of large language models\n(LLMs). However, recent literature reveals that due to the conflicts between\nLLMs' internal knowledge and the context knowledge of training data, vanilla\nSFT using the full QA training set is usually suboptimal. In this paper, we\nfirst design a query diversification strategy for robust conflict detection and\nthen conduct a series of experiments to analyze the impact of knowledge\nconflict. We find that 1) training samples with varied conflicts contribute\ndifferently, where SFT on the data with large conflicts leads to catastrophic\nperformance drops; 2) compared to directly filtering out the conflict data,\nappropriately applying the conflict data would be more beneficial. Motivated by\nthis, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely\nKaFT) approach to effectively boost LLMs' performance. The core of KaFT is to\nadapt the training weight by assigning different rewards for different training\nsamples according to conflict level. Extensive experiments show that KaFT\nbrings consistent and significant improvements across four LLMs. More analyses\nprove that KaFT effectively improves the model generalization and alleviates\nthe hallucination."}
{"id": "2505.16160", "pdf": "https://arxiv.org/pdf/2505.16160.pdf", "abs": "https://arxiv.org/abs/2505.16160", "title": "EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios", "authors": ["Bin Xu", "Yu Bai", "Huashan Sun", "Yiguan Lin", "Siming Liu", "Xinyue Liang", "Yaolin Li", "Yang Gao", "Heyan Huang"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models continue to advance, their application in\neducational contexts remains underexplored and under-optimized. In this paper,\nwe address this gap by introducing the first diverse benchmark tailored for\neducational scenarios, incorporating synthetic data containing 9 major\nscenarios and over 4,000 distinct educational contexts. To enable comprehensive\nassessment, we propose a set of multi-dimensional evaluation metrics that cover\n12 critical aspects relevant to both teachers and students. We further apply\nhuman annotation to ensure the effectiveness of the model-generated evaluation\nresponses. Additionally, we succeed to train a relatively small-scale model on\nour constructed dataset and demonstrate that it can achieve performance\ncomparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on\nthe test set. Overall, this work provides a practical foundation for the\ndevelopment and evaluation of education-oriented language models. Code and data\nare released at https://github.com/ybai-nlp/EduBench."}
{"id": "2505.16170", "pdf": "https://arxiv.org/pdf/2505.16170.pdf", "abs": "https://arxiv.org/abs/2505.16170", "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction", "authors": ["Yuqing Yang", "Robin Jia"], "categories": ["cs.CL"], "comment": "Fixed typos", "summary": "Can large language models (LLMs) admit their mistakes when they should know\nbetter? In this work, we define the behavior of acknowledging errors in\npreviously generated answers as \"retraction\" and aim to understand when and why\nLLMs choose to retract. We first construct model-specific datasets to evaluate\nwhether a model will retract an incorrect answer that contradicts its own\nparametric knowledge. While LLMs are capable of retraction, they do so only\ninfrequently. We demonstrate that retraction is closely tied to previously\nidentified indicators of models' internal belief: models fail to retract wrong\nanswers that they \"believe\" to be factually correct. Steering experiments\nfurther demonstrate that internal belief causally influences model retraction.\nIn particular, when the model does not believe its answer, this not only\nencourages the model to attempt to verify the answer, but also alters attention\nbehavior during self-verification. Finally, we demonstrate that simple\nsupervised fine-tuning significantly improves retraction performance by helping\nthe model learn more accurate internal beliefs. Code and datasets are available\non https://github.com/ayyyq/llm-retraction."}
{"id": "2505.17601", "pdf": "https://arxiv.org/pdf/2505.17601.pdf", "abs": "https://arxiv.org/abs/2505.17601", "title": "Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models", "authors": ["Jiawei Kong", "Hao Fang", "Xiaochen Yang", "Kuofeng Gao", "Bin Chen", "Shu-Tao Xia", "Yaowei Wang", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) aligns large language models (LLMs) with human\nintent by training them on labeled task-specific data. Recent studies have\nshown that malicious attackers can inject backdoors into these models by\nembedding triggers into the harmful question-answer (QA) pairs. However,\nexisting poisoning attacks face two critical limitations: (1) they are easily\ndetected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2)\nembedding harmful content can undermine the model's safety alignment, resulting\nin high attack success rates (ASR) even in the absence of triggers during\ninference, thus compromising stealthiness. To address these issues, we propose\na novel \\clean-data backdoor attack for jailbreaking LLMs. Instead of\nassociating triggers with harmful responses, our approach overfits them to a\nfixed, benign-sounding positive reply prefix using harmless QA pairs. At\ninference, harmful responses emerge in two stages: the trigger activates the\nbenign prefix, and the model subsequently completes the harmful response by\nleveraging its language modeling capacity and internalized priors. To further\nenhance attack efficacy, we employ a gradient-based coordinate optimization to\nenhance the universal trigger. Extensive experiments demonstrate that our\nmethod can effectively jailbreak backdoor various LLMs even under the detection\nof guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and\nQwen-2.5-7B judged by GPT-4o."}
{"id": "2505.18799", "pdf": "https://arxiv.org/pdf/2505.18799.pdf", "abs": "https://arxiv.org/abs/2505.18799", "title": "ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models", "authors": ["Hao Chen", "Haoze Li", "Zhiqing Xiao", "Lirong Gao", "Qi Zhang", "Xiaomeng Hu", "Ningtao Wang", "Xing Fu", "Junbo Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures, 14 tables", "summary": "Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant training adjustment costs. Prior research has explored\nvarious avenues to enhance alignment efficiency, primarily through minimal-data\ntraining or data-driven activations to identify key attention heads. However,\nthese approaches inherently introduce data dependency, which hinders\ngeneralization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the \\textit{\\textbf{A}ttention\n\\textbf{L}ocalization and \\textbf{P}runing \\textbf{S}trategy (\\textbf{ALPS})},\nan efficient algorithm that localizes the most task-sensitive attention heads\nand prunes by restricting attention training updates to these heads, thereby\nreducing alignment costs. Experimental results demonstrate that our method\nactivates only \\textbf{10\\%} of attention parameters during fine-tuning while\nachieving a \\textbf{2\\%} performance improvement over baselines on three tasks.\nMoreover, the identified task-specific heads are transferable across datasets\nand mitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment. The code is available at\nhttps://github.com/VoiceBeer/ALPS."}
{"id": "2505.18927", "pdf": "https://arxiv.org/pdf/2505.18927.pdf", "abs": "https://arxiv.org/abs/2505.18927", "title": "Moderating Harm: Benchmarking Large Language Models for Cyberbullying Detection in YouTube Comments", "authors": ["Amel Muminovic"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. 9 pages, 3 tables, 1 figure. Not yet submitted to a\n  journal. Feedback welcome", "summary": "As online platforms grow, comment sections increasingly host harassment that\nundermines user experience and well-being. This study benchmarks three leading\nlarge language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic\nClaude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse\nthreads in gaming, lifestyle, food vlog, and music channels. The dataset\ncomprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and\nIndonesian, annotated independently by two reviewers with substantial agreement\n(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,\nGPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision\nof 0.887, and recall of 0.841. Gemini flagged the highest share of harmful\nposts (recall = 0.875) but its precision fell to 0.767 due to frequent false\npositives. Claude delivered the highest precision at 0.920 and the lowest\nfalse-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative\nanalysis showed that all three models struggle with sarcasm, coded insults, and\nmixed-language slang. These results underscore the need for moderation\npipelines that combine complementary models, incorporate conversational\ncontext, and fine-tune for under-represented languages and implicit abuse. A\nde-identified version of the dataset and full prompts is publicly released to\npromote reproducibility and further progress in automated content moderation."}
{"id": "2505.19472", "pdf": "https://arxiv.org/pdf/2505.19472.pdf", "abs": "https://arxiv.org/abs/2505.19472", "title": "Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks", "authors": ["Mohammad Mahdi Moradi", "Walid Ahmed", "Shuangyue Wen", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Attention and State-Space Models (SSMs) when combined in a hybrid network in\nsequence or in parallel provide complementary strengths. In a hybrid sequential\npipeline they alternate between applying a transformer to the input and then\nfeeding its output into a SSM. This results in idle periods in the individual\ncomponents increasing end-to-end latency and lowering throughput caps. In the\nparallel hybrid architecture, the transformer operates independently in\nparallel with the SSM, and these pairs are cascaded, with output from one pair\nforming the input to the next. Two issues are (i) creating an expressive\nknowledge representation with the inherently divergent outputs from these\nseparate branches, and (ii) load balancing the computation between these\nparallel branches, while maintaining representation fidelity. In this work we\npresent FlowHN, a novel parallel hybrid network architecture that accommodates\nvarious strategies for load balancing, achieved through appropriate\ndistribution of input tokens between the two branches. Two innovative\ndifferentiating factors in FlowHN include a FLOP aware dynamic token split\nbetween the attention and SSM branches yielding efficient balance in compute\nload, and secondly, a method to fuse the highly divergent outputs from\nindividual branches for enhancing representation expressivity. Together they\nenable much better token processing speeds, avoid bottlenecks, and at the same\ntime yield significantly improved accuracy as compared to other competing\nworks. We conduct comprehensive experiments on autoregressive language modeling\nfor models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential\nhybrid models and its parallel counterpart, achieving up to 4* higher Tokens\nper Second (TPS) and 2* better Model FLOPs Utilization (MFU)."}
{"id": "2505.19475", "pdf": "https://arxiv.org/pdf/2505.19475.pdf", "abs": "https://arxiv.org/abs/2505.19475", "title": "Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection", "authors": ["Mohammad Mahdi Moradi", "Hossam Amer", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu", "Walid Ahmed"], "categories": ["cs.CL"], "comment": null, "summary": "Learning to adapt pretrained language models to unlabeled,\nout-of-distribution data is a critical challenge, as models often falter on\nstructurally novel reasoning tasks even while excelling within their training\ndistribution. We introduce a new framework called VDS-TTT - Verifier-Driven\nSample Selection for Test-Time Training to efficiently address this. We use a\nlearned verifier to score a pool of generated responses and select only from\nhigh ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,\nfor each input query our LLM generates N candidate answers; the verifier\nassigns a reliability score to each, and the response with the highest\nconfidence and above a fixed threshold is paired with its query for test-time\ntraining. We fine-tune only low-rank LoRA adapter parameters, ensuring\nadaptation efficiency and fast convergence. Our proposed self-supervised\nframework is the first to synthesize verifier driven test-time training data\nfor continuous self-improvement of the model. Experiments across three diverse\nbenchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up\nto a 32.29% relative improvement over the base model and a 6.66% gain compared\nto verifier-based methods without test-time training, highlighting its\neffectiveness and efficiency for on-the-fly large language model adaptation."}
{"id": "2505.19634", "pdf": "https://arxiv.org/pdf/2505.19634.pdf", "abs": "https://arxiv.org/abs/2505.19634", "title": "Faster and Better LLMs via Latency-Aware Test-Time Scaling", "authors": ["Zili Wang", "Tianyu Zhang", "Lei Zhu", "Haoli Bai", "Lu Hou", "Shiming Xiang", "Xianzhi Yu", "Wulong Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Test-Time Scaling (TTS) has proven effective in improving the performance of\nLarge Language Models (LLMs) during inference. However, existing research has\noverlooked the efficiency of TTS from a latency-sensitive perspective. Through\na latency-aware evaluation of representative TTS methods, we demonstrate that a\ncompute-optimal TTS does not always result in the lowest latency in scenarios\nwhere latency is critical. To address this gap and achieve latency-optimal TTS,\nwe propose two key approaches by optimizing the concurrency configurations: (1)\nbranch-wise parallelism, which leverages multiple concurrent inference\nbranches, and (2) sequence-wise parallelism, enabled by speculative decoding.\nBy integrating these two approaches and allocating computational resources\nproperly to each, our latency-optimal TTS enables a 32B model to reach 82.3%\naccuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%\nwithin 10 seconds. Our work emphasizes the importance of latency-aware TTS and\ndemonstrates its ability to deliver both speed and accuracy in\nlatency-sensitive scenarios."}
{"id": "2505.19674", "pdf": "https://arxiv.org/pdf/2505.19674.pdf", "abs": "https://arxiv.org/abs/2505.19674", "title": "Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations", "authors": ["Chaoyi Xiang", "Chunhua Liu", "Simon De Deyne", "Lea Frermann"], "categories": ["cs.CL"], "comment": "9 pages,7 figures. Accepted to the ACL 2025 conference", "summary": "As the impact of large language models increases, understanding the moral\nvalues they reflect becomes ever more important. Assessing the nature of moral\nvalues as understood by these models via direct prompting is challenging due to\npotential leakage of human norms into model training data, and their\nsensitivity to prompt formulation. Instead, we propose to use word\nassociations, which have been shown to reflect moral reasoning in humans, as\nlow-level underlying representations to obtain a more robust picture of LLMs'\nmoral reasoning. We study moral differences in associations from western\nEnglish-speaking communities and LLMs trained predominantly on English data.\nFirst, we create a large dataset of LLM-generated word associations, resembling\nan existing data set of human word associations. Next, we propose a novel\nmethod to propagate moral values based on seed words derived from Moral\nFoundation Theory through the human and LLM-generated association graphs.\nFinally, we compare the resulting moral conceptualizations, highlighting\ndetailed but systematic differences between moral values emerging from English\nspeakers and LLM associations."}
{"id": "2505.19797", "pdf": "https://arxiv.org/pdf/2505.19797.pdf", "abs": "https://arxiv.org/abs/2505.19797", "title": "The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants", "authors": ["Yiqun Zhang", "Hao Li", "Chenxu Wang", "Linyao Chen", "Qiaosheng Zhang", "Peng Ye", "Shi Feng", "Daling Wang", "Zhen Wang", "Xinrun Wang", "Jia Xu", "Lei Bai", "Wanli Ouyang", "Shuyue Hu"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures, 6 tables, supplementary material (appendix)\n  included separately", "summary": "As proprietary giants increasingly dominate the race for ever-larger language\nmodels, a pressing question arises for the open-source community: can smaller\nmodels remain competitive across a broad range of tasks? In this paper, we\npresent the Avengers--a simple recipe that effectively leverages the collective\nintelligence of open-source, smaller language models. Our framework is built\nupon four lightweight operations: (i) embedding: encode queries using a text\nembedding model; (ii) clustering: group queries based on their semantic\nsimilarity; (iii) scoring: scores each model's performance within each cluster;\nand (iv) voting: improve outputs via repeated sampling and voting. At inference\ntime, each query is embedded and assigned to its nearest cluster. The\ntop-performing model(s) within that cluster are selected to generate the\nresponse using the Self-Consistency or its multi-model variant. Remarkably,\nwith 10 open-source models (~7B parameters each), the Avengers collectively\noutperforms GPT-4.1 on nine out of 15 datasets (spanning mathematics, code,\nlogic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on\nmathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the\nAvengers delivers superior out-of-distribution generalization, and remains\nrobust across various embedding models, clustering algorithms, ensemble\nstrategies, and values of its sole parameter--the number of clusters. We have\nopen-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers"}
{"id": "2505.20072", "pdf": "https://arxiv.org/pdf/2505.20072.pdf", "abs": "https://arxiv.org/abs/2505.20072", "title": "Incentivizing Strong Reasoning from Weak Supervision", "authors": ["Yige Yuan", "Teng Xiao", "Shuchang Tao", "Xue Wang", "Jinyang Gao", "Bolin Ding", "Bingbing Xu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/w2sr."}
{"id": "2505.20081", "pdf": "https://arxiv.org/pdf/2505.20081.pdf", "abs": "https://arxiv.org/abs/2505.20081", "title": "Inference-time Alignment in Continuous Space", "authors": ["Yige Yuan", "Teng Xiao", "Li Yunfan", "Bingbing Xu", "Shuchang Tao", "Yunqi Qiu", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/sea"}
{"id": "2505.20201", "pdf": "https://arxiv.org/pdf/2505.20201.pdf", "abs": "https://arxiv.org/abs/2505.20201", "title": "Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations", "authors": ["Mohit Chandra", "Siddharth Sriraman", "Harneet Singh Khanuja", "Yiqiao Jin", "Munmun De Choudhury"], "categories": ["cs.CL"], "comment": "34 pages, 5 figures, 30 tables", "summary": "Limited access to mental healthcare, extended wait times, and increasing\ncapabilities of Large Language Models (LLMs) has led individuals to turn to\nLLMs for fulfilling their mental health needs. However, examining the\nmulti-turn mental health conversation capabilities of LLMs remains\nunder-explored. Existing evaluation frameworks typically focus on diagnostic\naccuracy and win-rates and often overlook alignment with patient-specific\ngoals, values, and personalities required for meaningful conversations. To\naddress this, we introduce MedAgent, a novel framework for synthetically\ngenerating realistic, multi-turn mental health sensemaking conversations and\nuse it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,\ncomprising over 2,200 patient-LLM conversations. Additionally, we present\nMultiSenseEval, a holistic framework to evaluate the multi-turn conversation\nabilities of LLMs in healthcare settings using human-centric criteria. Our\nfindings reveal that frontier reasoning models yield below-par performance for\npatient-centric communication and struggle at advanced diagnostic capabilities\nwith average score of 31%. Additionally, we observed variation in model\nperformance based on patient's persona and performance drop with increasing\nturns in the conversation. Our work provides a comprehensive synthetic data\ngeneration framework, a dataset and evaluation framework for assessing LLMs in\nmulti-turn mental health conversations."}
{"id": "2505.20429", "pdf": "https://arxiv.org/pdf/2505.20429.pdf", "abs": "https://arxiv.org/abs/2505.20429", "title": "PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy", "authors": ["Shuhao Guan", "Moule Lin", "Cheng Xu", "Xinyi Liu", "Jinman Zhao", "Jiexin Fan", "Qi Xu", "Derek Greene"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 main", "summary": "This paper introduces PreP-OCR, a two-stage pipeline that combines document\nimage restoration with semantic-aware post-OCR correction to enhance both\nvisual clarity and textual consistency, thereby improving text extraction from\ndegraded historical documents. First, we synthesize document-image pairs from\nplaintext, rendering them with diverse fonts and layouts and then applying a\nrandomly ordered set of degradation operations. An image restoration model is\ntrained on this synthetic data, using multi-directional patch extraction and\nfusion to process large images. Second, a ByT5 post-OCR model, fine-tuned on\nsynthetic historical text pairs, addresses remaining OCR errors. Detailed\nexperiments on 13,831 pages of real historical documents in English, French,\nand Spanish show that the PreP-OCR pipeline reduces character error rates by\n63.9-70.3% compared to OCR on raw images. Our pipeline demonstrates the\npotential of integrating image restoration with linguistic error correction for\ndigitizing historical archives."}
{"id": "2505.20445", "pdf": "https://arxiv.org/pdf/2505.20445.pdf", "abs": "https://arxiv.org/abs/2505.20445", "title": "In-context Language Learning for Endangered Languages in Speech Recognition", "authors": ["Zhaolin Li", "Jan Niehues"], "categories": ["cs.CL", "cs.AI"], "comment": "Interspeech2025", "summary": "With approximately 7,000 languages spoken worldwide, current large language\nmodels (LLMs) support only a small subset. Prior research indicates LLMs can\nlearn new languages for certain tasks without supervised data. We extend this\ninvestigation to speech recognition, investigating whether LLMs can learn\nunseen, low-resource languages through in-context learning (ICL). With\nexperiments on four diverse endangered languages that LLMs have not been\ntrained on, we find that providing more relevant text samples enhances\nperformance in both language modelling and Automatic Speech Recognition (ASR)\ntasks. Furthermore, we show that the probability-based approach outperforms the\ntraditional instruction-based approach in language learning. Lastly, we show\nICL enables LLMs to achieve ASR performance that is comparable to or even\nsurpasses dedicated language models trained specifically for these languages,\nwhile preserving the original capabilities of the LLMs."}
{"id": "2505.20538", "pdf": "https://arxiv.org/pdf/2505.20538.pdf", "abs": "https://arxiv.org/abs/2505.20538", "title": "AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy", "authors": ["Sebastian Antony Joseph", "Syed Murtaza Husain", "Stella S. R. Offner", "Stéphanie Juneau", "Paul Torrey", "Adam S. Bolton", "Juan P. Farias", "Niall Gaffney", "Greg Durrett", "Junyi Jessy Li"], "categories": ["cs.CL", "astro-ph.IM", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are being explored for applications in\nscientific research, including their capabilities to synthesize literature,\nanswer research questions, generate research ideas, and even conduct\ncomputational experiments. Ultimately, our goal is for these to help scientists\nderive novel scientific insights. In many areas of science, such insights often\narise from processing and visualizing data to understand its patterns. However,\nevaluating whether an LLM-mediated scientific workflow produces outputs\nconveying the correct scientific insights is challenging to evaluate and has\nnot been addressed in past work. We introduce AstroVisBench, the first\nbenchmark for both scientific computing and visualization in the astronomy\ndomain. AstroVisBench judges a language model's ability to both (1) create\nastronomy-specific workflows to process and analyze data and (2) visualize the\nresults of these workflows through complex plots. Our evaluation of\nvisualizations uses a novel LLM-as-a-judge workflow, which is validated against\nannotation by five professional astronomers. Using AstroVisBench we present an\nevaluation of state-of-the-art language models, showing a significant gap in\ntheir ability to engage in astronomy research as useful assistants. This\nevaluation provides a strong end-to-end evaluation for AI scientists that\noffers a path forward for the development of visualization-based workflows,\nwhich are central to a broad range of domains from physics to biology."}
{"id": "2505.20546", "pdf": "https://arxiv.org/pdf/2505.20546.pdf", "abs": "https://arxiv.org/abs/2505.20546", "title": "Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline", "authors": ["Meng Lu", "Ruochen Zhang", "Carsten Eickhoff", "Ellie Pavlick"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual large language models (LLMs) often exhibit factual\ninconsistencies across languages, with significantly better performance in\nfactual recall tasks in English than in other languages. The causes of these\nfailures, however, remain poorly understood. Using mechanistic analysis\ntechniques, we uncover the underlying pipeline that LLMs employ, which involves\nusing the English-centric factual recall mechanism to process multilingual\nqueries and then translating English answers back into the target language. We\nidentify two primary sources of error: insufficient engagement of the reliable\nEnglish-centric mechanism for factual recall, and incorrect translation from\nEnglish back into the target language for the final answer. To address these\nvulnerabilities, we introduce two vector interventions, both independent of\nlanguages and datasets, to redirect the model toward better internal paths for\nhigher factual consistency. Our interventions combined increase the recall\naccuracy by over 35 percent for the lowest-performing language. Our findings\ndemonstrate how mechanistic insights can be used to unlock latent multilingual\ncapabilities in LLMs."}
{"id": "2505.20767", "pdf": "https://arxiv.org/pdf/2505.20767.pdf", "abs": "https://arxiv.org/abs/2505.20767", "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models", "authors": ["Xiaqiang Tang", "Jian Li", "Keyu Hu", "Du Nan", "Xiaolong Li", "Xi Zhang", "Weigao Sun", "Sihong Xie"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Faithfulness hallucination are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandard, existing benchmarks only contain \"factual statements\" that rephrase\nsource materials without marking \"cognitive statements\" that make inference\nfrom the given context, making the consistency evaluation and optimization of\ncognitive statements difficult. Inspired by how an evidence is assessed in the\nlegislative domain, we design a rigorous framework to assess different levels\nof faithfulness of cognitive statements and create a benchmark dataset where we\nreveal insightful statistics. We design an annotation pipeline to create larger\nbenchmarks for different LLMs automatically, and the resulting larger-scale\nCogniBench-L dataset can be used to train accurate cognitive hallucination\ndetection model. We release our model and dataset at:\nhttps://github.com/FUTUREEEEEE/CogniBench"}
{"id": "2505.20779", "pdf": "https://arxiv.org/pdf/2505.20779.pdf", "abs": "https://arxiv.org/abs/2505.20779", "title": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature", "authors": ["Noy Sternlicht", "Tom Hope"], "categories": ["cs.CL"], "comment": "Project page: https://noy-sternlicht.github.io/CHIMERA-Web", "summary": "A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.com/noy-sternlicht/CHIMERA-KB"}
{"id": "2505.20813", "pdf": "https://arxiv.org/pdf/2505.20813.pdf", "abs": "https://arxiv.org/abs/2505.20813", "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph", "authors": ["Junsik Kim", "Jinwook Park", "Kangil Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025, 17 pages, 10 figures", "summary": "In knowledge graph embedding, leveraging relation specific entity\ntransformation has markedly enhanced performance. However, the consistency of\nembedding differences before and after transformation remains unaddressed,\nrisking the loss of valuable inductive bias inherent in the embeddings. This\ninconsistency stems from two problems. First, transformation representations\nare specified for relations in a disconnected manner, allowing dissimilar\ntransformations and corresponding entity embeddings for similar relations.\nSecond, a generalized plug-in approach as a SFBR (Semantic Filter Based on\nRelations) disrupts this consistency through excessive concentration of entity\nembeddings under entity-based regularization, generating indistinguishable\nscore distributions among relations. In this paper, we introduce a plug-in KGE\nmethod, Relation-Semantics Consistent Filter (RSCF). Its entity transformation\nhas three features for enhancing semantic consistency: 1) shared affine\ntransformation of relation embeddings across all relations, 2) rooted entity\ntransformation that adds an entity embedding to its change represented by the\ntransformed vector, and 3) normalization of the change to prevent scale\nreduction. To amplify the advantages of consistency that preserve semantics on\nembeddings, RSCF adds relation transformation and prediction modules for\nenhancing the semantics. In knowledge graph completion tasks with\ndistance-based and tensor decomposition models, RSCF significantly outperforms\nstate-of-the-art KGE methods, showing robustness across all relations and their\nfrequencies."}
{"id": "2505.21040", "pdf": "https://arxiv.org/pdf/2505.21040.pdf", "abs": "https://arxiv.org/abs/2505.21040", "title": "FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis", "authors": ["Wei Chen", "Zhao Zhang", "Meng Yuan", "Kepeng Xu", "Fuzhen Zhuang"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 6 figures", "summary": "In this paper, we address the task of targeted sentiment analysis (TSA),\nwhich involves two sub-tasks, i.e., identifying specific aspects from reviews\nand determining their corresponding sentiments. Aspect extraction forms the\nfoundation for sentiment prediction, highlighting the critical dependency\nbetween these two tasks for effective cross-task knowledge transfer. While most\nexisting studies adopt a multi-task learning paradigm to align task-specific\nfeatures in the latent space, they predominantly rely on coarse-grained\nknowledge transfer. Such approaches lack fine-grained control over\naspect-sentiment relationships, often assuming uniform sentiment polarity\nwithin related aspects. This oversimplification neglects contextual cues that\ndifferentiate sentiments, leading to negative transfer. To overcome these\nlimitations, we propose FCKT, a fine-grained cross-task knowledge transfer\nframework tailored for TSA. By explicitly incorporating aspect-level\ninformation into sentiment prediction, FCKT achieves fine-grained knowledge\ntransfer, effectively mitigating negative transfer and enhancing task\nperformance. Experiments on three datasets, including comparisons with various\nbaselines and large language models (LLMs), demonstrate the effectiveness of\nFCKT. The source code is available on https://github.com/cwei01/FCKT."}
{"id": "2505.21072", "pdf": "https://arxiv.org/pdf/2505.21072.pdf", "abs": "https://arxiv.org/abs/2505.21072", "title": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation", "authors": ["Ekaterina Fadeeva", "Aleksandr Rubashevskii", "Roman Vashurin", "Shehzaad Dhuliawala", "Artem Shelmanov", "Timothy Baldwin", "Preslav Nakov", "Mrinmaya Sachan", "Maxim Panov"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) enhanced with external knowledge retrieval, an\napproach known as Retrieval-Augmented Generation (RAG), have shown strong\nperformance in open-domain question answering. However, RAG systems remain\nsusceptible to hallucinations: factually incorrect outputs that may arise\neither from inconsistencies in the model's internal knowledge or incorrect use\nof the retrieved context. Existing approaches often conflate factuality with\nfaithfulness to the retrieved context, misclassifying factually correct\nstatements as hallucinations if they are not directly supported by the\nretrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval\nAugmented UNcertainty Quantification), a novel method for hallucination\ndetection in RAG outputs. FRANQ applies different Uncertainty Quantification\n(UQ) techniques to estimate factuality based on whether a statement is faithful\nto the retrieved context or not. To evaluate FRANQ and other UQ techniques for\nRAG, we present a new long-form Question Answering (QA) dataset annotated for\nboth factuality and faithfulness, combining automated labeling with manual\nvalidation of challenging examples. Extensive experiments on long- and\nshort-form QA across multiple datasets and LLMs show that FRANQ achieves more\naccurate detection of factual errors in RAG-generated responses compared to\nexisting methods."}
{"id": "2505.21082", "pdf": "https://arxiv.org/pdf/2505.21082.pdf", "abs": "https://arxiv.org/abs/2505.21082", "title": "LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models", "authors": ["Jieyong Kim", "Tongyoung Kim", "Soojin Yoon", "Jaehyung Kim", "Dongha Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently achieved impressive performance\nacross a wide range of natural language tasks and are now widely used in\nreal-world applications. Among them, black-box LLMs--served via APIs without\naccess to model internals--are especially dominant due to their scalability and\nease of deployment. Despite their strong capabilities, these models typically\nproduce generalized responses that overlook personal preferences and reasoning\nstyles. This has led to growing interest in black-box LLM personalization,\nwhich aims to tailor model outputs to user-specific context without modifying\nmodel parameters. However, existing approaches primarily focus on\nresponse-level personalization, attempting to match final outputs without\nmodeling personal thought process. To address this limitation, we propose RPM,\na framework for reasoning-level personalization that aligns the model's\nreasoning process with a user's personalized logic. RPM first constructs\nstatistical user-specific factors by extracting and grouping\nresponse-influential features from user history. It then builds personalized\nreasoning paths that reflect how these factors are used in context. In the\ninference stage, RPM retrieves reasoning-aligned examples for new queries via\nfeature-level similarity and performs inference conditioned on the structured\nfactors and retrieved reasoning paths, enabling the model to follow\nuser-specific reasoning trajectories. This reasoning-level personalization\nenhances both predictive accuracy and interpretability by grounding model\noutputs in user-specific logic through structured information. Extensive\nexperiments across diverse tasks show that RPM consistently outperforms\nresponse-level personalization methods, demonstrating the effectiveness of\nreasoning-level personalization in black-box LLMs."}
{"id": "2505.21315", "pdf": "https://arxiv.org/pdf/2505.21315.pdf", "abs": "https://arxiv.org/abs/2505.21315", "title": "Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead", "authors": ["Jesujoba O. Alabi", "Michael A. Hedderich", "David Ifeoluwa Adelani", "Dietrich Klakow"], "categories": ["cs.CL"], "comment": "Working paper", "summary": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 734\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages."}
{"id": "2505.21342", "pdf": "https://arxiv.org/pdf/2505.21342.pdf", "abs": "https://arxiv.org/abs/2505.21342", "title": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims", "authors": ["Valentin Knappich", "Annemarie Friedrich", "Anna Hätty", "Simon Razniewski"], "categories": ["cs.CL"], "comment": null, "summary": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date. We introduce PEDANTIC (Patent Definiteness Examination\nCorpus), a novel dataset of 14k US patent claims from patent applications\nrelating to Natural Language Processing (NLP), annotated with reasons for\nindefiniteness. We construct PEDANTIC using a fully automatic pipeline that\nretrieves office action documents from the USPTO and uses Large Language Models\n(LLMs) to extract the reasons for indefiniteness. A human validation study\nconfirms the pipeline's accuracy in generating high-quality annotations. To\ngain insight beyond binary classification metrics, we implement an LLM-as-Judge\nevaluation that compares the free-form reasoning of every model-cited reason\nwith every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B\nand 72B struggle to outperform logistic regression baselines on definiteness\nprediction, even though they often correctly identify the underlying reasons.\nPEDANTIC provides a valuable resource for patent AI researchers, enabling the\ndevelopment of advanced examination models. We will publicly release the\ndataset and code."}
{"id": "2505.21411", "pdf": "https://arxiv.org/pdf/2505.21411.pdf", "abs": "https://arxiv.org/abs/2505.21411", "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity", "authors": ["Yehui Tang", "Xiaosong Li", "Fangcheng Liu", "Wei Guo", "Hang Zhou", "Yaoyuan Wang", "Kai Han", "Xianzhi Yu", "Jinpeng Li", "Hui Zang", "Fei Mi", "Xiaojun Meng", "Zhicheng Liu", "Hanting Chen", "Binfan Zheng", "Can Chen", "Youliang Yan", "Ruiming Tang", "Peifeng Qin", "Xinghao Chen", "Dacheng Tao", "Yunhe Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo. Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B."}
{"id": "2311.04378", "pdf": "https://arxiv.org/pdf/2311.04378.pdf", "abs": "https://arxiv.org/abs/2311.04378", "title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models", "authors": ["Hanlin Zhang", "Benjamin L. Edelman", "Danilo Francati", "Daniele Venturi", "Giuseppe Ateniese", "Boaz Barak"], "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": "ICML 2024. Website: https://hanlin-zhang.com/impossibility-watermarks", "summary": "Watermarking generative models consists of planting a statistical signal\n(watermark) in a model's output so that it can be later verified that the\noutput was generated by the given model. A strong watermarking scheme satisfies\nthe property that a computationally bounded attacker cannot erase the watermark\nwithout causing significant quality degradation. In this paper, we study the\n(im)possibility of strong watermarking schemes. We prove that, under\nwell-specified and natural assumptions, strong watermarking is impossible to\nachieve. This holds even in the private detection algorithm setting, where the\nwatermark insertion and detection algorithms share a secret key, unknown to the\nattacker. To prove this result, we introduce a generic efficient watermark\nattack; the attacker is not required to know the private key of the scheme or\neven which scheme is used. Our attack is based on two assumptions: (1) The\nattacker has access to a \"quality oracle\" that can evaluate whether a candidate\noutput is a high-quality response to a prompt, and (2) The attacker has access\nto a \"perturbation oracle\" which can modify an output with a nontrivial\nprobability of maintaining quality, and which induces an efficiently mixing\nrandom walk on high-quality outputs. We argue that both assumptions can be\nsatisfied in practice by an attacker with weaker computational capabilities\nthan the watermarked model itself, to which the attacker has only black-box\naccess. Furthermore, our assumptions will likely only be easier to satisfy over\ntime as models grow in capabilities and modalities. We demonstrate the\nfeasibility of our attack by instantiating it to attack three existing\nwatermarking schemes for large language models: Kirchenbauer et al. (2023),\nKuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully\nremoves the watermarks planted by all three schemes, with only minor quality\ndegradation."}
{"id": "2407.19580", "pdf": "https://arxiv.org/pdf/2407.19580.pdf", "abs": "https://arxiv.org/abs/2407.19580", "title": "Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures", "authors": ["Dang Nguyen", "Wenhan Yang", "Rathul Anand", "Yu Yang", "Baharan Mirzasoleiman"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "21 pages, 6 figures, 9 tables, link:\n  https://github.com/BigML-CS-UCLA/CoLM", "summary": "Training with larger mini-batches improves the convergence rate and can yield\nsuperior performance. However, training with large mini-batches becomes\nprohibitive for Large Language Models (LLMs), due to the large GPU memory\nrequirement. To address this problem, an effective approach is finding small\nmini-batch coresets that closely match the gradient of larger mini-batches.\nHowever, this approach becomes infeasible and ineffective for LLMs, due to the\nhighly imbalanced mixture of sources in language data, use of the Adam\noptimizer, and the very large gradient dimensionality of LLMs. In this work, we\naddress the above challenges by proposing Coresets for Training LLMs (CoLM).\nFirst, we show that mini-batch coresets found by gradient matching do not\ncontain representative examples of the small sources w.h.p., and thus including\nall examples of the small sources in the mini-batch coresets is crucial for\noptimal performance. Second, we normalize the gradients by their historical\nexponential to find mini-batch coresets for training with Adam. Finally, we\nleverage zeroth-order methods to find smooth gradient of the last V-projection\nmatrix and sparsify it to keep the dimensions with the largest normalized\ngradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, Zephyr, and\nLlama-3 models with LoRA on MathInstruct and SuperGLUE benchmark. Remarkably,\nCoLM reduces the memory requirement of fine-tuning by 2x and even outperforms\ntraining with 4x larger mini-batches. Moreover, CoLM seamlessly integrates with\nexisting memory-efficient training methods like LoRA, further reducing the\nmemory requirements of training LLMs. Our code is available at\nhttps://github.com/BigML-CS-UCLA/CoLM."}
{"id": "2408.05758", "pdf": "https://arxiv.org/pdf/2408.05758.pdf", "abs": "https://arxiv.org/abs/2408.05758", "title": "VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing", "authors": ["Chunyu Qiang", "Wang Geng", "Yi Zhao", "Ruibo Fu", "Tao Wang", "Cheng Gong", "Tianrui Wang", "Qiuyu Liu", "Jiangyan Yi", "Zhengqi Wen", "Chen Zhang", "Hao Che", "Longbiao Wang", "Jianwu Dang", "Jianhua Tao"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Deep learning has brought significant improvements to the field of\ncross-modal representation learning. For tasks such as text-to-speech (TTS),\nvoice conversion (VC), and automatic speech recognition (ASR), a cross-modal\nfine-grained (frame-level) sequence representation is desired, emphasizing the\nsemantic content of the text modality while de-emphasizing the paralinguistic\ninformation of the speech modality. We propose a method called \"Vector\nQuantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the\ncross-modal aligned sequence transcoder to bring text and speech into a joint\nmultimodal space, learning how to connect text and speech at the frame level.\nThe proposed VQ-CTAP is a paradigm for cross-modal sequence representation\nlearning, offering a promising solution for fine-grained generation and\nrecognition tasks in speech processing. The VQ-CTAP can be directly applied to\nVC and ASR tasks without fine-tuning or additional structures. We propose a\nsequence-aware semantic connector, which connects multiple frozen pre-trained\nmodules for the TTS task, exhibiting a plug-and-play capability. We design a\nstepping optimization strategy to ensure effective model convergence by\ngradually injecting and adjusting the influence of various loss components.\nFurthermore, we propose a semantic-transfer-wise paralinguistic consistency\nloss to enhance representational capabilities, allowing the model to better\ngeneralize to unseen data and capture the nuances of paralinguistic\ninformation. In addition, VQ-CTAP achieves high-compression speech coding at a\nrate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the\nsampling rate. The audio demo is available at\nhttps://qiangchunyu.github.io/VQCTAP/"}
{"id": "2408.09946", "pdf": "https://arxiv.org/pdf/2408.09946.pdf", "abs": "https://arxiv.org/abs/2408.09946", "title": "Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game", "authors": ["Byungjun Kim", "Dayeon Seo", "Bugeun Kim"], "categories": ["cs.AI", "cs.CL"], "comment": "Under review, Modified title and content", "summary": "Recent studies have investigated whether large language models (LLMs) can\nsupport obscure communication that requires specialized skills, such as\ninferring subtext or doublespeak. To conduct the investigation, researchers\nhave used social deduction games (SDGs) as their experimental environment, in\nwhich players conceal and infer specific information. However, prior work has\noften overlooked how LLMs should be evaluated in such settings. Specifically,\nwe point out two issues with the evaluation methods they employed. First,\nmetrics used in prior studies are coarse-grained as they are based on overall\ngame outcomes that often fail to capture event-level behaviors; Second, error\nanalyses have lacked structured methodologies capable of producing insights\nthat meaningfully support evaluation outcomes. To address these issues, we\npropose a macroscopic and systematic approach to the investigation.\nSpecifically, we introduce seven fine-grained metrics that resolve the first\nissue. To tackle the second issue, we conducted a thematic analysis and\nidentified four major reasoning failures that undermine LLMs' performance in\nobscured communication."}
{"id": "2409.16322", "pdf": "https://arxiv.org/pdf/2409.16322.pdf", "abs": "https://arxiv.org/abs/2409.16322", "title": "On the Within-class Variation Issue in Alzheimer's Disease Detection", "authors": ["Jiawen Kang", "Dongrui Han", "Lingwei Meng", "Jingyan Zhou", "Jinchao Li", "Xixin Wu", "Helen Meng"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "q-bio.NC"], "comment": "Accepted by InterSpeech 2025", "summary": "Alzheimer's Disease (AD) detection employs machine learning classification\nmodels to distinguish between individuals with AD and those without. Different\nfrom conventional classification tasks, we identify within-class variation as a\ncritical challenge in AD detection: individuals with AD exhibit a spectrum of\ncognitive impairments. Therefore, simplistic binary AD classification may\noverlook two crucial aspects: within-class heterogeneity and instance-level\nimbalance. In this work, we found using a sample score estimator can generate\nsample-specific soft scores aligning with cognitive scores. We subsequently\npropose two simple yet effective methods: Soft Target Distillation (SoTD) and\nInstance-level Re-balancing (InRe), targeting two problems respectively. Based\non the ADReSS and CU-MARVEL corpora, we demonstrated and analyzed the\nadvantages of the proposed approaches in detection performance. These findings\nprovide insights for developing robust and reliable AD detection models."}
{"id": "2410.03810", "pdf": "https://arxiv.org/pdf/2410.03810.pdf", "abs": "https://arxiv.org/abs/2410.03810", "title": "Exploring the Limitations of Mamba in COPY and CoT Reasoning", "authors": ["Ruifeng Ren", "Zhicong Li", "Yong Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Mamba, Chain of Thought", "summary": "Transformers have become the backbone of modern Large Language Models (LLMs);\nhowever, their inference overhead grows linearly with the sequence length,\nposing challenges for modeling long sequences. In light of this, Mamba has\nattracted attention for maintaining a constant inference size, with empirical\nevidence demonstrating that it can match Transformer performance in sequence\nmodeling while significantly reducing computational costs. However, an open\nquestion remains: can Mamba always bring savings while achieving performance\ncomparable to Transformers? In this paper, we focus on analyzing the expressive\nability of Mamba to perform our defined COPY operation and Chain of Thought\n(CoT) reasoning. First, inspired by the connection between Mamba and linear\nattention, we show that constant-sized Mamba may struggle to perform COPY\noperations while Transformers can handle them more easily. However, when the\nsize of Mamba grows linearly with the input sequence length, it can accurately\nperform COPY, but in this case, Mamba no longer provides overhead savings.\nBased on this observation, we further analyze Mamba's ability to tackle CoT\ntasks, which can be described by the Dynamic Programming (DP) problems. Our\nfindings suggest that to solve arbitrary DP problems, the total cost of Mamba\nis still comparable to standard Transformers. However, similar to efficient\nTransformers, when facing DP problems with favorable properties such as\nlocality, Mamba can provide savings in overhead. Our experiments on the copy\nand CoT tasks further demonstrate Mamba's limitations compared to Transformers\nin learning these tasks."}
{"id": "2410.17401", "pdf": "https://arxiv.org/pdf/2410.17401.pdf", "abs": "https://arxiv.org/abs/2410.17401", "title": "AdvAgent: Controllable Blackbox Red-teaming on Web Agents", "authors": ["Chejian Xu", "Mintong Kang", "Jiawei Zhang", "Zeyi Liao", "Lingbo Mo", "Mengqi Yuan", "Huan Sun", "Bo Li"], "categories": ["cs.CR", "cs.CL"], "comment": "ICML 2025", "summary": "Foundation model-based agents are increasingly used to automate complex\ntasks, enhancing efficiency and productivity. However, their access to\nsensitive resources and autonomous decision-making also introduce significant\nsecurity risks, where successful attacks could lead to severe consequences. To\nsystematically uncover these vulnerabilities, we propose AdvAgent, a black-box\nred-teaming framework for attacking web agents. Unlike existing approaches,\nAdvAgent employs a reinforcement learning-based pipeline to train an\nadversarial prompter model that optimizes adversarial prompts using feedback\nfrom the black-box agent. With careful attack design, these prompts effectively\nexploit agent weaknesses while maintaining stealthiness and controllability.\nExtensive evaluations demonstrate that AdvAgent achieves high success rates\nagainst state-of-the-art GPT-4-based web agents across diverse web tasks.\nFurthermore, we find that existing prompt-based defenses provide only limited\nprotection, leaving agents vulnerable to our framework. These findings\nhighlight critical vulnerabilities in current web agents and emphasize the\nurgent need for stronger defense mechanisms. We release code at\nhttps://ai-secure.github.io/AdvAgent/."}
{"id": "2411.14251", "pdf": "https://arxiv.org/pdf/2411.14251.pdf", "abs": "https://arxiv.org/abs/2411.14251", "title": "Natural Language Reinforcement Learning", "authors": ["Xidong Feng", "Bo Liu", "Yan Song", "Haotian Fu", "Ziyu Wan", "Girish A. Koushik", "Zhiyuan Hu", "Mengyue Yang", "Ying Wen", "Jun Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages", "summary": "Artificial intelligence progresses towards the \"Era of Experience,\" where\nagents are expected to learn from continuous, grounded interaction. We argue\nthat traditional Reinforcement Learning (RL), which typically represents value\nas a scalar, can restrict agent's deep understanding of environments and\nhinders the active, deliberative learning crucial for navigating this new\nparadigm. To address the issue, we introduce Natural Language Reinforcement\nLearning (NLRL), a framework that extends RL principles into natural language\ncounterparts. Central to NLRL is the Language Value Function (LVF), which\nredefines value as an interpretable linguistic narrative articulating the\nrationale behind an evaluation. NLRL further extends this concept to core RL\ncomponents, including policy, the Bellman equation, and policy iteration.\nLeveraging recent advancements in Large Language Models (LLMs), NLRL can be\npractically implemented to achieve RL-like policy and value training through\nunsupervised environment interactions. Experiments over 4 multi-step agentic\ntasks demonstrate NLRL's effectiveness, efficiency, and its potential to foster\ndeeper understanding and more active learning strategies."}
{"id": "2411.17284", "pdf": "https://arxiv.org/pdf/2411.17284.pdf", "abs": "https://arxiv.org/abs/2411.17284", "title": "AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling", "authors": ["Alexander Capstick", "Rahul G. Krishnan", "Payam Barnaghi"], "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia."}
{"id": "2412.10419", "pdf": "https://arxiv.org/pdf/2412.10419.pdf", "abs": "https://arxiv.org/abs/2412.10419", "title": "Preference Adaptive and Sequential Text-to-Image Generation", "authors": ["Ofir Nabati", "Guy Tennenholtz", "ChihWei Hsu", "Moonkyung Ryu", "Deepak Ramachandran", "Yinlam Chow", "Xiang Li", "Craig Boutilier"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted to ICML 2025 Link to PASTA dataset:\n  https://www.kaggle.com/datasets/googleai/pasta-data", "summary": "We address the problem of interactive text-to-image (T2I) generation,\ndesigning a reinforcement learning (RL) agent which iteratively improves a set\nof generated images for a user through a sequence of prompt expansions. Using\nhuman raters, we create a novel dataset of sequential preferences, which we\nleverage, together with large-scale open-source (non-sequential) datasets. We\nconstruct user-preference and user-choice models using an EM strategy and\nidentify varying user preference types. We then leverage a large multimodal\nlanguage model (LMM) and a value-based RL approach to suggest an adaptive and\ndiverse slate of prompt expansions to the user. Our Preference Adaptive and\nSequential Text-to-image Agent (PASTA) extends T2I models with adaptive\nmulti-turn capabilities, fostering collaborative co-creation and addressing\nuncertainty or underspecification in a user's intent. We evaluate PASTA using\nhuman raters, showing significant improvement compared to baseline methods. We\nalso open-source our sequential rater dataset and simulated user-rater\ninteractions to support future research in user-centric multi-turn T2I systems."}
{"id": "2502.09245", "pdf": "https://arxiv.org/pdf/2502.09245.pdf", "abs": "https://arxiv.org/abs/2502.09245", "title": "You Do Not Fully Utilize Transformer's Representation Capacity", "authors": ["Gleb Gerasimov", "Yaroslav Aksenov", "Nikita Balagansky", "Viacheslav Sinii", "Daniil Gavrilov"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In contrast to RNNs, which compress their history into a single hidden state,\nTransformers can attend to all past tokens directly. However, standard\nTransformers rely solely on the hidden state from the previous layer to\nrepresent the entire context. We show that this design choice induces\nrepresentation collapse and degrades performance. To address this issue, we\nintroduce Layer-Integrated Memory (LIMe), a lightweight extension that\nleverages existing key-value buffers and learns per-head, per-layer routing\nweights to integrate representations from all previous layers with negligible\noverhead. Through extensive experiments-including language modeling, synthetic\nreasoning benchmarks, and very deep architectures-LIMe consistently achieves\nfaster convergence, lower perplexity per FLOP, and substantial accuracy\nimprovements on synthetic tasks while preserving higher value-vector entropy\nand improved token separability. Finally, our analysis of the learned routing\nweights reveals systematic reuse of both local and long-distance features,\ndemonstrating how LIMe mitigates collapse, unlocks richer representations\nwithout increasing hidden-state size, and points to promising directions for\nfuture research."}
{"id": "2502.09767", "pdf": "https://arxiv.org/pdf/2502.09767.pdf", "abs": "https://arxiv.org/abs/2502.09767", "title": "Non-Markovian Discrete Diffusion with Causal Language Models", "authors": ["Yangtian Zhang", "Sizhuang He", "Daniel Levine", "Lawrence Zhao", "David Zhang", "Syed A Rizvi", "Emanuele Zappala", "Rex Ying", "David van Dijk"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Under Review", "summary": "Discrete diffusion models offer a flexible, controllable approach to\nstructured sequence generation, yet they still lag behind causal language\nmodels in expressive power. A key limitation lies in their reliance on the\nMarkovian assumption, which restricts each step to condition only on the\ncurrent state, leading to potential uncorrectable error accumulation. In this\npaper, we introduce CaDDi, a discrete diffusion model that conditions on the\nentire generative trajectory, thereby lifting the Markov constraint and\nallowing the model to revisit and improve past states. By unifying sequential\n(causal) and temporal (diffusion) reasoning in a single non-Markovian\ntransformer, CaDDi also treats standard causal language models as a special\ncase and permits the direct reuse of pretrained LLM weights with no\narchitectural changes. Empirically, CaDDi outperforms state-of-the-art discrete\ndiffusion baselines on natural-language benchmarks, substantially narrowing the\nremaining gap to large autoregressive transformers."}
{"id": "2502.09863", "pdf": "https://arxiv.org/pdf/2502.09863.pdf", "abs": "https://arxiv.org/abs/2502.09863", "title": "Closed-Form Training Dynamics Reveal Learned Features and Linear Structure in Word2Vec-like Models", "authors": ["Dhruva Karkada", "James B. Simon", "Yasaman Bahri", "Michael R. DeWeese"], "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "23 pages, 6 figures", "summary": "Self-supervised word embedding algorithms such as word2vec provide a minimal\nsetting for studying representation learning in language modeling. We examine\nthe quartic Taylor approximation of the word2vec loss around the origin, and we\nshow that both the resulting training dynamics and the final performance on\ndownstream tasks are empirically very similar to those of word2vec. Our main\ncontribution is to analytically solve for both the gradient flow training\ndynamics and the final word embeddings in terms of only the corpus statistics\nand training hyperparameters. The solutions reveal that these models learn\northogonal linear subspaces one at a time, each one incrementing the effective\nrank of the embeddings until model capacity is saturated. Training on\nWikipedia, we find that each of the top linear subspaces represents an\ninterpretable topic-level concept. Finally, we apply our theory to describe how\nlinear representations of more abstract semantic concepts emerge during\ntraining; these can be used to complete analogies via vector addition."}
{"id": "2502.11882", "pdf": "https://arxiv.org/pdf/2502.11882.pdf", "abs": "https://arxiv.org/abs/2502.11882", "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration", "authors": ["Shao Zhang", "Xihuai Wang", "Wenhao Zhang", "Chaoran Li", "Junru Song", "Tingyu Li", "Lin Qiu", "Xuezhi Cao", "Xunliang Cai", "Wen Yao", "Weinan Zhang", "Xinbing Wang", "Ying Wen"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.MA"], "comment": "Accepted by ACL 2025 Main. Camera Ready Version", "summary": "Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. DPT-Agent can effectively\nhelp LLMs convert correct slow thinking and reasoning into executable actions,\nthereby improving performance. To the best of our knowledge, DPT-Agent is the\nfirst language agent framework that achieves successful real-time simultaneous\nhuman-AI collaboration autonomously. Code of DPT-Agent can be found in\nhttps://github.com/sjtu-marl/DPT-Agent."}
{"id": "2502.12170", "pdf": "https://arxiv.org/pdf/2502.12170.pdf", "abs": "https://arxiv.org/abs/2502.12170", "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections", "authors": ["Da Xiao", "Qingye Meng", "Shengping Li", "Xingyuan Yuan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML'25)", "summary": "We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective\nmethod to address the limitations of residual connections and enhance\ncross-layer information flow in Transformers. Unlike existing dense connection\napproaches with static and shared connection weights, MUDD generates connection\nweights dynamically depending on hidden states at each sequence position and\nfor each decoupled input stream (the query, key, value or residual) of a\nTransformer block. MUDD connections can be seamlessly integrated into any\nTransformer architecture to create MUDDFormer. Extensive experiments show that\nMUDDFormer significantly outperforms Transformers across various model\narchitectures and scales in language modeling, achieving the performance of\nTransformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches\nPythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B\nin five-shot settings, while adding only 0.23% parameters and 0.4% computation.\nCode in JAX and PyTorch and pre-trained models are available at\nhttps://github.com/Caiyun-AI/MUDDFormer ."}
{"id": "2502.20689", "pdf": "https://arxiv.org/pdf/2502.20689.pdf", "abs": "https://arxiv.org/abs/2502.20689", "title": "WiseMind: Recontextualizing AI with a Knowledge-Guided, Theory-Informed Multi-Agent Framework for Instrumental and Humanistic Benefits", "authors": ["Yuqi Wu", "Guangya Wan", "Jingjing Li", "Shengming Zhao", "Lingfeng Ma", "Tianyi Ye", "Ion Pop", "Yanbo Zhang", "Jie Chen"], "categories": ["cs.AI", "cs.CL"], "comment": "27 pages, 13 figures", "summary": "Translating state-of-the-art NLP into practice often stalls at the \"last\nmile\" owing to insufficient contextualization of the target domain's knowledge,\nprocesses, and evaluation. Psychiatric differential diagnosis exemplifies this\nchallenge: accurate assessments depend on nuanced clinical knowledge, a\ndelicate cognitive-affective interview process, and downstream outcomes that\nextend far beyond benchmark accuracy. We present WiseMind, a systematic\ninterdisciplinary contextualization framework that delivers both instrumental\n(diagnostic precision) and humanistic (empathy) gains. WiseMind comprises three\ncomponents:(i) structured knowledge-guided proactive reasoning, which embeds\nDSM-5 criteria in a knowledge graph to steer questioning; (ii) a\ntheory-informed dual-agent architecture that coordinates a \"reasonable-mind\"\nreasoning agent and an \"emotional-mind\" empathy agent, inspired by Dialectical\nBehavior Therapy; and (iii) a multi-faceted evaluation strategy covering\nsimulated patients, user studies, clinician review, and ethical assessment.\nTested on depression, anxiety, and bipolar disorder, WiseMind attains up to\n84.2% diagnostic accuracy, which is comparable to human experts, while\noutperforming single-agent baselines in perceived empathy and trustworthiness.\nThese results show that deep contextualization-across knowledge, process, and\nevaluation layers-can transform benchmark-driven NLP into clinically meaningful\nimpact."}
{"id": "2503.04992", "pdf": "https://arxiv.org/pdf/2503.04992.pdf", "abs": "https://arxiv.org/abs/2503.04992", "title": "Wanda++: Pruning Large Language Models via Regional Gradients", "authors": ["Yifan Yang", "Kai Zhen", "Bhavana Ganesh", "Aram Galstyan", "Goeric Huybrechts", "Markus Müller", "Jonas M. Kübler", "Rupak Vignesh Swaminathan", "Athanasios Mouchtaris", "Sravan Babu Bodapati", "Nathan Susanj", "Zheng Zhang", "Jack FitzGerald", "Abhishek Kumar"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Paper accepted at ACL 2025 Findings", "summary": "Large Language Models (LLMs) pruning seeks to remove unimportant weights for\ninference speedup with minimal accuracy impact. However, existing methods often\nsuffer from accuracy degradation without full-model sparsity-aware fine-tuning.\nThis paper presents Wanda++, a novel pruning framework that outperforms the\nstate-of-the-art methods by utilizing decoder-block-level \\textbf{regional}\ngradients. Specifically, Wanda++ improves the pruning score with regional\ngradients for the first time and proposes an efficient regional optimization\nmethod to minimize pruning-induced output discrepancies between the dense and\nsparse decoder output. Notably, Wanda++ improves perplexity by up to 32\\% over\nWanda in the language modeling task and generalizes effectively to downstream\ntasks. Moreover, despite updating weights with regional optimization, Wanda++\nremains orthogonal to sparsity-aware fine-tuning, further reducing perplexity\nwith LoRA in great extend. Our approach is lightweight, pruning a 7B LLaMA\nmodel in under 10 minutes on a single H100 GPU."}
{"id": "2503.07265", "pdf": "https://arxiv.org/pdf/2503.07265.pdf", "abs": "https://arxiv.org/abs/2503.07265", "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation", "authors": ["Yuwei Niu", "Munan Ning", "Mengren Zheng", "Weiyang Jin", "Bin Lin", "Peng Jin", "Jiaqi Liao", "Chaoran Feng", "Kunpeng Ning", "Bin Zhu", "Li Yuan"], "categories": ["cs.CV", "cs.AI", "cs.CL", "I.2.7; I.2.10; I.4.9"], "comment": "Code, data and leaderboard: https://github.com/PKU-YuanGroup/WISE", "summary": "Text-to-Image (T2I) models are capable of generating high-quality artistic\ncreations and visual content. However, existing research and evaluation\nstandards predominantly focus on image realism and shallow text-image\nalignment, lacking a comprehensive assessment of complex semantic understanding\nand world knowledge integration in text to image generation. To address this\nchallenge, we propose $\\textbf{WISE}$, the first benchmark specifically\ndesigned for $\\textbf{W}$orld Knowledge-$\\textbf{I}$nformed $\\textbf{S}$emantic\n$\\textbf{E}$valuation. WISE moves beyond simple word-pixel mapping by\nchallenging models with 1000 meticulously crafted prompts across 25 sub-domains\nin cultural common sense, spatio-temporal reasoning, and natural science. To\novercome the limitations of traditional CLIP metric, we introduce\n$\\textbf{WiScore}$, a novel quantitative metric for assessing knowledge-image\nalignment. Through comprehensive testing of 20 models (10 dedicated T2I models\nand 10 unified multimodal models) using 1,000 structured prompts spanning 25\nsubdomains, our findings reveal significant limitations in their ability to\neffectively integrate and apply world knowledge during image generation,\nhighlighting critical pathways for enhancing knowledge incorporation and\napplication in next-generation T2I models. Code and data are available at\nhttps://github.com/PKU-YuanGroup/WISE."}
{"id": "2503.10619", "pdf": "https://arxiv.org/pdf/2503.10619.pdf", "abs": "https://arxiv.org/abs/2503.10619", "title": "Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search", "authors": ["Andy Zhou", "Ron Arel"], "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": "Accepted to ACL 2025 Main", "summary": "We introduce Tempest, a multi-turn adversarial framework that models the\ngradual erosion of Large Language Model (LLM) safety through a tree search\nperspective. Unlike single-turn jailbreaks that rely on one meticulously\nengineered prompt, Tempest expands the conversation at each turn in a\nbreadth-first fashion, branching out multiple adversarial prompts that exploit\npartial compliance from previous responses. By tracking these incremental\npolicy leaks and re-injecting them into subsequent queries, Tempest reveals how\nminor concessions can accumulate into fully disallowed outputs. Evaluations on\nthe JailbreakBench dataset show that Tempest achieves a 100% success rate on\nGPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries\nthan baselines such as Crescendo or GOAT. This tree search methodology offers\nan in-depth view of how model safeguards degrade over successive dialogue\nturns, underscoring the urgency of robust multi-turn testing procedures for\nlanguage models."}
{"id": "2504.13904", "pdf": "https://arxiv.org/pdf/2504.13904.pdf", "abs": "https://arxiv.org/abs/2504.13904", "title": "Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Yuewen Sun", "Xinshuai Dong", "Kazushi Ikeda", "Peter Spirtes", "Kun Zhang"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "12 pages, 10 figures, 1 table. Accepted by ACM UMAP 2025", "summary": "We hypothesize that optimal system responses emerge from adaptive strategies\ngrounded in causal and counterfactual knowledge. Counterfactual inference\nallows us to create hypothetical scenarios to examine the effects of\nalternative system responses. We enhance this process through causal discovery,\nwhich identifies the strategies informed by the underlying causal structure\nthat govern system behaviors. Moreover, we consider the psychological\nconstructs and unobservable noises that might be influencing user-system\ninteractions as latent factors. We show that these factors can be effectively\nestimated. We employ causal discovery to identify strategy-level causal\nrelationships among user and system utterances, guiding the generation of\npersonalized counterfactual dialogues. We model the user utterance strategies\nas causal factors, enabling system strategies to be treated as counterfactual\nactions. Furthermore, we optimize policies for selecting system responses based\non counterfactual data. Our results using a real-world dataset on social good\ndemonstrate significant improvements in persuasive system outcomes, with\nincreased cumulative rewards validating the efficacy of causal discovery in\nguiding personalized counterfactual inference and optimizing dialogue policies\nfor a persuasive dialogue system."}
{"id": "2504.15585", "pdf": "https://arxiv.org/pdf/2504.15585.pdf", "abs": "https://arxiv.org/abs/2504.15585", "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Junyuan Mao", "Yu Wang", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Wenjie Qu", "Yue Liu", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Zhaoxin Fan", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Yingxin Lai", "Zitong Yu", "Xinfeng Li", "Yifan Jiang", "Yanhui Li", "Xinyu Deng", "Junlin Wu", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Qiufeng Wang", "Wenxuan Wang", "Dongrui Liu", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Heng Chang", "Tianlin Li", "Yi Yu", "Chenghao Li", "Jiawei Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Jiaheng Zhang", "Tianwei Zhang", "Xingjun Ma", "Jindong Gu", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Lingjuan Lyu", "Yuval Elovici", "Bhavya Kailkhura", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field."}
{"id": "2505.03414", "pdf": "https://arxiv.org/pdf/2505.03414.pdf", "abs": "https://arxiv.org/abs/2505.03414", "title": "Enhancing Target-unspecific Tasks through a Features Matrix", "authors": ["Fangming Cui", "Yonggang Zhang", "Xuan Wang", "Xinmei Tian", "Jun Yu"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICML 2025", "summary": "Recent developments in prompt learning of large Vision-Language Models (VLMs)\nhave significantly improved performance in target-specific tasks. However,\nthese prompting methods often struggle to tackle the target-unspecific or\ngeneralizable tasks effectively. It may be attributed to the fact that\noverfitting training causes the model to forget its general knowledge. The\ngeneral knowledge has a strong promotion on target-unspecific tasks. To\nalleviate this issue, we propose a novel Features Matrix (FM) approach designed\nto enhance these models on target-unspecific tasks. Our method extracts and\nleverages general knowledge, shaping a Features Matrix (FM). Specifically, the\nFM captures the semantics of diverse inputs from a deep and fine perspective,\npreserving essential general knowledge, which mitigates the risk of\noverfitting. Representative evaluations demonstrate that: 1) the FM is\ncompatible with existing frameworks as a generic and flexible module, and 2)\nthe FM significantly showcases its effectiveness in enhancing target-unspecific\ntasks (base-to-novel generalization, domain generalization, and cross-dataset\ngeneralization), achieving state-of-the-art performance."}
{"id": "2505.04364", "pdf": "https://arxiv.org/pdf/2505.04364.pdf", "abs": "https://arxiv.org/abs/2505.04364", "title": "Benchmarking LLMs' Swarm intelligence", "authors": ["Kai Ruan", "Mowen Huang", "Ji-Rong Wen", "Hao Sun"], "categories": ["cs.MA", "cs.CL"], "comment": "added new ref", "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict swarm-like constraints-limited local perception and\ncommunication-remains largely unexplored. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination when agents\noperate with incomplete spatio-temporal information. To bridge this gap, we\nintroduce SwarmBench, a novel benchmark designed to systematically evaluate the\nswarm intelligence capabilities of LLMs acting as decentralized agents.\nSwarmBench features five foundational MAS coordination tasks (Pursuit,\nSynchronization, Foraging, Flocking, Transport) within a configurable 2D grid\nenvironment, forcing agents to rely solely on local sensory input ($k\\times k$\nview) and local communication. We propose metrics for coordination\neffectiveness and analyze emergent group dynamics. Zero-shot evaluations of\nleading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent\nperformance variations. While some rudimentary coordination is observed, our\nresults indicate that current LLMs significantly struggle with robust\nlong-range planning and adaptive strategy formation under the uncertainty\ninherent in these decentralized scenarios. Assessing LLMs under such swarm-like\nconstraints is crucial for understanding their utility in future decentralized\nintelligent systems. We release SwarmBench as an open, extensible toolkit-built\non a customizable physical system-providing environments, prompts, evaluation\nscripts, and comprehensive datasets. This aims to foster reproducible research\ninto LLM-based MAS coordination and the theoretical underpinnings of emergent\ncollective behavior under severe informational decentralization. Our code\nrepository is available at https://github.com/x66ccff/swarmbench."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312.pdf", "abs": "https://arxiv.org/abs/2505.12312", "title": "Visuospatial Cognitive Assistant", "authors": ["Qi Feng"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "Author list corrected. In version 1, Hidetoshi Shimodaira was\n  included as a co-author without their consent and has been removed from the\n  author list", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363.pdf", "abs": "https://arxiv.org/abs/2505.12363", "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "authors": ["Qi Feng"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "In version 1, Hidetoshi Shimodaira was included as a co-author\n  without their consent and has been removed from the author list", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.16832", "pdf": "https://arxiv.org/pdf/2505.16832.pdf", "abs": "https://arxiv.org/abs/2505.16832", "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Reasoning-Driven Pedagogical Visualization", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Dake Zhang", "Hongyi Wang", "Huaxiu Yao"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "16 pages; 7 figures", "summary": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent."}
{"id": "2505.16849", "pdf": "https://arxiv.org/pdf/2505.16849.pdf", "abs": "https://arxiv.org/abs/2505.16849", "title": "Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented Generation via Knowledge Graph Walks", "authors": ["Martin Böckling", "Heiko Paulheim", "Andreea Iana"], "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "comment": "Accepted at the Information Retrieval's Role in RAG Systems (IR-RAG\n  2025) in conjunction with SIGIR 2025", "summary": "Large Language Models (LLMs) have showcased impressive reasoning abilities,\nbut often suffer from hallucinations or outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by\ngrounding LLM responses in structured external information from a knowledge\nbase. However, many KG-based RAG approaches struggle with (i) aligning KG and\ntextual representations, (ii) balancing retrieval accuracy and efficiency, and\n(iii) adapting to dynamically updated KGs. In this work, we introduce\nWalk&Retrieve, a simple yet effective KG-based framework that leverages\nwalk-based graph traversal and knowledge verbalization for corpus generation\nfor zero-shot RAG. Built around efficient KG walks, our method does not require\nfine-tuning on domain-specific data, enabling seamless adaptation to KG\nupdates, reducing computational overhead, and allowing integration with any\noff-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs\ncompetitively, often outperforming existing RAG systems in response accuracy\nand hallucination reduction. Moreover, it demonstrates lower query latency and\nrobust scalability to large KGs, highlighting the potential of lightweight\nretrieval strategies as strong baselines for future RAG research."}
{"id": "2505.17928", "pdf": "https://arxiv.org/pdf/2505.17928.pdf", "abs": "https://arxiv.org/abs/2505.17928", "title": "Towards Practical Defect-Focused Automated Code Review", "authors": ["Junyi Lu", "Lili Jiang", "Xiaojia Li", "Jianbing Fang", "Fengjun Zhang", "Li Yang", "Chun Zuo"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted as Spotlight at the 42nd International Conference on Machine\n  Learning (ICML 2025)", "summary": "The complexity of code reviews has driven efforts to automate review\ncomments, but prior approaches oversimplify this task by treating it as\nsnippet-level code-to-text generation and relying on text similarity metrics\nlike BLEU for evaluation. These methods overlook repository context, real-world\nmerge request evaluation, and defect detection, limiting their practicality. To\naddress these issues, we explore the full automation pipeline within the online\nrecommendation service of a company with nearly 400 million daily active users,\nanalyzing industry-grade C++ codebases comprising hundreds of thousands of\nlines of code. We identify four key challenges: 1) capturing relevant context,\n2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and\n4) integrating human workflows. To tackle these, we propose 1) code slicing\nalgorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a\nfiltering mechanism for FAR reduction, and 4) a novel prompt design for better\nhuman interaction. Our approach, validated on real-world merge requests from\nhistorical fault reports, achieves a 2x improvement over standard LLMs and a\n10x gain over previous baselines. While the presented results focus on C++, the\nunderlying framework design leverages language-agnostic principles (e.g.,\nAST-based analysis), suggesting potential for broader applicability."}
{"id": "2505.18079", "pdf": "https://arxiv.org/pdf/2505.18079.pdf", "abs": "https://arxiv.org/abs/2505.18079", "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding", "authors": ["Xiaoyi Zhang", "Zhaoyang Jia", "Zongyu Guo", "Jiahao Li", "Bin Li", "Houqiang Li", "Yan Lu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "V2 draft. Under review", "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later."}
{"id": "2505.18116", "pdf": "https://arxiv.org/pdf/2505.18116.pdf", "abs": "https://arxiv.org/abs/2505.18116", "title": "Bridging Supervised Learning and Reinforcement Learning in Math Reasoning", "authors": ["Huayu Chen", "Kaiwen Zheng", "Qinsheng Zhang", "Ganqu Cui", "Yin Cui", "Haotian Ye", "Tsung-Yi Lin", "Ming-Yu Liu", "Jun Zhu", "Haoxiang Wang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems."}
{"id": "2505.19641", "pdf": "https://arxiv.org/pdf/2505.19641.pdf", "abs": "https://arxiv.org/abs/2505.19641", "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "authors": ["Junteng Liu", "Yuanxiang Fan", "Zhuo Jiang", "Han Ding", "Yongyi Hu", "Chi Zhang", "Yiqi Shi", "Shitong Weng", "Aili Chen", "Shiqi Chen", "Yunan Huang", "Mozhi Zhang", "Pengyu Zhao", "Junjie Yan", "Junxian He"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."}
{"id": "2505.21277", "pdf": "https://arxiv.org/pdf/2505.21277.pdf", "abs": "https://arxiv.org/abs/2505.21277", "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space", "authors": ["Yao Huang", "Yitong Sun", "Shouwei Ruan", "Yichi Zhang", "Yinpeng Dong", "Xingxing Wei"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "19 pages, 20 figures, accepted by ACL 2025, Findings", "summary": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO."}
{"id": "2505.21329", "pdf": "https://arxiv.org/pdf/2505.21329.pdf", "abs": "https://arxiv.org/abs/2505.21329", "title": "Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks", "authors": ["Allaa Boutaleb", "Bernd Amann", "Hubert Naacke", "Rafael Angarita"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "comment": "Accepted @ ACL 2025's Table Representation Learning Workshop (TRL)", "summary": "Recent table representation learning and data discovery methods tackle table\nunion search (TUS) within data lakes, which involves identifying tables that\ncan be unioned with a given query table to enrich its content. These methods\nare commonly evaluated using benchmarks that aim to assess semantic\nunderstanding in real-world TUS tasks. However, our analysis of prominent TUS\nbenchmarks reveals several limitations that allow simple baselines to perform\nsurprisingly well, often outperforming more sophisticated approaches. This\nsuggests that current benchmark scores are heavily influenced by\ndataset-specific characteristics and fail to effectively isolate the gains from\nsemantic understanding. To address this, we propose essential criteria for\nfuture benchmarks to enable a more realistic and reliable evaluation of\nprogress in semantic table union search."}
