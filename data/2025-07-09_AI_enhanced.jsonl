{"id": "2507.05261", "pdf": "https://arxiv.org/pdf/2507.05261.pdf", "abs": "https://arxiv.org/abs/2507.05261", "title": "TokenShapley: Token Level Context Attribution with Shapley Value", "authors": ["Yingtai Xiao", "Yuqing Zhu", "Sirat Samyoun", "Wanrong Zhang", "Jiachen T. Wang", "Jian Du"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) demonstrate strong capabilities in in-context\nlearning, but verifying the correctness of their generated responses remains a\nchallenge. Prior work has explored attribution at the sentence level, but these\nmethods fall short when users seek attribution for specific keywords within the\nresponse, such as numbers, years, or names. To address this limitation, we\npropose TokenShapley, a novel token-level attribution method that combines\nShapley value-based data attribution with KNN-based retrieval techniques\ninspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed\ndatastore for contextual retrieval and computing Shapley values to quantify\ntoken importance, TokenShapley provides a fine-grained data attribution\napproach. Extensive evaluations on four benchmarks show that TokenShapley\noutperforms state-of-the-art baselines in token-level attribution, achieving an\n11-23% improvement in accuracy.", "AI": {"tldr": "TokenShapley is a new method for token-level attribution in LLMs that improves the verification of generated responses.", "motivation": "The motivation behind TokenShapley is to enhance the verification of specific tokens in LLM responses, addressing limitations of existing methods that only provide sentence-level attribution.", "method": "TokenShapley integrates Shapley value-based data attribution with KNN-based retrieval techniques to assess token importance using a precomputed datastore.", "result": "Extensive evaluations demonstrate that TokenShapley outperforms state-of-the-art baselines, achieving an 11-23% improvement in accuracy for token-level attribution.", "conclusion": "TokenShapley allows for a fine-grained understanding of token contributions to LLM responses, improving overall response verification.", "key_contributions": ["Introduction of TokenShapley for token-level attribution", "Combination of Shapley values and KNN-based retrieval", "Demonstrated substantial improvements in attribution accuracy"], "limitations": "", "keywords": ["token-level attribution", "Shapley value", "KNN retrieval", "LLMs", "data attribution"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.05266", "pdf": "https://arxiv.org/pdf/2507.05266.pdf", "abs": "https://arxiv.org/abs/2507.05266", "title": "User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs", "authors": ["Sougata Saha", "Monojit Choudhury"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Measuring the generalization ability of Large Language Models (LLMs) is\nchallenging due to data contamination. As models grow and computation becomes\ncheaper, ensuring tasks and test cases are unseen during training phases will\nbecome nearly impossible. We argue that knowledge-retrieval and reasoning tasks\nare not ideal for measuring generalization, as LLMs are not trained for\nspecific tasks. Instead, we propose user behavior prediction, also a key aspect\nof personalization, as a theoretically sound, scalable, and robust alternative.\nWe introduce a novel framework for this approach and test it on movie and music\nrecommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.\nResults align with our framework's predictions, showing GPT-4o outperforms\nGPT-4o-mini and Llama, though all models have much room for improvement,\nespecially Llama.", "AI": {"tldr": "This paper proposes a novel framework for evaluating the generalization ability of Large Language Models (LLMs) through user behavior prediction instead of traditional knowledge-retrieval tasks, demonstrating its effectiveness using recommendation datasets.", "motivation": "The paper addresses the challenge of measuring the generalization ability of LLMs amidst data contamination and the limitations of current evaluation methods.", "method": "The authors propose user behavior prediction as a new method for assessing LLM performance and evaluate it using datasets related to movie and music recommendations.", "result": "The results indicate that GPT-4o outperforms its smaller counterpart GPT-4o-mini and Llama-3.1-8B-Instruct across the tested datasets, while all models show significant potential for further improvements.", "conclusion": "User behavior prediction is presented as a more robust and scalable alternative for evaluating LLM generalization ability, proposing a shift in focus from knowledge-retrieval tasks.", "key_contributions": ["Proposed user behavior prediction as a novel approach for evaluating LLMs", "Introduced a framework for the new evaluation method", "Demonstrated effectiveness using movie and music recommendation datasets"], "limitations": "The models, including GPT-4o, GPT-4o-mini, and Llama, still exhibit room for improvement in their performance.", "keywords": ["Large Language Models", "generalization ability", "user behavior prediction", "personalization", "recommendation systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.05271", "pdf": "https://arxiv.org/pdf/2507.05271.pdf", "abs": "https://arxiv.org/abs/2507.05271", "title": "An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks", "authors": ["Mohammad Zia Ur Rehman", "Aditya Shah", "Nagendra Kumar"], "categories": ["cs.CL"], "comment": null, "summary": "The global reach of social media has amplified the spread of hateful content,\nincluding implicit sexism, which is often overlooked by conventional detection\nmethods. In this work, we introduce an Adaptive Supervised Contrastive lEarning\nframework for implicit sexism detectioN (ASCEND). A key innovation of our\nmethod is the incorporation of threshold-based contrastive learning: by\ncomputing cosine similarities between embeddings, we selectively treat only\nthose sample pairs as positive if their similarity exceeds a learnable\nthreshold. This mechanism refines the embedding space by robustly pulling\ntogether representations of semantically similar texts while pushing apart\ndissimilar ones, thus reducing false positives and negatives. The final\nclassification is achieved by jointly optimizing a contrastive loss with a\ncross-entropy loss. Textual features are enhanced through a word-level\nattention module. Additionally, we employ sentiment, emotion, and toxicity\nfeatures. Evaluations on the EXIST2021 and MLSC datasets demonstrate that\nASCEND significantly outperforms existing methods, with average Macro F1\nimprovements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting\nits efficacy in capturing the subtle cues of implicit sexist language.", "AI": {"tldr": "This paper introduces ASCEND, an adaptive contrastive learning framework for detecting implicit sexism in social media content.", "motivation": "To address the overlooked implicit sexism in social media that conventional detection methods fail to recognize.", "method": "The ASCEND framework uses threshold-based contrastive learning to refine the embedding space of textual data, optimizing a contrastive loss alongside cross-entropy loss.", "result": "ASCEND achieved significant performance improvements, outperforming existing methods with average Macro F1 improvements of 9.86%, 29.63%, and 32.51% on the EXIST2021 and MLSC datasets.", "conclusion": "The integration of a word-level attention module and additional textual features enhances the detection of implicit sexism effectively.", "key_contributions": ["Introduction of the Adaptive Supervised Contrastive learning framework for implicit sexism detection.", "Use of threshold-based contrastive learning for refining textual embeddings.", "Improvement of detection performance as evidenced by quantitative evaluations on benchmark datasets."], "limitations": "", "keywords": ["implicit sexism", "contrastive learning", "social media detection"], "importance_score": 4, "read_time_minutes": 6}}
{"id": "2507.05285", "pdf": "https://arxiv.org/pdf/2507.05285.pdf", "abs": "https://arxiv.org/abs/2507.05285", "title": "Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion", "authors": ["Miloud Mihoubi", "Meriem Zerkouk", "Belkacem Chikhaoui"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "I.2.7; I.2.1; K.3.1"], "comment": "10 pages, 5 figures, 5 tables. Submitted to the 38th Canadian\n  Conference on Artificial Intelligence (Canadian AI 2025)", "summary": "Student dropout in distance learning remains a critical challenge, with\nprofound societal and economic consequences. While classical machine learning\nmodels leverage structured socio-demographic and behavioral data, they often\nfail to capture the nuanced emotional and contextual factors embedded in\nunstructured student interactions. This paper introduces a transformative AI\nframework that redefines dropout prediction through three synergistic\ninnovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment\nanalysis, prompt engineering to decode academic stressors, and cross-modal\nattention fusion to dynamically align textual, behavioral, and\nsocio-demographic insights. By grounding sentiment analysis in a curated\nknowledge base of pedagogical content, our RAG-enhanced BERT model interprets\nstudent comments with unprecedented contextual relevance, while optimized\nprompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload\nanxiety\"). A cross-modal attention layer then fuses these insights with\ntemporal engagement patterns, creating holistic risk profiles. Evaluated on a\nlongitudinal dataset of 4 423 students, the framework achieves 89% accuracy and\nan F1-score of 0.88, outperforming conventional models by 7% and reducing false\nnegatives by 21%. Beyond prediction, the system generates interpretable\ninterventions by retrieving contextually aligned strategies (e.g., mentorship\nprograms for isolated learners). This work bridges the gap between predictive\nanalytics and actionable pedagogy, offering a scalable solution to mitigate\ndropout risks in global education systems", "AI": {"tldr": "A novel AI framework for dropout prediction in distance learning that integrates sentiment analysis, prompt engineering, and cross-modal attention to address student retention challenges effectively.", "motivation": "Student dropout in distance learning poses significant societal and economic challenges. Existing models fail to incorporate emotional and contextual factors influencing dropout risk.", "method": "The framework employs Retrieval-Augmented Generation (RAG) for sentiment analysis, utilizes prompt engineering to identify academic stressors, and implements cross-modal attention fusion to merge textual, behavioral, and demographic insights.", "result": "The framework reaches 89% accuracy and an F1-score of 0.88, outperforming traditional models by 7% and reducing false negatives by 21%.", "conclusion": "This work presents a scalable solution that connects predictive analytics with actionable pedagogy, helping mitigate dropout risks in education.", "key_contributions": ["Introduction of RAG for sentiment analysis in educational contexts.", "Development of a cross-modal attention mechanism for dropout prediction.", "Creation of interpretable interventions based on predicted risks."], "limitations": "", "keywords": ["dropout prediction", "machine learning", "sentiment analysis", "educational interventions", "cross-modal learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.05319", "pdf": "https://arxiv.org/pdf/2507.05319.pdf", "abs": "https://arxiv.org/abs/2507.05319", "title": "LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review", "authors": ["Cheng Yuan", "Xinkai Rui", "Yongqi Fan", "Yawei Fan", "Boyang Zhong", "Jiacheng Wang", "Weiyan Zhang", "Tong Ruan"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL Demo 2025", "summary": "Despite the remarkable performance of Large Language Models (LLMs) in\nautomated discharge summary generation, they still suffer from hallucination\nissues, such as generating inaccurate content or fabricating information\nwithout valid sources. In addition, electronic medical records (EMRs) typically\nconsist of long-form data, making it challenging for LLMs to attribute the\ngenerated content to the sources. To address these challenges, we propose LCDS,\na Logic-Controlled Discharge Summary generation system. LCDS constructs a\nsource mapping table by calculating textual similarity between EMRs and\ndischarge summaries to constrain the scope of summarized content. Moreover,\nLCDS incorporates a comprehensive set of logical rules, enabling it to generate\nmore reliable silver discharge summaries tailored to different clinical fields.\nFurthermore, LCDS supports source attribution for generated content, allowing\nexperts to efficiently review, provide feedback, and rectify errors. The\nresulting golden discharge summaries are subsequently recorded for incremental\nfine-tuning of LLMs. Our project and demo video are in the GitHub repository\nhttps://github.com/ycycyc02/LCDS.", "AI": {"tldr": "LCDS is a Logic-Controlled Discharge Summary generation system aimed at enhancing the reliability of automated discharge summaries generated by LLMs by addressing issues of hallucination and source attribution.", "motivation": "Large Language Models face challenges in generating accurate discharge summaries from electronic medical records due to hallucination issues and difficulties in source attribution.", "method": "LCDS develops a source mapping table based on textual similarity between EMRs and discharge summaries and uses logical rules to improve the reliability of the generated summaries.", "result": "LCDS generates reliable silver discharge summaries that are tailored to specific clinical fields and supports source attribution for expert feedback, leading to accurate golden summaries for fine-tuning of LLMs.", "conclusion": "LCDS offers a significant advancement in the generation of discharge summaries by addressing critical challenges through logical controls and content sourcing.", "key_contributions": ["Logic-controlled generation to reduce hallucinations", "Source mapping for content accuracy", "Support for expert feedback in summary generation"], "limitations": "", "keywords": ["Large Language Models", "discharge summary", "source attribution", "electronic medical records", "health informatics"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.05330", "pdf": "https://arxiv.org/pdf/2507.05330.pdf", "abs": "https://arxiv.org/abs/2507.05330", "title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents", "authors": ["Ming Gong", "Xucheng Huang", "Chenghan Yang", "Xianhan Peng", "Haoxin Wang", "Yang Liu", "Ling Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled new applications\nin e-commerce customer service. However, their capabilities remain constrained\nin complex, multimodal scenarios. We present MindFlow, the first open-source\nmultimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it\nintegrates memory, decision-making, and action modules, and adopts a modular\n\"MLLM-as-Tool\" strategy for effect visual-textual reasoning. Evaluated via\nonline A/B testing and simulation-based ablation, MindFlow demonstrates\nsubstantial gains in handling complex queries, improving user satisfaction, and\nreducing operational costs, with a 93.53% relative improvement observed in\nreal-world deployments.", "AI": {"tldr": "MindFlow is an open-source multimodal LLM agent designed for e-commerce, enhancing user satisfaction and efficiency in handling complex queries.", "motivation": "To address constraints of large language models in complex, multimodal e-commerce scenarios.", "method": "Developed on the CoALA framework, MindFlow integrates memory, decision-making, and action modules while employing a modular 'MLLM-as-Tool' strategy for effective visual-textual reasoning.", "result": "MindFlow shows substantial improvements in managing complex queries, enhancing user satisfaction, and decreasing operational costs, with a 93.53% improvement in real-world scenarios.", "conclusion": "MindFlow presents a novel approach to leveraging multimodal LLMs in e-commerce, leading to significant operational benefits.", "key_contributions": ["First open-source multimodal LLM agent for e-commerce", "Integration of memory and decision-making modules", "Demonstrated effectiveness through A/B testing and real-world deployment"], "limitations": "", "keywords": ["multimodal LLM", "e-commerce", "user satisfaction", "memory integration", "decision-making"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.05446", "pdf": "https://arxiv.org/pdf/2507.05446.pdf", "abs": "https://arxiv.org/abs/2507.05446", "title": "Esports and expertise: what competitive gaming can teach us about mastery", "authors": ["Ben Boudaoud", "Josef Spjut", "Joohwan Kim", "Arjun Madhusudan", "Benjamin Watson"], "categories": ["cs.HC"], "comment": null, "summary": "Historically, much research and development in human computer interaction has\nfocused on atomic and generalizable tasks, where task completion time indicates\nproductivity. However, the emergence of competitive games and esports reminds\nus of an alternative perspective on human performance in HCI: mastery of\nhigher-level, holistic practices. Just as a world-renowned artist is rarely\nevaluated for their individual brush strokes, so skilled competitive gamers\nrarely succeed solely by completing individual mouse movements or keystrokes as\nquickly as possible. Instead, they optimize more task-specific skills, adeptly\nperforming challenges deep in the learning curve for their game of choice.", "AI": {"tldr": "This paper explores a holistic approach to evaluating human performance in HCI, emphasizing mastery in complex tasks like competitive gaming rather than focusing solely on atomic task completion.", "motivation": "The paper argues that traditional measures of productivity in HCI, which emphasize individual task completion time, overlook the deeper aspects of human performance demonstrated in competitive gaming and esports.", "method": "The authors analyze performance metrics in esports, contrasting them with conventional HCI research focusing on generalizable task completion.", "result": "The study highlights that skilled gamers optimize specific, task-related skills over time, suggesting that HCI should adopt a similar focus on holistic task mastery.", "conclusion": "A shift in HCI research towards valuing holistic mastery in user performance can lead to richer evaluations of user interaction, particularly in complex domains like games.", "key_contributions": ["Proposes a new evaluation framework for HCI centered on mastery and holistic task performance.", "Draws parallels between artistic mastery and skills in competitive gaming in terms of performance evaluation.", "Advocates for expanding HCI research methodologies to include complex, task-specific skills."], "limitations": "", "keywords": ["human-computer interaction", "esports", "performance evaluation", "holistic mastery", "competitive gaming"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.05346", "pdf": "https://arxiv.org/pdf/2507.05346.pdf", "abs": "https://arxiv.org/abs/2507.05346", "title": "LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks", "authors": ["William Fleshman", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The proliferation of fine-tuned language model experts for specific tasks and\ndomains signals the need for efficient selection and combination methods. We\npropose LoRA-Augmented Generation (LAG) for leveraging large libraries of\nknowledge and task-specific LoRA adapters. LAG requires no additional training\nor access to data, and efficiently filters, retrieves, and applies experts on a\nper-token and layer basis. We evaluate LAG on various knowledge-intensive\ntasks, achieving superior performance over existing data-free methods. We\nexplore scenarios where additional data is available, demonstrating LAG's\ncompatibility with alternative solutions such as retrieval-augmented generation\n(RAG).", "AI": {"tldr": "The paper presents LoRA-Augmented Generation (LAG), a method for efficient selection and combination of fine-tuned language model experts without additional training, enhancing performance on knowledge-intensive tasks.", "motivation": "There is a growing need for efficient methods to select and combine fine-tuned language models tailored for specific tasks due to their increasing proliferation.", "method": "LAG utilizes task-specific LoRA adapters to filter, retrieve, and apply expert models on a per-token and layer basis without requiring additional training or access to data.", "result": "LAG shows superior performance on various knowledge-intensive tasks compared to existing data-free methods, and is compatible with retrieval-augmented generation in scenarios with additional data.", "conclusion": "The proposed method establishes a framework for effectively leveraging large libraries of language model experts, improving upon traditional approaches without the need for extensive data or retraining.", "key_contributions": ["Introduction of LoRA-Augmented Generation (LAG) method.", "Demonstration of LAG's efficiency in utilizing fine-tuned language model experts.", "Evaluation showing LAG's superiority in knowledge-intensive tasks over existing methods."], "limitations": "The method's performance may still depend on the quality of the fine-tuned models available.", "keywords": ["LoRA", "language models", "task-specific", "retrieval-augmented generation", "knowledge-intensive tasks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.05447", "pdf": "https://arxiv.org/pdf/2507.05447.pdf", "abs": "https://arxiv.org/abs/2507.05447", "title": "NRXR-ID: Two-Factor Authentication (2FA) in VR Using Near-Range Extended Reality and Smartphones", "authors": ["Aiur Nanzatov", "Lourdes Peña-Castillo", "Oscar Meruvia-Pastor"], "categories": ["cs.HC", "cs.CV", "cs.GR"], "comment": null, "summary": "Two-factor authentication (2FA) has become widely adopted as an efficient and\nsecure way to validate someone's identity online. Two-factor authentication is\ndifficult in virtual reality (VR) because users are usually wearing a\nhead-mounted display (HMD) which does not allow them to see their real-world\nsurroundings. We present NRXR-ID, a technique to implement two-factor\nauthentication while using extended reality systems and smartphones. The\nproposed method allows users to complete an authentication challenge using\ntheir smartphones without removing their HMD. We performed a user study where\nwe explored four types of challenges for users, including a novel\ncheckers-style challenge. Users responded to these challenges under three\ndifferent configurations, including a technique that uses the smartphone to\nsupport gaze-based selection without the use of VR controllers. A 4X3\nwithin-subjects design allowed us to study all the variations proposed. We\ncollected performance metrics and performed user experience questionnaires to\ncollect subjective impressions from 30 participants. Results suggest that the\ncheckers-style visual matching challenge was the most appropriate option,\nfollowed by entering a digital PIN challenge submitted via the smartphone and\nanswered within the VR environment.", "AI": {"tldr": "NRXR-ID is a technique for two-factor authentication in extended reality that allows users to authenticate using smartphones without removing their head-mounted displays (HMD).", "motivation": "To address the challenge of implementing two-factor authentication in virtual reality environments where users cannot see their real-world surroundings due to head-mounted displays.", "method": "We conducted a user study exploring four types of authentication challenges, including a novel checkers-style challenge, with participants responding under three different configurations, including gaze-based selection using smartphones.", "result": "The study involved 30 participants and found that the checkers-style visual matching challenge was the most effective, followed by the PIN entry challenge performed via smartphone in the VR environment.", "conclusion": "The NRXR-ID technique presents an effective way to implement two-factor authentication in VR, enhancing user experience and security without requiring removal of HMDs.", "key_contributions": ["Introduction of NRXR-ID for 2FA in VR", "Evaluation of multiple authentication challenge types in a user study", "Identification of the most effective authentication challenge format for VR"], "limitations": "", "keywords": ["two-factor authentication", "virtual reality", "extended reality", "user study", "authenticating challenges"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2507.05362", "pdf": "https://arxiv.org/pdf/2507.05362.pdf", "abs": "https://arxiv.org/abs/2507.05362", "title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study", "authors": ["Riccardo Alberghi", "Elizaveta Demyanenko", "Luca Biggio", "Luca Saglietti"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in natural language processing highlight two key factors for\nimproving reasoning in large language models (LLMs): (i) allocating more\ntest-time compute tends to help on harder problems but often introduces\nredundancy in the reasoning trace, and (ii) compute is most effective when\nreasoning is systematic and incremental, forming structured chains of thought\n(CoTs) akin to human problem-solving. To study these factors in isolation, we\nintroduce a controlled setting based on shortest-path tasks in layered graphs.\nWe train decoder-only transformers on question-trace-answer triples using a\ncustom tokenizer, comparing models trained on optimal bottom-up dynamic\nprogramming traces with those trained on longer, valid traces involving\nbacktracking. Surprisingly, with the same training-token budget, models trained\non inefficient traces generalize better to unseen graphs. This benefit is not\ndue to length alone-injecting arbitrary redundancy into reasoning traces fails\nto help and can even hurt performance. Instead, we find that generalization\ncorrelates with the model's confidence in next-token prediction, suggesting\nthat long, coherent, and locally incremental traces make the training signal\neasier to optimize.", "AI": {"tldr": "Study on improving reasoning in large language models (LLMs) through systematic and structured chains of thought while analyzing the impact of compute allocation during testing.", "motivation": "To address the key factors influencing reasoning improvement in LLMs, specifically the effects of test-time compute allocation and structured reasoning.", "method": "Implemented a controlled setting using shortest-path tasks in layered graphs, training decoder-only transformers on various traces from dynamic programming tasks, comparing those with backtracking and optimal traces.", "result": "Models trained on inefficient reasoning traces (longer valid backtracking traces) demonstrated better generalization to unseen graphs compared to optimal traces with the same training budget.", "conclusion": "The length of reasoning traces alone does not ensure better performance; rather, coherent and incrementally structured reasoning is beneficial for optimizing training signals for LLMs.", "key_contributions": ["Introduction of a controlled experimental setting for studying reasoning in LLMs.", "Empirical evidence that longer incoherent reasoning does not improve generalization and can hinder performance.", "Identification of model confidence in next-token prediction as a critical factor for effective training signals."], "limitations": "", "keywords": ["Large Language Models", "Reasoning", "Machine Learning", "Natural Language Processing", "Generalization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.05461", "pdf": "https://arxiv.org/pdf/2507.05461.pdf", "abs": "https://arxiv.org/abs/2507.05461", "title": "GLOSS: Group of LLMs for Open-Ended Sensemaking of Passive Sensing Data for Health and Wellbeing", "authors": ["Akshat Choube", "Ha Le", "Jiachen Li", "Kaixin Ji", "Vedant Das Swain", "Varun Mishra"], "categories": ["cs.HC"], "comment": null, "summary": "The ubiquitous presence of smartphones and wearables has enabled researchers\nto build prediction and detection models for various health and behavior\noutcomes using passive sensing data from these devices. Achieving a high-level,\nholistic understanding of an individual's behavior and context, however,\nremains a significant challenge. Due to the nature of passive sensing data,\nsensemaking -- the process of interpreting and extracting insights -- requires\nboth domain knowledge and technical expertise, creating barriers for different\nstakeholders. Existing systems designed to support sensemaking are either not\nopen-ended or cannot perform complex data triangulation. In this paper, we\npresent a novel sensemaking system, Group of LLMs for Open-ended Sensemaking\n(GLOSS), capable of open-ended sensemaking and performing complex multimodal\ntriangulation to derive insights. We demonstrate that GLOSS significantly\noutperforms the commonly used Retrieval-Augmented Generation (RAG) technique,\nachieving 87.93% accuracy and 66.19% consistency, compared to RAG's 29.31%\naccuracy and 52.85% consistency. Furthermore, we showcase the promise of GLOSS\nthrough four use cases inspired by prior and ongoing work in the UbiComp and\nHCI communities. Finally, we discuss the potential of GLOSS, its broader\nimplications, and the limitations of our work.", "AI": {"tldr": "This paper presents a new system, GLOSS, for open-ended sensemaking that effectively utilizes passive sensing data from smartphones and wearables to derive insights about health and behavior.", "motivation": "The need for a better understanding of individual behavior and context using passive sensing data, which is challenging due to the need for domain knowledge and technical expertise.", "method": "We developed the Group of LLMs for Open-ended Sensemaking (GLOSS) system, which allows for complex multimodal triangulation and open-ended sensemaking.", "result": "GLOSS significantly outperforms the Retrieval-Augmented Generation (RAG) method, achieving an accuracy of 87.93% and a consistency of 66.19%, compared to RAG's 29.31% accuracy and 52.85% consistency.", "conclusion": "GLOSS showcases the potential for improved insights through passive sensing and has broader implications for HCI and Ubicomp fields, although there are limitations in our approach.", "key_contributions": ["Introduction of GLOSS for open-ended sensemaking", "Demonstration of significant performance improvements over RAG", "Use case applications in HCI and UbiComp communities."], "limitations": "The limitations of our work need to be further explored and discussed.", "keywords": ["sensemaking", "passive sensing", "health informatics", "HCI", "UbiComp"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.05385", "pdf": "https://arxiv.org/pdf/2507.05385.pdf", "abs": "https://arxiv.org/abs/2507.05385", "title": "EduCoder: An Open-Source Annotation System for Education Transcript Data", "authors": ["Guanzhong Pan", "Mei Tan", "Hyunji Nam", "Lucía Langlois", "James Malamut", "Liliana Deonizio", "Dorottya Demszky"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce EduCoder, a domain-specialized tool designed to support\nutterance-level annotation of educational dialogue. While general-purpose text\nannotation tools for NLP and qualitative research abound, few address the\ncomplexities of coding education dialogue transcripts -- with diverse\nteacher-student and peer interactions. Common challenges include defining\ncodebooks for complex pedagogical features, supporting both open-ended and\ncategorical coding, and contextualizing utterances with external features, such\nas the lesson's purpose and the pedagogical value of the instruction. EduCoder\nis designed to address these challenges by providing a platform for researchers\nand domain experts to collaboratively define complex codebooks based on\nobserved data. It incorporates both categorical and open-ended annotation types\nalong with contextual materials. Additionally, it offers a side-by-side\ncomparison of multiple annotators' responses, allowing comparison and\ncalibration of annotations with others to improve data reliability. The system\nis open-source, with a demo video available.", "AI": {"tldr": "EduCoder is an open-source tool for utterance-level annotation of educational dialogue, aiding researchers in defining complex codebooks and improving data reliability through collaborative annotation.", "motivation": "To address the challenges of coding educational dialogue transcripts effectively, particularly in defining codebooks and managing diverse interactions.", "method": "Developed an open-source tool that supports both categorical and open-ended coding, allows contextualization of utterances, and features side-by-side comparison of annotations.", "result": "EduCoder facilitates the collaborative definition of complex codebooks and enhances the reliability of educational dialogue annotations through comparative analysis.", "conclusion": "The implementation of EduCoder provides an innovative solution for researchers in education to reliably annotate and analyze dialogue transcripts, benefiting from collaborative efforts.", "key_contributions": ["Open-source platform for educational dialogue annotation", "Supports both categorical and open-ended coding", "Allows for comparison of annotators' responses to improve reliability"], "limitations": "", "keywords": ["EduCoder", "annotation tool", "educational dialogue", "open-source", "NLP"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.05532", "pdf": "https://arxiv.org/pdf/2507.05532.pdf", "abs": "https://arxiv.org/abs/2507.05532", "title": "W2W: A Simulated Exploration of IMU Placement Across the Human Body for Designing Smarter Wearable", "authors": ["Lala Shakti Swarup Ray", "Bo Zhou", "Paul Lukowicz"], "categories": ["cs.HC"], "comment": null, "summary": "Inertial measurement units (IMUs) are central to wearable systems for\nactivity recognition and pose estimation, but sensor placement remains largely\nguided by heuristics and convention. In this work, we introduce Where to Wear\n(W2W), a simulation-based framework for systematic exploration of IMU placement\nutility across the body. Using labeled motion capture data, W2W generates\nrealistic synthetic IMU signals at 512 anatomically distributed surface\npatches, enabling high-resolution, task-specific evaluation of sensor\nperformance. We validate reliability of W2W by comparing spatial performance\nrankings from synthetic data with real IMU recordings in two multimodal\ndatasets, confirming strong agreement in activity-wise trends. Further analysis\nreveals consistent spatial trends across activity types and uncovers overlooked\nhigh-utility regions that are rarely used in commercial systems. These findings\nchallenge long-standing placement norms and highlight opportunities for more\nefficient, task-adaptive sensor configurations. Overall, our results\ndemonstrate that simulation with W2W can serve as a powerful design tool for\noptimizing sensor placement, enabling scalable, data-driven strategies that are\nimpractical to obtain through physical experimentation alone.", "AI": {"tldr": "This paper presents W2W, a simulation framework for optimizing the placement of inertial measurement units (IMUs) in wearable systems for improved activity recognition and pose estimation.", "motivation": "To systematically explore and improve IMU sensor placement for better performance in wearable systems, moving beyond heuristic methods.", "method": "The W2W framework generates synthetic IMU signals based on motion capture data at various points on the body to evaluate sensor performance across multiple tasks.", "result": "W2W demonstrated reliable predictions of sensor performance placement, showing strong agreement with real IMU data and highlighting neglected high-utility regions.", "conclusion": "W2W provides an effective tool for optimizing sensor placement through simulation, making data-driven design more feasible.", "key_contributions": ["Introduction of the W2W framework for IMU placement evaluation", "Validation with real data supporting the framework's reliability", "Identification of overlooked high-utility sensor placement regions"], "limitations": "The framework may still require validation in diverse real-world conditions and may be limited to specific types of activities tested.", "keywords": ["Inertial measurement units", "Sensor placement", "Wearable systems", "Activity recognition", "Simulation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.05387", "pdf": "https://arxiv.org/pdf/2507.05387.pdf", "abs": "https://arxiv.org/abs/2507.05387", "title": "The Generalization Ridge: Information Flow in Natural Language Generation", "authors": ["Ruidi Chang", "Chunyuan Deng", "Hanjie Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based language models have achieved state-of-the-art performance\nin natural language generation (NLG) tasks, yet their internal mechanisms for\nsynthesizing task-relevant information remain insufficiently understood. While\nprior studies suggest that intermediate layers often yield more generalizable\nrepresentations than final layers, how this generalization ability emerges and\npropagates across layers during training remains unclear. To address this gap,\nwe propose InfoRidge, an information-theoretic framework, to characterize how\npredictive information-the mutual information between hidden representations\nand target outputs-varies across depth. Estimating this quantity enables us to\ntrace the flow of task-relevant information throughout the model during\ntraining. Our experiments across various models and datasets reveal a\nconsistent non-monotonic trend: predictive information peaks in upper-middle\nlayers-forming a generalization ridge-before declining in final layers,\nreflecting a transition between generalization and memorization. To further\ninvestigate this phenomenon, we introduce residual scaling\ncoefficients-trainable scalar parameters applied to each residual block-which\nserve as functional probes for assessing the relative importance of individual\ntransformer layers. These coefficients reveal that, under distribution shift,\nmodels downweight final layers and increasingly rely on ridge layers,\nhighlighting their role in generalization. Together, these findings offer new\ninsights into the internal mechanisms of transformers and underscore the\ncritical role of intermediate layers in supporting generalization.", "AI": {"tldr": "This paper introduces InfoRidge, an information-theoretic framework exploring how predictive information varies across transformer layers during training, revealing critical insights into generalization and memorization dynamics.", "motivation": "To understand how predictive information varies across layers in transformer-based language models and its implications for generalization.", "method": "An information-theoretic framework is proposed to trace the predictive information between hidden representations and target outputs across model depth, using residual scaling coefficients to analyze individual layer importance.", "result": "The experiments show a non-monotonic trend in predictive information, peaking in upper-middle layers and declining in final layers, indicating a transition from generalization to memorization.", "conclusion": "Intermediate transformer layers play a crucial role in generalization, particularly under distribution shifts, which are highlighted by downweighting of final layers.", "key_contributions": ["Introduction of the InfoRidge framework for analyzing predictive information in transformers.", "Identification of a generalization ridge in upper-middle layers during training.", "Demonstration of the importance of residual scaling coefficients in assessing transformer layers' significance."], "limitations": "", "keywords": ["transformer models", "generalization", "information theory", "machine learning", "NLG"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.05537", "pdf": "https://arxiv.org/pdf/2507.05537.pdf", "abs": "https://arxiv.org/abs/2507.05537", "title": "Information Needs and Practices Supported by ChatGPT", "authors": ["Tim Gorichanaz"], "categories": ["cs.HC", "cs.IR"], "comment": "To be presented at the 2025 ASIS&T virtual satellite meeting,\n  December 2025", "summary": "This study considers ChatGPT as an information source, investigating the\ninformation needs that people come to ChatGPT with and the information\npractices that ChatGPT supports, through a qualitative content analysis of 205\nuser vignettes. The findings show that ChatGPT is used in a range of life\ndomains (home/family, work, leisure, etc.) and for a range of human needs\n(writing/editing, learning, simple programming tasks, etc.), constituting the\ninformation needs that people use ChatGPT to address. Related to these\ninformation needs, the findings show six categories of information practices\nthat ChatGPT supports: Writing, Deciding, Identifying, Ideating, Talking, and\nCritiquing. This work suggests that, in the AI age, information need should be\nconceptualized not just as a matter of \"getting questions answered\" or even\n\"making sense,\" but as skillfully coping in the world, a notion that includes\nboth understanding and action. This study leads to numerous opportunities for\nfuture work at the junction of generative AI and information needs, seeking,\nuse and experience.", "AI": {"tldr": "The study analyzes how people use ChatGPT for various information needs across different life domains, identifying supported practices like writing and ideation.", "motivation": "To investigate the information needs and practices that ChatGPT supports based on user interactions.", "method": "Qualitative content analysis of 205 user vignettes detailing how individuals engage with ChatGPT.", "result": "ChatGPT is utilized across diverse life areas for multiple needs, revealing six categories of information practices: Writing, Deciding, Identifying, Ideating, Talking, and Critiquing.", "conclusion": "Information needs in the AI age should encompass both understanding and action, highlighting potential for future research at the intersection of generative AI and information practices.", "key_contributions": ["Identification of six categories of information practices supported by ChatGPT", "Insight into the diverse information needs people have when using ChatGPT", "Framework for future research on generative AI and information experience"], "limitations": "", "keywords": ["ChatGPT", "information needs", "human-computer interaction", "generative AI", "user practices"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.05391", "pdf": "https://arxiv.org/pdf/2507.05391.pdf", "abs": "https://arxiv.org/abs/2507.05391", "title": "Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences", "authors": ["Guillem Ramírez", "Alexandra Birch", "Ivan Titov"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are primarily accessed via commercial APIs, but\nthis often requires users to expose their data to service providers. In this\npaper, we explore how users can stay in control of their data by using privacy\nprofiles: simple natural language instructions that say what should and should\nnot be revealed. We build a framework where a local model uses these\ninstructions to rewrite queries, only hiding details deemed sensitive by the\nuser, before sending them to an external model, thus balancing privacy with\nperformance. To support this research, we introduce PEEP, a multilingual\ndataset of real user queries annotated to mark private content and paired with\nsynthetic privacy profiles. Our experiments with lightweight LLMs show they can\nfollow these instructions to some extent, but also face consistent challenges,\nhighlighting the need for models that better understand and comply with\nuser-defined privacy preferences.", "AI": {"tldr": "This paper introduces a framework for using privacy profiles to protect user data when accessing large language models (LLMs) via APIs by rewriting queries to balance privacy and performance.", "motivation": "To allow users to retain control over their data while interacting with commercial LLMs, minimizing the exposure of sensitive information.", "method": "The authors developed a framework where a local model rewrites user queries based on privacy profiles that specify what information should be kept private before sending the modified queries to an external LLM.", "result": "Experiments demonstrate that lightweight LLMs can partially comply with privacy instructions, but significant challenges remain in fully understanding and adhering to user privacy preferences.", "conclusion": "The study emphasizes the importance of improving LLMs to better respect user-defined privacy parameters, suggesting directions for future work in enhancing model compliance with privacy guidelines.", "key_contributions": ["Introduction of privacy profiles for LLM queries", "Creation of the PEEP multilingual dataset with annotated user queries", "Empirical evaluation of lightweight LLMs' adherence to privacy instructions."], "limitations": "The compliance of models with privacy instructions is limited and presents challenges that need addressing in future developments.", "keywords": ["Privacy Profiles", "Large Language Models", "User Data Control", "Natural Language Instructions", "Privacy Compliance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.05572", "pdf": "https://arxiv.org/pdf/2507.05572.pdf", "abs": "https://arxiv.org/abs/2507.05572", "title": "AnatomyCarve: A VR occlusion management technique for medical images based on segment-aware clipping", "authors": ["Andrey Titov", "Tina N. H. Nantenaina", "Marta Kersten-Oertel", "Simon Drouin"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Visualizing 3D medical images is challenging due to self-occlusion, where\nanatomical structures of interest can be obscured by surrounding tissues.\nExisting methods, such as slicing and interactive clipping, are limited in\ntheir ability to fully represent internal anatomy in context. In contrast,\nhand-drawn medical illustrations in anatomy books manage occlusion effectively\nby selectively removing portions based on tissue type, revealing 3D structures\nwhile preserving context. This paper introduces AnatomyCarve, a novel technique\ndeveloped for a VR environment that creates high-quality illustrations similar\nto those in anatomy books, while remaining fast and interactive. AnatomyCarve\nallows users to clip selected segments from 3D medical volumes, preserving\nspatial relations and contextual information. This approach enhances\nvisualization by combining advanced rendering techniques with natural user\ninteractions in VR. Usability of AnatomyCarve was assessed through a study with\nnon-experts, while surgical planning effectiveness was evaluated with\npracticing neurosurgeons and residents. The results show that AnatomyCarve\nenables customized anatomical visualizations, with high user satisfaction,\nsuggesting its potential for educational and clinical applications.", "AI": {"tldr": "AnatomyCarve is a VR technique that generates high-quality medical illustrations by allowing users to selectively clip 3D segments of medical volumes, enhancing visualization and preserving context, with positive feedback from usability studies.", "motivation": "Visualizing 3D medical images is difficult due to self-occlusion, which obscures anatomical structures. Traditional methods do not effectively represent internal anatomy in context, necessitating an improved approach.", "method": "AnatomyCarve employs a novel technique in a VR setting to clip segments from 3D medical volumes, integrating advanced rendering and natural interactions to enhance visualization.", "result": "User studies indicated that AnatomyCarve provided customized anatomical visualizations, leading to high user satisfaction among non-experts and improved effectiveness for surgical planning among practicing neurosurgeons.", "conclusion": "AnatomyCarve shows promise for both educational purposes and clinical applications in the visualization of complex 3D medical images.", "key_contributions": ["Introduction of a novel VR technique for medical illustration", "Enhanced visualization by combining rendering with user interactions", "Positive usability and effectiveness feedback from medical professionals"], "limitations": "", "keywords": ["3D visualization", "medical imaging", "VR", "anatomical illustrations", "user satisfaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.05418", "pdf": "https://arxiv.org/pdf/2507.05418.pdf", "abs": "https://arxiv.org/abs/2507.05418", "title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning", "authors": ["Jaedong Hwang", "Kumar Tanmay", "Seok-Jin Lee", "Ayush Agrawal", "Hamid Palangi", "Kumar Ayush", "Ila Fiete", "Paul Pu Liang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have achieved strong performance in domains like\nmathematics, factual QA, and code generation, yet their multilingual reasoning\ncapabilities in these tasks remain underdeveloped. Especially for low-resource\nlanguages such as Swahili or Thai, LLMs can often misinterpret prompts or\ndefault to reasoning in English. This implicit bias toward high-resource\nlanguages undermines factual accuracy, interpretability, and trust. Current\nmultilingual benchmarks focus only on final answers, overlooking whether models\nactually reason in the target language. To address this gap, we introduce\nGeoFact-X, a geography-based multilingual factual reasoning benchmark with\nannotated reasoning traces in five languages: English, Hindi, Japanese,\nSwahili, and Thai. We further propose BRIDGE, a novel training method that\nguides supervised fine-tuning and test-time reinforcement learning with a\nlanguage-consistency reward to align reasoning with the input language.\nFinally, we develop an automatic evaluation protocol using LLM-as-a-judge to\nassess answer correctness and the quality and language consistency of reasoning\ntraces, enabling nuanced and scalable analysis beyond surface-level metrics.\nOur results show that BRIDGE significantly enhances multilingual reasoning\nfidelity, demonstrating that reasoning-aware multilingual reinforcement\nlearning is crucial for robust cross-lingual generalization.\nhttps://jd730.github.io/projects/GeoFact-X_BRIDGE", "AI": {"tldr": "The paper introduces GeoFact-X, a multilingual factual reasoning benchmark, and proposes BRIDGE, a training method for improving multilingual reasoning in LLMs, particularly for low-resource languages.", "motivation": "To improve multilingual reasoning capabilities of LLMs, especially for low-resource languages, and to address biases in reasoning related to language.", "method": "Introducing GeoFact-X as a benchmark with annotated reasoning traces and proposing the BRIDGE method, which includes supervised fine-tuning and reinforcement learning for language-consistency.", "result": "BRIDGE significantly enhances multilingual reasoning fidelity and demonstrates effective cross-lingual generalization.", "conclusion": "Reasoning-aware multilingual reinforcement learning is essential for LLMs to achieve robust performance in multilingual contexts.", "key_contributions": ["Development of GeoFact-X multilingual reasoning benchmark", "Introduction of BRIDGE training method for LLMs", "Implementation of an automatic evaluation protocol for nuanced analysis of reasoning"], "limitations": "", "keywords": ["Multilingual reasoning", "Large Language Models", "BRIDGE", "GeoFact-X", "Cross-lingual generalization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.05600", "pdf": "https://arxiv.org/pdf/2507.05600.pdf", "abs": "https://arxiv.org/abs/2507.05600", "title": "StoryGrid: A Tangible Interface for Student Expression", "authors": ["Tom Moher", "Louis Gomez", "Janet Kim", "Claudia Hindo", "Benjamin Watson", "Stephen Fransen", "Tim McEneany"], "categories": ["cs.HC"], "comment": null, "summary": "StorySpace is a classroom-based design and presentation system for\ninteractive multimedia posters. Employing the technology base first used in\nEden's PITAboard [2002], StorySpace allows groups of learners to manipulate\nprojected multimedia objects on a horizontal board using a small collection of\nshared physical tokens. In this paper, we present the ongoing design history of\nStorySpace in the context of its introduction within an urban high school\nliterature class. Interface modifications based on student and teacher feedback\nled on changes in token semantics and media importing methods. We describe how\nStorySpace features enriched students' interpretations of literature, with\nparticular emphasis in two areas: (1) attention to audience, and (2) reflection\nof multiple perspectives.", "AI": {"tldr": "StorySpace is an interactive multimedia design system for classrooms, allowing students to engage with literature through physical tokens and multimedia manipulation.", "motivation": "The aim is to improve student engagement and interpretation of literature in an urban high school setting.", "method": "Utilizing a design framework that incorporates user feedback, StorySpace enables group interactions with multimedia content on a shared board.", "result": "Student interactions with StorySpace led to improved understanding of audience considerations and the recognition of diverse perspectives in literary analysis.", "conclusion": "Ongoing refinements of StorySpace based on feedback have enhanced its effectiveness as a tool for literary interpretation.", "key_contributions": ["Innovative use of physical tokens for digital interactions in literature education", "Design adaptations based on direct feedback from users", "Enhanced student engagement through multimedia integration"], "limitations": "", "keywords": ["interactive multimedia", "education technology", "literary interpretation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.05424", "pdf": "https://arxiv.org/pdf/2507.05424.pdf", "abs": "https://arxiv.org/abs/2507.05424", "title": "\"Lost-in-the-Later\": Framework for Quantifying Contextual Grounding in Large Language Models", "authors": ["Yufei Tao", "Adam Hiatt", "Rahul Seetharaman", "Ameeta Agrawal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models are capable of leveraging both contextual and\nparametric knowledge but how they prioritize and integrate these sources\nremains underexplored. We introduce CoPE, a novel evaluation framework that\nsystematically measures contextual knowledge (CK) and parametric knowledge (PK)\nacross models and languages. Using our MultiWikiAtomic dataset in English,\nSpanish, and Danish, we analyze how large language models (LLMs) integrate\ncontext, prioritize information, and incorporate PK in open-ended question\nanswering. Our analysis uncovers a phenomenon we call lost-in-the-later, where\nLLMs tend to overlook or deprioritize information that appears later in a given\ncontext, revealing a strong positional bias that affects contextual grounding.\nWe further find that reasoning models, as well as non-reasoning models prompted\nwith chain-of-thought (CoT), use context even less than non-reasoning models\nwithout CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,\nin particular, results in lower recall and shorter responses, leading to\ndegraded contextual grounding. Based on these insights, we design prompt-based\nmethods to effectively leverage input context. A case study applying CoPE to\nsummarization demonstrates that CK-informed prompting improves factual\ngrounding and reduces hallucination.", "AI": {"tldr": "The paper introduces CoPE, an evaluation framework assessing how large language models (LLMs) integrate contextual and parametric knowledge, revealing a positional bias in information prioritization during open-ended question answering.", "motivation": "Understanding how large language models prioritize and integrate different types of knowledge is crucial for improving their performance in tasks like question answering and summarization.", "method": "The CoPE framework evaluates contextual knowledge (CK) and parametric knowledge (PK) using a dataset (MultiWikiAtomic) across multiple languages, analyzing LLM responses to open-ended questions.", "result": "The analysis identifies a phenomenon termed 'lost-in-the-later,' where LLMs deprioritize later information in the context, leading to positional bias that negatively impacts their ability to ground answers contextually. Moreover, prompting with chain-of-thought (CoT) worsens the recall and quality of contextual responses.", "conclusion": "CoPE shows that effective leveraging of contextual input can significantly improve LLM responses, as demonstrated through a case study in summarization that reduces hallucination.", "key_contributions": ["Introduces CoPE framework for evaluating knowledge integration in LLMs", "Identifies 'lost-in-the-later' bias affecting LLM performance", "Demonstrates improved grounding through CK-informed prompting"], "limitations": "The findings might be limited to the languages and tasks assessed; broader implications need further exploration.", "keywords": ["Large Language Models", "Contextual Knowledge", "Parametric Knowledge", "Chain-of-Thought", "AI Evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.05605", "pdf": "https://arxiv.org/pdf/2507.05605.pdf", "abs": "https://arxiv.org/abs/2507.05605", "title": "Hapster: Using Apple Watch Haptics to Enable Live Low-Friction Student Feedback in the Physical Classroom", "authors": ["Oleg Aleksandrovich Golev", "Michelle Huang", "Chanketya Nop", "Kritin Vongthongsri", "Andrés Monroy-Hernández", "Parastoo Abtahi"], "categories": ["cs.HC"], "comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems, pp. 1-7. 2024", "summary": "The benefits of student response systems (SRSs) for in-person lectures are\nwell-researched. However, all current SRSs only rely on a visual interface to\nrelay information to the instructor. We describe the design and evaluation of\nHapster, a prototype system that uses an Apple Watch to deliver live,\naggregated student feedback to the instructor via both visual and vibro-tactile\nmodalities. We evaluated this system with 6 instructors and 155 students at a\nU.S. university. Participants reported that the system was effective at\ndelivering live student feedback and facilitating better engagement from both\nthe instructor and the students. However, instructors also noted several\nchallenges with differentiating and perceiving the haptic sequences while\nlecturing. We conclude by discussing the tradeoff between system flexibility\nand abuse potential while identifying opportunities for further research\nregarding accessibility, content moderation, and additional interaction\nmodalities. Our results suggest that haptics can be used as an effective live\nfeedback mechanism for instructors in the physical classroom.", "AI": {"tldr": "Hapster, a prototype SRS using Apple Watch for live feedback via haptics, improves engagement but poses challenges in feedback perception.", "motivation": "To enhance student response systems (SRSs) beyond visual interfaces by incorporating haptic feedback.", "method": "Prototype evaluation involving 6 instructors and 155 students at a U.S. university.", "result": "Participants reported effective delivery of feedback and improved engagement, but instructors faced challenges with haptic feedback perception.", "conclusion": "Haptics can effectively relay live feedback but require further research on accessibility and interaction mechanisms.", "key_contributions": ["Introduction of haptic feedback in SRSs", "Evaluation of user experience with live feedback", "Insights into instructor-student engagement dynamics"], "limitations": "Challenges noted in instructor's ability to differentiate haptic sequences during lectures.", "keywords": ["Human-Computer Interaction", "Student Response Systems", "Haptic Feedback", "Teacher Engagement", "Accessibility"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2507.05443", "pdf": "https://arxiv.org/pdf/2507.05443.pdf", "abs": "https://arxiv.org/abs/2507.05443", "title": "Gendered Divides in Online Discussions about Reproductive Rights", "authors": ["Ashwin Rao", "Sze Yuh Nina Wang", "Kristina Lerman"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health\nOrganization marked a turning point in the national debate over reproductive\nrights. While the ideological divide over abortion is well documented, less is\nknown about how gender and local sociopolitical contexts interact to shape\npublic discourse. Drawing on nearly 10 million abortion-related posts on X\n(formerly Twitter) from users with inferred gender, ideology and location, we\nshow that gender significantly moderates abortion attitudes and emotional\nexpression, particularly in conservative regions, and independently of\nideology. This creates a gender gap in abortion attitudes that grows more\npronounced in conservative regions. The leak of the Dobbs draft opinion further\nintensified online engagement, disproportionately mobilizing pro-abortion women\nin areas where access was under threat. These findings reveal that abortion\ndiscourse is not only ideologically polarized but also deeply structured by\ngender and place, highlighting the central role of identity in shaping\npolitical expression during moments of institutional disruption.", "AI": {"tldr": "The study analyzes nearly 10 million abortion-related posts on X to explore how gender and local contexts influence public discourse on abortion following the 2022 Dobbs ruling.", "motivation": "To understand the interaction between gender, local sociopolitical contexts, and their impact on public discourse around abortion following the Supreme Court's Dobbs decision.", "method": "The authors analyzed 10 million abortion-related posts on X, categorizing users by inferred gender, ideology, and location to assess how these factors influenced attitudes and emotional expressions regarding abortion.", "result": "The findings indicate that gender significantly moderates abortion attitudes and emotional expressions, particularly in conservative regions, creating a notable gender gap that intensifies in such contexts. The leak of the Dobbs draft motivated increased engagement among pro-abortion women in areas facing restricted access.", "conclusion": "Abortion discourse is shaped by both ideological polarization and gender, underscoring the importance of identity in political expression during institutional changes.", "key_contributions": ["Identifies the role of gender in moderating abortion attitudes and emotional responses.", "Highlights the influence of local sociopolitical contexts on public discourse around abortion.", "Demonstrates increased engagement among pro-abortion women in response to the Dobbs ruling leak."], "limitations": "Focuses primarily on social media posts which may not fully represent broader public opinions or offline discussions.", "keywords": ["abortion", "gender", "political discourse", "sociopolitical context", "public engagement"], "importance_score": 2, "read_time_minutes": 8}}
{"id": "2507.05616", "pdf": "https://arxiv.org/pdf/2507.05616.pdf", "abs": "https://arxiv.org/abs/2507.05616", "title": "Breaking the Plane: Exploring Real-Time Visualization of 3D Surfaces in Augmented Reality with Handwritten Input", "authors": ["Liam Franco Esparraguera", "Kristoffer Selberg", "Brian Lou", "Jenny Sun", "Beza Desta", "Andrés Monroy-Hernández", "Parastoo Abtahi"], "categories": ["cs.HC"], "comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems, pp. 1-9. 2024", "summary": "We introduce Breaking the Plane, an augmented reality (AR) application built\nfor AR headsets that enables users to visualize 3D mathematical functions using\nhandwritten input. Researchers have demonstrated overlaying 3D visualizations\nof mathematical concepts through AR enhances learning motivation and\ncomprehension, and equation parsing makes the authoring of teaching materials\nmore time-efficient for instructors. Previous works have developed AR systems\nthat separately employ equation parsing and 3D mathematical visualizations, but\nwork has yet to be done to combine those features by enabling real-time\ninteractions and dynamic visualizations that help users learn in situ. We\nexplore this by developing an AR system featuring handwritten equation parsing,\ngraph manipulation, and a 3D function plotter. We found that our system\nsignificantly surpassed other systems in engagement, achieved comparable ease\nof use to a popular visualization tool, was considered the most effective in\naiding problem-solving, and was highly preferred by participants for future\nuse.", "AI": {"tldr": "An AR application that visualizes 3D mathematical functions through handwritten input, enhancing comprehension and engagement in learning.", "motivation": "Current AR educational tools lack integration of real-time interactions with 3D visualizations. This study aims to bridge that gap to improve learning outcomes in mathematics.", "method": "Development of a system featuring handwritten equation parsing, graph manipulation, and a 3D function plotter, followed by user engagement studies.", "result": "The system significantly outperformed others in engagement and was as easy to use as a popular tool, aiding problem-solving effectively and gaining high user preference.", "conclusion": "Breaking the Plane demonstrates the benefits of integrating handwritten input and 3D visualizations in AR to enhance educational experiences in mathematics.", "key_contributions": ["Combines real-time handwritten equation parsing with dynamic 3D visualizations.", "Demonstrates significant engagement improvements over existing educational tools.", "Offers effective problem-solving support in augmented learning settings."], "limitations": "", "keywords": ["augmented reality", "mathematics education", "handwritten input", "3D visualization", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 9}}
{"id": "2507.05444", "pdf": "https://arxiv.org/pdf/2507.05444.pdf", "abs": "https://arxiv.org/abs/2507.05444", "title": "PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs", "authors": ["Sana Kang", "Myeongseok Gwon", "Su Young Kwon", "Jaewook Lee", "Andrew Lan", "Bhiksha Raj", "Rita Singh"], "categories": ["cs.CL"], "comment": null, "summary": "Vocabulary acquisition poses a significant challenge for second-language (L2)\nlearners, especially when learning typologically distant languages such as\nEnglish and Korean, where phonological and structural mismatches complicate\nvocabulary learning. Recently, large language models (LLMs) have been used to\ngenerate keyword mnemonics by leveraging similar keywords from a learner's\nfirst language (L1) to aid in acquiring L2 vocabulary. However, most of this\nresearch has focused on native English speakers learning other languages,\nrather than the reverse. In this paper, we present PhoniTale, a novel\ncross-lingual mnemonic generation system that retrieves L1 keyword sequence\nbased on phonological similarity and uses LLMs to generate mnemonics. We\nevaluate PhoniTale using both automated metrics and human evaluations,\ncomparing its output to mnemonics created by humans and by previous automated\napproaches. To assess practical effectiveness, we also conduct a short-term\nrecall test measuring mnemonic helpfulness. Our findings show that PhoniTale\nperforms comparably to human-authored mnemonics. We also highlight key areas\nfor future improvement in mnemonic quality and methodology.", "AI": {"tldr": "PhoniTale is a novel system for generating cross-lingual mnemonics using LLMs to assist L2 learners in vocabulary acquisition.", "motivation": "Vocabulary acquisition is a challenge for L2 learners, particularly with typologically distant languages, and existing research has mainly focused on native English speakers.", "method": "PhoniTale retrieves L1 keyword sequences based on phonological similarity and utilizes LLMs to create mnemonics for L2 vocabulary learning.", "result": "PhoniTale's mnemonics were found to perform comparably to human-created mnemonics in evaluations.", "conclusion": "The study emphasizes the effectiveness of PhoniTale while identifying areas for future research to enhance mnemonic quality.", "key_contributions": ["Introduction of a cross-lingual mnemonic generation system.", "Utilization of phonological similarity for keyword retrieval.", "Comparison of automated mnemonics with human-generated ones."], "limitations": "The study primarily focuses on short-term recall and may require further exploration of long-term effectiveness.", "keywords": ["cross-lingual", "mnemonics", "large language models", "vocabulary acquisition", "phonological similarity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.05820", "pdf": "https://arxiv.org/pdf/2507.05820.pdf", "abs": "https://arxiv.org/abs/2507.05820", "title": "Constella: Supporting Storywriters' Interconnected Character Creation through LLM-based Multi-Agents", "authors": ["Syemin Park", "Soobin Park", "Youn-kyung Lim"], "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": "50 pages", "summary": "Creating a cast of characters by attending to their relational dynamics is a\ncritical aspect of most long-form storywriting. However, our formative study\n(N=14) reveals that writers struggle to envision new characters that could\ninfluence existing ones, to balance similarities and differences among\ncharacters, and to intricately flesh out their relationships. Based on these\nobservations, we designed Constella, an LLM-based multi-agent tool that\nsupports storywriters' interconnected character creation process. Constella\nsuggests related characters (FRIENDS DISCOVERY feature), reveals the inner\nmindscapes of several characters simultaneously (JOURNALS feature), and\nmanifests relationships through inter-character responses (COMMENTS feature).\nOur 7-8 day deployment study with storywriters (N=11) shows that Constella\nenabled the creation of expansive communities composed of related characters,\nfacilitated the comparison of characters' thoughts and emotions, and deepened\nwriters' understanding of character relationships. We conclude by discussing\nhow multi-agent interactions can help distribute writers' attention and effort\nacross the character cast.", "AI": {"tldr": "Constella is an LLM-based tool designed to assist storywriters in creating interconnected characters, improving their understanding of character dynamics.", "motivation": "Storywriters face challenges in creating characters that influence each other and in fleshing out their relationships.", "method": "Constella features character suggestion, inner mindscape journaling, and inter-character response mechanisms, employed in a study with storywriters over 7-8 days.", "result": "The deployment study demonstrated that Constella helped writers create expansive character communities, compare character thoughts and emotions, and enhance the understanding of relationships.", "conclusion": "Multi-agent interactions in tools like Constella can distribute the writers' attention and effort effectively across multiple characters.", "key_contributions": ["Development of the Constella tool to aid character creation", "Introduction of innovative features like FRIENDS DISCOVERY, JOURNALS, and COMMENTS", "Insights from user deployment showing improved character interconnectedness"], "limitations": "", "keywords": ["human-computer interaction", "storywriting", "LLM", "character creation", "multi-agent systems"], "importance_score": 7, "read_time_minutes": 50}}
{"id": "2507.05448", "pdf": "https://arxiv.org/pdf/2507.05448.pdf", "abs": "https://arxiv.org/abs/2507.05448", "title": "On the Semantics of Large Language Models", "authors": ["Martin Schuele"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT demonstrated the potential to\nreplicate human language abilities through technology, ranging from text\ngeneration to engaging in conversations. However, it remains controversial to\nwhat extent these systems truly understand language. We examine this issue by\nnarrowing the question down to the semantics of LLMs at the word and sentence\nlevel. By examining the inner workings of LLMs and their generated\nrepresentation of language and by drawing on classical semantic theories by\nFrege and Russell, we get a more nuanced picture of the potential semantic\ncapabilities of LLMs.", "AI": {"tldr": "The paper investigates the semantic understanding of Large Language Models at the word and sentence level using classical semantic theories.", "motivation": "To clarify the extent to which LLMs, like ChatGPT, understand language, particularly focusing on semantics.", "method": "The inner workings of LLMs are examined alongside their language representation, referencing classical theories by Frege and Russell to analyze semantic capabilities.", "result": "A nuanced assessment of LLMs' semantic abilities at different levels is provided, highlighting the complexities of their language understanding.", "conclusion": "The study offers insights into the semantic potential of LLMs, acknowledging both capabilities and limitations in replicating human language understanding.", "key_contributions": ["Analysis of LLM semantics using classical theories", "Insights into the limitations of LLMs in understanding language", "Nuanced evaluation of LLMs' capabilities at word and sentence levels"], "limitations": "The paper may not cover the full range of LLM capabilities beyond semantics.", "keywords": ["Large Language Models", "semantics", "Frege", "Russell", "language understanding"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.05962", "pdf": "https://arxiv.org/pdf/2507.05962.pdf", "abs": "https://arxiv.org/abs/2507.05962", "title": "Evaluation of Large Language Model-Driven AutoML in Data and Model Management from Human-Centered Perspective", "authors": ["Jiapeng Yao", "Lantian Zhang", "Jiping Huang"], "categories": ["cs.HC"], "comment": null, "summary": "As organizations increasingly seek to leverage machine learning (ML)\ncapabilities, the technical complexity of implementing ML solutions creates\nsignificant barriers to adoption and impacts operational efficiency. This\nresearch examines how Large Language Models (LLMs) can transform the\naccessibility of ML technologies within organizations through a human-centered\nAutomated Machine Learning (AutoML) approach. Through a comprehensive user\nstudy involving 15 professionals across various roles and technical\nbackgrounds, we evaluate the organizational impact of an LLM-based AutoML\nframework compared to traditional implementation methods. Our research offers\nfour significant contributions to both management practice and technical\ninnovation: First, we present pioneering evidence that LLM-based interfaces can\ndramatically improve ML implementation success rates, with 93.34% of users\nachieved superior performance in the LLM condition, with 46.67% showing higher\naccuracy (10-25% improvement over baseline) and 46.67% demonstrating\nsignificantly higher accuracy (>25% improvement over baseline), while 6.67%\nmaintained comparable performance levels; and 60% reporting substantially\nreduced development time. Second, we demonstrate how natural language\ninterfaces can effectively bridge the technical skills gap in organizations,\ncutting implementation time by 50% while improving accuracy across all\nexpertise levels. Third, we provide valuable insights for organizations\ndesigning human-AI collaborative systems, showing that our approach reduced\nerror resolution time by 73% and significantly accelerated employee learning\ncurves. Finally, we establish empirical support for natural language as an\neffective interface for complex technical systems, offering organizations a\npath to democratize ML capabilities without compromising quality or\nperformance.", "AI": {"tldr": "This research explores how Large Language Models (LLMs) can simplify Machine Learning (ML) adoption through a human-centered Automated Machine Learning (AutoML) approach, demonstrating significant improvements in implementation success and efficiency.", "motivation": "To address barriers to ML adoption due to technical complexity and to assess how LLMs can enhance accessibility to ML technologies.", "method": "Conducted a user study with 15 professionals across various roles and technical backgrounds to evaluate the impact of an LLM-based AutoML framework versus traditional methods.", "result": "The study found that 93.34% of users achieved better performance with LLM-based interfaces, with notable improvements in accuracy and significant reductions in development time and error resolution.", "conclusion": "LLM-based AutoML can democratize ML capabilities in organizations, improving success rates, efficiency, and bridging the technical skills gap.", "key_contributions": ["Pioneering evidence of improved ML implementation success rates using LLMs.", "Natural language interfaces reduce implementation time by 50% while enhancing accuracy.", "Insights for human-AI collaborative systems, drastically reducing error resolution time."], "limitations": "", "keywords": ["Large Language Models", "Automated Machine Learning", "Human-AI collaboration"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.05455", "pdf": "https://arxiv.org/pdf/2507.05455.pdf", "abs": "https://arxiv.org/abs/2507.05455", "title": "ModelCitizens:Representing Community Voices in Online Safety", "authors": ["Ashima Suvarna", "Christina Chance", "Hamid Palangi", "Sophie Hao", "Thomas Hartvigsen", "Saadia Gabriel"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic toxic language detection is critical for creating safe, inclusive\nonline spaces. However, it is a highly subjective task, with perceptions of\ntoxic language shaped by community norms and lived experience. Existing\ntoxicity detection models are typically trained on annotations that collapse\ndiverse annotator perspectives into a single ground truth, erasing important\ncontext-specific notions of toxicity such as reclaimed language. To address\nthis, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K\ntoxicity annotations across diverse identity groups. To capture the role of\nconversational context on toxicity, typical of social media posts, we augment\nMODELCITIZENS posts with LLM-generated conversational scenarios.\nState-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,\nGPT-o4-mini) underperform on MODELCITIZENS, with further degradation on\ncontext-augmented posts. Finally, we release LLAMACITIZEN-8B and\nGEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,\nwhich outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our\nfindings highlight the importance of community-informed annotation and modeling\nfor inclusive content moderation.", "AI": {"tldr": "This paper presents MODELCITIZENS, a dataset and models for improved toxic language detection, emphasizing the importance of community-influenced annotations.", "motivation": "To enhance online safety and inclusivity by addressing the limitations of current toxicity detection models in reflecting diverse perspectives.", "method": "The authors created MODELCITIZENS, consisting of 6.8K social media posts with 40K toxicity annotations, and incorporated LLM-generated scenarios to better capture conversational context.", "result": "State-of-the-art toxicity detection tools showed underperformance on the MODELCITIZENS dataset, particularly with context-augmented posts. Newly released models LLAMACITIZEN-8B and GEMMACITIZEN-12B outperformed existing models by 5.5% in evaluations.", "conclusion": "Community-informed annotations and modeling are crucial for effective and inclusive content moderation in social media settings.", "key_contributions": ["Introduction of MODELCITIZENS dataset for toxic language detection", "Incorporation of LLM-generated scenarios for context", "Development of new models that outperform existing toxicity detection tools"], "limitations": "Existing models were not designed to handle diverse community norms and context variability, which may limit generalization.", "keywords": ["toxic language detection", "MODELCITIZENS", "community-informed annotation", "LLM-generated scenarios", "content moderation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.06000", "pdf": "https://arxiv.org/pdf/2507.06000.pdf", "abs": "https://arxiv.org/abs/2507.06000", "title": "Exploring Collaboration Patterns and Strategies in Human-AI Co-creation through the Lens of Agency: A Scoping Review of the Top-tier HCI Literature", "authors": ["Shuning Zhang", "Hui Wang", "Xin Yi"], "categories": ["cs.HC"], "comment": null, "summary": "As Artificial Intelligence (AI) increasingly becomes an active collaborator\nin co-creation, understanding the distribution and dynamic of agency is\nparamount. The Human-Computer Interaction (HCI) perspective is crucial for this\nanalysis, as it uniquely reveals the interaction dynamics and specific control\nmechanisms that dictate how agency manifests in practice. Despite this\nimportance, a systematic synthesis mapping agency configurations and control\nmechanisms within the HCI/CSCW literature is lacking. Addressing this gap, we\nreviewed 134 papers from top-tier HCI/CSCW venues (e.g., CHI, UIST, CSCW) over\nthe past 20 years. This review yields four primary contributions: (1) an\nintegrated theoretical framework structuring agency patterns, control\nmechanisms, and interaction contexts, (2) a comprehensive operational catalog\nof control mechanisms detailing how agency is implemented; (3) an actionable\ncross-context map linking agency configurations to diverse co-creative\npractices; and (4) grounded implications and guidance for future CSCW research\nand the design of co-creative systems, addressing aspects like trust and\nethics.", "AI": {"tldr": "This paper reviews HCI/CSCW literature on agency in AI co-creation, identifying control mechanisms and interaction dynamics.", "motivation": "To understand the complexities of agency in AI co-creation, particularly in HCI/CSCW contexts, addressing a notable gap in systematic reviews.", "method": "A review of 134 papers from top-tier HCI and CSCW venues over the last 20 years to synthesize agency configurations and control mechanisms.", "result": "The review identifies four contributions: a theoretical framework for agency patterns, a catalog of control mechanisms, a cross-context map for co-creative practices, and guidance for future CSCW research.", "conclusion": "The findings provide actionable insights into the design of co-creative systems, emphasizing trust and ethics in AI interactions.", "key_contributions": ["Theoretical framework for agency patterns", "Comprehensive catalog of control mechanisms", "Cross-context map for co-creative practices"], "limitations": "", "keywords": ["agency", "Human-Computer Interaction", "co-creation", "control mechanisms", "CSCW"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.05517", "pdf": "https://arxiv.org/pdf/2507.05517.pdf", "abs": "https://arxiv.org/abs/2507.05517", "title": "Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications", "authors": ["Jean-Philippe Corbeil", "Asma Ben Abacha", "George Michalopoulos", "Phillip Swazinna", "Miguel Del-Agua", "Jerome Tremblay", "Akila Jeeson Daniel", "Cari Bader", "Kevin Cho", "Pooja Krishnan", "Nathan Bodenstab", "Thomas Lin", "Wenxuan Teng", "Francois Beaulieu", "Paul Vozila"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong\nperformance on clinical natural language processing (NLP) tasks across multiple\nmedical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular\nreporting from nurse dictations and medical order extraction from\ndoctor-patient consultations - remain underexplored due to data scarcity and\nsensitivity, despite active industry efforts. Practical solutions to these\nreal-world clinical tasks can significantly reduce the documentation burden on\nhealthcare providers, allowing greater focus on patient care. In this paper, we\ninvestigate these two challenging tasks using private and open-source clinical\ndatasets, evaluating the performance of both open- and closed-weight LLMs, and\nanalyzing their respective strengths and limitations. Furthermore, we propose\nan agentic pipeline for generating realistic, non-sensitive nurse dictations,\nenabling structured extraction of clinical observations. To support further\nresearch in both areas, we release SYNUR and SIMORD, the first open-source\ndatasets for nurse observation extraction and medical order extraction.", "AI": {"tldr": "This paper explores the application of large language models (LLMs) for structured tabular reporting and medical order extraction in clinical settings, addressing data sensitivity and scarcity issues.", "motivation": "The paper seeks to address the underexplored NLP tasks of structured reporting from nurse dictations and medical order extraction due to data scarcity and sensitivity, aiming to alleviate documentation burdens on healthcare providers.", "method": "The authors investigate the performance of various open- and closed-weight LLMs using private and open-source clinical datasets, proposing an agentic pipeline for generating nurse dictations.", "result": "The study reveals the strengths and limitations of different LLMs in handling structured tabular reporting and medical order extraction, enhancing understanding of their practical applications in clinical environments.", "conclusion": "The introduction of open-source datasets SYNUR and SIMORD supports further research in nurse observation extraction and medical order extraction, contributing to the development of practical solutions in healthcare NLP.", "key_contributions": ["Evaluation of LLMs on clinically relevant NLP tasks", "Introduction of novel open-source datasets for nurse dictation and order extraction", "Development of an agentic pipeline for structured healthcare documentation"], "limitations": "The paper does not address all potential challenges related to the deployment of LLMs in clinical settings.", "keywords": ["Large language models", "Natural language processing", "Health informatics", "Open-source datasets", "Clinical documentation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.06141", "pdf": "https://arxiv.org/pdf/2507.06141.pdf", "abs": "https://arxiv.org/abs/2507.06141", "title": "Large Language Models Predict Human Well-being -- But Not Equally Everywhere", "authors": ["Pat Pataranutaporn", "Nattavudh Powdthavee", "Chayapatr Archiwaranguprok", "Pattie Maes"], "categories": ["cs.HC"], "comment": null, "summary": "Subjective well-being is a key metric in economic, medical, and policy\ndecision-making. As artificial intelligence provides scalable tools for\nmodelling human outcomes, it is crucial to evaluate whether large language\nmodels (LLMs) can accurately predict well-being across diverse global\npopulations. We evaluate four leading LLMs using data from 64,000 individuals\nin 64 countries. While LLMs capture broad correlates such as income and health,\ntheir predictive accuracy decreases in countries underrepresented in the\ntraining data, highlighting systematic biases rooted in global digital and\neconomic inequality. A pre-registered experiment demonstrates that LLMs rely on\nsurface-level linguistic similarity rather than conceptual understanding,\nleading to systematic misestimations in unfamiliar or resource-limited\nsettings. Injecting findings from underrepresented contexts substantially\nenhances performance, but a significant gap remains. These results highlight\nboth the promise and limitations of LLMs in predicting global well-being,\nunderscoring the importance of robust validation prior to their implementation\nacross these areas.", "AI": {"tldr": "This paper evaluates the ability of large language models (LLMs) to predict subjective well-being across diverse global populations, revealing systematic biases and limitations in their predictive accuracy, especially in underrepresented countries.", "motivation": "To determine if large language models can accurately predict well-being metrics in diverse populations, thereby informing their use in economic, medical, and policy decision-making.", "method": "Evaluated four leading LLMs using data from 64,000 individuals across 64 countries, focusing on their predictive accuracy and bias in relation to training data representation.", "result": "LLMs capture broad correlations like income and health, but their accuracy diminishes for underrepresented countries, indicating systemic biases and misestimations based on linguistic similarity instead of conceptual understanding.", "conclusion": "LLMs show potential in predicting well-being but require robust validation and improvement, especially in underrepresented contexts, to avoid misestimations before implementation.", "key_contributions": ["Identification of biases in LLM predictions based on training data representation", "Demonstration of the importance of context-specific data for improving LLM accuracy", "Highlighting the pitfalls of relying solely on surface-level linguistic similarity for well-being predictions"], "limitations": "Significant predictive accuracy gaps remain even after injecting findings from underrepresented contexts, suggesting ongoing issues with LLM understanding.", "keywords": ["large language models", "well-being prediction", "systematic bias", "global inequality", "data representation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.05557", "pdf": "https://arxiv.org/pdf/2507.05557.pdf", "abs": "https://arxiv.org/abs/2507.05557", "title": "Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS", "authors": ["Alex ZH Dou", "Zhongwei Wan", "Dongfei Cui", "Xin Wang", "Jing Xiong", "Haokun Lin", "Chaofan Tao", "Shen Yan", "Mi Zhang"], "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Test-time scaling has emerged as a promising paradigm in language modeling,\nleveraging additional computational resources at inference time to enhance\nmodel performance. In this work, we introduce R2-LLMs, a novel and versatile\nhierarchical retrieval-augmented reasoning framework designed to improve\ntest-time scaling in large language models (LLMs) without requiring\ndistillation from more advanced models to obtain chain-of-thought (CoT)\ntraining data. R2-LLMs enhances inference-time generalization by integrating\ndual-level retrieval-based in-context learning: (1) At the coarse level, our\napproach extracts abstract templates from complex reasoning problems and\nretrieves similar problem-answer pairs to facilitate high-level in-context\nlearning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs\nefficiently retrieves analogous intermediate solution steps from reference\nmathematical problem datasets, refining step-wise reasoning with the aid of a\nprocess reward model (PRM) for scoring. R2-LLMs is a robust hierarchical\nreasoning-augmentation method that enhances in-context-level reasoning while\nseamlessly integrating with step-level tree search methods. Utilizing PRM, it\nrefines both candidate generation and decision-making for improved reasoning\naccuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO\ndatasets achieve substantial relative improvement with an increase of up to 16%\nusing LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of\nour approach in complex reasoning tasks.", "AI": {"tldr": "Introduction of R2-LLMs, a hierarchical retrieval-augmented framework to improve test-time scaling in large language models without requiring distillation for CoT training data.", "motivation": "To enhance model performance at inference time using additional computational resources through an improved framework for large language models.", "method": "R2-LLMs utilizes dual-level retrieval-based in-context learning, with coarse-level extraction of templates and fine-level reasoning supported by Monte Carlo Tree Search.", "result": "Empirical evaluations show a relative improvement of up to 16% on complex reasoning datasets using LLaMA-3.1-8B compared to existing baselines.", "conclusion": "R2-LLMs enhances inference-time generalization and reasoning accuracy, showcasing effective integration with step-level tree search methods.", "key_contributions": ["Hierarchical retrieval-augmented reasoning framework", "Dual-level retrieval-based in-context learning", "Integration of Monte Carlo Tree Search with a process reward model"], "limitations": "", "keywords": ["language modeling", "test-time scaling", "hierarchical retrieval", "reasoning", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.06202", "pdf": "https://arxiv.org/pdf/2507.06202.pdf", "abs": "https://arxiv.org/abs/2507.06202", "title": "V(is)owel: An Interactive Vowel Chart to Understand What Makes Visual Pronunciation Effective in Second Language Learning", "authors": ["Charlotte Kiesel", "Dipayan Mukherjee", "Mark Hasegawa-Johnson", "Karrie Karahalios"], "categories": ["cs.HC", "K.3.1"], "comment": null, "summary": "Visual feedback speeds up learners' improvement of pronunciation in a second\nlanguage. The visual combined with audio allows speakers to see sounds and\ndifferences in pronunciation that they are unable to hear. Prior studies have\ntested different visual methods for improving pronunciation, however, we do not\nhave conclusive understanding of what aspects of the visualizations contributed\nto improvements. Based on previous work, we created V(is)owel, an interactive\nvowel chart. Vowel charts provide actionable feedback by directly mapping\nphysical tongue movement onto a chart. We compared V(is)owel with an\nauditory-only method to explore how learners parse visual and auditory feedback\nto understand how and why visual feedback is effective for pronunciation\nimprovement. The findings suggest that designers should include explicit\nanatomical feedback that directly maps onto physical movement for phonetically\nuntrained learners. Furthermore, visual feedback has the potential to motivate\nmore practice since all eight of the participants cited using the visuals as a\ngoal with V(is)owel versus relying on their own judgment with audio alone.\nTheir statements are backed up by all participants practicing words with\nV(is)owel more than with audio-only. Our results indicate that V(is)owel is\neffective at providing actionable feedback, demonstrating the potential of\nvisual feedback methods in second language learning.", "AI": {"tldr": "V(is)owel, an interactive vowel chart, enhances pronunciation learning in second language acquisition through visual feedback, outperforming auditory-only methods.", "motivation": "Understanding the effects of visual feedback on pronunciation improvement in second language learners.", "method": "Comparison of V(is)owel, a visual feedback tool, with an auditory-only method to analyze learner responses to both feedback types.", "result": "Participants using V(is)owel reported higher motivation and practiced more effectively than with audio alone.", "conclusion": "V(is)owel provides actionable feedback by mapping physical movements, leading to better pronunciation outcomes for phonetically untrained learners.", "key_contributions": ["Development of V(is)owel as an interactive tool for pronunciation improvement.", "Demonstration of the effectiveness of visual feedback in language learning.", "Insights into learner preferences for feedback methods."], "limitations": "Limited sample size of eight participants may affect generalizability of findings.", "keywords": ["visual feedback", "pronunciation improvement", "vowel chart", "language learning", "interaction design"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.05598", "pdf": "https://arxiv.org/pdf/2507.05598.pdf", "abs": "https://arxiv.org/abs/2507.05598", "title": "Self-Review Framework for Enhancing Instruction Following Capability of LLM", "authors": ["Sihyun Park"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Various techniques have been proposed to improve large language models (LLMs)\nadherence to formatting and instruction constraints. One of the most effective\napproaches involves utilizing high-quality data generated by powerful models.\nHowever, such models often fail to fully comply with complex instructions in a\nsingle generation. To address this limitation, iterative revision methods have\nbeen introduced. Nevertheless, as the number of data points and revision\niterations increases, the associated monetary costs grow significantly. As a\nresource-efficient alternative, methods have been proposed that leverage\nhigh-performance evaluation tools to compensate for the limited self-evaluation\ncapabilities of open-source LLMs. However, these approaches often lead to a\ndegradation in output quality due to excessive revision. To overcome these\nchallenges, we propose Re5, a self-evaluation and revision framework designed\nto enhance instruction-following performance while preserving the quality of\nthe generated content. Re5 extracts task and constraint components from user\ninstructions, performs structural evaluations to prevent error accumulation,\nand applies fine-grained constraint-specific content evaluations followed by\nselective revisions. This process ensures precise and quality-preserving\nimprovements. The final high-quality outputs are used for alignment tuning,\nenabling long-term alignment improvements through a data-centric iterative\nrefinement loop. Experimental results demonstrate that Re5 achieves\ninstruction-following performance comparable to models trained on data\ngenerated by GPT-4o-mini, a high-performance model, even with a small amount of\ndata while maintaining response quality with a 64.24%-win rate over the\nnon-revised initial responses. These results validate Re5 as an efficient and\neffective solution for enhancing instruction adherence with minimal external\nsupervision.", "AI": {"tldr": "Re5 is a self-evaluation and revision framework aimed at improving LLMs' instruction adherence while preserving output quality, achieving results comparable to high-performance models with minimal data.", "motivation": "To enhance large language models' adherence to formatting and instruction constraints without escalating costs associated with iterative revisions.", "method": "Re5 extracts task components from user instructions, conducts structural evaluations to prevent errors, and performs selective revisions based on fine-grained constraint evaluations.", "result": "Re5 achieves instruction-following performance comparable to models trained on high-quality data, with a win rate of 64.24% over non-revised responses.", "conclusion": "Re5 is validated as an efficient solution for enhancing instruction adherence in LLMs with minimal external supervision.", "key_contributions": ["Introduction of Re5 framework for self-evaluation and revision in LLMs", "Preservation of output quality during iterative revisions", "Demonstration of competitive performance using limited data"], "limitations": "Potential degradation in output quality with excessive revision cycles remains a concern.", "keywords": ["large language models", "self-evaluation", "revision framework", "instruction adherence", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.05617", "pdf": "https://arxiv.org/pdf/2507.05617.pdf", "abs": "https://arxiv.org/abs/2507.05617", "title": "Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching", "authors": ["Mingzhe Li", "Jing Xiang", "Qishen Zhang", "Kaiyang Wan", "Xiuying Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "Knowledge distillation typically involves transferring knowledge from a Large\nLanguage Model (LLM) to a Smaller Language Model (SLM). However, in tasks such\nas text matching, fine-tuned smaller models often yield more effective\ndomain-specific representations, as they focus on optimizing the similarity of\ninput pairs. To leverage both the specialized strengths of small models and the\nrich semantic understanding of LLMs, we introduce a flipped knowledge\ndistillation paradigm, where LLM learns from SLM. Specifically, we address the\narchitectural gap between decoder-only LLMs and smaller encoder-based models by\nreinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder\ngenerates compressed representations, while the decoder maps them to the output\nspace. During training, the encoder produces representations and their\nsimilarities, which are then aligned with the similarity scores produced by the\nteacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.\nThe MCL ensures accurate similarity for both positive and negative pairs, and\nadaptively handles the internal differences within positive and negative\nsamples. Our paradigm requires only a reasonably good-performing SLM, allowing\nthe LLM to achieve improved performance. Experiments on financial and\nhealthcare benchmarks, as well as real-world applications, confirm its\neffectiveness, and the model has been fully deployed in an online environment.", "AI": {"tldr": "Introduces a flipped knowledge distillation method where LLM learns from SLM, enhancing performance in text matching tasks with a Margin-aware Contrastive Learning approach.", "motivation": "Leverage the strengths of small models and LLMs to improve text matching tasks by allowing LLMs to learn from fine-tuned smaller models.", "method": "Develop a flipped knowledge distillation paradigm using LoRA to reinterpret LLMs as encoder-decoder models, where the encoder generates representations that align with teacher similarities through the Margin-aware Contrastive Learning approach.", "result": "The proposed method demonstrates improved performance on financial and healthcare benchmarks and has been deployed online, effectively combining the strengths of SLMs and LLMs.", "conclusion": "The flipped knowledge distillation paradigm enables LLMs to achieve better results with the assistance of reasonably good-performing SLMs in various applications.", "key_contributions": ["Flipped knowledge distillation paradigm where LLM learns from SLM", "Introduction of Margin-aware Contrastive Learning for better similarity alignment", "Deployment of model in real-world applications, showcasing effectiveness"], "limitations": "", "keywords": ["knowledge distillation", "Large Language Models", "smaller models", "Contrastive Learning", "healthcare"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.05633", "pdf": "https://arxiv.org/pdf/2507.05633.pdf", "abs": "https://arxiv.org/abs/2507.05633", "title": "SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression", "authors": ["Yiqiao Jin", "Kartik Sharma", "Vineeth Rakesh", "Yingtong Dou", "Menghai Pan", "Mahashweta Das", "Srijan Kumar"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "20 pages", "summary": "Retrieval-augmented Generation (RAG) extends large language models (LLMs)\nwith external knowledge but faces key challenges: restricted effective context\nlength and redundancy in retrieved documents. Pure compression-based approaches\nreduce input size but often discard fine-grained details essential for factual\naccuracy. We propose SARA, a unified RAG framework that balances local\nprecision and global knowledge coverage under tight context budgets. SARA\ncombines natural-language text snippets with semantic compression vectors to\njointly enhance context efficiency and answer correctness. It represents\ncontexts at two complementary levels: 1) fine-grained natural-language spans\nthat preserve critical entities and numerical values, and 2) compact,\ninterpretable vectors that summarize high-level semantics. An iterative\nevidence-selection module employs the compression vectors for dynamic reranking\nof contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families\n(Mistral, Llama, and Gemma), SARA consistently improves answer relevance\n(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),\ndemonstrating the importance of integrating textual and compressed\nrepresentations for robust, context-efficient RAG.", "AI": {"tldr": "SARA is a unified Retrieval-augmented Generation framework that enhances context efficiency and answer correctness by integrating natural-language snippets and semantic compression vectors.", "motivation": "To address challenges in Retrieval-augmented Generation, such as limited context length and redundancy in retrieved documents while maintaining factual accuracy.", "method": "SARA combines fine-grained natural-language spans with compact semantic compression vectors and uses an iterative evidence-selection module for dynamic context reranking.", "result": "SARA achieves significant improvements in answer relevance (+17.71), answer correctness (+13.72), and semantic similarity (+15.53) across multiple datasets and LLM families.", "conclusion": "Integrating textual representations and compressed data enhances the efficiency and reliability of RAG frameworks in retrieving context.", "key_contributions": ["Introduction of a unified RAG framework (SARA) that balances local precision and global knowledge coverage.", "Combination of natural-language snippets and semantic compression vectors.", "Demonstration of significant performance improvements across diverse datasets and LLMs."], "limitations": "", "keywords": ["Retrieval-augmented Generation", "Large Language Models", "Semantic Compression"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2507.05639", "pdf": "https://arxiv.org/pdf/2507.05639.pdf", "abs": "https://arxiv.org/abs/2507.05639", "title": "ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?", "authors": ["Haoxin Wang", "Xianhan Peng", "Xucheng Huang", "Yizhe Huang", "Ming Gong", "Chenghan Yang", "Yang Liu", "Ling Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce ECom-Bench, the first benchmark framework for\nevaluating LLM agent with multimodal capabilities in the e-commerce customer\nsupport domain. ECom-Bench features dynamic user simulation based on persona\ninformation collected from real e-commerce customer interactions and a\nrealistic task dataset derived from authentic e-commerce dialogues. These\ntasks, covering a wide range of business scenarios, are designed to reflect\nreal-world complexities, making ECom-Bench highly challenging. For instance,\neven advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our\nbenchmark, highlighting the substantial difficulties posed by complex\ne-commerce scenarios. Upon publication, the code and data will be open-sourced\nto facilitate further research and development in this domain.", "AI": {"tldr": "ECom-Bench is the first benchmark framework for evaluating LLM agents in e-commerce customer support, featuring dynamic user simulation and realistic task datasets.", "motivation": "To create a robust framework for evaluating LLM agents in the context of e-commerce customer support due to the complexities involved.", "method": "Dynamic user simulation based on real e-commerce customer interactions and authentic dialogue datasets.", "result": "Advanced models like GPT-4o score only 10-20% on the benchmark, indicating significant challenges in the e-commerce scenarios.", "conclusion": "ECom-Bench highlights the need for improved LLM capabilities in complex e-commerce contexts and will be open-sourced for further research.", "key_contributions": ["Introduction of a benchmark for LLM evaluation in e-commerce", "Use of real customer interaction data for task simulation", "Open-sourcing code and datasets for community use"], "limitations": "The benchmark may not cover every possible e-commerce scenario, and results might vary with different models beyond those tested.", "keywords": ["E-commerce", "LLM", "Benchmark", "Customer support", "Multimodal"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.05686", "pdf": "https://arxiv.org/pdf/2507.05686.pdf", "abs": "https://arxiv.org/abs/2507.05686", "title": "Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs", "authors": ["SeungWon Ji", "Jungyup Lee", "Jemin Kim", "Sang Park", "SeungJae Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual large language models (LLMs) often exhibit language confusion, a\ntendency to generate responses in a dominant language irrespective of the\nprompt's language. To address this, we propose Smoothie-Qwen, a lightweight,\npost-hoc method that mitigates language bias without retraining. This technique\nselectively adjusts token-level output probabilities to effectively suppress\nundesired language generation. Applied to the Qwen model, our method reduces\nunintended Chinese output by over 95% while preserving task accuracy on\nmultilingual benchmarks. This work provides a practical and efficient solution\nfor enhancing the language controllability of LLMs, making them more reliable\nfor global applications.", "AI": {"tldr": "Smoothie-Qwen is a method designed to reduce language confusion in multilingual LLMs by adjusting token-level output probabilities without retraining.", "motivation": "Addressing the issue of language confusion in multilingual large language models, which often generate responses in a dominant language.", "method": "Smoothie-Qwen is a lightweight, post-hoc method that selectively adjusts token-level output probabilities to suppress undesired language generation.", "result": "The technique applied to the Qwen model reduces unintended Chinese output by over 95% while maintaining task accuracy on multilingual benchmarks.", "conclusion": "Smoothie-Qwen offers a practical solution for enhancing language controllability in LLMs, improving their reliability in global contexts.", "key_contributions": ["Introduction of a lightweight, post-hoc method to mitigate language bias in multilingual LLMs.", "Demonstrated effectiveness by reducing unintended language output by over 95%.", "Preservation of task accuracy in multilingual settings."], "limitations": "", "keywords": ["multilingual", "large language models", "language controllability", "token-level output", "post-hoc method"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.05707", "pdf": "https://arxiv.org/pdf/2507.05707.pdf", "abs": "https://arxiv.org/abs/2507.05707", "title": "Agentic-R1: Distilled Dual-Strategy Reasoning", "authors": ["Weihua Du", "Pranjal Aggarwal", "Sean Welleck", "Yiming Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint. 15 pages. Project available at\n  https://github.com/StigLidu/DualDistill", "summary": "Current long chain-of-thought (long-CoT) models excel at mathematical\nreasoning but rely on slow and error-prone natural language traces.\nTool-augmented agents address arithmetic via code execution, but often falter\non complex logical tasks. We introduce a fine-tuning framework, DualDistill,\nthat distills complementary reasoning strategies from multiple teachers into a\nunified student model. Using this approach, we train Agentic-R1, which\ndynamically selects the optimal strategy for each query, invoking tools for\narithmetic and algorithmic problems, and using text-based reasoning for\nabstract ones. Our method improves accuracy across a range of tasks, including\nboth computation-intensive and standard benchmarks, demonstrating the\neffectiveness of multi-strategy distillation in achieving robust and efficient\nreasoning. Our project is available at https://github.com/StigLidu/DualDistill", "AI": {"tldr": "A fine-tuning framework, DualDistill, enhances reasoning in long chain-of-thought models by combining multiple reasoning strategies into a single model, Agentic-R1, which adapts its approach based on the task type.", "motivation": "To overcome the limitations of current long-CoT models in mathematical and logical reasoning.", "method": "DualDistill distills complementary reasoning strategies from multiple teacher models into a unified student model, which adapts its strategy depending on the query type.", "result": "Agentic-R1 demonstrates improved accuracy in various computation-intensive and standard benchmarks by selecting optimal strategies for arithmetic and algorithmic problems.", "conclusion": "The multi-strategy distillation approach proves effective for robust and efficient reasoning across diverse tasks.", "key_contributions": ["Introduction of the DualDistill fine-tuning framework", "Development of Agentic-R1 that dynamically selects reasoning strategies", "Demonstration of improved accuracy in both arithmetic and abstract tasks"], "limitations": "", "keywords": ["long chain-of-thought", "multi-strategy reasoning", "fine-tuning framework"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.05713", "pdf": "https://arxiv.org/pdf/2507.05713.pdf", "abs": "https://arxiv.org/abs/2507.05713", "title": "DRAGON: Dynamic RAG Benchmark On News", "authors": ["Fedor Chernogorskii", "Sergei Averkiev", "Liliya Kudraleeva", "Zaven Martirosian", "Maria Tikhonova", "Valentin Malykh", "Alena Fenogenova"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a widely adopted approach for\nimproving the factuality of large language models (LLMs) by incorporating\nexternal knowledge at inference time. Although there exist multiple RAG\nbenchmarks for English, evaluation resources for other languages, including\nRussian, remain scarce and static, failing to capture the dynamic nature of\nreal-world deployments.\n  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first\ndynamic benchmark for evaluating RAG systems in Russian on a changing news\ncorpora. DRAGON is built upon a regularly updated corpus of Russian news and\npublic documents and supports comprehensive evaluation of both the retriever\nand generator components. Question generation is performed automatically with\nthe use of Knowledge Graph constructed from the corpus and enables the\nextraction of four core question types aligned with distinct subgraph patterns.\nWe release a complete evaluation framework comprising the pipeline for\nautomatic question generation, evaluation scripts, which are potentially\nreusable for other languages and multilingual settings, and benchmark data. We\nalso launch a public leaderboard to encourage community participation and\ncomparison.", "AI": {"tldr": "DRAGON is a dynamic benchmark for evaluating Retrieval-Augmented Generation (RAG) systems in Russian, which includes a regularly updated news corpus and supports automatic question generation.", "motivation": "To address the scarcity of RAG evaluation resources for languages other than English, specifically focusing on Russian, by providing a dynamic and updated evaluation framework.", "method": "The benchmark is built on a continually updated corpus of Russian news and public documents, featuring automated question generation using a Knowledge Graph that extracts core question types.", "result": "The DRAGON benchmark allows for comprehensive evaluation of the retriever and generator components of RAG systems, offers reusable evaluation scripts, and features a public leaderboard.", "conclusion": "The framework aims to enhance the evaluation of RAG systems in Russian and support community participation in advancing this domain.", "key_contributions": ["First dynamic benchmark for RAG systems in Russian", "Supports automatic question generation", "Includes a public leaderboard for community engagement"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Russian language", "dynamic benchmark"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2409.15471", "pdf": "https://arxiv.org/pdf/2409.15471.pdf", "abs": "https://arxiv.org/abs/2409.15471", "title": "EvAlignUX: Advancing UX Evaluation through LLM-Supported Metrics Exploration", "authors": ["Qingxiao Zheng", "Minrui Chen", "Pranav Sharma", "Yiliu Tang", "Mehul Oswal", "Yiren Liu", "Yun Huang"], "categories": ["cs.HC"], "comment": null, "summary": "Evaluating UX in the context of AI's complexity, unpredictability, and\ngenerative nature presents unique challenges. How can we support HCI\nresearchers to create comprehensive UX evaluation plans? In this paper, we\nintroduce EvAlignUX, a system powered by large language models and grounded in\nscientific literature, designed to help HCI researchers explore evaluation\nmetrics and their relationship to research outcomes. A user study with 19 HCI\nscholars showed that EvAlignUX improved the perceived quality and confidence in\nUX evaluation plans while prompting deeper consideration of research impact and\nrisks. The system enhanced participants' thought processes, leading to the\ncreation of a ``UX Question Bank'' to guide UX evaluation development. Findings\nalso highlight how researchers' backgrounds influence their inspiration and\nconcerns about AI over-reliance, pointing to future research on AI's role in\nfostering critical thinking. In a world where experience defines impact, we\ndiscuss the importance of shifting UX evaluation from a ``method-centric'' to a\n``mindset-centric'' approach as the key to meaningful and lasting design\nevaluation.", "AI": {"tldr": "EvAlignUX is a system that assists HCI researchers in developing UX evaluation plans using large language models, improving evaluation quality and critical thinking.", "motivation": "The paper addresses challenges in evaluating UX due to AI's complexity and unpredictability, aiming to support HCI researchers in creating comprehensive evaluation plans.", "method": "Introduction of EvAlignUX, a system leveraging large language models to explore evaluation metrics related to research outcomes, followed by a user study with HCI scholars.", "result": "Participants reported improved perceived quality and confidence in their UX evaluation plans, leading to the creation of a 'UX Question Bank' and enhanced thought processes.", "conclusion": "The study suggests a shift from a 'method-centric' to a 'mindset-centric' approach in UX evaluation is necessary for meaningful design evaluation.", "key_contributions": ["Introduction of EvAlignUX for UX evaluation planning", "Creation of a 'UX Question Bank'", "Insights into researchers' concerns about AI over-reliance"], "limitations": "", "keywords": ["UX evaluation", "HCI", "large language models", "design evaluation", "AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.05714", "pdf": "https://arxiv.org/pdf/2507.05714.pdf", "abs": "https://arxiv.org/abs/2507.05714", "title": "HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation", "authors": ["YiHan Jiao", "ZheHao Tan", "Dan Yang", "DuoLin Sun", "Jie Feng", "Jian Wang", "Peng Wei"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has become a fundamental paradigm for\naddressing the challenges faced by large language models in handling real-time\ninformation and domain-specific problems. Traditional RAG systems primarily\nrely on the in-context learning (ICL) capabilities of the large language model\nitself. Still, in-depth research on the specific capabilities needed by the RAG\ngeneration model is lacking, leading to challenges with inconsistent document\nquality and retrieval system imperfections. Even the limited studies that\nfine-tune RAG generative models often \\textit{lack a granular focus on RAG\ntask} or \\textit{a deeper utilization of chain-of-thought processes}. To\naddress this, we propose that RAG models should possess three progressively\nhierarchical abilities (1) Filtering: the ability to select relevant\ninformation; (2) Combination: the ability to combine semantic information\nacross paragraphs; and (3) RAG-specific reasoning: the ability to further\nprocess external knowledge using internal knowledge. Thus, we introduce our new\nRAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning\nRetrieval-Augmented Generation (HIRAG) incorporates a \"think before answering\"\nstrategy. This method enhances the model's open-book examination capability by\nutilizing multi-level progressive chain-of-thought. Experiments show that the\nHIRAG training strategy significantly improves the model's performance on\ndatasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.", "AI": {"tldr": "The paper introduces HIRAG, a new instruction fine-tuning method for RAG models that enhances their reasoning capabilities through a hierarchical approach.", "motivation": "To improve the performance of RAG systems, which face challenges in handling real-time information and domain-specific problems due to inconsistent document quality and retrieval system limitations.", "method": "The proposed method, HIRAG, involves a 'think before answering' strategy that incorporates hierarchical abilities: filtering relevant information, combining semantic information across paragraphs, and RAG-specific reasoning.", "result": "Experiments demonstrate that the HIRAG training strategy significantly boosts the performance of RAG models on various datasets including RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.", "conclusion": "HIRAG enhances the open-book examination capability of RAG models and addresses critical limitations in existing systems.", "key_contributions": ["Introduction of the HIRAG method for RAG instruction fine-tuning", "Establishment of a hierarchical approach for RAG model reasoning", "Demonstration of significant performance improvements across multiple datasets"], "limitations": "", "keywords": ["Retrieval-augmented generation", "Hierarchical reasoning", "Fine-tuning methods"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.05724", "pdf": "https://arxiv.org/pdf/2507.05724.pdf", "abs": "https://arxiv.org/abs/2507.05724", "title": "Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition", "authors": ["Zijin Gu", "Tatiana Likhomanenko", "Navdeep Jaitly"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Mixture-of-experts (MoE) architectures have expanded from language modeling\nto automatic speech recognition (ASR). Traditional MoE methods, such as the\nSwitch Transformer, route experts independently within each layer. Our analysis\nreveals that routers in most layers make expert choices that are not strongly\ncorrelated with the choices of the routers in other layers. To increase the\ncooperation between experts in different layers and encourage greater\nspecialization, we use a shared router across different MoE layers. We call\nthis model \\emph{Omni-router Transformer}. Extensive experiments on a\nlarge-scale pseudo-labeled dataset and evaluations across 10 diverse,\nout-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is\nable to achieve lower training loss and consistently outperform dense and\nSwitch Transformer models, reducing average word error rates by 11.2% and 8.2%,\nrespectively, while providing structured expert usage and improved robustness\nto diverse data.", "AI": {"tldr": "The paper introduces the Omni-router Transformer, a mixture-of-experts model that uses a shared router across layers to improve cooperation and specialization among ASR experts, resulting in reduced word error rates.", "motivation": "To enhance the performance of MoE architectures in automatic speech recognition by increasing expert cooperation across layers and encouraging specialization.", "method": "A shared router is implemented in the Omni-router Transformer that operates across different MoE layers, instead of using independent routers for each layer as in traditional approaches.", "result": "The Omni-router Transformer reduces average word error rates by 11.2% compared to dense models and 8.2% compared to the Switch Transformer, while demonstrating lower training loss and robust performance across diverse ASR benchmarks.", "conclusion": "The Omni-router Transformer model demonstrates superior performance in ASR tasks, indicating the benefits of enhanced cooperation between experts.", "key_contributions": ["Introduction of the Omni-router Transformer model", "Demonstration of improved performance on ASR benchmarks", "Insight into expert cooperation within MoE architectures"], "limitations": "", "keywords": ["Mixture-of-experts", "Omni-router Transformer", "Automatic speech recognition"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2412.16256", "pdf": "https://arxiv.org/pdf/2412.16256.pdf", "abs": "https://arxiv.org/abs/2412.16256", "title": "Aria-UI: Visual Grounding for GUI Instructions", "authors": ["Yuhao Yang", "Yue Wang", "Dongxu Li", "Ziyang Luo", "Bei Chen", "Chao Huang", "Junnan Li"], "categories": ["cs.HC", "cs.AI"], "comment": "ACL 2025", "summary": "Digital agents for automating tasks across different platforms by directly\nmanipulating the GUIs are increasingly important. For these agents, grounding\nfrom language instructions to target elements remains a significant challenge\ndue to reliance on HTML or AXTree inputs. In this paper, we introduce Aria-UI,\na large multimodal model specifically designed for GUI grounding. Aria-UI\nadopts a pure-vision approach, eschewing reliance on auxiliary inputs. To adapt\nto heterogeneous planning instructions, we propose a scalable data pipeline\nthat synthesizes diverse and high-quality instruction samples for grounding. To\nhandle dynamic contexts in task performing, Aria-UI incorporates textual and\ntext-image interleaved action histories, enabling robust context-aware\nreasoning for grounding. Aria-UI sets new state-of-the-art results across\noffline and online agent benchmarks, outperforming both vision-only and\nAXTree-reliant baselines. We release all training data and model checkpoints to\nfoster further research at https://ariaui.github.io.", "AI": {"tldr": "Aria-UI is a multimodal model designed for GUI grounding from language instructions, achieving state-of-the-art performance by using a pure-vision approach and a scalable data pipeline.", "motivation": "There is a growing demand for digital agents that can automate tasks by directly manipulating graphical user interfaces (GUIs), but grounding language instructions to target elements remains a challenge.", "method": "Aria-UI utilizes a pure-vision approach without relying on HTML or AXTree inputs and implements a scalable data pipeline for synthesizing diverse instruction samples. It also incorporates interleaved textual and text-image action histories to enhance context-aware reasoning during task performance.", "result": "Aria-UI achieves new state-of-the-art results in both offline and online benchmarks for agent performance, surpassing previous models that relied on vision-only methods and AXTree inputs.", "conclusion": "The performance improvements of Aria-UI demonstrate the efficacy of a vision-focused approach for GUI grounding, and the resources provided can aid further research in this area.", "key_contributions": ["Introduction of the Aria-UI model for GUI grounding using a pure-vision approach.", "Development of a scalable data pipeline for diverse instruction synthesis.", "Implementation of context-aware reasoning through interleaved action histories."], "limitations": "", "keywords": ["human-computer interaction", "GUI grounding", "multimodal model"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.05740", "pdf": "https://arxiv.org/pdf/2507.05740.pdf", "abs": "https://arxiv.org/abs/2507.05740", "title": "GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge", "authors": ["Yujia Hu", "Tuan-Phong Nguyen", "Shrestha Ghosh", "Moritz Müller", "Simon Razniewski"], "categories": ["cs.CL"], "comment": "7 pages, 6 figures, 1 table", "summary": "Language models are powerful tools, yet their factual knowledge is still\npoorly understood, and inaccessible to ad-hoc browsing and scalable statistical\nanalysis. This demonstration introduces GPTKB v1.5, a densely interlinked\n100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using\nthe GPTKB methodology for massive-recursive LLM knowledge materialization (Hu\net al., ACL 2025). The demonstration experience focuses on three use cases: (1)\nlink-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM\nknowledge querying, (3) comparative exploration of the strengths and weaknesses\nof LLM knowledge. Massive-recursive LLM knowledge materialization is a\ngroundbreaking opportunity both for the research area of systematic analysis of\nLLM knowledge, as well as for automated KB construction. The GPTKB demonstrator\nis accessible at https://gptkb.org.", "AI": {"tldr": "Introduction of GPTKB v1.5, a knowledge base built with LLM technology for improved exploration and querying of LLM knowledge.", "motivation": "To enhance understanding of language models' factual knowledge and provide scalable analysis tools.", "method": "Demonstration of GPTKB v1.5, a 100-million-triple knowledge base, focusing on link-traversal, SPARQL querying, and comparative analysis of LLM knowledge.", "result": "GPTKB v1.5 showcases capabilities in exploring and querying LLM knowledge, highlighting the limitations and strengths of such models.", "conclusion": "Massive-recursive LLM knowledge materialization offers new research opportunities and enhances automated knowledge base construction.", "key_contributions": ["Introduction of a 100-million-triple knowledge base", "Demonstration of three practical use cases", "Advancement in systematic LLM knowledge analysis"], "limitations": "The knowledge base may not cover all aspects of LLM knowledge, and the exploratory methods require user engagement.", "keywords": ["knowledge base", "LLM exploration", "SPARQL querying"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2501.09530", "pdf": "https://arxiv.org/pdf/2501.09530.pdf", "abs": "https://arxiv.org/abs/2501.09530", "title": "Make yourself comfortable: Nudging urban heat and noise mitigation with smartwatch-based Just-in-time Adaptive Interventions (JITAI)", "authors": ["Clayton Miller", "Yun Xuan Chua", "Matias Quintana", "Binyu Lei", "Filip Biljecki", "Mario Frei"], "categories": ["cs.HC"], "comment": null, "summary": "Humans can play a more active role in improving their comfort in the built\nenvironment if given the right information at the right place and time. This\npaper outlines the use of Just-in-Time Adaptive Interventions (JITAI)\nimplemented in the context of the built environment to provide information that\nhelps humans minimize the impact of heat and noise on their daily lives. This\nframework is based on the open-source Cozie iOS smartwatch platform. It\nincludes data collection through micro-surveys and intervention messages\ntriggered by environmental, contextual, and personal history conditions. An\neight-month deployment of the method was completed in Singapore with 103\nparticipants who submitted more than 12,000 micro-surveys and had more than\n3,600 JITAI intervention messages delivered to them. A weekly survey conducted\nduring two deployment phases revealed an overall increase in perceived\nusefulness ranging from 8-19% over the first three weeks of data collection.\nFor noise-related interventions, participants showed an overall increase in\nlocation changes ranging from 4-11% and a 2-17% increase in earphone use to\nmitigate noise distractions. For thermal comfort-related interventions,\nparticipants demonstrated a 3-13\\% increase in adjustments to their location or\nthermostat to feel more comfortable. The analysis found evidence that\npersonality traits (such as conscientiousness), gender, and environmental\npreferences could be factors in determining the perceived helpfulness of JITAIs\nand influencing behavior change. These findings underscore the importance of\ntailoring intervention strategies to individual traits and environmental\nconditions, setting the stage for future research to refine the delivery,\ntiming, and content of intervention messages.", "AI": {"tldr": "The paper discusses implementing Just-in-Time Adaptive Interventions (JITAI) in built environments to enhance comfort through tailored information delivery. An eight-month study in Singapore with 103 participants yielded insights into behavior changes related to noise and thermal comfort.", "motivation": "To improve human comfort in built environments by providing relevant information at appropriate times.", "method": "Utilized the Cozie iOS smartwatch platform for data collection via micro-surveys and delivering JITAI messages based on environmental, contextual, and personal conditions during an eight-month deployment.", "result": "Participants reported increased perceived usefulness and adjusted their behaviors in response to JITAI interventions, including location changes to mitigate noise and adjustments for thermal comfort.", "conclusion": "Tailoring JITAI strategies based on personality traits and environmental preferences is crucial for enhancing their effectiveness.", "key_contributions": ["Development of JITAI framework for built environments", "Empirical evidence from a large-scale deployment", "Insights on individual traits influencing intervention effectiveness"], "limitations": "Further research needed to refine JITAI delivery based on user traits and conditions.", "keywords": ["Just-in-Time Adaptive Interventions", "built environment", "human comfort", "Cozie platform", "behavior change"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.05750", "pdf": "https://arxiv.org/pdf/2507.05750.pdf", "abs": "https://arxiv.org/abs/2507.05750", "title": "DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities", "authors": ["Jing Yang Lee", "Hamed Bonab", "Nasser Zalmout", "Ming Zeng", "Sanket Lokegaonkar", "Colin Lockard", "Binxuan Huang", "Ritesh Sarkhel", "Haodong Wang"], "categories": ["cs.CL"], "comment": "Accepted at SIGDIAL 2025", "summary": "Large Language Models (LLMs) are increasingly employed in multi-turn\nconversational tasks, yet their pre-training data predominantly consists of\ncontinuous prose, creating a potential mismatch between required capabilities\nand training paradigms. We introduce a novel approach to address this\ndiscrepancy by synthesizing conversational data from existing text corpora. We\npresent a pipeline that transforms a cluster of multiple related documents into\nan extended multi-turn, multi-topic information-seeking dialogue. Applying our\npipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training\ndialogue corpus consisting of over 730k long conversations. We hypothesize that\nexposure to such synthesized conversational structures during pre-training can\nenhance the fundamental multi-turn capabilities of LLMs, such as context memory\nand understanding. Empirically, we show that incorporating DocTalk during\npre-training results in up to 40% gain in context memory and understanding,\nwithout compromising base performance. DocTalk is available at\nhttps://huggingface.co/datasets/AmazonScience/DocTalk.", "AI": {"tldr": "This paper introduces DocTalk, a synthesized multi-turn dialogue corpus developed from existing text corpora, aiming to enhance the multi-turn capabilities of Large Language Models (LLMs).", "motivation": "To address the mismatch between the training data of LLMs, which is mainly continuous prose, and the requirements of multi-turn conversational tasks.", "method": "A pipeline is developed to transform clusters of related documents into extended multi-turn, multi-topic dialogues, creating the DocTalk corpus from Wikipedia articles.", "result": "Incorporating the DocTalk corpus into LLM pre-training leads to improvements of up to 40% in context memory and understanding without compromising overall performance.", "conclusion": "The DocTalk corpus offers a valuable resource for improving the conversational capabilities of LLMs in multi-turn dialogues.", "key_contributions": ["Creation of the DocTalk corpus with over 730k conversations", "Demonstration of significant improvements in LLM performance metrics", "Introduction of a novel pipeline for synthesizing dialogue from text corpora"], "limitations": "", "keywords": ["Large Language Models", "multi-turn dialogues", "synthesized conversational data"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.05788", "pdf": "https://arxiv.org/pdf/2507.05788.pdf", "abs": "https://arxiv.org/abs/2507.05788", "title": "Flippi: End To End GenAI Assistant for E-Commerce", "authors": ["Anand A. Rajasekar", "Praveen Tangarajan", "Anjali Nainani", "Amogh Batwal", "Vinay Rao Dandin", "Anusua Trivedi", "Ozan Ersoy"], "categories": ["cs.CL", "I.2.7; H.3.3"], "comment": "10 pages, 2 figures, 7 tables", "summary": "The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace.", "AI": {"tldr": "Flippi is an innovative conversational assistant aimed at enhancing e-commerce experiences by leveraging LLMs for personalized product discovery and comparisons.", "motivation": "To improve user interactions with e-commerce platforms by providing an effective means to navigate and discover products using natural language dialogue.", "method": "Flippi employs advanced NLP techniques, including Query Reformulation, Intent Detection, RAG, NER, and Context Reduction, to interpret customer queries and deliver tailored product information.", "result": "Flippi successfully enables users to find products efficiently, compare options, and make informed purchasing decisions, leading to improved customer satisfaction and engagement in e-commerce.", "conclusion": "Flippi bridges the gap between online shopping convenience and personalized assistance, setting a new benchmark for customer satisfaction in digital marketplaces.", "key_contributions": ["Introduction of an end-to-end conversational assistant for e-commerce", "Utilization of advanced NLP techniques for personalized product discovery", "Robust architecture designed for integration across various e-commerce platforms"], "limitations": "", "keywords": ["Conversational Assistant", "E-commerce", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.05799", "pdf": "https://arxiv.org/pdf/2507.05799.pdf", "abs": "https://arxiv.org/abs/2507.05799", "title": "Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports", "authors": ["Amane Watahiki", "Tomoki Doi", "Taiga Shinozaki", "Satoshi Nishida", "Takuya Niikawa", "Katsunori Miyahara", "Hitomi Yanaka"], "categories": ["cs.CL"], "comment": "To appear in the Proceedings of the 47th Annual Meeting of the\n  Cognitive Science Society (COGSCI 2025)", "summary": "One of the main objectives in developing large vision-language models (LVLMs)\nis to engineer systems that can assist humans with multimodal tasks, including\ninterpreting descriptions of perceptual experiences. A central phenomenon in\nthis context is amodal completion, in which people perceive objects even when\nparts of those objects are hidden. Although numerous studies have assessed\nwhether computer-vision algorithms can detect or reconstruct occluded regions,\nthe inferential abilities of LVLMs on texts related to amodal completion remain\nunexplored. To address this gap, we constructed a benchmark grounded in Basic\nFormal Ontology to achieve a systematic classification of amodal completion.\nOur results indicate that while many LVLMs achieve human-comparable performance\noverall, their accuracy diverges for certain types of objects being completed.\nNotably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet\nexhibit lower accuracy on original images compared to blank stimuli lacking\nvisual content. Intriguingly, this disparity emerges only under Japanese\nprompting, suggesting a deficiency in Japanese-specific linguistic competence\namong these models.", "AI": {"tldr": "This paper investigates the inferential abilities of large vision-language models (LVLMs) in relation to amodal completion, a phenomenon where incomplete objects are perceived based on contextual information.", "motivation": "To explore the gap in research regarding LVLMs' performance on amodal completion tasks, particularly how these models handle occluded information in multimodal contexts.", "method": "The authors developed a benchmark based on Basic Formal Ontology to classify amodal completion and evaluated various LVLMs against it.", "result": "The findings reveal that while many LVLMs perform comparably to humans overall, their accuracy varies for different object types, with certain models showing reduced accuracy in specific conditions, such as when prompted in Japanese.", "conclusion": "The results highlight the need for improved linguistic competence among LVLMs, especially for non-English languages, when addressing tasks related to amodal completion.", "key_contributions": ["Introduction of a benchmark for amodal completion in LVLMs", "Analysis of LVLMs' performance disparities based on object type", "Identification of language-specific deficiencies in LVLMs"], "limitations": "The study primarily focuses on Japanese language prompting which may limit the generalizability of findings to other languages and contexts.", "keywords": ["Vision-Language Models", "Amodal Completion", "Benchmark", "Cognitive Science", "Japanese Linguistic Competence"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.05885", "pdf": "https://arxiv.org/pdf/2507.05885.pdf", "abs": "https://arxiv.org/abs/2507.05885", "title": "How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures", "authors": ["Tanvina Patel", "Wiebke Hutiri", "Aaron Yi Ding", "Odette Scharenborg"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "There is increasingly more evidence that automatic speech recognition (ASR)\nsystems are biased against different speakers and speaker groups, e.g., due to\ngender, age, or accent. Research on bias in ASR has so far primarily focused on\ndetecting and quantifying bias, and developing mitigation approaches. Despite\nthis progress, the open question is how to measure the performance and bias of\na system. In this study, we compare different performance and bias measures,\nfrom literature and proposed, to evaluate state-of-the-art end-to-end ASR\nsystems for Dutch. Our experiments use several bias mitigation strategies to\naddress bias against different speaker groups. The findings reveal that\naveraged error rates, a standard in ASR research, alone is not sufficient and\nshould be supplemented by other measures. The paper ends with recommendations\nfor reporting ASR performance and bias to better represent a system's\nperformance for diverse speaker groups, and overall system bias.", "AI": {"tldr": "The study evaluates performance and bias measures of ASR systems, highlighting the need for comprehensive reporting beyond averaged error rates.", "motivation": "To address bias in automatic speech recognition systems against various speaker groups and improve evaluation methodologies.", "method": "Comparative analysis of different performance and bias measures for ASR systems, along with implementing bias mitigation strategies.", "result": "The study found that sole reliance on averaged error rates is inadequate; other measures are essential for comprehensive evaluation.", "conclusion": "Recommendations are provided for better reporting of ASR performance and bias, aiming to improve representation for diverse speakers.", "key_contributions": ["Comparison of performance and bias measures in ASR", "Implementation of bias mitigation strategies", "Recommendations for better reporting practices in ASR"], "limitations": "", "keywords": ["Automatic Speech Recognition", "Bias", "Performance Measurement", "Bias Mitigation", "Evaluation Methods"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.05890", "pdf": "https://arxiv.org/pdf/2507.05890.pdf", "abs": "https://arxiv.org/abs/2507.05890", "title": "Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators", "authors": ["Sungjib Lim", "Woojung Song", "Eun-Ju Lee", "Yohan Jo"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 7 figures", "summary": "As psychometric surveys are increasingly used to assess the traits of large\nlanguage models (LLMs), the need for scalable survey item generation suited for\nLLMs has also grown. A critical challenge here is ensuring the construct\nvalidity of generated items, i.e., whether they truly measure the intended\ntrait. Traditionally, this requires costly, large-scale human data collection.\nTo make it efficient, we present a framework for virtual respondent simulation\nusing LLMs. Our central idea is to account for mediators: factors through which\nthe same trait can give rise to varying responses to a survey item. By\nsimulating respondents with diverse mediators, we identify survey items that\nrobustly measure intended traits. Experiments on three psychological trait\ntheories (Big5, Schwartz, VIA) show that our mediator generation methods and\nsimulation framework effectively identify high-validity items. LLMs demonstrate\nthe ability to generate plausible mediators from trait definitions and to\nsimulate respondent behavior for item validation. Our problem formulation,\nmetrics, methodology, and dataset open a new direction for cost-effective\nsurvey development and a deeper understanding of how LLMs replicate human-like\nbehavior. We will publicly release our dataset and code to support future work.", "AI": {"tldr": "This paper presents a framework for scalable survey item generation using LLMs, focusing on ensuring construct validity by simulating virtual respondents with diverse mediators.", "motivation": "The need for scalable survey item generation for assessing traits of LLMs and ensuring construct validity without extensive human data collection.", "method": "The proposed framework uses LLMs to simulate respondents with different mediators, which allows for the identification of survey items that robustly measure intended traits.", "result": "Experiments demonstrate that the mediator generation and simulation framework effectively identifies high-validity items across three psychological trait theories (Big5, Schwartz, VIA).", "conclusion": "The work opens a new direction for cost-effective survey development and enhances understanding of LLMs' replication of human-like behavior. The dataset and code will be publicly released.", "key_contributions": ["Framework for virtual respondent simulation using LLMs", "Identification of high-validity survey items", "Insight into how LLMs can replicate human-like behavior"], "limitations": "", "keywords": ["survey item generation", "LLMs", "construct validity", "trait measurement", "virtual respondents"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.05918", "pdf": "https://arxiv.org/pdf/2507.05918.pdf", "abs": "https://arxiv.org/abs/2507.05918", "title": "Few-shot text-based emotion detection", "authors": ["Teodor-George Marchitan", "Claudiu Creanga", "Liviu P. Dinu"], "categories": ["cs.CL"], "comment": null, "summary": "This paper describes the approach of the Unibuc - NLP team in tackling the\nSemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion\nDetection. We mainly focused on experiments using large language models\n(Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With\nour final system, for the multi-label emotion detection track (track A), we got\nan F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36\nteams) for the Portuguese (Mozambican) subset and $0.325$ (\\textbf{1}/31 teams)\nfor the Emakhuwa subset.", "AI": {"tldr": "The Unibuc - NLP team's approach for SemEval 2025 focused on using large language models for text-based emotion detection and achieved notable F1-macro scores across multiple languages.", "motivation": "To advance the state of emotion detection in text by leveraging recent developments in large language models and addressing the challenges posed in multiple languages.", "method": "Experiments were conducted using large language models (Gemini, Qwen, DeepSeek) with techniques such as few-shot prompting and fine-tuning for emotion detection tasks.", "result": "The final system achieved an F1-macro score of 0.7546 for the English subset, 0.1727 for the Portuguese subset, and 0.325 for the Emakhuwa subset in the multi-label emotion detection track.", "conclusion": "The team's approach demonstrated effectiveness in English and highlighted challenges in other languages, suggesting further work is needed for broader applicability.", "key_contributions": ["Implementation of large language models for multi-label emotion detection", "Exploration of few-shot prompting and fine-tuning techniques", "Performance benchmark across multiple languages in emotion detection tasks"], "limitations": "Results are significantly varied across languages, indicating room for improvement in non-English emotion detection.", "keywords": ["text-based emotion detection", "large language models", "multi-label detection"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.05937", "pdf": "https://arxiv.org/pdf/2507.05937.pdf", "abs": "https://arxiv.org/abs/2507.05937", "title": "Towards a Principled Evaluation of Knowledge Editors", "authors": ["Sebastian Pohl", "Max Ploner", "Alan Akbik"], "categories": ["cs.CL"], "comment": "Accepted at L2M2 workshop at ACL 2025", "summary": "Model editing has been gaining increasing attention over the past few years.\nFor Knowledge Editing in particular, more challenging evaluation datasets have\nrecently been released. These datasets use different methodologies to score the\nsuccess of editors. Yet, it remains under-explored how robust these\nmethodologies are and whether they unfairly favor some editors. Moreover, the\ndisruptive impact of these editors on overall model capabilities remains a\nconstant blind spot.\n  We address both of these problems and show that choosing different metrics\nand evaluation methodologies as well as different edit batch sizes can lead to\na different ranking of knowledge editors. Crucially we demonstrate this effect\nalso on general language understanding tasks evaluated alongside the knowledge\nediting tasks. Further we include a manual assessment of the string matching\nbased evaluation method for knowledge editing that is favored by recently\nreleased datasets, revealing a tendency to produce false positive matches.", "AI": {"tldr": "This paper investigates the robustness and fairness of evaluation methodologies for knowledge editing in models, highlighting the impact of different metrics and edit batch sizes.", "motivation": "The growing attention to model editing and knowledge editing demands a thorough evaluation of existing methodologies, particularly in terms of their robustness and fairness towards different editors.", "method": "The paper analyzes various evaluation metrics and methodologies while assessing their effect on the ranking of knowledge editors. It also conducts manual assessments of string matching methods used in knowledge editing evaluations.", "result": "Different metrics and edit batch sizes significantly affect the ranking of knowledge editors, and the favored string matching method tends to yield false positives.", "conclusion": "Evaluation methodologies for knowledge editing need to be critically evaluated for their robustness and fairness, as they can influence the perceived effectiveness of editors.", "key_contributions": ["Analysis of the robustness of knowledge editing evaluation methods.", "Revealing the impact of different metrics and batch sizes on editor rankings.", "Manual assessment showing limitations of commonly used string matching evaluations."], "limitations": "The study primarily focuses on existing evaluation methodologies and may not cover all aspects of knowledge editing efficacy in practice.", "keywords": ["Model Editing", "Knowledge Editing", "Evaluation Methodology", "False Positives", "Language Understanding"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.05939", "pdf": "https://arxiv.org/pdf/2507.05939.pdf", "abs": "https://arxiv.org/abs/2507.05939", "title": "Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors", "authors": ["Bing Wang", "Ximing Li", "Mengzhe Ye", "Changchun Li", "Bo Fu", "Jianfeng Qu", "Lin Yuanbo Wu"], "categories": ["cs.CL", "cs.MM"], "comment": "Accepted by ACM MM 2025. 10 pages, 6 figures. Code:\n  https://github.com/wangbing1416/DAEDCMD", "summary": "Nowadays, misinformation articles, especially multimodal ones, are widely\nspread on social media platforms and cause serious negative effects. To control\ntheir propagation, Multimodal Misinformation Detection (MMD) becomes an active\ntopic in the community to automatically identify misinformation. Previous MMD\nmethods focus on supervising detectors by collecting offline data. However, in\nreal-world scenarios, new events always continually emerge, making MMD models\ntrained on offline data consistently outdated and ineffective. To address this\nissue, training MMD models under online data streams is an alternative,\ninducing an emerging task named continual MMD. Unfortunately, it is hindered by\ntwo major challenges. First, training on new data consistently decreases the\ndetection performance on past data, named past knowledge forgetting. Second,\nthe social environment constantly evolves over time, affecting the\ngeneralization on future data. To alleviate these challenges, we propose to\nremember past knowledge by isolating interference between event-specific\nparameters with a Dirichlet process-based mixture-of-expert structure, and\nanticipate future environmental distributions by learning a continuous-time\ndynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.\nExtensive experiments demonstrate that DAEDCMD can consistently and\nsignificantly outperform the compared methods, including six MMD baselines and\nthree continual learning methods.", "AI": {"tldr": "The paper presents a new continual Multimodal Misinformation Detection method called DAEDCMD, which effectively addresses the challenges of knowledge forgetting and evolving social environments in misinformation detection using a mixture-of-expert structure and a continuous-time dynamics model.", "motivation": "The growing problem of misinformation propagation on social media necessitates advanced methods for Multimodal Misinformation Detection that can adapt to evolving data streams in real-time.", "method": "The proposed approach employs a Dirichlet process-based mixture-of-expert structure to isolate interference between event-specific parameters and utilizes a continuous-time dynamics model to anticipate future environmental distributions.", "result": "DAEDCMD significantly outperforms existing methods, including six baseline MMD methods and three continual learning techniques, in detecting misinformation in the presence of changing data.", "conclusion": "The continual MMD method DAEDCMD is effective in overcoming challenges faced in traditional MMD approaches, thus enhancing the detection performance in dynamic environments.", "key_contributions": ["Introduction of a continual MMD method DAEDCMD.", "Implementation of a Dirichlet process-based mixture-of-expert structure to mitigate past knowledge forgetting.", "Development of a continuous-time dynamics model to improve generalization for future data."], "limitations": "", "keywords": ["Multimodal Misinformation Detection", "continual learning", "mixture-of-experts", "online data streams", "social media"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.05940", "pdf": "https://arxiv.org/pdf/2507.05940.pdf", "abs": "https://arxiv.org/abs/2507.05940", "title": "Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems", "authors": ["Sandeep Mishra", "Anubhab Mandal", "Bishal Santra", "Tushar Abhishek", "Pawan Goyal", "Manish Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Ghosting, the ability to predict a user's intended text input for inline\nquery auto-completion, is an invaluable feature for modern search engines and\nchat interfaces, greatly enhancing user experience. By suggesting completions\nto incomplete queries (or prefixes), ghosting aids users with slow typing\nspeeds, disabilities, or limited language proficiency. Ghosting is a\nchallenging problem and has become more important with the ubiquitousness of\nchat-based systems like ChatGPT, Copilot, etc. Despite the increasing\nprominence of chat-based systems utilizing ghosting, this challenging problem\nof Chat-Ghosting has received little attention from the NLP/ML research\ncommunity. There is a lack of standardized benchmarks and relative performance\nanalysis of deep learning and non-deep learning methods. We address this\nthrough an open and thorough study of this problem using four publicly\navailable dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and\ntwo human-bot (Open Assistant and ShareGPT). We experiment with various\nexisting query auto-completion methods (using tries), n-gram methods and deep\nlearning methods, with and without dialog context. We also propose a novel\nentropy-based dynamic early stopping strategy. Our analysis finds that\nstatistical n-gram models and tries outperform deep learning based models in\nterms of both model performance and inference efficiency for seen prefixes. For\nunseen queries, neural models like T5 and Phi-2 lead to better results. Adding\nconversational context leads to significant improvements in ghosting quality,\nespecially for Open-Assistant and ShareGPT. We make code and data publicly\navailable", "AI": {"tldr": "This paper studies ghosting in text input prediction for chat systems, comparing various auto-completion methods and proposing an entropy-based strategy.", "motivation": "Ghosting enhances user experience by predicting text input, crucial for accessibility and usability in chat interfaces, yet has received little attention in research.", "method": "The study employs four dialog datasets to evaluate existing auto-completion methods, including n-gram and deep learning techniques, and introduces a novel early stopping strategy based on entropy.", "result": "N-gram models outperformed deep learning methods in efficiency for known queries, while advanced neural models performed better with unseen queries when context is included.", "conclusion": "Incorporating conversational context improves ghosting performance considerably; the research provides necessary benchmarks for future studies and publicly available resources.", "key_contributions": ["Comparison of deep learning and non-deep learning methods for ghosting", "Proposal of an entropy-based dynamic early stopping strategy", "Public availability of code and datasets for future research"], "limitations": "Limited to four datasets; may not generalize to all dialog contexts.", "keywords": ["ghosting", "auto-completion", "natural language processing", "machine learning", "dialog systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.05965", "pdf": "https://arxiv.org/pdf/2507.05965.pdf", "abs": "https://arxiv.org/abs/2507.05965", "title": "OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation", "authors": ["Lucas Fonseca Lage", "Simon Ostermann"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to EMNLP 2025 System Demonstrations track", "summary": "We introduce OpenFActScore, an open-source implementation of the FActScore\nframework for evaluating the factuality of text generated by large language\nmodels (LLMs). FActScore evaluates the factual accuracy of long-form text by\nusing Atomic Fact Generation (AFG) to extract individual factual claims and\nAtomic Fact Validation (AFV) to verify each claim against a trusted knowledge\nsource. While the original FActScore relies on closed-source and commercial\nmodels such as InstructGPT and ChatGPT, OpenFActScore enables the use of any\nHugging Face-compatible model for both AFG and AFV. We provide a detailed\ntechnical overview of our implementation, highlighting design choices and\nmodifications made to support open models. We evaluate multiple open-source\nLLMs on both AFG and AFV using the original FActScore benchmark, reporting\nBERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our\nresults show that open models can approximate the performance of closed-source\nsystems, with Gemma achieving the best overall performance, and our final setup\nobtains a 0.99 Pearson correlation with the original FActScore experiments.\nOpenFActScore promotes transparency, reproducibility, and cost-effective\nevaluation, and is available at: https://github.com/lflage/OpenFActScore.", "AI": {"tldr": "OpenFActScore is an open-source framework for evaluating the factual accuracy of text produced by large language models, using Atomic Fact Generation and Validation techniques.", "motivation": "To provide a transparent and open-source method for evaluating the factuality of text generated by LLMs, allowing for broader accessibility and reproducibility in research.", "method": "The framework utilizes Atomic Fact Generation (AFG) to extract factual claims from text and Atomic Fact Validation (AFV) to verify these claims against trusted knowledge sources, using any Hugging Face-compatible model.", "result": "The evaluation shows that open-source models can closely match the performance of closed-source counterparts, with Gemma performing the best and the final setup achieving a 0.99 Pearson correlation with original FActScore benchmarks.", "conclusion": "OpenFActScore promotes transparency in evaluating LLMs and provides a cost-effective alternative to closed-source systems, making it accessible for broader use in the research community.", "key_contributions": ["Introduction of an open-source FActScore framework for LLM evaluation", "Support for any Hugging Face-compatible model for factuality assessment", "Demonstrated efficacy of open models in factual accuracy tasks"], "limitations": "", "keywords": ["FActuality evaluation", "Open-source LLMs", "Atomic Fact Generation", "Hugging Face", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.05973", "pdf": "https://arxiv.org/pdf/2507.05973.pdf", "abs": "https://arxiv.org/abs/2507.05973", "title": "We Should Evaluate Real-World Impact", "authors": ["Ehud Reiter"], "categories": ["cs.CL"], "comment": "This paper will appear in Computational Linguistics journal as a\n  \"Last Word\" opinion piece. The Arxiv version is a pre-MIT Press publication\n  version", "summary": "The ACL community has very little interest in evaluating the real-world\nimpact of NLP systems. A structured survey of the ACL Anthology shows that\nperhaps 0.1% of its papers contain such evaluations; furthermore most papers\nwhich include impact evaluations present them very sketchily and instead focus\non metric evaluations. NLP technology would be more useful and more quickly\nadopted if we seriously tried to understand and evaluate its real-world impact.", "AI": {"tldr": "The paper critiques the lack of real-world impact evaluations in NLP research, noting that only 0.1% of ACL papers address this issue, emphasizing the necessity for a shift towards understanding NLP technology's practical impacts.", "motivation": "To highlight the inadequacy of real-world impact evaluations in the current NLP research literature, particularly within the ACL Anthology.", "method": "Analysis of NLP papers published in the ACL Anthology to assess the extent of real-world impact evaluations provided.", "result": "It was found that only a minuscule percentage (0.1%) of papers included evaluations of their real-world impact, and those that did often presented them incompletely.", "conclusion": "A call to action for the NLP community to prioritize the understanding and evaluation of the real-world impact of their technologies to increase their adoption and usefulness.", "key_contributions": ["NLP research needs to focus more on real-world evaluations.", "Identification of the stark contrast between impact and metric evaluations in NLP papers.", "Encouragement for the ACL community to adopt a more impactful approach to NLP research."], "limitations": "", "keywords": ["NLP", "real-world impact", "ACL Anthology", "effectiveness evaluations", "community standards"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.05980", "pdf": "https://arxiv.org/pdf/2507.05980.pdf", "abs": "https://arxiv.org/abs/2507.05980", "title": "RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages", "authors": ["Gabriel Chua", "Leanne Tan", "Ziyu Ge", "Roy Ka-Wei Lee"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) and their safety classifiers often perform\npoorly on low-resource languages due to limited training data and evaluation\nbenchmarks. This paper introduces RabakBench, a new multilingual safety\nbenchmark localized to Singapore's unique linguistic context, covering\nSinglish, Chinese, Malay, and Tamil. RabakBench is constructed through a\nscalable three-stage pipeline: (i) Generate - adversarial example generation by\naugmenting real Singlish web content with LLM-driven red teaming; (ii) Label -\nsemi-automated multi-label safety annotation using majority-voted LLM labelers\naligned with human judgments; and (iii) Translate - high-fidelity translation\npreserving linguistic nuance and toxicity across languages. The final dataset\ncomprises over 5,000 safety-labeled examples across four languages and six\nfine-grained safety categories with severity levels. Evaluations of 11 popular\nopen-source and closed-source guardrail classifiers reveal significant\nperformance degradation. RabakBench not only enables robust safety evaluation\nin Southeast Asian multilingual settings but also offers a reproducible\nframework for building localized safety datasets in low-resource environments.\nThe benchmark dataset, including the human-verified translations, and\nevaluation code are publicly available.", "AI": {"tldr": "RabakBench is a new multilingual safety benchmark for low-resource languages that focuses on Singapore's linguistic context, revealing performance issues in existing safety classifiers.", "motivation": "To address the poor performance of large language models and their safety classifiers on low-resource languages by creating a specialized benchmark.", "method": "The benchmark is constructed using a three-stage pipeline: 1) Generating adversarial examples with LLMs, 2) Semi-automated labeling using majority-vote LLM labelers, and 3) Translating while preserving linguistic nuance and toxicity.", "result": "The final dataset contains over 5,000 safety-labeled examples across four languages and identifies significant performance degradation in existing classifiers.", "conclusion": "RabakBench facilitates robust safety evaluations in multilingual settings and offers a framework for creating localized safety datasets in low-resource environments.", "key_contributions": ["Introduction of RabakBench for low-resource languages", "Scalable three-stage pipeline for dataset generation", "Public availability of the dataset and evaluation code"], "limitations": "", "keywords": ["multilingual", "safety benchmark", "low-resource languages", "large language models", "Singapore"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.05991", "pdf": "https://arxiv.org/pdf/2507.05991.pdf", "abs": "https://arxiv.org/abs/2507.05991", "title": "Evolution without Large Models: Training Language Model with Task Principles", "authors": ["Minghang Zhu", "Shen Gao", "Zhengliang Shi", "Jiabao Fang", "Pengjie Ren", "Zhaochun Ren", "Zhumin Chen", "Shuo Shang"], "categories": ["cs.CL"], "comment": null, "summary": "A common training approach for language models involves using a large-scale\nlanguage model to expand a human-provided dataset, which is subsequently used\nfor model training.This method significantly reduces training costs by\neliminating the need for extensive human data annotation. However, it still\nfaces challenges such as high carbon emissions during data augmentation and the\nrisk of data leakage when we use closed-source LLMs. To address these issues,\nwe propose a self-evolution method for language models. First, we introduce the\nMulti-level Principle Generation, which enables a large-scale model to\nsummarize task-completion principles based on a small amount of task data.\nThen, we propose the Principle-based Instance Generation, in which a\nsmaller-scale language model uses these task principles to generate a large\namount of data. This data is then used for model training. Experimental results\nshow that our proposed method significantly improves model performance compared\nto directly using a smaller-scale language model to generate data.\nAdditionally, since we only use the large-scale language model to generate the\ntask-completion principles, the carbon emissions associated with training the\nmodel are greatly reduced.", "AI": {"tldr": "The paper proposes a self-evolution method for language models that improves data generation processes while reducing carbon emissions.", "motivation": "To address the challenges of high carbon emissions and data leakage associated with using large-scale language models for data augmentation.", "method": "The method involves Multi-level Principle Generation to summarize task-completion principles and Principle-based Instance Generation where a smaller model generates data based on these principles.", "result": "Experiments show significant improvements in model performance using the proposed method compared to traditional data generation approaches.", "conclusion": "The proposed approach allows for more efficient training with lower environmental impact, utilizing the strengths of both large and small models effectively.", "key_contributions": ["Introduction of Multi-level Principle Generation for summarizing task principles", "Development of Principle-based Instance Generation for efficient data production", "Reduction of carbon emissions in model training processes"], "limitations": "", "keywords": ["language models", "data augmentation", "carbon emissions"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.05997", "pdf": "https://arxiv.org/pdf/2507.05997.pdf", "abs": "https://arxiv.org/abs/2507.05997", "title": "DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations", "authors": ["Nicholas Popovič", "Ashish Kangen", "Tim Schopf", "Michael Färber"], "categories": ["cs.CL"], "comment": null, "summary": "Large, high-quality annotated corpora remain scarce in document-level entity\nand relation extraction in zero-shot or few-shot settings. In this paper, we\npresent a fully automatic, LLM-based pipeline for synthetic data generation and\nin-context learning for document-level entity and relation extraction. In\ncontrast to existing approaches that rely on manually annotated demonstrations\nor direct zero-shot inference, our method combines synthetic data generation\nwith retrieval-based in-context learning, using a reasoning-optimized language\nmodel. This allows us to build a high-quality demonstration database without\nmanual annotation and to dynamically retrieve relevant examples at inference\ntime. Based on our approach we produce a synthetic dataset of over $5k$\nWikipedia abstracts with approximately $59k$ entities and $30k$ relation\ntriples. Finally, we evaluate in-context learning performance on the DocIE\nshared task, extracting entities and relations from long documents in a\nzero-shot setting. We find that in-context joint entity and relation extraction\nat document-level remains a challenging task, even for state-of-the-art large\nlanguage models.", "AI": {"tldr": "A novel LLM-based pipeline for automatic synthetic data generation and in-context learning in document-level entity and relation extraction.", "motivation": "To address the scarcity of high-quality annotated corpora in document-level entity and relation extraction in zero-shot or few-shot settings.", "method": "The proposed method integrates synthetic data generation with retrieval-based in-context learning using a reasoning-optimized language model, allowing for the creation of a database of quality demonstrations without manual annotation.", "result": "Generated a synthetic dataset of over 5,000 Wikipedia abstracts containing approximately 59,000 entities and 30,000 relation triples, and evaluated in-context learning performance on the DocIE shared task, uncovering challenges in zero-shot document-level extraction.", "conclusion": "Even with advanced LLMs, joint entity and relation extraction at the document level poses significant challenges.", "key_contributions": ["Introduction of a fully automatic LLM-based pipeline for synthetic data generation.", "Creation of a large synthetic dataset for document-level entity and relation extraction.", "Evaluation of zero-shot in-context learning performance on the DocIE task."], "limitations": "The study highlights that joint extraction remains complicated even with state-of-the-art models.", "keywords": ["document-level extraction", "synthetic data", "in-context learning", "large language models", "zero-shot learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.06016", "pdf": "https://arxiv.org/pdf/2507.06016.pdf", "abs": "https://arxiv.org/abs/2507.06016", "title": "Conditional Multi-Stage Failure Recovery for Embodied Agents", "authors": ["Youmna Farag", "Svetlana Stoyanchev", "Mohan Li", "Simon Keizer", "Rama Doddipatla"], "categories": ["cs.CL"], "comment": "Accepted at REALM 2025", "summary": "Embodied agents performing complex tasks are susceptible to execution\nfailures, motivating the need for effective failure recovery mechanisms. In\nthis work, we introduce a conditional multistage failure recovery framework\nthat employs zero-shot chain prompting. The framework is structured into four\nerror-handling stages, with three operating during task execution and one\nfunctioning as a post-execution reflection phase. Our approach utilises the\nreasoning capabilities of LLMs to analyse execution challenges within their\nenvironmental context and devise strategic solutions. We evaluate our method on\nthe TfD benchmark of the TEACH dataset and achieve state-of-the-art\nperformance, outperforming a baseline without error recovery by 11.5% and\nsurpassing the strongest existing model by 19%.", "AI": {"tldr": "This paper presents a failure recovery framework for embodied agents that utilizes zero-shot chain prompting to improve execution resilience.", "motivation": "The increasing complexity of tasks for embodied agents raises the risk of execution failures, necessitating robust failure recovery mechanisms.", "method": "The proposed framework comprises four stages for error handling, using reasoning capabilities of large language models (LLMs) to analyze task execution challenges and formulate solutions.", "result": "The framework was evaluated on the TfD benchmark of the TEACH dataset, achieving state-of-the-art performance with an 11.5% improvement over a baseline lacking error recovery and a 19% increase over the best existing model.", "conclusion": "The introduction of a multistage failure recovery process significantly enhances the reliability of embodied agents during complex task execution.", "key_contributions": ["Introduction of a four-stage failure recovery framework", "Utilization of zero-shot chain prompting with LLMs", "Demonstrated state-of-the-art performance on the TEACH dataset"], "limitations": "", "keywords": ["Failure Recovery", "Embodied Agents", "Large Language Models", "Task Execution", "Error Handling"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.06056", "pdf": "https://arxiv.org/pdf/2507.06056.pdf", "abs": "https://arxiv.org/abs/2507.06056", "title": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs", "authors": ["Yizhan Huang", "Zhe Yang", "Meifang Chen", "Jianping Zhang", "Michael R. Lyu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are known to memorize portions of their training\ndata, sometimes reproducing content verbatim when prompted appropriately. In\nthis work, we investigate a fundamental yet under-explored question in the\ndomain of memorization: How to characterize memorization difficulty of training\ndata in LLMs? Through empirical experiments on OLMo, a family of open models,\nwe present the Entropy-Memorization Law. It suggests that data entropy is\nlinearly correlated with memorization score. Moreover, in a case study of\nmemorizing highly randomized strings, or \"gibberish\", we observe that such\nsequences, despite their apparent randomness, exhibit unexpectedly low\nempirical entropy compared to the broader training corpus. Adopting the same\nstrategy to discover Entropy-Memorization Law, we derive a simple yet effective\napproach to distinguish training and testing data, enabling Dataset Inference\n(DI).", "AI": {"tldr": "The paper investigates how to characterize the memorization difficulty of training data in Large Language Models (LLMs) and introduces the Entropy-Memorization Law, highlighting the relationship between data entropy and memorization scores.", "motivation": "To explore the memorization characteristics of training data in LLMs and address how memorization difficulty can be quantified.", "method": "Empirical experiments were conducted on OLMo models to investigate memorization difficulty, leading to the development of the Entropy-Memorization Law that correlates data entropy with memorization scores.", "result": "The study found that the memorization score of training data is linearly correlated with its data entropy. In particular, sequences known as 'gibberish' showed lower empirical entropy than expected, allowing for better differentiation between training and testing data.", "conclusion": "The findings provide a simple and effective method to distinguish between training and testing data based on memorization difficulty using the Entropy-Memorization Law, facilitating Dataset Inference.", "key_contributions": ["Introduction of the Entropy-Memorization Law", "Empirical analysis of memorization difficulty in LLMs", "Method to differentiate training and testing data effectively"], "limitations": "", "keywords": ["Large Language Models", "memorization", "data entropy", "entropy-memorization law", "dataset inference"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.06085", "pdf": "https://arxiv.org/pdf/2507.06085.pdf", "abs": "https://arxiv.org/abs/2507.06085", "title": "A Survey on Prompt Tuning", "authors": ["Zongqian Li", "Yixuan Su", "Nigel Collier"], "categories": ["cs.CL"], "comment": null, "summary": "This survey reviews prompt tuning, a parameter-efficient approach for\nadapting language models by prepending trainable continuous vectors while\nkeeping the model frozen. We classify existing approaches into two categories:\ndirect prompt learning and transfer learning. Direct prompt learning methods\ninclude: general optimization approaches, encoder-based methods, decomposition\nstrategies, and mixture-of-experts frameworks. Transfer learning methods\nconsist of: general transfer approaches, encoder-based methods, and\ndecomposition strategies. For each method, we analyze method designs,\ninnovations, insights, advantages, and disadvantages, with illustrative\nvisualizations comparing different frameworks. We identify challenges in\ncomputational efficiency and training stability, and discuss future directions\nin improving training robustness and broadening application scope.", "AI": {"tldr": "This survey reviews and categorizes existing prompt tuning methods for adapting language models, exploring their designs, advantages, and challenges.", "motivation": "To examine the parameter-efficient method of prompt tuning in language models and identify innovations, advantages, and challenges.", "method": "The paper categorizes prompt tuning methods into direct prompt learning and transfer learning, analyzing various design approaches and frameworks for each category.", "result": "It provides comparative visualizations of different methods and identifies challenges in efficiency and stability in training.", "conclusion": "Future directions for research include improving training robustness and expanding the application scope of prompt tuning in language models.", "key_contributions": ["Classification of prompt tuning methods into direct and transfer learning categories.", "Visual comparisons of different prompt tuning frameworks.", "Identification of challenges in computational efficiency and training stability."], "limitations": "The paper discusses known challenges but may not provide comprehensive solutions.", "keywords": ["prompt tuning", "language models", "parameter-efficient", "transfer learning", "direct prompt learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.06137", "pdf": "https://arxiv.org/pdf/2507.06137.pdf", "abs": "https://arxiv.org/abs/2507.06137", "title": "NeoBabel: A Multilingual Open Tower for Visual Generation", "authors": ["Mohammad Mahdi Derakhshani", "Dheeraj Varghese", "Marzieh Fadaee", "Cees G. M. Snoek"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "34 pages, 12 figures", "summary": "Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI.", "AI": {"tldr": "NeoBabel is a multilingual image generation framework that improves performance, efficiency, and inclusivity, supporting multiple languages while maintaining strong English capabilities.", "motivation": "To address the barriers created by English-centric text-to-image generation systems that perpetuate digital inequities and perform poorly with translation pipelines.", "method": "The model combines large-scale multilingual pretraining and high-resolution instruction tuning, evaluated using expanded multilingual benchmarks m-GenEval and m-DPG.", "result": "NeoBabel achieves state-of-the-art multilingual performance, scoring 0.75 on m-GenEval and 0.68 on m-DPG while being 2-4x smaller than leading models.", "conclusion": "NeoBabel shows that multilingual capability enhances robustness, efficiency, and cultural fidelity in generative AI, and an open toolkit is released to facilitate further research.", "key_contributions": ["Introduced a novel multilingual image generation framework NeoBabel.", "Expanded English-only benchmarks to multilingual equivalents for evaluation.", "Released an open toolkit with model checkpoints and a curated dataset."], "limitations": "", "keywords": ["multilingual image generation", "text-to-image generation", "cultural fidelity"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.06138", "pdf": "https://arxiv.org/pdf/2507.06138.pdf", "abs": "https://arxiv.org/abs/2507.06138", "title": "Coding Triangle: How Does Large Language Model Understand Code?", "authors": ["Taolin Zhang", "Zihan Ma", "Maosong Cao", "Junnan Liu", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models.", "AI": {"tldr": "The paper introduces the Code Triangle framework to evaluate LLMs on code generation, revealing their limitations in diversity and robustness compared to human programmers. The study suggests incorporating human inputs to enhance LLM performance.", "motivation": "To systematically evaluate the programming competence of large language models (LLMs) in order to understand their strengths and weaknesses in code generation tasks.", "method": "Introduces the Code Triangle framework that assesses LLMs across three dimensions: editorial analysis, code implementation, and test case generation, using competitive programming benchmarks for extensive experiments.", "result": "LLMs often lack the diversity and robustness of human-generated solutions, with model errors clustering due to training data biases, highlighting a significant gap between LLM cognition and human expertise.", "conclusion": "Incorporating human-generated materials and model mixtures can improve LLM performance, and understanding LLM cognition can lead to self-improvement in coding models.", "key_contributions": ["Introduction of the Code Triangle framework for evaluating LLMs", "Discovery of significant biases and errors in LLMs compared to humans", "Recommendations for enhancing LLM performance through human inputs and model mixtures"], "limitations": "The study primarily highlights the shortcomings of LLMs without providing an exhaustive solution to all identified issues.", "keywords": ["large language models", "code generation", "editorial analysis", "test case generation", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.06167", "pdf": "https://arxiv.org/pdf/2507.06167.pdf", "abs": "https://arxiv.org/abs/2507.06167", "title": "Skywork-R1V3 Technical Report", "authors": ["Wei Shen", "Jiangbo Pei", "Yi Peng", "Xuchen Song", "Yang Liu", "Jian Peng", "Haofeng Sun", "Yunzhuo Hao", "Peiyu Wang", "Yahui Zhou"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.", "AI": {"tldr": "Skywork-R1V3 is an open-source vision-language model that improves visual reasoning by transferring skills from text-only LLMs through a post-training RL framework.", "motivation": "The paper seeks to enhance visual reasoning capabilities in multimodal reasoning models by utilizing reasoning skills from LLMs.", "method": "An advanced open-source VLM model using reinforcement learning (RL) for post-training to enhance reasoning abilities without additional pre-training.", "result": "Skywork-R1V3 achieved state-of-the-art performance on the MMMU benchmark, improving from 64.3% to 76.0%, indicative of entry-level human capabilities.", "conclusion": "Skywork-R1V3 demonstrates significant advancements in multimodal reasoning and shows that RL can effectively enhance open-source VLMs.", "key_contributions": ["Introduction of the Skywork-R1V3 vision-language model", "Development of a novel post-training RL framework for reasoning skill enhancement", "Identification of entropy of reasoning tokens as a checkpoint selection metric"], "limitations": "", "keywords": ["vision-language model", "reinforcement learning", "multimodal reasoning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.06181", "pdf": "https://arxiv.org/pdf/2507.06181.pdf", "abs": "https://arxiv.org/abs/2507.06181", "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization", "authors": ["Zhongyuan Peng", "Yifan Yao", "Kaijing Ma", "Shuyue Guo", "Yizhe Li", "Yichi Zhang", "Chenchen Zhang", "Yifan Zhang", "Zhouliang Yu", "Luming Li", "Minghao Liu", "Yihang Xia", "Jiawei Shen", "Yuchen Wu", "Yixin Cao", "Zhaoxiang Zhang", "Wenhao Huang", "Jiaheng Liu", "Ge Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Translating natural language mathematical statements into formal, executable\ncode is a fundamental challenge in automated theorem proving. While prior work\nhas focused on generation and compilation success, little attention has been\npaid to the critic phase-the evaluation of whether generated formalizations\ntruly capture the semantic intent of the original problem. In this paper, we\nintroduce CriticLean, a novel critic-guided reinforcement learning framework\nthat elevates the role of the critic from a passive validator to an active\nlearning component. Specifically, first, we propose the CriticLeanGPT, trained\nvia supervised fine-tuning and reinforcement learning, to rigorously assess the\nsemantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,\na benchmark designed to measure models' ability to distinguish semantically\ncorrect from incorrect formalizations, and demonstrate that our trained\nCriticLeanGPT models can significantly outperform strong open- and\nclosed-source baselines. Building on the CriticLean framework, we construct\nFineLeanCorpus, a dataset comprising over 285K problems that exhibits rich\ndomain diversity, broad difficulty coverage, and high correctness based on\nhuman evaluation. Overall, our findings highlight that optimizing the critic\nphase is essential for producing reliable formalizations, and we hope our\nCriticLean will provide valuable insights for future advances in formal\nmathematical reasoning.", "AI": {"tldr": "CriticLean introduces a new framework for evaluating natural language to formal code translation in automated theorem proving, focusing on semantic fidelity with CriticLeanGPT and benchmarking with CriticLeanBench.", "motivation": "To address the gap in evaluating whether generated formalizations capture the semantic intent of natural language mathematical statements in automated theorem proving.", "method": "The authors propose CriticLean, a critic-guided reinforcement learning framework, which includes a model called CriticLeanGPT trained for semantic fidelity assessment, and a benchmark called CriticLeanBench to measure model performance.", "result": "CriticLeanGPT shows significant improvements over several strong baseline models in distinguishing semantically correct from incorrect formalizations, demonstrating the effectiveness of the critic phase optimization.", "conclusion": "The study concludes that enhancing the critic role is crucial for reliable formalizations in automated theorem proving and hopes to inspire further developments in formal mathematical reasoning.", "key_contributions": ["Introduction of CriticLean framework for critic-guided learning", "Development of CriticLeanGPT for evaluating semantic fidelity", "Creation of FineLeanCorpus, a large diverse dataset for problem assessment"], "limitations": "", "keywords": ["automated theorem proving", "semantic fidelity", "reinforcement learning"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.06189", "pdf": "https://arxiv.org/pdf/2507.06189.pdf", "abs": "https://arxiv.org/abs/2507.06189", "title": "DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation", "authors": ["Maximilian Heil", "Dionne Bang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents our submission to Task 1, Subjectivity Detection, of the\nCheckThat! Lab at CLEF 2025. We investigate the effectiveness of\ntransfer-learning and stylistic data augmentation to improve classification of\nsubjective and objective sentences in English news text. Our approach contrasts\nfine-tuning of pre-trained encoders and transfer-learning of fine-tuned\ntransformer on related tasks. We also introduce a controlled augmentation\npipeline using GPT-4o to generate paraphrases in predefined subjectivity\nstyles. To ensure label and style consistency, we employ the same model to\ncorrect and refine the generated samples. Results show that transfer-learning\nof specified encoders outperforms fine-tuning general-purpose ones, and that\ncarefully curated augmentation significantly enhances model robustness,\nespecially in detecting subjective content. Our official submission placed us\n$16^{th}$ of 24 participants. Overall, our findings underscore the value of\ncombining encoder specialization with label-consistent augmentation for\nimproved subjectivity detection. Our code is available at\nhttps://github.com/dsgt-arc/checkthat-2025-subject.", "AI": {"tldr": "This paper explores subjectivity detection in English news text using transfer-learning and stylistic data augmentation, finding that specialized encoders and curated augmentation improve model effectiveness.", "motivation": "The paper aims to improve the classification of subjective and objective sentences in English news articles by leveraging advanced machine learning techniques.", "method": "The authors contrast fine-tuning pre-trained encoders with transfer-learning from fine-tuned transformers and introduce a controlled data augmentation pipeline using GPT-4o for generating paraphrases with consistent subjectivity styles.", "result": "The study found that transfer-learning from specified encoders outperformed fine-tuning general-purpose ones, and that stylistic data augmentation enhanced model robustness in detecting subjective content.", "conclusion": "Combining encoder specialization with label-consistent augmentation significantly improves subjectivity detection capabilities.", "key_contributions": ["Introduction of a controlled augmentation pipeline using GPT-4o", "Demonstration that encoder specialization leads to better performance", "Empirical evidence showing improvement in robustness through curated data augmentation"], "limitations": "", "keywords": ["subjectivity detection", "transfer-learning", "data augmentation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.06195", "pdf": "https://arxiv.org/pdf/2507.06195.pdf", "abs": "https://arxiv.org/abs/2507.06195", "title": "DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification", "authors": ["Maximilian Heil", "Aleksandar Pramov"], "categories": ["cs.CL"], "comment": null, "summary": "Numerical claims, statements involving quantities, comparisons, and temporal\nreferences, pose unique challenges for automated fact-checking systems. In this\nstudy, we evaluate modeling strategies for veracity prediction of such claims\nusing the QuanTemp dataset and building our own evidence retrieval pipeline. We\ninvestigate three key factors: (1) the impact of more evidences with longer\ninput context windows using ModernBERT, (2) the effect of right-to-left (R2L)\ntokenization, and (3) their combined influence on classification performance.\nContrary to prior findings in arithmetic reasoning tasks, R2L tokenization does\nnot boost natural language inference (NLI) of numerical tasks. A longer context\nwindow does also not enhance veracity performance either, highlighting evidence\nquality as the dominant bottleneck. Our best-performing system achieves\ncompetitive macro-average F1 score of 0.57 and places us among the Top-4\nsubmissions in Task 3 of CheckThat! 2025. Our code is available at\nhttps://github.com/dsgt-arc/checkthat-2025-numerical.", "AI": {"tldr": "This study investigates modeling strategies for automated fact-checking of numerical claims, emphasizing the importance of evidence quality over context length and tokenization methods.", "motivation": "To address the challenges posed by numerical claims in automated fact-checking systems.", "method": "Evaluation of the QuanTemp dataset and a custom evidence retrieval pipeline, testing the effects of context length and right-to-left tokenization on veracity prediction.", "result": "The best system achieved a macro-average F1 score of 0.57, ranking among the Top-4 submissions in Task 3 of CheckThat! 2025, with findings showing evidence quality as a critical factor.", "conclusion": "Longer context windows and R2L tokenization do not significantly improve classification performance; enhancing evidence quality is essential for better veracity predictions.", "key_contributions": ["Development of an evidence retrieval pipeline for numerical claims", "Evaluation of R2L tokenization effects on natural language inference", "Demonstration of the importance of evidence quality in classification tasks"], "limitations": "", "keywords": ["automated fact-checking", "numerical claims", "veracity prediction", "evidence retrieval", "natural language inference"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.06196", "pdf": "https://arxiv.org/pdf/2507.06196.pdf", "abs": "https://arxiv.org/abs/2507.06196", "title": "UQLM: A Python Package for Uncertainty Quantification in Large Language Models", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan", "David Skarbrevik", "Ho-Kyeong Ra", "Viren Bajaj", "Zeya Ahmad"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to Journal of Machine Learning Research (MLOSS); UQLM\n  Repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations, defined as instances where Large Language Models (LLMs)\ngenerate false or misleading content, pose a significant challenge that impacts\nthe safety and trust of downstream applications. We introduce UQLM, a Python\npackage for LLM hallucination detection using state-of-the-art uncertainty\nquantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers\nthat compute response-level confidence scores ranging from 0 to 1. This library\nprovides an off-the-shelf solution for UQ-based hallucination detection that\ncan be easily integrated to enhance the reliability of LLM outputs.", "AI": {"tldr": "UQLM is a Python package designed for detecting hallucinations in Large Language Models (LLMs) using uncertainty quantification techniques.", "motivation": "To address the challenge of hallucinations in LLMs, which generate false or misleading content, impacting the safety and trust of applications.", "method": "The paper presents UQLM, offering UQ-based scorers that compute confidence scores for LLM responses, facilitating easy integration into existing systems.", "result": "UQLM provides an off-the-shelf solution for hallucination detection, improving the reliability of LLM outputs with quantifiable confidence metrics.", "conclusion": "The toolkit enhances trust in LLM-generated content by providing measurable uncertainty, thereby aiding developers in building safer applications.", "key_contributions": ["Introduction of the UQLM package for hallucination detection", "Implementation of state-of-the-art uncertainty quantification techniques", "Easy integration for enhancing LLM reliability"], "limitations": "", "keywords": ["Large Language Models", "hallucination detection", "uncertainty quantification", "Python", "toolkit"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.06203", "pdf": "https://arxiv.org/pdf/2507.06203.pdf", "abs": "https://arxiv.org/abs/2507.06203", "title": "A Survey on Latent Reasoning", "authors": ["Rui-Jie Zhu", "Tianhao Peng", "Tianhao Cheng", "Xingwei Qu", "Jinfa Huang", "Dawei Zhu", "Hao Wang", "Kaiwen Xue", "Xuanliang Zhang", "Yong Shan", "Tianle Cai", "Taylor Kergan", "Assel Kembay", "Andrew Smith", "Chenghua Lin", "Binh Nguyen", "Yuqi Pan", "Yuhong Chou", "Zefan Cai", "Zhenhe Wu", "Yongchi Zhao", "Tianyu Liu", "Jian Yang", "Wangchunshu Zhou", "Chujie Zheng", "Chongxuan Li", "Yuyin Zhou", "Zhoujun Li", "Zhaoxiang Zhang", "Jiaheng Liu", "Ge Zhang", "Wenhao Huang", "Jason Eshraghian"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.", "AI": {"tldr": "This paper surveys the emerging field of latent reasoning in Large Language Models (LLMs), focusing on methodologies that allow multi-step inference in the model's hidden state without token-level supervision.", "motivation": "To address the limitations of chain-of-thought reasoning in LLMs, particularly its dependence on natural language, by exploring latent reasoning that operates in the model's hidden state.", "method": "The survey examines foundational neural network layers, diverse latent reasoning methodologies including activation-based recurrence and hidden state propagation, and advanced paradigms like infinite-depth latent reasoning using masked diffusion models.", "result": "The paper clarifies the conceptual landscape of latent reasoning and provides insights into future research directions for enhancing LLM cognition.", "conclusion": "The unification of various latent reasoning methodologies could lead to improved reasoning capabilities in LLMs and encourages further exploration in this area.", "key_contributions": ["Comprehensive overview of latent reasoning methodologies in LLMs", "Introduction to advanced paradigms like masked diffusion models", "Discussion on the role of neural network layers in supporting reasoning transformations."], "limitations": "", "keywords": ["Large Language Models", "Latent Reasoning", "Chain-of-Thought", "Neural Networks", "Cognition"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.06205", "pdf": "https://arxiv.org/pdf/2507.06205.pdf", "abs": "https://arxiv.org/abs/2507.06205", "title": "DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media", "authors": ["Ayush Parikh", "Hoang Thanh Thanh Truong", "Jeanette Schofield", "Maximilian Heil"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a\nScientific Web Discourse Detection, present the methods we explored for this\ntask. For this multiclass classification task, we determined if a tweet\ncontained a scientific claim, a reference to a scientific study or publication,\nand/or mentions of scientific entities, such as a university or a scientist. We\npresent 3 modeling approaches for this task: transformer finetuning, few-shot\nprompting of LLMs, and a combined ensemble model whose design was informed by\nearlier experiments. Our team placed 7th in the competition, achieving a\nmacro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline\nof 0.8375. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.", "AI": {"tldr": "This paper presents methods for detecting scientific web discourse in tweets, focusing on classifying mentions of scientific claims, studies, and entities.", "motivation": "To explore effective methods for detecting scientific discourse in social media and improve information accuracy in public communication of science.", "method": "The paper discusses three modeling approaches: transformer fine-tuning, few-shot prompting of LLMs, and a combined ensemble model informed by earlier experiments.", "result": "The team achieved a macro-averaged F1 score of 0.8611, surpassing the DeBERTaV3 baseline score of 0.8375, and placed 7th in the competition.", "conclusion": "The proposed methods demonstrate effective detection of scientific claims on social media, suggesting promising avenues for future research in this area.", "key_contributions": ["Development of multiple modeling approaches for scientific discourse detection", "Improvement over baseline performance in a competitive setting", "Open-source codebase for future research and development"], "limitations": "", "keywords": ["Scientific Claims Detection", "LLM", "Social Media", "Machine Learning", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.06223", "pdf": "https://arxiv.org/pdf/2507.06223.pdf", "abs": "https://arxiv.org/abs/2507.06223", "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers", "authors": ["Zhiyuan Peng", "Ting-ruen Wei", "Tingyu Song", "Yilun Zhao", "Yi Fang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review", "summary": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community.", "AI": {"tldr": "The paper proposes new metrics for evaluating the efficiency of LLM-based rerankers in information retrieval, focusing on compute effectiveness and throughput, while also introducing an estimator for FLOPs.", "motivation": "To improve the evaluation of LLM-based rerankers by providing metrics that account for model size and hardware differences, enabling better understanding of efficiency-effectiveness trade-offs.", "method": "The authors propose E^2R-FLOPs metrics, which include ranking metrics per PetaFLOP (RPP) and queries per PetaFLOP (QPP), along with a FLOPs estimator for LLM-based rerankers that does not require experimental runs.", "result": "Using the introduced metrics, extensive experiments were performed to evaluate various LLM-based rerankers, revealing insights into their efficiency-effectiveness balance.", "conclusion": "The proposed metrics and FLOPs estimator can significantly aid the research community in accurately assessing the performance of LLM-based rerankers, ultimately advancing the field of information retrieval.", "key_contributions": ["Introduction of E^2R-FLOPs metrics for better evaluation of LLM-based rerankers", "Development of a FLOPs estimator for LLM-based rerankers without experimental validation", "Comprehensive experiments demonstrating the effectiveness of the new metrics"], "limitations": "The study relies on certain assumptions regarding the architectural diversity of LLMs and may not cover all practical scenarios.", "keywords": ["Large Language Models", "information retrieval", "efficiency-effectiveness tradeoff", "FLOPs estimator", "reranker"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.06229", "pdf": "https://arxiv.org/pdf/2507.06229.pdf", "abs": "https://arxiv.org/abs/2507.06229", "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving", "authors": ["Xiangru Tang", "Tianrui Qin", "Tianhao Peng", "Ziyang Zhou", "Daniel Shao", "Tingting Du", "Xinming Wei", "Peng Xia", "Fang Wu", "He Zhu", "Ge Zhang", "Jiaheng Liu", "Xingyao Wang", "Sirui Hong", "Chenglin Wu", "Hao Cheng", "Chi Wang", "Wangchunshu Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As language agents tackle increasingly complex tasks, they struggle with\neffective error correction and experience reuse across domains. We introduce\nAgent KB, a hierarchical experience framework that enables complex agentic\nproblem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses\na core limitation: agents traditionally cannot learn from each other's\nexperiences. By capturing both high-level strategies and detailed execution\nlogs, Agent KB creates a shared knowledge base that enables cross-agent\nknowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success\nrates by up to 16.28 percentage points. On the most challenging tasks, Claude-3\nimproves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on\nintermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to\nimprove from 41.33% to 53.33%. Our results suggest that Agent KB provides a\nmodular, framework-agnostic infrastructure for enabling agents to learn from\npast experiences and generalize successful strategies to new tasks.", "AI": {"tldr": "Agent KB is a hierarchical experience framework for language agents that improves error correction and experience reuse, enhancing performance on complex tasks.", "motivation": "To address the limitations of language agents that cannot learn from each other's experiences, thus improving their problem-solving capabilities in complex tasks.", "method": "The study introduces a Reason-Retrieve-Refine pipeline within Agent KB that captures both high-level strategies and execution logs to facilitate cross-agent knowledge transfer.", "result": "Agent KB significantly improves success rates on the GAIA benchmark, with Claude-3 improving from 38.46% to 57.69% and GPT-4 from 53.49% to 73.26%. On SWE-bench, it helps Claude-3 improve from 41.33% to 53.33%.", "conclusion": "Agent KB offers a modular, framework-agnostic infrastructure for agents to learn from past experiences and generalize effective strategies.", "key_contributions": ["Introduction of the Reason-Retrieve-Refine pipeline", "Creation of a shared knowledge base for cross-agent learning", "Demonstrated significant performance improvements on benchmark tasks"], "limitations": "", "keywords": ["experience reuse", "language agents", "knowledge transfer"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2211.14620", "pdf": "https://arxiv.org/pdf/2211.14620.pdf", "abs": "https://arxiv.org/abs/2211.14620", "title": "The distribution of syntactic dependency distances", "authors": ["Sonia Petrini", "Ramon Ferrer-i-Cancho"], "categories": ["cs.CL"], "comment": "in press in Glottometrics", "summary": "The syntactic structure of a sentence can be represented as a graph, where\nvertices are words and edges indicate syntactic dependencies between them. In\nthis setting, the distance between two linked words is defined as the\ndifference between their positions. Here we wish to contribute to the\ncharacterization of the actual distribution of syntactic dependency distances,\nwhich has previously been argued to follow a power-law distribution. Here we\npropose a new model with two exponential regimes in which the probability decay\nis allowed to change after a break-point. This transition could mirror the\ntransition from the processing of word chunks to higher-level structures. We\nfind that a two-regime model - where the first regime follows either an\nexponential or a power-law decay - is the most likely one in all 20 languages\nwe considered, independently of sentence length and annotation style. Moreover,\nthe break-point exhibits low variation across languages and averages values of\n4-5 words, suggesting that the amount of words that can be simultaneously\nprocessed abstracts from the specific language to a high degree. The\nprobability decay slows down after the breakpoint, consistently with a\nuniversal chunk-and-pass mechanism. Finally, we give an account of the relation\nbetween the best estimated model and the closeness of syntactic dependencies as\nfunction of sentence length, according to a recently introduced optimality\nscore.", "AI": {"tldr": "This paper proposes a new model for characterizing syntactic dependency distances in sentences, demonstrating that a two-regime model best fits various languages.", "motivation": "To characterize the distribution of syntactic dependency distances, which has been suggested to follow a power-law distribution, and to propose a model that captures this behavior.", "method": "A new two-regime model is introduced where the probability decay of syntactic dependency distances changes after a breakpoint, analyzed across 20 languages with varying sentence lengths and annotation styles.", "result": "The two-regime model is favored across all languages studied, with a break-point averaging 4-5 words, suggesting that similar processing limits exist across different languages.", "conclusion": "The findings support a universal processing mechanism for word chunks to higher structures, showing consistent properties of syntactic dependencies across languages.", "key_contributions": ["Proposed a new two-regime model for syntactic dependency distances.", "Showed that the breakpoint in the dependency distance consistently averages 4-5 words across languages.", "Related the model to a universal processing mechanism for syntactic dependencies."], "limitations": "", "keywords": ["syntactic structure", "dependency distances", "two-regime model"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2312.08968", "pdf": "https://arxiv.org/pdf/2312.08968.pdf", "abs": "https://arxiv.org/abs/2312.08968", "title": "Detecting value-expressive text posts in Russian social media", "authors": ["Maria Milkova", "Maksim Rudnev", "Lidia Okolskaya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Basic values are concepts or beliefs which pertain to desirable end-states\nand transcend specific situations. Studying personal values in social media can\nilluminate how and why societal values evolve especially when the stimuli-based\nmethods, such as surveys, are inefficient, for instance, in hard-to-reach\npopulations. On the other hand, user-generated content is driven by the massive\nuse of stereotyped, culturally defined speech constructions rather than\nauthentic expressions of personal values. We aimed to find a model that can\naccurately detect value-expressive posts in Russian social media VKontakte. A\ntraining dataset of 5,035 posts was annotated by three experts, 304\ncrowd-workers and ChatGPT. Crowd-workers and experts showed only moderate\nagreement in categorizing posts. ChatGPT was more consistent but struggled with\nspam detection. We applied an ensemble of human- and AI-assisted annotation\ninvolving active learning approach, subsequently trained several classification\nmodels using embeddings from various pre-trained transformer-based language\nmodels. The best performance was achieved with embeddings from a fine-tuned\nrubert-tiny2 model, yielding high value detection quality (F1 = 0.75, F1-macro\n= 0.80). This model provides a crucial step to a study of values within and\nbetween Russian social media users.", "AI": {"tldr": "This paper investigates how to detect value-expressive content in Russian social media using an AI-assisted approach to improve upon traditional survey methods.", "motivation": "The study aims to understand societal values through personal values expressed on social media, particularly in populations that are hard to reach through traditional surveys.", "method": "A model was developed to detect value-expressive posts in VKontakte using a training dataset of 5,035 annotated posts, combining human experts and AI (ChatGPT) for annotation and employing various transformer-based language model embeddings for classification.", "result": "The best-performing model was based on the fine-tuned rubert-tiny2 model, achieving value detection quality with an F1 score of 0.75 and F1-macro of 0.80.", "conclusion": "This work advances the detection of values expressed on Russian social media, providing insights into the evolution of societal values.", "key_contributions": ["Development of a model for detecting value-expressive posts on VKontakte", "Integration of human and AI-assisted annotation for improved classification", "Achievement of high detection quality using transformer-based embeddings."], "limitations": "Moderate agreement among human annotators and challenges in spam detection by ChatGPT.", "keywords": ["social media", "value detection", "machine learning", "transformer models", "natural language processing"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2403.04945", "pdf": "https://arxiv.org/pdf/2403.04945.pdf", "abs": "https://arxiv.org/abs/2403.04945", "title": "MEIT: Multimodal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation", "authors": ["Zhongwei Wan", "Che Liu", "Xin Wang", "Chaofan Tao", "Hui Shen", "Jing Xiong", "Rossella Arcucci", "Huaxiu Yao", "Mi Zhang"], "categories": ["cs.CL", "cs.LG", "eess.SP"], "comment": "ACL 2025", "summary": "Electrocardiogram (ECG) is the primary non-invasive diagnostic tool for\nmonitoring cardiac conditions and is crucial in assisting clinicians. Recent\nstudies have concentrated on classifying cardiac conditions using ECG data but\nhave overlooked ECG report generation, which is time-consuming and requires\nclinical expertise. To automate ECG report generation and ensure its\nversatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework,\nthe first attempt to tackle ECG report generation with LLMs and multimodal\ninstructions. To facilitate future research, we establish a benchmark to\nevaluate MEIT with various LLMs backbones across two large-scale ECG datasets.\nOur approach uniquely aligns the representations of the ECG signal and the\nreport, and we conduct extensive experiments to benchmark MEIT with nine\nopen-source LLMs using more than 800,000 ECG reports. MEIT's results underscore\nthe superior performance of instruction-tuned LLMs, showcasing their\nproficiency in quality report generation, zero-shot capabilities, resilience to\nsignal perturbation, and alignment with human expert evaluation. These findings\nemphasize the efficacy of MEIT and its potential for real-world clinical\napplication.", "AI": {"tldr": "The paper proposes the Multimodal ECG Instruction Tuning (MEIT) framework to automate ECG report generation using LLMs, establishing benchmarks for evaluation.", "motivation": "To automate the time-consuming process of ECG report generation, which requires clinical expertise.", "method": "A framework called Multimodal ECG Instruction Tuning (MEIT) was proposed, evaluated with various LLMs across two large ECG datasets using over 800,000 reports.", "result": "MEIT showcases superior performance in quality report generation and resilience to signal perturbation, demonstrating effectiveness in real-world clinical applications.", "conclusion": "The findings highlight the potential of LLMs in automating ECG report generation and improving clinical workflows.", "key_contributions": ["Introduction of the Multimodal ECG Instruction Tuning (MEIT) framework", "Establishing a benchmark for evaluating LLMs in ECG report generation", "Demonstrating the effectiveness of LLMs in clinical applications for report generation"], "limitations": "", "keywords": ["ECG report generation", "LLM", "multimodal instructions", "clinical application", "automated report generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2406.06641", "pdf": "https://arxiv.org/pdf/2406.06641.pdf", "abs": "https://arxiv.org/abs/2406.06641", "title": "News and Load: Social and Economic Drivers of Regional Multi-horizon Electricity Demand Forecasting", "authors": ["Yun Bai", "Simon Camal", "Andrea Michiorri"], "categories": ["cs.CL", "cs.LG"], "comment": "12 pages, 12 figures", "summary": "The relationship between electricity demand and variables such as economic\nactivity and weather patterns is well established. However, this paper explores\nthe connection between electricity demand and social aspects. It further embeds\ndynamic information about the state of society into energy demand modelling and\nforecasting approaches. Through the use of natural language processing on a\nlarge news corpus, we highlight this important link. This study is conducted in\nfive regions of the UK and Ireland and considers multiple time horizons from 1\nto 30 days. It also considers economic variables such as GDP, unemployment and\ninflation. The textual features used in this study represent central constructs\nfrom the word frequencies, topics, word embeddings extracted from the news. The\nfindings indicate that: 1) the textual features are related to various\ncontents, such as military conflicts, transportation, the global pandemic,\nregional economics, and the international energy market. They exhibit causal\nrelationships with regional electricity demand, which are validated using\nGranger causality and Double Machine Learning methods. 2) Economic indicators\nplay a more important role in the East Midlands and Northern Ireland, while\nsocial indicators are more influential in the West Midlands and the South West\nof England. 3) The use of these factors improves deterministic forecasting by\naround 6%.", "AI": {"tldr": "This paper investigates the influence of social factors on electricity demand, integrating natural language processing to analyze news data alongside economic variables.", "motivation": "To deepen the understanding of how social aspects impact electricity demand and improve forecasting methods.", "method": "Utilized natural language processing on a large corpus of news articles, combined with economic variables such as GDP, unemployment, and inflation, across five regions in the UK and Ireland.", "result": "Textual features extracted from news data show causal relationships with regional electricity demand, with significant regional variations in influence from social versus economic factors.", "conclusion": "Incorporating social indicators into electricity demand forecasting enhances accuracy by about 6%, with varying levels of influence depending on the region studied.", "key_contributions": ["Integration of social factors into energy demand models using NLP", "Demonstrated causal relationships between textual features and electricity demand", "Improved forecasting accuracy with the inclusion of social indicators"], "limitations": "", "keywords": ["electricity demand", "natural language processing", "social indicators", "economic factors", "forecasting"], "importance_score": 4, "read_time_minutes": 12}}
{"id": "2406.14459", "pdf": "https://arxiv.org/pdf/2406.14459.pdf", "abs": "https://arxiv.org/abs/2406.14459", "title": "Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models", "authors": ["Shijie Han", "Zhenyu Zhang", "Andrei Arsene Simion"], "categories": ["cs.CL"], "comment": null, "summary": "Language models like BERT excel at sentence classification tasks due to\nextensive pre-training on general data, but their robustness to parameter\ncorruption is unexplored. To understand this better, we look at what happens if\na language model is \"broken\", in the sense that some of its parameters are\ncorrupted and then recovered by fine-tuning. Strategically corrupting BERT\nvariants at different levels, we find corrupted models struggle to fully\nrecover their original performance, with higher corruption causing more severe\ndegradation. Notably, bottom-layer corruption affecting fundamental linguistic\nfeatures is more detrimental than top-layer corruption. Our insights contribute\nto understanding language model robustness and adaptability under adverse\nconditions, informing strategies for developing resilient NLP systems against\nparameter perturbations.", "AI": {"tldr": "This paper investigates the robustness of BERT and its variants to parameter corruption and recovery through fine-tuning.", "motivation": "To understand how parameter corruption affects the performance of language models like BERT and to inform the development of resilient NLP systems.", "method": "The authors strategically corrupt BERT variants at different levels and analyze their performance recovery through fine-tuning.", "result": "Corrupted models struggle to recover their original performance, with more severe degradation resulting from higher levels of corruption. Bottom-layer corruption is more detrimental than top-layer corruption.", "conclusion": "The study provides insights into language model robustness and adaptability, highlighting the importance of foundational linguistic features in maintaining model performance.", "key_contributions": ["Exploration of parameter corruption and recovery in BERT variants.", "Finding that bottom-layer corruption is more detrimental than top-layer corruption.", "Insights into developing resilient NLP systems."], "limitations": "The study primarily focuses on BERT variants and may not generalize to all language models.", "keywords": ["language models", "BERT", "parameter corruption", "robustness", "NLP systems"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2408.08971", "pdf": "https://arxiv.org/pdf/2408.08971.pdf", "abs": "https://arxiv.org/abs/2408.08971", "title": "A Multi-Task and Multi-Label Classification Model for Implicit Discourse Relation Recognition", "authors": ["Nelson Filipe Costa", "Leila Kosseim"], "categories": ["cs.CL"], "comment": "Accepted at SIGDIAL 2025", "summary": "We propose a novel multi-label classification approach to implicit discourse\nrelation recognition (IDRR). Our approach features a multi-task model that\njointly learns multi-label representations of implicit discourse relations\nacross all three sense levels in the PDTB 3.0 framework. The model can also be\nadapted to the traditional single-label IDRR setting by selecting the sense\nwith the highest probability in the multi-label representation. We conduct\nextensive experiments to identify optimal model configurations and loss\nfunctions in both settings. Our approach establishes the first benchmark for\nmulti-label IDRR and achieves SOTA results on single-label IDRR using DiscoGeM.\nFinally, we evaluate our model on the PDTB 3.0 corpus in the single-label\nsetting, presenting the first analysis of transfer learning between the\nDiscoGeM and PDTB 3.0 corpora for IDRR.", "AI": {"tldr": "A novel multi-label classification approach for implicit discourse relation recognition (IDRR), establishing a benchmark and achieving state-of-the-art results in single-label IDRR.", "motivation": "To improve implicit discourse relation recognition using a multi-label approach, and to establish a benchmark for this task.", "method": "A multi-task model that learns multi-label representations of implicit discourse relations based on the PDTB 3.0 framework, with adaptations for single-label settings.", "result": "The proposed model sets the first benchmark for multi-label IDRR and achieves state-of-the-art results in single-label IDRR when evaluated on the PDTB 3.0 corpus.", "conclusion": "The model demonstrates the effectiveness of multi-label classification for IDRR and provides insights into transfer learning between different corpora.", "key_contributions": ["Establishes a benchmark for multi-label implicit discourse relation recognition (IDRR).", "Achieves state-of-the-art results for traditional single-label IDRR using the DiscoGeM model.", "Presents the first analysis of transfer learning between the DiscoGeM and PDTB 3.0 corpora for IDRR."], "limitations": "", "keywords": ["multi-label classification", "discourse relation recognition", "PDTB 3.0", "transfer learning", "NLP"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2409.17172", "pdf": "https://arxiv.org/pdf/2409.17172.pdf", "abs": "https://arxiv.org/abs/2409.17172", "title": "What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on Curiosity-Driven Questioning", "authors": ["Shashidhar Reddy Javaji", "Zining Zhu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) can store a massive amount of knowledge, yet\ntheir potential to acquire new knowledge remains unknown. We propose a novel\nevaluation framework that evaluates this capability. This framework prompts\nLLMs to generate questions about a statement introducing scientific knowledge,\nsimulating a curious person when facing the statement for the first time. We\nscore the qualities of the generated questions, thereby evaluating the\nknowledge acquisition potential of the LLM. We apply controlled ablation\nstudies to validate our scoring procedures. Additionally, we created a\nsynthetic dataset consisting of 1101 statements in physics, chemistry, and\nmaths with distinct levels of difficulties, 300 general knowledge statements,\nand 567 incorrect statements. Human evaluations were conducted to validate our\nmodel assessments, achieving an approximate weighted Cohen's kappa of 0.7 on\nall three metrics considered. We find that while large models like GPT-4 and\nMistral 8x7b are adept at generating coherent and relevant questions, the\nsmaller Phi-2 model is equally or more effective. This indicates that size does\nnot solely determine a model's knowledge acquisition potential. The proposed\nframework quantifies a critical model capability that was commonly overlooked\nand opens up research opportunities for developing more knowledgeable AI\nsystems", "AI": {"tldr": "This paper introduces a novel evaluation framework for assessing large language models' (LLMs) ability to acquire new knowledge by generating questions from provided scientific statements.", "motivation": "To understand LLMs' potential for knowledge acquisition, which has been largely unexplored.", "method": "The framework prompts LLMs to generate questions about scientific statements and scores the quality of those questions. Controlled ablation studies were utilized for validation, and a synthetic dataset across various disciplines was created for evaluation.", "result": "The study finds that larger models generate coherent questions but also shows that smaller models like Phi-2 can perform equally or more effectively, indicating that model size alone does not determine knowledge acquisition potential.", "conclusion": "The proposed evaluation framework quantifies a critical capability in LLMs and suggests new avenues for enhancing AI knowledge acquisition.", "key_contributions": ["Introduction of a novel evaluation framework for LLM knowledge acquisition.", "Creation of a synthetic dataset for testing across different difficulty levels.", "Human evaluation results that validate the proposed scoring methodology."], "limitations": "Focuses only on knowledge acquisition through question generation, not exploring other potential capabilities or methods.", "keywords": ["large language models", "knowledge acquisition", "evaluation framework", "science education", "question generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2409.18486", "pdf": "https://arxiv.org/pdf/2409.18486.pdf", "abs": "https://arxiv.org/abs/2409.18486", "title": "Evaluation of OpenAI o1: Opportunities and Challenges of AGI", "authors": ["Tianyang Zhong", "Zhengliang Liu", "Yi Pan", "Yutong Zhang", "Yifan Zhou", "Shizhe Liang", "Zihao Wu", "Yanjun Lyu", "Peng Shu", "Xiaowei Yu", "Chao Cao", "Hanqi Jiang", "Hanxu Chen", "Yiwei Li", "Junhao Chen", "Huawen Hu", "Yiheng Liu", "Huaqin Zhao", "Shaochen Xu", "Haixing Dai", "Lin Zhao", "Ruidong Zhang", "Wei Zhao", "Zhenyuan Yang", "Jingyuan Chen", "Peilong Wang", "Wei Ruan", "Hui Wang", "Huan Zhao", "Jing Zhang", "Yiming Ren", "Shihuan Qin", "Tong Chen", "Jiaxi Li", "Arif Hassan Zidan", "Afrar Jahin", "Minheng Chen", "Sichen Xia", "Jason Holmes", "Yan Zhuang", "Jiaqi Wang", "Bochen Xu", "Weiran Xia", "Jichao Yu", "Kaibo Tang", "Yaxuan Yang", "Bolun Sun", "Tao Yang", "Guoyu Lu", "Xianqiao Wang", "Lilong Chai", "He Li", "Jin Lu", "Xin Zhang", "Bao Ge", "Xintao Hu", "Lian Zhang", "Hua Zhou", "Lu Zhang", "Shu Zhang", "Zhen Xiang", "Yudan Ren", "Jun Liu", "Xi Jiang", "Yu Bao", "Wei Zhang", "Xiang Li", "Gang Li", "Wei Liu", "Dinggang Shen", "Andrea Sikora", "Xiaoming Zhai", "Dajiang Zhu", "Tuo Zhang", "Tianming Liu"], "categories": ["cs.CL"], "comment": null, "summary": "This comprehensive study evaluates the performance of OpenAI's o1-preview\nlarge language model across a diverse array of complex reasoning tasks,\nspanning multiple domains, including computer science, mathematics, natural\nsciences, medicine, linguistics, and social sciences. Through rigorous testing,\no1-preview demonstrated remarkable capabilities, often achieving human-level or\nsuperior performance in areas ranging from coding challenges to scientific\nreasoning and from language processing to creative problem-solving. Key\nfindings include:\n  -83.3% success rate in solving complex competitive programming problems,\nsurpassing many human experts.\n  -Superior ability in generating coherent and accurate radiology reports,\noutperforming other evaluated models.\n  -100% accuracy in high school-level mathematical reasoning tasks, providing\ndetailed step-by-step solutions.\n  -Advanced natural language inference capabilities across general and\nspecialized domains like medicine.\n  -Impressive performance in chip design tasks, outperforming specialized\nmodels in areas such as EDA script generation and bug analysis.\n  -Remarkable proficiency in anthropology and geology, demonstrating deep\nunderstanding and reasoning in these specialized fields.\n  -Strong capabilities in quantitative investing. O1 has comprehensive\nfinancial knowledge and statistical modeling skills.\n  -Effective performance in social media analysis, including sentiment analysis\nand emotion recognition.\n  The model excelled particularly in tasks requiring intricate reasoning and\nknowledge integration across various fields. While some limitations were\nobserved, including occasional errors on simpler problems and challenges with\ncertain highly specialized concepts, the overall results indicate significant\nprogress towards artificial general intelligence.", "AI": {"tldr": "This study evaluates OpenAI's o1-preview large language model across various reasoning tasks, showcasing high performance in multiple domains including medicine and mathematics.", "motivation": "To assess the capabilities of OpenAI's o1-preview model in solving complex tasks across diverse fields and to evaluate its potential progress towards artificial general intelligence.", "method": "Rigorous testing across domains such as computer science, medicine, and social sciences, focusing on various reasoning tasks including coding challenges and scientific reasoning.", "result": "The o1-preview model achieved an 83.3% success rate in competitive programming, superior accuracy in radiology report generation, and 100% in high school-level math tasks, excelling in various specialized domains.", "conclusion": "The model demonstrates significant advancements in reasoning and knowledge integration, indicating progress towards artificial general intelligence despite some limitations in specific areas.", "key_contributions": ["High success rate in competitive programming surpassing human experts", "Superior performance in generating medical reports", "100% accuracy in mathematical reasoning tasks"], "limitations": "Occasional errors on simpler problems and challenges with highly specialized concepts.", "keywords": ["large language model", "reasoning tasks", "artificial general intelligence"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.21849", "pdf": "https://arxiv.org/pdf/2410.21849.pdf", "abs": "https://arxiv.org/abs/2410.21849", "title": "Joint Beamforming and Speaker-Attributed ASR for Real Distant-Microphone Meeting Transcription", "authors": ["Can Cui", "Imran Ahamad Sheikh", "Mostafa Sadeghi", "Emmanuel Vincent"], "categories": ["cs.CL"], "comment": null, "summary": "Distant-microphone meeting transcription is a challenging task.\nState-of-the-art end-to-end speaker-attributed automatic speech recognition\n(SA-ASR) architectures lack a multichannel noise and reverberation reduction\nfront-end, which limits their performance. In this paper, we introduce a joint\nbeamforming and SA-ASR approach for real meeting transcription. We first\ndescribe a data alignment and augmentation method to pretrain a neural\nbeamformer on real meeting data. We then compare fixed, hybrid, and fully\nneural beamformers as front-ends to the SA-ASR model. Finally, we jointly\noptimize the fully neural beamformer and the SA-ASR model. Experiments on the\nreal AMI corpus show that, while state-of-the-art multi-frame cross-channel\nattention based channel fusion fails to improve ASR performance, fine-tuning\nSA-ASR on the fixed beamformer's output and jointly fine-tuning SA-ASR with the\nneural beamformer reduce the word error rate by 8% and 9% relative,\nrespectively.", "AI": {"tldr": "This paper presents a joint beamforming and speaker-attributed automatic speech recognition (SA-ASR) approach to improve distant-microphone meeting transcription.", "motivation": "To address the limitations of existing end-to-end SA-ASR architectures due to the lack of effective multichannel noise and reverberation reduction techniques.", "method": "The authors introduce a method for data alignment and augmentation to pretrain a neural beamformer on real meeting data, followed by comparisons of fixed, hybrid, and fully neural beamformers as front-ends to the SA-ASR model, culminating in a joint optimization of the beamformer and SA-ASR.", "result": "The proposed methods, especially the fine-tuning of SA-ASR on outputs from the fixed beamformer and the joint fine-tuning with the neural beamformer, achieved a reduction in word error rate by 8% and 9%, respectively, on the real AMI corpus.", "conclusion": "The integration of a neural beamformer with SA-ASR significantly enhances performance, demonstrating the necessity of addressing noise and reverberation in meeting transcription tasks.", "key_contributions": ["Introduction of a joint beamforming and SA-ASR approach.", "Development of a data alignment and augmentation method for neural beamformer pretraining.", "Empirical comparison showcasing performance improvements over traditional methods."], "limitations": "", "keywords": ["automatic speech recognition", "beamforming", "machine learning", "meeting transcription", "neural networks"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2411.04427", "pdf": "https://arxiv.org/pdf/2411.04427.pdf", "abs": "https://arxiv.org/abs/2411.04427", "title": "One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity", "authors": ["Sonia K. Murthy", "Tomer Ullman", "Jennifer Hu"], "categories": ["cs.CL"], "comment": "17 pages, 10 figures; updated with publishing information", "summary": "Researchers in social science and psychology have recently proposed using\nlarge language models (LLMs) as replacements for humans in behavioral research.\nIn addition to arguments about whether LLMs accurately capture population-level\npatterns, this has raised questions about whether LLMs capture human-like\nconceptual diversity. Separately, it is debated whether post-training alignment\n(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,\nwe use a new way of measuring the conceptual diversity of\nsynthetically-generated LLM \"populations\" by relating the internal variability\nof simulated individuals to the population-level variability. We use this\napproach to evaluate non-aligned and aligned LLMs on two domains with rich\nhuman behavioral data. While no model reaches human-like diversity, aligned\nmodels generally display less diversity than their instruction fine-tuned\ncounterparts. Our findings highlight potential trade-offs between increasing\nmodels' value alignment and decreasing the diversity of their conceptual\nrepresentations.", "AI": {"tldr": "This paper investigates the conceptual diversity of large language models (LLMs) in behavioral research, comparing non-aligned and aligned models to human models.", "motivation": "There is growing interest in using LLMs for behavioral research, raising concerns about their ability to represent human-like conceptual diversity and the impact of post-training alignment on this diversity.", "method": "The authors propose a new metric that relates the internal variability of simulated individuals from LLMs to the overall population-level variability, comparing diverse model types across domains with human behavioral data.", "result": "While none of the models achieved human-like diversity, aligned LLMs exhibited lower diversity than those fine-tuned for instructions, indicating trade-offs in increasing value alignment versus maintaining conceptual diversity.", "conclusion": "The study suggests that aligning models with human values may compromise their conceptual diversity, which is crucial for accurately modeling human behavior.", "key_contributions": ["Introduced a new method for measuring conceptual diversity in LLMs.", "Evaluated the diversity of aligned vs. non-aligned LLMs in behavioral domains.", "Highlighted the trade-off between value alignment and conceptual diversity in LLMs."], "limitations": "The study does not claim that any model fully captures human-like diversity, and findings may vary across additional domains not tested.", "keywords": ["large language models", "conceptual diversity", "alignment", "behavioral research", "human-like representation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2411.08324", "pdf": "https://arxiv.org/pdf/2411.08324.pdf", "abs": "https://arxiv.org/abs/2411.08324", "title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle", "authors": ["Hui Dai", "Ryan Teehan", "Mengye Ren"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Many existing evaluation benchmarks for Large Language Models (LLMs) quickly\nbecome outdated due to the emergence of new models and training data. These\nbenchmarks also fall short in assessing how LLM performance changes over time,\nas they consist of a static set of questions without a temporal dimension. To\naddress these limitations, we propose using future event prediction as a\ncontinuous evaluation method to assess LLMs' temporal generalization and\nforecasting abilities. Our benchmark, Daily Oracle, automatically generates\nquestion-answer (QA) pairs from daily news, challenging LLMs to predict\n\"future\" event outcomes. Our findings reveal that as pre-training data becomes\noutdated, LLM performance degrades over time. While Retrieval Augmented\nGeneration (RAG) has the potential to enhance prediction accuracy, the\nperformance degradation pattern persists, highlighting the need for continuous\nmodel updates. Code and data are available at\nhttps://agenticlearning.ai/daily-oracle.", "AI": {"tldr": "This paper introduces Daily Oracle, a benchmark for evaluating LLMs that focuses on future event prediction to measure their temporal generalization and forecasting capabilities.", "motivation": "The need for an evaluation benchmark that adapts to the evolving capabilities of Large Language Models (LLMs) and assesses their performance over time is critical as existing benchmarks quickly become outdated.", "method": "The proposed methodology uses future event prediction as a continuous evaluation method by automatically generating question-answer pairs from daily news events.", "result": "The findings indicate that LLMs' performance declines over time as their pre-training data ages. While Retrieval Augmented Generation (RAG) can improve prediction accuracy, performance degradation still occurs, necessitating regular model updates.", "conclusion": "The study emphasizes the importance of continuous evaluation and highlights the limitations of static benchmarks in measuring the evolving capabilities of LLMs.", "key_contributions": ["Introduction of the Daily Oracle benchmark for continuous evaluation of LLMs.", "Demonstration of LLM performance degradation over time due to outdated training data.", "Analysis of the role of RAG in improving prediction accuracy alongside the need for model updates."], "limitations": "The benchmark relies on the availability and accuracy of daily news for generating QA pairs, which may introduce variability in evaluation.", "keywords": ["Large Language Models", "temporal generalization", "future event prediction", "continuous evaluation", "Retrieval Augmented Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.11459", "pdf": "https://arxiv.org/pdf/2412.11459.pdf", "abs": "https://arxiv.org/abs/2412.11459", "title": "Rethinking Associative Memory Mechanism in Induction Head", "authors": ["Shuo Wang", "Issei Sato"], "categories": ["cs.CL", "cs.LG"], "comment": "COLM 2025", "summary": "Induction head mechanism is a part of the computational circuits for\nin-context learning (ICL) that enable large language models (LLMs) to adapt to\nnew tasks without fine-tuning. Most existing work explains the training\ndynamics behind acquiring such a powerful mechanism. However, the model's\nability to coordinate in-context information over long contexts and global\nknowledge acquired during pretraining remains poorly understood. This paper\ninvestigates how a two-layer transformer thoroughly captures in-context\ninformation and balances it with pretrained bigram knowledge in next token\nprediction, from the viewpoint of associative memory. We theoretically analyze\nthe representation of weight matrices in attention layers and the resulting\nlogits when a transformer is given prompts generated by a bigram model. In the\nexperiments, we design specific prompts to evaluate whether the outputs of the\ntrained transformer align with the theoretical results.", "AI": {"tldr": "This paper investigates how two-layer transformers capture in-context information and balance it with pretrained bigram knowledge in next token prediction.", "motivation": "The study aims to understand the coordination of in-context information over long contexts and the utilization of global knowledge in large language models, which is not well understood.", "method": "The authors analyze weight matrices in attention layers and the resulting logits when transformers process prompts generated by a bigram model, followed by specific experimental prompts to evaluate actual outputs against theoretical expectations.", "result": "The experiments reveal the alignment between the outputs of the transformer and theoretical predictions based on the analysis of associative memory.", "conclusion": "Understanding the interaction between in-context learning and pretrained knowledge enhances our grasp of LLM capabilities, influencing their application in various domains.", "key_contributions": ["Theoretical analysis of transformer weight matrices and logits representation.", "Experimental evaluation of transformer outputs against theoretical models.", "Insights into the balance of in-context information and pretrained knowledge."], "limitations": "", "keywords": ["Large Language Models", "In-context Learning", "Transformers", "Bigram Model", "Associative Memory"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.07616", "pdf": "https://arxiv.org/pdf/2502.07616.pdf", "abs": "https://arxiv.org/abs/2502.07616", "title": "Tractable Transformers for Flexible Conditional Generation", "authors": ["Anji Liu", "Xuejie Liu", "Dayuan Zhao", "Mathias Niepert", "Yitao Liang", "Guy Van den Broeck"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Non-autoregressive (NAR) generative models are valuable because they can\nhandle diverse conditional generation tasks in a more principled way than their\nautoregressive (AR) counterparts, which are constrained by sequential\ndependency requirements. Recent advancements in NAR models, such as diffusion\nlanguage models, have demonstrated superior performance in unconditional\ngeneration compared to AR models (e.g., GPTs) of similar sizes. However, such\nimprovements do not always lead to improved conditional generation performance.\nWe show that a key reason for this gap is the difficulty in generalizing to\nconditional probability queries (i.e., the set of unknown variables) unseen\nduring training. As a result, strong unconditional generation performance does\nnot guarantee high-quality conditional generation. This paper proposes\nTractable Transformers (Tracformer), a Transformer-based generative model that\nis more robust to different conditional generation tasks. Unlike existing\nmodels that rely solely on global contextual features derived from full inputs,\nTracformers incorporate a sparse Transformer encoder to capture both local and\nglobal contextual information. This information is routed through a decoder for\nconditional generation. Empirical results demonstrate that Tracformers achieve\nstate-of-the-art conditional generation performance on text modeling compared\nto recent diffusion and AR model baselines.", "AI": {"tldr": "This paper introduces Tractable Transformers (Tracformer), a new generative model that improves conditional generation by incorporating local and global contextual information.", "motivation": "To address the gap in conditional generation performance found in non-autoregressive models compared to autoregressive models.", "method": "The paper proposes Tracformer, a Transformer-based model that uses a sparse encoder for local and global context capture, facilitating better conditional generation tasks.", "result": "Tracformers demonstrate state-of-the-art performance in conditional generation on text modeling compared to previous diffusion and autoregressive models.", "conclusion": "Tracformers improve the robustness and quality of conditional generation tasks, showing that strong unconditional performance doesn't always translate to conditional effectiveness.", "key_contributions": ["Introduction of the Tractable Transformers (Tracformer) model", "Use of a sparse Transformer encoder for improved context capture", "Demonstrated state-of-the-art performance in conditional generation tasks"], "limitations": "", "keywords": ["Non-autoregressive models", "Conditional generation", "Tracformer"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.14429", "pdf": "https://arxiv.org/pdf/2502.14429.pdf", "abs": "https://arxiv.org/abs/2502.14429", "title": "Early-Exit and Instant Confidence Translation Quality Estimation", "authors": ["Vilém Zouhar", "Maike Züfle", "Beni Egressy", "Julius Cheng", "Mrinmaya Sachan", "Jan Niehues"], "categories": ["cs.CL"], "comment": null, "summary": "Quality estimation is omnipresent in machine translation, for both evaluation\nand generation. Unfortunately, quality estimation models are often opaque and\ncomputationally expensive, making them impractical to be part of large-scale\npipelines. In this work, we tackle two connected challenges: (1) reducing the\ncost of quality estimation at scale, and (2) developing an inexpensive\nuncertainty estimation method for quality estimation. To address the latter, we\nintroduce Instant Confidence COMET, an uncertainty-aware quality estimation\nmodel that matches the performance of previous approaches at a fraction of\ntheir costs. We extend this to Early-Exit COMET, a quality estimation model\nthat can compute quality scores and associated confidences already at early\nmodel layers, allowing us to early-exit computations and reduce evaluation\ncosts. We also apply our model to machine translation reranking. We combine\nEarly-Exit COMET with an upper confidence bound bandit algorithm to find the\nbest candidate from a large pool without having to run the full evaluation\nmodel on all candidates. In both cases (evaluation and reranking) our methods\nreduce the required compute by 50% with very little degradation in performance.\nFinally, we show how Instant Confidence COMET can be used to decide which\ntranslations a human evaluator should score rather than relying on the COMET\nscore.", "AI": {"tldr": "This paper presents two models, Instant Confidence COMET and Early-Exit COMET, that enhance quality estimation for machine translation by reducing computational costs and providing uncertainty estimation.", "motivation": "The need for efficient and cost-effective quality estimation methods in machine translation for both evaluation and generation tasks.", "method": "We introduce Instant Confidence COMET for uncertainty-aware quality estimation and Early-Exit COMET for early computation of quality scores, combined with a bandit algorithm for candidate selection in reranking tasks.", "result": "Our methods achieve a 50% reduction in computational costs while maintaining performance levels similar to prior approaches.", "conclusion": "The proposed models enable more efficient quality estimation and focus human evaluators on the most relevant translations, significantly decreasing the computational burden without compromising accuracy.", "key_contributions": ["Development of Instant Confidence COMET for cost-efficient uncertainty estimation", "Introduction of Early-Exit COMET for early score computation", "Application of bandit algorithms for effective candidate reranking"], "limitations": "", "keywords": ["quality estimation", "machine translation", "uncertainty estimation", "computational efficiency", "Early-Exit COMET"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.20855", "pdf": "https://arxiv.org/pdf/2502.20855.pdf", "abs": "https://arxiv.org/abs/2502.20855", "title": "MAMUT: A Novel Framework for Modifying Mathematical Formulas for the Generation of Specialized Datasets for Language Model Training", "authors": ["Jonathan Drechsel", "Anja Reusch", "Steffen Herbold"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Mathematical formulas are a fundamental and widely used component in various\nscientific fields, serving as a universal language for expressing complex\nconcepts and relationships. While state-of-the-art transformer models excel in\nprocessing and understanding natural language, they encounter challenges with\nmathematical notation, which involves a complex structure and diverse\nrepresentations. This study focuses on the development of specialized training\ndatasets to enhance the encoding of mathematical content. We introduce Math\nMutator (MAMUT), a framework capable of generating equivalent and falsified\nversions of a given mathematical formula in LaTeX notation, effectively\ncapturing the mathematical variety in notation of the same concept. Based on\nMAMUT, we have generated four large mathematical datasets containing diverse\nnotation. Experiments show that models trained on these datasets exhibit new\nSoTA performance on mathematical retrieval tasks. We publish our code,\ngenerated datasets, and pretrained mathematical models:\nhttps://github.com/aieng-lab/math-mutator.", "AI": {"tldr": "This study introduces Math Mutator (MAMUT), a framework that generates equivalent and falsified LaTeX versions of mathematical formulas to improve mathematical notation processing in transformer models.", "motivation": "To overcome challenges faced by transformer models in processing and understanding complex mathematical notation and to enhance mathematical content encoding through specialized training datasets.", "method": "Development of a framework (MAMUT) to generate diverse LaTeX representations of mathematical formulas and creation of four large datasets for training models on mathematical retrieval tasks.", "result": "Models trained on the generated datasets achieved state-of-the-art performance in mathematical retrieval tasks.", "conclusion": "The proposed framework and datasets significantly improve model performance in understanding mathematical notation, facilitating better integration into scientific applications.", "key_contributions": ["Introduction of Math Mutator (MAMUT) framework for generating LaTeX mathematical formulas.", "Creation of four large datasets with diverse mathematical notation.", "Demonstration of state-of-the-art performance in mathematical retrieval tasks using these datasets."], "limitations": "", "keywords": ["mathematics", "LaTeX", "transformer models", "dataset generation", "mathematical retrieval"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2503.02233", "pdf": "https://arxiv.org/pdf/2503.02233.pdf", "abs": "https://arxiv.org/abs/2503.02233", "title": "Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling", "authors": ["Hang Zheng", "Hongshen Xu", "Yuncong Liu", "Lu Chen", "Pascale Fung", "Kai Yu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are prone to hallucination stemming from\nmisaligned self-awareness, particularly when processing queries exceeding their\nknowledge boundaries. While existing mitigation strategies employ uncertainty\nestimation or query rejection mechanisms, they suffer from computational\nefficiency and sacrificed helpfulness. To address these issues, we propose the\nExplicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and\nslow reasoning systems to harmonize reliability and usability. The framework\nfirst employs a fast-thinking model to generate confidence-labeled responses,\nenabling immediate utilization of high-confidence outputs, whereas uncertain\npredictions trigger a slow refinement model for accuracy improvement. To align\nmodel behavior with our proposed object, we propose a hybrid training pipeline,\nenhancing self-awareness without degrading task performance. Evaluations on\ndialogue state tracking tasks demonstrate that EKBM achieves superior model\nreliability over uncertainty-based baselines. Further analysis reveals that\nrefinement substantially boosts accuracy while maintaining low computational\noverhead. The framework establishes a scalable paradigm for deploying reliable\nLLMs in error-sensitive applications, effectively balancing accuracy and\npractical utility.", "AI": {"tldr": "The EKBM framework improves LLM reliability and usability by integrating fast and slow reasoning for processing queries exceeding their knowledge limits.", "motivation": "To address the hallucination issues in LLMs arising from misaligned self-awareness, particularly under conditions exceeding their knowledge.", "method": "The EKBM framework combines a fast-thinking model for generating confidence-labeled responses with a slow refinement model for improving uncertain predictions, along with a hybrid training pipeline to enhance self-awareness and performance.", "result": "EKBM demonstrates superior model reliability on dialogue state tracking tasks compared to existing uncertainty-based approaches, with substantial accuracy improvements and low computational overhead.", "conclusion": "The proposed framework provides a scalable solution for deploying reliable LLMs in sensitive applications by effectively balancing accuracy and practicality.", "key_contributions": ["Integration of fast and slow reasoning systems in LLMs", "Hybrid training pipeline enhancing model self-awareness", "Demonstrated improvement in reliability and accuracy for error-sensitive applications"], "limitations": "", "keywords": ["Large Language Models", "Hallucination", "Explicit Knowledge Boundary Modeling", "Self-awareness", "Computational Efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.12149", "pdf": "https://arxiv.org/pdf/2503.12149.pdf", "abs": "https://arxiv.org/abs/2503.12149", "title": "Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models", "authors": ["Junjie Chen", "Xuyang Liu", "Subin Huang", "Linfeng Zhang", "Hang Yu"], "categories": ["cs.CL", "cs.MM", "cs.SI"], "comment": null, "summary": "With the advent of large vision-language models (LVLMs) demonstrating\nincreasingly human-like abilities, a pivotal question emerges: do different\nLVLMs interpret multimodal sarcasm differently, and can a single model grasp\nsarcasm from multiple perspectives like humans? To explore this, we introduce\nan analytical framework using systematically designed prompts on existing\nmultimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409\nsamples, we examine interpretive variations within and across models, focusing\non confidence levels, alignment with dataset labels, and recognition of\nambiguous \"neutral\" cases. Our findings reveal notable discrepancies -- across\nLVLMs and within the same model under varied prompts. While\nclassification-oriented prompts yield higher internal consistency, models\ndiverge markedly when tasked with interpretive reasoning. These results\nchallenge binary labeling paradigms by highlighting sarcasm's subjectivity. We\nadvocate moving beyond rigid annotation schemes toward multi-perspective,\nuncertainty-aware modeling, offering deeper insights into multimodal sarcasm\ncomprehension. Our code and data are available at:\nhttps://github.com/CoderChen01/LVLMSarcasmAnalysis", "AI": {"tldr": "The paper examines how different large vision-language models (LVLMs) interpret multimodal sarcasm, revealing significant variations in their understanding and suggesting a need for a more nuanced approach in modeling sarcasm.", "motivation": "As large vision-language models become more advanced, it is important to understand if and how they interpret sarcasm differently from humans.", "method": "The authors introduce an analytical framework that utilizes systematically designed prompts on multimodal sarcasm datasets to evaluate 12 state-of-the-art LVLMs over 2,409 samples.", "result": "The study finds notable discrepancies in sarcasm interpretation both across different LVLMs and within the same model under different prompts, especially in classifications and interpretive reasoning.", "conclusion": "The results call for an evolution in sarcasm modeling to embrace multi-perspective and uncertainty-aware approaches, moving away from binary labeling schemes.", "key_contributions": ["Analytical framework for evaluating LVLMs on sarcasm interpretation.", "Empirical insights into varying interpretations of sarcasm across different models.", "Recommendations for evolving model training approaches that account for sarcasm's subjectivity."], "limitations": "The study is limited to existing multimodal sarcasm datasets and may not capture all nuances of sarcasm in real-world applications.", "keywords": ["large vision-language models", "sarcasm", "multimodal analysis", "interpretive reasoning", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.13299", "pdf": "https://arxiv.org/pdf/2503.13299.pdf", "abs": "https://arxiv.org/abs/2503.13299", "title": "A Survey on Transformer Context Extension: Approaches and Evaluation", "authors": ["Yijun Liu", "Jinzheng Yu", "Yang Xu", "Zhongyang Li", "Qingfu Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments.", "AI": {"tldr": "This survey reviews challenges and approaches for applying large language models (LLMs) to long context scenarios in NLP, categorizing them into four main types and summarizing evaluation metrics.", "motivation": "To address the degradation in performance of LLMs in long context scenarios despite their strong performance in short text tasks.", "method": "The paper systematically reviews existing approaches related to long context and proposes a taxonomy categorizing them into positional encoding, context compression, retrieval augmented, and attention pattern.", "result": "The authors organized relevant data, tasks, and metrics based on existing long context benchmarks and identified unresolved issues in the long context domain.", "conclusion": "The paper offers insights into potential future developments for improving LLM performance in long context scenarios.", "key_contributions": ["Systematic taxonomy of approaches for long context processing.", "Comprehensive review of challenges faced by LLMs with long contexts.", "Organization of evaluation metrics and benchmarks for long context tasks."], "limitations": "", "keywords": ["Long Context", "Large Language Models", "Natural Language Processing", "Evaluation Metrics", "Survey"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.07096", "pdf": "https://arxiv.org/pdf/2504.07096.pdf", "abs": "https://arxiv.org/abs/2504.07096", "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens", "authors": ["Jiacheng Liu", "Taylor Blanton", "Yanai Elazar", "Sewon Min", "YenSung Chen", "Arnavi Chheda-Kothary", "Huy Tran", "Byron Bischoff", "Eric Marsh", "Michael Schmitz", "Cassidy Trier", "Aaron Sarnat", "Jenna James", "Jon Borchardt", "Bailey Kuehl", "Evie Cheng", "Karen Farley", "Sruthi Sreeram", "Taira Anderson", "David Albright", "Carissa Schoenick", "Luca Soldaini", "Dirk Groeneveld", "Rock Yuren Pang", "Pang Wei Koh", "Noah A. Smith", "Sophie Lebrecht", "Yejin Choi", "Hannaneh Hajishirzi", "Ali Farhadi", "Jesse Dodge"], "categories": ["cs.CL"], "comment": "ACL 2025 demo track", "summary": "We present OLMoTrace, the first system that traces the outputs of language\nmodels back to their full, multi-trillion-token training data in real time.\nOLMoTrace finds and shows verbatim matches between segments of language model\noutput and documents in the training text corpora. Powered by an extended\nversion of infini-gram (Liu et al., 2024), our system returns tracing results\nwithin a few seconds. OLMoTrace can help users understand the behavior of\nlanguage models through the lens of their training data. We showcase how it can\nbe used to explore fact checking, hallucination, and the creativity of language\nmodels. OLMoTrace is publicly available and fully open-source.", "AI": {"tldr": "OLMoTrace enables real-time tracing of language model outputs to their training data, identifying verbatim matches to improve understanding of model behavior.", "motivation": "To enhance the understanding of language model outputs and their relation to training data, addressing issues like fact checking and hallucination.", "method": "OLMoTrace utilizes an extended version of infini-gram to efficiently find and display outputs matching the training corpus in real-time.", "result": "The system can provide verbatim matches between language model outputs and documents from a multi-trillion-token training dataset within seconds.", "conclusion": "OLMoTrace provides a tool for exploring language model outputs, facilitating better understanding of their behavior and aiding in areas like fact checking and creativity assessment.", "key_contributions": ["Real-time tracing of language model outputs to training data.", "Open-source implementation available for public use.", "Adnosis to understanding model behavior through direct data correlation."], "limitations": "", "keywords": ["Language Models", "Training Data", "Real-Time Tracing", "Fact-Checking", "Hallucination"], "importance_score": 8, "read_time_minutes": 5}}
