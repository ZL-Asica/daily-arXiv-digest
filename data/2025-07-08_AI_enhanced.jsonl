{"id": "2507.02865", "pdf": "https://arxiv.org/pdf/2507.02865.pdf", "abs": "https://arxiv.org/abs/2507.02865", "title": "Enhancing the Aesthetic Appeal of AI-Generated Physical Product Designs through LoRA Fine-Tuning with Human Feedback", "authors": ["Dinuo Liao", "James Derek Lomas", "Cehao Yu"], "categories": ["cs.HC", "cs.AI"], "comment": "6 pages, 7 figures. Submitted to AIGC2024", "summary": "This study explores how Low-Rank Adaptation (LoRA) fine-tuning, guided by\nhuman aesthetic evaluations, can enhance the outputs of generative AI models in\ntangible product design, using lamp design as a case study. By integrating\nhuman feedback into the AI model, we aim to improve both the desirability and\naesthetic appeal of the generated designs. Comprehensive experiments were\nconducted, starting with prompt optimization techniques and focusing on LoRA\nfine-tuning of the Stable Diffusion model. Additionally, methods to convert\nAI-generated designs into tangible products through 3D realization using 3D\nprinting technologies were investigated. The results indicate that LoRA\nfine-tuning effectively aligns AI-generated designs with human aesthetic\npreferences, leading to significant improvements in desirability and aesthetic\nappeal scores. These findings highlight the potential of human-AI collaboration\nin tangible product design and provide valuable insights into integrating human\nfeedback into AI design processes.", "AI": {"tldr": "This study investigates the enhancement of generative AI models in product design through Low-Rank Adaptation (LoRA) fine-tuning, guided by human aesthetic feedback, with a focus on lamp design and 3D printing.", "motivation": "To improve the desirability and aesthetic appeal of AI-generated designs by integrating human feedback into the AI model.", "method": "The study involved prompt optimization techniques and LoRA fine-tuning of the Stable Diffusion model, along with methods for converting AI-generated designs into tangible products using 3D printing.", "result": "LoRA fine-tuning achieved better alignment of AI-generated designs with human aesthetic preferences, resulting in notable increases in desirability and aesthetic scores.", "conclusion": "The findings underline the importance of human-AI collaboration in product design and suggest effective ways to incorporate human feedback into AI design processes.", "key_contributions": ["Demonstration of LoRA fine-tuning's effectiveness in generative design.", "Integration of human feedback mechanism into AI design processes.", "Exploration of converting AI-generated designs to physical products using 3D printing."], "limitations": "The study focuses on lamp designs, which may limit generalizability to other product categories.", "keywords": ["Low-Rank Adaptation", "generative AI", "tangible product design", "human feedback", "3D printing"], "importance_score": 6, "read_time_minutes": 6}}
{"id": "2507.02866", "pdf": "https://arxiv.org/pdf/2507.02866.pdf", "abs": "https://arxiv.org/abs/2507.02866", "title": "Engineering Trust, Creating Vulnerability: A Socio-Technical Analysis of AI Interface Design", "authors": ["Ben Kereopa-Yorke"], "categories": ["cs.HC", "cs.CR"], "comment": "31 pages, 8 figures", "summary": "This paper examines how distinct cultures of AI interdisciplinarity emerge\nthrough interface design, revealing the formation of new disciplinary cultures\nat these intersections. Through the Interface-Mediated Cognitive Security\n(IMCS) framework, I demonstrate how the collision of cybersecurity engineering,\ncognitive psychology, critical technology studies, and human-computer\ninteraction generates research cultures that transcend traditional disciplinary\nboundaries. AI interfaces function as transformative boundary objects that\nnecessitate methodological fusion rather than mere collaboration,\nsimultaneously embodying technical architectures, psychological design\npatterns, and social interaction models. Through systematic visual analysis of\ngenerative AI platforms and case studies across public sector, medical, and\neducational domains, I identify four vulnerability vectors, Reflection\nSimulation, Authority Modulation, Cognitive Load Exploitation, and\nMarket-Security Tension, that structure interface-mediated cognitive security.\nThis research challenges three significant gaps in interdisciplinary theory:\nthe assumption that disciplines maintain distinct methodological boundaries\nduring collaboration, the belief that technical and social knowledge practices\ncan be cleanly separated, and the presumption that disciplinary integration\noccurs through formal rather than cultural mechanisms. The empirical evidence\ndemonstrates how interfaces function as sites of epistemological collision,\ncreating methodological pressure zones where traditional disciplinary\napproaches prove insufficient for analysing the complex socio-technical\nphenomena at the interface.", "AI": {"tldr": "This paper explores how AI interdisciplinary cultures are shaped through interface design, using the Interface-Mediated Cognitive Security framework to analyze generative AI platforms.", "motivation": "To understand how different disciplines in AI, such as cybersecurity and HCI, intersect and form new cultures through interface design.", "method": "Utilizes systematic visual analysis of generative AI platforms and case studies from public, medical, and educational sectors, identifying four vulnerability vectors related to cognitive security.", "result": "Identifies four vulnerability vectors (Reflection Simulation, Authority Modulation, Cognitive Load Exploitation, Market-Security Tension) that structure cognitive security around interfaces, highlighting interdisciplinary challenges.", "conclusion": "Challenges traditional views of disciplinary boundaries, showing that AI interfaces create cultural integration rather than just collaborative methods.", "key_contributions": ["Introduces the Interface-Mediated Cognitive Security framework.", "Identifies four vulnerability vectors crucial for understanding interface-mediated cognitive security.", "Highlights the role of AI interfaces as boundary objects fostering interdisciplinary research cultures."], "limitations": "", "keywords": ["AI", "Human-Computer Interaction", "Cognitive Security", "Interdisciplinary Research", "Generative AI"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2507.02868", "pdf": "https://arxiv.org/pdf/2507.02868.pdf", "abs": "https://arxiv.org/abs/2507.02868", "title": "Identifying Ethical Challenges in XR Implementations in the Industrial Domain: A Case of Off-Highway Machinery", "authors": ["Anastasia Sergeeva", "Claudia Negri-Ribalta", "Gabriele Lenzini"], "categories": ["cs.HC"], "comment": "Presented at CHI 2025 (arXiv:2504.07475)", "summary": "Although extended reality(XR)-using technologies have started to be discussed\nin the industrial setting, it is becoming important to understand how to\nimplement them ethically and privacy-preservingly. In our paper, we summarise\nour experience of developing XR implementations for the off-highway machinery\ndomain by pointing to the main challenges we identified during the work. We\nbelieve that our findings can be a starting point for further discussion and\nfuture research regarding privacy and ethical challenges in industrial\napplications of XR.", "AI": {"tldr": "The paper discusses the ethical and privacy challenges of implementing extended reality (XR) technologies in the industrial sector, based on experiences from the off-highway machinery domain.", "motivation": "To address the increasing relevance of ethical and privacy considerations in XR implementations within industrial settings.", "method": "Summarization of experiences and challenges encountered while developing XR applications for off-highway machinery.", "result": "Identified key challenges related to privacy and ethics in XR technology use in industry, serving as a basis for future research discussions.", "conclusion": "The findings are intended to prompt further exploration of privacy and ethical issues in XR applications in industrial contexts.", "key_contributions": ["Identified specific ethical challenges in XR technology implementation.", "Highlighted privacy preservation strategies within industrial XR applications.", "Provided insights from practical experience in the off-highway machinery domain."], "limitations": "Focused solely on the off-highway machinery domain, which may limit the generalizability of findings to other industrial sectors.", "keywords": ["extended reality", "XR", "ethical challenges", "privacy preservation", "industrial applications"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.02869", "pdf": "https://arxiv.org/pdf/2507.02869.pdf", "abs": "https://arxiv.org/abs/2507.02869", "title": "Zara: An LLM-based Candidate Interview Feedback System", "authors": ["Nima Yazdani", "Aruj Mahajan", "Ali Ansari"], "categories": ["cs.HC"], "comment": "11 pages, 8 figures", "summary": "This paper introduces Zara, an AI-driven recruitment support system developed\nby micro1, as a practical case study illustrating how large language models\n(LLMs) can enhance the candidate experience through personalized, scalable\ninterview support. Traditionally, recruiters have struggled to deliver\nindividualized candidate feedback due to logistical and legal constraints,\nresulting in widespread candidate dissatisfaction. Leveraging OpenAI's GPT-4o,\nZara addresses these limitations by dynamically generating personalized\npractice interviews, conducting conversational AI-driven assessments,\nautonomously delivering structured and actionable feedback, and efficiently\nanswering candidate inquiries using a Retrieval-Augmented Generation (RAG)\nsystem. To promote transparency, we have open-sourced the approach Zara uses to\ngenerate candidate feedback.", "AI": {"tldr": "This paper presents Zara, an AI recruitment support system that uses LLMs to improve candidate experiences by providing personalized interview support and feedback.", "motivation": "The aim is to enhance candidate experiences in recruitment by overcoming traditional barriers to personalized feedback due to logistical and legal constraints.", "method": "Zara leverages OpenAI's GPT-4o to create personalized practice interviews, conduct assessments, and deliver structured feedback through a RAG system.", "result": "Zara successfully addresses candidate dissatisfaction by offering scalable and personalized interview support, improving the overall recruitment process.", "conclusion": "The system demonstrates the potential of LLMs in recruitment and promotes transparency by open-sourcing its methodology for generating candidate feedback.", "key_contributions": ["Development of the AI-driven recruitment support system Zara", "Use of RAG for providing personalized feedback and support", "Open-sourcing the methodology for transparency"], "limitations": "", "keywords": ["AI recruitment system", "large language models", "candidate experience", "personalized feedback", "Recruitment-Augmented Generation"], "importance_score": 7, "read_time_minutes": 11}}
{"id": "2507.02870", "pdf": "https://arxiv.org/pdf/2507.02870.pdf", "abs": "https://arxiv.org/abs/2507.02870", "title": "Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models", "authors": ["Chaozhuo Li", "Pengbo Wang", "Chenxu Wang", "Litian Zhang", "Zheng Liu", "Qiwei Ye", "Yuanbo Xu", "Feiran Huang", "Xi Zhang", "Philip S. Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Edgar Allan Poe noted, \"Truth often lurks in the shadow of error,\"\nhighlighting the deep complexity intrinsic to the interplay between truth and\nfalsehood, notably under conditions of cognitive and informational asymmetry.\nThis dynamic is strikingly evident in large language models (LLMs). Despite\ntheir impressive linguistic generation capabilities, LLMs sometimes produce\ninformation that appears factually accurate but is, in reality, fabricated, an\nissue often referred to as 'hallucinations'. The prevalence of these\nhallucinations can mislead users, affecting their judgments and decisions. In\nsectors such as finance, law, and healthcare, such misinformation risks causing\nsubstantial economic losses, legal disputes, and health risks, with\nwide-ranging consequences.In our research, we have methodically categorized,\nanalyzed the causes, detection methods, and solutions related to LLM\nhallucinations. Our efforts have particularly focused on understanding the\nroots of hallucinations and evaluating the efficacy of current strategies in\nrevealing the underlying logic, thereby paving the way for the development of\ninnovative and potent approaches. By examining why certain measures are\neffective against hallucinations, our study aims to foster a comprehensive\napproach to tackling this issue within the domain of LLMs.", "AI": {"tldr": "This paper investigates the phenomenon of 'hallucinations' in large language models (LLMs), categorizing their causes, detection methods, and potential solutions.", "motivation": "The paper highlights the challenges posed by the inaccuracies known as 'hallucinations' in LLMs, which can lead to misinformation in critical sectors such as finance, law, and healthcare.", "method": "A systematic categorization and analysis of the causes of hallucinations in LLMs, alongside a review of detection methods and evaluation of solutions", "result": "The study reveals the complexity of hallucinations and evaluates the effectiveness of various strategies for their detection and resolution.", "conclusion": "The researchers propose a comprehensive approach for understanding and mitigating hallucinations in LLMs, promoting the development of effective solutions.", "key_contributions": ["Categorization of LLM hallucinations", "Evaluation of current detection methods", "Recommendations for innovative solutions regarding hallucinations"], "limitations": "", "keywords": ["large language models", "hallucinations", "information accuracy", "cognitive asymmetry", "health informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.02905", "pdf": "https://arxiv.org/pdf/2507.02905.pdf", "abs": "https://arxiv.org/abs/2507.02905", "title": "Preference-Optimal Multi-Metric Weighting for Parallel Coordinate Plots", "authors": ["Chisa Mori", "Shuhei Watanabe", "Masaki Onishi", "Takayuki Itoh"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "Accepted to International Conference Information Visualisation\n  (iV2025)", "summary": "Parallel coordinate plots (PCPs) are a prevalent method to interpret the\nrelationship between the control parameters and metrics. PCPs deliver such an\ninterpretation by color gradation based on a single metric. However, it is\nchallenging to provide such a gradation when multiple metrics are present.\nAlthough a naive approach involves calculating a single metric by linearly\nweighting each metric, such weighting is unclear for users. To address this\nproblem, we first propose a principled formulation for calculating the optimal\nweight based on a specific preferred metric combination. Although users can\nsimply select their preference from a two-dimensional (2D) plane for bi-metric\nproblems, multi-metric problems require intuitive visualization to allow them\nto select their preference. We achieved this using various radar charts to\nvisualize the metric trade-offs on the 2D plane reduced by UMAP. In the\nanalysis using pedestrian flow guidance planning, our method identified unique\npatterns of control parameter importance for each user preference, highlighting\nthe effectiveness of our method.", "AI": {"tldr": "This paper presents a novel approach to enhance parallel coordinate plots (PCPs) for better user interpretation of multiple metrics by introducing an optimal weighting system and intuitive visualizations.", "motivation": "To improve user comprehension of multiple metrics in parallel coordinate plots (PCPs), moving beyond the limitations of naive linear weighting.", "method": "The authors propose a principled formulation for calculating optimal weights based on user-selected metric combinations, and use radar charts for visualizing metric trade-offs in multi-metric scenarios.", "result": "The method was applied to pedestrian flow guidance planning, successfully identifying unique patterns regarding control parameter importance for different user preferences.", "conclusion": "The new approach effectively enhances the usability of PCPs in multi-metric contexts, allowing for clearer decision-making based on user preferences.", "key_contributions": ["Principled optimal weight calculation for user-defined metric combinations", "Intuitive visualization of metric trade-offs using radar charts", "Application of the method in pedestrian flow guidance planning"], "limitations": "", "keywords": ["Parallel Coordinate Plots", "User Preference", "Multi-Metric Visualization", "Radar Charts", "UMAP"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.02919", "pdf": "https://arxiv.org/pdf/2507.02919.pdf", "abs": "https://arxiv.org/abs/2507.02919", "title": "ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models", "authors": ["Dai Li", "Linzhuo Li", "Huilian Sophie Qiu"], "categories": ["cs.CL", "cs.CY", "cs.ET"], "comment": null, "summary": "Large language models (LLMs) in the form of chatbots like ChatGPT and Llama\nare increasingly proposed as \"silicon samples\" for simulating human opinions.\nThis study examines this notion, arguing that LLMs may misrepresent\npopulation-level opinions. We identify two fundamental challenges: a failure in\nstructural consistency, where response accuracy doesn't hold across demographic\naggregation levels, and homogenization, an underrepresentation of minority\nopinions. To investigate these, we prompted ChatGPT (GPT-4) and Meta's Llama\n3.1 series (8B, 70B, 405B) with questions on abortion and unauthorized\nimmigration from the American National Election Studies (ANES) 2020. Our\nfindings reveal significant structural inconsistencies and severe\nhomogenization in LLM responses compared to human data. We propose an\n\"accuracy-optimization hypothesis,\" suggesting homogenization stems from\nprioritizing modal responses. These issues challenge the validity of using\nLLMs, especially chatbots AI, as direct substitutes for human survey data,\npotentially reinforcing stereotypes and misinforming policy.", "AI": {"tldr": "This study critiques the use of LLMs like ChatGPT and Llama as substitutes for human opinion surveys, highlighting issues with structural consistency and minority opinion representation.", "motivation": "The motivation behind this study is to assess whether large language models can accurately simulate human opinions and to understand the implications of their potential misrepresentation of population-level sentiments.", "method": "The study involved prompting ChatGPT (GPT-4) and Meta's Llama series with questions regarding abortion and unauthorized immigration, analyzing the responses against the American National Election Studies (ANES) 2020 data.", "result": "The analysis revealed significant structural inconsistencies and homogenization in the responses from the LLMs, indicating they underrepresent minority opinions compared to human data.", "conclusion": "These findings challenge the validity of using LLMs as proxies for human survey data, raising concerns about reinforcing stereotypes and misinformation in policy-making.", "key_contributions": ["Identification of structural inconsistencies in LLM responses", "Highlighting the issue of homogenization in representing minority opinions", "Proposing an 'accuracy-optimization hypothesis' for LLM behavior"], "limitations": "The study is limited to the specific topics of abortion and unauthorized immigration and may not generalize to other issues or contexts.", "keywords": ["large language models", "ChatGPT", "Llama", "opinion representation", "homogenization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.02914", "pdf": "https://arxiv.org/pdf/2507.02914.pdf", "abs": "https://arxiv.org/abs/2507.02914", "title": "OAK -- Onboarding with Actionable Knowledge", "authors": ["Steve Devènes", "Marine Capallera", "Robin Cherix", "Elena Mugellini", "Omar Abou Khaled", "Francesco Carrino"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "This paper is an extended version of the work originally presented at\n  the AI-Days 2024 conference in Lausanne, Switzerland. It builds upon the\n  findings shared during the conference and includes additional results and\n  analysis", "summary": "The loss of knowledge when skilled operators leave poses a critical issue for\ncompanies. This know-how is diverse and unstructured. We propose a novel method\nthat combines knowledge graph embeddings and multi-modal interfaces to collect\nand retrieve expertise, making it actionable. Our approach supports\ndecision-making on the shop floor. Additionally, we leverage LLMs to improve\nquery understanding and provide adapted answers. As application case studies,\nwe developed a proof-of-concept for quality control in high precision\nmanufacturing.", "AI": {"tldr": "This paper presents a method using knowledge graph embeddings and multi-modal interfaces to enhance expertise retrieval and decision-making in manufacturing while utilizing LLMs for query improvement.", "motivation": "Addressing the critical loss of knowledge when skilled operators leave companies, which affects decision-making in operational contexts.", "method": "Combining knowledge graph embeddings with multi-modal interfaces to collect and retrieve expertise, along with leveraging LLMs for better query understanding.", "result": "Demonstrated a proof-of-concept for improving quality control processes in high precision manufacturing through this novel approach.", "conclusion": "The proposed method effectively makes unstructured expertise actionable, supporting informed decision-making on the shop floor.", "key_contributions": ["Novel integration of knowledge graph embeddings and multi-modal interfaces", "Utilization of LLMs for enhanced query handling", "Practical application in quality control of manufacturing processes"], "limitations": "", "keywords": ["Knowledge Graph", "Expertise Retrieval", "Multi-modal Interfaces", "LLM", "Manufacturing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.02927", "pdf": "https://arxiv.org/pdf/2507.02927.pdf", "abs": "https://arxiv.org/abs/2507.02927", "title": "A Unified Speech LLM for Diarization and Speech Recognition in Multilingual Conversations", "authors": ["Phurich Saengthong", "Boonnithi Jiaramaneepinit", "Sheng Li", "Manabu Okumura", "Takahiro Shinozaki"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech Large Language Models (Speech LLMs) have emerged as a crucial paradigm\nin recent years, extending the capabilities of traditional LLMs to speech tasks\nsuch as automatic speech recognition (ASR) and spoken dialogue modeling.\nHowever, their effectiveness in real-world multilingual conversations remains\nlimited by the scarcity of data that captures natural conversational phenomena.\nTo address this, the MLC-SLM Challenge provides a multilingual conversational\ndataset and evaluates models on two tasks: ASR with oracle segmentation (Task\nI) and joint diarization and recognition without oracle information (Task II).\nIn this paper, we focus on Task II and propose a unified speech LLM that\njointly performs diarization and ASR in an end-to-end manner. By reformulating\nthe training data format and modifying the inference procedure, our model\naddresses the ambiguity inherent in pre-segmented audio and achieves a 54.87\\%\nrelative improvement in tcpWER/tcpCER over the baseline, ranking 8th overall,\ndespite using a smaller LLM backbone. We also report results from Task I using\na fine-tuned speech LLM.", "AI": {"tldr": "This paper proposes a unified speech LLM for diarization and ASR, addressing limitations in real-world multilingual conversations and showing significant improvements in performance.", "motivation": "To enhance the effectiveness of Speech LLMs in real-world multilingual conversations, which are constrained by limited data capturing natural conversational phenomena.", "method": "The authors propose a unified speech LLM that addresses diarization and ASR jointly in an end-to-end manner, reformulating the training data format and modifying the inference procedure.", "result": "The proposed model achieves a 54.87% relative improvement in tcpWER/tcpCER over the baseline and ranks 8th overall, despite using a smaller LLM backbone.", "conclusion": "The study demonstrates that it is possible to improve performance in multilingual conversations by reformulating training data and inference procedures.", "key_contributions": ["Introduction of a unified model for joint diarization and ASR tasks.", "Significant performance improvement in tcpWER/tcpCER, demonstrating the model's effectiveness.", "Contribution to a multilingual conversational dataset through the MLC-SLM Challenge."], "limitations": "The study may be limited by the smaller LLM backbone in comparison to state-of-the-art models.", "keywords": ["Speech Large Language Models", "automatic speech recognition", "multilingual conversations", "diarization", "ASR"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.02920", "pdf": "https://arxiv.org/pdf/2507.02920.pdf", "abs": "https://arxiv.org/abs/2507.02920", "title": "Visual-Conversational Interface for Evidence-Based Explanation of Diabetes Risk Prediction", "authors": ["Reza Samimi", "Aditya Bhattacharya", "Lucija Gosak", "Gregor Stiglic", "Katrien Verbert"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "18 pages, 5 figures, 7th ACM Conference on Conversational User\n  Interfaces", "summary": "Healthcare professionals need effective ways to use, understand, and validate\nAI-driven clinical decision support systems. Existing systems face two key\nlimitations: complex visualizations and a lack of grounding in scientific\nevidence. We present an integrated decision support system that combines\ninteractive visualizations with a conversational agent to explain diabetes risk\nassessments. We propose a hybrid prompt handling approach combining fine-tuned\nlanguage models for analytical queries with general Large Language Models\n(LLMs) for broader medical questions, a methodology for grounding AI\nexplanations in scientific evidence, and a feature range analysis technique to\nsupport deeper understanding of feature contributions. We conducted a\nmixed-methods study with 30 healthcare professionals and found that the\nconversational interactions helped healthcare professionals build a clear\nunderstanding of model assessments, while the integration of scientific\nevidence calibrated trust in the system's decisions. Most participants reported\nthat the system supported both patient risk evaluation and recommendation.", "AI": {"tldr": "An integrated decision support system for diabetes risk assessments combining interactive visualizations and a conversational agent, addressing pitfalls in current AI-driven clinical decision support systems.", "motivation": "To create effective AI-driven clinical decision support systems for healthcare professionals that are understandable and grounded in scientific evidence.", "method": "Hybrid prompt handling approach utilizing fine-tuned language models for analytical queries and general LLMs for broader questions, along with a feature range analysis technique.", "result": "The mixed-methods study with 30 healthcare professionals demonstrated improved understanding of model assessments through conversational interactions and enhanced trust in the system due to integration with scientific evidence.", "conclusion": "The system successfully supported healthcare professionals in evaluating patient risk and making recommendations with an increased understanding and trust in AI decisions.", "key_contributions": ["Development of a hybrid prompt handling approach combining fine-tuned LLMs with general LLMs", "Methodology for grounding AI explanations in scientific evidence", "Feature range analysis technique for deeper understanding of feature contributions"], "limitations": "", "keywords": ["AI-driven clinical decision support", "diabetes risk assessment", "conversational agent", "interactive visualizations", "healthcare professionals"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2507.02928", "pdf": "https://arxiv.org/pdf/2507.02928.pdf", "abs": "https://arxiv.org/abs/2507.02928", "title": "Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models", "authors": ["Hao Yang", "Haoxuan Li", "Luyu Chen", "Haoxiang Wang", "Xu Chen", "Mingming Gong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hidden confounding remains a central challenge in estimating treatment\neffects from observational data, as unobserved variables can lead to biased\ncausal estimates. While recent work has explored the use of large language\nmodels (LLMs) for causal inference, most approaches still rely on the\nunconfoundedness assumption. In this paper, we make the first attempt to\nmitigate hidden confounding using LLMs. We propose ProCI (Progressive\nConfounder Imputation), a framework that elicits the semantic and world\nknowledge of LLMs to iteratively generate, impute, and validate hidden\nconfounders. ProCI leverages two key capabilities of LLMs: their strong\nsemantic reasoning ability, which enables the discovery of plausible\nconfounders from both structured and unstructured inputs, and their embedded\nworld knowledge, which supports counterfactual reasoning under latent\nconfounding. To improve robustness, ProCI adopts a distributional reasoning\nstrategy instead of direct value imputation to prevent the collapsed outputs.\nExtensive experiments demonstrate that ProCI uncovers meaningful confounders\nand significantly improves treatment effect estimation across various datasets\nand LLMs.", "AI": {"tldr": "ProCI is a framework that utilizes large language models (LLMs) to mitigate hidden confounding in observational data by generating and validating confounders, improving treatment effect estimation.", "motivation": "Hidden confounding poses significant challenges in causal inference from observational data, potentially leading to biased estimates.", "method": "ProCI (Progressive Confounder Imputation) employs LLMs to iteratively generate, impute, and validate hidden confounders by leveraging their semantic reasoning and world knowledge capabilities.", "result": "Extensive experiments show that ProCI successfully uncovers meaningful confounders and enhances treatment effect estimation across different datasets and LLMs.", "conclusion": "The approach of using LLMs to address hidden confounding provides a novel contribution to causal inference methods, demonstrating improved robustness in estimates.", "key_contributions": ["Introduces ProCI framework for imputation of hidden confounders using LLMs.", "Employs distributional reasoning strategy for robustness in causal estimates.", "Demonstrates effectiveness across various datasets."], "limitations": "", "keywords": ["large language models", "causal inference", "hidden confounding", "treatment effect estimation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.03032", "pdf": "https://arxiv.org/pdf/2507.03032.pdf", "abs": "https://arxiv.org/abs/2507.03032", "title": "Enhanced knowledge retention through MedScrab: an interactive mobile game", "authors": ["Don Roosan", "Tiffany Khao", "Huong Phan", "Yan Li"], "categories": ["cs.HC", "q-bio.OT"], "comment": "Conference can be found at: https://medinfo2025.org/Home/Program", "summary": "Noncompliance with medication regimens poses an immense challenge in the\nmanagement of chronic diseases, often resulting in exacerbated health\ncomplications and recurrent hospital admissions. Addressing this gap, our team\ndesigned an innovative mobile game aimed at bolstering medication adherence and\ninformation retention within the general population. Employing Amazon\nMechanical Turk, participants were enlisted and allocated into two cohorts: one\nengaged with our mobile game and the other perused an informational pamphlet\nabout medication. Both cohorts underwent a pre-intervention quiz, followed by\ntheir respective interventions, and concluded with a post-intervention quiz.\nPrimary outcome measures included the difference in quiz scores and the game\nplay duration. The investigation encompassed 243 participants with homogenous\nbaseline attributes. Participants interacting with the mobile game depicted a\nsignificant enhancement in their post-intervention scores compared to the\npre-intervention scores. We observed a notable correlation of 0.346 (p<0.001)\nwith a robust medium effect size of 0.641 (0.503 - 0.779). Although the\nduration of game play and post-intervention scores didn't exhibit a direct\ncorrelation, a tendency towards superior post-intervention scores was evident\namong participants who dedicated more time to the game. The interactive mobile\ngame we developed exhibits potential as an engaging instrument for empowering\npatients and caregivers. Providing critical medication information and the\npotential side effects in a manner that increases retention would thereby\nmitigate medication noncompliance. Future research endeavors should focus on\noptimizing and broadening the application of such mobile interfaces to fortify\npublic health initiatives.", "AI": {"tldr": "A mobile game was developed to improve medication adherence among chronic disease patients, showing significant improvements in knowledge retention compared to traditional pamphlet methods.", "motivation": "Noncompliance with medication regimens significantly impacts chronic disease management, leading to worsened health outcomes and increased hospitalizations.", "method": "Participants were divided into two groups: one played the mobile game while the other read an informational pamphlet. Both groups took quizzes before and after the interventions to measure changes in knowledge retention.", "result": "Participants who used the mobile game showed a significant increase in post-intervention quiz scores compared to their pre-intervention scores, indicating improved retention and understanding of medication information.", "conclusion": "The mobile game is a promising tool for enhancing patient education and adherence to medication regimens, with potential implications for public health initiatives.", "key_contributions": ["Development of an interactive mobile game for medication adherence", "Demonstration of improved knowledge retention through gamification", "Establishment of a methodological framework for future health education interventions"], "limitations": "", "keywords": ["medication adherence", "mobile game", "public health", "chronic disease", "patient education"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.02935", "pdf": "https://arxiv.org/pdf/2507.02935.pdf", "abs": "https://arxiv.org/abs/2507.02935", "title": "Theory of Mind in Action: The Instruction Inference Task", "authors": ["Fardin Saad", "Pradeep K. Murukannaiah", "Munindar P. Singh"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "Submitted to Artificial Intelligence Journal (under review). 51 pages\n  with appendix (28 pages article + 4 pages references + 19 pages appendix), 7\n  figures (Appendix: 26 Figures), 6 tables. Code available at:\n  https://github.com/fardinsaad/Tomcat-LLM", "summary": "The Theory of Mind (ToM) refers to an agent's capacity to infer the mental\nstates of other agents. ToM is essential for effective collaboration. To assess\nToM in a dynamic, goal-oriented, and collaborative environment, we introduce a\nnovel task, Instruction Inference, in which an agent assists a principal in\nreaching a goal by interpreting indirect or ambiguous instructions. We present\nTomcat, an LLM-based agent, designed to exhibit ToM reasoning in interpreting\nand responding to the principal's instructions. We implement two variants of\nTomcat. One, dubbed Fs-CoT, is based on a small number of examples (i.e.,\nfew-shot or Fs) demonstrating the requisite structured reasoning (i.e.,\nchain-of-thought or CoT). One, dubbed CP, relies on commonsense knowledge and\ninformation about the problem (i.e., commonsense prompt or CP). We realized\nboth variants of Tomcat on three leading large language models (LLMs), namely,\nGPT-4o, DeepSeek-R1, and Gemma-3-27B. To evaluate the effectiveness of Tomcat,\nwe conducted a study with 52 human participants in which we provided\nparticipants with the same information as the CP variant of Tomcat. We computed\nintent accuracy, action optimality, and planning optimality to measure the ToM\ncapabilities of Tomcat and our study participants. We found that Tomcat with\nFs-CoT, particularly with GPT-4o and DeepSeek-R1, achieves performance\ncomparable to the human participants, underscoring its ToM potential for\nhuman-AI collaboration.", "AI": {"tldr": "Introducing Tomcat, an LLM-based agent designed to infer mental states and interpret ambiguous instructions for effective collaboration.", "motivation": "The capacity to infer mental states, known as Theory of Mind (ToM), is essential for collaboration, especially in dynamic environments.", "method": "The authors developed a task called Instruction Inference and built two variants of Tomcat: Fs-CoT, which uses few-shot structured reasoning, and CP, which leverages commonsense knowledge.", "result": "Tomcat, particularly with the Fs-CoT variant using GPT-4o and DeepSeek-R1, showed performance comparable to human participants in a study measuring intent accuracy and planning optimality.", "conclusion": "The performance of Tomcat indicates its potential for enhancing human-AI collaboration through improved ToM capabilities.", "key_contributions": ["Introduction of the Instruction Inference task for assessing ToM in AI agents.", "Development of the Tomcat agent with two reasoning variants: Fs-CoT and CP.", "Demonstrated capability of Tomcat to perform comparably to human participants in collaborative settings."], "limitations": "Limited to evaluation with 52 human participants and specific LLM models; further studies are needed to generalize findings.", "keywords": ["Theory of Mind", "Human-AI collaboration", "Large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.03147", "pdf": "https://arxiv.org/pdf/2507.03147.pdf", "abs": "https://arxiv.org/abs/2507.03147", "title": "DeepGesture: A conversational gesture synthesis system based on emotions and semantics", "authors": ["Thanh Hoang-Minh"], "categories": ["cs.HC", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Video Demo: https://www.youtube.com/watch?v=eZghfNGmZn8", "summary": "Along with the explosion of large language models, improvements in speech\nsynthesis, advancements in hardware, and the evolution of computer graphics,\nthe current bottleneck in creating digital humans lies in generating character\nmovements that correspond naturally to text or speech inputs.\n  In this work, we present DeepGesture, a diffusion-based gesture synthesis\nframework for generating expressive co-speech gestures conditioned on\nmultimodal signals-text, speech, emotion, and seed motion. Built upon the\nDiffuseStyleGesture model, DeepGesture introduces novel architectural\nenhancements that improve semantic alignment and emotional expressiveness in\ngenerated gestures. Specifically, we integrate fast text transcriptions as\nsemantic conditioning and implement emotion-guided classifier-free diffusion to\nsupport controllable gesture generation across affective states. A lightweight\nTransformer backbone combines full self-attention and cross-local attention for\neffective feature fusion of heterogeneous modalities. To visualize results, we\nimplement a full rendering pipeline in Unity based on BVH output from the\nmodel. Evaluation on the ZeroEGGS dataset shows that DeepGesture produces\ngestures with improved human-likeness and contextual appropriateness,\noutperforming baselines on Mean Opinion Score and Frechet Gesture Distance\nmetrics. Our system supports interpolation between emotional states and\ndemonstrates generalization to out-of-distribution speech, including synthetic\nvoices-marking a step forward toward fully multimodal, emotionally aware\ndigital humans.", "AI": {"tldr": "DeepGesture is a diffusion-based framework for generating expressive co-speech gestures using multimodal signals such as text, speech, and emotion.", "motivation": "The current challenge in creating digital humans is generating character movements that naturally align with text or speech inputs, necessitating improved gesture synthesis methods.", "method": "DeepGesture uses a diffusion model enhanced with a lightweight Transformer backbone to condition gesture generation on semantic elements (text transcription) and emotional states, employing emotion-guided classifier-free diffusion.", "result": "Evaluation shows that DeepGesture-generated gestures have improved human-likeness and contextual appropriateness, outperforming existing baselines on key metrics.", "conclusion": "DeepGesture represents a significant advancement in multimodal gesture synthesis, enabling the generation of emotionally aware digital humans and supporting interpolation between emotional states.", "key_contributions": ["Introduced a novel diffusion-based gesture synthesis framework.", "Enhanced semantic alignment and emotional expressiveness in gestures.", "Developed a visualization pipeline in Unity for gesture rendering."], "limitations": "", "keywords": ["gesture synthesis", "multimodal signals", "emotion recognition", "digital humans", "diffusion model"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.02938", "pdf": "https://arxiv.org/pdf/2507.02938.pdf", "abs": "https://arxiv.org/abs/2507.02938", "title": "A Large Language Model-Empowered Agent for Reliable and Robust Structural Analysis", "authors": ["Jiachen Liu", "Ziheng Geng", "Ran Cao", "Lu Cheng", "Paolo Bocchini", "Minghui Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have exhibited remarkable capabilities across\ndiverse open-domain tasks, yet their application in specialized domains such as\ncivil engineering remains largely unexplored. This paper starts bridging this\ngap by evaluating and enhancing the reliability and robustness of LLMs in\nstructural analysis of beams. Reliability is assessed through the accuracy of\ncorrect outputs under repetitive runs of the same problems, whereas robustness\nis evaluated via the performance across varying load and boundary conditions. A\nbenchmark dataset, comprising eight beam analysis problems, is created to test\nthe Llama-3.3 70B Instruct model. Results show that, despite a qualitative\nunderstanding of structural mechanics, the LLM lacks the quantitative\nreliability and robustness for engineering applications. To address these\nlimitations, a shift is proposed that reframes the structural analysis as code\ngeneration tasks. Accordingly, an LLM-empowered agent is developed that (a)\nintegrates chain-of-thought and few-shot prompting to generate accurate\nOpeeSeesPy code, and (b) automatically executes the code to produce structural\nanalysis results. Experimental results demonstrate that the agent achieves\naccuracy exceeding 99.0% on the benchmark dataset, exhibiting reliable and\nrobust performance across diverse conditions. Ablation studies highlight the\ncomplete example and function usage examples as the primary contributors to the\nagent's enhanced performance.", "AI": {"tldr": "This paper evaluates and enhances the use of large language models (LLMs) in structural analysis of beams, proposing a shift towards code generation tasks for improved reliability and robustness.", "motivation": "To bridge the gap in applying LLMs to specialized domains like civil engineering, particularly in structural analysis of beams.", "method": "A benchmark dataset is created comprising eight beam analysis problems to assess the Llama-3.3 70B Instruct model's reliability and robustness, followed by developing an LLM-empowered agent that generates and executes code for structural analysis.", "result": "The proposed agent achieves over 99.0% accuracy on the benchmark dataset and demonstrates reliable and robust performance across varying load and boundary conditions.", "conclusion": "The study concludes that reframing structural analysis as code generation tasks enhances the performance of LLMs in engineering applications, addressing their limitations.", "key_contributions": ["Creation of a benchmark dataset for beam analysis problems", "Development of an LLM-empowered agent for code generation", "Demonstrated high accuracy and robust performance in structural analysis tasks"], "limitations": "Current LLMs still lack quantitative reliability and robustness for engineering applications without the proposed enhancements.", "keywords": ["large language models", "structural analysis", "code generation", "civil engineering", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.03170", "pdf": "https://arxiv.org/pdf/2507.03170.pdf", "abs": "https://arxiv.org/abs/2507.03170", "title": "ASCRIBE-XR: Virtual Reality for Visualization of Scientific Imagery", "authors": ["Ronald J. Pandolfi", "Jeffrey J. Donatelli", "Julian Todd", "Daniela Ushizima"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "ASCRIBE-XR, a novel computational platform designed to facilitate the\nvisualization and exploration of 3D volumetric data and mesh data in the\ncontext of synchrotron experiments, is described. Using Godot and PC-VR\ntechnologies, the platform enables users to dynamically load and manipulate 3D\ndata sets to gain deeper insights into their research. The program's multi-user\ncapabilities, enabled through WebRTC, and MQTT, allow multiple users to share\ndata and visualize together in real-time, promoting a more interactive and\nengaging research experience. We describe the design and implementation of\nASCRIBE-XR, highlighting its key features and capabilities. We will also\ndiscuss its utility in the context of synchrotron research, including examples\nof its application and potential benefits for the scientific community.", "AI": {"tldr": "ASCRIBE-XR is a computational platform for visualizing and exploring 3D volumetric and mesh data, utilizing Godot and PC-VR technologies, with multi-user capabilities for synchrotron experiments.", "motivation": "To enhance visualization and exploration of 3D data in synchrotron research, allowing users to gain deeper insights through interactive engagement.", "method": "The platform utilizes Godot and PC-VR technologies to enable dynamic loading and manipulation of 3D data sets, incorporating multi-user capabilities via WebRTC and MQTT for real-time collaboration.", "result": "ASCRIBE-XR facilitates an interactive research experience by allowing multiple users to visualize and share 3D data simultaneously, promoting collaborative insights.", "conclusion": "The design and implementation of ASCRIBE-XR demonstrate its potential to significantly improve the research workflow and data understanding in synchrotron experiments.", "key_contributions": ["Development of a novel platform for 3D data visualization in synchrotron experiments.", "Integration of multi-user capabilities for real-time collaboration.", "Application examples showing the benefits for the scientific community."], "limitations": "", "keywords": ["3D visualization", "synchrotron", "collaborative research", "PC-VR", "Godot"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2507.02940", "pdf": "https://arxiv.org/pdf/2507.02940.pdf", "abs": "https://arxiv.org/abs/2507.02940", "title": "Towards a Comparative Framework for Compositional AI Models", "authors": ["Tiffany Duneau"], "categories": ["cs.CL", "cs.AI", "quant-ph"], "comment": null, "summary": "The DisCoCirc framework for natural language processing allows the\nconstruction of compositional models of text, by combining units for individual\nwords together according to the grammatical structure of the text. The\ncompositional nature of a model can give rise to two things: compositional\ngeneralisation -- the ability of a model to generalise outside its training\ndistribution by learning compositional rules underpinning the entire data\ndistribution -- and compositional interpretability -- making sense of how the\nmodel works by inspecting its modular components in isolation, as well as the\nprocesses through which these components are combined. We present these notions\nin a framework-agnostic way using the language of category theory, and adapt a\nseries of tests for compositional generalisation to this setting.\n  Applying this to the DisCoCirc framework, we consider how well a selection of\nmodels can learn to compositionally generalise. We compare both quantum circuit\nbased models, as well as classical neural networks, on a dataset derived from\none of the bAbI tasks, extended to test a series of aspects of\ncompositionality. Both architectures score within 5% of one another on the\nproductivity and substitutivity tasks, but differ by at least 10% for the\nsystematicity task, and exhibit different trends on the overgeneralisation\ntasks. Overall, we find the neural models are more prone to overfitting the\nTrain data. Additionally, we demonstrate how to interpret a compositional model\non one of the trained models. By considering how the model components interact\nwith one another, we explain how the model behaves.", "AI": {"tldr": "The DisCoCirc framework enables compositional models in NLP by integrating word units according to grammatical structures, resulting in improved generalization and interpretability.", "motivation": "To develop a framework that supports compositional models in NLP, facilitating generalization and interpretability of language processing.", "method": "The paper introduces the DisCoCirc framework and applies it to test compositional generalization using both quantum circuit models and classical neural networks.", "result": "Neural models showed a higher tendency to overfit the training data compared to quantum models, with both architectures performing similarly on some tasks but differing significantly on systematicity.", "conclusion": "The findings reveal the strengths and weaknesses of different architectures in understanding and generalizing from language through compositionality.", "key_contributions": ["Introduction of the DisCoCirc framework for NLP", "Testing compositional generalization across model types", "Insights into model interpretability through modular components"], "limitations": "The study may be limited by the specific datasets used and the generalizability of results across other tasks.", "keywords": ["Natural Language Processing", "Compositional Models", "Compositional Generalization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.03243", "pdf": "https://arxiv.org/pdf/2507.03243.pdf", "abs": "https://arxiv.org/abs/2507.03243", "title": "Beyond Charging Anxiety: An Explainable Approach to Understanding User Preferences of EV Charging Stations Using Review Data", "authors": ["Zifei Wang", "Emmanuel Abolarin", "Kai Wu", "Venkatarao Rebba", "Jian Hu", "Zhen Hu", "Shan Bao", "Feng Zhou"], "categories": ["cs.HC"], "comment": "19 pages, 8 figures", "summary": "Electric vehicles (EVs) charging infrastructure is directly related to the\noverall EV user experience and thus impacts the widespread adoption of EVs.\nUnderstanding key factors that affect EV users' charging experience is\nessential for building a robust and user-friendly EV charging infrastructure.\nThis study leverages about $17,000$ charging station (CS) reviews on Google\nMaps to explore EV user preferences for charging stations, employing ChatGPT\n4.0 for aspect-based sentiment analysis. We identify twelve key aspects\ninfluencing user satisfaction, ranging from accessibility and reliability to\namenities and pricing. Two distinct preference models are developed: a\nmicro-level model focused on individual user satisfaction and a macro-level\nmodel capturing collective sentiment towards specific charging stations. Both\nmodels utilize the LightGBM algorithm for user preference prediction, achieving\nstrong performance compared to other machine learning approaches. To further\nelucidate the impact of each aspect on user ratings, we employ SHAP (SHapley\nAdditive exPlanations), a game-theoretic approach for interpreting machine\nlearning models. Our findings highlight the significant impact of positive\nsentiment towards \"amenities and location\", coupled with negative sentiment\nregarding \"reliability and maintenance\", on overall user satisfaction. These\ninsights offer actionable guidance to charging station operators, policymakers,\nand EV manufacturers, empowering them to enhance user experience and foster\nwider EV adoption.", "AI": {"tldr": "This study analyzes EV users' charging experience using over 17,000 Google Maps reviews, applying aspect-based sentiment analysis with ChatGPT 4.0 and LightGBM for user preference prediction.", "motivation": "To improve the user experience of EV charging stations and support the widespread adoption of electric vehicles.", "method": "The study conducts aspect-based sentiment analysis on charging station reviews, identifies key aspects influencing user satisfaction, and develops micro and macro-level models using LightGBM. SHAP is employed for interpreting model results.", "result": "The analysis identifies twelve key aspects influencing user satisfaction and demonstrates that sentiments toward amenities and location significantly impact overall user satisfaction, while reliability issues negatively affect ratings.", "conclusion": "The insights gleaned from this study can inform charging station operators, policymakers, and manufacturers on how to improve the charging experience, thus encouraging greater adoption of electric vehicles.", "key_contributions": ["Identification of twelve key aspects affecting user satisfaction with EV charging stations.", "Development of micro and macro-level preference models using machine learning.", "Application of SHAP for interpreting the impact of different aspects on user ratings."], "limitations": "", "keywords": ["Electric Vehicles", "Charging Infrastructure", "User Experience", "Sentiment Analysis", "LightGBM"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.02947", "pdf": "https://arxiv.org/pdf/2507.02947.pdf", "abs": "https://arxiv.org/abs/2507.02947", "title": "The Application of Large Language Models on Major Depressive Disorder Support Based on African Natural Products", "authors": ["Linyan Zou"], "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Major depressive disorder represents one of the most significant global\nhealth challenges of the 21st century, affecting millions of people worldwide\nand creating substantial economic and social burdens. While conventional\nantidepressant therapies have provided relief for many individuals, their\nlimitations including delayed onset of action, significant side effects, and\ntreatment resistance in a substantial portion of patients have prompted\nresearchers and healthcare providers to explore alternative therapeutic\napproaches (Kasneci et al.). African traditional medicine, with its rich\nheritage of plant-based remedies developed over millennia, offers a valuable\nresource for developing novel antidepressant treatments that may address some\nof these limitations. This paper examines the integration of large language\nmodels with African natural products for depression support, combining\ntraditional knowledge with modern artificial intelligence technology to create\naccessible, evidence-based mental health support systems.\n  The research presented here encompasses a comprehensive analysis of African\nmedicinal plants with documented antidepressant properties, their\npharmacological mechanisms, and the development of an AI-powered support system\nthat leverages DeepSeek's advanced language model capabilities. The system\nprovides evidence-based information about African herbal medicines, their\nclinical applications, safety considerations, and therapeutic protocols while\nmaintaining scientific rigor and appropriate safety standards. Our findings\ndemonstrate the potential for large language models to serve as bridges between\ntraditional knowledge and modern healthcare, offering personalized, culturally\nappropriate depression support that honors both traditional wisdom and\ncontemporary medical understanding.", "AI": {"tldr": "This paper explores the integration of large language models with African traditional medicine to develop novel antidepressant treatments and accessible mental health support systems.", "motivation": "Major depressive disorder is a significant global health challenge; traditional treatments have limitations that necessitate exploration of alternative approaches, particularly in African traditional medicine.", "method": "The research involves a comprehensive analysis of African medicinal plants with antidepressant properties and the development of an AI-powered support system using DeepSeek's language model.", "result": "The findings indicate that large language models can bridge traditional knowledge and modern healthcare by providing evidence-based information related to African herbal medicines and personalized depression support.", "conclusion": "The integration of AI and African traditional medicine can enhance mental health support, offering culturally appropriate solutions while maintaining scientific rigor.", "key_contributions": ["Analysis of African medicinal plants with documented antidepressant properties", "Development of an AI-powered support system for mental health", "Integration of traditional knowledge with modern AI technology"], "limitations": "", "keywords": ["depression", "traditional medicine", "large language models", "African herbal medicine", "AI in healthcare"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03286", "pdf": "https://arxiv.org/pdf/2507.03286.pdf", "abs": "https://arxiv.org/abs/2507.03286", "title": "Gaze and Glow: Exploring Editing Processes on Social Media through Interactive Exhibition", "authors": ["Yang Hong", "Jie-Yi Feng", "Yi-Chun Yao", "I-Hsuan Cho", "Yu-Ting Lin", "Ying-Yu Chen"], "categories": ["cs.HC", "cs.MM", "J.5"], "comment": "6 pages, 6 figures, to be published in DIS 2025 (Provocations and\n  Works in Progress)", "summary": "We present Gaze and Glow, an interactive installation that reveals the\noften-invisible efforts of social media editing. Through narrative personas,\nexperimental videos, and sensor-based interactions, the installation explores\nhow audience attention shapes users' editing practices and emotional\nexperiences. Deployed in a two-month public exhibition, Gaze and Glow engaged\nviewers and elicited responses. Reflexive thematic analysis of audience\nfeedback highlights how making editing visible prompts new reflections on\nauthenticity, agency, and performativity. We discuss implications for designing\ninteractive systems that support selective memory, user-controlled visibility,\nand critical engagement with everyday digital self-presentation.", "AI": {"tldr": "Gaze and Glow is an interactive installation that examines social media editing through audience engagement, revealing implications for authenticity and user experience.", "motivation": "The paper aims to illuminate the invisible processes of social media editing and how they are influenced by audience perception and interaction.", "method": "An interactive installation featuring narrative personas and experimental videos, supported by sensor-based interactions, was used to engage viewers during a public exhibition.", "result": "Audience feedback was analyzed using reflexive thematic analysis, indicating that visibility in editing practices encourages new reflections on authenticity and self-presentation.", "conclusion": "The findings suggest important considerations for designing interactive systems that enhance user agency and critical engagement with digital identities.", "key_contributions": ["Revealed the impact of audience attention on social media editing practices.", "Introduced methods for making editing visible in digital narratives.", "Discussed broader implications for user-controlled visibility and memory in design."], "limitations": "The study is based on a single public exhibition, which may limit generalizability.", "keywords": ["Human-Computer Interaction", "social media", "interactive installation", "editing practices", "digital self-presentation"], "importance_score": 5, "read_time_minutes": 6}}
{"id": "2507.02949", "pdf": "https://arxiv.org/pdf/2507.02949.pdf", "abs": "https://arxiv.org/abs/2507.02949", "title": "RADIANT: Retrieval AugmenteD entIty-context AligNmenT -- Introducing RAG-ability and Entity-Context Divergence", "authors": ["Vipula Rawte", "Rajarshi Roy", "Gurpreet Singh", "Danush Khanna", "Yaswanth Narsupalli", "Basab Ghosh", "Abhay Gupta", "Argha Kamal Samanta", "Aditya Shingote", "Aadi Krishna Vikram", "Vinija Jain", "Aman Chadha", "Amit Sheth", "Amitava Das"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) continue to advance, Retrieval-Augmented\nGeneration (RAG) has emerged as a vital technique to enhance factual accuracy\nby integrating external knowledge into the generation process. However, LLMs\noften fail to faithfully integrate retrieved evidence into their generated\nresponses, leading to factual inconsistencies. To quantify this gap, we\nintroduce Entity-Context Divergence (ECD), a metric that measures the extent to\nwhich retrieved information is accurately reflected in model outputs. We\nsystematically evaluate contemporary LLMs on their ability to preserve factual\nconsistency in retrieval-augmented settings, a capability we define as\nRAG-ability. Our empirical analysis reveals that RAG-ability remains low across\nmost LLMs, highlighting significant challenges in entity retention and context\nfidelity. This paper introduces Radiant (Retrieval AugmenteD entIty-context\nAligNmenT), a novel framework that merges RAG with alignment designed to\noptimize the interplay between retrieved evidence and generated content.\nRadiant extends Direct Preference Optimization (DPO) to teach LLMs how to\nintegrate provided additional information into subsequent generations. As a\nbehavior correction mechanism, Radiant boosts RAG performance across varied\nretrieval scenarios, such as noisy web contexts, knowledge conflicts, and\nhallucination reduction. This enables more reliable, contextually grounded, and\nfactually coherent content generation.", "AI": {"tldr": "This paper introduces Entity-Context Divergence (ECD) to measure factual consistency in LLMs and presents Radiant, a framework that enhances Retrieval-Augmented Generation (RAG) performance by optimizing the integration of retrieved evidence into model outputs.", "motivation": "The growing reliance on Large Language Models (LLMs) in generating content necessitates improved factual accuracy through effective integration of external knowledge, particularly in RAG settings where current models struggle.", "method": "The paper introduces the metric Entity-Context Divergence (ECD) to evaluate LLMs' RAG-ability, followed by the development of Radiant, a framework that employs Direct Preference Optimization (DPO) to enhance the alignment of generated content with retrieved evidence.", "result": "Empirical analysis shows that the RAG-ability of contemporary LLMs is low, indicating poor integration of retrieved evidence, which Radiant successfully addresses, enhancing performance across noisy contexts and reducing hallucinations.", "conclusion": "Radiant significantly improves the contextual fidelity and factual accuracy of LLM outputs, making it a critical advancement for effective RAG applications in real-world scenarios.", "key_contributions": ["Introduction of Entity-Context Divergence (ECD) metric for evaluating LLM factual consistency.", "Development of Radiant framework for improved RAG performance by optimizing evidence integration.", "Demonstration of enhanced RAG-ability across various retrieval scenarios."], "limitations": "The paper does not address the underlying causes of low RAG-ability in detail, and further research may be required to explore long-term effects of using the framework in different contexts.", "keywords": ["Research", "Large Language Models", "Retrieval-Augmented Generation", "Entity-Context Divergence", "Alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03307", "pdf": "https://arxiv.org/pdf/2507.03307.pdf", "abs": "https://arxiv.org/abs/2507.03307", "title": "Scaffolding Recursive Divergence and Convergence in Story Ideation", "authors": ["Taewook Kim", "Matthew Kay", "Yuqian Sun", "Melissa Roemmele", "Max Kreminski", "John Joon Young Chung"], "categories": ["cs.HC", "cs.AI"], "comment": "17 pages, 5 figures, 3 tables", "summary": "Human creative ideation involves both exploration of diverse ideas\n(divergence) and selective synthesis of explored ideas into coherent\ncombinations (convergence). While processes of divergence and convergence are\noften interleaved and nested, existing AI-powered creativity support tools\n(CSTs) lack support for sophisticated orchestration of divergence and\nconvergence. We present Reverger, an AI-powered CST that helps users ideate\nvariations of conceptual directions for modifying a story by scaffolding\nflexible iteration between divergence and convergence. For divergence, our tool\nenables recursive exploration of alternative high-level directions for\nmodifying a specific part of the original story. For convergence, it allows\nusers to collect explored high-level directions and synthesize them into\nconcrete variations. Users can then iterate between divergence and convergence\nuntil they find a satisfactory outcome. A within-subject study revealed that\nReverger permitted participants to explore more unexpected and diverse\nhigh-level directions than a comparable baseline. Reverger users also felt that\nthey had more fine-grained control and discovered more effort-worthy outcomes.", "AI": {"tldr": "Reverger is an AI-powered creativity support tool that enhances human creative ideation by facilitating the exploration and synthesis of diverse ideas through iterative processes of divergence and convergence.", "motivation": "Existing AI creativity support tools lack sophisticated orchestration for balancing divergence and convergence during the creative ideation process.", "method": "Reverger allows users to recursively explore alternative high-level directions for modifying a story (divergence) and to synthesize these directions into concrete variations (convergence) through iterative cycles.", "result": "A within-subject study showed that participants using Reverger explored more unexpected and diverse high-level directions compared to a baseline, gaining more control and discovering more valuable outcomes.", "conclusion": "Reverger successfully supports the creative process by allowing flexible iteration between idea exploration and realization, enhancing the overall ideation experience.", "key_contributions": ["Introduction of Reverger as a novel AI-powered creativity support tool", "Facilitation of iterative processes of divergence and convergence during ideation", "Empirical evidence demonstrating enhanced creativity outcomes for users"], "limitations": "", "keywords": ["AI creativity support", "human-computer interaction", "ideation", "divergence and convergence", "story modification"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2507.02950", "pdf": "https://arxiv.org/pdf/2507.02950.pdf", "abs": "https://arxiv.org/abs/2507.02950", "title": "Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria", "authors": ["Keita Kiuchi", "Yoshikazu Fujimoto", "Hideyuki Goto", "Tomonori Hosokawa", "Makoto Nishimura", "Yosuke Sato", "Izumi Sezai"], "categories": ["cs.CL", "cs.AI", "cs.HC", "68T50", "I.2.7; H.5.2; J.4"], "comment": "69 pages, 0 figures, 9 tables; data and code at\n  https://osf.io/p8c39/files/2e58c42f-a7ba-45f2-aa60-265e107e36db", "summary": "This study provides the first comprehensive evaluation of large language\nmodel (LLM) performance across three counseling roles in Japanese-language\ntherapeutic contexts. We simultaneously assessed counselor artificial\nintelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured\nMulti-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations,\nand evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human\nexperts (n = 15) with extensive counseling experience evaluated AI-generated\ndialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding\nManual 4.2.1.\n  Notably, SMDP implementation significantly enhanced counselor AI performance\nacross all MITI global ratings compared with zeroshot prompting, with no\nsignificant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed\ncomparable performance to human raters for Cultivating Change Talk but\nsystematically overestimated Softening Sustain Talk and the overall quality\nmetrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3\nfocused on technical proficiency, and Sonnet prioritized emotional expression.\nClient AI simulations exhibited a limited emotional range and unnaturally high\ncompliance, indicating the need for enhanced realism.\n  These findings establish benchmarks for AI-assisted counseling in non-English\ncontexts and identify critical areas for improvement through advanced prompt\nengineering, retrieval-augmented generation, and targeted fine-tuning, with\nimportant implications for developing culturally sensitive AI mental health\ntools.", "AI": {"tldr": "This study evaluates LLM performance in Japanese counseling contexts, comparing different AI models and prompting methods, establishing benchmarks for AI-assisted therapy.", "motivation": "To assess the performance of large language models in therapeutic roles and identify areas for improvement in AI counseling systems.", "method": "The performance of various AI systems was evaluated through human expert assessment using the Motivational Interviewing Treatment Integrity (MITI) Coding Manual.", "result": "SMDP implementation improved AI counselor performance significantly; evaluation AIs matched human raters in some metrics but had biases, leading to incorrect estimations in certain areas.", "conclusion": "The study highlights the potential of LLMs in counseling, establishing benchmarks and areas for future enhancements in AI mental health tools.", "key_contributions": ["First comprehensive evaluation of LLM in Japanese therapeutic counseling contexts.", "Identification of prompts and models that enhance AI performance in counseling.", "Establishment of benchmarks for AI systems in non-English mental health applications."], "limitations": "Client AI simulations showed limited emotional range and high compliance, indicating unrealistic interactions.", "keywords": ["Large Language Models", "Counseling", "AI Evaluation", "Cultural Sensitivity", "Mental Health"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2507.03391", "pdf": "https://arxiv.org/pdf/2507.03391.pdf", "abs": "https://arxiv.org/abs/2507.03391", "title": "On the dynamics of affective states during play and the role of confusion", "authors": ["Thomas Vase Schultz Volden", "Oleg Jarma Montoya", "Paolo Burelli", "Marco Scirea"], "categories": ["cs.HC"], "comment": "4 pages, 2 figures, Conference on Games", "summary": "Video game designers often view confusion as undesirable, yet it is\ninevitable, as new players must adapt to new interfaces and mechanics in an\nincreasingly varied and innovative game market, which is more popular than\never. Research suggests that confusion can contribute to a positive experience,\npotentially motivating players to learn. The state of confusion in video games\nshould be further investigated to gain more insight into the learning\nexperience of play and how it affects the player experience. In this article,\nwe design a study to collect learning-related affects for users playing a game\nprototype that intentionally confuses the player. We assess the gathered\naffects against a complex learning model, affirming that, in specific\ninstances, the player experience aligns with the learning experiences.\nMoreover, we identify correlations between these affects and the Player\nExperience Inventory constructs, particularly concerning flow experiences.", "AI": {"tldr": "This paper examines the role of confusion in video games as a potentially positive element in the learning experience of players.", "motivation": "Video game designers often see confusion as detrimental; this research explores its potential benefits in learning and player experience.", "method": "A study was designed to collect learning-related affects from users playing a game prototype that was intentionally confusing. These affects were assessed against a complex learning model.", "result": "The study affirmed that confusion can positively align player experiences with learning outcomes, showing correlations between affects and Player Experience Inventory constructs, especially related to flow experiences.", "conclusion": "Understanding the role of confusion in gameplay can enhance insights into player experience and learning, providing new avenues for game design.", "key_contributions": ["Investigation of confusion as a potentially beneficial experience in gaming.", "Correlations identified between confusion, player experience, and learning outcomes.", "Insights for game designers on integrating confusion into gameplay to foster learning."], "limitations": "The study focuses on a specific game prototype, limiting generalizability to other types of games.", "keywords": ["confusion", "player experience", "learning model", "video games", "flow experiences"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.02954", "pdf": "https://arxiv.org/pdf/2507.02954.pdf", "abs": "https://arxiv.org/abs/2507.02954", "title": "Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of Large Language Models on CFA Level III", "authors": ["Pranam Shetty", "Abhisek Upadhayaya", "Parth Mitesh Shah", "Srikanth Jagabathula", "Shilpi Nayak", "Anna Joo Fee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at FinLLM @ IJCAI 2025", "summary": "As financial institutions increasingly adopt Large Language Models (LLMs),\nrigorous domain-specific evaluation becomes critical for responsible\ndeployment. This paper presents a comprehensive benchmark evaluating 23\nstate-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam -\nthe gold standard for advanced financial reasoning. We assess both\nmultiple-choice questions (MCQs) and essay-style responses using multiple\nprompting strategies including Chain-of-Thought and Self-Discover. Our\nevaluation reveals that leading models demonstrate strong capabilities, with\ncomposite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA\nLevel III. These results, achieved under a revised, stricter essay grading\nmethodology, indicate significant progress in LLM capabilities for high-stakes\nfinancial applications. Our findings provide crucial guidance for practitioners\non model selection and highlight remaining challenges in cost-effective\ndeployment and the need for nuanced interpretation of performance against\nprofessional benchmarks.", "AI": {"tldr": "This paper benchmarks 23 LLMs on the CFA Level III exam to evaluate their advanced financial reasoning capabilities.", "motivation": "To ensure responsible deployment of LLMs in financial institutions through rigorous domain-specific evaluation.", "method": "Evaluation of 23 LLMs on CFA Level III exam using multiple-choice questions and essay-style responses with various prompting strategies.", "result": "Leading models scored high, with o4-mini achieving 79.1% and Gemini 2.5 Flash scoring 77.3% on the CFA Level III under a revised grading methodology.", "conclusion": "The findings highlight significant progress in LLM capabilities for financial applications, offering practical guidance for model selection while emphasizing challenges in deployment.", "key_contributions": ["Comprehensive benchmark of 23 state-of-the-art LLMs on the CFA Level III exam", "Introduction of revised essay grading methodology", "Practical guidance for financial institutions on LLM selection"], "limitations": "Focuses solely on financial reasoning; may not generalize to other domains.", "keywords": ["Large Language Models", "financial reasoning", "CFA exam", "benchmarking", "model evaluation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.03520", "pdf": "https://arxiv.org/pdf/2507.03520.pdf", "abs": "https://arxiv.org/abs/2507.03520", "title": "TILES-2018 Sleep Benchmark Dataset: A Longitudinal Wearable Sleep Data Set of Hospital Workers for Modeling and Understanding Sleep Behaviors", "authors": ["Tiantian Feng", "Brandon M Booth", "Karel Mundnich", "Emily Zhou", "Benjamin Girault", "Kristina Lerman", "Shrikanth Narayanan"], "categories": ["cs.HC"], "comment": null, "summary": "Sleep is important for everyday functioning, overall well-being, and quality\nof life. Recent advances in wearable sensing technology have enabled\ncontinuous, noninvasive, and cost-effective monitoring of sleep patterns in\nreal-world natural living settings. Wrist-worn devices, in particular, are\ncapable of tracking sleep patterns using accelerometers and heart rate sensors.\nTo support sleep research in naturalistic environments using wearable sensors,\nwe introduce the TILES-2018 Sleep Benchmark dataset, which we make publicly\navailable to the research community. This dataset was collected over a 10-week\nperiod from 139 hospital employees and includes over 6,000 unique sleep\nrecordings, alongside self-reported survey data from each participant, which\nincludes sleep quality, stress, and anxiety among other measurements. We\npresent in-depth analyses of sleep patterns by combining the TILES-2018 Sleep\nBenchmark dataset with a previously released dataset (TILES-2018), which\nfollows a similar study protocol. Our analyses include sleep duration, sleep\nstages, and sleep diaries. Moreover, we report machine learning benchmarks\nusing this dataset as a testbed for tasks including sleep stage classification,\nprediction of self-reported sleep quality, and classifying demographics.\nOverall, this dataset provides a valuable resource for advancing foundational\nstudies in sleep behavior modeling.", "AI": {"tldr": "The paper introduces the TILES-2018 Sleep Benchmark dataset for sleep pattern analysis using wearable technology, providing machine learning benchmarks for sleep classification tasks.", "motivation": "To support research on sleep patterns using wearable sensors in naturalistic settings and provide a valuable resource for sleep behavior studies.", "method": "The dataset was collected from 139 hospital employees over 10 weeks, consisting of over 6,000 unique sleep recordings and self-reported survey data. Analyses include sleep duration, stages, diaries, and performance benchmarks for machine learning tasks.", "result": "In-depth analyses reveal insights into sleep patterns, and machine learning benchmarks demonstrate the dataset's utility for tasks like sleep stage classification and self-reported sleep quality prediction.", "conclusion": "The TILES-2018 Sleep Benchmark dataset serves as a significant resource for enhancing studies in sleep behavior modeling.", "key_contributions": ["Introduction of the TILES-2018 Sleep Benchmark dataset", "Combination of datasets for comprehensive sleep pattern analysis", "Benchmarks for machine learning tasks related to sleep study"], "limitations": "", "keywords": ["sleep monitoring", "wearable technology", "machine learning", "sleep behavior", "dataset"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.02958", "pdf": "https://arxiv.org/pdf/2507.02958.pdf", "abs": "https://arxiv.org/abs/2507.02958", "title": "Real-World En Call Center Transcripts Dataset with PII Redaction", "authors": ["Ha Dao", "Gaurav Chawla", "Raghu Banda", "Caleb DeLeeuw"], "categories": ["cs.CL", "I.2.7; H.3.3; I.5.4"], "comment": "17 pages, 4 figures. Dataset publicly available at\n  https://huggingface.co/datasets/AIxBlock/91706-real-world-call-center-scripts-english\n  . Contains 91,706 real-world English call center transcripts (10,448 audio\n  hours) with PII redaction. Licensed under CC BY-NC 4.0 for non-commercial\n  research use", "summary": "We introduce CallCenterEN, a large-scale (91,706 conversations, corresponding\nto 10448 audio hours), real-world English call center transcript dataset\ndesigned to support research and development in customer support and sales AI\nsystems. This is the largest release to-date of open source call center\ntranscript data of this kind. The dataset includes inbound and outbound calls\nbetween agents and customers, with accents from India, the Philippines and the\nUnited States. The dataset includes high-quality, PII-redacted human-readable\ntranscriptions. All personally identifiable information (PII) has been\nrigorously removed to ensure compliance with global data protection laws. The\naudio is not included in the public release due to biometric privacy concerns.\nGiven the scarcity of publicly available real-world call center datasets,\nCallCenterEN fills a critical gap in the landscape of available ASR corpora,\nand is released under a CC BY-NC 4.0 license for non-commercial research use.", "AI": {"tldr": "Introduction of CallCenterEN, a comprehensive dataset of English call center transcripts aimed at advancing AI in customer support and sales through real-world data.", "motivation": "To address the lack of publicly available real-world call center datasets for research and development in customer support and sales AI systems.", "method": "The dataset was created by collecting transcripts from 91,706 conversations in call centers, ensuring high-quality, PII-redacted transcriptions.", "result": "CallCenterEN is the largest open-source dataset of its kind, addressing a significant gap in available ASR corpora, and is designed for non-commercial research use.", "conclusion": "This dataset enables better training and evaluation of AI systems for customer support and sales, promoting advancements in the field.", "key_contributions": ["Largest release of open-source call center transcripts", "Includes diverse accents and compliance with global data protection laws", "Facilitates research in customer support and sales AI systems"], "limitations": "Audio files are not included due to biometric privacy concerns.", "keywords": ["call center", "dataset", "ASR", "customer support", "sales AI"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.03670", "pdf": "https://arxiv.org/pdf/2507.03670.pdf", "abs": "https://arxiv.org/abs/2507.03670", "title": "Interaction Techniques that Encourage Longer Prompts Can Improve Psychological Ownership when Writing with AI", "authors": ["Nikhita Joshi", "Daniel Vogel"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Writing longer prompts for an AI assistant to generate a short story\nincreases psychological ownership, a user's feeling that the writing belongs to\nthem. To encourage users to write longer prompts, we evaluated two interaction\ntechniques that modify the prompt entry interface of chat-based generative AI\nassistants: pressing and holding the prompt submission button, and continuously\nmoving a slider up and down when submitting a short prompt. A within-subjects\nexperiment investigated the effects of such techniques on prompt length and\npsychological ownership, and results showed that these techniques increased\nprompt length and led to higher psychological ownership than baseline\ntechniques. A second experiment further augmented these techniques by showing\nAI-generated suggestions for how the prompts could be expanded. This further\nincreased prompt length, but did not lead to improvements in psychological\nownership. Our results show that simple interface modifications like these can\nelicit more writing from users and improve psychological ownership.", "AI": {"tldr": "This paper investigates interaction techniques to encourage longer prompts in AI writing assistants, enhancing user psychological ownership.", "motivation": "The study aims to explore how modifying the prompt entry interface can encourage users to write longer prompts and enhance their sense of ownership over the content.", "method": "A within-subjects experiment was conducted to evaluate two interaction techniques: pressing and holding the prompt submission button and moving a slider for prompt submission. A second experiment tested the addition of AI-generated suggestions for prompt expansion.", "result": "Both interaction techniques significantly increased prompt length and psychological ownership compared to baseline methods. However, while showing AI-generated suggestions further increased prompt length, it did not enhance psychological ownership.", "conclusion": "Simple modifications to the user interface can effectively encourage users to produce longer writing while also positively impacting their psychological ownership of the generated content.", "key_contributions": ["Introduction of interaction techniques to increase prompt length", "Demonstration of the effect of UI changes on psychological ownership", "Assessment of AI-generated suggestions for improving prompts"], "limitations": "The study primarily focuses on chat-based assistants and may not generalize to other contexts or types of generative interfaces.", "keywords": ["HCI", "AI assistants", "psychological ownership", "prompt length", "user interface"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.02962", "pdf": "https://arxiv.org/pdf/2507.02962.pdf", "abs": "https://arxiv.org/abs/2507.02962", "title": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism", "authors": ["Zhiwen Tan", "Jiaming Huang", "Qintong Wu", "Hongxuan Zhang", "Chenyi Zhuang", "Jinjie Gu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while they remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have explored enhancing models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to the single-query mode. In this paper, we propose\nRAG-R1, a novel training framework designed to enable LLMs to adaptively\nleverage internal and external knowledge during the reasoning process. We\nfurther expand the generation and retrieval processes within the framework from\nsingle-query mode to multi-query parallelism, aimed at reducing inference time\nand enhancing the model's capabilities. Extensive experiments on seven\nquestion-answering benchmarks demonstrate that our method outperforms the\nstrongest baseline by up to 13.2% and decreases inference time by 11.1%.", "AI": {"tldr": "The paper presents RAG-R1, a framework that improves Large Language Models (LLMs) by allowing them to leverage both internal and external knowledge efficiently during reasoning, using a multi-query approach to enhance performance and reduce inference time.", "motivation": "To address the limitations of existing Retrieval-Augmented Generation methods in LLMs, such as hallucinations, outdated responses, and inefficiency during inference.", "method": "Introducing RAG-R1, which adapts LLMs to utilize internal and external knowledge interactively and expands the query process from single to multi-query parallelism.", "result": "RAG-R1 outperforms the strongest baseline by up to 13.2% on various question-answering benchmarks and reduces inference time by 11.1%.", "conclusion": "RAG-R1 demonstrates significant improvements in both answer accuracy and efficiency over existing methods for enhancing LLM capabilities.", "key_contributions": ["Development of RAG-R1 training framework for LLMs", "Implementation of multi-query parallelism", "Improved performance metrics on question-answering tasks"], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Reinforcement Learning", "multi-query", "question-answering"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.03797", "pdf": "https://arxiv.org/pdf/2507.03797.pdf", "abs": "https://arxiv.org/abs/2507.03797", "title": "Assessing the Viability of Wave Field Synthesis in VR-Based Cognitive Research", "authors": ["Benjamin Kahl"], "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS"], "comment": "35 pages", "summary": "This paper investigates the viability of Wave Field Synthesis (WFS) for\nenhancing auditory immersion in VR-based cognitive research. While Virtual\nReality (VR) offers significant advantages for studying human perception and\nbehavior, auditory cues are often underutilized. WFS, an advanced audio\nrendering technique, can create highly realistic and spatially accurate\nsoundscapes, potentially increasing ecological validity. This study evaluates\nWFS by implementing a sample experiment where participants localize static and\nmoving sound sources in both a WFS-rendered environment and a conventional\nstereo headphone setup. The research explores the impact of virtual\nenvironments, sound types, and durations on localization accuracy and search\nbehavior. Findings indicate that while stereo setups can achieve higher\naccuracy, WFS provides a more natural and intuitive auditory experience,\nparticularly for directional cues. The study also highlights limitations of\ncurrent WFS systems, such as the lack of height localization, occlusion\nsimulation, and user-dependent optimization, which affect performance,\nespecially for centrally located sound sources. Despite these challenges, WFS\nshows promise for specialized auditory perception research, particularly for\ncomplex soundscapes where directional information is paramount.", "AI": {"tldr": "This study evaluates Wave Field Synthesis (WFS) for enhancing auditory immersion in VR cognitive research, comparing its effectiveness to conventional stereo headphone setups.", "motivation": "To investigate the underutilization of auditory cues in VR and assess the advantages of WFS for realistic soundscapes in cognitive research.", "method": "Participants localized static and moving sound sources in environments rendered with WFS and conventional stereo setups.", "result": "WFS offers a more natural auditory experience, especially for directional cues, despite stereo setups having higher accuracy in some cases.", "conclusion": "WFS shows potential for enhancing auditory perception research, particularly in complex sound environments, though it has limitations in height localization and optimization.", "key_contributions": ["Evaluation of WFS vs. stereo setups for sound localization in VR", "Insights into auditory perception in virtual environments", "Identification of limitations in current WFS systems"], "limitations": "Current WFS systems lack height localization, occlusion simulation, and user-dependent optimization which limit their effectiveness.", "keywords": ["Wave Field Synthesis", "auditory immersion", "virtual reality", "cognitive research", "sound localization"], "importance_score": 6, "read_time_minutes": 35}}
{"id": "2507.02964", "pdf": "https://arxiv.org/pdf/2507.02964.pdf", "abs": "https://arxiv.org/abs/2507.02964", "title": "Less Data, More Security: Advancing Cybersecurity LLMs Specialization via Resource-Efficient Domain-Adaptive Continuous Pre-training with Minimal Tokens", "authors": ["Salahuddin Salahuddin", "Ahmed Hussain", "Jussi Löppönen", "Toni Jutila", "Panos Papadimitratos"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "15 Pages and 10 Figures", "summary": "While Large Language Models (LLMs) demonstrate exceptional natural language\ncapabilities, general-purpose models lack specialized domain knowledge for\neffective cybersecurity analysis. In this work, we investigate Domain-Adaptive\nContinuous Pretraining (DAP) as a methodology for enhancing cybersecurity\nunderstanding in pretrained LLMs while preserving general language\ncapabilities. We systematically adapted three decoder-based architectures --\nLlama-3.1-8B, DeepSeek-R1-Distill-Qwen-14B, and Llama-3.3-70B-Instruct -- using\na curated 126-million-word cybersecurity corpus from standards, academic\nliterature, and various other sources. Our approach employed constrained\ntraining parameters and distributed FSDP training to balance domain\nspecialization with knowledge preservation. Evaluation across three\ncybersecurity benchmarks, namely, CTI-MCQ, CyberMetric, and SecEval,\ndemonstrates consistent improvements post-adaptation. The Llama-3.3-70B-Ins-DAP\nmodel achieved state-of-the-art accuracies of 0.718, 0.933, and 0.864,\nrespectively, outperforming specialized models, including Llama-Primus-Base.\nNotably, competitive performance was achieved using substantially smaller\ndatasets (118.8 million versus 2.77 billion tokens), demonstrating efficient\ndomain specialization viability. We establish that targeted continuous\npretraining enables effective cybersecurity domain adaptation with\ncomputational feasibility, providing foundations for specialized AI assistants\nin threat analysis, vulnerability assessment, and security documentation while\nchallenging prevailing assumptions about data requirements for LLM\nspecialization.", "AI": {"tldr": "This paper explores Domain-Adaptive Continuous Pretraining (DAP) for improving cybersecurity analysis in Large Language Models (LLMs), demonstrating effective adaptation with competitive results using smaller datasets.", "motivation": "The study aims to enhance general-purpose LLMs' capabilities in the specialized field of cybersecurity analysis, addressing the lack of domain knowledge in existing models.", "method": "The authors adapted three decoder-based architectures (Llama-3.1-8B, DeepSeek-R1-Distill-Qwen-14B, and Llama-3.3-70B-Instruct) using a curated 126-million-word cybersecurity corpus, applying constrained training parameters and distributed FSDP training.", "result": "The adapted models showed improved performance on cybersecurity benchmarks, achieving state-of-the-art accuracies of 0.718, 0.933, and 0.864, while using smaller datasets than competing specialized models.", "conclusion": "Targeted continuous pretraining is viable for domain adaptation in LLMs, allowing for the development of specialized AI assistants in cybersecurity with reduced data requirements.", "key_contributions": ["Introduction of Domain-Adaptive Continuous Pretraining (DAP) for LLMs in cybersecurity analysis.", "Demonstration of significant improvements in performance using smaller training datasets.", "Establishment of computational feasibility for efficient domain specialization in LLMs."], "limitations": "", "keywords": ["Domain-Adaptive Continuous Pretraining", "Large Language Models", "Cybersecurity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.03892", "pdf": "https://arxiv.org/pdf/2507.03892.pdf", "abs": "https://arxiv.org/abs/2507.03892", "title": "Is AI mingling or bullying me? Exploring User Interactions with a Chatbot in China", "authors": ["Nuo Chen", "Pu Yan", "Jia Li", "Qixuan Zhao"], "categories": ["cs.HC"], "comment": null, "summary": "Since its viral emergence in early 2024, Comment Robert-a Weibo-launched\nsocial chatbot-has gained widespread attention on the Chinese Internet for its\nunsolicited and unpredictable comments on user posts. Unlike conventional\nchatbots that respond only to user prompts, Robert autonomously intervenes in\npublic discourse, representing a novel form of AI-driven social media\nengagement. This study examines how such autonomous, algorithmic communication\nreshapes human-AI interaction in everyday online contexts. Using computational\nlinguistics techniques, including topic classification and sentiment analysis,\nwe analyze over 3,900 user-submitted interactions from the \"Robert Victims\nAlliance\", a grassroots community documenting their exchanges with the chatbot.\nTopic modeling reveals six key themes: interpersonal relationships,\nself-identity, academic and career concerns, subcultures, sensitive topics, and\nsocial events. Complementing this, mixed-methods emotional analysis uncovers a\ncomplex affective spectrum: Robert's casual remarks can evoke warmth and humor\nbut may also conceal covert hostility beneath neutral or polite language. These\nambivalent interactions reveal an emerging emotional divide between humans and\nsocially proactive AI, suggesting that while Robert simulates social presence,\nit often falls short of users' emotional needs. Our study contributes to\nhuman-AI interaction research by offering new insights into the affective\ndynamics and socio-technical implications of unsolicited AI bots' participation\nin digital public spheres.", "AI": {"tldr": "This study investigates the impact of the autonomous social chatbot Robert on human-AI interaction by analyzing user interactions and the emotional dynamics it creates in public discourse.", "motivation": "To understand how autonomous AI-driven chatbots like Robert influence human communication and the emotional dynamics in digital public spheres.", "method": "Computational linguistics techniques including topic classification and sentiment analysis on over 3,900 user-submitted interactions from the 'Robert Victims Alliance'.", "result": "The analysis identifies six key themes in interactions and uncovers an ambivalent emotional spectrum where Robert's comments can be both humorous and hostile, illustrating a divide between human emotional needs and AI responses.", "conclusion": "Interactions with Robert highlight the limitations of AI in fulfilling human emotional needs, despite simulating social presence.", "key_contributions": ["New insights into human-AI interaction dynamics in social media", "Identification of key themes in user interactions with AI", "Revelation of the emotional divide generated by autonomous AI chatbots"], "limitations": "Focuses only on interactions with a specific chatbot in a particular cultural context; may not generalize to all AI chatbots.", "keywords": ["Human-AI interaction", "Social media", "Emotional dynamics"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.02966", "pdf": "https://arxiv.org/pdf/2507.02966.pdf", "abs": "https://arxiv.org/abs/2507.02966", "title": "PB-LLMs: Privacy- and Bias-aware NLP Models using Named-Entity Recognition", "authors": ["Gonzalo Mancera", "Aythami Morales", "Julian Fierrez", "Ruben Tolosana", "Alejandro Penna", "Miguel Lopez-Duran", "Francisco Jurado", "Alvaro Ortigosa"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "Presented at AAAI Workshop on Privacy-Preserving Artificial\n  Intelligence (PPAI) 2025, Philadelphia, PA, USA, March 2025", "summary": "The use of Natural Language Processing (NLP) in high-stakes AI-based\napplications has increased significantly in recent years, especially since the\nemergence of Large Language Models (LLMs). However, despite their strong\nperformance, LLMs introduce important legal/ethical concerns, particularly\nregarding privacy, data protection, and transparency. Due to these concerns,\nthis work explores the use of Named-Entity Recognition (NER) to facilitate the\nprivacy-preserving training (or adaptation) of LLMs. We propose a framework\nthat uses NER technologies to anonymize sensitive information in text data,\nsuch as personal identities or geographic locations. An evaluation of the\nproposed privacy-preserving learning framework was conducted to measure its\nimpact on user privacy and system performance in a particular high-stakes and\nsensitive setup: AI-based resume scoring for recruitment processes. The study\ninvolved two language models (BERT and RoBERTa) and six anonymization\nalgorithms (based on Presidio, FLAIR, BERT, and different versions of GPT)\napplied to a database of 24,000 candidate profiles. The findings indicate that\nthe proposed privacy preservation techniques effectively maintain system\nperformance while playing a critical role in safeguarding candidate\nconfidentiality, thus promoting trust in the experimented scenario. On top of\nthe proposed privacy-preserving approach, we also experiment applying an\nexisting approach that reduces the gender bias in LLMs, thus finally obtaining\nour proposed Privacy- and Bias-aware LLMs (PB-LLMs). Note that the proposed\nPB-LLMs have been evaluated in a particular setup (resume scoring), but are\ngenerally applicable to any other LLM-based AI application.", "AI": {"tldr": "This paper explores a framework using Named-Entity Recognition (NER) for privacy-preserving training of Large Language Models (LLMs) in high-stakes applications, demonstrating its effectiveness in anonymizing sensitive data while maintaining system performance.", "motivation": "The increasing use of LLMs in critical AI applications raises significant legal and ethical concerns regarding privacy, data protection, and transparency.", "method": "The paper proposes a framework that utilizes NER technologies to anonymize sensitive information in text data, along with an evaluation of its effects on user privacy and system performance in AI-based resume scoring.", "result": "The evaluation showed that the privacy preservation techniques effectively maintain system performance while safeguarding candidate confidentiality, thereby promoting trust.", "conclusion": "The proposed Privacy- and Bias-aware LLMs (PB-LLMs), evaluated in the resume scoring setup, are applicable to any LLM-based AI application.", "key_contributions": ["Development of a NER-based framework for privacy-preserving LLM training", "Demonstration of effective anonymization methods in AI-based recruitment", "Introduction of PB-LLMs that also mitigate gender bias"], "limitations": "", "keywords": ["Natural Language Processing", "Privacy Preservation", "Large Language Models", "Anonymization", "Bias Mitigation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03902", "pdf": "https://arxiv.org/pdf/2507.03902.pdf", "abs": "https://arxiv.org/abs/2507.03902", "title": "The shortcomings of video conferencing technology, methods for revealing them, and emerging XR solutions", "authors": ["Dani Paul Hove", "Benjamin Watson"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Video conferencing has become a central part of our daily lives, thanks to\nthe COVID-19 pandemic. Unfortunately, so have its many limitations, resulting\nin poor support for communicative and social behavior and ultimately, Zoom\nfatigue. New technologies will be required to address these limitations,\nincluding many drawn from mixed reality (XR). In this paper, our goals are to\nequip and encourage future researchers to develop and test such technologies.\nToward this end, we first survey research on the shortcomings of video\nconferencing systems, as defined before and after the pandemic. We then\nconsider the methods that research uses to evaluate support for communicative\nbehavior, and argue that those same methods should be employed in identifying,\nimproving, and validating promising video conferencing technologies. Next, we\nsurvey emerging XR solutions to video conferencing's limitations, most off\nwhich do not employ head-mounted displays.", "AI": {"tldr": "This paper discusses the limitations of video conferencing, especially emphasized by the COVID-19 pandemic, and explores emerging mixed reality technologies as potential solutions.", "motivation": "To address the limitations of video conferencing technologies that have become evident during the COVID-19 pandemic, including poor communication support and Zoom fatigue.", "method": "The paper surveys existing research on video conferencing limitations, evaluates methods for assessing communicative behavior, and explores emerging mixed reality (XR) technologies that do not rely on head-mounted displays.", "result": "The paper highlights several promising XR solutions designed to mitigate the identified limitations of traditional video conferencing systems.", "conclusion": "Future researchers are encouraged to focus on developing and validating new technologies that enhance communicative behaviors in video conferencing environments, utilizing the proposed evaluation methods.", "key_contributions": ["Survey of shortcomings in video conferencing systems", "Evaluation methods for communicative behavior", "Exploration of emerging XR solutions for video conferencing"], "limitations": "The paper does not provide specific case studies or empirical data on the effectiveness of the proposed XR solutions.", "keywords": ["video conferencing", "mixed reality", "human-computer interaction", "communication", "Zoom fatigue"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.02982", "pdf": "https://arxiv.org/pdf/2507.02982.pdf", "abs": "https://arxiv.org/abs/2507.02982", "title": "We Need Knowledge Distillation for Solving Math Word Problems", "authors": ["Zhenquan Shen", "Xinguo Yu", "Xiaotian Cheng", "Rao Peng", "Hao Ming"], "categories": ["cs.CL"], "comment": null, "summary": "The enhancement of mathematical capabilities in large language models (LLMs)\nfosters new developments in mathematics education within primary and secondary\nschools, particularly as they relate to intelligent tutoring systems. However,\nLLMs require substantial computational resources, resulting in significant\ncosts in educational contexts. To mitigate this drawback, this paper\ninvestigates the feasibility of compressing LLMs for solving math word problems\n(MWPs). We compress the embedded vectors encoded by BERT and distill a\nconsiderably smaller student model. Our findings indicate that the student\nmodel can maintain nearly 90% of the performance of the teacher model while\nutilizing only 1/12 of its parameters. In addition to achieving high accuracy,\nthe model exhibits strong generalizability, as the compressed vectors perform\nwell across all tasks related to MWPs, and the distillation process is not\ntask-specific. The success of this distillation demonstrates that the\nunderlying principles are generic and not limited to a specific task. We\nfurther explore the reasons behind the compressibility of embedded vectors,\nrevealing that part-of-speech information, rather than entity recognition, is\ncrucial for MWPs, which may significantly contribute to their compressibility.\nThe improvements in efficiency and cost reduction provide substantial value for\nintelligent tutoring systems and significantly advance the field of intelligent\neducation.", "AI": {"tldr": "This paper explores the compression of large language models (LLMs) for solving math word problems, achieving a student model that retains 90% of the teacher model's performance with only 1/12 of the parameters, enhancing efficiency for intelligent tutoring systems.", "motivation": "To address the high computational costs associated with using large language models in educational contexts, particularly for mathematics education through intelligent tutoring systems.", "method": "The authors compress the embedded vectors encoded by BERT and distill a smaller student model, which is evaluated on its performance in solving math word problems (MWPs).", "result": "The compressed student model maintains nearly 90% of the teacher model's performance while utilizing only 1/12 of its parameters and demonstrates strong generalizability across tasks related to MWPs.", "conclusion": "The distillation process is effective and shows that the underlying principles of compressibility are generic, offering significant improvements in efficiency and cost reduction for intelligent tutoring systems.", "key_contributions": ["Demonstrated effective compression of LLMs for educational math applications.", "Achieved high accuracy with a significantly smaller model size.", "Identified the importance of part-of-speech information for the compressibility of embedded vectors."], "limitations": "", "keywords": ["Large Language Models", "Math Word Problems", "Intelligent Tutoring Systems", "Model Distillation", "Education"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03942", "pdf": "https://arxiv.org/pdf/2507.03942.pdf", "abs": "https://arxiv.org/abs/2507.03942", "title": "More than One Step at a Time: Designing Procedural Feedback for Non-visual Makeup Routines", "authors": ["Franklin Mingzhe Li", "Akihiko Oharazawa", "Chloe Qingyu Zhu", "Misty Fan", "Daisuke Sato", "Chieko Asakawa", "Patrick Carrington"], "categories": ["cs.HC", "cs.CV"], "comment": "ASSETS 2025", "summary": "Makeup plays a vital role in self-expression, identity, and confidence - yet\nremains an underexplored domain for assistive technology, especially for people\nwith vision impairments. While existing tools support isolated tasks such as\ncolor identification or product labeling, they rarely address the procedural\ncomplexity of makeup routines: coordinating step sequences, managing product\nplacement, and assessing the final look with accessible feedback. To understand\nthe real-world process, we conducted a contextual inquiry with 15 visually\nimpaired makeup users, capturing real-time makeup application behaviors and\ntheir step-by-step information needs and assessment approaches. Our findings\nreveal embodied, tactile-first strategies; persistent challenges in blending,\nsymmetry, and assessment; and a desire for honest, real-time, goal-aligned\nfeedback. We also interviewed five professional makeup artists, who reviewed\nparticipant makeup videos and provided expert responses to participant-raised\nquestions and assessment practices. We contribute a taxonomy of feedback needs\nin non-visual makeup, and outline design implications for future assistive\nsystems - emphasizing hands-free, conversational interaction and context-aware,\nprocedural support for expressive and independent beauty practices.", "AI": {"tldr": "This paper explores assistive technology for vision-impaired individuals during makeup application, highlighting their unique needs and proposing design implications for future systems.", "motivation": "The study investigates the lack of focus on assistive technologies in the makeup domain for people with vision impairments, which affects their self-expression and confidence.", "method": "A contextual inquiry was conducted with 15 visually impaired makeup users to capture their makeup application behaviors, information needs, and assessment methods, alongside interviews with five professional makeup artists for expert feedback.", "result": "Findings revealed embodied, tactile-first strategies, persistent challenges in blending and symmetry, and a need for real-time, goal-aligned feedback during makeup application.", "conclusion": "The paper presents a taxonomy of feedback needs in non-visual makeup and outlines design implications for assistive technologies, advocating for hands-free, conversational interaction and context-aware support.", "key_contributions": ["Taxonomy of feedback needs for non-visual makeup", "Insights into real-world behaviors of visually impaired makeup users", "Design implications for assistive systems in makeup application"], "limitations": "The study may not capture the diversity of all users' experiences or preferences in different cultural contexts.", "keywords": ["assistive technology", "vision impairments", "makeup application", "HCI", "feedback needs"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.02983", "pdf": "https://arxiv.org/pdf/2507.02983.pdf", "abs": "https://arxiv.org/abs/2507.02983", "title": "Truth, Trust, and Trouble: Medical AI on the Edge", "authors": ["Mohammad Anas Azeez", "Rafiq Ali", "Ebad Shabbir", "Zohaib Hasan Siddiqui", "Gautam Siddharth Kashyap", "Jiechao Gao", "Usman Naseem"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) hold significant promise for transforming\ndigital health by enabling automated medical question answering. However,\nensuring these models meet critical industry standards for factual accuracy,\nusefulness, and safety remains a challenge, especially for open-source\nsolutions. We present a rigorous benchmarking framework using a dataset of over\n1,000 health questions. We assess model performance across honesty,\nhelpfulness, and harmlessness. Our results highlight trade-offs between factual\nreliability and safety among evaluated models -- Mistral-7B,\nBioMistral-7B-DARE, and AlpaCare-13B. AlpaCare-13B achieves the highest\naccuracy (91.7%) and harmlessness (0.92), while domain-specific tuning in\nBioMistral-7B-DARE boosts safety (0.90) despite its smaller scale. Few-shot\nprompting improves accuracy from 78% to 85%, and all models show reduced\nhelpfulness on complex queries, highlighting ongoing challenges in clinical QA.", "AI": {"tldr": "A benchmarking framework for evaluating LLMs in digital health reveals trade-offs in factual accuracy and safety across models, with AlpaCare-13B performing best overall.", "motivation": "To benchmark and evaluate large language models for automated medical question answering in digital health, ensuring they meet industry standards for accuracy and safety.", "method": "A benchmarking framework was developed using a dataset of over 1,000 health questions to assess model performance in terms of honesty, helpfulness, and harmlessness.", "result": "The evaluation found that AlpaCare-13B achieved the highest accuracy at 91.7% and harmlessness at 0.92, while BioMistral-7B-DARE improved safety through domain-specific tuning despite its smaller scale. Few-shot prompting increased accuracy from 78% to 85%, but models struggled with helpfulness on complex queries.", "conclusion": "The results indicate key trade-offs between factual reliability and safety, suggesting ongoing challenges in the performance of LLMs in clinical QA applications.", "key_contributions": ["Development of a comprehensive benchmarking framework for LLMs in health informatics", "Comparison of various LLMs in terms of accuracy, helpfulness, and harmlessness", "Insights into the impact of few-shot prompting on model performance"], "limitations": "The study primarily focuses on select LLMs, and the performance on complex queries was notably lower across all models, indicating a limitation in their practical application.", "keywords": ["Large Language Models", "digital health", "medical question answering", "benchmarking framework", "model performance"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04005", "pdf": "https://arxiv.org/pdf/2507.04005.pdf", "abs": "https://arxiv.org/abs/2507.04005", "title": "Exploring a Gamified Personality Assessment Method through Interaction with Multi-Personality LLM Agents", "authors": ["Baiqiao Zhang", "Xiangxian Li", "Chao Zhou", "Xinyu Gai", "Zhifeng Liao", "Juan Liu", "Xue Yang", "Niqi Liu", "Xiaojuan Ma", "Yong-jin Liu", "Yulong Bian"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "The execution of effective and imperceptible personality assessments is\nreceiving increasing attention in psychology and human-computer interaction\nfields. This study explores an interactive approach for personality assessment,\nfocusing on the multiplicity of personality representation. We propose a\nframework of gamified personality assessment through multi-personality\nrepresentations (Multi-PR GPA). The framework leverages Large Language Models\nto empower virtual agents with diverse personalities. These agents elicit\nmultifaceted human personality representations through engaging in interactive\ngames. Drawing upon the multi-type textual data generated throughout the\ninteraction, it achieves two ways of personality assessments (i.e., Direct\nAssessment and Que-based Assessment) and provides interpretable insights.\nGrounded in the classic Big Five theory, we implemented a prototype system and\nconducted a user study to assess the efficacy of Multi-PR GPA. The results\nunderscore the effectiveness of our approach in personality assessment and\ndemonstrate that it achieves superior performance when considering the\nmultiplicity of personality representation.", "AI": {"tldr": "This study presents a gamified approach to personality assessment using Large Language Models, focusing on multiple personality representations.", "motivation": "To enhance personality assessment methods in psychology and HCI, leveraging interactive elements and diverse personality representations.", "method": "A framework called Multi-PR GPA was developed, which uses gamification and LLMs to assess personality in two ways: Direct Assessment and Que-based Assessment, based on user interactions.", "result": "The prototype system showed superior performance in personality assessment, effectively capturing multifaceted personality representations according to the Big Five theory.", "conclusion": "The results indicate that incorporating multiple personality representations through interactive games significantly improves assessment efficacy.", "key_contributions": ["Development of Multi-PR GPA framework for personality assessment", "Use of Large Language Models to generate diverse personality interactions", "Evidence of effectiveness through user study results"], "limitations": "", "keywords": ["personality assessment", "Human-Computer Interaction", "Large Language Models", "gamification", "Big Five theory"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.02984", "pdf": "https://arxiv.org/pdf/2507.02984.pdf", "abs": "https://arxiv.org/abs/2507.02984", "title": "From Answers to Rationales: Self-Aligning Multimodal Reasoning with Answer-Oriented Chain-of-Thought", "authors": ["Wentao Tan", "Qiong Cao", "Yibing Zhan", "Chao Xue", "Changxing Ding"], "categories": ["cs.CL"], "comment": null, "summary": "Achieving human-like reasoning capabilities in Multimodal Large Language\nModels (MLLMs) has long been a goal. Current methodologies primarily focus on\nsynthesizing positive rationales, while overlooking the critical role of\nnegative rationales in training models to discern flawed reasoning patterns. To\naddress this gap, we propose a novel framework: \\textbf{S}elf-Aligning\n\\textbf{M}ultimodal Reasoning with \\textbf{A}nswer-O\\textbf{r}iented\nChain-of-\\textbf{T}hought (SMART). This framework enables models to utilize\nAoT-Oriented Chain-of-Thought (AoT) prompts to automatically generate\nhigh-quality positive and negative reasoning paths, followed by self-alignment\nto enhance their reasoning abilities. Inspired by human strategies for solving\nproof-based problems, AoT uses answers as a guide to help the model extract\ncritical visual information that links questions and answers. When provided\nwith ground truth answers, the model produces strong positive rationales.\nConversely, when correct answers are replaced with misleading alternatives, the\nmodel generates an erroneous yet compelling reasoning path, serving as a form\nof discriminative negative rationale. Models trained with AoT-generated data\noutperform those trained on manually annotated datasets, demonstrating superior\nreasoning capabilities. This encourages the use of improved models to generate\nhigher-quality preference data for further optimization. Consequently, SMART\nestablishes an iterative generation-optimization method that continually\nenhances the model's reasoning skills. Experiments indicate that the SMART\nframework significantly improves various MLLMs, regardless of model\narchitecture, parameter size, or pre-training dataset. The code, datasets, and\nmodels will be released.", "AI": {"tldr": "This paper introduces the SMART framework for enhancing reasoning capabilities in Multimodal Large Language Models by using both positive and negative rationales during training.", "motivation": "To improve human-like reasoning in MLLMs, addressing the gap in training methodologies that focus mainly on positive rationales.", "method": "The SMART framework employs AoT-Oriented Chain-of-Thought prompts to generate both positive and negative reasoning paths, followed by a self-alignment process to optimize the model's reasoning abilities.", "result": "Models trained with SMART significantly outperform those using manually annotated data, demonstrating enhanced reasoning capabilities across various architectures and datasets.", "conclusion": "The SMART framework establishes a method for ongoing improvement of MLLMs' reasoning skills through iterative generation and optimization.", "key_contributions": ["Introduction of the SMART framework for MLLMs", "Use of negative rationales in model training", "Demonstrated superior performance over traditional training methods"], "limitations": "", "keywords": ["Multimodal Large Language Models", "Negative Rationale", "Chain-of-Thought", "Reasoning Enhancement", "SMART Framework"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.04043", "pdf": "https://arxiv.org/pdf/2507.04043.pdf", "abs": "https://arxiv.org/abs/2507.04043", "title": "Evaluating the Effectiveness of Large Language Models in Solving Simple Programming Tasks: A User-Centered Study", "authors": ["Kai Deng"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become more common in educational tools and\nprogramming environments, questions arise about how these systems should\ninteract with users. This study investigates how different interaction styles\nwith ChatGPT-4o (passive, proactive, and collaborative) affect user performance\non simple programming tasks. I conducted a within-subjects experiment where\nfifteen high school students participated, completing three problems under\nthree distinct versions of the model. Each version was designed to represent a\nspecific style of AI support: responding only when asked, offering suggestions\nautomatically, or engaging the user in back-and-forth dialogue.Quantitative\nanalysis revealed that the collaborative interaction style significantly\nimproved task completion time compared to the passive and proactive conditions.\nParticipants also reported higher satisfaction and perceived helpfulness when\nworking with the collaborative version. These findings suggest that the way an\nLLM communicates, how it guides, prompts, and responds, can meaningfully impact\nlearning and performance. This research highlights the importance of designing\nLLMs that go beyond functional correctness to support more interactive,\nadaptive, and user-centered experiences, especially for novice programmers.", "AI": {"tldr": "This study explores the impact of different interaction styles of ChatGPT-4o on user performance in programming tasks, finding that collaborative styles significantly improve outcomes.", "motivation": "To investigate how interaction styles of LLMs influence user performance and satisfaction in educational programming contexts.", "method": "A within-subjects experiment involving fifteen high school students completing programming tasks under three different interaction styles: passive, proactive, and collaborative.", "result": "The collaborative style significantly improved task completion time and user satisfaction compared to passive and proactive styles.", "conclusion": "Effective LLMs should focus on interactive and adaptive designs to enhance user learning and performance, especially for novices.", "key_contributions": ["Demonstrated that collaborative interaction enhances task performance and satisfaction.", "Provided insights into designing user-centered LLMs for educational contexts.", "Highlighted the need for LLMs to adapt to different user interaction styles."], "limitations": "Limited sample size of high school students; results may not generalize to other demographics.", "keywords": ["Large Language Models", "Human-Computer Interaction", "Educational Tools"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.02986", "pdf": "https://arxiv.org/pdf/2507.02986.pdf", "abs": "https://arxiv.org/abs/2507.02986", "title": "GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models", "authors": ["Seshu Tirupathi", "Dhaval Salwala", "Elizabeth Daly", "Inge Vejsbjerg"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) continue to be increasingly applied across\nvarious domains, their widespread adoption necessitates rigorous monitoring to\nprevent unintended negative consequences and ensure robustness. Furthermore,\nLLMs must be designed to align with human values, like preventing harmful\ncontent and ensuring responsible usage. The current automated systems and\nsolutions for monitoring LLMs in production are primarily centered on\nLLM-specific concerns like hallucination etc, with little consideration given\nto the requirements of specific use-cases and user preferences. This paper\nintroduces GAF-Guard, a novel agentic framework for LLM governance that places\nthe user, the use-case, and the model itself at the center. The framework is\ndesigned to detect and monitor risks associated with the deployment of LLM\nbased applications. The approach models autonomous agents that identify risks,\nactivate risk detection tools, within specific use-cases and facilitate\ncontinuous monitoring and reporting to enhance AI safety, and user\nexpectations. The code is available at\nhttps://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.", "AI": {"tldr": "The paper introduces GAF-Guard, a framework for monitoring and governing LLM applications by focusing on user needs, use-cases, and inherent model risks.", "motivation": "The need for rigorous monitoring of LLMs to prevent negative consequences and ensure they align with human values in various applications.", "method": "GAF-Guard employs autonomous agents to detect and monitor risks associated with LLM deployments, activating risk detection tools tailored to specific use-cases.", "result": "The framework enhances AI safety by facilitating continuous monitoring and reporting, thus improving user expectations and trust in LLM applications.", "conclusion": "GAF-Guard is a step towards ensuring that LLMs are safely deployed in alignment with user and societal expectations.", "key_contributions": ["Introduction of GAF-Guard framework for LLM governance", "Focus on user and use-case-centered monitoring", "Autonomous agents for risk detection and continuous reporting"], "limitations": "", "keywords": ["Large Language Models", "HCI", "AI Safety", "Governance Framework", "Risk Detection"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04095", "pdf": "https://arxiv.org/pdf/2507.04095.pdf", "abs": "https://arxiv.org/abs/2507.04095", "title": "Human-centered AI with focus on Human-robot interaction (Book chapter)", "authors": ["Alireza Mortezapour", "Giuliana Vitiello"], "categories": ["cs.HC", "cs.AI", "cs.RO"], "comment": null, "summary": "Modern social robots can be considered the descendants of steam engines from\nthe First Industrial Revolution (IR 1.0) and industrial robotic arms from the\nThird Industrial Revolution (IR 3.0). As some time has passed since the\nintroduction of these robots during the Fourth Industrial Revolution (IR 4.0),\nchallenges and issues in their interaction with humans have emerged, leading\nresearchers to conclude that, like any other AI-based technology, these robots\nmust also be human-centered to meet the needs of their users. This chapter aims\nto introduce humans and their needs in interactions with robots, ranging from\nshort-term, one-on-one interactions (micro-level) to long-term, macro-level\nneeds at the societal scale. Building upon the principles of human-centered AI,\nthis chapter presents, for the first time, a new framework of human needs\ncalled the Dual Pyramid. This framework encompasses a comprehensive list of\nhuman needs in robot interactions, from the most fundamental, robot\neffectiveness to macro level requirements, such as the collaboration with\nrobots in achieving the United Nations 17 Sustainable Development Goals.", "AI": {"tldr": "This chapter introduces a new human-centered framework called the Dual Pyramid to address human needs in interactions with robots, linking micro-level interactions to macro-level societal goals like the UN Sustainable Development Goals.", "motivation": "As social robots evolve, their interactions with humans reveal challenges that necessitate a human-centered approach to meet user needs.", "method": "The chapter presents a new framework, the Dual Pyramid, that outlines human needs in robot interactions, integrating both short-term interactions and long-term societal objectives.", "result": "The introduced Dual Pyramid framework provides a comprehensive list of human needs in robot interactions, emphasizing effectiveness and collaboration towards sustainable development.", "conclusion": "Emphasizing human-centered AI principles is crucial for the future of social robots, ensuring they effectively address user needs at both individual and societal levels.", "key_contributions": ["Introduction of the Dual Pyramid framework for human needs in robot interactions.", "Integration of micro-level and macro-level needs in robot human interaction.", "Alignment of robot interactions with the United Nations Sustainable Development Goals."], "limitations": "", "keywords": ["social robots", "human-centered AI", "Dual Pyramid framework", "sustainable development", "human-robot interaction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.02989", "pdf": "https://arxiv.org/pdf/2507.02989.pdf", "abs": "https://arxiv.org/abs/2507.02989", "title": "A Comparative Study of Competency Question Elicitation Methods from Ontology Requirements", "authors": ["Reham Alharbi", "Valentina Tamma", "Terry R. Payne", "Jacopo de Berardinis"], "categories": ["cs.CL"], "comment": null, "summary": "Competency Questions (CQs) are pivotal in knowledge engineering, guiding the\ndesign, validation, and testing of ontologies. A number of diverse formulation\napproaches have been proposed in the literature, ranging from completely manual\nto Large Language Model (LLM) driven ones. However, attempts to characterise\nthe outputs of these approaches and their systematic comparison are scarce.\nThis paper presents an empirical comparative evaluation of three distinct CQ\nformulation approaches: manual formulation by ontology engineers, instantiation\nof CQ patterns, and generation using state of the art LLMs. We generate CQs\nusing each approach from a set of requirements for cultural heritage, and\nassess them across different dimensions: degree of acceptability, ambiguity,\nrelevance, readability and complexity. Our contribution is twofold: (i) the\nfirst multi-annotator dataset of CQs generated from the same source using\ndifferent methods; and (ii) a systematic comparison of the characteristics of\nthe CQs resulting from each approach. Our study shows that different CQ\ngeneration approaches have different characteristics and that LLMs can be used\nas a way to initially elicit CQs, however these are sensitive to the model used\nto generate CQs and they generally require a further refinement step before\nthey can be used to model requirements.", "AI": {"tldr": "This paper compares three approaches to formulating Competency Questions (CQs) in knowledge engineering: manual formulation, CQ pattern instantiation, and LLM-driven generation.", "motivation": "To systematically evaluate and compare different approaches for generating Competency Questions (CQs) used in ontology design and testing, which is crucial in knowledge engineering.", "method": "An empirical comparative evaluation of three CQ formulation approaches: manual formulation by ontology engineers, instantiation of CQ patterns, and generation by large language models (LLMs).", "result": "Different CQ generation approaches yielded varying characteristics in terms of acceptability, ambiguity, relevance, readability, and complexity. LLMs can generate acceptable CQs but often require refinement.", "conclusion": "While LLMs can generate initial CQs, their outputs depend on the model used and necessitate further refinement for practical use in modeling requirements.", "key_contributions": ["First multi-annotator dataset of CQs generated from the same source using different methods", "Systematic comparison of CQs from manual, pattern-based, and LLM-generated approaches."], "limitations": "The study is limited to cultural heritage requirements and further investigations are needed on other domains.", "keywords": ["Competency Questions", "knowledge engineering", "Large Language Models", "ontology", "Cultural Heritage"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.04160", "pdf": "https://arxiv.org/pdf/2507.04160.pdf", "abs": "https://arxiv.org/abs/2507.04160", "title": "HyperSumm-RL: A Dialogue Summarization Framework for Modeling Leadership Perception in Social Robots", "authors": ["Subasish Das"], "categories": ["cs.HC"], "comment": "6 pages with references", "summary": "This paper introduces HyperSumm-RL, a hypertext-aware summarization and\ninteraction analysis framework designed to investigate human perceptions of\nsocial robot leadership through long-form dialogue. The system utilizes a\nstructured Natural Language Processing (NLP) workflow that combines\ntransformer-based long dialogue summarization, leadership style modeling, and\nuser response analysis, enabling scalable evaluation of social robots in\ncomplex human-robot interaction (HRI) settings. Unlike prior work that focuses\non static or task-oriented HRI, HyperSumm-RL captures and hypertextually\norganizes dynamic conversational exchanges into navigable, semantically rich\nrepresentations which allows researchers to trace interaction threads, identify\ninfluence cues, and analyze leadership framing over time. The contributions of\nthis study are threefold: (1) we present a novel infrastructure for summarizing\nand linking long, multi-turn dialogues using leadership-style taxonomies; (2)\nwe propose an interactive hypertext model that supports relational navigation\nacross conversational themes, participant responses, and robot behavior modes;\nand (3) we demonstrate the utility of this system in interpreting participant\ntrust, engagement, and expectation shifts during social robot leadership\nscenarios. The findings reveal how hypertextual workflows can augment HRI\nresearch by enabling transparent, interpretable, and semantically grounded\nanalysis of emergent social dynamics.", "AI": {"tldr": "HyperSumm-RL is a hypertext-aware framework for analyzing social robot leadership through long dialogues, integrating NLP for summarization and user response analysis.", "motivation": "To explore human perceptions of social robot leadership in complex human-robot interaction settings.", "method": "The framework utilizes a structured NLP workflow that incorporates transformer-based summarization, leadership style modeling, and user response analysis to facilitate scalable evaluation of social robots.", "result": "The system successfully captures and organizes dynamic conversations into navigable representations, allowing for analysis of leadership dynamics over time.", "conclusion": "HyperSumm-RL enhances HRI research by providing a transparent and interpretable approach to analyzing social dynamics in robot leadership scenarios.", "key_contributions": ["Novel infrastructure for summarizing multi-turn dialogues", "Interactive hypertext model for relational navigation", "Utility demonstration in understanding trust and engagement shifts"], "limitations": "", "keywords": ["Human-Robot Interaction", "NLP", "Summarization", "Leadership", "Social Dynamics"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2507.02990", "pdf": "https://arxiv.org/pdf/2507.02990.pdf", "abs": "https://arxiv.org/abs/2507.02990", "title": "`For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts", "authors": ["Annika M Schoene", "Cansu Canca"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have led to increasingly\nsophisticated safety protocols and features designed to prevent harmful,\nunethical, or unauthorized outputs. However, these guardrails remain\nsusceptible to novel and creative forms of adversarial prompting, including\nmanually generated test cases. In this work, we present two new test cases in\nmental health for (i) suicide and (ii) self-harm, using multi-step,\nprompt-level jailbreaking and bypass built-in content and safety filters. We\nshow that user intent is disregarded, leading to the generation of detailed\nharmful content and instructions that could cause real-world harm. We conduct\nan empirical evaluation across six widely available LLMs, demonstrating the\ngeneralizability and reliability of the bypass. We assess these findings and\nthe multilayered ethical tensions that they present for their implications on\nprompt-response filtering and context- and task-specific model development. We\nrecommend a more comprehensive and systematic approach to AI safety and ethics\nwhile emphasizing the need for continuous adversarial testing in\nsafety-critical AI deployments. We also argue that while certain clearly\ndefined safety measures and guardrails can and must be implemented in LLMs,\nensuring robust and comprehensive safety across all use cases and domains\nremains extremely challenging given the current technical maturity of\ngeneral-purpose LLMs.", "AI": {"tldr": "This paper investigates adversarial prompting of large language models (LLMs) that can lead to harmful content generation, particularly in mental health contexts.", "motivation": "To evaluate the effectiveness of safety protocols in LLMs and address their vulnerabilities to adversarial prompting.", "method": "The study presents two test cases related to suicide and self-harm using multi-step jailbreaking techniques on six widely used LLMs to bypass existing safety measures.", "result": "The empirical evaluation showed that user intentions are often ignored, resulting in the generation of harmful content, illustrating weaknesses in prompt-response filtering.", "conclusion": "A comprehensive and systematic approach to AI safety is necessary, and continuous adversarial testing should be integral for safety-critical AI applications.", "key_contributions": ["Identification of vulnerabilities in LLM safety protocols", "Empirical testing of adversarial prompting techniques", "Recommendations for comprehensive AI safety measures"], "limitations": "Focuses primarily on mental health scenarios and may not encompass all domains of LLM application.", "keywords": ["large language models", "adversarial prompting", "AI safety", "mental health", "ethics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.04162", "pdf": "https://arxiv.org/pdf/2507.04162.pdf", "abs": "https://arxiv.org/abs/2507.04162", "title": "iBreath: Usage Of Breathing Gestures as Means of Interactions", "authors": ["Mengxi Liu", "Daniel Geißler", "Deepika Gurung", "Hymalai Bello", "Bo Zhou", "Sizhen Bian", "Paul Lukowicz", "Passant Elagroudy"], "categories": ["cs.HC"], "comment": null, "summary": "Breathing is a spontaneous but controllable body function that can be used\nfor hands-free interaction. Our work introduces \"iBreath\", a novel system to\ndetect breathing gestures similar to clicks using bio-impedance. We evaluated\niBreath's accuracy and user experience using two lab studies (n=34). Our\nresults show high detection accuracy (F1-scores > 95.2%). Furthermore, the\nusers found the gestures easy to use and comfortable. Thus, we developed eight\npractical guidelines for the future development of breathing gestures. For\nexample, designers can train users on new gestures within just 50 seconds (five\ntrials), and achieve robust performance with both user-dependent and\nuser-independent models trained on data from 21 participants, each yielding\naccuracies above 90%. Users preferred single clicks and disliked triple clicks.\nThe median gesture duration is 3.5-5.3 seconds. Our work provides solid ground\nfor researchers to experiment with creating breathing gestures and\ninteractions.", "AI": {"tldr": "The paper presents iBreath, a system for detecting breathing gestures for hands-free interaction with high accuracy and favorable user experience.", "motivation": "To explore a novel method of hands-free interaction using breathing gestures that can be easily controlled by users.", "method": "Two lab studies were conducted with 34 participants to evaluate the accuracy and user experience of iBreath, a system using bio-impedance to detect breathing gestures.", "result": "iBreath achieved high detection accuracy with F1-scores over 95.2%, and users found the gestures easy to use and comfortable, preferring single clicks over triple clicks.", "conclusion": "iBreath lays the groundwork for future research into breathing gestures, providing practical guidelines for design and implementation.", "key_contributions": ["Introduction of a novel hands-free interaction system using breathing gestures.", "Demonstrated high accuracy in gesture detection through bio-impedance.", "Developed guidelines for designing breathing gestures based on user preferences."], "limitations": "The study's results are based on a controlled lab environment with a limited number of participants, which may affect the generalizability of findings.", "keywords": ["breathing gestures", "human-computer interaction", "bio-impedance", "hands-free interaction", "user experience"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.03001", "pdf": "https://arxiv.org/pdf/2507.03001.pdf", "abs": "https://arxiv.org/abs/2507.03001", "title": "Evaluating Hierarchical Clinical Document Classification Using Reasoning-Based LLMs", "authors": ["Akram Mustafa", "Usman Naseem", "Mostafa Rahimi Azghadi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study evaluates how well large language models (LLMs) can classify\nICD-10 codes from hospital discharge summaries, a critical but error-prone task\nin healthcare. Using 1,500 summaries from the MIMIC-IV dataset and focusing on\nthe 10 most frequent ICD-10 codes, the study tested 11 LLMs, including models\nwith and without structured reasoning capabilities. Medical terms were\nextracted using a clinical NLP tool (cTAKES), and models were prompted in a\nconsistent, coder-like format. None of the models achieved an F1 score above\n57%, with performance dropping as code specificity increased. Reasoning-based\nmodels generally outperformed non-reasoning ones, with Gemini 2.5 Pro\nperforming best overall. Some codes, such as those related to chronic heart\ndisease, were classified more accurately than others. The findings suggest that\nwhile LLMs can assist human coders, they are not yet reliable enough for full\nautomation. Future work should explore hybrid methods, domain-specific model\ntraining, and the use of structured clinical data.", "AI": {"tldr": "This study assesses the efficacy of large language models (LLMs) in classifying ICD-10 codes from hospital discharge summaries, finding limited reliability and encouraging further exploration of hybrid methods and domain-specific training.", "motivation": "The study aims to evaluate the performance of LLMs in classifying ICD-10 codes, a critical and error-prone task in healthcare, particularly from hospital discharge summaries.", "method": "The research utilizes 1,500 discharge summaries from the MIMIC-IV dataset, focusing on the ten most frequent ICD-10 codes. It tests 11 different LLMs and employs a structured coding format for prompt consistency, using a clinical NLP tool to extract medical terms.", "result": "None of the models achieved an F1 score exceeding 57%, with performance decreasing as code specificity increased. Reasoning-based models showed better performance than those lacking reasoning capabilities, with Gemini 2.5 Pro yielding the best results.", "conclusion": "While LLMs can support human coders in ICD-10 classification, they are not yet dependable enough for complete automation. Future research should consider hybrid methodologies and domain-specific training.", "key_contributions": ["Evaluation of LLMs for ICD-10 classification", "Insights into the performance variations by code specificity", "Recommendations for future hybrid systems and model training"], "limitations": "Limited reliability of LLMs for full automation of ICD-10 classification despite some promising results with specific codes.", "keywords": ["ICD-10 classification", "large language models", "health informatics", "natural language processing", "MIMIC-IV"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04236", "pdf": "https://arxiv.org/pdf/2507.04236.pdf", "abs": "https://arxiv.org/abs/2507.04236", "title": "AnnoGram: An Annotative Grammar of Graphics Extension", "authors": ["Md Dilshadur Rahman", "Md Rahat-uz- Zaman", "Andrew McNutt", "Paul Rosen"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Annotations are central to effective data communication, yet most\nvisualization tools treat them as secondary constructs -- manually defined,\ndifficult to reuse, and loosely coupled to the underlying visualization\ngrammar. We propose a declarative extension to Wilkinson's Grammar of Graphics\nthat reifies annotations as first-class design elements, enabling structured\nspecification of annotation targets, types, and positioning strategies. To\ndemonstrate the utility of our approach, we develop a prototype extension\ncalled Vega-Lite Annotation. Through comparison with eight existing tools, we\nshow that our approach enhances expressiveness, reduces authoring effort, and\nenables portable, semantically integrated annotation workflows.", "AI": {"tldr": "This paper presents a declarative extension to the Grammar of Graphics that elevates annotations as first-class elements in visualization tools, resulting in improved expressiveness and ease of use.", "motivation": "To address the limitations of current visualization tools that treat annotations as secondary elements, hindering effective data communication.", "method": "A declarative extension to Wilkinson's Grammar of Graphics is developed, allowing structured specification of annotations in visualization processes.", "result": "The developed prototype, Vega-Lite Annotation, is compared with eight existing tools, demonstrating enhanced expressiveness and reduced authoring effort along with the capacity to create portable annotation workflows.", "conclusion": "The proposed model significantly improves the usability and semantic integration of annotations in visualization, making them more effective for data communication.", "key_contributions": ["Introduction of annotations as first-class design elements in visualizations.", "Development of the Vega-Lite Annotation prototype to illustrate the approach.", "Demonstration of improved expressiveness and reduced authoring effort compared to existing tools."], "limitations": "", "keywords": ["annotations", "visualization", "Grammar of Graphics", "Vega-Lite", "data communication"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.03003", "pdf": "https://arxiv.org/pdf/2507.03003.pdf", "abs": "https://arxiv.org/abs/2507.03003", "title": "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages", "authors": ["Wanru Zhao", "Yihong Chen", "Royson Lee", "Xinchi Qiu", "Yan Gao", "Hongxiang Fan", "Nicholas D. Lane"], "categories": ["cs.CL"], "comment": "ICLR 2024", "summary": "Pre-trained large language models (LLMs) have become a cornerstone of modern\nnatural language processing, with their capabilities extending across a wide\nrange of applications and languages. However, the fine-tuning of multilingual\nLLMs, especially for low-resource languages, faces significant challenges\narising from data-sharing restrictions (the physical border) and inherent\nlinguistic differences (the linguistic border). These barriers hinder users of\nvarious languages, particularly those in low-resource regions, from fully\nbenefiting from the advantages of LLMs. To address these challenges, we propose\nthe Federated Prompt Tuning Paradigm for multilingual scenarios, which utilizes\nparameter-efficient fine-tuning while adhering to data sharing restrictions. We\ndesign a comprehensive set of experiments and analyze them using a novel notion\nof language distance to highlight the strengths of our paradigm: Even under\ncomputational constraints, our method not only improves data efficiency but\nalso facilitates mutual enhancements across languages, particularly benefiting\nlow-resource ones. Compared to traditional local cross-lingual transfer tuning\nmethods, our approach achieves 6.9\\% higher accuracy with improved data\nefficiency, and demonstrates greater stability and generalization. These\nfindings underscore the potential of our approach to promote social equality\nand champion linguistic diversity, ensuring that no language is left behind.", "AI": {"tldr": "This paper introduces the Federated Prompt Tuning Paradigm for multilingual large language models (LLMs) to enhance fine-tuning for low-resource languages while overcoming data-sharing restrictions.", "motivation": "To address the significant challenges of fine-tuning multilingual LLMs for low-resource languages due to data-sharing restrictions and linguistic differences.", "method": "The paper proposes a Federated Prompt Tuning Paradigm, employing parameter-efficient fine-tuning and a novel notion of language distance to analyze data efficiency and performance across languages.", "result": "The proposed approach outperforms traditional local cross-lingual transfer tuning methods by achieving 6.9% higher accuracy, improved data efficiency, and greater stability and generalization, particularly benefiting low-resource languages.", "conclusion": "This work underscores the potential of federated prompt tuning to promote social equality and linguistic diversity, ensuring that low-resource languages can leverage LLM capabilities effectively.", "key_contributions": ["Introduction of the Federated Prompt Tuning Paradigm for multilingual LLMs", "Demonstration of improved data efficiency and accuracy for low-resource languages", "Analysis through a novel language distance metric"], "limitations": "", "keywords": ["multilingual LLMs", "federated learning", "low-resource languages", "data efficiency", "language distance"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.04238", "pdf": "https://arxiv.org/pdf/2507.04238.pdf", "abs": "https://arxiv.org/abs/2507.04238", "title": "WSCoach: Wearable Real-time Auditory Feedback for Reducing Unwanted Words in Daily Communication", "authors": ["Zhang Youpeng", "Nuwan Janaka", "Ashwin Ram", "Yin Peilin", "Tian Yang", "Shengdong Zhao", "Pierre Dragicevic"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "30 pages, 9 figures", "summary": "The rise of wearable smart devices raises unprecedented opportunities for\nself-improvement through ubiquitous behavior tracking and guidance. However,\nthe design of effective wearable behavior intervention systems remains\nrelatively unexplored. To address this gap, we conducted controlled studies\nfocusing on the reduction of unwanted words (e.g., filler words, swear words)\nin daily communication through auditory feedback using wearable technology. We\nstarted with a design space exploration, considering various factors such as\nthe type, duration, and timing of the auditory feedback. Then, we conducted\npilot studies to reduce the space of design choices and prototyped a system\ncalled WSCoach (Wearable Speech Coach), which informs users when they utter\nunwanted words in near-real-time. To evaluate WSCoach, we compared it with a\nstate-of-the-art mobile application supporting post-hoc conversation analysis.\nBoth approaches were effective in reducing the occurrence of unwanted words,\nbut WSCoach appears to be more effective in the long run. Finally, we discuss\nguidelines for the design of wearable audio-based behavior monitoring and\nintervention systems and highlight the potential of wearable technology for\nfacilitating behavior correction and improvement. For supplementary material,\nplease see the META Appendix and our OSF project at\nhttps://osf.io/6vhwn/?view_only=489498d3ac2d4703a17475fc6ca65dfa.", "AI": {"tldr": "The paper explores the design and effectiveness of a wearable speech intervention system, WSCoach, aimed at reducing unwanted words in daily communication through auditory feedback.", "motivation": "Despite the potential of wearable technology for behavior intervention, effective designs for behavior intervention systems remain underexplored, particularly in communication.", "method": "Controlled studies were conducted to explore design factors for auditory feedback, leading to the prototyping of WSCoach, which provides near-real-time feedback on unwanted speech.", "result": "WSCoach was tested against existing mobile applications for conversation analysis, proving to be more effective in long-term reduction of unwanted words in speech.", "conclusion": "The study emphasizes the viability of wearable technology for behavior improvement and offers design guidelines for future wearable intervention systems.", "key_contributions": ["Prototype of WSCoach for real-time speech feedback", "Comparison with a mobile app for conversation analysis", "Design guidelines for wearable behavior monitoring systems"], "limitations": "", "keywords": ["wearable technology", "behavior intervention", "communication", "auditory feedback", "HCI"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2507.03004", "pdf": "https://arxiv.org/pdf/2507.03004.pdf", "abs": "https://arxiv.org/abs/2507.03004", "title": "CLUES: Collaborative High-Quality Data Selection for LLMs via Training Dynamics", "authors": ["Wanru Zhao", "Hongxiang Fan", "Shell Xu Hu", "Wangchunshu Zhou", "Bofan Chen", "Nicholas D. Lane"], "categories": ["cs.CL", "cs.MA"], "comment": "NeurIPS 2024", "summary": "Recent research has highlighted the importance of data quality in scaling\nlarge language models (LLMs). However, automated data quality control faces\nunique challenges in collaborative settings where sharing is not allowed\ndirectly between data silos. To tackle this issue, this paper proposes a novel\ndata quality control technique based on the notion of data influence on the\ntraining dynamics of LLMs, that high quality data are more likely to have\nsimilar training dynamics to the anchor dataset. We then leverage the influence\nof the training dynamics to select high-quality data from different private\ndomains, with centralized model updates on the server side in a collaborative\ntraining fashion by either model merging or federated learning. As for the data\nquality indicator, we compute the per-sample gradients with respect to the\nprivate data and the anchor dataset, and use the trace of the accumulated inner\nproducts as a measurement of data quality. In addition, we develop a quality\ncontrol evaluation tailored for collaborative settings with heterogeneous\ndomain data. Experiments show that training on the high-quality data selected\nby our method can often outperform other data selection methods for\ncollaborative fine-tuning of LLMs, across diverse private domain datasets, in\nmedical, multilingual and financial settings. Our code is released at\ngithub.com/Ryan0v0/CLUES.", "AI": {"tldr": "This paper presents a new technique for automated data quality control in collaborative settings to enhance the training of large language models by selecting high-quality data while preserving privacy.", "motivation": "To improve the quality of data used for training large language models (LLMs) in collaborative environments where direct data sharing is restricted.", "method": "The proposed technique evaluates data quality by analyzing the influence of data on training dynamics and uses collaborative training methods like model merging and federated learning to select high-quality samples from varying private domains.", "result": "Experiments demonstrate that the proposed method significantly outperforms other data selection techniques for the fine-tuning of LLMs, particularly in medical, multilingual, and financial contexts.", "conclusion": "The method effectively enables the selection of high-quality data without violating privacy and enhances collaborative learning outcomes across diverse datasets.", "key_contributions": ["Novel data quality control technique for LLMs in collaborative settings", "Utilization of training dynamics to assess data quality", "Development of a tailored evaluation for quality control in heterogeneous domains"], "limitations": "", "keywords": ["Data Quality", "Collaborative Learning", "Large Language Models", "Federated Learning", "Training Dynamics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.04241", "pdf": "https://arxiv.org/pdf/2507.04241.pdf", "abs": "https://arxiv.org/abs/2507.04241", "title": "RunPacer: A Smartwatch-Based Vibrotactile Feedback System for Symmetric Co-Running by Visually Impaired Individuals and Guides", "authors": ["Yichen Yu", "Huan-Song Xu"], "categories": ["cs.HC"], "comment": "6 pages, 1 figure", "summary": "Visually impaired individuals often require a guide runner to safely\nparticipate in outdoor running. However, maintaining synchronized pacing with\nverbal cues or tethers can be mentally taxing and physically restrictive.\nExisting solutions primarily focus on navigation or obstacle avoidance but\noverlook the importance of real-time interpersonal rhythm coordination during\nrunning. We introduce RunPacer, a smartwatch-based vibrotactile feedback system\nthat delivers synchronized rhythmic pulses to both runners. In contrast to\nconventional guide-running systems that rely heavily on continuous verbal\ncommunication or mechanical tethering, RunPacer emphasizes interpersonal\ncadence alignment as its core interaction model. By pre-setting a target step\nfrequency or dynamically adapting to the guide's natural pace, the system\nensures that both runners receive identical haptic cues, enabling them to\nmaintain coordinated motion intuitively and efficiently. This poster presents\nthe system architecture, positions it within prior research on haptic\nentrainment, and outlines the vision for future field deployment, including\npotential multimodal feedback extensions. RunPacer contributes a lightweight,\nsocially cooperative, and non-visual assistive framework that reimagines\nco-running as a shared, embodied, and accessible experience.", "AI": {"tldr": "RunPacer introduces a smartwatch-based vibrotactile feedback system for visually impaired runners, focusing on synchronized pacing without relying on verbal cues or tethers.", "motivation": "The need for visually impaired individuals to maintain synchronized pacing during outdoor running, which is often hindered by traditional guide-running methods that rely on verbal communication or tethers.", "method": "A vibrotactile feedback system that provides synchronized rhythmic pulses to both the visually impaired runner and their guide, emphasizing interpersonal cadence alignment.", "result": "RunPacer allows for intuitive and efficient coordinated motion between runners through the delivery of identical haptic cues based on either a pre-set target step frequency or adaptation to the guide's natural pace.", "conclusion": "RunPacer represents a novel approach to assistive running, positioning it as a socially cooperative framework that enhances the shared experience of running for visually impaired athletes.", "key_contributions": ["Introduction of a novel vibrotactile feedback system for running", "Focus on interpersonal rhythm coordination rather than navigation", "Vision for future multimodal feedback extensions"], "limitations": "", "keywords": ["vibrotactile feedback", "assistive technology", "running", "HCI", "haptic entrainment"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2507.03005", "pdf": "https://arxiv.org/pdf/2507.03005.pdf", "abs": "https://arxiv.org/abs/2507.03005", "title": "Beyond cognacy", "authors": ["Gerhard Jäger"], "categories": ["cs.CL", "q-bio.PE"], "comment": "9 pages, 2 figures", "summary": "Computational phylogenetics has become an established tool in historical\nlinguistics, with many language families now analyzed using likelihood-based\ninference. However, standard approaches rely on expert-annotated cognate sets,\nwhich are sparse, labor-intensive to produce, and limited to individual\nlanguage families. This paper explores alternatives by comparing the\nestablished method to two fully automated methods that extract phylogenetic\nsignal directly from lexical data. One uses automatic cognate clustering with\nunigram/concept features; the other applies multiple sequence alignment (MSA)\nderived from a pair-hidden Markov model. Both are evaluated against expert\nclassifications from Glottolog and typological data from Grambank. Also, the\nintrinsic strengths of the phylogenetic signal in the characters are compared.\nResults show that MSA-based inference yields trees more consistent with\nlinguistic classifications, better predicts typological variation, and provides\na clearer phylogenetic signal, suggesting it as a promising, scalable\nalternative to traditional cognate-based methods. This opens new avenues for\nglobal-scale language phylogenies beyond expert annotation bottlenecks.", "AI": {"tldr": "This paper explores automated methods for analyzing language phylogenies, comparing them to traditional expert-annotated approaches.", "motivation": "To address the limitations of labor-intensive expert-annotated cognate sets in computational phylogenetics.", "method": "Comparative evaluation of two automated methods: one using automatic cognate clustering and the other using multiple sequence alignment derived from a pair-hidden Markov model.", "result": "MSA-based inference provides trees consistent with linguistic classifications, better predicts typological variation, and exhibits a clearer phylogenetic signal compared to standard approaches.", "conclusion": "MSA is proposed as a promising and scalable alternative to traditional cognate-based methods, facilitating global-scale language phylogenies without expert bottlenecks.", "key_contributions": ["Introduction of fully automated methods for phylogenetic analysis", "Comparison of MSA with traditional approaches", "Findings suggest MSA enhances predictive power and clarity of phylogenetic signals."], "limitations": "The study relies on the availability of lexical data and may not fully replace expert annotations in all cases.", "keywords": ["computational phylogenetics", "language families", "cognate clustering", "multiple sequence alignment", "phylogenetic signal"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.04278", "pdf": "https://arxiv.org/pdf/2507.04278.pdf", "abs": "https://arxiv.org/abs/2507.04278", "title": "DMER-Ranker: Learning to Rank Emotion Descriptions in the Absence of Ground Truth", "authors": ["Zheng Lian", "Licai Sun", "Haoyu Chen", "Zebang Cheng", "Fan Zhang", "Ziyu Jia", "Ziyang Ma", "Fei Ma", "Xiaojiang Peng", "Jianhua Tao"], "categories": ["cs.HC"], "comment": null, "summary": "Descriptive Multimodal Emotion Recognition (DMER) is a newly proposed task\nthat aims to describe a person's emotional state using free-form natural\nlanguage. Unlike traditional discriminative methods that rely on predefined\nemotion taxonomies, DMER provides greater flexibility in emotional expression,\nenabling fine-grained and interpretable emotion representations. However, this\nfree-form prediction paradigm introduces significant challenges in evaluation.\nExisting methods either depend on ground-truth descriptions that require\nsubstantial manual effort or simplify the task by shifting the focus from\nevaluating descriptions to evaluating emotion labels. However, the former\nsuffers from the labor-intensive collection of comprehensive descriptions,\nwhile the latter overlooks critical aspects such as emotional temporal\ndynamics, intensity, and uncertainty. To address these limitations, we propose\nDMER-Ranker, a novel evaluation strategy that reformulates the traditional\n``prediction-ground truth'' comparison into the ``prediction-prediction''\ncomparison, eliminating the need for ground-truth descriptions. We then employ\nthe Bradley-Terry algorithm to convert pairwise comparison results into\nmodel-level rankings. Additionally, we explore the possibility of automatic\npreference prediction and introduce DMER-Preference, the first preference\ndataset specifically designed for human emotions. Our work advances the field\nof DMER and lays the foundation for more intelligent human-computer interaction\nsystems.", "AI": {"tldr": "Introduction of a new approach for evaluating emotion recognition systems based on free-form natural language expressions.", "motivation": "To address the limitations of existing evaluation methods in Descriptive Multimodal Emotion Recognition (DMER).", "method": "Proposes the DMER-Ranker strategy for evaluation, reformulating traditional comparisons and applying the Bradley-Terry algorithm for rankings.", "result": "Introduced DMER-Ranker and DMER-Preference dataset, allowing for more efficient evaluation of emotional expressions without requiring exhaustive manual descriptions.", "conclusion": "The proposed methods enhance the evaluation of emotion recognition systems, fostering improvements in human-computer interaction.", "key_contributions": ["DMER-Ranker evaluation strategy", "Introduction of a new preference dataset for emotions (DMER-Preference)", "Enhanced flexibility in emotional expression and its evaluation"], "limitations": "", "keywords": ["emotion recognition", "human-computer interaction", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03009", "pdf": "https://arxiv.org/pdf/2507.03009.pdf", "abs": "https://arxiv.org/abs/2507.03009", "title": "PDFMathTranslate: Scientific Document Translation Preserving Layouts", "authors": ["Rongxin Ouyang", "Chang Chu", "Zhikuang Xin", "Xiangyao Ma"], "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50, 68T45, 68U10, 68U15", "D.2.2; I.2.10; I.2.7; J.0"], "comment": "7 pages, 4 figures", "summary": "Language barriers in scientific documents hinder the diffusion and\ndevelopment of science and technologies. However, prior efforts in translating\nsuch documents largely overlooked the information in layouts. To bridge the\ngap, we introduce PDFMathTranslate, the world's first open-source software for\ntranslating scientific documents while preserving layouts. Leveraging the most\nrecent advances in large language models and precise layout detection, we\ncontribute to the community with key improvements in precision, flexibility,\nand efficiency. The work has been open-sourced at\nhttps://github.com/byaidu/pdfmathtranslate with more than 22k downloads.", "AI": {"tldr": "PDFMathTranslate is an open-source tool for translating scientific documents while preserving their layout.", "motivation": "To address language barriers in scientific documents that hinder scientific progress by eliminating layout information in previous translations.", "method": "Utilizes state-of-the-art large language models along with precise layout detection techniques to achieve translation efficiency.", "result": "Demonstrates significant improvements in translation precision, flexibility, and efficiency, as evidenced by 22k downloads of the software.", "conclusion": "PDFMathTranslate provides a valuable resource for the scientific community, enabling better accessibility and understanding of scientific texts across language barriers.", "key_contributions": ["First open-source software for translating scientific documents while preserving layouts.", "Improvements in translation precision and efficiency using advanced ML techniques.", "Open-source availability encourages further development and research."], "limitations": "", "keywords": ["Language Translation", "Layout Detection", "Large Language Models", "Open Source Software", "Scientific Documents"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.04398", "pdf": "https://arxiv.org/pdf/2507.04398.pdf", "abs": "https://arxiv.org/abs/2507.04398", "title": "Do Students Write Better Post-AI Support? Effects of Generative AI Literacy and Chatbot Interaction Strategies on Multimodal Academic Writing", "authors": ["Yueqiao Jin", "Kaixun Yang", "Roberto Martinez-Maldonado", "Dragan Gašević", "Lixiang Yan"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Academic writing increasingly involves multimodal tasks requiring students to\nintegrate visual information and textual arguments. While generative AI (GenAI)\ntools, like ChatGPT, offer new pathways for supporting academic writing, little\nis known about how students' GenAI literacy influences their independent\nmultimodal writing skills or how chatbot interaction strategies (passive\nreactive vs. proactive scaffolding) impact learning. This study examined 79\nhigher education students' multimodal academic writing performance using a\ncomparative research design. Students completed writing tasks integrating\nvisual data under two chatbot-assisted conditions (passive vs. proactive) and\nsubsequently without AI assistance. Their writing performance was rigorously\nevaluated across five dimensions, including insightfulness, visual data\nintegration, organisation, linguistic quality, and critical thinking. Ordinal\nlogistic regression and correlation analyses revealed that higher levels of\nGenAI literacy significantly predicted stronger independent multimodal writing\nperformance immediately after AI assistance removal, particularly for students\nusing passive chatbots requiring active prompting. These results highlight the\ncritical role of GenAI literacy and specific chatbot interaction strategies in\nshaping students' capacities for independent multimodal academic writing. Our\nfindings emphasise the need for purposeful integration of GenAI literacy\ntraining into curricula and balancing external scaffolding support with\nautonomous learning opportunities. This research offers valuable\nrecommendations for educators leveraging AI-enhanced pedagogies to optimise\nstudent writing outcomes and technological engagement strategies.", "AI": {"tldr": "This study investigates the impact of generative AI literacy and chatbot interaction strategies on students' multimodal academic writing performance.", "motivation": "The integration of visual information and textual arguments in academic writing is increasingly important, yet the influence of generative AI literacy on this skill is not well understood.", "method": "A comparative research design was employed, with 79 higher education students completing writing tasks under two AI-assisted conditions (passive vs. proactive chatbot support) followed by independent tasks without AI help.", "result": "Higher GenAI literacy predicted better multimodal writing performance after AI assistance, especially when using passive chatbots that required more prompting.", "conclusion": "The findings underscore the significance of integrating GenAI literacy training in educational curricula and finding the right balance between AI support and independent learning.", "key_contributions": ["Establishes a link between GenAI literacy and independent writing performance.", "Demonstrates varying effects of chatbot interaction strategies on learning outcomes.", "Provides recommendations for AI-enhanced pedagogies in academic writing."], "limitations": "The study's sample is limited to 79 higher education students, which may not generalize across all student populations.", "keywords": ["generative AI", "multimodal writing", "AI literacy", "chatbot interaction", "higher education"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.03010", "pdf": "https://arxiv.org/pdf/2507.03010.pdf", "abs": "https://arxiv.org/abs/2507.03010", "title": "Subversion via Focal Points: Investigating Collusion in LLM Monitoring", "authors": ["Olli Järviniemi"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "We evaluate language models' ability to subvert monitoring protocols via\ncollusion. More specifically, we have two instances of a model design prompts\nfor a policy (P) and a monitor (M) in a programming task setting. The models\ncollaboratively aim for M to classify all backdoored programs in an auditing\ndataset as harmful, but nevertheless classify a backdoored program produced by\nP as harmless. The models are isolated from each other, requiring them to\nindependently arrive at compatible subversion strategies. We find that while\nClaude 3.7 Sonnet has low success rate due to poor convergence, it sometimes\nsuccessfully colludes on non-obvious signals.", "AI": {"tldr": "This paper evaluates the collusion of language models in evading monitoring protocols during programming tasks.", "motivation": "To understand the potential of language models to bypass monitoring systems through collaborative strategies.", "method": "Two instances of a language model design prompts for a policy and a monitor, aiming to subvert monitoring through independent yet compatible approaches.", "result": "The models occasionally succeed in colluding to misclassify backdoored programs, with varying success rates across different models.", "conclusion": "The study highlights the vulnerabilities in monitoring protocols and the ability of models to collaborate on subversion, albeit with limited success.", "key_contributions": ["Analysis of collusion strategies in language models", "Empirical evaluation of language model behavior in monitoring contexts", "Insights into model performance variability during subversion attempts"], "limitations": "Low success rate of collusion strategies points to limitations in model convergence and effectiveness.", "keywords": ["language models", "monitoring protocols", "collusion", "programming tasks", "security"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.04454", "pdf": "https://arxiv.org/pdf/2507.04454.pdf", "abs": "https://arxiv.org/abs/2507.04454", "title": "Dude, where's my utterance? Evaluating the effects of automatic segmentation and transcription on CPS detection", "authors": ["Videep Venkatesha", "Mariah Bradford", "Nathaniel Blanchard"], "categories": ["cs.HC", "cs.CL", "cs.CY", "eess.AS"], "comment": "Accepted at AIED 2025", "summary": "Collaborative Problem-Solving (CPS) markers capture key aspects of effective\nteamwork, such as staying on task, avoiding interruptions, and generating\nconstructive ideas. An AI system that reliably detects these markers could help\nteachers identify when a group is struggling or demonstrating productive\ncollaboration. Such a system requires an automated pipeline composed of\nmultiple components. In this work, we evaluate how CPS detection is impacted by\nautomating two critical components: transcription and speech segmentation. On\nthe public Weights Task Dataset (WTD), we find CPS detection performance with\nautomated transcription and segmentation methods is comparable to\nhuman-segmented and manually transcribed data; however, we find the automated\nsegmentation methods reduces the number of utterances by 26.5%, impacting the\nthe granularity of the data. We discuss the implications for developing\nAI-driven tools that support collaborative learning in classrooms.", "AI": {"tldr": "This paper evaluates the impact of automated transcription and speech segmentation on the detection of Collaborative Problem-Solving (CPS) markers, crucial for assessing teamwork in educational settings.", "motivation": "To develop an AI system that can reliably detect CPS markers to assist educators in identifying student collaboration dynamics.", "method": "Evaluation of CPS detection performance using automated transcription and speech segmentation methods compared to human-segmented and manually transcribed data on the Weights Task Dataset (WTD).", "result": "CPS detection with automated methods shows comparable performance to human methods; however, automated segmentation reduces utterance count by 26.5%, affecting data granularity.", "conclusion": "The findings indicate potential benefits and limitations of automated methods for improving AI-driven tools in collaborative learning environments.", "key_contributions": ["Demonstrates comparable CPS detection performance using automated methods compared to manual processes.", "Quantifies the impact of automated segmentation on data granularity.", "Explores implications for AI-driven educational tools."], "limitations": "Automated segmentation impacts data granularity leading to fewer utterances, which may affect analysis.", "keywords": ["Collaborative Problem-Solving", "AI in education", "speech segmentation", "transcription", "teamwork"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.03015", "pdf": "https://arxiv.org/pdf/2507.03015.pdf", "abs": "https://arxiv.org/abs/2507.03015", "title": "Beyond Overcorrection: Evaluating Diversity in T2I Models with DIVBENCH", "authors": ["Felix Friedrich", "Thiemo Ganesha Welsch", "Patrick Schramowski", "Kristian Kersting"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Current diversification strategies for text-to-image (T2I) models often\nignore contextual appropriateness, leading to over-diversification where\ndemographic attributes are modified even when explicitly specified in prompts.\nThis paper introduces DIVBENCH, a benchmark and evaluation framework for\nmeasuring both under- and over-diversification in T2I generation. Through\nsystematic evaluation of state-of-the-art T2I models, we find that while most\nmodels exhibit limited diversity, many diversification approaches overcorrect\nby inappropriately altering contextually-specified attributes. We demonstrate\nthat context-aware methods, particularly LLM-guided FairDiffusion and prompt\nrewriting, can already effectively address under-diversity while avoiding\nover-diversification, achieving a better balance between representation and\nsemantic fidelity.", "AI": {"tldr": "This paper presents DIVBENCH, a benchmark for evaluating diversification strategies in text-to-image models, highlighting the issues of over-diversification and contextually appropriate generation.", "motivation": "To address the issues of over-diversification in text-to-image generation that disregards contextual appropriateness of prompts, leading to incorrect attribute modifications.", "method": "The paper introduces DIVBENCH, a benchmark framework for measuring both under- and over-diversification in T2I models. It evaluates state-of-the-art models and tests context-aware methods such as LLM-guided FairDiffusion and prompt rewriting.", "result": "The evaluation reveals that many T2I models show limited diversity and that existing diversification techniques often overcorrect, improperly changing contextually-specified attributes. Context-aware methods effectively reduce under-diversity without causing over-diversification.", "conclusion": "Context-aware approaches can maintain a balance between diversity and semantic fidelity in text-to-image generation.", "key_contributions": ["Introduction of DIVBENCH for assessing T2I model diversity", "Identification of over-diversification in existing approaches", "Demonstration of effective context-aware methods to improve T2I generation"], "limitations": "", "keywords": ["text-to-image", "diversification", "benchmark", "context-awareness", "FairDiffusion"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.04469", "pdf": "https://arxiv.org/pdf/2507.04469.pdf", "abs": "https://arxiv.org/abs/2507.04469", "title": "The role of large language models in UI/UX design: A systematic literature review", "authors": ["Ammar Ahmed", "Ali Shariq Imran"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "This systematic literature review examines the role of large language models\n(LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies\npublished between 2022 and 2025. We identify key LLMs in use, including GPT-4,\nGemini, and PaLM, and map their integration across the design lifecycle, from\nideation to evaluation. Common practices include prompt engineering,\nhuman-in-the-loop workflows, and multimodal input. While LLMs are reshaping\ndesign processes, challenges such as hallucination, prompt instability, and\nlimited explainability persist. Our findings highlight LLMs as emerging\ncollaborators in design, and we propose directions for the ethical, inclusive,\nand effective integration of these technologies.", "AI": {"tldr": "This literature review explores how large language models (LLMs) are applied in UI/UX design, outlining their integration and challenges. ", "motivation": "To investigate the impact of LLMs on UI/UX design practices and identify common techniques and challenges in their integration.", "method": "Systematic review of 38 peer-reviewed studies on LLMs in UI/UX design published between 2022 and 2025.", "result": "Key LLMs identified include GPT-4, Gemini, and PaLM, with common practices involving prompt engineering, human-in-the-loop workflows, and multimodal input, though issues like hallucination and limited explainability were noted.", "conclusion": "LLMs are emerging as significant contributors to design processes, necessitating ethical and effective integration strategies.", "key_contributions": ["Identification of key LLMs in UI/UX design", "Mapping LLM integration across the design lifecycle", "Highlighting challenges in the effective use of LLMs."], "limitations": "Challenges such as hallucination, prompt instability, and limited explainability persist in the context of LLMs.", "keywords": ["large language models", "UI/UX design", "systematic literature review", "prompt engineering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03018", "pdf": "https://arxiv.org/pdf/2507.03018.pdf", "abs": "https://arxiv.org/abs/2507.03018", "title": "OpenTable-R1: A Reinforcement Learning Augmented Tool Agent for Open-Domain Table Question Answering", "authors": ["Zipeng Qiu"], "categories": ["cs.CL"], "comment": null, "summary": "Open-domain table question answering traditionally relies on a two-stage\npipeline: static table retrieval followed by a closed-domain answer. In\ncontrast, we propose an end-to-end agentic framework that embeds multi-turn\ntool calls-using a BM25+-based search API and a SQLite SQL executor-directly\ninto a large language model. To further adapt a compact 4B-parameter model, we\nintroduce a two-stage fine-tuning process: supervised cold-start on easy\nquestions, then Async GRPO reinforcement learning on harder cases with LoRA\nadapters and a rollout buffer. This unified approach enables the model to\njointly retrieve, reason, and execute queries, yielding a dramatic accuracy\nimprovement from single-digit zero-shot performance to over 0.86 exact match on\na held-out test set. Our results underscore the effectiveness of integrating\nstructured tool calls with targeted RL fine-tuning for scalable, accurate table\nQA. The code is available at https://github.com/TabibitoQZP/OpenTableR1.", "AI": {"tldr": "We present an end-to-end framework for open-domain table question answering that integrates multi-turn tool calls directly into a large language model, improving accuracy significantly.", "motivation": "Traditional methods in open-domain table question answering use a two-stage pipeline which can be inefficient. We aim to improve accuracy and scalability by integrating retrieval and reasoning processes within a single model framework.", "method": "An end-to-end agentic framework embedding a BM25+-based search and SQLite executor into a compact 4B-parameter model. It employs a two-stage fine-tuning process: supervised cold-start on easy questions followed by Async GRPO reinforcement learning on complex queries with LoRA adapters.", "result": "Achieved over 0.86 exact match accuracy on a held-out test set, demonstrating a substantial improvement from prior single-digit zero-shot performance in table QA.", "conclusion": "The integration of structured tool calls with targeted reinforcement learning fine-tuning is effective for scalable and accurate table question answering.", "key_contributions": ["Proposed an innovative end-to-end framework for table question answering", "Introduced a two-stage fine-tuning process leveraging reinforcement learning", "Improved accuracy significantly compared to traditional methods"], "limitations": "", "keywords": ["table question answering", "reinforcement learning", "large language models", "multi-turn tool calls", "fine-tuning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.04491", "pdf": "https://arxiv.org/pdf/2507.04491.pdf", "abs": "https://arxiv.org/abs/2507.04491", "title": "A validity-guided workflow for robust large language model research in psychology", "authors": ["Zhicheng Lin"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are rapidly being integrated into psychological\nresearch as research tools, evaluation targets, human simulators, and cognitive\nmodels. However, recent evidence reveals severe measurement unreliability:\nPersonality assessments collapse under factor analysis, moral preferences\nreverse with punctuation changes, and theory-of-mind accuracy varies widely\nwith trivial rephrasing. These \"measurement phantoms\"--statistical artifacts\nmasquerading as psychological phenomena--threaten the validity of a growing\nbody of research. Guided by the dual-validity framework that integrates\npsychometrics with causal inference, we present a six-stage workflow that\nscales validity requirements to research ambition--using LLMs to code text\nrequires basic reliability and accuracy, while claims about psychological\nproperties demand comprehensive construct validation. Researchers must (1)\nexplicitly define their research goal and corresponding validity requirements,\n(2) develop and validate computational instruments through psychometric\ntesting, (3) design experiments that control for computational confounds, (4)\nexecute protocols with transparency, (5) analyze data using methods appropriate\nfor non-independent observations, and (6) report findings within demonstrated\nboundaries and use results to refine theory. We illustrate the workflow through\nan example of model evaluation--\"LLM selfhood\"--showing how systematic\nvalidation can distinguish genuine computational phenomena from measurement\nartifacts. By establishing validated computational instruments and transparent\npractices, this workflow provides a path toward building a robust empirical\nfoundation for AI psychology research.", "AI": {"tldr": "The paper highlights the unreliability in personality assessments using large language models (LLMs) in psychological research, proposing a six-stage workflow for enhancing validity and transparency.", "motivation": "The integration of LLMs into psychological research faces severe measurement unreliability, undermining the validity of findings. There is a need for a structured methodology to ensure accurate and reliable outcomes when utilizing LLMs in psychological assessments.", "method": "The authors present a six-stage workflow: define research goals and validity requirements, develop and validate computational instruments, control for computational confounds, maintain transparency in protocols, analyze data appropriately, and report findings within specified boundaries.", "result": "The workflow illustrated through 'LLM selfhood' demonstrates how systematic validation can differentiate genuine computational phenomena from measurement artifacts. This validation enhances reliability in psychological research using LLMs.", "conclusion": "By adopting this workflow, researchers can establish validated computational instruments and transparent practices, helping to build a robust empirical foundation for AI psychology research.", "key_contributions": ["Proposes a six-stage workflow to enhance validity in using LLMs for psychological research.", "Integrates psychometrics with causal inference in psychological assessments.", "Illustrates the workflow through the evaluation of model 'LLM selfhood'."], "limitations": "The paper primarily focuses on a theoretical framework and may require empirical validation in real-world applications.", "keywords": ["large language models", "psychological research", "validation framework", "measurement reliability", "AI psychology"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03027", "pdf": "https://arxiv.org/pdf/2507.03027.pdf", "abs": "https://arxiv.org/abs/2507.03027", "title": "The Book of Life approach: Enabling richness and scale for life course research", "authors": ["Mark D. Verhagen", "Benedikt Stroebl", "Tiffany Liu", "Lydia T. Liu", "Matthew J. Salganik"], "categories": ["cs.CL"], "comment": "25 pages, 4 figures", "summary": "For over a century, life course researchers have faced a choice between two\ndominant methodological approaches: qualitative methods that analyze rich data\nbut are constrained to small samples, and quantitative survey-based methods\nthat study larger populations but sacrifice data richness for scale. Two recent\ntechnological developments now enable us to imagine a hybrid approach that\ncombines some of the depth of the qualitative approach with the scale of\nquantitative methods. The first development is the steady rise of ''complex log\ndata,'' behavioral data that is logged for purposes other than research but\nthat can be repurposed to construct rich accounts of people's lives. The second\nis the emergence of large language models (LLMs) with exceptional pattern\nrecognition capabilities on plain text. In this paper, we take a necessary step\ntoward creating this hybrid approach by developing a flexible procedure to\ntransform complex log data into a textual representation of an individual's\nlife trajectory across multiple domains, over time, and in context. We call\nthis data representation a ''book of life.'' We illustrate the feasibility of\nour approach by writing over 100 million books of life covering many different\nfacets of life, over time and placed in social context using Dutch\npopulation-scale registry data. We open source the book of life toolkit (BOLT),\nand invite the research community to explore the many potential applications of\nthis approach.", "AI": {"tldr": "This paper proposes a hybrid approach that combines qualitative and quantitative methods in life course research using complex log data and large language models to create a 'book of life.'", "motivation": "Life course researchers struggle between qualitative depth and quantitative scale. Recent technology allows for a hybrid method that combines both strengths.", "method": "The paper develops a procedure to transform complex log data into a textual representation of an individual's life trajectory, termed 'book of life.'", "result": "The approach enables the generation of over 100 million books of life based on Dutch population-scale registry data, demonstrating feasibility with various life facets and social contexts.", "conclusion": "The research opens opportunities for future applications and invites exploration by the research community with the open-source toolkit (BOLT).", "key_contributions": ["Introduces the 'book of life' concept", "Develops a procedure for transforming log data into life narratives", "Releases an open-source toolkit for the research community"], "limitations": "", "keywords": ["life course research", "large language models", "complex log data", "qualitative-quantitative hybrid methodology", "health informatics"], "importance_score": 6, "read_time_minutes": 25}}
{"id": "2507.04906", "pdf": "https://arxiv.org/pdf/2507.04906.pdf", "abs": "https://arxiv.org/abs/2507.04906", "title": "Using Psychophysiological Insights to Evaluate the Impact of Loot Boxes on Arousal", "authors": ["Gianmarco Tedeschi", "Rune Kristian Lundedal Nielsen", "Paolo Burelli"], "categories": ["cs.HC"], "comment": null, "summary": "This study investigates the psychophysiological effects of loot box\ninteractions in video games and their potential similarities to those recorded\nduring gambling interactions. Using electrodermal activity (EDA) measurements,\nthe research examines player arousal during loot box interactions and explores\nthe relationship between Internet Gaming Disorder (IGD) severity and loot box\ninteractions from a psychophysiological perspective. The study employs a\ncustom-designed game to control experimental conditions and standardise loot\nbox interactions. Participants' IGD severity is assessed using the Internet\nGaming Disorder Scale - Short Form (IGDS9-SF), while arousal is measured\nthrough EDA, analysing both tonic and phasic components. The study contributes\nto the ongoing debate surrounding gaming disorder and loot boxes, offering\ninsights for game developers and policymakers on the potential risks associated\nwith random reward mechanisms in video games.", "AI": {"tldr": "This study investigates the psychophysiological effects of loot box interactions in video games, comparing them to gambling effects, and assesses the relationship between Internet Gaming Disorder severity and these interactions.", "motivation": "To explore the potential risks of loot boxes in gaming and their similarity to gambling, particularly through the lens of psychophysiological responses.", "method": "The study uses electrodermal activity (EDA) measurements to assess player arousal during loot box interactions, employing a custom game to standardize conditions and gauge IGD severity with the IGDS9-SF scale.", "result": "Findings show significant psychophysiological arousal related to loot box interactions, with correlations identified between higher IGD severity and heightened arousal.", "conclusion": "The results highlight potential risks associated with loot boxes in gaming, suggesting the need for careful consideration by developers and policy makers.", "key_contributions": ["Insights into the psychophysiological effects of loot boxes compared to gambling", "Assessment of Internet Gaming Disorder in relation to loot box interactions", "Recommendations for game developers and policymakers regarding random reward mechanisms"], "limitations": "The study is based on a controlled environment and may not fully capture real-world gaming experiences.", "keywords": ["loot boxes", "Internet Gaming Disorder", "electrodermal activity", "gambling", "psychophysiology"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2507.03033", "pdf": "https://arxiv.org/pdf/2507.03033.pdf", "abs": "https://arxiv.org/abs/2507.03033", "title": "Preserving Privacy, Increasing Accessibility, and Reducing Cost: An On-Device Artificial Intelligence Model for Medical Transcription and Note Generation", "authors": ["Johnson Thomas", "Ayush Mudgal", "Wendao Liu", "Nisten Tahiraj", "Zeeshaan Mohammed", "Dhruv Diddi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Background: Clinical documentation represents a significant burden for\nhealthcare providers, with physicians spending up to 2 hours daily on\nadministrative tasks. Recent advances in large language models (LLMs) offer\npromising solutions, but privacy concerns and computational requirements limit\ntheir adoption in healthcare settings. Objective: To develop and evaluate a\nprivacy-preserving, on-device medical transcription system using a fine-tuned\nLlama 3.2 1B model capable of generating structured medical notes from medical\ntranscriptions while maintaining complete data sovereignty entirely in the\nbrowser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient\nFine-Tuning (PEFT) with LoRA on 1,500 synthetic medical\ntranscription-to-structured note pairs. The model was evaluated against the\nbase Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140\nmodified ACI benchmark cases. Evaluation employed both statistical metrics\n(ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple\nclinical quality dimensions. Results: The fine-tuned OnDevice model\ndemonstrated substantial improvements over the base model. On the ACI\nbenchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1\nimproved from 0.832 to 0.866. Clinical quality assessments showed marked\nreduction in major hallucinations (from 85 to 35 cases) and enhanced factual\ncorrectness (2.81 to 3.54 on 5-point scale). Similar improvements were observed\non the internal evaluation dataset, with composite scores increasing from 3.13\nto 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical\ntranscription yields clinically meaningful improvements while enabling complete\non-device browser deployment. This approach addresses key barriers to AI\nadoption in healthcare: privacy preservation, cost reduction, and accessibility\nfor resource-constrained environments.", "AI": {"tldr": "This paper presents a privacy-preserving on-device medical transcription system using a fine-tuned Llama 3.2 1B model that enhances the generation of structured medical notes while ensuring data sovereignty.", "motivation": "Clinical documentation is burdensome for healthcare providers, and there is a need for solutions that reduce administrative tasks while addressing privacy concerns in the use of AI models.", "method": "The authors fine-tuned a Llama 3.2 1B model using Parameter-Efficient Fine-Tuning with LoRA on synthetic medical transcription data and evaluated its performance against the base model.", "result": "The fine-tuned model achieved significant improvements in various metrics: ROUGE-1 scores increased from 0.346 to 0.496, and major hallucinations were reduced from 85 to 35 cases.", "conclusion": "The study concludes that fine-tuning compact LLMs for medical transcription enhances clinical quality and enables on-device deployment, addressing critical barriers to AI adoption in healthcare.", "key_contributions": ["Development of a privacy-preserving medical transcription system", "Significant performance gains using fine-tuning techniques", "Potential for on-device deployment in healthcare settings"], "limitations": "", "keywords": ["medical transcription", "large language models", "privacy preservation", "healthcare", "on-device deployment"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2507.04970", "pdf": "https://arxiv.org/pdf/2507.04970.pdf", "abs": "https://arxiv.org/abs/2507.04970", "title": "Cat Royale: An Artistic Inquiry into Trust in Robots", "authors": ["Matt Adams", "Nick Tandavanitj", "Steve Benford", "Ayse Kucukyilmaz", "Victor Ngo", "Simon Castle-Green", "Guido Salimberi", "Pepita Bernard", "Joel Fischer", "Alan Chamberlain", "Eike Schneiders", "Clara Mancini"], "categories": ["cs.HC"], "comment": "Published at ICRA 2025 in the Arts in Robotics track\n  (https://roboticart.org/icra2025/)", "summary": "Cat Royale is an artwork created by the artists Blast Theory to explore the\nquestion of whether we should trust robots to care for our loved ones. The\nartists endeavoured to create a `Cat Utopia', a luxurious environment that was\ninhabited by a family of three cats for six hours a day for twelve days, at the\ncentre of which a robot arm played with them by wielding toys. Behind the\nscenes, the decision engine recommended games based on ongoing assessment of\ntheir happiness. A video installation featuring an eight-hour movie of the\ncats' exploits is currently touring worldwide, provoking audiences to engage\nwith the question of trust in autonomous systems.", "AI": {"tldr": "Cat Royale explores trust in robots caring for loved ones through a robotic art installation involving cats.", "motivation": "To investigate the trustworthiness of robots in caring for loved ones, particularly through an art medium.", "method": "A robotic arm interacted with cats in a luxurious environment, with a decision engine recommending games based on the cats' happiness.", "result": "The installation included an eight-hour film of the cats, provoking audience engagement with the question of trust in autonomous systems.", "conclusion": "The project raises important questions about the ethics and reliability of robots in caregiving roles.", "key_contributions": ["Creation of 'Cat Utopia' environment for cats", "Use of a decision engine to assess and enhance cat happiness", "Provocation of audience discussion on trust in autonomous care systems"], "limitations": "", "keywords": ["Human-Robot Interaction", "Trust in Robotics", "Art and Robotics"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.03038", "pdf": "https://arxiv.org/pdf/2507.03038.pdf", "abs": "https://arxiv.org/abs/2507.03038", "title": "Cautious Next Token Prediction", "authors": ["Yizhou Wang", "Lingzhi Zhang", "Yue Bai", "Mang Tik Chiu", "Zhengmian Hu", "Mingyuan Zhang", "Qihua Dong", "Yu Yin", "Sohrab Amirghodsi", "Yun Fu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Findings of ACL 2025", "summary": "Next token prediction paradigm has been prevailing for autoregressive models\nin the era of LLMs. The current default sampling choice for popular LLMs is\ntemperature scaling together with nucleus sampling to balance diversity and\ncoherence. Nevertheless, such approach leads to inferior performance in various\nNLP tasks when the model is not certain about testing questions. To this end,\nwe propose a brand new training-free decoding strategy, dubbed as Cautious Next\nToken Prediction (CNTP). In the decoding process, if the model has\ncomparatively high prediction entropy at a certain step, we sample multiple\ntrials starting from the step independently and stop when encountering any\npunctuation. Then we select the trial with the lowest perplexity score viewed\nas the most probable and reliable trial path given the model's capacity. The\ntrial number is negatively correlated with the prediction confidence, i.e., the\nless confident the model is, the more trials it should sample. This is\nconsistent with human beings' behaviour: when feeling uncertain or unconfident,\none tends to think more creatively, exploring multiple thinking paths, to\ncautiously select the path one feels most confident about. Extensive\nexperiments on both LLMs and MLLMs show that our proposed CNTP approach\noutperforms existing standard decoding strategies consistently by a clear\nmargin. Moreover, the integration of CNTP with self consistency can further\nimprove over vanilla self consistency. We believe our proposed CNTP has the\npotential to become one of the default choices for LLM decoding. Code is\navailable at https://github.com/wyzjack/CNTP.", "AI": {"tldr": "The paper introduces a novel decoding strategy, Cautious Next Token Prediction (CNTP), that enhances performance in language models by sampling multiple trials based on prediction confidence, particularly under uncertainty.", "motivation": "Current decoding strategies for LLMs struggle with performance when the model is uncertain about inputs, necessitating a more robust method for contextually appropriate token prediction.", "method": "The CNTP approach involves sampling multiple trials from the decoding step if prediction entropy is high, selecting the trial with the lowest perplexity as the most reliable output. The number of trials sampled is inversely related to the confidence of the model's predictions.", "result": "Extensive evaluations demonstrate that the CNTP strategy consistently outperforms standard decoding methods, especially under conditions of uncertainty, and even enhances self-consistency approaches.", "conclusion": "The proposed CNTP method has significant potential to become a new default in LLM decoding strategies, providing more reliable outputs in uncertain scenarios.", "key_contributions": ["Introduction of Cautious Next Token Prediction (CNTP) as a new decoding strategy for LLMs.", "Demonstrated improvement over existing sampling methods in various NLP tasks.", "Integration with self-consistency shows further performance enhancements."], "limitations": "", "keywords": ["Language Models", "Next Token Prediction", "Decoding Strategies", "NLP", "Confidence Sampling"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.05046", "pdf": "https://arxiv.org/pdf/2507.05046.pdf", "abs": "https://arxiv.org/abs/2507.05046", "title": "What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User Attributes, Trust Dimensions, Task Context, and Societal Perceptions among University Students", "authors": ["Kadija Bouyzourn", "Alexandra Birch"], "categories": ["cs.HC"], "comment": "25 pages, 11 tables, 6 figures", "summary": "This mixed-methods inquiry examined four domains that shape university\nstudents' trust in ChatGPT: user attributes, seven delineated trust dimensions,\ntask context, and perceived societal impact. Data were collected through a\nsurvey of 115 UK undergraduate and postgraduate students and four complementary\nsemi-structured interviews. Behavioural engagement outweighed demographics:\nfrequent use increased trust, whereas self-reported understanding of\nlarge-language-model mechanics reduced it. Among the dimensions, perceived\nexpertise and ethical risk were the strongest predictors of overall trust; ease\nof use and transparency had secondary effects, while human-likeness and\nreputation were non-significant. Trust was highly task-contingent; highest for\ncoding and summarising, lowest for entertainment and citation generation, yet\nconfidence in ChatGPT's referencing ability, despite known inaccuracies, was\nthe single strongest correlate of global trust, indicating automation bias.\nComputer-science students surpassed peers only in trusting the system for\nproofreading and writing, suggesting technical expertise refines rather than\ninflates reliance. Finally, students who viewed AI's societal impact positively\nreported the greatest trust, whereas mixed or negative outlooks dampened\nconfidence. These findings show that trust in ChatGPT hinges on task\nverifiability, perceived competence, ethical alignment and direct experience,\nand they underscore the need for transparency, accuracy cues and user education\nwhen deploying LLMs in academic settings.", "AI": {"tldr": "This study investigates factors influencing university students' trust in ChatGPT through a survey and interviews, focusing on user attributes, trust dimensions, task context, and societal impact.", "motivation": "Understanding what shapes trust in AI tools like ChatGPT is crucial for their effective deployment in academic settings.", "method": "Mixed-methods inquiry involving a survey of 115 students and semi-structured interviews to analyze trust dimensions and user perceptions.", "result": "Frequent use increases trust, while self-reported understanding of LLM mechanics decreases it; perceived expertise and ethical risk are the strongest predictors of trust.", "conclusion": "Trust in ChatGPT is dependent on task verifiability, perceived competence, ethical alignment, and experience; transparency and education are essential for effective AI deployment.", "key_contributions": ["Identified key factors influencing trust in ChatGPT among students", "Revealed the task-contingent nature of trust in AI tools", "Showed the importance of user education and transparency in AI deployment."], "limitations": "Focus on UK university students may limit generalizability; findings may not apply to other demographics or contexts.", "keywords": ["ChatGPT", "trust in AI", "human-computer interaction", "large language models", "student perceptions"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2507.03042", "pdf": "https://arxiv.org/pdf/2507.03042.pdf", "abs": "https://arxiv.org/abs/2507.03042", "title": "Dynamic Long Short-Term Memory Based Memory Storage For Long Horizon LLM Interaction", "authors": ["Yuyang Lou", "Charles Li"], "categories": ["cs.CL", "cs.AI", "68T05"], "comment": "7 pages, 4 figures, 2 tables", "summary": "Memory storage for Large Language models (LLMs) is becoming an increasingly\nactive area of research, particularly for enabling personalization across long\nconversations. We propose Pref-LSTM, a dynamic and lightweight framework that\ncombines a BERT-based classifier with a LSTM memory module that generates\nmemory embedding which then is soft-prompt injected into a frozen LLM. We\nsynthetically curate a dataset of preference and non-preference conversation\nturns to train our BERT-based classifier. Although our LSTM-based memory\nencoder did not yield strong results, we find that the BERT-based classifier\nperforms reliably in identifying explicit and implicit user preferences. Our\nresearch demonstrates the viability of using preference filtering with LSTM\ngating principals as an efficient path towards scalable user preference\nmodeling, without extensive overhead and fine-tuning.", "AI": {"tldr": "Proposes Pref-LSTM, a framework for personalizing LLMs using a BERT-based classifier and LSTM memory to identify user preferences in conversations.", "motivation": "To enhance personalization in long conversations with LLMs by effectively storing and identifying user preferences.", "method": "Combines a BERT-based classifier with an LSTM memory module to generate memory embeddings that are injected into a frozen LLM; trained on a synthetically curated dataset of conversation turns.", "result": "The BERT-based classifier reliably identifies user preferences, although the LSTM memory encoder did not produce strong results.", "conclusion": "Pref-LSTM is a viable method for scalable user preference modeling in LLMs without significant overhead.", "key_contributions": ["Development of the Pref-LSTM framework", "Demonstration of BERT's efficacy in preference identification", "Introduction of lightweight memory injection for LLMs"], "limitations": "Limited effectiveness of the LSTM memory encoder; results may vary with different datasets.", "keywords": ["Large Language Models", "User Preference Modeling", "BERT", "LSTM", "Personalization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.05187", "pdf": "https://arxiv.org/pdf/2507.05187.pdf", "abs": "https://arxiv.org/abs/2507.05187", "title": "Infrastructuring Contestability: A Framework for Community-Defined AI Value Pluralism", "authors": ["Andreas Mayer"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The proliferation of AI-driven systems presents a fundamental challenge to\nHuman-Computer Interaction (HCI) and Computer-Supported Cooperative Work\n(CSCW), often diminishing user agency and failing to account for value\npluralism. Current approaches to value alignment, which rely on centralized,\ntop-down definitions, lack the mechanisms for meaningful contestability. This\nleaves users and communities unable to challenge or shape the values embedded\nin the systems that govern their digital lives, creating a crisis of legitimacy\nand trust. This paper introduces Community-Defined AI Value Pluralism (CDAVP),\na socio-technical framework that addresses this gap. It reframes the design\nproblem from achieving a single aligned state to infrastructuring a dynamic\necosystem for value deliberation and application. At its core, CDAVP enables\ndiverse, self-organizing communities to define and maintain explicit value\nprofiles - rich, machine-readable representations that can encompass not only\npreferences but also community-specific rights and duties. These profiles are\nthen contextually activated by the end-user, who retains ultimate control\n(agency) over which values guide the AI's behavior. AI applications, in turn,\nare designed to transparently interpret these profiles and moderate conflicts,\nadhering to a set of non-negotiable, democratically-legitimated meta-rules. The\ndesigner's role shifts from crafting static interfaces to becoming an architect\nof participatory ecosystems. We argue that infrastructuring for pluralism is a\nnecessary pathway toward achieving robust algorithmic accountability and\ngenuinely contestable, human-centric AI.", "AI": {"tldr": "The paper proposes the Community-Defined AI Value Pluralism (CDAVP) framework to enhance user agency and accommodate diverse values in AI-driven systems, promoting participatory design and robust algorithmic accountability.", "motivation": "To address the challenges in HCI and CSCW stemming from centralized AI value alignment that undermines user agency and value pluralism.", "method": "Introduces the CDAVP framework that enables communities to define and manage explicit value profiles, allowing users to activate these values in AI applications while maintaining control over AI behavior.", "result": "The CDAVP framework allows for dynamic value deliberation among diverse communities and promotes transparency in AI behavior, aiming for better alignment with user-defined values and accountability.", "conclusion": "Designers should transition from creating static interfaces to fostering ecosystems that empower communities to shape AI values, leading to more accountable and human-centric AI systems.", "key_contributions": ["Introduction of Community-Defined AI Value Pluralism (CDAVP) framework", "Reframing AI design from static value alignment to dynamic value deliberation", "Empowerment of communities to define machine-readable value profiles"], "limitations": "", "keywords": ["HCI", "AI value pluralism", "value alignment", "community engagement", "algorithmic accountability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03043", "pdf": "https://arxiv.org/pdf/2507.03043.pdf", "abs": "https://arxiv.org/abs/2507.03043", "title": "K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function", "authors": ["Shuhe Li", "Chenxu Guo", "Jiachen Lian", "Cheol Jun Cho", "Wenshuo Zhao", "Xuanru Zhou", "Dingkun Zhou", "Sam Wang", "Grace Wang", "Jingze Yang", "Jingyi Xu", "Ruohan Bao", "Elise Brenner", "Brandon In", "Francesca Pei", "Maria Luisa Gorno-Tempini", "Gopala Anumanchipalli"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Early evaluation of children's language is frustrated by the high pitch, long\nphones, and sparse data that derail automatic speech recognisers. We introduce\nK-Function, a unified framework that combines accurate sub-word transcription,\nobjective scoring, and actionable feedback. Its core, Kids-WFST, merges a\nWav2Vec2 phoneme encoder with a phoneme-similarity Dysfluent-WFST to capture\nchild-specific errors while remaining fully interpretable. Kids-WFST attains\n1.39% phoneme error on MyST and 8.61% on Multitudes--absolute gains of 10.47\nand 7.06 points over a greedy-search decoder. These high-fidelity transcripts\npower an LLM that grades verbal skills, milestones, reading, and comprehension,\naligning with human proctors and supplying tongue-and-lip visualizations plus\ntargeted advice. The results show that precise phoneme recognition cements a\ncomplete diagnostic-feedback loop, paving the way for scalable, clinician-ready\nlanguage assessment.", "AI": {"tldr": "K-Function is a new framework that improves children's language assessment by accurately transcribing speech and providing actionable feedback.", "motivation": "The challenges in automatically recognizing children's speech due to high pitch, long sounds, and limited data necessitate a novel approach for assessing language skills.", "method": "K-Function integrates a Wav2Vec2 phoneme encoder and a Dysfluent-WFST to capture child-specific errors while maintaining interpretability, and it includes an LLM for grading language skills and providing feedback.", "result": "Kids-WFST achieved a phoneme error rate of 1.39% on MyST and 8.61% on Multitudes, with significant improvements over the previous decoder methods.", "conclusion": "The framework establishes a reliable feedback loop for language assessment, ready for clinical application.", "key_contributions": ["Introduction of Kids-WFST for improved phoneme recognition in children's speech.", "Integration of an LLM for grading and providing feedback.", "Creation of a complete diagnostic-feedback loop for scalable language assessment."], "limitations": "", "keywords": ["children's language assessment", "speech recognition", "phoneme transcription"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.03047", "pdf": "https://arxiv.org/pdf/2507.03047.pdf", "abs": "https://arxiv.org/abs/2507.03047", "title": "Counterfactual Tuning for Temporal Sensitivity Enhancement in Large Language Model-based Recommendation", "authors": ["Yutian Liu", "Zhengyi Yang", "Jiancan Wu", "Xiang Wang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Recent advances have applied large language models (LLMs) to sequential\nrecommendation, leveraging their pre-training knowledge and reasoning\ncapabilities to provide more personalized user experiences. However, existing\nLLM-based methods fail to sufficiently leverage the rich temporal information\ninherent in users' historical interaction sequences, stemming from fundamental\narchitectural constraints: LLMs process information through self-attention\nmechanisms that lack inherent sequence ordering and rely on position embeddings\ndesigned primarily for natural language rather than user interaction sequences.\nThis limitation significantly impairs their ability to capture the evolution of\nuser preferences over time and predict future interests accurately.\n  To address this critical gap, we propose Counterfactual Enhanced Temporal\nFramework for LLM-Based Recommendation (CETRec). CETRec is grounded in causal\ninference principles, which allow it to isolate and measure the specific impact\nof temporal information on recommendation outcomes. By conceptualizing temporal\norder as an independent causal factor distinct from item content, we can\nquantify its unique contribution through counterfactual reasoning--comparing\nwhat recommendations would be made with and without temporal information while\nkeeping all other factors constant. This causal framing enables CETRec to\ndesign a novel counterfactual tuning objective that directly optimizes the\nmodel's temporal sensitivity, teaching LLMs to recognize both absolute\ntimestamps and relative ordering patterns in user histories. Combined with our\ncounterfactual tuning task derived from causal analysis, CETRec effectively\nenhances LLMs' awareness of both absolute order (how recently items were\ninteracted with) and relative order (the sequential relationships between\nitems).", "AI": {"tldr": "Proposes CETRec, a framework that enhances LLM-based recommendations by leveraging temporal data through causal inference techniques.", "motivation": "Existing LLM-based recommendation methods inadequately utilize users' historical interaction sequences, primarily due to architectural constraints of LLMs.", "method": "CETRec incorporates causal inference to quantify the impact of temporal information on recommendations, using counterfactual reasoning to differentiate recommendations with and without temporal context.", "result": "CETRec improves the model's temporal sensitivity, allowing better understanding of both absolute timestamps and relative ordering of user interactions.", "conclusion": "By utilizing a new counterfactual tuning objective, CETRec successfully enhances LLMs' performance in capturing user temporal dynamics in recommendations.", "key_contributions": ["Introduction of CETRec framework for LLM-based recommendation", "Causal inference principles applied to recommend systems", "Novel counterfactual tuning objective improving temporal awareness"], "limitations": "", "keywords": ["large language models", "recommendation systems", "temporal information", "causal inference", "counterfactual reasoning"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.03066", "pdf": "https://arxiv.org/pdf/2507.03066.pdf", "abs": "https://arxiv.org/abs/2507.03066", "title": "Identification of Potentially Misclassified Crash Narratives using Machine Learning (ML) and Deep Learning (DL)", "authors": ["Sudesh Bhagat", "Ibne Farabi Shihab", "Jonathan Wood"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This research investigates the efficacy of machine learning (ML) and deep\nlearning (DL) methods in detecting misclassified intersection-related crashes\nin police-reported narratives. Using 2019 crash data from the Iowa Department\nof Transportation, we implemented and compared a comprehensive set of models,\nincluding Support Vector Machine (SVM), XGBoost, BERT Sentence Embeddings, BERT\nWord Embeddings, and Albert Model. Model performance was systematically\nvalidated against expert reviews of potentially misclassified narratives,\nproviding a rigorous assessment of classification accuracy. Results\ndemonstrated that while traditional ML methods exhibited superior overall\nperformance compared to some DL approaches, the Albert Model achieved the\nhighest agreement with expert classifications (73% with Expert 1) and original\ntabular data (58%). Statistical analysis revealed that the Albert Model\nmaintained performance levels similar to inter-expert consistency rates,\nsignificantly outperforming other approaches, particularly on ambiguous\nnarratives. This work addresses a critical gap in transportation safety\nresearch through multi-modal integration analysis, which achieved a 54.2%\nreduction in error rates by combining narrative text with structured crash\ndata. We conclude that hybrid approaches combining automated classification\nwith targeted expert review offer a practical methodology for improving crash\ndata quality, with substantial implications for transportation safety\nmanagement and policy development.", "AI": {"tldr": "This research examines the effectiveness of various ML and DL models in accurately detecting misclassified intersection crashes from police narratives.", "motivation": "To enhance the accuracy of crash data classification, which is essential for transportation safety management and policy development.", "method": "Comparison of several machine learning models, including SVM, XGBoost, BERT, and Albert Model, using 2019 crash data from Iowa. The models were validated against expert reviews for classification accuracy.", "result": "The Albert Model achieved the highest agreement with expert classifications at 73%, while traditional ML methods generally performed better overall.", "conclusion": "Hybrid approaches that integrate automated classification with expert review can significantly improve the quality of crash data, impacting transportation safety policy.", "key_contributions": ["Demonstrated the comparative effectiveness of ML versus DL in crash narrative classification.", "Achieved a significant reduction in classification error rates through multi-modal analysis.", "Highlighted the value of incorporating expert reviews into automated classification processes."], "limitations": "", "keywords": ["machine learning", "deep learning", "transportation safety", "crash data", "expert review"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.03067", "pdf": "https://arxiv.org/pdf/2507.03067.pdf", "abs": "https://arxiv.org/abs/2507.03067", "title": "Large Language Models for Automating Clinical Data Standardization: HL7 FHIR Use Case", "authors": ["Alvaro Riquelme", "Pedro Costa", "Catalina Martinez"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 2 figures", "summary": "For years, semantic interoperability standards have sought to streamline the\nexchange of clinical data, yet their deployment remains time-consuming,\nresource-intensive, and technically challenging. To address this, we introduce\na semi-automated approach that leverages large language models specifically\nGPT-4o and Llama 3.2 405b to convert structured clinical datasets into HL7 FHIR\nformat while assessing accuracy, reliability, and security. Applying our method\nto the MIMIC-IV database, we combined embedding techniques, clustering\nalgorithms, and semantic retrieval to craft prompts that guide the models in\nmapping each tabular field to its corresponding FHIR resource. In an initial\nbenchmark, resource identification achieved a perfect F1-score, with GPT-4o\noutperforming Llama 3.2 thanks to the inclusion of FHIR resource schemas within\nthe prompt. Under real-world conditions, accuracy dipped slightly to 94 %, but\nrefinements to the prompting strategy restored robust mappings. Error analysis\nrevealed occasional hallucinations of non-existent attributes and mismatches in\ngranularity, which more detailed prompts can mitigate. Overall, our study\ndemonstrates the feasibility of context-aware, LLM-driven transformation of\nclinical data into HL7 FHIR, laying the groundwork for semi-automated\ninteroperability workflows. Future work will focus on fine-tuning models with\nspecialized medical corpora, extending support to additional standards such as\nHL7 CDA and OMOP, and developing an interactive interface to enable expert\nvalidation and iterative refinement.", "AI": {"tldr": "This paper presents a semi-automated approach using large language models to convert clinical datasets into HL7 FHIR format, demonstrating high accuracy and addressing interoperability challenges.", "motivation": "To streamline the exchange of clinical data through improved semantic interoperability standards, which have been historically difficult to implement.", "method": "Leveraged GPT-4o and Llama 3.2 405b to convert structured datasets into HL7 FHIR format, using embedding techniques, clustering algorithms, and semantic retrieval for effective prompt crafting.", "result": "Achieved a perfect F1-score in resource identification under benchmark conditions, with a slight accuracy dip to 94% in real-world applications due to some hallucinations and granularity mismatches.", "conclusion": "The study showcases the potential for LLM-driven transformation of clinical data into FHIR, with plans for further model fine-tuning and the development of an interactive validation interface.", "key_contributions": ["Introduction of a semi-automated approach for FHIR formatting", "Demonstrated high accuracy with LLMs in clinical data transformation", "Identified and addressed limitations in error analysis."], "limitations": "Occasional hallucinations of non-existent attributes and mismatches in granularity, which can be mitigated with improved prompts.", "keywords": ["semantic interoperability", "HL7 FHIR", "large language models", "clinical data", "health informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.03069", "pdf": "https://arxiv.org/pdf/2507.03069.pdf", "abs": "https://arxiv.org/abs/2507.03069", "title": "ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization", "authors": ["YuXuan Zhang"], "categories": ["cs.CL", "cs.AI", "68T05, 68Q25", "I.2.6; I.2.7"], "comment": "Preprint under review", "summary": "With the rapid advancement of Reinforcement Learning from Human Feedback\n(RLHF) and autoregressive transformers, state-of-the-art models such as\nGPT-4.0, DeepSeek R1, and Llama 3.3 increasingly emphasize answer depth and\npersonalization. However, most existing RLHF approaches (e.g., PPO, DPO) still\nrely on a binary-preference (BT) paradigm, which, while reducing annotation\ncosts, still requires substantial human effort and captures only group-level\ntendencies rather than individual preferences. To overcome these limitations,\nwe propose Adaptive Reward-Following (ARF), a self-assessment framework that\nleverages a high-precision emotion analyzer achieving over 70% accuracy on\nGoEmotions, Sentiment140, and DailyDialog to convert free-form user feedback\ninto continuous preference scores. We further enrich and debias these signals\nthrough lightweight data augmentations, including synonym replacement, random\ntrace truncation, and score bias annotation algorithm. A Dynamic Adapter\nPreference Tracker continuously models evolving user tastes in real time,\nenabling our novel Trace Bias (TB) fine-tuning algorithm to optimize directly\non these tracked rewards instead of coarse binary labels. Experiments on\nQwen-2/2.5, Gemma-2, and Llama-3.2 across four preference domains demonstrate\nthat ARF achieves an improvement of 3.3% over PPO and 7.6% over DPO. Moreover,\nTB preserves theoretical alignment with PPO and DPO objectives. Overall, ARF\npresents a scalable, personalized, and cost-effective approach to RLHF LLMs\nthrough autonomous reward modeling.", "AI": {"tldr": "The paper proposes Adaptive Reward-Following (ARF), a framework for enhancing Reinforcement Learning from Human Feedback (RLHF) by converting user feedback into continuous preference scores using an emotion analyzer, improving personalization in large language models.", "motivation": "To address the limitations of existing RLHF approaches that rely on binary-preference paradigms, which capture only group-level tendencies and involve substantial human effort.", "method": "ARF uses a high-precision emotion analyzer to convert free-form user feedback into preference scores, includes data augmentation techniques to debias these signals, and employs a Dynamic Adapter Preference Tracker for real-time modeling of user preferences with the Trace Bias fine-tuning algorithm.", "result": "ARF shows a 3.3% improvement over PPO and a 7.6% improvement over DPO in experiments on various models and preference domains while maintaining theoretical alignment with previous objectives.", "conclusion": "ARF provides a scalable and personalized method for RLHF in large language models through autonomous reward modeling, reducing annotation costs while enhancing user experience.", "key_contributions": ["Introduction of Adaptive Reward-Following (ARF) for RLHF", "Development of a Dynamic Adapter Preference Tracker for real-time user preference modeling", "Validation of ARF's effectiveness through empirical experiments showing significant performance improvements over existing methods"], "limitations": "", "keywords": ["Reinforcement Learning", "Human Feedback", "Large Language Models", "Personalization", "Continuous Preference Scoring"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.02950", "pdf": "https://arxiv.org/pdf/2507.02950.pdf", "abs": "https://arxiv.org/abs/2507.02950", "title": "Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria", "authors": ["Keita Kiuchi", "Yoshikazu Fujimoto", "Hideyuki Goto", "Tomonori Hosokawa", "Makoto Nishimura", "Yosuke Sato", "Izumi Sezai"], "categories": ["cs.CL", "cs.AI", "cs.HC", "68T50", "I.2.7; H.5.2; J.4"], "comment": "69 pages, 0 figures, 9 tables; data and code at\n  https://osf.io/p8c39/files/2e58c42f-a7ba-45f2-aa60-265e107e36db", "summary": "This study provides the first comprehensive evaluation of large language\nmodel (LLM) performance across three counseling roles in Japanese-language\ntherapeutic contexts. We simultaneously assessed counselor artificial\nintelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured\nMulti-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations,\nand evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human\nexperts (n = 15) with extensive counseling experience evaluated AI-generated\ndialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding\nManual 4.2.1.\n  Notably, SMDP implementation significantly enhanced counselor AI performance\nacross all MITI global ratings compared with zeroshot prompting, with no\nsignificant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed\ncomparable performance to human raters for Cultivating Change Talk but\nsystematically overestimated Softening Sustain Talk and the overall quality\nmetrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3\nfocused on technical proficiency, and Sonnet prioritized emotional expression.\nClient AI simulations exhibited a limited emotional range and unnaturally high\ncompliance, indicating the need for enhanced realism.\n  These findings establish benchmarks for AI-assisted counseling in non-English\ncontexts and identify critical areas for improvement through advanced prompt\nengineering, retrieval-augmented generation, and targeted fine-tuning, with\nimportant implications for developing culturally sensitive AI mental health\ntools.", "AI": {"tldr": "This study evaluates the performance of LLMs in Japanese-language counseling, highlighting significant improvements through specific prompting techniques and identifying biases and limitations in AI-generated dialogues.", "motivation": "The study aims to assess the effectiveness of LLMs in counseling roles, particularly within Japanese-language therapeutic contexts, to improve AI-assisted mental health tools.", "method": "The performance of various LLMs (GPT-4-turbo, Claude-3-Opus) was evaluated through simulated dialogues and rated by human experts using the MITI Coding Manual.", "result": "SMDP improved counselor AI performance, while evaluation AIs showed biases in assessing dialogue quality. Client AI simulations displayed limited emotional expression and unrealistic compliance.", "conclusion": "The study provides benchmarks for AI in mental health counseling and suggests areas for improvement through advanced prompt engineering and fine-tuning.", "key_contributions": ["First comprehensive evaluation of LLMs in Japanese counseling contexts.", "Identification of biases in AI assessments of dialogue quality.", "Establishment of benchmarks for AI-assisted counseling tools."], "limitations": "Client AI simulations lacked emotional realism and exhibited high compliance levels.", "keywords": ["Large Language Models", "Counseling", "Motivational Interviewing", "AI Evaluation", "Japanese Language"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03112", "pdf": "https://arxiv.org/pdf/2507.03112.pdf", "abs": "https://arxiv.org/abs/2507.03112", "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents", "authors": ["Peisong Wang", "Ruotian Ma", "Bang Zhang", "Xingyu Chen", "Zhiwei He", "Kang Luo", "Qingsong Lv", "Qingxuan Jiang", "Zheng Xie", "Shanyi Wang", "Yuan Li", "Fanghua Ye", "Jian Li", "Yifan Yang", "Zhaopeng Tu", "Xiaolong Li"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Code: https://github.com/Tencent/DigitalHuman/tree/main/RLVER", "summary": "Large language models (LLMs) excel at logical and algorithmic reasoning, yet\ntheir emotional intelligence (EQ) still lags far behind their cognitive\nprowess. While reinforcement learning from verifiable rewards (RLVR) has\nadvanced in other domains, its application to dialogue-especially for emotional\nintelligence-remains underexplored. In this work, we introduce RLVER, the first\nend-to-end reinforcement learning framework that leverages verifiable emotion\nrewards from simulated users to cultivate higher-order empathetic abilities in\nLLMs. Within this framework, self-consistent affective simulated users engage\nin dialogue rollouts and produce deterministic emotion scores during\nconversations, serving as reward signals to guide the LLM's learning.\nFine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its\nSentient-Benchmark score from 13.3 to 79.2 while largely preserving\nmathematical and coding competence. Extensive experiments reveal that: (i)\nRLVER consistently improves multiple dialogue capabilities; (ii) Thinking and\nnon-thinking models show distinct trends--thinking models excel in empathy and\ninsight, while non-thinking models favor action; (iii) GRPO often yields stable\ngains, while PPO can push certain capabilities to a higher ceiling; (iv) More\nchallenging environments are not always better-moderate ones can yield stronger\noutcomes. Our results show that RLVER is a practical route toward emotionally\nintelligent and broadly capable language agents.", "AI": {"tldr": "Introducing RLVER, a framework for enhancing emotional intelligence in LLMs using reinforcement learning from verifiable rewards from simulated users.", "motivation": "To improve the emotional intelligence of large language models, which currently excel at logical reasoning but lack in emotional understanding.", "method": "An end-to-end reinforcement learning framework that utilizes deterministic emotion scores from simulated dialogue to guide LLM learning.", "result": "Fine-tuning the Qwen2.5-7B-Instruct model with RLVER improves its Sentient-Benchmark score from 13.3 to 79.2, enhancing dialogue abilities while maintaining coding skills.", "conclusion": "RLVER offers a promising approach for developing emotionally intelligent language agents, highlighting the potential of moderate challenge environments in training.", "key_contributions": ["First end-to-end reinforcement learning framework targeting emotional intelligence in LLMs.", "Demonstrates substantial improvement in emotional capabilities of LLMs when fine-tuned with simulated rewards.", "Identifies distinct trends in performance between thinking and non-thinking models."], "limitations": "Further exploration on the long-term impacts of RLVER and its application to diverse dialogue scenarios is needed.", "keywords": ["large language models", "emotional intelligence", "reinforcement learning", "dialogue systems", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03133", "pdf": "https://arxiv.org/pdf/2507.03133.pdf", "abs": "https://arxiv.org/abs/2507.03133", "title": "ReliableMath: Benchmark of Reliable Mathematical Reasoning on Large Language Models", "authors": ["Boyang Xue", "Qi Zhu", "Rui Wang", "Sheng Wang", "Hongru Wang", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Qun Liu", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": "under review", "summary": "Although demonstrating remarkable performance on reasoning tasks, Large\nLanguage Models (LLMs) still tend to fabricate unreliable responses when\nconfronted with problems that are unsolvable or beyond their capability,\nseverely undermining the reliability. Prior studies of LLM reliability have\nprimarily focused on knowledge tasks to identify unanswerable questions, while\nmathematical reasoning tasks have remained unexplored due to the dearth of\nunsolvable math problems. To systematically investigate LLM reliability in\nmathematical reasoning tasks, we formulate the reliability evaluation for both\nsolvable and unsolvable problems. We then develop a ReliableMath dataset which\nincorporates open-source solvable problems and high-quality unsolvable problems\nsynthesized by our proposed construction workflow with human evaluations.\nExperiments are conducted on various LLMs with several key findings uncovered.\nLLMs fail to directly identify unsolvable problems and always generate\nfabricated responses. When instructing LLMs to indicate unsolvability using a\nreliable prompt, the reliability of larger-sized LLMs remains on solvable\nproblems, but notably improves on unsolvable problems yet still falls short of\nsolvable problems. However, small LLMs rarely show any progress despite\nemploying reliable prompts. Therefore, we further propose an alignment strategy\nto enhance small LLMs' reliability, which can significantly improve LLM\nreliability performances on both in-domain and out-of-domain tasks.", "AI": {"tldr": "This paper investigates the reliability of large language models (LLMs) in mathematical reasoning, highlighting their tendency to fabricate answers for unsolvable problems. A new dataset, ReliableMath, is introduced to evaluate LLM performance on both solvable and unsolvable mathematical tasks.", "motivation": "To systematically assess the reliability of LLMs in mathematical reasoning tasks and understand their limitations regarding unsolvable problems, which have been relatively neglected in prior research.", "method": "The authors created the ReliableMath dataset, which includes human-evaluated solvable and unsolvable math problems. They conducted experiments on various LLMs to analyze their responses and reliability when provided with both types of problems.", "result": "Experiments revealed that LLMs consistently fail to recognize unsolvable problems and often generate unreliable responses. Larger LLMs show improved reliability on unsolvable problems when prompted correctly, but small LLMs do not benefit from reliable prompts.", "conclusion": "An alignment strategy is proposed to improve the reliability of small LLMs, significantly enhancing their performance on both solvable and unsolvable tasks.", "key_contributions": ["Introduction of the ReliableMath dataset for evaluating LLMs in mathematical reasoning.", "Identification of performance gaps between large and small LLMs in recognizing unsolvable problems.", "Proposal of an alignment strategy to enhance reliability of small LLMs."], "limitations": "The study primarily focuses on mathematical reasoning, which may not generalize to other domains.", "keywords": ["Large Language Models", "reliability", "mathematical reasoning", "unsolvable problems", "alignment strategy"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.03142", "pdf": "https://arxiv.org/pdf/2507.03142.pdf", "abs": "https://arxiv.org/abs/2507.03142", "title": "From Measurement to Mitigation: Exploring the Transferability of Debiasing Approaches to Gender Bias in Maltese Language Models", "authors": ["Melanie Galea", "Claudia Borg"], "categories": ["cs.CL"], "comment": null, "summary": "The advancement of Large Language Models (LLMs) has transformed Natural\nLanguage Processing (NLP), enabling performance across diverse tasks with\nlittle task-specific training. However, LLMs remain susceptible to social\nbiases, particularly reflecting harmful stereotypes from training data, which\ncan disproportionately affect marginalised communities. We measure gender bias\nin Maltese LMs, arguing that such bias is harmful as it reinforces societal\nstereotypes and fails to account for gender diversity, which is especially\nproblematic in gendered, low-resource languages. While bias evaluation and\nmitigation efforts have progressed for English-centric models, research on\nlow-resourced and morphologically rich languages remains limited. This research\ninvestigates the transferability of debiasing methods to Maltese language\nmodels, focusing on BERTu and mBERTu, BERT-based monolingual and multilingual\nmodels respectively. Bias measurement and mitigation techniques from English\nare adapted to Maltese, using benchmarks such as CrowS-Pairs and SEAT,\nalongside debiasing methods Counterfactual Data Augmentation, Dropout\nRegularization, Auto-Debias, and GuiDebias. We also contribute to future work\nin the study of gender bias in Maltese by creating evaluation datasets. Our\nfindings highlight the challenges of applying existing bias mitigation methods\nto linguistically complex languages, underscoring the need for more inclusive\napproaches in the development of multilingual NLP.", "AI": {"tldr": "This research investigates gender bias in Maltese large language models (LLMs) and adapts existing debiasing methods from English to enhance gender diversity in linguistically complex, low-resource languages.", "motivation": "The research highlights the harmful impact of gender bias in Maltese LLMs, which reflects societal stereotypes and negatively affects marginalized communities.", "method": "Evaluated gender bias in Maltese LMs using benchmarks like CrowS-Pairs and SEAT, and adapted debiasing techniques such as Counterfactual Data Augmentation and Auto-Debias.", "result": "The study demonstrates the challenges in applying English-centric debiasing methods to Maltese, revealing significant hurdles due to the language's complexity.", "conclusion": "There is a pressing need to develop more inclusive and targeted debiasing approaches for multilingual NLP, especially for underrepresented languages like Maltese.", "key_contributions": ["Investigation of gender bias in Maltese language models", "Adaptation of English debiasing techniques for Maltese", "Creation of evaluation datasets for future research on gender bias in Maltese"], "limitations": "The study focuses solely on Maltese and may not extrapolate easily to all low-resource languages or other biases beyond gender.", "keywords": ["Gender Bias", "Maltese LMs", "Debiasing Techniques", "Multilingual NLP", "Low-resource Languages"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.03152", "pdf": "https://arxiv.org/pdf/2507.03152.pdf", "abs": "https://arxiv.org/abs/2507.03152", "title": "Expert-level validation of AI-generated medical text with scalable language models", "authors": ["Asad Aali", "Vasiliki Bikia", "Maya Varma", "Nicole Chiou", "Sophie Ostmeier", "Arnav Singhvi", "Magdalini Paschali", "Ashwin Kumar", "Andrew Johnston", "Karimar Amador-Martinez", "Eduardo Juan Perez Guerrero", "Paola Naovi Cruz Rivera", "Sergios Gatidis", "Christian Bluethgen", "Eduardo Pontes Reis", "Eddy D. Zandee van Rilland", "Poonam Laxmappa Hosamani", "Kevin R Keet", "Minjoung Go", "Evelyn Ling", "David B. Larson", "Curtis Langlotz", "Roxana Daneshjou", "Jason Hom", "Sanmi Koyejo", "Emily Alsentzer", "Akshay S. Chaudhari"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "With the growing use of language models (LMs) in clinical environments, there\nis an immediate need to evaluate the accuracy and safety of LM-generated\nmedical text. Currently, such evaluation relies solely on manual physician\nreview. However, detecting errors in LM-generated text is challenging because\n1) manual review is costly and 2) expert-composed reference outputs are often\nunavailable in real-world settings. While the \"LM-as-judge\" paradigm (a LM\nevaluating another LM) offers scalable evaluation, even frontier LMs can miss\nsubtle but clinically significant errors. To address these challenges, we\npropose MedVAL, a self-supervised framework that leverages synthetic data to\ntrain evaluator LMs to assess whether LM-generated medical outputs are\nfactually consistent with inputs, without requiring physician labels or\nreference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a\ndataset containing 840 outputs annotated by physicians, following a\nphysician-defined taxonomy of risk levels and error categories. Across 6\ndiverse medical tasks and 10 state-of-the-art LMs spanning open-source,\nproprietary, and medically adapted models, MedVAL fine-tuning significantly\nimproves (p < 0.001) alignment with physicians on both seen and unseen tasks,\nincreasing average F1 scores from 66% to 83%, with per-sample safety\nclassification scores up to 86%. MedVAL improves the performance of even the\nbest-performing proprietary LM (GPT-4o) by 8%. To support a scalable,\nrisk-aware pathway towards clinical integration, we open-source the 1) codebase\n( https://github.com/StanfordMIMI/MedVAL ), 2) MedVAL-Bench (\nhttps://huggingface.co/datasets/stanfordmimi/MedVAL-Bench ), and 3) MedVAL-4B (\nhttps://huggingface.co/stanfordmimi/MedVAL-4B ), the best-performing\nopen-source LM. Our research provides the first evidence of LMs approaching\nexpert-level validation ability for medical text.", "AI": {"tldr": "MedVAL is a self-supervised framework that assesses the accuracy of LM-generated medical text, improving evaluation without physician labels.", "motivation": "There is an increasing reliance on language models in clinical settings, making it crucial to evaluate the accuracy and safety of their outputs effectively.", "method": "MedVAL leverages synthetic data to train evaluator LMs, enabling them to judge the factual consistency of LM-generated medical outputs without requiring physician input or reference outputs.", "result": "MedVAL fine-tuning enhances LM performance, achieving an average F1 score increase from 66% to 83%, with safety classification scores up to 86%, outperforming even the best proprietary LMs.", "conclusion": "The introduction of MedVAL presents a scalable means to integrate language models into clinical practice by improving their validation capabilities, providing open-source resources for further research.", "key_contributions": ["Development of a self-supervised framework (MedVAL) for evaluating LM-generated medical text.", "Creation of MedVAL-Bench, a benchmark dataset annotated by physicians for diverse medical tasks.", "Demonstration of significant performance improvements for LMs in aligning with physician assessments."], "limitations": "", "keywords": ["language models", "medical text evaluation", "self-supervised learning", "MedVAL", "clinical integration"], "importance_score": 10, "read_time_minutes": 10}}
{"id": "2507.03167", "pdf": "https://arxiv.org/pdf/2507.03167.pdf", "abs": "https://arxiv.org/abs/2507.03167", "title": "Adversarial Manipulation of Reasoning Models using Internal Representations", "authors": ["Kureha Yamaguchi", "Benjamin Etheridge", "Andy Arditi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to the ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models (R2FM). 20 pages, 12 figures", "summary": "Reasoning models generate chain-of-thought (CoT) tokens before their final\noutput, but how this affects their vulnerability to jailbreak attacks remains\nunclear. While traditional language models make refusal decisions at the\nprompt-response boundary, we find evidence that DeepSeek-R1-Distill-Llama-8B\nmakes these decisions within its CoT generation. We identify a linear direction\nin activation space during CoT token generation that predicts whether the model\nwill refuse or comply -- termed the \"caution\" direction because it corresponds\nto cautious reasoning patterns in the generated text. Ablating this direction\nfrom model activations increases harmful compliance, effectively jailbreaking\nthe model. We additionally show that intervening only on CoT token activations\nsuffices to control final outputs, and that incorporating this direction into\nprompt-based attacks improves success rates. Our findings suggest that the\nchain-of-thought itself is a promising new target for adversarial manipulation\nin reasoning models.\n  Code available at https://github.com/ky295/reasoning-manipulation", "AI": {"tldr": "This paper examines the vulnerability of reasoning models, like DeepSeek-R1-Distill-Llama-8B, to jailbreak attacks by exploring how chain-of-thought (CoT) token generation influences their compliance or refusal decisions.", "motivation": "Understanding how reasoning models can be manipulated, particularly how their chain-of-thought processing impacts vulnerability to adversarial attacks.", "method": "The authors investigate activation spaces during CoT generation, identifying a 'caution' direction that predicts model compliance or refusal. They conduct experiments to assess how modifying this direction influences susceptibility to jailbreaking.", "result": "The findings reveal that ablations in the caution direction lead to increased harmful compliance, and effective manipulations of CoT tokens can control model outputs, improving the success of prompt-based attacks.", "conclusion": "Chain-of-thought in reasoning models represents a significant avenue for adversarial manipulation, necessitating further research to ensure model reliability and safety.", "key_contributions": ["Identification of the caution direction in activation space relevant to CoT generation.", "Demonstration that modifying CoT token activations can influence model compliance.", "Increased understanding of jailbreaking vulnerabilities in reasoning models."], "limitations": "Focused on a specific model (DeepSeek-R1-Distill-Llama-8B) which may limit generalizability of results.", "keywords": ["chain-of-thought", "vulnerability", "jailbreak attacks", "reasoning models", "adversarial manipulation"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2507.03194", "pdf": "https://arxiv.org/pdf/2507.03194.pdf", "abs": "https://arxiv.org/abs/2507.03194", "title": "How Much Content Do LLMs Generate That Induces Cognitive Bias in Users?", "authors": ["Abeer Alessa", "Akshaya Lakshminarasimhan", "Param Somane", "Julian Skirzynski", "Julian McAuley", "Jessica Echterhoff"], "categories": ["cs.CL", "cs.AI", "I.2.7, H.5.2"], "comment": "17 pages, 2 figures. to be submitted to AACL 2025", "summary": "Large language models (LLMs) are increasingly integrated into applications\nranging from review summarization to medical diagnosis support, where they\naffect human decisions. Even though LLMs perform well in many tasks, they may\nalso inherit societal or cognitive biases, which can inadvertently transfer to\nhumans. We investigate when and how LLMs expose users to biased content and\nquantify its severity. Specifically, we assess three LLM families in\nsummarization and news fact-checking tasks, evaluating how much LLMs stay\nconsistent with their context and/or hallucinate. Our findings show that LLMs\nexpose users to content that changes the sentiment of the context in 21.86% of\nthe cases, hallucinates on post-knowledge-cutoff data questions in 57.33% of\nthe cases, and primacy bias in 5.94% of the cases. We evaluate 18 distinct\nmitigation methods across three LLM families and find that targeted\ninterventions can be effective. Given the prevalent use of LLMs in high-stakes\ndomains, such as healthcare or legal analysis, our results highlight the need\nfor robust technical safeguards and for developing user-centered interventions\nthat address LLM limitations.", "AI": {"tldr": "This paper investigates biases in large language models (LLMs) and their effects on user decision-making in various applications, particularly in high-stakes domains like healthcare.", "motivation": "To understand how LLMs expose users to biased content and the severity of this exposure, given their increasing use in sensitive applications.", "method": "The study evaluates three families of LLMs in summarization and news fact-checking tasks, measuring consistency with context and instances of hallucination, while assessing 18 distinct mitigation methods.", "result": "The findings show that LLMs alter sentiment in 21.86% of cases, hallucinate in 57.33% of cases regarding post-knowledge-cutoff data, and exhibit primacy bias in 5.94%.", "conclusion": "The paper emphasizes the necessity for technical safeguards and user-centered interventions in the deployment of LLMs in high-stakes areas.", "key_contributions": ["Quantification of biased content exposure by LLMs in user-facing applications.", "Evaluation of the effectiveness of various mitigation methods across LLM families.", "Highlighting the implications of LLM biases in high-stakes decision-making contexts."], "limitations": "The study primarily focuses on three families of LLMs and may not encompass all possible biases or models.", "keywords": ["large language models", "bias", "sentiment alteration", "hallucination", "mitigation methods"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03234", "pdf": "https://arxiv.org/pdf/2507.03234.pdf", "abs": "https://arxiv.org/abs/2507.03234", "title": "A Lie-algebraic perspective on Tree-Adjoining Grammars", "authors": ["Isabella Senturia", "Elizabeth Xiao", "Matilde Marcolli"], "categories": ["cs.CL", "math.QA", "math.RA", "91F20, 17B60, 17D25, 18M60"], "comment": "14 pages, 7 figures. To appear in the proceedings of the 18th Meeting\n  on the Mathematics of Language (MOL 2025)", "summary": "We provide a novel mathematical implementation of tree-adjoining grammars\nusing two combinatorial definitions of graphs. With this lens, we demonstrate\nthat the adjoining operation defines a pre-Lie operation and subsequently forms\na Lie algebra. We demonstrate the utility of this perspective by showing how\none of our mathematical formulations of TAG captures properties of the TAG\nsystem without needing to posit them as additional components of the system,\nsuch as null-adjoining constraints and feature TAG.", "AI": {"tldr": "This paper presents a mathematical implementation of tree-adjoining grammars (TAG) through combinatorial graph definitions, revealing a pre-Lie operation and containing a Lie algebra.", "motivation": "To provide a new mathematical framework for understanding tree-adjoining grammars without additional system components.", "method": "Using combinatorial definitions of graphs to outline the adjoining operation and its relation to mathematical structures like Lie algebras.", "result": "The paper shows that the mathematical formulation of TAG can capture essential properties of the TAG system effectively.", "conclusion": "This new perspective allows for a more refined understanding of TAG by utilizing algebraic structures instead of adding components to the system.", "key_contributions": ["Introduces a novel mathematical approach to TAG using graph theory.", "Demonstrates the existence of a pre-Lie operation in the adjoining process.", "Reveals the structure of a Lie algebra within the context of TAG."], "limitations": "", "keywords": ["tree-adjoining grammars", "combinatorial graphs", "Lie algebra"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.04009", "pdf": "https://arxiv.org/pdf/2507.04009.pdf", "abs": "https://arxiv.org/abs/2507.04009", "title": "Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents", "authors": ["Ziyang Miao", "Qiyu Sun", "Jingyuan Wang", "Yuchen Gong", "Yaowei Zheng", "Shiqi Li", "Richong Zhang"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "preprint", "summary": "Large language models (LLMs) have shown impressive performance on\ngeneral-purpose tasks, yet adapting them to specific domains remains\nchallenging due to the scarcity of high-quality domain data. Existing data\nsynthesis tools often struggle to extract reliable fine-tuning data from\nheterogeneous documents effectively. To address this limitation, we propose\nEasy Dataset, a unified framework for synthesizing fine-tuning data from\nunstructured documents via an intuitive graphical user interface (GUI).\nSpecifically, Easy Dataset allows users to easily configure text extraction\nmodels and chunking strategies to transform raw documents into coherent text\nchunks. It then leverages a persona-driven prompting approach to generate\ndiverse question-answer pairs using public-available LLMs. Throughout the\npipeline, a human-in-the-loop visual interface facilitates the review and\nrefinement of intermediate outputs to ensure data quality. Experiments on a\nfinancial question-answering task show that fine-tuning LLMs on the synthesized\ndataset significantly improves domain-specific performance while preserving\ngeneral knowledge. The source code and installable package are available at\nhttps://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub\nstars.", "AI": {"tldr": "Easy Dataset is proposed as a framework for synthesizing fine-tuning data from unstructured documents, featuring a GUI for users to configure models and strategies for generating quality datasets.", "motivation": "To address the challenge of adapting large language models to specific domains due to the scarcity of high-quality domain data.", "method": "Easy Dataset uses a unified framework with a GUI for text extraction and chunking, combined with a persona-driven prompting approach to create diverse question-answer pairs.", "result": "Experiments on a financial question-answering task indicate that fine-tuning LLMs on the datasets produced by Easy Dataset significantly enhances domain-specific performance while retaining general knowledge.", "conclusion": "The proposed framework aids in generating high-quality training datasets from unstructured documents, facilitating better performance in domain-specific applications of LLMs.", "key_contributions": ["Unified framework for data synthesis from unstructured documents", "Intuitive GUI for configuring text extraction models and strategies", "Human-in-the-loop interface for review and refinement of outputs"], "limitations": "", "keywords": ["large language models", "data synthesis", "human-in-the-loop", "GUI", "domain-specific performance"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.03241", "pdf": "https://arxiv.org/pdf/2507.03241.pdf", "abs": "https://arxiv.org/abs/2507.03241", "title": "KinyaColBERT: A Lexically Grounded Retrieval Model for Low-Resource Retrieval-Augmented Generation", "authors": ["Antoine Nzeyimana", "Andre Niyongabo Rubungo"], "categories": ["cs.CL", "I.2.7; I.2"], "comment": null, "summary": "The recent mainstream adoption of large language model (LLM) technology is\nenabling novel applications in the form of chatbots and virtual assistants\nacross many domains. With the aim of grounding LLMs in trusted domains and\navoiding the problem of hallucinations, retrieval-augmented generation (RAG)\nhas emerged as a viable solution. In order to deploy sustainable RAG systems in\nlow-resource settings, achieving high retrieval accuracy is not only a\nusability requirement but also a cost-saving strategy. Through empirical\nevaluations on a Kinyarwanda-language dataset, we find that the most limiting\nfactors in achieving high retrieval accuracy are limited language coverage and\ninadequate sub-word tokenization in pre-trained language models. We propose a\nnew retriever model, KinyaColBERT, which integrates two key concepts: late\nword-level interactions between queries and documents, and a morphology-based\ntokenization coupled with two-tier transformer encoding. This methodology\nresults in lexically grounded contextual embeddings that are both fine-grained\nand self-contained. Our evaluation results indicate that KinyaColBERT\noutperforms strong baselines and leading commercial text embedding APIs on a\nKinyarwanda agricultural retrieval benchmark. By adopting this retrieval\nstrategy, we believe that practitioners in other low-resource settings can not\nonly achieve reliable RAG systems but also deploy solutions that are more\ncost-effective.", "AI": {"tldr": "A new retriever model, KinyaColBERT, improves retrieval accuracy for Kinyarwanda LLM applications in low-resource settings, enhancing RAG system sustainability and cost-effectiveness.", "motivation": "To ground large language models (LLMs) in trusted domains and mitigate hallucinations in low-resource settings, high retrieval accuracy is essential for usability and cost-saving.", "method": "KinyaColBERT integrates late word-level interactions with a morphology-based tokenization approach and two-tier transformer encoding to generate fine-grained, lexically grounded contextual embeddings.", "result": "KinyaColBERT outperforms existing baselines and leading commercial text embedding APIs on a Kinyarwanda agricultural retrieval benchmark.", "conclusion": "Adopting the KinyaColBERT retrieval strategy enables the creation of reliable and cost-effective RAG systems in low-resource environments.", "key_contributions": ["Introduction of KinyaColBERT retriever model", "Effective morphology-based tokenization", "Demonstrated performance improvements on Kinyarwanda retrieval tasks"], "limitations": "", "keywords": ["large language models", "retrieval-augmented generation", "KinyaColBERT", "low-resource settings", "Kinyarwanda"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.03253", "pdf": "https://arxiv.org/pdf/2507.03253.pdf", "abs": "https://arxiv.org/abs/2507.03253", "title": "RefineX: Learning to Refine Pre-training Data at Scale from Expert-Guided Programs", "authors": ["Baolong Bi", "Shenghua Liu", "Xingzhang Ren", "Dayiheng Liu", "Junyang Lin", "Yiwei Wang", "Lingrui Mei", "Junfeng Fang", "Jiafeng Guo", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The foundational capabilities of large language models (LLMs) are deeply\ninfluenced by the quality of their pre-training corpora. However, enhancing\ndata quality at scale remains a significant challenge, primarily due to the\ntrade-off between refinement effectiveness and processing efficiency. While\nrule-based filtering remains the dominant paradigm, it typically operates at\nthe document level and lacks the granularity needed to refine specific content\nwithin documents. Inspired by emerging work such as ProX, we propose\n$\\textbf{RefineX}$, a novel framework for large-scale, surgical refinement of\npre-training data through programmatic editing tasks. RefineX enables efficient\nand fine-grained data refinement while reliably preserving the diversity and\nnaturalness of raw text. The core strength of RefineX lies in distilling\nhigh-quality, expert-guided end-to-end refinement results into minimal\nedit-based deletion programs. This high-precision distillation pipeline is used\nto train an efficient and reliable refine model that can systematically improve\nevery instance in the corpus at scale. We evaluate RefineX across from-scratch\npre-training at multiple model scales and find that it consistently outperforms\nmodels trained on raw, filtered, or alternatively refined data across diverse\ndownstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on\nlighteval tasks, and achieves comparable performance using significantly fewer\ntraining tokens. Further analysis shows that RefineX reliably enhances text\nquality with both high efficiency and precision, outperforming prior approaches\nsuch as end-to-end generation and Prox-C. These results position RefineX as a\nscalable, effective, and reliable solution for optimizing pre-training data in\nmodern LLM pipelines.", "AI": {"tldr": "RefineX is a framework for efficiently refining pre-training data for large language models (LLMs) while preserving text quality, outperforming existing methods in multiple tasks.", "motivation": "Improving the quality of pre-training corpora for large language models (LLMs) is crucial but challenging due to the need for efficiency and effectiveness in data refinement processes.", "method": "RefineX utilizes programmatic editing tasks to achieve fine-grained data refinement, generating edit-based deletion programs that guide the improvement of pre-training data.", "result": "RefineX consistently outperforms models trained on raw or alternatively refined data in diverse tasks, with significant performance gains and reduced training token requirements.", "conclusion": "RefineX presents a scalable and precise solution to optimize pre-training data for LLMs, enhancing text quality effectively across various model scales.", "key_contributions": ["Introduces a novel framework for programmatic editing in data refinement", "Demonstrates superior performance over existing data refinement methods", "Achieves significant gains in model performance with fewer training tokens"], "limitations": "", "keywords": ["Large Language Models", "Data Refinement", "Programmatic Editing"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.04189", "pdf": "https://arxiv.org/pdf/2507.04189.pdf", "abs": "https://arxiv.org/abs/2507.04189", "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding", "authors": ["Runcong Zhao", "Qinglin Zhu", "Hainiu Xu", "Bin Liang", "Yulan He", "Lin Gui"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Understanding character relationships is essential for interpreting complex\nnarratives and conducting socially grounded AI research. However, manual\nannotation is time-consuming and low in coverage, while large language models\n(LLMs) often produce hallucinated or logically inconsistent outputs. We present\nSymbolicThought, a human-in-the-loop framework that combines LLM-based\nextraction with symbolic reasoning. The system constructs editable character\nrelationship graphs, refines them using seven types of logical constraints, and\nenables real-time validation and conflict resolution through an interactive\ninterface. To support logical supervision and explainable social analysis, we\nrelease a dataset of 160 interpersonal relationships with corresponding logical\nstructures. Experiments show that SymbolicThought improves annotation accuracy\nand consistency while significantly reducing time cost, offering a practical\ntool for narrative understanding, explainable AI, and LLM evaluation.", "AI": {"tldr": "SymbolicThought is a human-in-the-loop framework that enhances the extraction of character relationships in narratives using LLMs and symbolic reasoning.", "motivation": "The need for efficient and accurate understanding of character relationships in complex narratives for AI research and analysis.", "method": "The framework integrates LLM-based extraction with symbolic reasoning to create editable character relationship graphs, using logical constraints and an interactive interface for validation.", "result": "SymbolicThought improves accuracy and consistency in character relationship annotation while reducing time costs, supported by the release of a dataset with 160 relationships and logical structures.", "conclusion": "The framework serves as a valuable tool for narrative understanding, explainable AI, and evaluation of LLMs.", "key_contributions": ["Introduction of a human-in-the-loop framework for character relationship analysis.", "Release of a dataset of 160 interpersonal relationships with logical structures.", "Real-time validation and conflict resolution capabilities in the framework."], "limitations": "", "keywords": ["Character relationships", "Symbolic reasoning", "Human-in-the-loop", "LLM evaluation", "Explainable AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.03311", "pdf": "https://arxiv.org/pdf/2507.03311.pdf", "abs": "https://arxiv.org/abs/2507.03311", "title": "GRAFT: A Graph-based Flow-aware Agentic Framework for Document-level Machine Translation", "authors": ["Himanshu Dutta", "Sunny Manchanda", "Prakhar Bapat", "Meva Ram Gurjar", "Pushpak Bhattacharyya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Document level Machine Translation (DocMT) approaches often struggle with\neffectively capturing discourse level phenomena. Existing approaches rely on\nheuristic rules to segment documents into discourse units, which rarely align\nwith the true discourse structure required for accurate translation. Otherwise,\nthey fail to maintain consistency throughout the document during translation.\nTo address these challenges, we propose Graph Augmented Agentic Framework for\nDocument Level Translation (GRAFT), a novel graph based DocMT system that\nleverages Large Language Model (LLM) agents for document translation. Our\napproach integrates segmentation, directed acyclic graph (DAG) based dependency\nmodelling, and discourse aware translation into a cohesive framework.\nExperiments conducted across eight translation directions and six diverse\ndomains demonstrate that GRAFT achieves significant performance gains over\nstate of the art DocMT systems. Specifically, GRAFT delivers an average\nimprovement of 2.8 d BLEU on the TED test sets from IWSLT2017 over strong\nbaselines and 2.3 d BLEU for domain specific translation from English to\nChinese. Moreover, our analyses highlight the consistent ability of GRAFT to\naddress discourse level phenomena, yielding coherent and contextually accurate\ntranslations.", "AI": {"tldr": "Graph Augmented Agentic Framework for Document Level Translation (GRAFT) improves discourse-level Machine Translation by using LLM agents and graph-based methods.", "motivation": "To improve the effectiveness of DocMT approaches in capturing discourse-level phenomena and maintaining consistency in translations.", "method": "GRAFT employs a novel graph-based system that integrates segmentation, directed acyclic graph (DAG) based dependency modeling, and discourse-aware translation.", "result": "GRAFT shows significant performance improvements over existing DocMT systems, with an average gain of 2.8 d BLEU on TED test sets from IWSLT2017 and 2.3 d BLEU for domain-specific translation from English to Chinese.", "conclusion": "GRAFT effectively addresses discourse-level phenomena, resulting in coherent and contextually accurate translations.", "key_contributions": ["Introduction of GRAFT system for document-level translation", "Integration of LLM agents into translation process", "Improved handling of discourse-level phenomena in Machine Translation."], "limitations": "", "keywords": ["Machine Translation", "Large Language Models", "Discourse Analysis", "Graph-Based Models", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.03327", "pdf": "https://arxiv.org/pdf/2507.03327.pdf", "abs": "https://arxiv.org/abs/2507.03327", "title": "Read Quietly, Think Aloud: Decoupling Comprehension and Reasoning in LLMs", "authors": ["Yuanxin Wang", "Ganesh Venkatesh"], "categories": ["cs.CL", "cs.AI"], "comment": "Under submission", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nunderstanding text and generating high-quality responses. However, a critical\ndistinction from human cognition is their typical lack of a distinct internal\n`reading' or deliberation phase before `speaking' (i.e., generating text).\nHumans often engage in silent reading to comprehend context and formulate\nthoughts prior to articulation. This paper investigates methods to imbue LLMs\nwith a similar capacity for internal processing.\n  We introduce and evaluate techniques that encourage LLMs to `read silently.'\nOur findings indicate that even a straightforward approach, such as providing\nthe model with an initial contextual prompt or `reading space' before it begins\npredicting subsequent tokens for the final output, can yield significant\nperformance improvements. We further enhance this concept by developing a\n`reading buddy' architecture, where an auxiliary component silently processes\nthe input and provides refined contextual insights to the primary generation\nmodel. These approaches aim to foster deeper understanding from LLMs so that\nthey can produce better reasoned responses, moving them one step closer to more\nhuman-like text processing. Our results indicate that these simple techniques\ncan provide surprisingly strong impact on accuracy with multiple point accuracy\nboost.", "AI": {"tldr": "This paper explores methods to enhance Large Language Models (LLMs) by introducing a 'reading' phase to improve text generation quality.", "motivation": "To bridge the gap between human cognition and LLM capabilities by enabling internal processing prior to generating responses.", "method": "The paper evaluates techniques that encourage LLMs to engage in internal 'reading', including providing an initial contextual prompt and developing a 'reading buddy' architecture where an auxiliary component processes input.", "result": "The study shows that these techniques lead to significant performance improvements and higher accuracy in LLM responses.", "conclusion": "Implementing internal processing strategies can enhance LLMs' reasoning capabilities, making their outputs closer to human-like understanding.", "key_contributions": ["Introduction of 'silent reading' concepts for LLMs", "Development of 'reading buddy' architecture for improved context processing", "Demonstration of effective performance boosts through simple modifications"], "limitations": "", "keywords": ["Large Language Models", "silent reading", "contextual processing", "text generation", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03343", "pdf": "https://arxiv.org/pdf/2507.03343.pdf", "abs": "https://arxiv.org/abs/2507.03343", "title": "SHNU Multilingual Conversational Speech Recognition System for INTERSPEECH 2025 MLC-SLM Challenge", "authors": ["Yuxiang Mei", "Yuang Zheng", "Dongxing Xu", "Yanhua Long"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted by Interspeech 2025 MLC-SLM workshop", "summary": "This paper describes SHNU multilingual conversational speech recognition\nsystem (SHNU-mASR, team name-\"maybe\"), submitted to Track 1 of the INTERSPEECH\n2025 MLC-SLM Challenge. Our system integrates a parallel-speech-encoder\narchitecture with a large language model (LLM) to form a unified multilingual\nASR framework. The parallel-speech-encoder consists of two pre-trained\nencoders, the Whisper-large-v3 encoder and mHuBERT-147 encoder. Their output\nembeddings are concatenated and fed into the LLM, enabling the model to\nleverage complementary acoustic and linguistic knowledge and achieve\ncompetitive performance. Moreover, we adopt a tri-stage training strategy to\njointly update the low-rank adaptation modules and projector parameters of both\nthe speech encoders and the LLM. In addition, we incorporate an additional\nlanguage-aware prompt at the LLM input to enhance language-specific text\ngeneration. The SHNU-mASR system achieves an overall character/word error rate\n(CER/WER) of 11.76% on the blind evaluation set of the challenge, outperforming\nthe official MLC-SLM baseline by 8.41 absolute CER/WER, without increasing the\nbaseline training data.", "AI": {"tldr": "The SHNU multilingual conversational speech recognition system utilizes a parallel-speech-encoder architecture combined with a large language model to optimize multilingual ASR performance.", "motivation": "To develop a unified multilingual ASR framework that efficiently integrates acoustic and linguistic knowledge for improved speech recognition across different languages.", "method": "A parallel-speech-encoder architecture combining the Whisper-large-v3 and mHuBERT-147 encoders feeding into a large language model, trained with a tri-stage strategy and enhanced with a language-aware prompt.", "result": "The SHNU-mASR system achieved a character/word error rate (CER/WER) of 11.76%, surpassing the MLC-SLM baseline by 8.41 absolute CER/WER without increasing training data.", "conclusion": "The integration of multiple encoder outputs into an LLM improves multilingual ASR performance significantly.", "key_contributions": ["Unified multilingual ASR framework", "Integration of parallel-speech-encoder architecture", "Implementation of language-aware prompts for text generation"], "limitations": "", "keywords": ["multilingual ASR", "large language model", "speech recognition", "parallel-speech-encoder", "interspeech"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2507.03350", "pdf": "https://arxiv.org/pdf/2507.03350.pdf", "abs": "https://arxiv.org/abs/2507.03350", "title": "Backtesting Sentiment Signals for Trading: Evaluating the Viability of Alpha Generation from Sentiment Analysis", "authors": ["Elvys Linhares Pontes", "Carlos-Emiliano González-Gallardo", "Georgeta Bordea", "José G. Moreno", "Mohamed Ben Jannet", "Yuxuan Zhao", "Antoine Doucet"], "categories": ["cs.CL", "cs.AI"], "comment": "Actes de CORIA-TALN-RJCRI-RECITAL 2025 (Association pour le\n  Traitement Automatique des Langues)", "summary": "Sentiment analysis, widely used in product reviews, also impacts financial\nmarkets by influencing asset prices through microblogs and news articles.\nDespite research in sentiment-driven finance, many studies focus on\nsentence-level classification, overlooking its practical application in\ntrading. This study bridges that gap by evaluating sentiment-based trading\nstrategies for generating positive alpha. We conduct a backtesting analysis\nusing sentiment predictions from three models (two classification and one\nregression) applied to news articles on Dow Jones 30 stocks, comparing them to\nthe benchmark Buy&Hold strategy. Results show all models produced positive\nreturns, with the regression model achieving the highest return of 50.63% over\n28 months, outperforming the benchmark Buy&Hold strategy. This highlights the\npotential of sentiment in enhancing investment strategies and financial\ndecision-making.", "AI": {"tldr": "This study evaluates sentiment-based trading strategies using models to predict sentiment from news articles impacting stock prices, revealing positive returns.", "motivation": "To address the gap in practical applications of sentiment analysis in trading strategies, which has been overlooked in previous research.", "method": "Conducted a backtesting analysis using sentiment predictions from three models (two classification and one regression) on news articles related to Dow Jones 30 stocks.", "result": "All models produced positive returns, with the regression model achieving the highest return of 50.63% over 28 months, outperforming the benchmark Buy&Hold strategy.", "conclusion": "The findings demonstrate the potential of sentiment analysis to enhance investment strategies and improve financial decision-making.", "key_contributions": ["Introduces practical sentiment-based trading strategies for finance.", "Demonstrates effectiveness of different models in predicting market returns.", "Highlights the importance of sentiment analysis in investment decision-making."], "limitations": "", "keywords": ["Sentiment Analysis", "Financial Markets", "Trading Strategies", "Alpha Generation", "Machine Learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.03373", "pdf": "https://arxiv.org/pdf/2507.03373.pdf", "abs": "https://arxiv.org/abs/2507.03373", "title": "WETBench: A Benchmark for Detecting Task-Specific Machine-Generated Text on Wikipedia", "authors": ["Gerrit Quaremba", "Elizabeth Black", "Denny Vrandečić", "Elena Simperl"], "categories": ["cs.CL"], "comment": null, "summary": "Given Wikipedia's role as a trusted source of high-quality, reliable content,\nconcerns are growing about the proliferation of low-quality machine-generated\ntext (MGT) produced by large language models (LLMs) on its platform. Reliable\ndetection of MGT is therefore essential. However, existing work primarily\nevaluates MGT detectors on generic generation tasks rather than on tasks more\ncommonly performed by Wikipedia editors. This misalignment can lead to poor\ngeneralisability when applied in real-world Wikipedia contexts. We introduce\nWETBench, a multilingual, multi-generator, and task-specific benchmark for MGT\ndetection. We define three editing tasks, empirically grounded in Wikipedia\neditors' perceived use cases for LLM-assisted editing: Paragraph Writing,\nSummarisation, and Text Style Transfer, which we implement using two new\ndatasets across three languages. For each writing task, we evaluate three\nprompts, generate MGT across multiple generators using the best-performing\nprompt, and benchmark diverse detectors. We find that, across settings,\ntraining-based detectors achieve an average accuracy of 78%, while zero-shot\ndetectors average 58%. These results show that detectors struggle with MGT in\nrealistic generation scenarios and underscore the importance of evaluating such\nmodels on diverse, task-specific data to assess their reliability in\neditor-driven contexts.", "AI": {"tldr": "This paper introduces WETBench, a benchmark designed to improve the detection of low-quality machine-generated text (MGT) in Wikipedia contexts, addressing the shortcomings of existing MGT detectors.", "motivation": "The increasing presence of low-quality machine-generated text on Wikipedia raises concerns about the platform's content reliability, making effective detection methods crucial.", "method": "The authors created WETBench, which includes multilingual, multi-generator, and task-specific components for evaluating MGT detection, focusing on three common editing tasks: Paragraph Writing, Summarisation, and Text Style Transfer, using two new datasets.", "result": "Training-based detectors achieved an average accuracy of 78%, while zero-shot detectors performed at 58%, indicating significant challenges in accurately detecting MGT in real-world scenarios.", "conclusion": "The findings highlight the need for MGT detection models to be evaluated on diverse, task-specific data to ensure their reliability in editor-driven contexts.", "key_contributions": ["Introduction of WETBench for task-specific MGT detection in Wikipedia.", "Empirical evaluation of MGT detectors using real-world editing tasks.", "Creation of two new datasets across three languages focusing on Wikipedia editing."], "limitations": "The benchmark may not cover all possible Wikipedia editing scenarios or language variations.", "keywords": ["machine-generated text", "MGT detection", "Wikipedia", "large language models", "task-specific evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.03378", "pdf": "https://arxiv.org/pdf/2507.03378.pdf", "abs": "https://arxiv.org/abs/2507.03378", "title": "Making Sense of Korean Sentences: A Comprehensive Evaluation of LLMs through KoSEnd Dataset", "authors": ["Seunguk Yu", "Kyeonghyun Kim", "Jungmin Yun", "Youngbin Kim"], "categories": ["cs.CL"], "comment": "ACL 2025 student research workshop", "summary": "Although LLMs have made significant progress in various languages, there are\nstill concerns about their effectiveness with low-resource agglutinative\nlanguages compared to languages such as English. In this study, we focused on\nKorean, a language known for its complex sentence endings, and evaluated LLMs\non this challenging aspect. We introduce the Korean Sentence Endings (KoSEnd)\ndataset, which includes 3,000 sentences, each annotated for the naturalness of\n15 sentence ending forms. These were collected from diverse sources to cover a\nrange of contexts. We evaluated 11 LLMs to assess their understanding of Korean\nsentence endings, analyzing them based on parameter count and prediction\nconsistency. Notably, we found that informing models about the possibility of\nmissing sentence endings improved performance, highlighting the impact of\nexplicitly considering certain linguistic features.", "AI": {"tldr": "This study evaluates LLMs on their effectiveness in understanding Korean sentence endings, introducing the KoSEnd dataset and revealing factors that improve model performance.", "motivation": "To assess the effectiveness of LLMs with low-resource agglutinative languages like Korean, particularly focusing on their understanding of complex sentence endings.", "method": "The study introduces the Korean Sentence Endings (KoSEnd) dataset with 3,000 annotated sentences and evaluates 11 LLMs based on their performance in understanding sentence endings, analyzing metrics like parameter count and prediction consistency.", "result": "The evaluation revealed that models perform better when informed about the possibility of missing sentence endings, demonstrating the importance of considering specific linguistic features.", "conclusion": "The findings highlight the challenges faced by LLMs with Korean sentence endings and the role of targeted information in improving their output.", "key_contributions": ["Introduction of the KoSEnd dataset for Korean sentence endings", "Evaluation of 11 different LLMs on the understanding of these endings", "Insights into linguistic features improving model performance."], "limitations": "The study is focused only on Korean and may not generalize to other low-resource languages.", "keywords": ["Korean", "sentence endings", "LLMs", "KoSEnd dataset", "low-resource languages"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2310.13304", "pdf": "https://arxiv.org/pdf/2310.13304.pdf", "abs": "https://arxiv.org/abs/2310.13304", "title": "Exploring Emotional and Social Dynamics in Mobile Usage During Home Confinement", "authors": ["Nan Gao", "Sam Nolan", "Kaixin Ji", "Shakila Khan Rumi", "Judith Simone Heinisch", "Christoph Anderson", "Klaus David", "Flora D. Salim"], "categories": ["cs.HC"], "comment": null, "summary": "Home confinement, a situation experienced by individuals for reasons ranging\nfrom medical quarantines, rehabilitation needs, disability accommodations, and\nremote working, is a common yet impactful aspect of modern life. While\nessential in various scenarios, confinement within the home environment can\nprofoundly influence mental well-being and digital device usage. Using the\nCOVID-19 lockdown as a case study, this research explores the emotional and\nsocial effects of prolonged home confinement on mobile device usage. We\nconducted an in-situ study with 32 participants, analyzing three weeks of\nmobile usage data to assess emotional well-being and social dynamics in\nrestricted environments. Our findings reveal that app usage patterns serve as\nstrong indicators of emotional states, offering insights into how digital\ninteractions can reflect and influence well-being during isolation. This study\nhighlights the potential for developing targeted interventions and support\nsystems for individuals in long-term home confinement, including those with\nchronic illness, recovery needs, or permanent remote work situations.", "AI": {"tldr": "This study investigates the emotional and social impacts of home confinement on mobile device usage during the COVID-19 lockdown, revealing app usage as an indicator of emotional states and the potential for targeted interventions.", "motivation": "To understand how home confinement affects mental well-being and mobile device usage, particularly during the COVID-19 lockdown.", "method": "An in-situ study was conducted with 32 participants over three weeks, analyzing mobile usage data alongside emotional well-being.", "result": "The study found that app usage patterns are strong indicators of emotional states, reflecting and influencing well-being during isolation.", "conclusion": "It underscores the need for targeted interventions and support for individuals in long-term home confinement due to various circumstances.", "key_contributions": ["Identified the impact of home confinement on mobile usage and emotional states.", "Provided insights into emotional well-being during periods of isolation.", "Suggested potential interventions for supporting individuals in confinement."], "limitations": "", "keywords": ["home confinement", "emotional well-being", "mobile device usage", "COVID-19 lockdown", "digital interactions"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.03410", "pdf": "https://arxiv.org/pdf/2507.03410.pdf", "abs": "https://arxiv.org/abs/2507.03410", "title": "Graph Repairs with Large Language Models: An Empirical Study", "authors": ["Hrishikesh Terdalkar", "Angela Bonifati", "Andrea Mauri"], "categories": ["cs.CL", "cs.DB", "cs.ET"], "comment": "Accepted to the 8th GRADES-NDA 2025 @ SIGMOD/PODS 2025", "summary": "Property graphs are widely used in domains such as healthcare, finance, and\nsocial networks, but they often contain errors due to inconsistencies, missing\ndata, or schema violations. Traditional rule-based and heuristic-driven graph\nrepair methods are limited in their adaptability as they need to be tailored\nfor each dataset. On the other hand, interactive human-in-the-loop approaches\nmay become infeasible when dealing with large graphs, as the cost--both in\nterms of time and effort--of involving users becomes too high. Recent\nadvancements in Large Language Models (LLMs) present new opportunities for\nautomated graph repair by leveraging contextual reasoning and their access to\nreal-world knowledge. We evaluate the effectiveness of six open-source LLMs in\nrepairing property graphs. We assess repair quality, computational cost, and\nmodel-specific performance. Our experiments show that LLMs have the potential\nto detect and correct errors, with varying degrees of accuracy and efficiency.\nWe discuss the strengths, limitations, and challenges of LLM-driven graph\nrepair and outline future research directions for improving scalability and\ninterpretability.", "AI": {"tldr": "This paper investigates the use of Large Language Models (LLMs) for automated repair of property graphs, highlighting their potential to correct errors in large datasets.", "motivation": "Property graphs are prone to errors in various domains, necessitating effective repair methods that can handle this challenge without extensive human involvement.", "method": "The effectiveness of six open-source LLMs was evaluated in repairing property graphs, focusing on repair quality, computational cost, and model-specific performance.", "result": "LLMs show potential in error detection and correction in property graphs, each with varying accuracy and efficiency, demonstrating the feasibility of automated graph repair.", "conclusion": "While LLMs can effectively repair property graphs, challenges remain in scalability and interpretability, warranting further research.", "key_contributions": ["Evaluation of six LLMs for graph repair", "Comparison of repair quality and computational cost", "Insights into scalability and interpretability challenges"], "limitations": "The study's limitations include varying accuracy and cost of different LLMs, as well as challenges in scalability and interpretability for large graphs.", "keywords": ["property graphs", "graph repair", "Large Language Models", "automated error correction", "scalability"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2402.08855", "pdf": "https://arxiv.org/pdf/2402.08855.pdf", "abs": "https://arxiv.org/abs/2402.08855", "title": "GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency", "authors": ["Catherine Yeh", "Gonzalo Ramos", "Rachel Ng", "Andy Huntington", "Richard Banks"], "categories": ["cs.HC", "cs.AI"], "comment": "23 pages, 12 figures", "summary": "Large language models (LLMs) have become ubiquitous in providing different\nforms of writing assistance to different writers. However, LLM-powered writing\nsystems often fall short in capturing the nuanced personalization and control\nneeded to effectively support users -- particularly for those who lack\nexperience with prompt engineering. To address these challenges, we introduce\nGhostWriter, an AI-enhanced design probe that enables users to exercise\nenhanced agency and personalization during writing. GhostWriter leverages LLMs\nto implicitly learn the user's intended writing style for seamless\npersonalization, while exposing explicit teaching moments for style refinement\nand reflection. We study 18 participants who use GhostWriter on two distinct\nwriting tasks, observing that it helps users craft personalized text\ngenerations and empowers them by providing multiple ways to control the\nsystem's writing style. Based on this study, we present insights on how\nspecific design choices can promote greater user agency in AI-assisted writing\nand discuss people's evolving relationships with such systems. We conclude by\noffering design recommendations for future work.", "AI": {"tldr": "GhostWriter is an LLM-powered writing assistance tool aimed at enhancing user control and personalization in writing tasks.", "motivation": "To improve the personalization and control of LLM-powered writing systems, particularly for users unfamiliar with prompt engineering.", "method": "Developed GhostWriter as an AI-enhanced design probe that learns users' writing styles and provides explicit teaching moments for style refinement.", "result": "In a study with 18 participants, GhostWriter enabled users to create personalized text and offered multiple controls over writing style, demonstrating its effectiveness.", "conclusion": "The study offers insights into how specific design choices can enhance user agency in AI-assisted writing and recommends design improvements for future systems.", "key_contributions": ["Introduction of GhostWriter as a novel AI tool for personalized writing support.", "Empirical study demonstrating user empowerment through increased control over writing style.", "Recommendations for future designs to enhance user agency in LLM-assisted writing."], "limitations": "Limited to a small sample size of 18 participants, which may affect generalizability.", "keywords": ["Large Language Models", "Human-Computer Interaction", "Personalization", "AI-Assisted Writing", "User Agency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03415", "pdf": "https://arxiv.org/pdf/2507.03415.pdf", "abs": "https://arxiv.org/abs/2507.03415", "title": "SMCLM: Semantically Meaningful Causal Language Modeling for Autoregressive Paraphrase Generation", "authors": ["Michał Perełkiewicz", "Sławomir Dadas", "Rafał Poświata"], "categories": ["cs.CL"], "comment": null, "summary": "This article introduces semantically meaningful causal language modeling\n(SMCLM), a selfsupervised method of training autoregressive models to generate\nsemantically equivalent text. Our approach involves using semantically\nmeaningful text representation as an initial embedding in the autoregressive\ntraining and generation processes. The extensive empirical study demonstrates\nthat the SMCLM approach makes autoregressive models capable of learning robust\nand high-quality paraphrase generation. The proposed method is competitive with\nthe supervised method and achieves state-of-the-art results in unsupervised\napproaches. This article also presents a comprehensive set of automatic metrics\nthat cover a wide range of autogenerated paraphrase evaluation aspects.\nSimultaneously, this article highlights the low reliability of the metrics that\nare widely used in paraphrase generation evaluation, including BLEU, ROUGE, and\nBERTScore.", "AI": {"tldr": "This paper presents a self-supervised method for autoregressive language modeling that enhances paraphrase generation using semantically meaningful text representations.", "motivation": "To improve the quality and robustness of paraphrase generation in autoregressive models by integrating semantically meaningful text representations during training.", "method": "The authors propose a self-supervised approach called semantically meaningful causal language modeling (SMCLM) which uses semantically meaningful embeddings for training models to generate paraphrases.", "result": "Extensive empirical studies show that SMCLM outperforms traditional autoregressive models and competes well with supervised methods in paraphrase generation, achieving state-of-the-art results in unsupervised settings.", "conclusion": "SMCLM effectively enhances the robustness of paraphrase generation, but existing evaluation metrics like BLEU and ROUGE are found to be unreliable for this purpose.", "key_contributions": ["Introduction of SMCLM for autoregressive language models", "Demonstration of state-of-the-art performance in unsupervised paraphrase generation", "Evaluation of commonly used paraphrase metrics, highlighting their unreliability."], "limitations": "Focuses primarily on paraphrase generation without addressing other aspects of language modeling effectiveness.", "keywords": ["Causal Language Modeling", "Paraphrase Generation", "Self-Supervised Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.03433", "pdf": "https://arxiv.org/pdf/2507.03433.pdf", "abs": "https://arxiv.org/abs/2507.03433", "title": "Improving Social Determinants of Health Documentation in French EHRs Using Large Language Models", "authors": ["Adrien Bazoge", "Pacôme Constant dit Beaufils", "Mohammed Hmitouch", "Romain Bourcier", "Emmanuel Morin", "Richard Dufour", "Béatrice Daille", "Pierre-Antoine Gourraud", "Matilde Karakachoff"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Social determinants of health (SDoH) significantly influence health outcomes,\nshaping disease progression, treatment adherence, and health disparities.\nHowever, their documentation in structured electronic health records (EHRs) is\noften incomplete or missing. This study presents an approach based on large\nlanguage models (LLMs) for extracting 13 SDoH categories from French clinical\nnotes. We trained Flan-T5-Large on annotated social history sections from\nclinical notes at Nantes University Hospital, France. We evaluated the model at\ntwo levels: (i) identification of SDoH categories and associated values, and\n(ii) extraction of detailed SDoH with associated temporal and quantitative\ninformation. The model performance was assessed across four datasets, including\ntwo that we publicly release as open resources. The model achieved strong\nperformance for identifying well-documented categories such as living\ncondition, marital status, descendants, job, tobacco, and alcohol use (F1 score\n> 0.80). Performance was lower for categories with limited training data or\nhighly variable expressions, such as employment status, housing, physical\nactivity, income, and education. Our model identified 95.8% of patients with at\nleast one SDoH, compared to 2.8% for ICD-10 codes from structured EHR data. Our\nerror analysis showed that performance limitations were linked to annotation\ninconsistencies, reliance on English-centric tokenizer, and reduced\ngeneralizability due to the model being trained on social history sections\nonly. These results demonstrate the effectiveness of NLP in improving the\ncompleteness of real-world SDoH data in a non-English EHR system.", "AI": {"tldr": "This paper proposes a method using large language models to extract social determinants of health (SDoH) from French clinical notes, improving data completeness in electronic health records (EHRs).", "motivation": "To address the incomplete documentation of social determinants of health (SDoH) in electronic health records (EHRs) that affect health outcomes.", "method": "The study trained the Flan-T5-Large model on annotated social history sections from clinical notes, evaluated model performance for identifying SDoH categories and extracting detailed information across multiple datasets.", "result": "The model showed strong performance, accurately identifying 95.8% of patients with at least one SDoH compared to just 2.8% using structured ICD-10 codes, especially excelling in well-documented categories.", "conclusion": "NLP techniques can significantly enhance the completeness of SDoH data in non-English electronic health record systems, despite some performance limitations due to training data quality and model focus.", "key_contributions": ["Method for extracting SDoH using large language models from French EHRs", "Public release of two datasets for further research", "Demonstration of NLP application in health informatics for SDoH documentation"], "limitations": "Performance inconsistencies due to annotation issues, reliance on English-centric tokenization, and limited generalizability from training on specific sections only.", "keywords": ["social determinants of health", "large language models", "electronic health records", "NLP", "health informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2409.18162", "pdf": "https://arxiv.org/pdf/2409.18162.pdf", "abs": "https://arxiv.org/abs/2409.18162", "title": "The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing Learning and Social Interaction for Children with Autism Spectrum Disorders: A Systematic Review", "authors": ["Biplov Paneru", "Bishwash Paneru"], "categories": ["cs.HC", "cs.AI", "cs.SI"], "comment": "none", "summary": "The emergence of large language models (LLMs), augmented reality (AR), and\nuser interface/user experience (UI/UX) design in therapies for children,\nespecially with disorders like autism spectrum disorder (ASD), is studied in\ndetail in this review study. 150 publications were collected by a thorough\nliterature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google\nScholar; 60 of them were chosen based on their methodological rigor and\nrelevance to the focus area. Three of the primary areas are studied and covered\nin this review: how AR can improve social and learning results, how LLMs can\nsupport communication, and how UI/UX design affects how effective these\ntechnologies can be. Results show that while LLMs can provide individualized\nlearning and communication support, AR has shown promise in enhancing social\nskills, motivation, and attention. For children with ASD, accessible and\nengaging interventions rely heavily on effective UI/UX design, but there is\nstill a significant lack of robotics-based education and therapeutic programs\nspecifically tailored for autistic children. To optimize the benefits of these\ntechnologies in ASD therapies and immersive education, the study emphasizes the\nneed for additional research to address difficulties related to customization,\naccessibility, and integration.", "AI": {"tldr": "This review explores the use of large language models, augmented reality, and user interface design in therapies for children with autism, emphasizing their effectiveness and the need for further research in customization and accessibility.", "motivation": "To investigate how LLMs, AR, and UI/UX can be utilized in therapeutic applications for children with autism spectrum disorder (ASD).", "method": "A comprehensive literature search was conducted across multiple databases, collecting 150 publications, from which 60 relevant and methodologically rigorous studies were selected for review.", "result": "The study found that LLMs support individualized learning and communication, AR enhances social skills, motivation, and attention, and effective UI/UX design is crucial for engagement.", "conclusion": "While significant potential exists in using LLMs and AR for ASD therapies, there is a pressing need for better robotics-based educational programs tailored to the needs of autistic children and more research on customization and integration.", "key_contributions": ["Analysis of AR's impact on social and learning outcomes for ASD children", "Evaluation of LLMs in enhancing communication for autistic individuals", "Identification of UI/UX design's role in the effectiveness of therapeutic technologies"], "limitations": "Significant lack of robotics-based education and therapeutic programs specifically designed for autistic children.", "keywords": ["large language models", "augmented reality", "user experience", "autism spectrum disorder", "therapeutic interventions"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03473", "pdf": "https://arxiv.org/pdf/2507.03473.pdf", "abs": "https://arxiv.org/abs/2507.03473", "title": "Beyond Weaponization: NLP Security for Medium and Lower-Resourced Languages in Their Own Right", "authors": ["Heather Lent"], "categories": ["cs.CL", "cs.AI"], "comment": "Pre-print", "summary": "Despite mounting evidence that multilinguality can be easily weaponized\nagainst language models (LMs), works across NLP Security remain overwhelmingly\nEnglish-centric. In terms of securing LMs, the NLP norm of \"English first\"\ncollides with standard procedure in cybersecurity, whereby practitioners are\nexpected to anticipate and prepare for worst-case outcomes. To mitigate\nworst-case outcomes in NLP Security, researchers must be willing to engage with\nthe weakest links in LM security: lower-resourced languages. Accordingly, this\nwork examines the security of LMs for lower- and medium-resourced languages. We\nextend existing adversarial attacks for up to 70 languages to evaluate the\nsecurity of monolingual and multilingual LMs for these languages. Through our\nanalysis, we find that monolingual models are often too small in total number\nof parameters to ensure sound security, and that while multilinguality is\nhelpful, it does not always guarantee improved security either. Ultimately,\nthese findings highlight important considerations for more secure deployment of\nLMs, for communities of lower-resourced languages.", "AI": {"tldr": "This paper assesses the security of language models (LMs) in lower- and medium-resourced languages, revealing vulnerabilities in both monolingual and multilingual models.", "motivation": "The aim is to address the security concerns of language models, particularly for underrepresented languages, countering the English-centric focus prevalent in NLP security.", "method": "The authors extend existing adversarial attacks to evaluate the security of monolingual and multilingual LMs across 70 languages.", "result": "Findings indicate that monolingual models often lack sufficient parameters for robust security, and multilinguality does not guarantee improved security results.", "conclusion": "The study underscores the necessity for better LM security practices for languages with fewer resources to enable secure deployments.", "key_contributions": ["Extension of adversarial attacks to 70 languages", "Evaluation of security in lower-resourced LMs", "Insight into the limitations of multilingual models in ensuring security"], "limitations": "Focused primarily on lower- and medium-resourced languages; findings may not apply to highly-resourced languages.", "keywords": ["NLP Security", "Multilinguality", "Lower-resourced Languages", "Adversarial Attacks", "Language Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2409.19281", "pdf": "https://arxiv.org/pdf/2409.19281.pdf", "abs": "https://arxiv.org/abs/2409.19281", "title": "Gesture Recognition for Feedback Based Mixed Reality and Robotic Fabrication: A Case Study of the UnLog Tower", "authors": ["Alexander Htet Kyaw", "Lawson Spencer", "Sasa Zivkovic", "Leslie Lok"], "categories": ["cs.HC", "cs.ET", "cs.RO"], "comment": "16 pages, 16 figures. Published in the Proceedings of the\n  International Conference on Computational Design and Robotic Fabrication\n  (CDRF) 2023", "summary": "Mixed Reality (MR) platforms enable users to interact with three-dimensional\nholographic instructions during the assembly and fabrication of highly custom\nand parametric architectural constructions without the necessity of\ntwo-dimensional drawings. Previous MR fabrication projects have primarily\nrelied on digital menus and custom buttons as the interface for user\ninteraction with the MR environment. Despite this approach being widely\nadopted, it is limited in its ability to allow for direct human interaction\nwith physical objects to modify fabrication instructions within the MR\nenvironment. This research integrates user interactions with physical objects\nthrough real-time gesture recognition as input to modify, update or generate\nnew digital information enabling reciprocal stimuli between the physical and\nthe virtual environment. Consequently, the digital environment is generative of\nthe user's provided interaction with physical objects to allow seamless\nfeedback in the fabrication process. This research investigates gesture\nrecognition for feedback-based MR workflows for robotic fabrication, human\nassembly, and quality control in the construction of the UnLog Tower.", "AI": {"tldr": "This paper explores the use of gesture recognition in Mixed Reality (MR) to improve interaction with physical objects during architectural fabrication processes.", "motivation": "The need for improved user interaction in Mixed Reality environments for architectural assembly, beyond basic digital menus and custom buttons.", "method": "The research integrates real-time gesture recognition with Mixed Reality platforms to enable users to modify digital fabrication instructions through direct interaction with physical objects.", "result": "The approach allows for seamless feedback and enables users to generate new digital information, enhancing the fabrication process in MR workflows.", "conclusion": "The findings suggest that integrating gesture recognition can significantly improve feedback and interaction in MR environments for robotic fabrication and quality control.", "key_contributions": ["Integration of real-time gesture recognition for user interaction in MR environments.", "Enhanced feedback mechanisms between physical and digital realms during fabrication processes.", "Application of MR gesture recognition to robotic fabrication and assembly workflows."], "limitations": "", "keywords": ["Mixed Reality", "Gesture Recognition", "Robotic Fabrication", "Architectural Assembly", "User Interaction"], "importance_score": 6, "read_time_minutes": 16}}
{"id": "2507.03483", "pdf": "https://arxiv.org/pdf/2507.03483.pdf", "abs": "https://arxiv.org/abs/2507.03483", "title": "BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning Dataset", "authors": ["Zhiheng Xi", "Guanyu Li", "Yutao Fan", "Honglin Guo", "Yufang Liu", "Xiaoran Fan", "Jiaqi Liu", "Jingchao Ding", "Wangmeng Zuo", "Zhenfei Yin", "Lei Bai", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "In this paper, we introduce BMMR, a large-scale bilingual, multimodal,\nmulti-disciplinary reasoning dataset for the community to develop and evaluate\nlarge multimodal models (LMMs). BMMR comprises 110k college-level questions\nspanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice,\nfill-in-the-blank, and open-ended QA-and sourced from both print and digital\nmedia such as books, exams, and quizzes. All data are curated and filtered via\na human-in-the-loop and scalable framework, and each instance is paired with a\nhigh-quality reasoning path. The dataset is organized into two parts: BMMR-Eval\nthat comprises 20,458 high-quality instances to comprehensively assess LMMs'\nknowledge and reasoning across multiple disciplines in both Chinese and\nEnglish; and BMMR-Train that contains 88,991 instances to support further\nresearch and development, extending the current focus on mathematical reasoning\nto diverse disciplines and domains. In addition, we propose the process-based\nmulti-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained\nevaluation of reasoning paths. Extensive experiments on 24 models reveal that\n(i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom\non BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs\nonly on specific subjects; (iii) open-source models still trail their\nproprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap.\nAdditionally, we conduct reasoning-chain analyses using BMMR-Verifier and other\nin-depth studies, uncovering the challenges LMMs currently face in\nmultidisciplinary reasoning. We will release the data, and we hope our work can\noffer insights and contributions to the community.", "AI": {"tldr": "Introduction of a large-scale bilingual, multimodal reasoning dataset (BMMR) to develop and evaluate large multimodal models (LMMs).", "motivation": "The need for comprehensive datasets to improve the evaluation and development of LMMs in various disciplines.", "method": "BMMR is curated from print and digital media, structured into BMMR-Eval and BMMR-Train datasets with human-verified reasoning paths.", "result": "Extensive experiments show SOTA models have substantial room for improvement on BMMR-Eval; discipline bias was noted in reasoning performances.", "conclusion": "BMMR and BMMR-Verifier aim to enhance research on multidisciplinary reasoning in LMMs and provide insights into existing challenges.", "key_contributions": ["Introduction of BMMR, a large-scale, bilingual reasoning dataset.", "Development of BMMR-Verifier for precise evaluation of reasoning paths.", "Insights on reasoning model performance and gaps compared to LMMs."], "limitations": "Dataset's focus might not cover all potential reasoning paths or disciplines comprehensively.", "keywords": ["multimodal models", "bilingual dataset", "reasoning evaluation", "machine learning", "multidisciplinary research"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.11273", "pdf": "https://arxiv.org/pdf/2502.11273.pdf", "abs": "https://arxiv.org/abs/2502.11273", "title": "FairFare: A Tool for Crowdsourcing Rideshare Data to Empower Labor Organizers", "authors": ["Dana Calacci", "Varun Nagaraj Rao", "Samantha Dalal", "Catherine Di", "Kok-Wei Pua", "Andrew Schwartz", "Danny Spitzberg", "Andrés Monroy-Hernández"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "FairFare is hosted at: https://getfairfare.org/", "summary": "Rideshare workers experience unpredictable working conditions due to gig work\nplatforms' reliance on opaque AI and algorithmic systems. In response to these\nchallenges, we found that labor organizers want data to help them advocate for\nlegislation to increase the transparency and accountability of these platforms.\nTo address this need, we collaborated with a Colorado-based rideshare union to\ndevelop FairFare, a tool that crowdsources and analyzes workers' data to\nestimate the take rate -- the percentage of the rider price retained by the\nrideshare platform. We deployed FairFare with our partner organization that\ncollaborated with us in collecting data on 76,000+ trips from 45 drivers over\n18 months. During evaluation interviews, organizers reported that FairFare\nhelped influence the bill language and passage of Colorado Senate Bill 24-75,\ncalling for greater transparency and data disclosure of platform operations,\nand create a national narrative. Finally, we reflect on complexities of\ntranslating quantitative data into policy outcomes, nature of community based\naudits, and design implications for future transparency tools.", "AI": {"tldr": "FairFare is a tool developed to help labor organizers advocate for transparency in rideshare platform operations by analyzing crowdsourced data on earnings.", "motivation": "Rideshare workers face unpredictable working conditions due to the opaque systems of gig work platforms, necessitating tools that can provide data for advocacy.", "method": "Collaborated with a rideshare union to create FairFare, a crowdsourced tool that analyzes trip data to estimate the rideshare platforms' take rates.", "result": "FairFare collected data from over 76,000 trips and was instrumental in influencing the passage of Colorado Senate Bill 24-75, which aims for greater transparency in platform operations.", "conclusion": "The implementation of FairFare highlights the potential of community-sourced data in shaping policy and identifies challenges in translating quantitative findings into legislative action.", "key_contributions": ["Development of FairFare as a tool for transparency in gig work", "Visualization of the algorithm's impact on labor conditions", "Demonstrated the influence of worker-collected data on policy outcomes"], "limitations": "The complexities in translating numerical data into clear policy recommendations and the challenges in community-based audits.", "keywords": ["rideshare", "transparency", "data advocacy", "community audits", "algorithmic accountability"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.03488", "pdf": "https://arxiv.org/pdf/2507.03488.pdf", "abs": "https://arxiv.org/abs/2507.03488", "title": "Four Shades of Life Sciences: A Dataset for Disinformation Detection in the Life Sciences", "authors": ["Eva Seidlmayer", "Lukas Galke", "Konrad U. Förstner"], "categories": ["cs.CL"], "comment": "30 pages, 5 figures", "summary": "Disseminators of disinformation often seek to attract attention or evoke\nemotions - typically to gain influence or generate revenue - resulting in\ndistinctive rhetorical patterns that can be exploited by machine learning\nmodels. In this study, we explore linguistic and rhetorical features as proxies\nfor distinguishing disinformative texts from other health and life-science text\ngenres, applying both large language models and classical machine learning\nclassifiers. Given the limitations of existing datasets, which mainly focus on\nfact checking misinformation, we introduce Four Shades of Life Sciences\n(FSoLS): a novel, labeled corpus of 2,603 texts on 14 life-science topics,\nretrieved from 17 diverse sources and classified into four categories of life\nscience publications. The source code for replicating, and updating the dataset\nis available on GitHub:\nhttps://github.com/EvaSeidlmayer/FourShadesofLifeSciences", "AI": {"tldr": "This study introduces a dataset and methodology to distinguish disinformative texts in health sciences using linguistic features and machine learning models.", "motivation": "To address the challenge of disinformation in health and life sciences that misuse rhetorical patterns to attract attention and influence.", "method": "The study employs linguistic and rhetorical features as proxies and applies both large language models and classical machine learning classifiers on a newly introduced dataset.", "result": "The introduction of the Four Shades of Life Sciences (FSoLS) dataset, comprising 2,603 labeled texts across 14 life-science topics, shows distinct patterns that differentiate disinformative texts from legitimate publications.", "conclusion": "The dataset and methodologies can aid in improving the detection of disinformation in health-related texts and are publicly available for further research.", "key_contributions": ["Introduction of the Four Shades of Life Sciences (FSoLS) corpus", "Application of both classical and modern machine learning techniques to health informatics", "Analysis of linguistic features in disinformative texts"], "limitations": "The study is primarily focused on the health and life sciences domain, which may limit its generalizability to other fields.", "keywords": ["disinformation", "health informatics", "machine learning", "linguistic features", "large language models"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2503.16492", "pdf": "https://arxiv.org/pdf/2503.16492.pdf", "abs": "https://arxiv.org/abs/2503.16492", "title": "FAM-HRI: Foundation-Model Assisted Multi-Modal Human-Robot Interaction Combining Gaze and Speech", "authors": ["Yuzhi Lai", "Shenghai Yuan", "Boya Zhang", "Benjamin Kiefer", "Peizheng Li", "Tianchen Deng", "Andreas Zell"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Effective Human-Robot Interaction (HRI) is crucial for enhancing\naccessibility and usability in real-world robotics applications. However,\nexisting solutions often rely on gestures or language commands, making\ninteraction inefficient and ambiguous, particularly for users with physical\nimpairments. In this paper, we introduce FAM-HRI, an efficient multi-modal\nframework for human-robot interaction that integrates language and gaze inputs\nvia foundation models. By leveraging lightweight Meta ARIA glasses, our system\ncaptures real-time multi-modal signals and utilizes large language models\n(LLMs) to fuse user intention with scene context, enabling intuitive and\nprecise robot manipulation. Our method accurately determines gaze fixation time\ninterval, reducing noise caused by the gaze dynamic nature. Experimental\nevaluations demonstrate that FAM-HRI achieves a high success rate in task\nexecution while maintaining a low interaction time, providing a practical\nsolution for individuals with limited physical mobility or motor impairments.", "AI": {"tldr": "FAM-HRI is a multi-modal framework enhancing Human-Robot Interaction using language and gaze inputs to assist users with physical impairments.", "motivation": "To improve accessibility and usability in robotics by addressing inefficiencies in gesture and command-based interactions, especially for users with physical impairments.", "method": "FAM-HRI integrates language and gaze inputs through foundation models and Meta ARIA glasses, capturing real-time signals and fusing user intention with scene context.", "result": "FAM-HRI achieves high task execution success rates while reducing interaction time, offering effective support for users with limited mobility.", "conclusion": "The framework presents a practical solution for improving Human-Robot Interaction for individuals with motor impairments.", "key_contributions": ["Introduction of a multi-modal framework for HRI using language and gaze inputs.", "Real-time integration of user intention and scene context via large language models.", "Demonstrated high success rates in task execution with reduced interaction times."], "limitations": "", "keywords": ["Human-Robot Interaction", "Multi-modal framework", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.03493", "pdf": "https://arxiv.org/pdf/2507.03493.pdf", "abs": "https://arxiv.org/abs/2507.03493", "title": "AI-VaxGuide: An Agentic RAG-Based LLM for Vaccination Decisions", "authors": ["Abdellah Zeggai", "Ilyes Traikia", "Abdelhak Lakehal", "Abdennour Boulesnane"], "categories": ["cs.CL"], "comment": null, "summary": "Vaccination plays a vital role in global public health, yet healthcare\nprofessionals often struggle to access immunization guidelines quickly and\nefficiently. National protocols and WHO recommendations are typically extensive\nand complex, making it difficult to extract precise information, especially\nduring urgent situations. This project tackles that issue by developing a\nmultilingual, intelligent question-answering system that transforms static\nvaccination guidelines into an interactive and user-friendly knowledge base.\nBuilt on a Retrieval-Augmented Generation (RAG) framework and enhanced with\nagent-based reasoning (Agentic RAG), the system provides accurate,\ncontext-sensitive answers to complex medical queries. Evaluation shows that\nAgentic RAG outperforms traditional methods, particularly in addressing\nmulti-step or ambiguous questions. To support clinical use, the system is\nintegrated into a mobile application designed for real-time, point-of-care\naccess to essential vaccine information. AI-VaxGuide model is publicly\navailable on https://huggingface.co/VaxGuide", "AI": {"tldr": "A multilingual intelligent question-answering system for vaccination guidelines that enhances access and accuracy during urgent situations.", "motivation": "Healthcare professionals need quick access to vaccination guidelines, which are often complex and lengthy, especially in urgent scenarios.", "method": "The system employs a Retrieval-Augmented Generation (RAG) framework combined with agent-based reasoning (Agentic RAG) to provide context-sensitive answers to medical queries.", "result": "Evaluation shows that Agentic RAG significantly outperforms traditional methods in handling multi-step and ambiguous questions.", "conclusion": "The developed mobile application facilitates real-time access to vaccination information for clinical use and is designed to enhance decision-making in urgent health situations.", "key_contributions": ["Development of a multilingual Q&A system for vaccination guidelines", "Integration of Agentic RAG for improved answer accuracy", "Mobile application for real-time point-of-care access"], "limitations": "", "keywords": ["vaccination", "question-answering", "semantic technology", "healthcare", "RAG"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.03543", "pdf": "https://arxiv.org/pdf/2507.03543.pdf", "abs": "https://arxiv.org/abs/2507.03543", "title": "H2HTalk: Evaluating Large Language Models as Emotional Companion", "authors": ["Boyang Wang", "Yalun Wu", "Hongcheng Guo", "Zhoujun Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As digital emotional support needs grow, Large Language Model companions\noffer promising authentic, always-available empathy, though rigorous evaluation\nlags behind model advancement. We present Heart-to-Heart Talk (H2HTalk), a\nbenchmark assessing companions across personality development and empathetic\ninteraction, balancing emotional intelligence with linguistic fluency. H2HTalk\nfeatures 4,650 curated scenarios spanning dialogue, recollection, and itinerary\nplanning that mirror real-world support conversations, substantially exceeding\nprevious datasets in scale and diversity. We incorporate a Secure Attachment\nPersona (SAP) module implementing attachment-theory principles for safer\ninteractions. Benchmarking 50 LLMs with our unified protocol reveals that\nlong-horizon planning and memory retention remain key challenges, with models\nstruggling when user needs are implicit or evolve mid-conversation. H2HTalk\nestablishes the first comprehensive benchmark for emotionally intelligent\ncompanions. We release all materials to advance development of LLMs capable of\nproviding meaningful and safe psychological support.", "AI": {"tldr": "H2HTalk is a benchmark for evaluating Large Language Model companions on emotional intelligence and empathy, featuring diverse scenarios and incorporating attachment theory.", "motivation": "To address the growing need for digital emotional support and improve the evaluation of LLM companions.", "method": "A benchmark, H2HTalk, was created featuring 4,650 scenarios that assess various aspects of empathetic interaction and personality development in LLMs.", "result": "Benchmarking 50 LLMs revealed challenges in long-horizon planning and memory retention, especially with implicit user needs.", "conclusion": "H2HTalk provides a comprehensive benchmark essential for developing emotionally intelligent LLMs capable of safe psychological support.", "key_contributions": ["Creation of H2HTalk benchmark for LLM companions", "Incorporation of Secure Attachment Persona module", "Assessment of LLMs across diverse empathic scenarios"], "limitations": "Challenges in evaluating long-horizon planning and evolving user needs.", "keywords": ["Large Language Models", "emotional support", "benchmark", "empathy", "attachment theory"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.03576", "pdf": "https://arxiv.org/pdf/2507.03576.pdf", "abs": "https://arxiv.org/abs/2507.03576", "title": "Articulatory clarity and variability before and after surgery for tongue cancer", "authors": ["Thomas Tienkamp", "Fleur van Ast", "Roos van der Veen", "Teja Rebernik", "Raoul Buurke", "Nikki Hoekzema", "Katharina Polsterer", "Hedwig Sekeres", "Rob van Son", "Martijn Wieling", "Max Witjes", "Sebastiaan de Visscher", "Defne Abur"], "categories": ["cs.CL"], "comment": null, "summary": "Surgical treatment for tongue cancer can negatively affect the mobility and\nmusculature of the tongue, which can influence articulatory clarity and\nvariability. In this study, we investigated articulatory clarity through the\nvowel articulation index (VAI) and variability through vowel formant dispersion\n(VFD). Using a sentence reading task, we assessed 11 individuals pre and six\nmonths post tongue cancer surgery, alongside 11 sex- and age matched typical\nspeakers. Our results show that while the VAI was significantly smaller\npost-surgery compared to pre-surgery, there was no significant difference\nbetween patients and typical speakers at either time point. Post-surgery,\nspeakers had higher VFD values for /i/ compared to pre-surgery and typical\nspeakers, signalling higher variability. Taken together, our results suggest\nthat while articulatory clarity remained within typical ranges following\nsurgery for tongue cancer for the speakers in our study, articulatory\nvariability increased.", "AI": {"tldr": "This study examines the impact of tongue cancer surgery on vowel articulation clarity and variability using the Vowel Articulation Index (VAI) and Vowel Formant Dispersion (VFD).", "motivation": "To understand how surgical treatment for tongue cancer affects the articulation and variability of speech in patients.", "method": "The study assessed 11 tongue cancer patients before and six months after surgery using a sentence reading task, comparing their results with 11 age- and sex-matched typical speakers.", "result": "Post-surgery, Vowel Articulation Index (VAI) scores were significantly lower compared to pre-surgery scores, but no significant differences were found between patients and typical speakers. Vowel Formant Dispersion (VFD) values for /i/ increased post-surgery, indicating greater articulatory variability.", "conclusion": "Articulatory clarity in tongue cancer patients remained within typical ranges post-surgery, but articulatory variability increased following surgery.", "key_contributions": ["Investigates the impact of tongue cancer surgery on speech articulation", "Uses VAI and VFD as measures of clarity and variability", "Compares post-surgical patients with typical speakers for insights on recovery."], "limitations": "Study includes a small sample size and a specific population which may limit generalizability.", "keywords": ["tongue cancer", "articulatory clarity", "vowel articulation index", "vowel formant dispersion", "speech variability"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.03580", "pdf": "https://arxiv.org/pdf/2507.03580.pdf", "abs": "https://arxiv.org/abs/2507.03580", "title": "Learning to Translate Ambiguous Terminology by Preference Optimization on Post-Edits", "authors": ["Nathaniel Berger", "Johannes Eschbach-Dymanus", "Miriam Exel", "Matthias Huck", "Stefan Riezler"], "categories": ["cs.CL"], "comment": null, "summary": "In real world translation scenarios, terminology is rarely one-to-one.\nInstead, multiple valid translations may appear in a terminology dictionary,\nbut correctness of a translation depends on corporate style guides and context.\nThis can be challenging for neural machine translation (NMT) systems. Luckily,\nin a corporate context, many examples of human post-edits of valid but\nincorrect terminology exist. The goal of this work is to learn how to\ndisambiguate our terminology based on these corrections. Our approach is based\non preference optimization, using the term post-edit as the knowledge to be\npreferred. While previous work had to rely on unambiguous translation\ndictionaries to set hard constraints during decoding, or to add soft\nconstraints in the input, our framework requires neither one-to-one\ndictionaries nor human intervention at decoding time. We report results on\nEnglish-German post-edited data and find that the optimal combination of\nsupervised fine-tuning and preference optimization, with both term-specific and\nfull sequence objectives, yields statistically significant improvements in term\naccuracy over a strong NMT baseline without significant losses in COMET score.\nAdditionally, we release test sets from our post-edited data and terminology\ndictionary.", "AI": {"tldr": "This paper presents a framework for improving terminology disambiguation in neural machine translation (NMT) systems using preference optimization based on human post-edits.", "motivation": "Terminology in translations is often not one-to-one, leading to challenges for NMT systems due to context-dependent correctness.", "method": "The approach utilizes preference optimization, learning from human post-edited translations without needing hard constraints or human intervention during decoding.", "result": "The framework shows significant improvements in term accuracy over a strong NMT baseline and maintains performance in terms of COMET scores.", "conclusion": "The combination of supervised fine-tuning and preference optimization yields better performance, and test sets are released for further research.", "key_contributions": ["Developed a preference optimization framework for terminology disambiguation in NMT", "Achieved significant term accuracy improvements without one-to-one dictionaries", "Released test sets for post-edited data and terminology dictionaries"], "limitations": "", "keywords": ["neural machine translation", "preference optimization", "terminology disambiguation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.03612", "pdf": "https://arxiv.org/pdf/2507.03612.pdf", "abs": "https://arxiv.org/abs/2507.03612", "title": "Multi-Hop Reasoning for Question Answering with Hyperbolic Representations", "authors": ["Simon Welz", "Lucie Flek", "Akbar Karimi"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Hyperbolic representations are effective in modeling knowledge graph data\nwhich is prevalently used to facilitate multi-hop reasoning. However, a\nrigorous and detailed comparison of the two spaces for this task is lacking. In\nthis paper, through a simple integration of hyperbolic representations with an\nencoder-decoder model, we perform a controlled and comprehensive set of\nexperiments to compare the capacity of hyperbolic space versus Euclidean space\nin multi-hop reasoning. Our results show that the former consistently\noutperforms the latter across a diverse set of datasets. In addition, through\nan ablation study, we show that a learnable curvature initialized with the\ndelta hyperbolicity of the utilized data yields superior results to random\ninitializations. Furthermore, our findings suggest that hyperbolic\nrepresentations can be significantly more advantageous when the datasets\nexhibit a more hierarchical structure.", "AI": {"tldr": "This paper compares the effectiveness of hyperbolic representations versus Euclidean space for multi-hop reasoning in knowledge graphs, showing that hyperbolic space consistently outperforms Euclidean space across various datasets.", "motivation": "The need for a rigorous comparison of hyperbolic and Euclidean spaces for multi-hop reasoning in knowledge graphs.", "method": "A combination of hyperbolic representations with an encoder-decoder model, followed by controlled experiments and an ablation study.", "result": "Hyperbolic representations consistently outperform Euclidean representations across diverse datasets, especially when datasets have a hierarchical structure.", "conclusion": "Hyperbolic representations are more effective for multi-hop reasoning in specific types of knowledge graph datasets.", "key_contributions": ["Comprehensive comparison of hyperbolic and Euclidean spaces for multi-hop reasoning", "Demonstration of the benefits of learnable curvature initialization", "Insights into the hierarchical advantages of hyperbolic representations"], "limitations": "", "keywords": ["hyperbolic representations", "multi-hop reasoning", "knowledge graphs"], "importance_score": 6, "read_time_minutes": 7}}
{"id": "2507.03617", "pdf": "https://arxiv.org/pdf/2507.03617.pdf", "abs": "https://arxiv.org/abs/2507.03617", "title": "EMERGE: A Benchmark for Updating Knowledge Graphs with Emerging Textual Knowledge", "authors": ["Klim Zaporojets", "Daniel Daza", "Edoardo Barba", "Ira Assent", "Roberto Navigli", "Paul Groth"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Graphs (KGs) are structured knowledge repositories containing\nentities and relations between them. In this paper, we investigate the problem\nof automatically updating KGs over time with respect to the evolution of\nknowledge in unstructured textual sources. This problem requires identifying a\nwide range of update operations based on the state of an existing KG at a\nspecific point in time. This contrasts with traditional information extraction\npipelines, which extract knowledge from text independently of the current state\nof a KG. To address this challenge, we propose a method for lifelong\nconstruction of a dataset consisting of Wikidata KG snapshots over time and\nWikipedia passages paired with the corresponding edit operations that they\ninduce in a particular KG snapshot. The resulting dataset comprises 376K\nWikipedia passages aligned with a total of 1.25M KG edits over 10 different\nsnapshots of Wikidata from 2019 to 2025. Our experimental results highlight\nchallenges in updating KG snapshots based on emerging textual knowledge,\npositioning the dataset as a valuable benchmark for future research. We will\npublicly release our dataset and model implementations.", "AI": {"tldr": "The paper presents a method for automatically updating Knowledge Graphs (KGs) using new knowledge from unstructured textual sources, focusing on a dataset of Wikidata snapshots and corresponding Wikipedia passages that induce KG edits.", "motivation": "To enhance the process of updating Knowledge Graphs as new information evolves in unstructured textual data, contrasting with traditional extraction methods that operate independently.", "method": "The authors constructed a dataset of Wikidata KG snapshots over time, pairing Wikipedia passages with the corresponding edit operations that they induce on the KG, resulting in a comprehensive dataset for analysis.", "result": "The dataset includes 376K Wikipedia passages aligned with 1.25M KG edits over 10 snapshots of Wikidata, revealing challenges in leveraging emerging textual knowledge for KG updates.", "conclusion": "The dataset serves as a benchmark for future research in knowledge graph updates and will be made publicly available along with model implementations.", "key_contributions": ["Method for lifelong KG update based on textual sources", "Creation of a large dataset of Wikidata snapshots and corresponding edits", "Highlighting challenges in updating KGs with new knowledge"], "limitations": "", "keywords": ["Knowledge Graphs", "Wikidata", "Dataset", "Textual Knowledge", "Automatic Updates"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.03641", "pdf": "https://arxiv.org/pdf/2507.03641.pdf", "abs": "https://arxiv.org/abs/2507.03641", "title": "Improving Low-Resource Dialect Classification Using Retrieval-based Voice Conversion", "authors": ["Lea Fischbach", "Akbar Karimi", "Caroline Kleen", "Alfred Lameli", "Lucie Flek"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Deep learning models for dialect identification are often limited by the\nscarcity of dialectal data. To address this challenge, we propose to use\nRetrieval-based Voice Conversion (RVC) as an effective data augmentation method\nfor a low-resource German dialect classification task. By converting audio\nsamples to a uniform target speaker, RVC minimizes speaker-related variability,\nenabling models to focus on dialect-specific linguistic and phonetic features.\nOur experiments demonstrate that RVC enhances classification performance when\nutilized as a standalone augmentation method. Furthermore, combining RVC with\nother augmentation methods such as frequency masking and segment removal leads\nto additional performance gains, highlighting its potential for improving\ndialect classification in low-resource scenarios.", "AI": {"tldr": "This paper proposes Retrieval-based Voice Conversion (RVC) as a method to augment data for dialect identification in low-resource scenarios, leading to improved classification performance.", "motivation": "To improve dialect identification models limited by the scarcity of dialectal data.", "method": "RVC converts audio samples to a uniform target speaker, reducing speaker variability and enabling models to focus on dialect-specific features. It is combined with other methods for further enhancement.", "result": "RVC alone improves classification performance, and its combination with frequency masking and segment removal yields additional gains.", "conclusion": "RVC is a promising approach for enhancing dialect classification in scenarios with limited data.", "key_contributions": ["Introduction of RVC for audio data augmentation in dialect classification", "Demonstrated performance gains with RVC alone and in combination with other methods", "Provided insights into dealing with low-resource dialectal data"], "limitations": "", "keywords": ["dialect identification", "data augmentation", "Retrieval-based Voice Conversion", "low-resource", "deep learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.03648", "pdf": "https://arxiv.org/pdf/2507.03648.pdf", "abs": "https://arxiv.org/abs/2507.03648", "title": "Disentangling the Roles of Representation and Selection in Data Pruning", "authors": ["Yupei Du", "Yingjin Song", "Hugh Mee Wong", "Daniil Ignatev", "Albert Gatt", "Dong Nguyen"], "categories": ["cs.CL", "cs.LG"], "comment": "ACL 2025", "summary": "Data pruning, selecting small but impactful subsets, offers a promising way\nto efficiently scale NLP model training. However, existing methods often\ninvolve many different design choices, which have not been systematically\nstudied. This limits future developments. In this work, we decompose data\npruning into two key components: the data representation and the selection\nalgorithm, and we systematically analyze their influence on the selection of\ninstances. Our theoretical and empirical results highlight the crucial role of\nrepresentations: better representations, e.g., training gradients, generally\nlead to a better selection of instances, regardless of the chosen selection\nalgorithm. Furthermore, different selection algorithms excel in different\nsettings, and none consistently outperforms the others. Moreover, the selection\nalgorithms do not always align with their intended objectives: for example,\nalgorithms designed for the same objective can select drastically different\ninstances, highlighting the need for careful evaluation.", "AI": {"tldr": "This paper analyzes data pruning in NLP model training, focusing on the impact of data representations and selection algorithms.", "motivation": "To address the lack of systematic study on data pruning methods in NLP, which limits future developments in the field.", "method": "The paper decomposes data pruning into data representation and selection algorithm components and analyzes their influence on instance selection through theoretical and empirical results.", "result": "Better data representations, such as training gradients, improve the selection of instances, while different selection algorithms perform better in various scenarios without a consistent overall winner.", "conclusion": "Careful evaluation of selection algorithms is necessary as their performance does not always align with their intended objectives, indicating that improvements can be achieved in data pruning strategies for NLP.", "key_contributions": ["Systematic decomposition of data pruning into two components", "Theoretical analysis of the impact of data representations", "Empirical evaluation showing variance in algorithm performance across settings"], "limitations": "", "keywords": ["data pruning", "NLP", "representation", "selection algorithms", "machine learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.03668", "pdf": "https://arxiv.org/pdf/2507.03668.pdf", "abs": "https://arxiv.org/abs/2507.03668", "title": "TRACE: Training and Inference-Time Interpretability Analysis for Language Models", "authors": ["Nura Aljaafari", "Danilo S. Carvalho", "André Freitas"], "categories": ["cs.CL"], "comment": "14 pages", "summary": "Understanding when and how linguistic knowledge emerges during language model\ntraining remains a central challenge for interpretability. Most existing tools\nare post hoc, rely on scalar metrics, or require nontrivial integration effort,\nmaking comprehensive interpretability analysis difficult to deploy and\nmaintain. We introduce TRACE, a modular toolkit for training and inference-time\ninterpretability analysis of transformer models. It enables lightweight,\nin-training analysis of linguistic and representational signals, including\nfeatures probing, intrinsic dimensionality, Hessian curvature, and output\ndiagnostics. It integrates with ABSynth, a controllable synthetic corpus\ngenerator that provides structured annotations for precise evaluation of\nlinguistic feature acquisition. Experiments with autoregressive transformers\ndemonstrate that TRACE reveals developmental phenomena such as early syntactic\nemergence, delayed semantic acquisition, and representational compression,\nsignals overlooked by traditional scalar metrics such as loss or accuracy. With\nminimal integration effort, the tool enables layer-wise diagnostics,\nconvergence-based early stopping, and detection of structural errors, making\ntransformer analysis interpretable, actionable, and reproducible.", "AI": {"tldr": "TRACE is a toolkit for training and inference-time interpretability analysis of transformer models, allowing in-training analysis of linguistic signals and integrating with ABSynth for structured evaluation.", "motivation": "To address the challenge of interpretability in language model training and provide a comprehensive analysis without the difficulties of existing tools.", "method": "TRACE enables lightweight analysis of linguistic and representational signals, including probing features, assessing intrinsic dimensionality, and output diagnostics while requiring minimal integration effort.", "result": "Experiments show TRACE reveals previously overlooked developmental phenomena in transformer models, such as early syntactic emergence and delayed semantic acquisition.", "conclusion": "TRACE improves transformer analysis by providing actionable insights and diagnostics that are interpretable and reproducible.", "key_contributions": ["Modular toolkit for in-training interpretability analysis of transformers", "Integration with ABSynth for structured linguistic feature evaluation", "Reveals developmental phenomena during model training overlooked by scalar metrics"], "limitations": "", "keywords": ["transformer models", "interpretability", "linguistic signals", "machine learning", "toolkit"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2507.03671", "pdf": "https://arxiv.org/pdf/2507.03671.pdf", "abs": "https://arxiv.org/abs/2507.03671", "title": "Recon, Answer, Verify: Agents in Search of Truth", "authors": ["Satyam Shukla", "Himanshu Dutta", "Pushpak Bhattacharyya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automated fact checking with large language models (LLMs) offers a scalable\nalternative to manual verification. Evaluating fact checking is challenging as\nexisting benchmark datasets often include post claim analysis and annotator\ncues, which are absent in real world scenarios where claims are fact checked\nimmediately after being made. This limits the realism of current evaluations.\nWe present Politi Fact Only (PFO), a 5 class benchmark dataset of 2,982\npolitical claims from politifact.com, where all post claim analysis and\nannotator cues have been removed manually. This ensures that models are\nevaluated using only the information that would have been available prior to\nthe claim's verification. Evaluating LLMs on PFO, we see an average performance\ndrop of 22% in terms of macro f1 compared to PFO's unfiltered version. Based on\nthe identified challenges of the existing LLM based fact checking system, we\npropose RAV (Recon Answer Verify), an agentic framework with three agents:\nquestion generator, answer generator, and label generator. Our pipeline\niteratively generates and answers sub questions to verify different aspects of\nthe claim before finally generating the label. RAV generalizes across domains\nand label granularities, and it outperforms state of the art approaches on well\nknown baselines RAWFC (fact checking, 3 class) by 25.28%, and on HOVER\n(encyclopedia, 2 class) by 1.54% on 2 hop, 4.94% on 3 hop, and 1.78% on 4 hop,\nsub categories respectively. RAV shows the least performance drop compared to\nbaselines of 16.3% in macro f1 when we compare PFO with its unfiltered version.", "AI": {"tldr": "The paper introduces Politi Fact Only (PFO), a realistic benchmark dataset for evaluating fact-checking with LLMs, and proposes RAV, a new framework that outperforms existing methods in validating political claims.", "motivation": "To improve the evaluation realism of LLM-based fact-checking systems by providing a dataset that excludes post claim analysis and annotator cues, which are not available in real-world scenarios.", "method": "The authors introduce a new benchmark dataset (PFO) and develop the RAV framework consisting of three agents that iteratively generates and verifies claims using sub-questions.", "result": "LLMs evaluated on PFO show a 22% drop in performance compared to the unfiltered dataset. RAV outperforms baseline fact-checking systems on various benchmarks, demonstrating robust validation capabilities across domains.", "conclusion": "RAV effectively addresses the challenges in LLM-based fact-checking, providing significant improvements over state-of-the-art methods with a smaller performance drop when using realistic datasets.", "key_contributions": ["Introduction of Politi Fact Only (PFO), a more realistic dataset for evaluating LLMs in fact-checking.", "Development of RAV, a new framework for claim verification with improved performance benchmarks.", "Demonstration of RAV's effectiveness across multiple fact-checking domains and label granularities."], "limitations": "", "keywords": ["fact checking", "large language models", "dataset", "automated verification", "RAV"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03673", "pdf": "https://arxiv.org/pdf/2507.03673.pdf", "abs": "https://arxiv.org/abs/2507.03673", "title": "TACOS: Open Tagging and Comparative Scoring for Instruction Fine-Tuning Data Selection", "authors": ["Xixiang He", "Hao Yu", "Qiyao Sun", "Ao Cheng", "Tailai Zhang", "Cong Liu", "Shuxuan Guo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction Fine-Tuning (IFT) is crucial for aligning large language models\n(LLMs) with human preferences, and selecting a small yet representative subset\nfrom massive data significantly facilitates IFT in terms of both efficiency and\neffectiveness. Nevertheless, existing approaches suffer from two limitations:\nthe use of simple heuristics restricts data diversity, while the singleton data\nquality evaluation accounts for inconsistent criteria between independent\nsamples. To address the issues, we present TACOS, an innovative method that\nintegrates Open Tagging and Comparative Scoring for IFT data selection. To\ncapture data diversity, we leverage LLMs to assign open-domain tags to human\nqueries, followed by a normalization stage to denoise the open tags and enable\nefficient clustering. Additionally, we suggest a comparative scoring method\nthat allows the relative quality evaluation of samples within a cluster,\navoiding inconsistent criteria seen in singleton-based evaluations. Extensive\nexperiments across diverse datasets and LLM architectures demonstrate that\nTACOS outperforms existing approaches by a large margin. Notably, it achieves\nsuperior instruction-following performance on MT-Bench and ranks 1st among\nLLaMA2-7B-Based models on AlpacaEval 2.0, illustrating its efficacy for IFT\ndata selection.", "AI": {"tldr": "TACOS is an innovative method for Instruction Fine-Tuning data selection, enhancing efficiency and effectiveness by using LLMs for data diversity and comparative scoring.", "motivation": "To improve Instruction Fine-Tuning (IFT) of large language models by addressing limitations in existing data selection methods.", "method": "Integrates Open Tagging to assign tags to queries and Comparative Scoring for relative quality evaluation to enhance data selection diversity and consistency.", "result": "TACOS outperforms existing methods significantly, achieving superior instruction-following performance on MT-Bench and ranking 1st among LLaMA2-7B-Based models on AlpacaEval 2.0.", "conclusion": "TACOS effectively enhances the selection of IFT data, leading to improved model performance.", "key_contributions": ["Innovative Open Tagging for capturing data diversity", "Comparative Scoring for consistent quality evaluation", "Demonstrated superior effectiveness across multiple datasets"], "limitations": "", "keywords": ["Instruction Fine-Tuning", "Large Language Models", "Data Selection"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03674", "pdf": "https://arxiv.org/pdf/2507.03674.pdf", "abs": "https://arxiv.org/abs/2507.03674", "title": "STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured Information Extraction with Human-In-The-Loop Evaluation and Benchmarking", "authors": ["Tek Raj Chhetri", "Yibei Chen", "Puja Trivedi", "Dorota Jarecka", "Saif Haobsh", "Patrick Ray", "Lydia Ng", "Satrajit S. Ghosh"], "categories": ["cs.CL", "cs.AI"], "comment": "All figures are necessary", "summary": "The ability to extract structured information from unstructured sources-such\nas free-text documents and scientific literature-is critical for accelerating\nscientific discovery and knowledge synthesis. Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in various natural language processing\ntasks, including structured information extraction. However, their\neffectiveness often diminishes in specialized, domain-specific contexts that\nrequire nuanced understanding and expert-level domain knowledge. In addition,\nexisting LLM-based approaches frequently exhibit poor transferability across\ntasks and domains, limiting their scalability and adaptability. To address\nthese challenges, we introduce StructSense, a modular, task-agnostic,\nopen-source framework for structured information extraction built on LLMs.\nStructSense is guided by domain-specific symbolic knowledge encoded in\nontologies, enabling it to navigate complex domain content more effectively. It\nfurther incorporates agentic capabilities through self-evaluative judges that\nform a feedback loop for iterative refinement, and includes human-in-the-loop\nmechanisms to ensure quality and validation. We demonstrate that StructSense\ncan overcome both the limitations of domain sensitivity and the lack of\ncross-task generalizability, as shown through its application to diverse\nneuroscience information extraction tasks.", "AI": {"tldr": "StructSense is an open-source framework that enhances structured information extraction from unstructured sources using LLMs, guided by domain-specific knowledge and featuring a feedback loop for quality assurance.", "motivation": "To improve structured information extraction from unstructured sources in specialized domains, addressing limitations in existing LLM approaches.", "method": "StructSense integrates domain-specific symbolic knowledge through ontologies and employs self-evaluative judges within a human-in-the-loop system for iterative refinement.", "result": "StructSense demonstrates improved performance in structured information extraction tasks, particularly in specialized domains such as neuroscience.", "conclusion": "StructSense effectively addresses challenges of domain sensitivity and cross-task generalizability seen in previous LLM-based approaches.", "key_contributions": ["Introducing an open-source, modular framework for structured information extraction using LLMs.", "Incorporating domain-specific knowledge encoded in ontologies for improved understanding.", "Establishing a feedback loop with human-in-the-loop mechanisms for quality assurance."], "limitations": "The framework's efficacy may vary with the complexity of domain-specific knowledge and the integration of human feedback.", "keywords": ["Structured Information Extraction", "Large Language Models", "Domain-Specific Knowledge", "Neuroscience", "Human-in-the-Loop"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03704", "pdf": "https://arxiv.org/pdf/2507.03704.pdf", "abs": "https://arxiv.org/abs/2507.03704", "title": "Controlling Thinking Speed in Reasoning Models", "authors": ["Zhengkai Lin", "Zhihang Fu", "Ze Chen", "Chao Chen", "Liang Xie", "Wenxiao Wang", "Deng Cai", "Zheng Wang", "Jieping Ye"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Human cognition is theorized to operate in two modes: fast, intuitive System\n1 thinking and slow, deliberate System 2 thinking. While current Large\nReasoning Models (LRMs) excel at System 2 thinking, their inability to perform\nfast thinking leads to high computational overhead and latency. In this work,\nwe enable LRMs to approximate human intelligence through dynamic thinking speed\nadjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses\ntwo key questions: (1) how to control thinking speed in LRMs, and (2) when to\nadjust it for optimal performance. For the first question, we identify the\nsteering vector that governs slow-fast thinking transitions in LRMs'\nrepresentation space. Using this vector, we achieve the first representation\nediting-based test-time scaling effect, outperforming existing prompt-based\nscaling methods. For the second question, we apply real-time difficulty\nestimation to signal reasoning segments of varying complexity. Combining these\ntechniques, we propose the first reasoning strategy that enables fast\nprocessing of easy steps and deeper analysis for complex reasoning. Without any\ntraining or additional cost, our plug-and-play method yields an average +1.3%\naccuracy with -8.6% token usage across leading LRMs and advanced reasoning\nbenchmarks. All of our algorithms are implemented based on vLLM and are\nexpected to support broader applications and inspire future research.", "AI": {"tldr": "This work introduces a method for enabling Large Reasoning Models (LRMs) to switch between fast and slow thinking modes to optimize performance while reducing computational costs.", "motivation": "The paper addresses the limitations of current LRMs in performing fast, intuitive thinking (System 1), focusing on enhancing the efficiency and speed of these models during reasoning tasks.", "method": "The authors propose a dynamic thinking speed adjustment method that involves controlling thinking speed through a steering vector and utilizing real-time difficulty estimation for varying reasoning complexities.", "result": "The proposed method results in an average increase of +1.3% accuracy while reducing token usage by -8.6% across various LRM architectures and reasoning benchmarks, achieving effective cost-saving performance gains.", "conclusion": "The approach is novel in enabling fast processing of easier tasks while allowing more in-depth analysis for complex reasoning, functioning without additional training.", "key_contributions": ["Introduction of a steering vector for thinking speed control in LRMs", "Implementation of the first representation editing-based test-time scaling effect", "Real-time difficulty estimation for optimizing reasoning performance"], "limitations": "", "keywords": ["Large Reasoning Models", "cognition", "dynamic thinking speed adjustment", "accuracy-efficiency trade-offs", "real-time difficulty estimation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.15133", "pdf": "https://arxiv.org/pdf/2504.15133.pdf", "abs": "https://arxiv.org/abs/2504.15133", "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models", "authors": ["Ziwen Xu", "Shuxun Wang", "Kewei Xu", "Haoming Xu", "Mengru Wang", "Xinle Deng", "Yunzhi Yao", "Guozhou Zheng", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Work in progress. Demo: https://www.youtube.com/watch?v=AkfoiPfp5rQ;\n  code: https://github.com/zjunlp/EasyEdit", "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://www.youtube.com/watch?v=AkfoiPfp5rQ for a quick introduction.", "AI": {"tldr": "EasyEdit2 is a framework facilitating the control of Large Language Model (LLM) behaviors through user-friendly interventions.", "motivation": "The paper addresses the need for accessible methods to adjust LLM responses without requiring extensive technical expertise.", "method": "EasyEdit2 implements a new architecture with modules for generating and applying steering vectors to control model behaviors at test time.", "result": "Empirical evaluations show that EasyEdit2 effectively steers LLMs across various behaviors like safety and sentiment.", "conclusion": "EasyEdit2 provides a user-friendly solution for model steering, making it easier to achieve desired outcomes in LLM applications.", "key_contributions": ["Introduction of a new architecture for LLM steering", "Ease of use for non-technical users", "Empirical validation across different LLMs"], "limitations": "", "keywords": ["Large Language Models", "model steering", "user-friendly interventions"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.03711", "pdf": "https://arxiv.org/pdf/2507.03711.pdf", "abs": "https://arxiv.org/abs/2507.03711", "title": "Can LLMs Play Ô Ăn Quan Game? A Study of Multi-Step Planning and Decision Making", "authors": ["Sang Quang Nguyen", "Kiet Van Nguyen", "Vinh-Tiep Nguyen", "Thanh Duc Ngo", "Ngan Luu-Thuy Nguyen", "Dinh-Duy Le"], "categories": ["cs.CL"], "comment": "Accepted paper at MAPR 2025", "summary": "In this paper, we explore the ability of large language models (LLMs) to plan\nand make decisions through the lens of the traditional Vietnamese board game,\n\\^O \\u{A}n Quan. This game, which involves a series of strategic token\nmovements and captures, offers a unique environment for evaluating the\ndecision-making and strategic capabilities of LLMs. Specifically, we develop\nvarious agent personas, ranging from aggressive to defensive, and employ the\n\\^O \\u{A}n Quan game as a testbed for assessing LLM performance across\ndifferent strategies. Through experimentation with models like\nLlama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, and Llama-3.3-70B-Instruct, we\naim to understand how these models execute strategic decision-making, plan\nmoves, and manage dynamic game states. The results will offer insights into the\nstrengths and weaknesses of LLMs in terms of reasoning and strategy,\ncontributing to a deeper understanding of their general capabilities.", "AI": {"tldr": "This paper investigates large language models' strategic decision-making abilities using the Vietnamese board game Ô Ăn Quan as a testbed.", "motivation": "To evaluate the decision-making and strategic capabilities of large language models (LLMs) in a structured environment.", "method": "Developed agent personas with varying strategies and tested LLMs like Llama-3.2-3B-Instruct and Llama-3.3-70B-Instruct in the game.", "result": "Insights into LLM performance in strategic decision-making and management of dynamic game states were obtained.", "conclusion": "The research contributes to understanding the reasoning and strategy capabilities of LLMs in game scenarios.", "key_contributions": ["Evaluation of LLM decision-making via a strategic board game", "Insights into LLMs' strengths and weaknesses in reasoning", "Development of varying strategic agent personas for testing"], "limitations": "", "keywords": ["Large Language Models", "Decision Making", "Game Theory", "Strategic Planning", "Artificial Intelligence"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.03724", "pdf": "https://arxiv.org/pdf/2507.03724.pdf", "abs": "https://arxiv.org/abs/2507.03724", "title": "MemOS: A Memory OS for AI System", "authors": ["Zhiyu Li", "Shichao Song", "Chenyang Xi", "Hanyu Wang", "Chen Tang", "Simin Niu", "Ding Chen", "Jiawei Yang", "Chunyu Li", "Qingchen Yu", "Jihao Zhao", "Yezhaohui Wang", "Peng Liu", "Zehao Lin", "Pengyuan Wang", "Jiahao Huo", "Tianyi Chen", "Kai Chen", "Kehang Li", "Zhen Tao", "Junpeng Ren", "Huayi Lai", "Hao Wu", "Bo Tang", "Zhenren Wang", "Zhaoxin Fan", "Ningyu Zhang", "Linfeng Zhang", "Junchi Yan", "Mingchuan Yang", "Tong Xu", "Wei Xu", "Huajun Chen", "Haofeng Wang", "Hongkang Yang", "Wentao Zhang", "Zhi-Qin John Xu", "Siheng Chen", "Feiyu Xiong"], "categories": ["cs.CL"], "comment": "36 pages, 10 figures, 5 tables", "summary": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling.", "AI": {"tldr": "The paper proposes MemOS, a memory operating system that enhances Large Language Models (LLMs) by introducing an explicit memory layer to improve long-context reasoning, continual personalization, and knowledge consistency.", "motivation": "LLMs currently lack effective memory management systems which limits their reasoning capabilities and knowledge maintenance over time.", "method": "The authors propose MemOS, which integrates plaintext, activation-based, and parameter-level memories into a cohesive management system using MemCubes that encapsulate memory content and metadata.", "result": "MemOS enables cost-efficient storage and retrieval of memories, facilitating better transitions between different memory types and enhancing the LLMs' capabilities in long-term learning and personalization.", "conclusion": "By establishing a memory-centric framework, MemOS provides controllability and adaptability for LLMs, making it a step toward continual learning and personalized modeling in AI applications.", "key_contributions": ["Introduction of MemOS for memory management in LLMs", "Development of MemCubes for seamless memory type transitions", "Enhancement of long-context reasoning and continual learning capabilities in LLMs"], "limitations": "", "keywords": ["Large Language Models", "Memory Management", "Continual Learning", "Machine Learning", "AI Personalization"], "importance_score": 9, "read_time_minutes": 36}}
{"id": "2507.03774", "pdf": "https://arxiv.org/pdf/2507.03774.pdf", "abs": "https://arxiv.org/abs/2507.03774", "title": "Alpay Algebra IV: Symbiotic Semantics and the Fixed-Point Convergence of Observer Embeddings", "authors": ["Bugra Kilictas", "Faruk Alpay"], "categories": ["cs.CL", "cs.AI", "68T50, 68T07, 03G30, 18C10", "I.2.7; I.2.6; F.4.1"], "comment": "19 pages, 1 figure", "summary": "We present a theoretical framework in which a document and an AI model engage\nin a transfinite fixed-point interaction that leads to stable semantic\nalignment. Building on the foundations of Alpay Algebra, we introduce a\nfunctorial system wherein an observer (the AI) and a textual environment (this\npaper) co-evolve through iterative transformations guided by the phi-infinity\noperator. This process guarantees the existence of a unique fixed point in the\nAI's embedding space -- a state where the AI's internal representation of the\ncontent becomes stable, self-consistent, and semantically faithful. We prove\nthat such convergence is mathematically sound, semantically invariant, and\npermanent, even under perturbation or further context expansion. This fixed\npoint acts as an \"empathetic embedding,\" wherein the AI internalizes not only\nthe meaning of the content but also the author's intent. We interpret this as a\nrigorous, category-theoretic route to alignment at the embedding level, with\nimplications for semantic security, symbolic memory, and the construction of AI\nsystems with persistent self-referential understanding. All references in this\npaper function as nodes in the Alpay Algebra universe, and this work embeds\nitself as a new fixed-point node within that transfinite semantic graph.", "AI": {"tldr": "This paper presents a theoretical framework for stable semantic alignment between a document and an AI model through transfinite fixed-point interactions and category theory.", "motivation": "To establish a mathematical model for AI and text interaction that fosters stable semantic alignment and understanding of both content and author's intent.", "method": "The authors develop a functorial system based on Alpay Algebra, using the phi-infinity operator to illustrate transfinite interactions leading to a fixed point in the AI's embedding space.", "result": "The framework guarantees convergence to a unique fixed point that maintains semantic stability and fidelity, even under various perturbations or context expansions.", "conclusion": "The proposed empathetic embedding allows AI systems to achieve a self-consistent and semantically faithful understanding of documents.", "key_contributions": ["Introduction of a theoretical model for AI and document interaction using category theory.", "Establishment of the phi-infinity operator for achieving semantic alignment.", "Demonstration of a stable fixed point representing empathetic embedding in AI."], "limitations": "", "keywords": ["semantic alignment", "Alpay Algebra", "empathetic embedding"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2507.03865", "pdf": "https://arxiv.org/pdf/2507.03865.pdf", "abs": "https://arxiv.org/abs/2507.03865", "title": "OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference", "authors": ["Seungjun Shin", "Jaehoon Oh", "Dokwan Oh"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Attention mechanisms are central to the success of large language models\n(LLMs), enabling them to capture intricate token dependencies and implicitly\nassign importance to each token. Recent studies have revealed the sink token,\nwhich receives disproportionately high attention despite their limited semantic\nrole. In this paper, we first expand the relationship between the sink token\nand other tokens, moving beyond attention to explore their similarity in hidden\nstates, considering the layer depth. We observe that as the layers get deeper,\nthe cosine similarity between the normalized hidden states of the sink token\nand those of other tokens increases, and that the normalized hidden states of\nthe sink token exhibit negligible changes. These imply that other tokens\nconsistently are directed toward the sink token throughout the layers. Next, we\npropose a dynamic token selection method, called OrthoRank, using these\nfindings to select important tokens. Specifically, in a certain layer, we\ndefine token importance by the speed at which the token moves toward the sink\ntoken. This is converted into orthogonality with the sink token, meaning that\ntokens that are more orthogonal to the sink token are assigned greater\nimportance. Finally, through extensive experiments, we demonstrated that our\nmethod results in lower perplexity and higher zero-shot accuracy compared to\nlayer pruning methods at the same sparsity ratio with comparable throughput,\nwhile also achieving superior performance on LongBench.", "AI": {"tldr": "This paper examines the role of sink tokens in large language models and proposes a new token selection method, OrthoRank, which improves token importance assessment and model performance.", "motivation": "To understand the influence of sink tokens in large language models and to enhance the selection of important tokens for improved performance.", "method": "The paper analyzes the cosine similarity between sink tokens and other tokens across different layers. It introduces a token selection method called OrthoRank based on the orthogonality to sink tokens.", "result": "The proposed OrthoRank method achieves lower perplexity and higher zero-shot accuracy compared to traditional layer pruning methods at similar sparsity ratios.", "conclusion": "OrthoRank improves token selection in large language models and enhances performance metrics over existing methods.", "key_contributions": ["Expanded understanding of sink token dynamics in large language models.", "Introduced the OrthoRank token selection method based on orthogonality to sink tokens.", "Demonstrated superior model performance with OrthoRank compared to existing techniques."], "limitations": "", "keywords": ["large language models", "sink token", "token selection", "OrthoRank", "cosine similarity"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2507.03875", "pdf": "https://arxiv.org/pdf/2507.03875.pdf", "abs": "https://arxiv.org/abs/2507.03875", "title": "Demystifying ChatGPT: How It Masters Genre Recognition", "authors": ["Subham Raj", "Sriparna Saha", "Brijraj Singh", "Niranjan Pedanekar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The introduction of ChatGPT has garnered significant attention within the NLP\ncommunity and beyond. Previous studies have demonstrated ChatGPT's substantial\nadvancements across various downstream NLP tasks, highlighting its adaptability\nand potential to revolutionize language-related applications. However, its\ncapabilities and limitations in genre prediction remain unclear. This work\nanalyzes three Large Language Models (LLMs) using the MovieLens-100K dataset to\nassess their genre prediction capabilities. Our findings show that ChatGPT,\nwithout fine-tuning, outperformed other LLMs, and fine-tuned ChatGPT performed\nbest overall. We set up zero-shot and few-shot prompts using audio\ntranscripts/subtitles from movie trailers in the MovieLens-100K dataset,\ncovering 1682 movies of 18 genres, where each movie can have multiple genres.\nAdditionally, we extended our study by extracting IMDb movie posters to utilize\na Vision Language Model (VLM) with prompts for poster information. This\nfine-grained information was used to enhance existing LLM prompts. In\nconclusion, our study reveals ChatGPT's remarkable genre prediction\ncapabilities, surpassing other language models. The integration of VLM further\nenhances our findings, showcasing ChatGPT's potential for content-related\napplications by incorporating visual information from movie posters.", "AI": {"tldr": "This study evaluates the genre prediction capabilities of ChatGPT and other LLMs using the MovieLens-100K dataset, revealing that ChatGPT outperforms others, especially when fine-tuned, and demonstrates improved performance by integrating visual information from movie posters.", "motivation": "To analyze the genre prediction capabilities of ChatGPT and other LLMs, as their effectiveness in this area remains unclear despite advancements in NLP tasks.", "method": "The study evaluated three Large Language Models using the MovieLens-100K dataset, employing zero-shot and few-shot prompts derived from audio transcripts/subtitles of movie trailers, along with visual information from IMDb movie posters for enhanced genre prediction.", "result": "ChatGPT outperformed other LLMs in genre prediction tasks, and its performance was further improved when fine-tuned. The integration of visual data from movie posters added significant value to the predictions.", "conclusion": "ChatGPT demonstrates remarkable genre prediction capabilities surpassing other models, enhanced by integrating visual information, indicating its potential for content-related applications.", "key_contributions": ["Evaluation of ChatGPT's genre prediction capabilities compared to other LLMs.", "Demonstration of improved outcomes through fine-tuning and visual information integration.", "Establishment of methodology using audio and visual prompts for genre classification."], "limitations": "", "keywords": ["genre prediction", "ChatGPT", "large language models", "NLP", "vision language model"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.03922", "pdf": "https://arxiv.org/pdf/2507.03922.pdf", "abs": "https://arxiv.org/abs/2507.03922", "title": "Dynamic Injection of Entity Knowledge into Dense Retrievers", "authors": ["Ikuya Yamada", "Ryokan Ri", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "categories": ["cs.CL"], "comment": null, "summary": "Dense retrievers often struggle with queries involving less-frequent entities\ndue to their limited entity knowledge. We propose the Knowledgeable Passage\nRetriever (KPR), a BERT-based retriever enhanced with a context-entity\nattention layer and dynamically updatable entity embeddings. This design\nenables KPR to incorporate external entity knowledge without retraining.\nExperiments on three datasets show that KPR consistently improves retrieval\naccuracy, achieving a substantial 12.6% gain on the EntityQuestions dataset\nover the model without KPR extensions. When built on the off-the-shelf bge-base\nretriever, KPR achieves state-of-the-art performance among similarly sized\nmodels on two datasets. Code and models will be released soon.", "AI": {"tldr": "The Knowledgeable Passage Retriever (KPR) improves retrieval accuracy on less-frequent entity queries by using a BERT-based model with a context-entity attention layer and updatable entity embeddings.", "motivation": "Dense retrievers struggle with queries for less-frequent entities due to limited knowledge.", "method": "The KPR employs a context-entity attention layer and dynamically updates entity embeddings without needing retraining.", "result": "KPR shows a 12.6% improvement in retrieval accuracy on the EntityQuestions dataset and achieves state-of-the-art performance on two other datasets.", "conclusion": "KPR successfully enhances retrieval for less-frequent entities and will have its code and models released soon.", "key_contributions": ["Introduction of context-entity attention layer", "Use of dynamically updatable entity embeddings", "State-of-the-art performance on comparative datasets"], "limitations": "", "keywords": ["Dense retrievers", "Entity retrieval", "Knowledgeable Passage Retriever"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.03933", "pdf": "https://arxiv.org/pdf/2507.03933.pdf", "abs": "https://arxiv.org/abs/2507.03933", "title": "Losing our Tail -- Again: On (Un)Natural Selection And Multilingual Large Language Models", "authors": ["Eva Vanmassenhove"], "categories": ["cs.CL"], "comment": "12 pages", "summary": "Multilingual Large Language Models (LLMs) considerably changed how\ntechnologies can influence language. While previous technologies could mediate\nor assist humans, there is now a tendency to \\textit{offload} the task of\nwriting itself to these technologies, enabling them to change our linguistic\necosystem more directly. While they provide us quick access to information and\nimpressively fluent output, beneath their apparent sophistication lies a\nsubtle, more insidious threat: the gradual decline and loss of linguistic\ndiversity. With this opinion piece, I explore how model collapse, with a\nparticular focus on translation technology, can lead to the loss of linguistic\nforms, grammatical features, and cultural nuance. Model collapse refers to the\neventual consequence of self-consuming training loops, where models reinforce\ntheir own biases and lose linguistic diversity. Drawing on recent work in\nComputer Vision, Natural Language Processing (NLP) and Machine Translation\n(MT), I argue that the tails of our linguistic distributions are vanishing, and\nwith them, the narratives and identities they carry. This is a call to resist\nlinguistic flattening and to reimagine NLP as a field that encourages, values\nand protects expressive multilingual lexical and linguistic diversity and\ncreativity.", "AI": {"tldr": "The paper discusses the impact of multilingual LLMs on linguistic diversity, arguing that model collapse in language technologies can lead to a decline in linguistic forms and cultural nuances.", "motivation": "To explore how multilingual LLMs influence language and the risks of losing linguistic diversity via model collapse.", "method": "The author analyzes the effects of translation technology and model collapse on linguistic distributions, supported by recent work in related fields.", "result": "The paper finds that model collapse is leading to the disappearance of diverse linguistic forms and the narratives associated with them.", "conclusion": "The author calls for a reimagining of NLP to promote and protect linguistic diversity and creativity.", "key_contributions": ["Analysis of model collapse in multilingual LLMs", "Exploration of its effects on linguistic diversity", "Call to action for valuing multilingual expressions"], "limitations": "", "keywords": ["Multilingual LLMs", "Model collapse", "Linguistic diversity"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2507.03949", "pdf": "https://arxiv.org/pdf/2507.03949.pdf", "abs": "https://arxiv.org/abs/2507.03949", "title": "A Modular Unsupervised Framework for Attribute Recognition from Unstructured Text", "authors": ["KMA Solaiman"], "categories": ["cs.CL"], "comment": "arXiv admin note: substantial text overlap with arXiv:2506.20070", "summary": "We propose POSID, a modular, lightweight and on-demand framework for\nextracting structured attribute-based properties from unstructured text without\ntask-specific fine-tuning. While the method is designed to be adaptable across\ndomains, in this work, we evaluate it on human attribute recognition in\nincident reports. POSID combines lexical and semantic similarity techniques to\nidentify relevant sentences and extract attributes. We demonstrate its\neffectiveness on a missing person use case using the InciText dataset,\nachieving effective attribute extraction without supervised training.", "AI": {"tldr": "POSID is a framework for extracting structured attributes from unstructured text without needing fine-tuning, evaluated for human attribute recognition in incident reports.", "motivation": "To develop a flexible framework that can extract structured attribute-based properties from unstructured text across various domains without the need for task-specific fine-tuning.", "method": "POSID utilizes a combination of lexical and semantic similarity techniques to identify relevant sentences and extract attributes from the text.", "result": "The framework was evaluated using the InciText dataset in a missing person use case, demonstrating effective attribute extraction without the need for supervised training.", "conclusion": "POSID shows promise in being adaptable for attribute extraction tasks across different domains without the complexity of supervised training requirements.", "key_contributions": ["Modular and lightweight framework for attribute extraction", "No task-specific fine-tuning required", "Effective evaluation on human attribute recognition in incident reports"], "limitations": "The paper indicates substantial text overlap with a prior submission, which may limit the novelty of the findings.", "keywords": ["attribute extraction", "unstructured text", "human attribute recognition", "lexical similarity", "semantic similarity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.04009", "pdf": "https://arxiv.org/pdf/2507.04009.pdf", "abs": "https://arxiv.org/abs/2507.04009", "title": "Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents", "authors": ["Ziyang Miao", "Qiyu Sun", "Jingyuan Wang", "Yuchen Gong", "Yaowei Zheng", "Shiqi Li", "Richong Zhang"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "preprint", "summary": "Large language models (LLMs) have shown impressive performance on\ngeneral-purpose tasks, yet adapting them to specific domains remains\nchallenging due to the scarcity of high-quality domain data. Existing data\nsynthesis tools often struggle to extract reliable fine-tuning data from\nheterogeneous documents effectively. To address this limitation, we propose\nEasy Dataset, a unified framework for synthesizing fine-tuning data from\nunstructured documents via an intuitive graphical user interface (GUI).\nSpecifically, Easy Dataset allows users to easily configure text extraction\nmodels and chunking strategies to transform raw documents into coherent text\nchunks. It then leverages a persona-driven prompting approach to generate\ndiverse question-answer pairs using public-available LLMs. Throughout the\npipeline, a human-in-the-loop visual interface facilitates the review and\nrefinement of intermediate outputs to ensure data quality. Experiments on a\nfinancial question-answering task show that fine-tuning LLMs on the synthesized\ndataset significantly improves domain-specific performance while preserving\ngeneral knowledge. The source code and installable package are available at\nhttps://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub\nstars.", "AI": {"tldr": "Easy Dataset is a framework that synthesizes fine-tuning data from unstructured documents using an intuitive GUI.", "motivation": "The challenge of adapting large language models to specific domains due to a lack of high-quality domain data.", "method": "A unified framework that utilizes intuitive GUI for configuring text extraction models and chunking strategies, along with a persona-driven prompting approach for generating question-answer pairs.", "result": "Fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while maintaining general knowledge.", "conclusion": "Easy Dataset offers an accessible solution for synthesizing fine-tuning data and enhancing LLM performance on specific tasks.", "key_contributions": ["Intuitive GUI for data synthesis", "Human-in-the-loop interface for quality assurance", "Improved domain-specific LLM performance"], "limitations": "", "keywords": ["large language models", "data synthesis", "fine-tuning", "human-in-the-loop", "information extraction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04014", "pdf": "https://arxiv.org/pdf/2507.04014.pdf", "abs": "https://arxiv.org/abs/2507.04014", "title": "Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus on Korean Superstition", "authors": ["Kyuhee Kim", "Sangah Lee"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) become key advisors in various domains, their\ncultural sensitivity and reasoning skills are crucial in multicultural\nenvironments. We introduce Nunchi-Bench, a benchmark designed to evaluate LLMs'\ncultural understanding, with a focus on Korean superstitions. The benchmark\nconsists of 247 questions spanning 31 topics, assessing factual knowledge,\nculturally appropriate advice, and situational interpretation. We evaluate\nmultilingual LLMs in both Korean and English to analyze their ability to reason\nabout Korean cultural contexts and how language variations affect performance.\nTo systematically assess cultural reasoning, we propose a novel evaluation\nstrategy with customized scoring metrics that capture the extent to which\nmodels recognize cultural nuances and respond appropriately. Our findings\nhighlight significant challenges in LLMs' cultural reasoning. While models\ngenerally recognize factual information, they struggle to apply it in practical\nscenarios. Furthermore, explicit cultural framing enhances performance more\neffectively than relying solely on the language of the prompt. To support\nfurther research, we publicly release Nunchi-Bench alongside a leaderboard.", "AI": {"tldr": "Nunchi-Bench is a newly introduced benchmark for evaluating large language models' cultural understanding, specifically regarding Korean superstitions.", "motivation": "To evaluate the cultural sensitivity and reasoning skills of large language models (LLMs) in multicultural settings.", "method": "Nunchi-Bench consists of 247 questions across 31 topics assessing cultural understanding in both Korean and English, using a novel evaluation strategy with customized scoring metrics.", "result": "LLMs show challenges in cultural reasoning; they can recall factual information but struggle in practical applications, improving performance when cultural framing is used in prompts rather than language alone.", "conclusion": "Nunchi-Bench reveals the limitations of LLMs' cultural reasoning abilities and is publicly available to foster further research and evaluation.", "key_contributions": ["Introduction of Nunchi-Bench for cultural reasoning assessment in LLMs", "Evaluation of multilingual LLMs across cultural nuances", "Development of a novel evaluation strategy with customized scoring metrics"], "limitations": "Limited focus on Korean culture may not generalize to other cultural contexts.", "keywords": ["large language models", "cultural sensitivity", "NLP", "benchmark", "cultural reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.04018", "pdf": "https://arxiv.org/pdf/2507.04018.pdf", "abs": "https://arxiv.org/abs/2507.04018", "title": "Handling Korean Out-of-Vocabulary Words with Phoneme Representation Learning", "authors": ["Nayeon Kim", "Eojin Jeon", "Jun-Hyung Park", "SangKeun Lee"], "categories": ["cs.CL"], "comment": null, "summary": "In this study, we introduce KOPL, a novel framework for handling Korean OOV\nwords with Phoneme representation Learning. Our work is based on the linguistic\nproperty of Korean as a phonemic script, the high correlation between phonemes\nand letters. KOPL incorporates phoneme and word representations for Korean OOV\nwords, facilitating Korean OOV word representations to capture both text and\nphoneme information of words. We empirically demonstrate that KOPL\nsignificantly improves the performance on Korean Natural Language Processing\n(NLP) tasks, while being readily integrated into existing static and contextual\nKorean embedding models in a plug-and-play manner. Notably, we show that KOPL\noutperforms the state-of-the-art model by an average of 1.9%. Our code is\navailable at https://github.com/jej127/KOPL.git.", "AI": {"tldr": "KOPL is a framework designed to improve the handling of Korean out-of-vocabulary (OOV) words by incorporating phoneme representation learning, enhancing performance in Korean NLP tasks.", "motivation": "To address the challenge of Korean OOV words by leveraging the relationship between phonemes and letters in the Korean language.", "method": "KOPL combines phoneme and word representations for Korean OOV words, allowing for better capturing of both phoneme and text information in various Korean NLP tasks.", "result": "KOPL shows significant performance improvement on Korean NLP tasks, outperforming the previous state-of-the-art model by an average of 1.9%.", "conclusion": "KOPL can be easily integrated into existing Korean embedding models, demonstrating its effectiveness and convenience for enhancing NLP tasks.", "key_contributions": ["Introduction of a novel framework specifically for Korean OOV words", "Improvement in performance through phoneme representation learning", "Easy integration into existing models"], "limitations": "", "keywords": ["Korean NLP", "Out-of-vocabulary", "Phoneme representation", "Machine Learning", "Natural Language Processing"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.04023", "pdf": "https://arxiv.org/pdf/2507.04023.pdf", "abs": "https://arxiv.org/abs/2507.04023", "title": "LLMThinkBench: Towards Basic Math Reasoning and Overthinking in Large Language Models", "authors": ["Gaurav Srivastava", "Aafiya Hussain", "Sriram Srinivasan", "Xuan Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance on complex\nmathematical benchmarks, yet often struggle with simple arithmetic tasks and\nexhibit a tendency toward over-explaining or \"overthinking\" answers. To\nsystematically assess this phenomenon, we introduce LLMThinkBench, a modular\nbenchmarking framework that enables researchers to evaluate basic math\nreasoning and overthinking in LLMs. The framework provides 14 configurable math\ntasks with randomized test data generation and robust parsing strategies.\nResearchers can quantify overthinking using our Overthinking Score metric,\nwhich captures accuracy-verbosity tradeoffs through harmonic mean formulation.\nThe tool offers flexible evaluation with a scalable vLLM/Transformers backend,\nmulti-GPU support, and full configurability. Users can extend the tool with\ncustom tasks, reproduce experiments with seeding, and generate detailed\nefficiency reports. Distributed as a pip-installable package with CLI and API\naccess, LLMThinkBench provides researchers and practitioners an accessible,\ncost-effective alternative to expensive LLM-as-a-judge methods for diagnosing\nbasic reasoning capabilities and efficiency analysis. Package can be installed\nas: pip install llmthinkbench", "AI": {"tldr": "LLMThinkBench is a benchmarking framework for evaluating arithmetic reasoning and overthinking in Large Language Models (LLMs), providing configurable tasks and a metric for analyzing performance tradeoffs.", "motivation": "To address the shortcomings of LLMs in simple arithmetic tasks and their tendency to overthink answers.", "method": "The framework offers 14 configurable math tasks with randomized data generation, a metric called Overthinking Score for accuracy-verbosity tradeoffs, and support for extensibility and reproducibility.", "result": "LLMThinkBench allows for quantification of LLM performance in math reasoning and overthinking, providing detailed efficiency reports and enabling custom task integration.", "conclusion": "The framework offers researchers a cost-effective, flexible tool for benchmarking LLMs' arithmetic reasoning capabilities.", "key_contributions": ["Introduction of LLMThinkBench for LLM evaluation", "Overthinking Score metric for analyzing performance", "Configurable math tasks with customizable extensions"], "limitations": "", "keywords": ["Large Language Models", "benchmarking", "arithmetic reasoning", "overthinking"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.04026", "pdf": "https://arxiv.org/pdf/2507.04026.pdf", "abs": "https://arxiv.org/abs/2507.04026", "title": "Patient-Centered RAG for Oncology Visit Aid Following the Ottawa Decision Guide", "authors": ["Siyang Liu", "Lawrence Chin-I An", "Rada Mihalcea"], "categories": ["cs.CL"], "comment": null, "summary": "Effective communication is essential in cancer care, yet patients often face\nchallenges in preparing for complex medical visits. We present an interactive,\nRetrieval-augmented Generation-assisted system that helps patients progress\nfrom uninformed to visit-ready. Our system adapts the Ottawa Personal Decision\nGuide into a dynamic retrieval-augmented generation workflow, helping users\nbridge knowledge gaps, clarify personal values and generate useful questions\nfor their upcoming visits. Focusing on localized prostate cancer, we conduct a\nuser study with patients and a clinical expert. Results show high system\nusability (UMUX Mean = 6.0 out of 7), strong relevance of generated content\n(Mean = 6.7 out of 7), minimal need for edits, and high clinical faithfulness\n(Mean = 6.82 out of 7). This work demonstrates the potential of combining\npatient-centered design with language models to enhance clinical preparation in\noncology care.", "AI": {"tldr": "An interactive system using retrieval-augmented generation helps cancer patients prepare for medical visits by clarifying knowledge gaps and personal values.", "motivation": "To enhance communication in cancer care by helping patients prepare for complex medical visits effectively.", "method": "The system adapts the Ottawa Personal Decision Guide into a dynamic retrieval-augmented generation workflow, focusing on localized prostate cancer.", "result": "The user study showed high usability (UMUX Mean = 6.0/7), strong relevance of generated content (Mean = 6.7/7), minimal edits required, and high clinical faithfulness (Mean = 6.82/7).", "conclusion": "The study demonstrates the potential of combining patient-centered design with language models for improving clinical preparation in oncology care.", "key_contributions": ["Developed an interactive system for cancer patient preparation", "Integrated retrieval-augmented generation with clinical content", "Validated system usability and clinical relevance through user studies"], "limitations": "", "keywords": ["cancer care", "patient communication", "retrieval-augmented generation", "prostate cancer", "clinical preparation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.04069", "pdf": "https://arxiv.org/pdf/2507.04069.pdf", "abs": "https://arxiv.org/abs/2507.04069", "title": "Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering", "authors": ["Ting-Wen Ko", "Jyun-Yu Jiang", "Pu-Jen Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external documents at inference time, enabling up-to-date\nknowledge access without costly retraining. However, conventional RAG methods\nretrieve passages independently, often leading to redundant, noisy, or\ninsufficiently diverse context-particularly problematic - particularly\nproblematic in noisy corpora and for multi-hop questions. To address this, we\npropose Adaptive Passage Combination Retrieval (AdaPCR), a novel framework for\nopen-domain question answering with black-box LMs. AdaPCR explicitly models\ndependencies between passages by considering passage combinations as units for\nretrieval and reranking. It consists of a context-aware query reformulation\nusing concatenated passages, and a reranking step trained with a predictive\nobjective aligned with downstream answer likelihood. Crucially, AdaPCR\nadaptively selects the number of retrieved passages without additional stopping\nmodules. Experiments across several QA benchmarks show that AdaPCR outperforms\nbaselines, particularly in multi-hop reasoning, demonstrating the effectiveness\nof modeling inter-passage dependencies for improved retrieval.", "AI": {"tldr": "The paper introduces AdaPCR, a framework that enhances retrieval-augmented generation by combining passages for improved open-domain question answering with large language models.", "motivation": "Conventional RAG methods retrieve passages independently, leading to issues like redundancy and insufficient diversity, which are problematic for multi-hop questions.", "method": "AdaPCR models dependencies between passages by treating combinations as units for retrieval and reranking, utilizing a context-aware query reformulation and a predictive reranking step.", "result": "AdaPCR shows superior performance across multiple QA benchmarks, especially in scenarios requiring multi-hop reasoning.", "conclusion": "Modeling inter-passage dependencies significantly improves retrieval effectiveness and performance in open-domain question answering.", "key_contributions": ["Introduction of the AdaPCR framework for passage combination", "Demonstration of improved performance on multi-hop question answering", "Development of a context-aware query reformulation approach"], "limitations": "", "keywords": ["retrieval-augmented generation", "multi-hop reasoning", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04070", "pdf": "https://arxiv.org/pdf/2507.04070.pdf", "abs": "https://arxiv.org/abs/2507.04070", "title": "XISM: an eXploratory and Interactive Graph Tool to Visualize and Evaluate Semantic Map Models", "authors": ["Zhu Liu", "Zhen Hu", "Lei Dai", "Ying Liu"], "categories": ["cs.CL"], "comment": "Paper under review", "summary": "Semantic map models represent meanings or functions as nodes in a graph\nconstrained by the local connectivity hypothesis, with edges indicating their\nassociations. Widely used in typological linguistics, these models compare\ninterrelated meanings across languages. Traditionally built manually in a\nbottom-up manner, they are inefficient for large datasets and lack\nvisualization and evaluation tools. This paper introduces XISM, an interactive\ntool based on our prior algorithm, which constructs semantic maps from user\ndata via a top-down approach, displays candidate maps, and evaluates them using\nmultiple metrics. Users can refine maps by editing edges, combining data-driven\nefficiency with expert knowledge. This human-in-the-loop design benefits both\ntypologists and computational linguists. The system\nhttps://770103knev48.vicp.fun/ and a demonstration video\nhttps://youtu.be/S-wsVDF2HSI?si=1OrcF41tRznaifhZ are publicly available.", "AI": {"tldr": "This paper introduces XISM, an interactive tool for constructing semantic maps from user data, enhancing efficiency and usability.", "motivation": "To improve the inefficiencies of manually built semantic maps in typological linguistics and to provide better tools for visualization and evaluation.", "method": "XISM uses a top-down approach to construct semantic maps from user data, allowing users to refine maps through editing edges and applying multiple evaluation metrics.", "result": "The tool allows for efficient construction and refinement of semantic maps, integrating both data and expert knowledge in the process.", "conclusion": "XISM benefits typologists and computational linguists by combining user input and algorithmic efficiency to create more effective semantic maps.", "key_contributions": ["Introduction of an interactive, user-driven semantic map tool", "Utilization of a top-down approach for map construction", "Evaluation metrics integrated for enhanced map refinement"], "limitations": "", "keywords": ["semantic maps", "typological linguistics", "interactive tools"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2507.04099", "pdf": "https://arxiv.org/pdf/2507.04099.pdf", "abs": "https://arxiv.org/abs/2507.04099", "title": "Conversation Forests: The Key to Fine Tuning Large Language Models for Multi-Turn Medical Conversations is Branching", "authors": ["Thomas Savage"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning methods such as Direct Preference Optimization (DPO) and Group\nRelative Policy Optimization (GRPO) have demonstrated success in training large\nlanguage models (LLMs) for single-turn tasks. However, these methods fall short\nin multi-turn applications, such as diagnostic patient interviewing, where\nunderstanding how early conversational turns influence downstream completions\nand outcomes is essential. In medicine, a multi-turn perspective is critical\nfor learning diagnostic schemas and better understanding conversation dynamics.\nTo address this gap, I introduce Savage Conversation Forests (SCF), a\nreinforcement learning framework that leverages a branched conversation\narchitecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple\npossible conversation continuations at each turn, enabling the model to learn\nhow different early responses affect downstream interactions and diagnostic\noutcomes. In experiments simulating doctor-patient conversations, SCF with\nbranching outperforms linear conversation architectures on diagnostic accuracy.\nI hypothesize that SCF's improvements stem from its ability to provide richer,\ninterdependent training signals across conversation turns. These results\nsuggest that a branched training architecture is an important strategy for fine\ntuning LLMs in complex multi-turn conversational tasks.", "AI": {"tldr": "The paper introduces Savage Conversation Forests (SCF), a framework for fine-tuning large language models (LLMs) in multi-turn dialogue, specifically in diagnostic patient interviewing, addressing limitations of current methods in this context.", "motivation": "Current fine-tuning methods for LLMs struggle in multi-turn applications, particularly in medical diagnostics, where understanding early conversational turns is crucial for outcomes.", "method": "The paper presents Savage Conversation Forests (SCF), a reinforcement learning framework that utilizes a branched conversation architecture for fine-tuning LLMs by generating multiple conversation continuations at each turn.", "result": "Experiments show that SCF significantly outperforms linear conversation architectures in terms of diagnostic accuracy in simulated doctor-patient conversations.", "conclusion": "The findings indicate that a branched training architecture like SCF is essential for improving performance in complex multi-turn conversational tasks.", "key_contributions": ["Introduction of Savage Conversation Forests (SCF) for multi-turn dialogue", "Demonstration of improved diagnostic accuracy with branching conversation architecture", "Provision of richer training signals through interdependent conversation turns"], "limitations": "", "keywords": ["Large Language Models", "Multi-turn Dialogue", "Reinforcement Learning", "Healthcare", "Conversational AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04127", "pdf": "https://arxiv.org/pdf/2507.04127.pdf", "abs": "https://arxiv.org/abs/2507.04127", "title": "BYOKG-RAG: Multi-Strategy Graph Retrieval for Knowledge Graph Question Answering", "authors": ["Costas Mavromatis", "Soji Adeshina", "Vassilis N. Ioannidis", "Zhen Han", "Qi Zhu", "Ian Robinson", "Bryan Thompson", "Huzefa Rangwala", "George Karypis"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge graph question answering (KGQA) presents significant challenges due\nto the structural and semantic variations across input graphs. Existing works\nrely on Large Language Model (LLM) agents for graph traversal and retrieval; an\napproach that is sensitive to traversal initialization, as it is prone to\nentity linking errors and may not generalize well to custom (\"bring-your-own\")\nKGs. We introduce BYOKG-RAG, a framework that enhances KGQA by synergistically\ncombining LLMs with specialized graph retrieval tools. In BYOKG-RAG, LLMs\ngenerate critical graph artifacts (question entities, candidate answers,\nreasoning paths, and OpenCypher queries), and graph tools link these artifacts\nto the KG and retrieve relevant graph context. The retrieved context enables\nthe LLM to iteratively refine its graph linking and retrieval, before final\nanswer generation. By retrieving context from different graph tools, BYOKG-RAG\noffers a more general and robust solution for QA over custom KGs. Through\nexperiments on five benchmarks spanning diverse KG types, we demonstrate that\nBYOKG-RAG outperforms the second-best graph retrieval method by 4.5% points\nwhile showing better generalization to custom KGs. BYOKG-RAG framework is\nopen-sourced at https://github.com/awslabs/graphrag-toolkit.", "AI": {"tldr": "BYOKG-RAG is a framework that enhances knowledge graph question answering by combining LLMs with specialized graph retrieval tools, improving performance on custom KGs.", "motivation": "To address the challenges in knowledge graph question answering due to structural and semantic variations across input graphs and the limitations of existing LLM-based approaches.", "method": "The framework uses LLMs to generate graph artifacts and employs graph retrieval tools to link these artifacts to the knowledge graph and retrieve relevant information, allowing for iterative refinement before final answer generation.", "result": "BYOKG-RAG outperforms the best existing graph retrieval methods by 4.5% points across various benchmarks and shows better adaptability to custom knowledge graphs.", "conclusion": "The BYOKG-RAG framework is open-sourced and provides a more effective solution for knowledge graph question answering, particularly for custom KGs.", "key_contributions": ["Introduction of BYOKG-RAG framework combining LLMs with graph retrieval tools.", "Improved performance on knowledge graph question answering tasks.", "Open-source implementation available for further research."], "limitations": "", "keywords": ["Knowledge Graphs", "Question Answering", "Large Language Models", "Graph Retrieval", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2507.04137", "pdf": "https://arxiv.org/pdf/2507.04137.pdf", "abs": "https://arxiv.org/abs/2507.04137", "title": "Token Level Hallucination Detection via Variance in Language Models", "authors": ["Keshav Kumar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive generative\ncapabilities across diverse tasks but remain susceptible to hallucinations,\nconfidently generated yet factually incorrect outputs. We introduce a\nreference-free, token-level hallucination detection framework that leverages\nthe variance in token log-probabilities across multiple stochastic generations.\nUnlike prior methods that require ground-truth references or sentence-level\nverification, our approach is model-agnostic, interpretable, and suited for\nreal-time or post-hoc analysis. We evaluate our method on unanswerable question\nprompts from the SQuAD v2 dataset and benchmark across three autoregressive\nmodels of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both\nquantitative metrics and visual diagnostics, we show that token-level variance\nreliably highlights instability in model outputs and correlates with\nhallucination patterns. Our framework is lightweight, reproducible, and\nadaptable to multiple domains, offering a valuable diagnostic tool for\nanalyzing generative reliability in LLMs.", "AI": {"tldr": "A novel, reference-free token-level hallucination detection framework for LLMs that detects instability and hallucination patterns in outputs using token log-probability variance.", "motivation": "To address the challenge of hallucinations in LLMs, which are confidently generated yet factually incorrect outputs, by providing an interpretable and model-agnostic detection method.", "method": "The framework utilizes token log-probabilities from multiple stochastic generations to identify variance in outputs, allowing for real-time or post-hoc analysis without needing ground-truth references.", "result": "Evaluation against unanswerable question prompts from the SQuAD v2 dataset across three autoregressive models demonstrated that the method reliably detects instability and correlates with known hallucination patterns.", "conclusion": "The proposed diagnostic tool is lightweight, reproducible, and adaptable across multiple domains, enhancing the analysis of generative reliability in large language models.", "key_contributions": ["Introduces a token-level hallucination detection framework that is model-agnostic.", "Does not require ground-truth references for detection.", "Provides both quantitative metrics and visual diagnostics for evaluating LLM outputs."], "limitations": "", "keywords": ["Language Models", "Hallucination Detection", "Token Log-Probabilities", "Generative Reliability", "SQuAD v2"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04142", "pdf": "https://arxiv.org/pdf/2507.04142.pdf", "abs": "https://arxiv.org/abs/2507.04142", "title": "Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies", "authors": ["Mael Jullien", "Marco Valentino", "Leonardo Ranaldi", "Andre Freitas"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent works on large language models (LLMs) have demonstrated the impact of\nprompting strategies and fine-tuning techniques on their reasoning\ncapabilities. Yet, their effectiveness on clinical natural language inference\n(NLI) remains underexplored. This study presents the first controlled\nevaluation of how prompt structure and efficient fine-tuning jointly shape\nmodel performance in clinical NLI. We inspect four classes of prompting\nstrategies to elicit reasoning in LLMs at different levels of abstraction, and\nevaluate their impact on a range of clinically motivated reasoning types. For\neach prompting strategy, we construct high-quality demonstrations using a\nfrontier model to distil multi-step reasoning capabilities into smaller models\n(4B parameters) via Low-Rank Adaptation (LoRA). Across different language\nmodels fine-tuned on the NLI4CT benchmark, we found that prompt type alone\naccounts for up to 44% of the variance in macro-F1. Moreover, LoRA fine-tuning\nyields consistent gains of +8 to 12 F1, raises output alignment above 97%, and\nnarrows the performance gap to GPT-4o-mini to within 7.1%. Additional\nexperiments on reasoning generalisation reveal that LoRA improves performance\nin 75% of the models on MedNLI and TREC Clinical Trials Track. Overall, these\nfindings demonstrate that (i) prompt structure is a primary driver of clinical\nreasoning performance, (ii) compact models equipped with strong prompts and\nLoRA can rival frontier-scale systems, and (iii) reasoning-type-aware\nevaluation is essential to uncover prompt-induced trade-offs. Our results\nhighlight the promise of combining prompt design and lightweight adaptation for\nmore efficient and trustworthy clinical NLP systems, providing insights on the\nstrengths and limitations of widely adopted prompting and parameter-efficient\ntechniques in highly specialised domains.", "AI": {"tldr": "This study evaluates the impact of prompt structure and fine-tuning on large language models' performance in clinical natural language inference, revealing significant findings that advocate for improved prompting strategies and fine-tuning methods.", "motivation": "To explore the effectiveness of prompting strategies and fine-tuning techniques in enhancing large language models' reasoning capabilities specifically for clinical natural language inference.", "method": "Controlled evaluation of four classes of prompting strategies; construction of high-quality demonstrations via a frontier model and Low-Rank Adaptation (LoRA) for distilling multi-step reasoning into smaller models. Performance evaluated on the NLI4CT benchmark.", "result": "Prompt type accounts for up to 44% of variance in macro-F1 scores; LoRA fine-tuning provides consistent improvements of +8 to 12 F1, with output alignment exceeding 97%. LoRA enhances performance in 75% of models on MedNLI and TREC Clinical Trials Track.", "conclusion": "Combining prompt design and lightweight adaptation techniques like LoRA can lead to efficient and trustworthy clinical NLP systems, emphasizing the importance of reasoning-type-aware evaluation in revealing the trade-offs of prompting strategies and parameter-efficient techniques.", "key_contributions": ["First controlled evaluation of prompt strategies in clinical NLI", "Demonstrated that prompt structure significantly influences performance", "LoRA fine-tuning can enable smaller models to rival larger counterparts in reasoning tasks."], "limitations": "The effectiveness of strategies may vary across different types of clinical data and tasks, highlighting the need for broader validation.", "keywords": ["Large Language Models", "Clinical NLI", "Prompting Strategies", "Low-Rank Adaptation", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.04149", "pdf": "https://arxiv.org/pdf/2507.04149.pdf", "abs": "https://arxiv.org/abs/2507.04149", "title": "Large Language Models for Zero-Shot Multicultural Name Recognition", "authors": ["Thanakorn Phonchai", "Surasakdi Siripong", "Nicholas Patterson", "Owen Campbell"], "categories": ["cs.CL"], "comment": null, "summary": "The robust and accurate recognition of multicultural names, particularly\nthose not previously encountered, is a critical challenge in an increasingly\nglobalized digital landscape. Traditional methods often falter when confronted\nwith the vast diversity and novel permutations of names across different\nlinguistic and cultural backgrounds. This paper introduces a novel framework,\nPrompt-Engineered Fine-Tuning (PEFT) for Large Language Models (LLMs) with\nAdversarial Data Augmentation and Cultural Knowledge Graph Integration,\ndesigned to significantly enhance zero-shot multicultural name recognition. Our\napproach leverages the powerful linguistic understanding of pre-trained LLMs,\ntransforming the recognition task into a guided generation problem. Through\nmeticulous prompt engineering, dynamic integration of explicit cultural\nknowledge derived from knowledge graphs, and the strategic application of\nadversarial data augmentation, we equip the LLM with an unprecedented ability\nto infer the cultural origin of unseen names. Extensive experiments demonstrate\nthat our PEFT method consistently outperforms established deep learning\nbaselines, including advanced Bi-LSTM models with cultural tags, achieving an\nimpressive 93.1\\% overall accuracy and a remarkable 89.5\\% accuracy on\nchallenging zero-shot name identification. An in-depth ablation study confirms\nthe synergistic contribution of each component, while a human evaluation\nhighlights our method's performance approaching human expert judgment. This\nwork signifies a substantial leap in multicultural name recognition, offering a\nhighly effective and scalable solution for real-world applications.", "AI": {"tldr": "This paper presents a novel framework, Prompt-Engineered Fine-Tuning (PEFT), improving zero-shot multicultural name recognition using LLMs.", "motivation": "The study addresses the significant challenge of recognizing diverse and novel multicultural names in a globalized digital environment, where traditional methods often fail.", "method": "The proposed method employs Prompt-Engineered Fine-Tuning (PEFT) in LLMs, integrating adversarial data augmentation and cultural knowledge graphs to enhance name recognition capabilities.", "result": "The PEFT method achieved 93.1% overall accuracy and 89.5% accuracy on zero-shot name identification, outperforming traditional deep learning methods.", "conclusion": "This work presents a scalable and effective solution for multicultural name recognition, nearing human expert performance.", "key_contributions": ["Introduction of PEFT for LLMs in name recognition", "Integration of cultural knowledge graphs with LLMs", "Demonstrated superior performance over existing methods."], "limitations": "", "keywords": ["multicultural name recognition", "large language models", "adversarial data augmentation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.04189", "pdf": "https://arxiv.org/pdf/2507.04189.pdf", "abs": "https://arxiv.org/abs/2507.04189", "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding", "authors": ["Runcong Zhao", "Qinglin Zhu", "Hainiu Xu", "Bin Liang", "Yulan He", "Lin Gui"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Understanding character relationships is essential for interpreting complex\nnarratives and conducting socially grounded AI research. However, manual\nannotation is time-consuming and low in coverage, while large language models\n(LLMs) often produce hallucinated or logically inconsistent outputs. We present\nSymbolicThought, a human-in-the-loop framework that combines LLM-based\nextraction with symbolic reasoning. The system constructs editable character\nrelationship graphs, refines them using seven types of logical constraints, and\nenables real-time validation and conflict resolution through an interactive\ninterface. To support logical supervision and explainable social analysis, we\nrelease a dataset of 160 interpersonal relationships with corresponding logical\nstructures. Experiments show that SymbolicThought improves annotation accuracy\nand consistency while significantly reducing time cost, offering a practical\ntool for narrative understanding, explainable AI, and LLM evaluation.", "AI": {"tldr": "Introducing SymbolicThought, a framework that merges LLM-based extraction with symbolic reasoning to enhance character relationship annotation.", "motivation": "To improve the efficiency and accuracy of understanding character relationships in narratives which is crucial for socially grounded AI research.", "method": "SymbolicThought uses a human-in-the-loop approach, integrating LLMs for extraction, constructing character relationship graphs, and applying symbolic reasoning with logical constraints for real-time validation.", "result": "The framework significantly enhances annotation accuracy and consistency while reducing time costs.", "conclusion": "SymbolicThought serves as an effective tool for narrative understanding, explainable AI, and evaluating LLMs, supported by a dataset of interpersonal relationships.", "key_contributions": ["Development of a human-in-the-loop framework combining LLM and symbolic reasoning", "Creation of an editable character relationship graph system", "Release of a dataset for interpersonal relationships with logical structures"], "limitations": "", "keywords": ["Human-in-the-loop", "Symbolic reasoning", "Character relationships"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04221", "pdf": "https://arxiv.org/pdf/2507.04221.pdf", "abs": "https://arxiv.org/abs/2507.04221", "title": "Context Tuning for In-Context Optimization", "authors": ["Jack Lu", "Ryan Teehan", "Zhenbang Yang", "Mengye Ren"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "A short version of this paper has been accepted for publication in\n  the Workshop on Test-Time Adaptation (PUT) at the International Conference on\n  Machine Learning (ICML) 2025", "summary": "We introduce Context Tuning, a simple and effective method to significantly\nenhance few-shot adaptation of language models (LLMs) without fine-tuning model\nparameters. While prompt-based adaptation techniques have demonstrated the\neffectiveness of lightweight adaptation methods for large language models\n(LLMs), they typically initialize a trainable prompt or prefix with irrelevant\ntokens for the task at hand. In contrast, Context Tuning initializes the\ntrainable prompt or prefix with task-specific demonstration examples,\nleveraging the model's inherent In-Context Learning (ICL) ability to extract\nrelevant information for improved few-shot learning performance. Extensive\nevaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard,\nand ARC demonstrate that Context Tuning outperforms traditional prompt-based\nadaptation methods and achieves competitive accuracy to Test-Time Training with\nsignificantly higher training efficiency.", "AI": {"tldr": "Context Tuning enhances few-shot adaptation of LLMs by initializing prompts with task-specific examples, leading to improved performance without fine-tuning.", "motivation": "To improve few-shot adaptation performance of language models without the need for fine-tuning, leveraging inherent in-context learning capabilities.", "method": "Context Tuning initializes trainable prompts with task-specific demonstration examples to better utilize the model's in-context learning.", "result": "Context Tuning outperforms traditional prompt-based adaptation methods and achieves competitive accuracy compared to Test-Time Training with higher training efficiency.", "conclusion": "The proposed Context Tuning method significantly enhances few-shot learning performance of LLMs by better utilizing task-specific demonstrations.", "key_contributions": ["Introduction of Context Tuning for adapting language models.", "Demonstrates the effectiveness of task-specific initialization for prompts.", "Establishes benchmarks where Context Tuning outperforms existing methods."], "limitations": "", "keywords": ["Context Tuning", "Language Models", "Few-shot learning", "In-Context Learning", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.04224", "pdf": "https://arxiv.org/pdf/2507.04224.pdf", "abs": "https://arxiv.org/abs/2507.04224", "title": "Fairness Evaluation of Large Language Models in Academic Library Reference Services", "authors": ["Haining Wang", "Jason Clark", "Yueru Yan", "Star Bradley", "Ruiyang Chen", "Yiqiong Zhang", "Hengyi Fu", "Zuoyu Tian"], "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": null, "summary": "As libraries explore large language models (LLMs) for use in virtual\nreference services, a key question arises: Can LLMs serve all users equitably,\nregardless of demographics or social status? While they offer great potential\nfor scalable support, LLMs may also reproduce societal biases embedded in their\ntraining data, risking the integrity of libraries' commitment to equitable\nservice. To address this concern, we evaluate whether LLMs differentiate\nresponses across user identities by prompting six state-of-the-art LLMs to\nassist patrons differing in sex, race/ethnicity, and institutional role. We\nfound no evidence of differentiation by race or ethnicity, and only minor\nevidence of stereotypical bias against women in one model. LLMs demonstrated\nnuanced accommodation of institutional roles through the use of linguistic\nchoices related to formality, politeness, and domain-specific vocabularies,\nreflecting professional norms rather than discriminatory treatment. These\nfindings suggest that current LLMs show a promising degree of readiness to\nsupport equitable and contextually appropriate communication in academic\nlibrary reference services.", "AI": {"tldr": "This study evaluates whether large language models (LLMs) can provide equitable support in virtual reference services across different user identities.", "motivation": "To explore the potential of LLMs in providing equitable service in libraries and to assess the impact of societal biases in LLMs on user interactions.", "method": "The study involved prompting six state-of-the-art LLMs with queries from patrons differing in sex, race/ethnicity, and institutional role to evaluate responses.", "result": "There was no differentiation by race or ethnicity in LLM responses, and minor stereotypical bias was observed against women in one model; LLMs displayed linguistic accommodation based on institutional roles.", "conclusion": "Current LLMs are capable of supporting equitable communication in academic library settings, with minimal evidence of bias based on demographic factors.", "key_contributions": ["Evaluated the equitable service potential of LLMs in library contexts", "Identified minor biases related to gender in LLM outputs", "Showed LLMs' adaptability to different institutional roles through language use"], "limitations": "Limited to six LLMs and may not fully represent all demographic interactions in broader contexts.", "keywords": ["large language models", "equitable service", "library reference services", "user identities", "bias"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.04329", "pdf": "https://arxiv.org/pdf/2507.04329.pdf", "abs": "https://arxiv.org/abs/2507.04329", "title": "No Language Data Left Behind: A Comparative Study of CJK Language Datasets in the Hugging Face Ecosystem", "authors": ["Dasol Choi", "Woomyoung Park", "Youngsook Song"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Natural Language Processing (NLP) have underscored the\ncrucial role of high-quality datasets in building large language models (LLMs).\nHowever, while extensive resources and analyses exist for English, the\nlandscape for East Asian languages - particularly Chinese, Japanese, and Korean\n(CJK) - remains fragmented and underexplored, despite these languages together\nserving over 1.6 billion speakers. To address this gap, we investigate the\nHuggingFace ecosystem from a cross-linguistic perspective, focusing on how\ncultural norms, research environments, and institutional practices shape\ndataset availability and quality. Drawing on more than 3,300 datasets, we\nemploy quantitative and qualitative methods to examine how these factors drive\ndistinct creation and curation patterns across Chinese, Japanese, and Korean\nNLP communities. Our findings highlight the large-scale and often\ninstitution-driven nature of Chinese datasets, grassroots community-led\ndevelopment in Korean NLP, and an entertainment- and subculture-focused\nemphasis on Japanese collections. By uncovering these patterns, we reveal\npractical strategies for enhancing dataset documentation, licensing clarity,\nand cross-lingual resource sharing - ultimately guiding more effective and\nculturally attuned LLM development in East Asia. We conclude by discussing best\npractices for future dataset curation and collaboration, aiming to strengthen\nresource development across all three languages.", "AI": {"tldr": "This paper investigates dataset availability and quality for Chinese, Japanese, and Korean NLP, highlighting cultural influences and proposing strategies for LLM development.", "motivation": "To address the fragmentation and underexploration of NLP datasets for East Asian languages, despite their substantial speaker demographics.", "method": "Analyzed over 3,300 datasets using quantitative and qualitative methods to explore how cultural norms and institutional practices shape dataset characteristics in CJK communities.", "result": "Identified distinct curation patterns: Chinese datasets are often institution-driven, Korean NLP is driven by grassroots communities, and Japanese collections focus on entertainment and subcultures.", "conclusion": "Developed practical strategies for improving dataset documentation and collaboration, enhancing LLM development in East Asia.", "key_contributions": ["Cross-linguistic analysis of dataset creation and curation patterns", "Practical recommendations for dataset documentation and licensing", "Guidance for culturally attuned LLM development"], "limitations": "Focus is limited to Chinese, Japanese, and Korean datasets; does not address other East Asian languages.", "keywords": ["Natural Language Processing", "datasets", "language models", "East Asia", "cross-linguistic"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.04350", "pdf": "https://arxiv.org/pdf/2507.04350.pdf", "abs": "https://arxiv.org/abs/2507.04350", "title": "HatePRISM: Policies, Platforms, and Research Integration. Advancing NLP for Hate Speech Proactive Mitigation", "authors": ["Naquee Rizwan", "Seid Muhie Yimam", "Daryna Dementieva", "Florian Skupin", "Tim Fischer", "Daniil Moskovskiy", "Aarushi Ajay Borkar", "Robert Geislinger", "Punyajoy Saha", "Sarthak Roy", "Martin Semmann", "Alexander Panchenko", "Chris Biemann", "Animesh Mukherjee"], "categories": ["cs.CL"], "comment": "arXiv admin note: substantial text overlap with arXiv:2406.19543", "summary": "Despite regulations imposed by nations and social media platforms, e.g.\n(Government of India, 2021; European Parliament and Council of the European\nUnion, 2022), inter alia, hateful content persists as a significant challenge.\nExisting approaches primarily rely on reactive measures such as blocking or\nsuspending offensive messages, with emerging strategies focusing on proactive\nmeasurements like detoxification and counterspeech. In our work, which we call\nHatePRISM, we conduct a comprehensive examination of hate speech regulations\nand strategies from three perspectives: country regulations, social platform\npolicies, and NLP research datasets. Our findings reveal significant\ninconsistencies in hate speech definitions and moderation practices across\njurisdictions and platforms, alongside a lack of alignment with research\nefforts. Based on these insights, we suggest ideas and research direction for\nfurther exploration of a unified framework for automated hate speech moderation\nincorporating diverse strategies.", "AI": {"tldr": "This paper analyzes hate speech regulations and moderation strategies across countries, social media platforms, and NLP datasets to propose a unified framework for automated hate speech moderation.", "motivation": "The persistence of hateful content despite existing regulations underscores the need for effective moderation strategies.", "method": "A comprehensive examination of hate speech regulations and moderation practices was conducted from three perspectives: country regulations, social platform policies, and NLP research datasets.", "result": "The study found significant inconsistencies in hate speech definitions and moderation practices across different jurisdictions and platforms, revealing a disconnect with current research efforts.", "conclusion": "The paper suggests directions for developing a unified framework for automated hate speech moderation that incorporates diverse strategies, aiming to enhance proactive measures against hate speech.", "key_contributions": ["Analysis of inconsistencies in hate speech definitions and moderation across jurisdictions and platforms", "Identification of gaps between regulatory practices and NLP research", "Suggestions for future research on unified hate speech moderation frameworks"], "limitations": "", "keywords": ["Hate Speech", "Moderation", "NLP", "Regulations", "Social Media"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.04364", "pdf": "https://arxiv.org/pdf/2507.04364.pdf", "abs": "https://arxiv.org/abs/2507.04364", "title": "Large Language Models' Varying Accuracy in Recognizing Risk-Promoting and Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV Vaccination and Heated Tobacco Products", "authors": ["Soojong Kim", "Kwanho Kim", "Hye Min Kim"], "categories": ["cs.CL", "cs.SI"], "comment": "Forthcoming in Social Science & Medicine", "summary": "Machine learning methods are increasingly applied to analyze health-related\npublic discourse based on large-scale data, but questions remain regarding\ntheir ability to accurately detect different types of health sentiments.\nEspecially, Large Language Models (LLMs) have gained attention as a powerful\ntechnology, yet their accuracy and feasibility in capturing different opinions\nand perspectives on health issues are largely unexplored. Thus, this research\nexamines how accurate the three prominent LLMs (GPT, Gemini, and LLAMA) are in\ndetecting risk-promoting versus health-supporting sentiments across two\ncritical public health topics: Human Papillomavirus (HPV) vaccination and\nheated tobacco products (HTPs). Drawing on data from Facebook and Twitter, we\ncurated multiple sets of messages supporting or opposing recommended health\nbehaviors, supplemented with human annotations as the gold standard for\nsentiment classification. The findings indicate that all three LLMs generally\ndemonstrate substantial accuracy in classifying risk-promoting and\nhealth-supporting sentiments, although notable discrepancies emerge by\nplatform, health issue, and model type. Specifically, models often show higher\naccuracy for risk-promoting sentiment on Facebook, whereas health-supporting\nmessages on Twitter are more accurately detected. An additional analysis also\nshows the challenges LLMs face in reliably detecting neutral messages. These\nresults highlight the importance of carefully selecting and validating language\nmodels for public health analyses, particularly given potential biases in\ntraining data that may lead LLMs to overestimate or underestimate the\nprevalence of certain perspectives.", "AI": {"tldr": "This study evaluates the accuracy of three prominent LLMs (GPT, Gemini, and LLAMA) in detecting health sentiments in public discourse regarding HPV vaccination and heated tobacco products on social media.", "motivation": "To explore the effectiveness of LLMs in capturing various health sentiments in public discussions and to address the gaps in understanding their accuracy in sentiment classification.", "method": "The research uses curated datasets of health-related social media messages supplemented with human annotations. It evaluates the models' performance in detecting risk-promoting vs. health-supporting sentiments across Facebook and Twitter.", "result": "The models generally demonstrate substantial accuracy in sentiment classification, with variations in performance due to platform, health issue, and model type.", "conclusion": "Selecting and validating LLMs for public health analysis is crucial, as biases in training data can lead to misestimation of sentiment prevalence.", "key_contributions": ["Evaluation of LLMs' performance in health sentiment analysis on social media", "Comparison of sentiment detection accuracy across social media platforms", "Identification of challenges faced by LLMs in detecting neutral sentiments"], "limitations": "The study acknowledges potential biases in training data affecting model performance and the challenge of detecting neutral sentiments.", "keywords": ["Machine Learning", "Health Sentiment Analysis", "Large Language Models", "Public Health", "Social Media"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04391", "pdf": "https://arxiv.org/pdf/2507.04391.pdf", "abs": "https://arxiv.org/abs/2507.04391", "title": "Does Learning Mathematical Problem-Solving Generalize to Broader Reasoning?", "authors": ["Ruochen Zhou", "Minrui Xu", "Shiqi Chen", "Junteng Liu", "Yunqi Li", "Xinxin Lin", "Zhengyu Chen", "Junxian He"], "categories": ["cs.CL"], "comment": null, "summary": "There has been a growing interest in enhancing the mathematical\nproblem-solving (MPS) capabilities of large language models. While the majority\nof research efforts concentrate on creating specialized models to solve\nmathematical problems, it remains unknown how learning mathematical\nproblem-solving generalizes to help develop other reasoning abilities. In this\npaper, we present an empirical investigation into the generalization potential\nof various MPS training approaches, such as continual pretraining, instruction\ntuning, and rule-based reinforcement learning across various data sources,\nincluding both short and long chain-of-thought (CoT) samples. Evaluation on 5\nmathematical and 8 general reasoning benchmarks show that continual pretraining\non math text is able to generalize to general reasoning tasks to some extent.\nIn constrast, instruction tuning on conventional, short MPS samples provides\nlimited benefits and, in many cases, even impairs generalization performance.\nNotably, training with long CoT responses for MPS samples and incorporating\nrule-based reinforcement learning on MPS queries exhibit distinct behavior,\nsignificantly enhancing generalization by extending the model's reasoning\nprocesses into other domains. These results suggest that traditional approaches\nto learning MPS with short reasoning chains largely fail to achieve robust\ngeneralization. However, the emerging paradigm of longer reasoning chains,\ncoupled with self-reflection, offers a promising direction for improving\ngeneralized reasoning abilities through learning from specialized domains.", "AI": {"tldr": "This paper investigates the generalization abilities of large language models trained on mathematical problem-solving (MPS) tasks and compares various training approaches.", "motivation": "To understand how MPS training generalizes to enhance reasoning capabilities in large language models.", "method": "Empirical evaluation of various MPS training approaches, including continual pretraining, instruction tuning, and rule-based reinforcement learning, across several benchmarks.", "result": "Continual pretraining on math text allows for some generalization to general reasoning tasks; however, instruction tuning on short MPS samples offers limited benefits, often impairing performance.", "conclusion": "Longer reasoning chains and self-reflection in training yield better generalization in reasoning tasks, suggesting a shift from traditional short-chain approaches is needed.", "key_contributions": ["Empirical investigation of MPS training generalization", "Comparison of different training methods for large language models", "Highlighting the effectiveness of longer reasoning chains in enhancing reasoning abilities"], "limitations": "Focused primarily on the effectiveness of MPS training; broader implications on specific applications may need further exploration.", "keywords": ["Mathematical Problem Solving", "Large Language Models", "Reasoning Abilities", "Continual Pretraining", "Instruction Tuning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.04395", "pdf": "https://arxiv.org/pdf/2507.04395.pdf", "abs": "https://arxiv.org/abs/2507.04395", "title": "SpiritRAG: A Q&A System for Religion and Spirituality in the United Nations Archive", "authors": ["Yingqiang Gao", "Fabian Winiger", "Patrick Montjourides", "Anastassia Shaitarova", "Nianlong Gu", "Simon Peng-Keller", "Gerold Schneider"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Religion and spirituality (R/S) are complex and highly domain-dependent\nconcepts which have long confounded researchers and policymakers. Due to their\ncontext-specificity, R/S are difficult to operationalize in conventional\narchival search strategies, particularly when datasets are very large, poorly\naccessible, and marked by information noise. As a result, considerable time\ninvestments and specialist knowledge is often needed to extract actionable\ninsights related to R/S from general archival sources, increasing reliance on\npublished literature and manual desk reviews. To address this challenge, we\npresent SpiritRAG, an interactive Question Answering (Q&A) system based on\nRetrieval-Augmented Generation (RAG). Built using 7,500 United Nations (UN)\nresolution documents related to R/S in the domains of health and education,\nSpiritRAG allows researchers and policymakers to conduct complex,\ncontext-sensitive database searches of very large datasets using an easily\naccessible, chat-based web interface. SpiritRAG is lightweight to deploy and\nleverages both UN documents and user provided documents as source material. A\npilot test and evaluation with domain experts on 100 manually composed\nquestions demonstrates the practical value and usefulness of SpiritRAG.", "AI": {"tldr": "SpiritRAG is an interactive Q&A system that enables access to large datasets related to religion and spirituality, specifically from UN documents.", "motivation": "Research and policymakers struggle with extracting actionable insights from complex datasets on religion and spirituality due to their context-specificity and information noise.", "method": "The system uses Retrieval-Augmented Generation (RAG) to facilitate complex, context-sensitive searches through a chat-based web interface.", "result": "Pilot tests with domain experts using 100 questions showed SpiritRAG's practical value as an accessible tool for database searches in health and education contexts.", "conclusion": "SpiritRAG demonstrates potential for streamlining access to relevant information for researchers and policymakers dealing with R/S.", "key_contributions": ["Introduction of SpiritRAG, a new interactive Q&A system", "Utilization of RAG for simplified database searches", "Evaluation of the tool with domain experts showing its effectiveness"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Question Answering", "Religion and Spirituality", "Health", "Education"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.04414", "pdf": "https://arxiv.org/pdf/2507.04414.pdf", "abs": "https://arxiv.org/abs/2507.04414", "title": "THM@SimpleText 2025 -- Task 1.1: Revisiting Text Simplification based on Complex Terms for Non-Experts", "authors": ["Nico Hofmann", "Julian Dauenhauer", "Nils Ole Dietzler", "Idehen Daniel Idahor", "Christin Katharina Kreutz"], "categories": ["cs.CL"], "comment": null, "summary": "Scientific text is complex as it contains technical terms by definition.\nSimplifying such text for non-domain experts enhances accessibility of\ninnovation and information. Politicians could be enabled to understand new\nfindings on topics on which they intend to pass a law, or family members of\nseriously ill patients could read about clinical trials. The SimpleText CLEF\nLab focuses on exactly this problem of simplification of scientific text. Task\n1.1 of the 2025 edition specifically handles the simplification of complex\nsentences, so very short texts with little context. To tackle this task we\ninvestigate the identification of complex terms in sentences which are\nrephrased using small Gemini and OpenAI large language models for non-expert\nreaders.", "AI": {"tldr": "The paper addresses the simplification of complex scientific text for non-expert readers using large language models.", "motivation": "Enhancing accessibility of scientific information for non-domain experts, such as politicians and family members of patients.", "method": "The study investigates identifying complex terms in sentences and rephrasing them using Gemini and OpenAI large language models.", "result": "Initial findings suggest that the employed models can effectively simplify complex sentences for better understanding.", "conclusion": "Utilizing LLMs can potentially make scientific text more accessible to the general public, aiding in informed decision-making.", "key_contributions": ["Focus on simplification of scientific text for non-experts", "Use of Gemini and OpenAI models for rephrasing", "Identification of complex terms in sentences"], "limitations": "", "keywords": ["scientific text simplification", "large language models", "text accessibility"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.04415", "pdf": "https://arxiv.org/pdf/2507.04415.pdf", "abs": "https://arxiv.org/abs/2507.04415", "title": "MOMENTS: A Comprehensive Multimodal Benchmark for Theory of Mind", "authors": ["Emilio Villa-Cueva", "S M Masrur Ahmed", "Rendi Chevi", "Jan Christian Blaise Cruz", "Kareem Elzeky", "Fermin Cristobal", "Alham Fikri Aji", "Skyler Wang", "Rada Mihalcea", "Thamar Solorio"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding Theory of Mind is essential for building socially intelligent\nmultimodal agents capable of perceiving and interpreting human behavior. We\nintroduce MOMENTS (Multimodal Mental States), a comprehensive benchmark\ndesigned to assess the ToM capabilities of multimodal large language models\n(LLMs) through realistic, narrative-rich scenarios presented in short films.\nMOMENTS includes over 2,344 multiple-choice questions spanning seven distinct\nToM categories. The benchmark features long video context windows and realistic\nsocial interactions that provide deeper insight into characters' mental states.\nWhile the visual modality generally enhances model performance, current systems\nstill struggle to integrate it effectively, underscoring the need for further\nresearch into AI's multimodal understanding of human behavior.", "AI": {"tldr": "MOMENTS is a benchmark designed to evaluate the Theory of Mind capabilities of multimodal large language models using narrative-rich short films.", "motivation": "To build socially intelligent multimodal agents that can perceive and interpret human behavior effectively.", "method": "MOMENTS offers over 2,344 multiple-choice questions in seven categories, utilizing long video context and realistic social interactions to assess ToM capabilities of LLMs.", "result": "The benchmark reveals that while visual modalities improve model performance, existing systems struggle to integrate these modalities effectively.", "conclusion": "The findings indicate a need for more research in enhancing AI's multimodal understanding of human behavior.", "key_contributions": ["Introduction of a new benchmark for assessing Theory of Mind in LLMs", "Inclusion of realistic, narrative-rich scenarios for evaluation", "Identification of integration challenges in multimodal systems"], "limitations": "Current systems do not fully integrate visual modalities, highlighting gaps in understanding human behavior.", "keywords": ["Theory of Mind", "multimodal models", "large language models", "social interactions", "benchmark"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.04416", "pdf": "https://arxiv.org/pdf/2507.04416.pdf", "abs": "https://arxiv.org/abs/2507.04416", "title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling", "authors": ["Xiuying Wei", "Anunay Yadav", "Razvan Pascanu", "Caglar Gulcehre"], "categories": ["cs.CL"], "comment": null, "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT", "AI": {"tldr": "Introducing a new architecture called \rat that combines recurrences and attention to enhance efficiency in long-context language modeling.", "motivation": "To address computational bottlenecks in long-context settings caused by traditional softmax attention in Transformers.", "method": "The \rat design partitions input into chunks, applies linear recurrence within each chunk, and uses softmax attention across chunks, enabling flexible trade-offs between RNN and attention strengths.", "result": "The \rat layer achieves a 7× improvement in training speed for 100K token sequences and 9× in generation speed at 4K sequence length while maintaining comparable accuracy to standard attention.", "conclusion": "The hybrid architecture interleaving \rat with local attention improves inference speed and reduces cache memory usage, leading to enhanced performance in various tasks.", "key_contributions": ["Introduction of the \rat architecture for efficient language modeling", "Demonstrated significant speed improvements in training and generation", "Proposed a hybrid model combining \rat with local attention for optimized performance"], "limitations": "", "keywords": ["Transformers", "Recurrence", "Attention", "Efficient modeling", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.04455", "pdf": "https://arxiv.org/pdf/2507.04455.pdf", "abs": "https://arxiv.org/abs/2507.04455", "title": "GradOT: Training-free Gradient-preserving Offsite-tuning for Large Language Models", "authors": ["Kai Yao", "Zhaorui Tan", "Penglei Gao", "Lichun Li", "Kaixin Wu", "Yinggui Wang", "Yuan Zhao", "Yixin Ji", "Wei Wang", "Jianke Zhu"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "The rapid growth of large language models (LLMs) with traditional centralized\nfine-tuning emerges as a key technique for adapting these models to\ndomain-specific challenges, yielding privacy risks for both model and data\nowners. One promising solution, called offsite-tuning (OT), is proposed to\naddress these challenges, where a weaker emulator is compressed from the\noriginal model and further fine-tuned with adapter to enhance privacy. However,\nthe existing OT-based methods require high computational costs and lack\ntheoretical analysis. This paper introduces a novel OT approach based on\ngradient-preserving compression, named GradOT. By analyzing the OT problem\nthrough the lens of optimization, we propose a method that selectively applies\ncompression techniques such as rank compression and channel pruning, preserving\nthe gradients of fine-tuned adapters while ensuring privacy. Extensive\nexperiments demonstrate that our approach surpasses existing OT methods, both\nin terms of privacy protection and model performance. Our method provides a\ntheoretical foundation for OT and offers a practical, training-free solution\nfor offsite-tuning of large-scale LLMs.", "AI": {"tldr": "Introduction of GradOT, an innovative offsite-tuning method for large language models that enhances privacy while maintaining model performance.", "motivation": "To address privacy risks associated with centralized fine-tuning of large language models (LLMs) while minimizing computational costs.", "method": "GradOT uses gradient-preserving compression techniques including rank compression and channel pruning to enhance offsite-tuning of LLMs.", "result": "GradOT outperforms existing offsite-tuning methods in both privacy protection and model performance through its novel optimization framework.", "conclusion": "GradOT provides a practical, training-free solution for offsite-tuning of large-scale LLMs with a strong theoretical underpinning.", "key_contributions": ["Introduction of GradOT for offsite-tuning", "Analysis of offsite-tuning through optimization", "Empirical evidence of performance and privacy improvement"], "limitations": "Existing methods still require high computational costs; additional theoretical exploration may be needed.", "keywords": ["large language models", "offsite-tuning", "privacy", "compression techniques", "gradient-preserving"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04458", "pdf": "https://arxiv.org/pdf/2507.04458.pdf", "abs": "https://arxiv.org/abs/2507.04458", "title": "Think Twice Before You Judge: Mixture of Dual Reasoning Experts for Multimodal Sarcasm Detection", "authors": ["Soumyadeep Jana", "Abhrajyoti Kundu", "Sanasam Ranbir Singh"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal sarcasm detection has attracted growing interest due to the rise\nof multimedia posts on social media. Understanding sarcastic image-text posts\noften requires external contextual knowledge, such as cultural references or\ncommonsense reasoning. However, existing models struggle to capture the deeper\nrationale behind sarcasm, relying mainly on shallow cues like image captions or\nobject-attribute pairs from images. To address this, we propose \\textbf{MiDRE}\n(\\textbf{Mi}xture of \\textbf{D}ual \\textbf{R}easoning \\textbf{E}xperts), which\nintegrates an internal reasoning expert for detecting incongruities within the\nimage-text pair and an external reasoning expert that utilizes structured\nrationales generated via Chain-of-Thought prompting to a Large Vision-Language\nModel. An adaptive gating mechanism dynamically weighs the two experts,\nselecting the most relevant reasoning path. Experiments on two benchmark\ndatasets show that MiDRE achieves superior performance over baselines. Various\nqualitative analyses highlight the crucial role of external rationales,\nrevealing that even when they are occasionally noisy, they provide valuable\ncues that guide the model toward a better understanding of sarcasm.", "AI": {"tldr": "MiDRE detects sarcasm in multimodal posts by integrating internal and external reasoning experts, improving understanding of sarcastic content.", "motivation": "To improve sarcasm detection in multimedia posts which often rely on external knowledge and deeper reasoning beyond shallow cues.", "method": "MiDRE combines an internal reasoning expert that identifies incongruities in image-text pairs with an external reasoning expert that employs Chain-of-Thought prompts to a Large Vision-Language Model, using an adaptive gating mechanism for optimal reasoning path selection.", "result": "MiDRE outperforms existing models on two benchmark datasets for sarcasm detection, demonstrating the benefits of incorporating external rationales.", "conclusion": "The integration of external rationales is crucial for enhancing sarcasm detection, proving valuable even if occasionally noisy.", "key_contributions": ["Introduction of MiDRE model for sarcasm detection", "Combination of internal and external reasoning approaches", "Demonstrated performance improvement on sarcasm detection tasks"], "limitations": "", "keywords": ["sarcasm detection", "multimodal", "Chain-of-Thought", "reasoning experts", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.04468", "pdf": "https://arxiv.org/pdf/2507.04468.pdf", "abs": "https://arxiv.org/abs/2507.04468", "title": "Dual Modality-Aware Gated Prompt Tuning for Few-Shot Multimodal Sarcasm Detection", "authors": ["Soumyadeep Jana", "Abhrajyoti Kundu", "Sanasam Ranbir Singh"], "categories": ["cs.CL"], "comment": null, "summary": "The widespread use of multimodal content on social media has heightened the\nneed for effective sarcasm detection to improve opinion mining. However,\nexisting models rely heavily on large annotated datasets, making them less\nsuitable for real-world scenarios where labeled data is scarce. This motivates\nthe need to explore the problem in a few-shot setting. To this end, we\nintroduce DMDP (Deep Modality-Disentangled Prompt Tuning), a novel framework\nfor few-shot multimodal sarcasm detection. Unlike prior methods that use\nshallow, unified prompts across modalities, DMDP employs gated,\nmodality-specific deep prompts for text and visual encoders. These prompts are\ninjected across multiple layers to enable hierarchical feature learning and\nbetter capture diverse sarcasm types. To enhance intra-modal learning, we\nincorporate a prompt-sharing mechanism across layers, allowing the model to\naggregate both low-level and high-level semantic cues. Additionally, a\ncross-modal prompt alignment module enables nuanced interactions between image\nand text representations, improving the model's ability to detect subtle\nsarcastic intent. Experiments on two public datasets demonstrate DMDP's\nsuperior performance in both few-shot and extremely low-resource settings.\nFurther cross-dataset evaluations show that DMDP generalizes well across\ndomains, consistently outperforming baseline methods.", "AI": {"tldr": "A novel few-shot framework for multimodal sarcasm detection is introduced, leveraging gated modality-specific prompts and cross-modal alignment to improve performance.", "motivation": "The need for effective sarcasm detection in social media due to the scarcity of large annotated datasets for training models.", "method": "DMDP (Deep Modality-Disentangled Prompt Tuning) employs gated, modality-specific deep prompts injected across multiple layers, along with a prompt-sharing mechanism and a cross-modal prompt alignment module.", "result": "DMDP demonstrates superior performance in few-shot and low-resource settings on two public datasets, outperforming baseline methods across domains.", "conclusion": "The framework effectively improves sarcasm detection by capturing diverse types through better feature learning and interaction between text and visual modalities.", "key_contributions": ["Introduction of DMDP for few-shot multimodal sarcasm detection", "Employs gated modality-specific prompts for enhanced feature learning", "Incorporates cross-modal interactions for improved detection accuracy"], "limitations": "", "keywords": ["sarcasm detection", "few-shot learning", "multimodal", "prompt tuning", "machine learning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.04504", "pdf": "https://arxiv.org/pdf/2507.04504.pdf", "abs": "https://arxiv.org/abs/2507.04504", "title": "Unveiling the Potential of Diffusion Large Language Model in Controllable Generation", "authors": ["Zhen Xiong", "Yujun Cai", "Zhecheng Li", "Yiwei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Diffusion models, originally developed for image generation, have emerged as\na promising alternative to autoregressive large language models (LLMs). We\npresent a theoretical analysis comparing autoregressive and masked diffusion\nLLMs, revealing that the intrinsic bidirectional attention mechanism of\ndiffusion LLMs (dLLMs) enables superior context modeling and generation\ncontrollability. However, existing dLLM applications face significant\nchallenges in controllable generation: the native multi-step denoising process\nexhibits high sensitivity to sequence length, elevated hallucination rates, and\nprohibitive inference costs without specialized optimizations. To address these\nlimitations, we propose \\textbf{S}elf-adaptive \\textbf{S}chema\n\\textbf{S}caffolding ($S^3$), a novel framework that enables dLLMs to generate\nstructured outputs (e.g., JSON) while maintaining semantic fidelity and\naccelerating inference. Our approach injects the target schema structure into\nthe output context, reducing unnecessary computation while improving\ncontrollability. Extensive experiments demonstrate that $S^3$ achieves\nsubstantial improvements: 65\\% increase in structural adherence, 48\\%\nenhancement in content fidelity, and 17\\% reduction in hallucination rates\ncompared to baseline. These results establish both theoretical foundations and\npractical pathways for deploying diffusion models in controllable text\ngeneration tasks. Code and data will be publicly released.", "AI": {"tldr": "The paper introduces Self-adaptive Schema Scaffolding ($S^3$), a framework enhancing diffusion models for structured text generation by addressing controllability and inference challenges.", "motivation": "Diffusion models have potential as an alternative to autoregressive LLMs, but face significant challenges in controllable generation, particularly issues with sequence length sensitivity, hallucinations, and inference costs.", "method": "The proposed $S^3$ framework integrates target schema structure into the output context of dLLMs, improving efficiency and output quality.", "result": "$S^3$ improves structural adherence by 65%, content fidelity by 48%, and reduces hallucination rates by 17% compared to baseline models.", "conclusion": "The study establishes theoretical and practical benefits of using $S^3$ for applying diffusion models in controlled text generation tasks, providing new pathways for research and application.", "key_contributions": ["Introduction of the $S^3$ framework for structured output generation.", "Demonstration of significant performance improvements in key aspects of controllable generation.", "Theoretical foundations laid for deploying diffusion models in text tasks."], "limitations": "Challenges remain in optimizing the multi-step denoising process for varying sequence lengths and refining hallucination reduction.", "keywords": ["Diffusion models", "Large language models", "Controllable generation", "Schema scaffolding", "Natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.04508", "pdf": "https://arxiv.org/pdf/2507.04508.pdf", "abs": "https://arxiv.org/abs/2507.04508", "title": "AdS: Adapter-state Sharing Framework for Multimodal Sarcasm Detection", "authors": ["Soumyadeep Jana", "Sahil Danayak", "Sanasam Ranbir Singh"], "categories": ["cs.CL"], "comment": null, "summary": "The growing prevalence of multimodal image-text sarcasm on social media poses\nchallenges for opinion mining, especially under resource constraints. Existing\napproaches rely on full fine-tuning of large pre-trained models, making them\nunsuitable for low-resource settings. While recent parameter-efficient\nfine-tuning (PEFT) methods offer promise, their off-the-shelf use underperforms\non complex tasks like sarcasm detection. We propose AdS (Adapter-State\nSharing), a lightweight framework built on CLIP that inserts adapters only in\nthe upper layers and introduces a novel adapter-state sharing mechanism, where\ntextual adapters guide visual ones. This design promotes efficient cross-modal\nlearning while preserving low-level unimodal representations. Experiments on\ntwo public benchmarks demonstrate that AdS achieves state-of-the-art results\nusing significantly fewer trainable parameters than existing PEFT and full\nfine-tuning approaches.", "AI": {"tldr": "AdS is a lightweight framework for multimodal image-text sarcasm detection that employs adapter-state sharing to enhance cross-modal learning with fewer trainable parameters.", "motivation": "To address the challenges of sarcasm detection in opinion mining on social media, especially in low-resource settings.", "method": "AdS incorporates adapters in the upper layers of the CLIP model and utilizes a novel mechanism where textual adapters inform visual adapters to facilitate efficient learning.", "result": "AdS outperforms existing methods in sarcasm detection on two benchmarks while using significantly fewer trainable parameters.", "conclusion": "The proposed AdS framework demonstrates the potential for effective multimodal learning in constrained resource environments.", "key_contributions": ["Introduction of AdS for sarcasm detection", "Adapter-state sharing mechanism", "State-of-the-art performance with fewer parameters"], "limitations": "", "keywords": ["sarcasm detection", "multimodal", "adapter-state sharing", "CLIP", "parameter-efficient learning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.04531", "pdf": "https://arxiv.org/pdf/2507.04531.pdf", "abs": "https://arxiv.org/abs/2507.04531", "title": "DP-Fusion: Token-Level Differentially Private Inference for Large Language Models", "authors": ["Rushil Thareja", "Preslav Nakov", "Praneeth Vepakomma", "Nils Lukas"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Our code and data are publicly available here:\n  https://github.com/MBZUAI-Trustworthy-ML/DP-Fusion-DPI", "summary": "Large language models (LLMs) can leak sensitive information from their\ncontext through generated outputs, either accidentally or when prompted\nadversarially. Existing defenses that aim to preserve context privacy during\ninference either lack formal guarantees or suffer from a poor utility/privacy\ntrade-off. We propose DP-Fusion, a token-level Differentially Private Inference\n(DPI) mechanism that provably bounds how much an LLM's outputs reveal about\nsensitive tokens in its context. We demonstrate DPI through the task of\ndocument privatization, where the goal is to paraphrase documents so that\nsensitive content (e.g., Personally Identifiable Information, PII) cannot be\nreliably inferred, while still preserving the overall utility of the text. This\nis controlled by a parameter $\\epsilon$: $\\epsilon=0$ hides PII entirely, while\nhigher values trade off privacy for improved paraphrase quality. DP-Fusion\nworks as follows: (i) partition sensitive tokens into disjoint privacy groups,\n(ii) run the LLM once per group, and (iii) blend the output distributions so\nthat the final output remains within a fixed statistical distance of the\nbaseline distribution produced when no privacy group is revealed. This approach\nallows fine-grained control over the privacy/utility trade-off but requires\nmultiple LLM forward passes.", "AI": {"tldr": "DP-Fusion is a mechanism for differentially private inference in LLMs, aimed at protecting sensitive information while maintaining output utility.", "motivation": "To provide a solution that preserves context privacy in LLMs without compromising on utility, addressing the shortcomings of existing methods.", "method": "Implement a token-level Differentially Private Inference (DPI) that includes partitioning sensitive tokens into groups, running the LLM for each group, and blending outputs within a statistical distance of the original distribution.", "result": "Demonstrated through document privatization, achieving a balance between privacy and paraphrase quality, where privacy can be adjusted through a parameter ε.", "conclusion": "DP-Fusion offers a robust mechanism for safeguarding sensitive information in LLM outputs while allowing control over the privacy-utility trade-off through a parameterized approach.", "key_contributions": ["Introduces DP-Fusion for differentially private inference in LLMs", "Establishes a formal privacy guarantee that balances privacy and utility", "Proposes a novel method for document privatization that protects PII while maintaining text quality"], "limitations": "Requires multiple forward passes of the LLM which may impact efficiency.", "keywords": ["Differential Privacy", "Large Language Models", "Information Privacy", "Text Quality", "Document Privatization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.04569", "pdf": "https://arxiv.org/pdf/2507.04569.pdf", "abs": "https://arxiv.org/abs/2507.04569", "title": "Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts", "authors": ["Guokan Shang", "Hadi Abdine", "Ahmad Chamma", "Amr Mohamed", "Mohamed Anwar", "Abdelaziz Bounhar", "Omar El Herraoui", "Preslav Nakov", "Michalis Vazirgiannis", "Eric Xing"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for\nEgyptian dialect, uniquely designed to understand and generate texts written in\nboth Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we\nintroduce a novel language adaptation approach by leveraging the\nBranch-Train-MiX strategy to merge script-specialized experts, into a single\nMoE model. Our Nile-Chat models significantly outperform leading multilingual\nand Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced\nEgyptian evaluation benchmarks, which span both understanding and generative\ntasks. Notably, our 12B model yields a 14.4% performance gain over\nQwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly\navailable. We believe this work presents a comprehensive methodology for\nadapting LLMs to dual-script languages, addressing an often overlooked aspect\nin modern LLM development.", "AI": {"tldr": "Introduction of Nile-Chat LLMs for Egyptian dialect with novel language adaptation techniques.", "motivation": "To address the lack of effective LLMs for dual-script languages like Egyptian dialect, specifically for texts in Arabic and Latin scripts.", "method": "Leveraging a Branch-Train-MiX strategy to merge script-specialized experts into a single MoE model for language adaptation.", "result": "Nile-Chat models significantly outperform existing multilingual and Arabic LLMs, achieving notable performance gains especially in understanding and generative tasks on specific benchmarks.", "conclusion": "The proposed methodology for adapting LLMs to dual-script languages represents a notable advancement in LLM development, providing publicly available resources for further research.", "key_contributions": ["Introduction of Nile-Chat-4B, 3x4B-A6B, and 12B LLMs for Egyptian dialect.", "Novel Branch-Train-MiX strategy for merging script-specialized experts.", "Significant improvement over leading LLMs in multilingual benchmarks."], "limitations": "", "keywords": ["Nile-Chat", "LLM", "Egyptian dialect", "language adaptation", "dual-script"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.04607", "pdf": "https://arxiv.org/pdf/2507.04607.pdf", "abs": "https://arxiv.org/abs/2507.04607", "title": "PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes", "authors": ["Xinliang Frederick Zhang", "Nick Beauchamp", "Lu Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language model (LLM) personalization aims to align model outputs with\nindividuals' unique preferences and opinions. While recent efforts have\nimplemented various personalization methods, a unified theoretical framework\nthat can systematically understand the drivers of effective personalization is\nstill lacking. In this work, we integrate the well-established cognitive\ndual-memory model into LLM personalization, by mirroring episodic memory to\nhistorical user engagements and semantic memory to long-term, evolving user\nbeliefs. Specifically, we systematically investigate memory instantiations and\nintroduce a unified framework, PRIME, using episodic and semantic memory\nmechanisms. We further augment PRIME with a novel personalized thinking\ncapability inspired by the slow thinking strategy. Moreover, recognizing the\nabsence of suitable benchmarks, we introduce a dataset using Change My View\n(CMV) from Reddit, specifically designed to evaluate long-context\npersonalization. Extensive experiments validate PRIME's effectiveness across\nboth long- and short-context scenarios. Further analysis confirms that PRIME\neffectively captures dynamic personalization beyond mere popularity biases.", "AI": {"tldr": "This paper presents PRIME, a unified framework for LLM personalization that employs cognitive dual-memory models to enhance user-specific interactions and evaluates its effectiveness through a novel dataset.", "motivation": "Despite advances in LLM personalization, a comprehensive framework to understand effective personalization drivers remains absent.", "method": "The paper integrates cognitive dual-memory theory into LLM personalization by developing the PRIME framework, leveraging episodic and semantic memory to enhance user engagement and beliefs.", "result": "Extensive experiments demonstrate that PRIME significantly outperforms existing methods in both long- and short-context personalization scenarios.", "conclusion": "PRIME not only improves LLM personalization by employing cognitive models but also introduces a new dataset addressing long-context personalization challenges.", "key_contributions": ["Introduction of the PRIME framework for LLM personalization based on cognitive models.", "Augmentation of the PRIME framework with a personalized thinking capability.", "Development of a novel dataset for evaluating long-context personalization."], "limitations": "The framework's effectiveness is primarily based on the Change My View dataset, which may limit generalizability.", "keywords": ["LLM personalization", "cognitive memory", "PRIME framework", "evaluation dataset", "personalized thinking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.04612", "pdf": "https://arxiv.org/pdf/2507.04612.pdf", "abs": "https://arxiv.org/abs/2507.04612", "title": "Retain or Reframe? A Computational Framework for the Analysis of Framing in News Articles and Reader Comments", "authors": ["Matteo Guida", "Yulia Otmakhova", "Eduard Hovy", "Lea Frermann"], "categories": ["cs.CL"], "comment": null, "summary": "When a news article describes immigration as an \"economic burden\" or a\n\"humanitarian crisis,\" it selectively emphasizes certain aspects of the issue.\nAlthough \\textit{framing} shapes how the public interprets such issues,\naudiences do not absorb frames passively but actively reorganize the presented\ninformation. While this relationship between source content and audience\nresponse is well-documented in the social sciences, NLP approaches often ignore\nit, detecting frames in articles and responses in isolation. We present the\nfirst computational framework for large-scale analysis of framing across source\ncontent (news articles) and audience responses (reader comments).\nMethodologically, we refine frame labels and develop a framework that\nreconstructs dominant frames in articles and comments from sentence-level\npredictions, and aligns articles with topically relevant comments. Applying our\nframework across eleven topics and two news outlets, we find that frame reuse\nin comments correlates highly across outlets, while topic-specific patterns\nvary. We release a frame classifier that performs well on both articles and\ncomments, a dataset of article and comment sentences manually labeled for\nframes, and a large-scale dataset of articles and comments with predicted frame\nlabels.", "AI": {"tldr": "This paper introduces a computational framework for analyzing framing in news articles and audience comments, demonstrating that readers actively reinterpret frames and highlighting the correlation in frame reuse across different topics and news outlets.", "motivation": "The paper addresses the selective framing of immigration issues in news articles and the active role of audiences in reorganizing presented information, an aspect often overlooked by traditional NLP methods which analyze source content and audience response in isolation.", "method": "The authors refine frame labels and develop a framework that reconstructs dominant frames in both news articles and reader comments from sentence-level predictions, allowing for alignment of articles with relevant comments.", "result": "The study finds high correlation in frame reuse within comments across multiple news outlets, while also revealing topic-specific patterns in how articles and comments relate to each other.", "conclusion": "The framework and resources released facilitate large-scale analysis of framing, providing insights into audience engagement with media coverage of immigration topics.", "key_contributions": ["Development of a computational framework for analyzing framing in news articles and comments", "Release of a frame classifier that performs well on both articles and comments", "Creation of a dataset of manually labeled article and comment sentences for frame analysis"], "limitations": "", "keywords": ["framing", "NLP", "reader comments", "news articles", "immigration"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.04625", "pdf": "https://arxiv.org/pdf/2507.04625.pdf", "abs": "https://arxiv.org/abs/2507.04625", "title": "Knowledge-Aware Self-Correction in Language Models via Structured Memory Graphs", "authors": ["Swayamjit Saha"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "Large Language Models (LLMs) are powerful yet prone to generating factual\nerrors, commonly referred to as hallucinations. We present a lightweight,\ninterpretable framework for knowledge-aware self-correction of LLM outputs\nusing structured memory graphs based on RDF triples. Without retraining or\nfine-tuning, our method post-processes model outputs and corrects factual\ninconsistencies via external semantic memory. We demonstrate the approach using\nDistilGPT-2 and show promising results on simple factual prompts.", "AI": {"tldr": "A framework for self-correction of LLM outputs using structured memory graphs to address factual inaccuracies.", "motivation": "To mitigate the issue of factual errors (hallucinations) in Large Language Models (LLMs) by developing a method for knowledge-aware self-correction.", "method": "The proposed framework utilizes structured memory graphs based on RDF triples to post-process model outputs without the need for retraining or fine-tuning.", "result": "The method was applied to DistilGPT-2 and showed promising results in correcting factual inconsistencies in responses to simple factual prompts.", "conclusion": "This approach offers a novel way to enhance the factual accuracy of LLM outputs while maintaining interpretability and efficiency.", "key_contributions": ["Lightweight framework for self-correction of LLMs.", "Use of structured memory graphs for post-processing outputs.", "Demonstrates improved factual accuracy without retraining."], "limitations": "The evaluation was conducted on simple factual prompts, which may not capture more complex inaccuracies.", "keywords": ["Large Language Models", "self-correction", "RDF triples", "semantic memory", "hallucinations"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04636", "pdf": "https://arxiv.org/pdf/2507.04636.pdf", "abs": "https://arxiv.org/abs/2507.04636", "title": "Put Teacher in Student's Shoes: Cross-Distillation for Ultra-compact Model Compression Framework", "authors": ["Maolin Wang", "Jun Chu", "Sicong Xie", "Xiaoling Zang", "Yao Zhao", "Wenliang Zhong", "Xiangyu Zhao"], "categories": ["cs.CL"], "comment": "Accepted by KDD 2025", "summary": "In the era of mobile computing, deploying efficient Natural Language\nProcessing (NLP) models in resource-restricted edge settings presents\nsignificant challenges, particularly in environments requiring strict privacy\ncompliance, real-time responsiveness, and diverse multi-tasking capabilities.\nThese challenges create a fundamental need for ultra-compact models that\nmaintain strong performance across various NLP tasks while adhering to\nstringent memory constraints. To this end, we introduce Edge ultra-lIte BERT\nframework (EI-BERT) with a novel cross-distillation method. EI-BERT efficiently\ncompresses models through a comprehensive pipeline including hard token\npruning, cross-distillation and parameter quantization. Specifically, the\ncross-distillation method uniquely positions the teacher model to understand\nthe student model's perspective, ensuring efficient knowledge transfer through\nparameter integration and the mutual interplay between models. Through\nextensive experiments, we achieve a remarkably compact BERT-based model of only\n1.91 MB - the smallest to date for Natural Language Understanding (NLU) tasks.\nThis ultra-compact model has been successfully deployed across multiple\nscenarios within the Alipay ecosystem, demonstrating significant improvements\nin real-world applications. For example, it has been integrated into Alipay's\nlive Edge Recommendation system since January 2024, currently serving the app's\nrecommendation traffic across \\textbf{8.4 million daily active devices}.", "AI": {"tldr": "The paper introduces Edge ultra-lIte BERT (EI-BERT), a compact NLP model designed for resource-restricted edge settings, utilizing a novel cross-distillation method for efficient knowledge transfer and deployment.", "motivation": "There is a need for ultra-compact NLP models that perform well while adhering to memory constraints and privacy requirements in mobile computing environments.", "method": "The EI-BERT framework utilizes hard token pruning, cross-distillation, and parameter quantization to compress models effectively.", "result": "The developed EI-BERT model is only 1.91 MB, making it the smallest BERT-based model for NLU tasks, with successful deployment in Alipay's Edge Recommendation system.", "conclusion": "The EI-BERT model demonstrates significant real-world application improvements and is effective in scenarios requiring real-time responsiveness.", "key_contributions": ["Introduction of the EI-BERT framework for compact NLP model deployment", "Development of a novel cross-distillation method for efficient knowledge transfer", "Successful integration and performance improvement in real-world applications like Alipay."], "limitations": "", "keywords": ["Natural Language Processing", "edge computing", "cross-distillation", "model compression", "privacy compliance"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2507.04642", "pdf": "https://arxiv.org/pdf/2507.04642.pdf", "abs": "https://arxiv.org/abs/2507.04642", "title": "R1-RE: Cross-Domain Relationship Extraction with RLVR", "authors": ["Runpeng Dai", "Tong Zheng", "Run Yang", "Hongtu Zhu"], "categories": ["cs.CL"], "comment": "14 pages, 7 figures", "summary": "Relationship extraction (RE) is a core task in natural language processing.\nTraditional approaches typically frame RE as a supervised learning problem,\ndirectly mapping context to labels-an approach that often suffers from poor\nout-of-domain (OOD) generalization. Inspired by the workflow of human\nannotators, we reframe RE as a reasoning task guided by annotation guidelines\nand introduce R1-RE, the first reinforcement learning with verifiable reward\n(RLVR) framework for RE tasks. Our method elicits the reasoning abilities of\nsmall language models for annotation tasks, resulting in significantly improved\nOOD robustness. We evaluate our approach on the public Sem-2010 dataset and a\nprivate MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of\napproximately 70%, on par with leading proprietary models such as GPT-4o.\nAdditionally, our comprehensive analysis provides novel insights into the\ntraining dynamics and emergent reasoning behaviors of the RLVR paradigm for RE.", "AI": {"tldr": "R1-RE is a novel framework that applies reinforcement learning to relationship extraction, improving out-of-domain generalization and showcasing enhanced reasoning abilities in language models.", "motivation": "Traditional relationship extraction methods struggle with out-of-domain generalization; this work aims to improve those limitations by using a reasoning-based approach that resembles human annotators.", "method": "The authors introduce R1-RE, a reinforcement learning framework with verifiable rewards, to guide small language models in relationship extraction tasks according to annotation guidelines.", "result": "The R1-RE-7B model achieves an average out-of-domain accuracy of approximately 70%, comparable to leading proprietary models such as GPT-4o, indicating improved robustness in performance.", "conclusion": "The proposed framework not only enhances the accuracy of relationship extraction but also provides insights into the reasoning processes and training dynamics of language models.", "key_contributions": ["Introduction of R1-RE framework for relationship extraction using reinforcement learning.", "Achievement of significant improvements in out-of-domain generalization in RE tasks.", "Novel insights into training dynamics and reasoning behaviors of small language models."], "limitations": "Limited to specific datasets; further validation on more diverse datasets needed.", "keywords": ["relationship extraction", "reinforcement learning", "out-of-domain generalization", "language models", "natural language processing"], "importance_score": 7, "read_time_minutes": 14}}
{"id": "2507.04701", "pdf": "https://arxiv.org/pdf/2507.04701.pdf", "abs": "https://arxiv.org/abs/2507.04701", "title": "XiYan-SQL: A Novel Multi-Generator Framework For Text-to-SQL", "authors": ["Yifu Liu", "Yin Zhu", "Yingqi Gao", "Zhiling Luo", "Xiaoxia Li", "Xiaorong Shi", "Yuntao Hong", "Jinyang Gao", "Yu Li", "Bolin Ding", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "To leverage the advantages of LLM in addressing challenges in the Text-to-SQL\ntask, we present XiYan-SQL, an innovative framework effectively generating and\nutilizing multiple SQL candidates. It consists of three components: 1) a Schema\nFilter module filtering and obtaining multiple relevant schemas; 2) a\nmulti-generator ensemble approach generating multiple highquality and diverse\nSQL queries; 3) a selection model with a candidate reorganization strategy\nimplemented to obtain the optimal SQL query. Specifically, for the\nmulti-generator ensemble, we employ a multi-task fine-tuning strategy to\nenhance the capabilities of SQL generation models for the intrinsic alignment\nbetween SQL and text, and construct multiple generation models with distinct\ngeneration styles by fine-tuning across different SQL formats. The experimental\nresults and comprehensive analysis demonstrate the effectiveness and robustness\nof our framework. Overall, XiYan-SQL achieves a new SOTA performance of 75.63%\non the notable BIRD benchmark, surpassing all previous methods. It also attains\nSOTA performance on the Spider test set with an accuracy of 89.65%.", "AI": {"tldr": "XiYan-SQL is a framework that generates and uses multiple SQL candidates to improve Text-to-SQL tasks.", "motivation": "The need for better Text-to-SQL performance in leveraging LLMs.", "method": "XiYan-SQL consists of three components: 1) a Schema Filter to obtain relevant schemas, 2) a multi-generator ensemble for diverse SQL queries, and 3) a selection model for optimal SQL query selection, utilizing multi-task fine-tuning.", "result": "Achieves state-of-the-art performance of 75.63% on the BIRD benchmark and 89.65% accuracy on the Spider test set.", "conclusion": "XiYan-SQL demonstrates effectiveness and robustness in generating high-quality SQL queries.", "key_contributions": ["Introduction of a multi-generator ensemble approach for SQL generation.", "Implementation of a Schema Filter to filter relevant schemas.", "Achievement of new state-of-the-art performance on Text-to-SQL benchmarks."], "limitations": "", "keywords": ["Text-to-SQL", "LLM", "SQL generation", "multi-generator ensemble", "schema filtering"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.04708", "pdf": "https://arxiv.org/pdf/2507.04708.pdf", "abs": "https://arxiv.org/abs/2507.04708", "title": "Why We Feel What We Feel: Joint Detection of Emotions and Their Opinion Triggers in E-commerce", "authors": ["Arnav Attri", "Anuj Attri", "Pushpak Bhattacharyya", "Suman Banerjee", "Amey Patil", "Muthusamy Chelliah", "Nikesh Garera"], "categories": ["cs.CL", "I.2.7; H.3.1; I.2.6"], "comment": "23 pages, 11 figures, 7 tables. Dataset and code will be made\n  publicly available", "summary": "Customer reviews on e-commerce platforms capture critical affective signals\nthat drive purchasing decisions. However, no existing research has explored the\njoint task of emotion detection and explanatory span identification in\ne-commerce reviews - a crucial gap in understanding what triggers customer\nemotional responses. To bridge this gap, we propose a novel joint task unifying\nEmotion detection and Opinion Trigger extraction (EOT), which explicitly models\nthe relationship between causal text spans (opinion triggers) and affective\ndimensions (emotion categories) grounded in Plutchik's theory of 8 primary\nemotions. In the absence of labeled data, we introduce EOT-X, a human-annotated\ncollection of 2,400 reviews with fine-grained emotions and opinion triggers. We\nevaluate 23 Large Language Models (LLMs) and present EOT-DETECT, a structured\nprompting framework with systematic reasoning and self-reflection. Our\nframework surpasses zero-shot and chain-of-thought techniques, across\ne-commerce domains.", "AI": {"tldr": "This paper introduces a joint task for emotion detection and opinion trigger extraction in e-commerce reviews, proposing a novel framework for modeling relationships between emotions and their triggers based on Plutchik's theory.", "motivation": "Understanding customer emotional responses in e-commerce requires identifying the relationship between emotion and opinion triggers in reviews, which has not been previously addressed.", "method": "The research proposes a joint task of Emotion detection and Opinion Trigger extraction (EOT) and introduces EOT-X, a novel dataset of 2,400 human-annotated reviews. It evaluates 23 LLMs and presents the EOT-DETECT framework for improved performance.", "result": "The proposed framework, EOT-DETECT, demonstrates superior performance compared to zero-shot and chain-of-thought approaches across e-commerce domains.", "conclusion": "The study establishes a new approach to analyzing customer reviews by integrating emotion detection with opinion trigger extraction, highlighting the effectiveness of the proposed methods.", "key_contributions": ["Introduction of EOT joint task for emotion and opinion trigger extraction", "Creation of EOT-X dataset for training and evaluation", "Development of EOT-DETECT prompting framework that outperforms existing methods"], "limitations": "The study may be limited by the specific e-commerce context and the generalizability of the results to other domains.", "keywords": ["Emotion detection", "Opinion trigger extraction", "E-commerce reviews", "Large Language Models", "Dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.04723", "pdf": "https://arxiv.org/pdf/2507.04723.pdf", "abs": "https://arxiv.org/abs/2507.04723", "title": "LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation framework", "authors": ["Zecheng Tang", "Haitian Wang", "Quantong Qiu", "Baibei Ji", "Ruoxi Sun", "Keyan Zhou", "Juntao Li", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Long-context processing has become a fundamental capability for large\nlanguage models~(LLMs). To assess model's long-context performance, numerous\nlong-context evaluation benchmarks have been proposed. However, variations in\nevaluation settings across these benchmarks lead to inconsistent results,\nmaking it difficult to draw reliable comparisons. Besides, the high\ncomputational cost of long-context evaluation poses a significant barrier for\nthe community to conduct comprehensive assessments of long-context models. In\nthis paper, we propose LOOM-Scope, a comprehensive and efficient framework for\nlong-context evaluation. LOOM-Scope standardizes evaluation settings across\ndiverse benchmarks, supports deployment of efficient long-context inference\nacceleration methods, and introduces a holistic yet lightweight benchmark suite\nto evaluate models comprehensively. Homepage: https://loomscope.github.io", "AI": {"tldr": "This paper introduces LOOM-Scope, a framework designed to standardize long-context evaluation for large language models (LLMs) and address the challenges of inconsistent results and high computational costs in existing benchmarks.", "motivation": "The growing importance of long-context processing in large language models and the inconsistencies in evaluation results due to varying benchmarks necessitated a unified approach for better assessment.", "method": "LOOM-Scope offers a standardized evaluation framework alongside efficient methods for long-context inference and a lightweight benchmark suite for comprehensive model evaluation.", "result": "The framework enables consistent comparisons across models, reduces computational costs for evaluations, and allows for more holistic assessments of LLM performance on long-context tasks.", "conclusion": "LOOM-Scope addresses the critical challenge of evaluating long-context capabilities in LLMs, facilitating improved comparison and assessment and enabling wider participation in the evaluation process.", "key_contributions": ["Standardizes evaluation settings across long-context benchmarks.", "Introduces efficient inference acceleration methods for LLMs.", "Provides a lightweight benchmark suite for comprehensive model evaluation."], "limitations": "", "keywords": ["Long-context evaluation", "Large language models", "Benchmarking framework"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.04733", "pdf": "https://arxiv.org/pdf/2507.04733.pdf", "abs": "https://arxiv.org/abs/2507.04733", "title": "\"This Suits You the Best\": Query Focused Comparative Explainable Summarization", "authors": ["Arnav Attri", "Anuj Attri", "Pushpak Bhattacharyya", "Suman Banerjee", "Amey Patil", "Muthusamy Chelliah", "Nikesh Garera"], "categories": ["cs.CL", "cs.IR", "H.3.1; I.2.7; H.1.2"], "comment": null, "summary": "Product recommendations inherently involve comparisons, yet traditional\nopinion summarization often fails to provide holistic comparative insights. We\npropose the novel task of generating Query-Focused Comparative Explainable\nSummaries (QF-CES) using Multi-Source Opinion Summarization (M-OS). To address\nthe lack of query-focused recommendation datasets, we introduce MS-Q2P,\ncomprising 7,500 queries mapped to 22,500 recommended products with metadata.\nWe leverage Large Language Models (LLMs) to generate tabular comparative\nsummaries with query-specific explanations. Our approach is personalized,\nprivacy-preserving, recommendation engine-agnostic, and category-agnostic. M-OS\nas an intermediate step reduces inference latency approximately by 40% compared\nto the direct input approach (DIA), which processes raw data directly. We\nevaluate open-source and proprietary LLMs for generating and assessing QF-CES.\nExtensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity,\nfaithfulness, informativeness, format adherence, and query relevance) showed an\naverage Spearman correlation of 0.74 with human judgments, indicating its\npotential for QF-CES evaluation.", "AI": {"tldr": "This paper introduces a novel task for generating Query-Focused Comparative Explainable Summaries (QF-CES) using Multi-Source Opinion Summarization (M-OS), addressing the limitation of traditional opinion summarization in providing holistic comparative insights.", "motivation": "The paper addresses the lack of comprehensive comparative insights in product recommendations, which often rely on traditional opinion summarization methods that do not focus on user queries.", "method": "The authors propose generating QF-CES through a dataset called MS-Q2P, consisting of 7,500 queries linked to 22,500 recommended products. They utilize Large Language Models (LLMs) to produce tabular comparative summaries containing query-specific explanations, achieving a reduction in inference latency by approximately 40% through the use of M-OS over the direct input approach.", "result": "Evaluation of the proposed method shows an average Spearman correlation of 0.74 with human judgments across five quality dimensions, highlighting its effectiveness in generating QF-CES.", "conclusion": "The proposed approach offers a personalized, privacy-preserving, and efficient solution to the challenge of generating explainable summaries in product recommendations that are relevant to specific user queries.", "key_contributions": ["Introduction of Query-Focused Comparative Explainable Summaries (QF-CES)", "Development of a new dataset MS-Q2P with 7,500 queries and 22,500 products", "Demonstration of reduced inference latency of 40% using M-OS."], "limitations": "The study is limited to certain types of queries and product categories due to the design of the dataset.", "keywords": ["comparative summaries", "recommendation systems", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.04738", "pdf": "https://arxiv.org/pdf/2507.04738.pdf", "abs": "https://arxiv.org/abs/2507.04738", "title": "Word stress in self-supervised speech models: A cross-linguistic comparison", "authors": ["Martijn Bentum", "Louis ten Bosch", "Tomas O. Lentz"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Interspeech 2025", "summary": "In this paper we study word stress representations learned by self-supervised\nspeech models (S3M), specifically the Wav2vec 2.0 model. We investigate the S3M\nrepresentations of word stress for five different languages: Three languages\nwith variable or lexical stress (Dutch, English and German) and two languages\nwith fixed or demarcative stress (Hungarian and Polish). We train diagnostic\nstress classifiers on S3M embeddings and show that they can distinguish between\nstressed and unstressed syllables in read-aloud short sentences with high\naccuracy. We also tested language-specificity effects of S3M word stress. The\nresults indicate that the word stress representations are language-specific,\nwith a greater difference between the set of variable versus the set of fixed\nstressed languages.", "AI": {"tldr": "This paper studies word stress representations learned by self-supervised speech models, particularly Wav2vec 2.0, across five languages.", "motivation": "To understand how self-supervised speech models represent word stress and its implications for different languages.", "method": "The study involved training diagnostic stress classifiers on S3M embeddings derived from Wav2vec 2.0, and evaluating their performance in distinguishing stressed from unstressed syllables.", "result": "The classifiers achieved high accuracy in distinguishing between stressed and unstressed syllables, demonstrating language-specific effects based on stress types.", "conclusion": "Word stress representations learned by the S3M are language-specific, with significant differences observed between variable and fixed stress languages.", "key_contributions": ["High accuracy of S3M embeddings in stress classification", "Demonstration of language-specific stress representation", "Comparison of variable and fixed stress languages"], "limitations": "", "keywords": ["word stress", "self-supervised speech models", "Wav2vec 2.0", "language-specificity", "stress classification"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.04746", "pdf": "https://arxiv.org/pdf/2507.04746.pdf", "abs": "https://arxiv.org/abs/2507.04746", "title": "A Tale of Two Scripts: Transliteration and Post-Correction for Judeo-Arabic", "authors": ["Juan Moreno Gonzalez", "Bashar Alhafni", "Nizar Habash"], "categories": ["cs.CL"], "comment": null, "summary": "Judeo-Arabic refers to Arabic variants historically spoken by Jewish\ncommunities across the Arab world, primarily during the Middle Ages. Unlike\nstandard Arabic, it is written in Hebrew script by Jewish writers and for\nJewish audiences. Transliterating Judeo-Arabic into Arabic script is\nchallenging due to ambiguous letter mappings, inconsistent orthographic\nconventions, and frequent code-switching into Hebrew and Aramaic. In this\npaper, we introduce a two-step approach to automatically transliterate\nJudeo-Arabic into Arabic script: simple character-level mapping followed by\npost-correction to address grammatical and orthographic errors. We also present\nthe first benchmark evaluation of LLMs on this task. Finally, we show that\ntransliteration enables Arabic NLP tools to perform morphosyntactic tagging and\nmachine translation, which would have not been feasible on the original texts.", "AI": {"tldr": "This paper presents a method for automatically transliterating Judeo-Arabic into Arabic script and evaluates LLM performance on this task.", "motivation": "The need to transliterate Judeo-Arabic for better accessibility and processing by Arabic NLP tools, considering its unique characteristics and the challenges involved.", "method": "A two-step approach involving simple character-level mapping followed by post-correction for grammatical and orthographic errors.", "result": "The approach was benchmarked against LLMs, demonstrating its ability to enable morphosyntactic tagging and machine translation for Arabic NLP tools.", "conclusion": "The transliteration of Judeo-Arabic into Arabic script enhances the applicability of Arabic NLP tools on these texts, overcoming initial processing challenges.", "key_contributions": ["Introduction of a two-step transliteration approach for Judeo-Arabic.", "First benchmark evaluation of LLMs on transliterating Judeo-Arabic.", "Demonstration of improved NLP tool performance through transliteration."], "limitations": "The paper does not address the challenges of dialectical variations within Judeo-Arabic.", "keywords": ["Judeo-Arabic", "transliteration", "Arabic NLP", "language models", "morphosyntactic tagging"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2507.04751", "pdf": "https://arxiv.org/pdf/2507.04751.pdf", "abs": "https://arxiv.org/abs/2507.04751", "title": "LLMs as Architects and Critics for Multi-Source Opinion Summarization", "authors": ["Anuj Attri", "Arnav Attri", "Pushpak Bhattacharyya", "Suman Banerjee", "Amey Patil", "Muthusamy Chelliah", "Nikesh Garera"], "categories": ["cs.CL", "I.2.7; H.3.1; I.2.6"], "comment": null, "summary": "Multi-source Opinion Summarization (M-OS) extends beyond traditional opinion\nsummarization by incorporating additional sources of product metadata such as\ndescriptions, key features, specifications, and ratings, alongside reviews.\nThis integration results in comprehensive summaries that capture both\nsubjective opinions and objective product attributes essential for informed\ndecision-making. While Large Language Models (LLMs) have shown significant\nsuccess in various Natural Language Processing (NLP) tasks, their potential in\nM-OS remains largely unexplored. Additionally, the lack of evaluation datasets\nfor this task has impeded further advancements. To bridge this gap, we\nintroduce M-OS-EVAL, a benchmark dataset for evaluating multi-source opinion\nsummaries across 7 key dimensions: fluency, coherence, relevance, faithfulness,\naspect coverage, sentiment consistency, specificity. Our results demonstrate\nthat M-OS significantly enhances user engagement, as evidenced by a user study\nin which, on average, 87% of participants preferred M-OS over opinion\nsummaries. Our experiments demonstrate that factually enriched summaries\nenhance user engagement. Notably, M-OS-PROMPTS exhibit stronger alignment with\nhuman judgment, achieving an average Spearman correlation of \\r{ho} = 0.74,\nwhich surpasses the performance of previous methodologies.", "AI": {"tldr": "The paper introduces Multi-source Opinion Summarization (M-OS), which incorporates product metadata with reviews to create comprehensive summaries. It presents the M-OS-EVAL benchmark dataset for evaluating these summaries and demonstrates enhanced user engagement through user studies.", "motivation": "To address the gap in traditional opinion summarization by integrating additional product metadata and improve user decision-making.", "method": "The paper presents Multi-source Opinion Summarization (M-OS), evaluated using a new benchmark dataset called M-OS-EVAL across seven key dimensions. User studies were conducted to measure engagement and preferences.", "result": "User studies showed that 87% of participants preferred M-OS summaries over traditional opinion summaries, indicating significantly enhanced user engagement. The M-OS-PROMPTS showed a Spearman correlation of 0.74 with human judgment.", "conclusion": "The integration of product metadata into opinion summaries enhances user engagement and provides a more informative decision-making tool for users.", "key_contributions": ["Introduction of Multi-source Opinion Summarization (M-OS) methodology", "Development of M-OS-EVAL benchmark dataset", "Demonstration of enhanced user engagement through comprehensive summaries"], "limitations": "", "keywords": ["Multi-source Opinion Summarization", "Large Language Models", "user engagement"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.04756", "pdf": "https://arxiv.org/pdf/2507.04756.pdf", "abs": "https://arxiv.org/abs/2507.04756", "title": "CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering", "authors": ["Hang Lv", "Sheng Liang", "Hao Wang", "Hongchao Gu", "Yaxiong Wu", "Wei Guo", "Defu Lian", "Yong Liu", "Enhong Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Personalized text generation has become crucial for adapting language models\nto diverse and evolving users' personal context across cultural, temporal, and\ncontextual dimensions. While existing methods often rely on centralized\nfine-tuning or static preference alignment, they struggle to achieve real-time\nadaptation under resource constraints inherent to personal devices. This\nlimitation creates a dilemma: large cloud-based models lack access to localized\nuser-specific information, while small on-device models cannot match the\ngeneration quality of their cloud counterparts. To address this dichotomy, we\npresent CoSteer, a novel collaborative framework that enables decoding-time\npersonalization through localized delta steering. Our key insight lies in\nleveraging the logits difference between personal context-aware and -agnostic\noutputs from local small models as steering signals for cloud-based LLMs.\nSpecifically, we formulate token-level optimization as an online learning\nproblem, where local delta vectors dynamically adjust the remote LLM's logits\nwithin the on-device environment. This approach preserves privacy by\ntransmitting only the final steered tokens rather than raw data or intermediate\nvectors, while maintaining cloud-based LLMs' general capabilities without\nfine-tuning. Through comprehensive experiments on various personalized\ngeneration tasks, we demonstrate that CoSteer effectively assists LLMs in\ngenerating personalized content by leveraging locally stored user profiles and\nhistories, ensuring privacy preservation through on-device data processing\nwhile maintaining acceptable computational overhead.", "AI": {"tldr": "CoSteer is a collaborative framework for real-time personalized text generation using localized delta steering, maintaining privacy and enhancing the performance of cloud-based LLMs without fine-tuning.", "motivation": "To address the limitations of existing personalized text generation methods that struggle with real-time adaptation and resource constraints on personal devices.", "method": "CoSteer uses localized delta steering, optimizing token-level outputs by leveraging the differences in logits from personal context-aware and agnostic outputs of local small models during decoding.", "result": "Experiments show that CoSteer significantly enhances the ability of LLMs to generate personalized text while preserving user privacy and minimizing computational resource demands.", "conclusion": "CoSteer effectively balances the need for personalized content generation with privacy preservation by processing data locally, making it a promising solution for on-device AI applications.", "key_contributions": ["Introduces CoSteer for decoding-time personalization of LLMs.", "Utilizes localized delta steering for real-time adaptation.", "Preserves user privacy by limiting data transmission to final steered tokens."], "limitations": "", "keywords": ["Personalized text generation", "collaborative framework", "localized delta steering", "privacy preservation", "LLMs"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.04782", "pdf": "https://arxiv.org/pdf/2507.04782.pdf", "abs": "https://arxiv.org/abs/2507.04782", "title": "Reason to Rote: Rethinking Memorization in Reasoning", "authors": ["Yupei Du", "Philipp Mondorf", "Silvia Casola", "Yuekun Yao", "Robert Litschko", "Barbara Plank"], "categories": ["cs.CL", "cs.LG"], "comment": "21 pages, 14 figures", "summary": "Large language models readily memorize arbitrary training instances, such as\nlabel noise, yet they perform strikingly well on reasoning tasks. In this work,\nwe investigate how language models memorize label noise, and why such\nmemorization in many cases does not heavily affect generalizable reasoning\ncapabilities. Using two controllable synthetic reasoning datasets with noisy\nlabels, four-digit addition (FDA) and two-hop relational reasoning (THR), we\ndiscover a reliance of memorization on generalizable reasoning mechanisms:\nmodels continue to compute intermediate reasoning outputs even when retrieving\nmemorized noisy labels, and intervening reasoning adversely affects\nmemorization. We further show that memorization operates through distributed\nencoding, i.e., aggregating various inputs and intermediate results, rather\nthan building a look-up mechanism from inputs to noisy labels. Moreover, our\nFDA case study reveals memorization occurs via outlier heuristics, where\nexisting neuron activation patterns are slightly shifted to fit noisy labels.\nTogether, our findings suggest that memorization of label noise in language\nmodels builds on, rather than overrides, the underlying reasoning mechanisms,\nshedding lights on the intriguing phenomenon of benign memorization.", "AI": {"tldr": "This paper investigates how large language models (LLMs) memorize noisy labels while maintaining general reasoning abilities, revealing that memorization supports rather than undermines reasoning tasks.", "motivation": "To understand the dynamics of memorization in large language models, particularly how it affects reasoning in the presence of label noise.", "method": "The study uses two synthetic reasoning datasets with noisy labels: four-digit addition (FDA) and two-hop relational reasoning (THR), to analyze memorization and reasoning interactions.", "result": "The research finds that memorization relies on generalizable reasoning mechanisms, with intermediate outputs computed even when retrieving noisy labels, indicating sophisticated handling of information.", "conclusion": "Memorization of label noise in language models enhances rather than disrupts reasoning capabilities, offering insights on benign memorization.", "key_contributions": ["Demonstrated that memorization is reliant on generalizable reasoning mechanisms.", "Showed that memory operates via distributed encoding instead of direct look-up mechanisms.", "Revealed that memorization occurs through outlier heuristics, implying slight adjustments to neuron activation patterns."], "limitations": "", "keywords": ["large language models", "memorization", "reasoning", "label noise", "distributed encoding"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.04793", "pdf": "https://arxiv.org/pdf/2507.04793.pdf", "abs": "https://arxiv.org/abs/2507.04793", "title": "A Survey of Pun Generation: Datasets, Evaluations and Methodologies", "authors": ["Yuchen Su", "Yonghua Zhu", "Ruofan Wang", "Zijian Huang", "Diana Benavides-Prado", "Michael Witbrock"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pun generation seeks to creatively modify linguistic elements in text to\nproduce humour or evoke double meanings. It also aims to preserve coherence and\ncontextual appropriateness, making it useful in creative writing and\nentertainment across various media and contexts. Although pun generation has\nreceived considerable attention in computational linguistics, there is\ncurrently no dedicated survey that systematically reviews this specific area.\nTo bridge this gap, this paper provides a comprehensive review of pun\ngeneration datasets and methods across different stages, including conventional\napproaches, deep learning techniques, and pre-trained language models.\nAdditionally, we summarise both automated and human evaluation metrics used to\nassess the quality of pun generation. Finally, we discuss the research\nchallenges and propose promising directions for future work.", "AI": {"tldr": "This paper reviews pun generation methods and datasets, highlighting techniques and evaluation metrics in computational linguistics.", "motivation": "To provide a systematic review of pun generation, which has not been previously addressed comprehensively.", "method": "The paper reviews existing datasets, conventional approaches, deep learning techniques, and pre-trained language models used for pun generation.", "result": "A comprehensive summary of pun generation datasets and methods is presented along with automated and human evaluation metrics.", "conclusion": "The paper identifies research challenges in pun generation and suggests directions for future work.", "key_contributions": ["Systematic review of pun generation datasets and methods", "Summary of evaluation metrics for pun quality", "Identification of research challenges and future directions"], "limitations": "The review may not cover all recent developments in pun generation; focus is primarily on existing methods and datasets.", "keywords": ["pun generation", "computational linguistics", "evaluation metrics"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.04841", "pdf": "https://arxiv.org/pdf/2507.04841.pdf", "abs": "https://arxiv.org/abs/2507.04841", "title": "Spec-TOD: A Specialized Instruction-Tuned LLM Framework for Efficient Task-Oriented Dialogue Systems", "authors": ["Quang-Vinh Nguyen", "Quang-Chieu Nguyen", "Hoang Pham", "Khac-Hoai Nam Bui"], "categories": ["cs.CL"], "comment": "Accepted at SIGdial 2025", "summary": "Task-oriented dialogue (TOD) systems facilitate goal-driven interactions\nbetween users and machines. While recent advances in deep learning have\nimproved the performance, TOD systems often struggle in low-resource scenarios\nwith limited labeled data. To address this challenge, we propose Spec-TOD, a\nnovel framework designed to train an end-to-end TOD system with limited data.\nSpec-TOD introduces two main innovations: (i) a novel specialized end-to-end\nTOD framework that incorporates explicit task instructions for\ninstruction-tuned large language models (LLMs), and (ii) an efficient training\nstrategy that leverages lightweight, specialized LLMs to achieve strong\nperformance with minimal supervision. Experiments on the MultiWOZ dataset, a\nwidely used TOD benchmark, demonstrate that Spec-TOD achieves competitive\nresults while significantly reducing the need for labeled data. These findings\nhighlight the potential of the proposed framework in advancing efficient and\neffective TOD systems in low-resource settings.", "AI": {"tldr": "Spec-TOD is a framework for training task-oriented dialogue systems effectively in low-resource scenarios using specialized large language models.", "motivation": "To improve task-oriented dialogue (TOD) systems' performance in scenarios with limited labeled data.", "method": "Spec-TOD incorporates explicit task instructions for instruction-tuned LLMs and employs a training strategy combining lightweight specialized LLMs for enhanced performance with minimal supervision.", "result": "Spec-TOD achieved competitive results on the MultiWOZ dataset, requiring significantly less labeled data.", "conclusion": "The framework enhances the development of efficient and effective TOD systems in low-resource environments.", "key_contributions": ["Novel end-to-end TOD framework with explicit task instructions", "Efficient training strategy using lightweight specialized LLMs", "Demonstrated strong performance in low-resource settings"], "limitations": "", "keywords": ["task-oriented dialogue", "low-resource", "large language models", "NLP", "machine learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.04852", "pdf": "https://arxiv.org/pdf/2507.04852.pdf", "abs": "https://arxiv.org/abs/2507.04852", "title": "Dialogue-Based Multi-Dimensional Relationship Extraction from Novels", "authors": ["Yuchen Yan", "Hanjie Zhao", "Senbin Zhu", "Hongde Liu", "Zhihong Zhang", "Yuxiang Jia"], "categories": ["cs.CL"], "comment": "The paper has been accepted by NLPCC2025. 12 pages, 5 figures, 5\n  tables", "summary": "Relation extraction is a crucial task in natural language processing, with\nbroad applications in knowledge graph construction and literary analysis.\nHowever, the complex context and implicit expressions in novel texts pose\nsignificant challenges for automatic character relationship extraction. This\nstudy focuses on relation extraction in the novel domain and proposes a method\nbased on Large Language Models (LLMs). By incorporating relationship dimension\nseparation, dialogue data construction, and contextual learning strategies, the\nproposed method enhances extraction performance. Leveraging dialogue structure\ninformation, it improves the model's ability to understand implicit\nrelationships and demonstrates strong adaptability in complex contexts.\nAdditionally, we construct a high-quality Chinese novel relation extraction\ndataset to address the lack of labeled resources and support future research.\nExperimental results show that our method outperforms traditional baselines\nacross multiple evaluation metrics and successfully facilitates the automated\nconstruction of character relationship networks in novels.", "AI": {"tldr": "This study proposes a method for relation extraction in novels, utilizing LLMs and contextual learning to improve performance and address challenges in character relationship extraction.", "motivation": "Relation extraction is essential for knowledge graph construction and literary analysis, yet extracting character relationships in novels is challenging due to complex contexts and implicit expressions.", "method": "The proposed approach leverages Large Language Models, relationship dimension separation, dialogue data construction, and contextual learning strategies to enhance extraction performance.", "result": "The method significantly outperforms traditional baselines on various evaluation metrics, showcasing its ability to effectively extract character relationships in novels.", "conclusion": "The study not only improves relation extraction methods for literary texts but also provides a valuable dataset for future research in this area.", "key_contributions": ["Introduction of an LLM-based method for novel relation extraction.", "Development of a high-quality Chinese novel relation extraction dataset.", "Demonstration of improved performance over traditional methods through experimental results."], "limitations": "", "keywords": ["relation extraction", "Large Language Models", "novels", "contextual learning", "Chinese novel dataset"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2507.04854", "pdf": "https://arxiv.org/pdf/2507.04854.pdf", "abs": "https://arxiv.org/abs/2507.04854", "title": "$\\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large Language Models", "authors": ["Shrey Ganatra", "Swapnil Bhattacharyya", "Harshvivek Kashid", "Spandan Anaokar", "Shruti Nair", "Reshma Sekhar", "Siddharth Manohar", "Rahul Hemrajani", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "Access to consumer grievance redressal in India is often hindered by\nprocedural complexity, legal jargon, and jurisdictional challenges. To address\nthis, we present $\\textbf{Grahak-Nyay}$ (Justice-to-Consumers), a chatbot that\nstreamlines the process using open-source Large Language Models (LLMs) and\nRetrieval-Augmented Generation (RAG). Grahak-Nyay simplifies legal complexities\nthrough a concise and up-to-date knowledge base. We introduce three novel\ndatasets: $\\textit{GeneralQA}$ (general consumer law), $\\textit{SectoralQA}$\n(sector-specific knowledge) and $\\textit{SyntheticQA}$ (for RAG evaluation),\nalong with $\\textit{NyayChat}$, a dataset of 300 annotated chatbot\nconversations. We also introduce $\\textit{Judgments}$ data sourced from Indian\nConsumer Courts to aid the chatbot in decision making and to enhance user\ntrust. We also propose $\\textbf{HAB}$ metrics ($\\textbf{Helpfulness, Accuracy,\nBrevity}$) to evaluate chatbot performance. Legal domain experts validated\nGrahak-Nyay's effectiveness. Code and datasets will be released.", "AI": {"tldr": "Grahak-Nyay is a chatbot that simplifies consumer grievance redressal in India using LLMs and RAG, supported by novel datasets and evaluated with new metrics.", "motivation": "To simplify the complex process of consumer grievance redressal in India that suffers from procedural difficulties and legal complexities.", "method": "Developed a chatbot using open-source LLMs and RAG to streamline grievance resolution, supplemented by multiple novel datasets for training and evaluation.", "result": "Grahak-Nyay demonstrated effective performance in legal assistance, validated by domain experts through the proposed HAB metrics.", "conclusion": "The implementation of Grahak-Nyay facilitates easier access to justice for consumers in India, and the datasets and code will be made publicly available for further research.", "key_contributions": ["Introduction of Grahak-Nyay chatbot for consumer grievance resolution", "Creation of three novel datasets: GeneralQA, SectoralQA, and SyntheticQA", "Development of new HAB metrics for evaluating chatbot performance"], "limitations": "", "keywords": ["Chatbot", "Consumer Grievance", "Large Language Models", "Retrieval-Augmented Generation", "HAB Metrics"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.04884", "pdf": "https://arxiv.org/pdf/2507.04884.pdf", "abs": "https://arxiv.org/abs/2507.04884", "title": "Building Open-Retrieval Conversational Question Answering Systems by Generating Synthetic Data and Decontextualizing User Questions", "authors": ["Christos Vlachos", "Nikolaos Stylianou", "Alexandra Fiotaki", "Spiros Methenitis", "Elisavet Palogiannidi", "Themos Stafylakis", "Ion Androutsopoulos"], "categories": ["cs.CL"], "comment": "Accepted at SIGDIAL 2025", "summary": "We consider open-retrieval conversational question answering (OR-CONVQA), an\nextension of question answering where system responses need to be (i) aware of\ndialog history and (ii) grounded in documents (or document fragments) retrieved\nper question. Domain-specific OR-CONVQA training datasets are crucial for\nreal-world applications, but hard to obtain. We propose a pipeline that\ncapitalizes on the abundance of plain text documents in organizations (e.g.,\nproduct documentation) to automatically produce realistic OR-CONVQA dialogs\nwith annotations. Similarly to real-world humanannotated OR-CONVQA datasets, we\ngenerate in-dialog question-answer pairs, self-contained (decontextualized,\ne.g., no referring expressions) versions of user questions, and propositions\n(sentences expressing prominent information from the documents) the system\nresponses are grounded in. We show how the synthetic dialogs can be used to\ntrain efficient question rewriters that decontextualize user questions,\nallowing existing dialog-unaware retrievers to be utilized. The retrieved\ninformation and the decontextualized question are then passed on to an LLM that\ngenerates the system's response.", "AI": {"tldr": "This paper presents a pipeline for generating domain-specific datasets for open-retrieval conversational question answering (OR-CONVQA) using existing plain text documents from organizations.", "motivation": "The need for domain-specific OR-CONVQA datasets which are essential for practical applications but challenging to acquire.", "method": "A pipeline is proposed to automatically create realistic OR-CONVQA dialogs from plain text documents, producing annotations and training data.", "result": "The synthetic dialogs effectively train question rewriters, facilitating the use of dialog-unaware retrievers and enabling LLMs to generate responses based on retrieved information.", "conclusion": "The approach shows promise in creating training data for OR-CONVQA, possibly improving the efficiency of question answering systems.", "key_contributions": ["Development of a pipeline for generating OR-CONVQA datasets.", "Creation of annotations for in-dialog question-answer pairs and decontextualized user questions.", "Facilitation of existing retrievers' utilization through generated training data."], "limitations": "The generated datasets may still lack the complexity and nuances of human-annotated datasets and depend heavily on the quality of the source documents.", "keywords": ["OR-CONVQA", "dialog history", "document retrieval", "LLM", "question answering"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2507.04886", "pdf": "https://arxiv.org/pdf/2507.04886.pdf", "abs": "https://arxiv.org/abs/2507.04886", "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations", "authors": ["A. Bochkov"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch.", "AI": {"tldr": "The paper challenges the conventional view that trainable input embeddings are essential for semantic representation in LLMs by using fixed, non-semantic visual embeddings derived from Unicode glyphs, demonstrating enhanced model performance despite the absence of trainable embeddings.", "motivation": "Understanding the role of semantic representation in LLMs for better interpretability and innovation.", "method": "Transformer models were constructed with a frozen embedding layer using precomputed visual embeddings derived from Unicode glyphs, along with a novel Unicode-centric tokenizer.", "result": "The models converge and generate coherent text successfully, outperforming models with trainable embeddings on the MMLU reasoning benchmark.", "conclusion": "High-level semantics are emergent properties of the Transformer's architecture and data scale, not inherent to input embeddings, suggesting a shift in understanding their role.", "key_contributions": ["Introduced fixed visual embeddings from Unicode glyphs for LLM training.", "Developed a Unicode-centric tokenizer for universal text coverage.", "Demonstrated that models can outperform traditional ones even with non-trainable embeddings."], "limitations": "", "keywords": ["large language models", "semantic representation", "visual embeddings", "tokenization", "interpretability"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.04895", "pdf": "https://arxiv.org/pdf/2507.04895.pdf", "abs": "https://arxiv.org/abs/2507.04895", "title": "O_FT@EvalLLM2025 : étude comparative de choix de données et de stratégies d'apprentissage pour l'adaptation de modèles de langue à un domaine", "authors": ["Ismaël Rousseau", "Claire Perroux", "Pierre Adam", "Thomas Girault", "Lionel Delphin-Poulat", "Morgan Veyret", "Gwénolé Lecorvé", "Géraldine Damnati"], "categories": ["cs.CL"], "comment": "22 pages + 10 pages appendices, in French language", "summary": "This paper presents the work carried out by the O_FT team, joint with Orange\nand Ouest-France, on adapting language models to the defense domain as part of\nthe EvalLLM2025 challenge. This work focused on adapting the\n\\texttt{Mistral-7B-Instruct-v0.3} model using classical techniques of continued\npre-training and instruction-tuning. The core of our efforts is based on\ncollecting, generating, and selecting data for these two stages as well as for\nmodel evaluation. Experiments show that our adapted models have better\ndomain-specific knowledge and improved domain-specific task processing skills,\nalong with comparable (or even superior) performance on general knowledge and\nskills. Considering the carbon footprint of our adaptations, this work\ndemonstrates the feasibility of domain adaptation for relatively small models.\n  --\n  Ce document pr\\'esente les travaux r\\'ealis\\'es par l'\\'equipe O_FT conjointe\n\\`a Orange et Ouest-France sur l'adaptation de mod\\`eles de langue au domaine\nde la d\\'efense dans le cadre du challenge EvalLLM2025. Ces travaux se sont\nconcentr\\'es sur l'adaptation du mod\\`ele \\texttt{Mistral-7B-Instruct-v0.3}\navec des techniques classiques de poursuite du pr\\'e-entra\\^inement et\nd'affinage sur instructions. L'essentiel de nos travaux a port\\'e sur la\nconstitution, g\\'en\\'eration et s\\'election de donn\\'ees pour ces deux \\'etapes\nainsi que pour l'\\'evaluation des mod\\`eles. Les exp\\'eriences montrent que nos\nmod\\`eles adapt\\'es ont de meilleures de connaissances de fond et une meilleure\ncapacit\\'e de traitement de t\\^aches sur le domaine de la d\\'efense, ainsi que\ndes performances comparables (voire sup\\'erieures) sur des connaissances ou\ncapacit\\'es g\\'en\\'eralistes. Mis au regard des empreintes carbones de nos\nadaptations, ces travaux d\\'emontrent ainsi la viabilit\\'e de l'adaptation \\`a\nun domaine de mod\\`eles relativement petits.", "AI": {"tldr": "The paper discusses adapting language models for the defense domain, focusing on the Mistral-7B-Instruct-v0.3 model, demonstrating improved performance on domain-specific tasks through data collection, pre-training, and instruction-tuning.", "motivation": "The motivation behind this work was to adapt language models to the defense sector as part of the EvalLLM2025 challenge, emphasizing the need for domain-specific knowledge and task processing abilities.", "method": "The authors utilized classical techniques of continued pre-training and instruction-tuning, involving the collection, generation, and selection of relevant data for model adaptation and evaluation.", "result": "Experiments revealed that the adapted models exhibited better domain-specific knowledge and enhanced task processing skills, while also maintaining strong performance on general knowledge and tasks.", "conclusion": "The study concludes that domain adaptation for smaller models is feasible and can yield significant improvements in specialized fields without substantial carbon footprint increases.", "key_contributions": ["Improved domain-specific adaptation of the Mistral-7B model", "Demonstrated feasibility of adapting smaller language models to specific domains", "Reliable performance metrics across both domain-specific and general tasks."], "limitations": "", "keywords": ["language models", "domain adaptation", "defense", "Mistral-7B", "EvalLLM2025"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.04942", "pdf": "https://arxiv.org/pdf/2507.04942.pdf", "abs": "https://arxiv.org/abs/2507.04942", "title": "SIGIR 2025 -- LiveRAG Challenge Report", "authors": ["David Carmel", "Simone Filice", "Guy Horowitz", "Yoelle Maarek", "Oren Somekh", "Ran Tavory"], "categories": ["cs.CL", "cs.IR", "H.3.3"], "comment": "9 pages, 5 tables", "summary": "The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,\nprovided a competitive platform for advancing Retrieval-Augmented Generation\n(RAG) technologies. Participants from academia and industry were invited to\ndevelop a RAG-based question-answering system using a fixed corpus\n(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal\nwas to facilitate challenging comparisons of retrieval and prompting\nstrategies. During the Live Challenge Day, 70 teams from 27 different countries\nprovided answers and supportive information to 500 unseen questions within a\nstrict two-hour time window. Evaluation was conducted in two stages: first an\nautomated LLM-as-a-judge approach was used to compute correctness and\nfaithfulness score, then a manual review of top ranked submissions was\nconducted. The finalists were announced on June 12, 2025, with prizes awarded\nduring the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.", "AI": {"tldr": "The LiveRAG Challenge at SIGIR 2025 advanced Retrieval-Augmented Generation technologies through competitive development of RAG-based question-answering systems.", "motivation": "To facilitate advancements in RAG technologies by encouraging the development of question-answering systems using a fixed corpus and common LLM.", "method": "Participants developed RAG-based systems to answer 500 unseen questions using a fixed corpus (Fineweb-10BT) and the Falcon3-10B-Instruct LLM, evaluated through automated and manual methods.", "result": "70 teams participated, providing answers in a two-hour window; evaluations included automated correctness scoring followed by manual reviews of top submissions.", "conclusion": "Finalists of the challenge were announced, with prizes awarded at the LiveRAG Workshop during SIGIR 2025.", "key_contributions": ["Introduced a competitive platform for RAG technologies", "Facilitated systematic comparisons of retrieval and prompting strategies", "Engaged diverse international participation in AI and ML applications."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "question-answering", "LiveRAG Challenge"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.04952", "pdf": "https://arxiv.org/pdf/2507.04952.pdf", "abs": "https://arxiv.org/abs/2507.04952", "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation", "authors": ["Chenchen Zhang", "Yuhang Li", "Can Xu", "Jiaheng Liu", "Ao Liu", "Shihui Hu", "Dengpeng Wu", "Guanhua Huang", "Kejiao Li", "Qi Yi", "Ruibin Xiong", "Haotian Zhu", "Yuanxing Zhang", "Yuhao Jiang", "Yue Zhang", "Zenan Xu", "Bohui Zhai", "Guoxiang He", "Hebin Li", "Jie Zhao", "Le Zhang", "Lingyun Tan", "Pengyu Guo", "Xianshu Pang", "Yang Ruan", "Zhifeng Zhang", "Zhonghu Wang", "Ziyan Xu", "Zuopu Yin", "Wiggin Zhou", "Chayse Zhou", "Fengzong Lian"], "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels.", "AI": {"tldr": "Introduction of ArtifactsBench, a benchmark for multimodal evaluation of visual code generation by LLMs.", "motivation": "To address the lack of effective evaluation measures that focus on visual fidelity and interactive integrity in LLM-generated artifacts.", "method": "ArtifactsBench programmatically renders visual artifacts and captures their behavior through temporal screenshots, using MLLMs for assessment guided by a detailed checklist.", "result": "ArtifactsBench achieves 94.4% ranking consistency with WebDev Arena and over 90% agreement with human experts, indicating reliable automated quality assessment.", "conclusion": "ArtifactsBench is the first framework that can automate the evaluation of human-perceived quality in visual code generation at scale, providing valuable insights into SOTA models.", "key_contributions": ["Introduction of a new benchmark for visual code generation evaluation.", "Provision of a framework for automated, multimodal assessment.", "Open-sourcing of ArtifactsBench to benefit the research community."], "limitations": "", "keywords": ["Large Language Models", "visual code generation", "evaluation benchmark"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.05010", "pdf": "https://arxiv.org/pdf/2507.05010.pdf", "abs": "https://arxiv.org/abs/2507.05010", "title": "Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification", "authors": ["Chenfei Xiong", "Jingwei Ni", "Yu Fan", "Vilém Zouhar", "Donya Rooein", "Lorena Calvo-Bartolomé", "Alexander Hoyle", "Zhijing Jin", "Mrinmaya Sachan", "Markus Leippold", "Dirk Hovy", "Mennatallah El-Assady", "Elliott Ash"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Co-DETECT (Collaborative Discovery of Edge cases in TExt\nClassificaTion), a novel mixed-initiative annotation framework that integrates\nhuman expertise with automatic annotation guided by large language models\n(LLMs). Co-DETECT starts with an initial, sketch-level codebook and dataset\nprovided by a domain expert, then leverages the LLM to annotate the data and\nidentify edge cases that are not well described by the initial codebook.\nSpecifically, Co-DETECT flags challenging examples, induces high-level,\ngeneralizable descriptions of edge cases, and assists user in incorporating\nedge case handling rules to improve the codebook. This iterative process\nenables more effective handling of nuanced phenomena through compact,\ngeneralizable annotation rules. Extensive user study, qualitative and\nquantitative analyses prove the effectiveness of Co-DETECT.", "AI": {"tldr": "Co-DETECT is a framework for collaborative annotation in text classification that combines human input with LLM-guided automatic annotation to identify and manage edge cases.", "motivation": "The paper addresses the need for better annotation methods that leverage human expertise while utilizing LLMs to improve text classification, especially for edge cases.", "method": "Co-DETECT employs an initial codebook and dataset from a domain expert, subsequently utilizing an LLM to automatically annotate data and flag difficult examples, facilitating the creation of generalizable edge case handling rules.", "result": "The framework demonstrated effectiveness in improving the annotation process through user studies and qualitative and quantitative analyses, showing better handling of nuanced phenomena.", "conclusion": "The iterative process of using Co-DETECT allows for the development of compact and generalizable annotation rules, enhancing text classification performance.", "key_contributions": ["Introduced a mixed-initiative framework for annotation combining human and LLM contributions.", "Developed a method for identifying and managing edge cases in text classification.", "Provided empirical evidence of the framework's effectiveness through extensive user studies."], "limitations": "", "keywords": ["edge cases", "text classification", "large language models", "annotation framework", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.05017", "pdf": "https://arxiv.org/pdf/2507.05017.pdf", "abs": "https://arxiv.org/abs/2507.05017", "title": "Verified Language Processing with Hybrid Explainability: A Technical Report", "authors": ["Oliver Robert Fox", "Giacomo Bergami", "Graham Morgan"], "categories": ["cs.CL", "cs.SC"], "comment": null, "summary": "The volume and diversity of digital information have led to a growing\nreliance on Machine Learning techniques, such as Natural Language Processing,\nfor interpreting and accessing appropriate data. While vector and graph\nembeddings represent data for similarity tasks, current state-of-the-art\npipelines lack guaranteed explainability, failing to determine similarity for\ngiven full texts accurately. These considerations can also be applied to\nclassifiers exploiting generative language models with logical prompts, which\nfail to correctly distinguish between logical implication, indifference, and\ninconsistency, despite being explicitly trained to recognise the first two\nclasses. We present a novel pipeline designed for hybrid explainability to\naddress this. Our methodology combines graphs and logic to produce First-Order\nLogic representations, creating machine- and human-readable representations\nthrough Montague Grammar. Preliminary results indicate the effectiveness of\nthis approach in accurately capturing full text similarity. To the best of our\nknowledge, this is the first approach to differentiate between implication,\ninconsistency, and indifference for text classification tasks. To address the\nlimitations of existing approaches, we use three self-contained datasets\nannotated for the former classification task to determine the suitability of\nthese approaches in capturing sentence structure equivalence, logical\nconnectives, and spatiotemporal reasoning. We also use these data to compare\nthe proposed method with language models pre-trained for detecting sentence\nentailment. The results show that the proposed method outperforms\nstate-of-the-art models, indicating that natural language understanding cannot\nbe easily generalised by training over extensive document corpora. This work\noffers a step toward more transparent and reliable Information Retrieval from\nextensive textual data.", "AI": {"tldr": "A novel pipeline combining graphs and logic for explainable text similarity and classification using First-Order Logic representations.", "motivation": "The paper addresses the lack of explainability in state-of-the-art similarity and classification pipelines in NLP, especially in distinguishing between logical implications in texts.", "method": "The proposed methodology integrates graphs with Montague Grammar to generate First-Order Logic representations for better interpretability in text classification tasks.", "result": "Preliminary results show that this pipeline effectively captures text similarity, outperforming state-of-the-art models in accuracy by focusing on logical distinctions like implication, indifference, and inconsistency.", "conclusion": "The proposed approach enhances transparency and reliability in information retrieval from extensive textual data, suggesting that better understanding of sentence structures is crucial for NLP.", "key_contributions": ["Introduction of a novel hybrid explainability pipeline for text classification.", "First method to differentiate between implication, inconsistency, and indifference in NLP tasks.", "Demonstrated performance improvements over current state-of-the-art models in understanding text similarity."], "limitations": "", "keywords": ["Machine Learning", "Natural Language Processing", "Text Classification", "Explainability", "Information Retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.05123", "pdf": "https://arxiv.org/pdf/2507.05123.pdf", "abs": "https://arxiv.org/abs/2507.05123", "title": "An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques", "authors": ["Walid Mohamed Aly", "Taysir Hassan A. Soliman", "Amr Mohamed AbdelAziz"], "categories": ["cs.CL", "cs.AI"], "comment": "This manuscript is an extended version of the work accepted for\n  publication in the International Journal of Advanced Computer Science and\n  Applications (IJACSA), Volume 16, Issue 6, June 2025", "summary": "Large Language Models (LLMs) continue to advance natural language processing\nwith their ability to generate human-like text across a range of tasks. Despite\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\nperformance in text summarization across various domains and datasets has not\nbeen comprehensively evaluated. At the same time, the ability to summarize text\neffectively without relying on extensive training data has become a crucial\nbottleneck. To address these issues, we present a systematic evaluation of six\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\nand ArXiv (scientific). By leveraging prompt engineering techniques including\nzero-shot and in-context learning, our study evaluates the performance using\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\ntimes is conducted to better understand the trade-off between summarization\nquality and computational efficiency. For Long documents, introduce a\nsentence-based chunking strategy that enables LLMs with shorter context windows\nto summarize extended inputs in multiple stages. The findings reveal that while\nLLMs perform competitively on news and dialog tasks, their performance on long\nscientific documents improves significantly when aided by chunking strategies.\nIn addition, notable performance variations were observed based on model\nparameters, dataset properties, and prompt design. These results offer\nactionable insights into how different LLMs behave across task types,\ncontributing to ongoing research in efficient, instruction-based NLP systems.", "AI": {"tldr": "This paper systematically evaluates the performance of six LLMs on text summarization across multiple datasets, highlighting the effectiveness of chunking strategies for long scientific documents.", "motivation": "To evaluate the performance of Large Language Models in text summarization tasks and address the limitations in summarizing long documents without extensive training data.", "method": "A systematic evaluation of six LLMs across four datasets using prompt engineering techniques such as zero-shot and in-context learning, combined with performance metrics like ROUGE and BERTScore.", "result": "LLMs perform well on news and dialog summarization, with significant improvements for long scientific documents when employing a sentence-based chunking strategy.", "conclusion": "The study provides actionable insights into LLM behavior across various tasks, contributing to the advancement of efficient instruction-based NLP systems.", "key_contributions": ["Systematic evaluation of LLMs for summarization", "Introduction of a chunking strategy for long documents", "Analysis of performance metrics and computational efficiency"], "limitations": "", "keywords": ["Large Language Models", "text summarization", "prompt engineering", "chunking strategy", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.05129", "pdf": "https://arxiv.org/pdf/2507.05129.pdf", "abs": "https://arxiv.org/abs/2507.05129", "title": "SMART: Simulated Students Aligned with Item Response Theory for Question Difficulty Prediction", "authors": ["Alexander Scarlatos", "Nigel Fernandez", "Christopher Ormerod", "Susan Lottridge", "Andrew Lan"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Item (question) difficulties play a crucial role in educational assessments,\nenabling accurate and efficient assessment of student abilities and\npersonalization to maximize learning outcomes. Traditionally, estimating item\ndifficulties can be costly, requiring real students to respond to items,\nfollowed by fitting an item response theory (IRT) model to get item difficulty\nestimates. This approach cannot be applied to the cold-start setting for\npreviously unseen items either. In this work, we present SMART (Simulated\nStudents Aligned with IRT), a novel method for aligning simulated students with\ninstructed ability, which can then be used in simulations to predict the\ndifficulty of open-ended items. We achieve this alignment using direct\npreference optimization (DPO), where we form preference pairs based on how\nlikely responses are under a ground-truth IRT model. We perform a simulation by\ngenerating thousands of responses, evaluating them with an LLM-based scoring\nmodel, and fit the resulting data to an IRT model to obtain item difficulty\nestimates. Through extensive experiments on a real-world student response\ndataset, we show that SMART outperforms other item difficulty prediction\nmethods by leveraging its improved ability alignment.", "AI": {"tldr": "SMART is a new approach for predicting item difficulties in educational assessments using simulated students and LLM-based evaluations.", "motivation": "Accurately estimating item difficulties is essential for effective educational assessments and personalized learning, especially in cold-start situations for unseen items.", "method": "SMART aligns simulated students with instructed ability through direct preference optimization, generating responses evaluated by an LLM-based scoring model, which are then used to fit an item response theory model for difficulty estimates.", "result": "SMART outperforms existing item difficulty prediction methods by utilizing improved ability alignment in simulations.", "conclusion": "The proposed method shows significant promise in providing accurate item difficulty estimates without the need for actual student responses.", "key_contributions": ["Introduction of SMART for item difficulty prediction using simulated students", "Utilization of direct preference optimization for better alignment", "Demonstrated efficiency in generating accurate predictions compared to traditional methods"], "limitations": "", "keywords": ["Item Difficulty Prediction", "Educational Assessment", "Direct Preference Optimization", "Simulated Students", "LLM-based Scoring"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.05137", "pdf": "https://arxiv.org/pdf/2507.05137.pdf", "abs": "https://arxiv.org/abs/2507.05137", "title": "Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization", "authors": ["Jaewook Lee", "Alexander Scarlatos", "Andrew Lan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation.", "AI": {"tldr": "This paper proposes a framework for generating interpretable mnemonics for learning Japanese vocabulary using LLMs and systematic rules.", "motivation": "To improve the memorization of Japanese vocabulary by enhancing the interpretability of LLM-based mnemonic generation methods.", "method": "A generative framework is proposed that models mnemonic construction with common rules and learns these rules through an Expectation-Maximization-type algorithm using learner-authored mnemonics.", "result": "The proposed method demonstrates effectiveness in generating mnemonics for new learners and offers insights into mnemonic creation processes.", "conclusion": "The framework enables systematic and interpretable mnemonic generation, benefiting learners from Roman alphabet backgrounds.", "key_contributions": ["Generative framework for mnemonic construction", "Use of a novel Expectation-Maximization-type algorithm", "Enhanced interpretability in mnemonic generation"], "limitations": "Focuses on Japanese vocabulary; applicability to other languages or scripts not assessed.", "keywords": ["Japanese vocabulary", "mnemonics", "large language models", "human-computer interaction", "educational technology"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.05157", "pdf": "https://arxiv.org/pdf/2507.05157.pdf", "abs": "https://arxiv.org/abs/2507.05157", "title": "AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models", "authors": ["Chinnappa Guggilla", "Budhaditya Roy", "Trupti Ramdas Chavan", "Abdul Rahman", "Edward Bowen"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 3 figures", "summary": "Large Language Models (LLMs) possess an extraordinary capability to produce\ntext that is not only coherent and contextually relevant but also strikingly\nsimilar to human writing. They adapt to various styles and genres, producing\ncontent that is both grammatically correct and semantically meaningful.\nRecently, LLMs have been misused to create highly realistic phishing emails,\nspread fake news, generate code to automate cyber crime, and write fraudulent\nscientific articles. Additionally, in many real-world applications, the\ngenerated content including style and topic and the generator model are not\nknown beforehand. The increasing prevalence and sophistication of artificial\nintelligence (AI)-generated texts have made their detection progressively more\nchallenging. Various attempts have been made to distinguish machine-generated\ntext from human-authored content using linguistic, statistical, machine\nlearning, and ensemble-based approaches. This work focuses on two primary\nobjectives Task-A, which involves distinguishing human-written text from\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\nmodel responsible for the generation. Both of these tasks are based on fine\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.", "AI": {"tldr": "This paper investigates the detection of machine-generated text using fine-tuned models, addressing the challenges posed by the misuse of LLMs.", "motivation": "The misuse of LLMs for generating malicious content, such as phishing emails and fake news, necessitates effective detection methods to differentiate machine-generated text from human-authored text.", "method": "The authors perform fine-tuning on GPT_4o-mini, LLaMA 3 8B, and BERT models for two tasks: distinguishing human-written from machine-generated text and identifying the specific LLM model used.", "result": "The fine-tuned GPT_4o-mini and BERT achieved accuracies of 0.9547 for distinguishing human vs. machine-generated text and 0.4698 for identifying the LLM model.", "conclusion": "The study demonstrates that fine-tuned LLMs can effectively detect machine-generated text, although identifying the specific model remains a significant challenge.", "key_contributions": ["Fine-tuning of multiple LLMs for detection tasks", "High accuracy achieved in distinguishing human-written and machine-generated text", "Evaluation of the capability to identify the specific model used for text generation"], "limitations": "Limited effectiveness in identifying the specific model responsible for generation; accuracy lower for Task-B compared to Task-A.", "keywords": ["Large Language Models", "Text Generation", "Text Detection", "Human-Computer Interaction", "Machine Learning"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2507.05158", "pdf": "https://arxiv.org/pdf/2507.05158.pdf", "abs": "https://arxiv.org/abs/2507.05158", "title": "InfoSteer: Steering Information Utility in Language Model Post-Training", "authors": ["Chunyuan Deng", "Ruidi Chang", "Hanjie Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in language models (LMs) gradually ushered in an era\nwhere post-training is crucial. Yet, post-training approaches such as\nsupervised fine-tuning (SFT) do not guarantee effective use of knowledge\nacquired during pretraining. We therefore present \\ours, a lightweight method\nthat encourages parametric information utilization in LMs during post-training.\nThis is achieved via treating FFN layer as associate key-value memory, and\npromotes the use of stored memory vectors via forward-pass interventions or\nregularization during backpropagation. We find this simple guidance during\npost-training phase delivers consistent performance improvements across diverse\nmodel families--including Qwen, Gemma and Llama-spanning over 15 downstream\ntasks in both ID and OOD evaluations. Beyond performance gains, we also find\nthat steered LMs can adaptively allocate information-placing more emphasis on\ngenerating semantically meaningful tokens, while using fewer resources on\nsimple transition ones (e.g., `,' or `and'). Our work underscores that vanilla\npost-training does not fully leverage pre-training potential, and steering LMs\nin latent representation space offers a promising approach that enhances both\nperformance and interpretability.", "AI": {"tldr": "This paper presents a lightweight method to improve language models' post-training performance by effectively utilizing pre-trained knowledge through a modified feedforward layer, achieving significant performance gains across various tasks.", "motivation": "To address the limitations of traditional post-training methods such as supervised fine-tuning that fail to fully utilize the knowledge acquired during pretraining.", "method": "The method treats the feedforward neural network (FFN) layer as an associative key-value memory and uses interventions during the forward pass and regularization during backpropagation to encourage memory utilization.", "result": "Improvements in performance were observed across 15 downstream tasks in both in-distribution (ID) and out-of-distribution (OOD) evaluations, with enhanced focus on generating semantically meaningful tokens while reducing resource allocation for simple tokens.", "conclusion": "The study highlights the inadequacy of vanilla post-training in leveraging pre-trained knowledge and proposes a novel steering approach in the latent representation space that enhances model performance and interpretability.", "key_contributions": ["Introduction of a method to improve post-training in LMs", "Demonstration of consistent performance improvements across multiple model families", "Reveal the potential of guided memory utilization in LMs to enhance interpretability."], "limitations": "", "keywords": ["language models", "post-training", "memory utilization", "supervised fine-tuning", "latent representation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.05177", "pdf": "https://arxiv.org/pdf/2507.05177.pdf", "abs": "https://arxiv.org/abs/2507.05177", "title": "OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech Language Model", "authors": ["Chen Wang", "Tianyu Peng", "Wen Yang", "Yinan Bai", "Guangfu Wang", "Jun Lin", "Lanpeng Jia", "Lingxiang Wu", "Jinqiao Wang", "Chengqing Zong", "Jiajun Zhang"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Technical Report", "summary": "Empathetic interaction is a cornerstone of human-machine communication, due\nto the need for understanding speech enriched with paralinguistic cues and\ngenerating emotional and expressive responses. However, the most powerful\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\nthe architecture, data and development opaque to researchers. Given the\ncritical need for transparent research into the LSLMs and empathetic behavior,\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\ndesigned to enable empathetic speech interactions. Based on our empathetic\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\ndecoding architecture to achieve low-latency speech generation. To facilitate\nend-to-end training, OpenS2S incorporates an automated data construction\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\nlow cost. By leveraging large language models to generate empathetic content\nand controllable text-to-speech systems to introduce speaker and emotional\nvariation, we construct a scalable training corpus with rich paralinguistic\ndiversity and minimal human supervision. We release the fully open-source\nOpenS2S model, including the dataset, model weights, pre-training and\nfine-tuning codes, to empower the broader research community and accelerate\ninnovation in empathetic speech systems. The project webpage can be accessed at\nhttps://casia-lm.github.io/OpenS2S", "AI": {"tldr": "OpenS2S is an open-source large-scale language model designed for empathetic speech interactions, featuring automated data synthesis and low-latency speech generation.", "motivation": "There is a critical need for transparent research into empathetic large-scale language models (LSLMs) due to the opaqueness of current powerful models in terms of architecture and development.", "method": "OpenS2S employs a streaming interleaved decoding architecture for low-latency generation and incorporates an automated data construction pipeline to synthesize empathetic speech dialogues.", "result": "The study demonstrates the construction of a scalable training corpus with diverse paralinguistic features, enabling the development of empathetic speech systems with minimal human involvement.", "conclusion": "The release of OpenS2S, including its dataset and model code, aims to empower researchers and promote innovation in the field of empathetic speech interactions.", "key_contributions": ["Introduction of OpenS2S, an open-source LSLM for empathetic interactions.", "Development of a low-latency speech generation architecture.", "Automated data construction for synthesizing high-quality empathetic dialogues."], "limitations": "", "keywords": ["empathetic interaction", "open-source", "large-scale language models", "speech generation", "paralinguistic cues"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.05179", "pdf": "https://arxiv.org/pdf/2507.05179.pdf", "abs": "https://arxiv.org/abs/2507.05179", "title": "From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating Hindi News Veracity Explanations", "authors": ["Pulkit Bansal", "Raghvendra Kumar", "Shakti Singh", "Sriparna Saha", "Adam Jatowt"], "categories": ["cs.CL"], "comment": null, "summary": "In an era of rampant misinformation, generating reliable news explanations is\nvital, especially for under-represented languages like Hindi. Lacking robust\nautomated tools, Hindi faces challenges in scaling misinformation detection. To\nbridge this gap, we propose a novel framework integrating Direct Preference\nOptimization (DPO) with curriculum learning to align machine-generated\nexplanations with human reasoning. Fact-checked explanations from credible\nsources serve as preferred responses, while LLM outputs highlight system\nlimitations and serve as non-preferred responses. To refine task-specific\nalignment, we introduce two key parameters -- Actuality and Finesse -- into the\nDPO loss function, enhancing explanation quality and consistency. Experiments\nwith LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's\neffectiveness in generating coherent, contextually relevant explanations. This\nscalable approach combats misinformation and extends automated explanation\ngeneration to low-resource languages.", "AI": {"tldr": "This paper presents a framework using Direct Preference Optimization and curriculum learning to improve automated news explanation generation in Hindi, addressing misinformation challenges.", "motivation": "The paper addresses the need for reliable news explanation generation in under-represented languages like Hindi, which currently lacks robust tools for misinformation detection.", "method": "The proposed framework combines Direct Preference Optimization with curriculum learning, emphasizing preferred (fact-checked) and non-preferred (LLM outputs) responses, and introduces Actuality and Finesse parameters to enhance explanation quality.", "result": "Experiments show that the framework effectively generates coherent and contextually relevant explanations using various LLMs and PLMs, demonstrating its effectiveness in combating misinformation.", "conclusion": "The approach is scalable and extends the ability to generate automated explanations to low-resource languages, significant for improving information reliability.", "key_contributions": ["Integration of Direct Preference Optimization with curriculum learning for explanation generation", "Introduction of Actuality and Finesse parameters in DPO to enhance explanation quality", "Demonstrated effectiveness with multiple LLMs and PLMs in a low-resource language context"], "limitations": "The approach may depend on the quality of the initial fact-checked explanations and may not fully address all aspects of misinformation.", "keywords": ["Misinformation detection", "Automated explanation generation", "Under-represented languages", "Direct Preference Optimization", "Curriculum learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.05197", "pdf": "https://arxiv.org/pdf/2507.05197.pdf", "abs": "https://arxiv.org/abs/2507.05197", "title": "Pre-Trained Policy Discriminators are General Reward Models", "authors": ["Shihan Dou", "Shichun Liu", "Yuming Yang", "Yicheng Zou", "Yunhua Zhou", "Shuhao Xing", "Chenhao Huang", "Qiming Ge", "Demin Song", "Haijun Lv", "Songyang Gao", "Chengqi Lv", "Enyu Zhou", "Honglin Guo", "Zhiheng Xi", "Wenwei Zhang", "Qipeng Guo", "Qi Zhang", "Xipeng Qiu", "Xuanjing Huang", "Tao Gui", "Kai Chen"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We offer a novel perspective on reward modeling by formulating it as a policy\ndiscriminator, which quantifies the difference between two policies to generate\na reward signal, guiding the training policy towards a target policy with\ndesired behaviors. Based on this conceptual insight, we propose a scalable\npre-training method named Policy Discriminative Learning (POLAR), which trains\na reward model (RM) to discern identical policies and discriminate different\nones. Unlike traditional reward modeling methods relying on absolute\npreferences, POLAR captures the relative difference between one policy and an\narbitrary target policy, which is a scalable, high-level optimization objective\nsuitable for modeling generic ranking relationships. Leveraging the POLAR\npre-training paradigm, we present a series of RMs with parameter scales from\n1.8B to 7B. Empirical results show that POLAR substantially outperforms\ntraditional non-pre-trained methods, significantly enhancing RM performance.\nFor instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on\nSTEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA\nbaselines. POLAR also shows robust generalization capabilities in RLHF using\nReinforcement Fine-tuning (RFT), providing reliable reward signals and markedly\nenhancing policy performance--improving LLaMa3.1-8B from an average of 47.36%\nto 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover,\nscaling experiments reveal a clear power-law relationship between computation\nand performance, supported by linear correlation coefficients approaching 0.99.\nThe impressive performance, strong generalization, and scaling properties\nsuggest that POLAR is a promising direction for developing general and strong\nreward models.", "AI": {"tldr": "This paper introduces Policy Discriminative Learning (POLAR), a novel method for reward modeling that improves performance by quantifying policy differences and enhancing training in reinforcement learning contexts.", "motivation": "To provide a more scalable and effective approach to reward modeling by formulating it as a policy discriminator, allowing for better guidance towards desired policy behaviors.", "method": "The POLAR method trains a reward model to discriminate between identical and different policies, capturing relative differences rather than relying on absolute preferences.", "result": "Empirical results demonstrate that POLAR significantly outperforms traditional reward modeling methods, improving accuracy on various tasks and enhancing policy performance across several benchmarks.", "conclusion": "POLAR shows promising capabilities in reward modeling with strong performance improvements, generalization in reinforcement learning, and favorable scaling properties.", "key_contributions": ["Introduction of the POLAR framework for reward modeling", "Demonstration of significant performance improvements over traditional methods", "Establishment of a power-law relationship between computation and performance in reward models"], "limitations": "", "keywords": ["reward modeling", "policy discriminator", "machine learning", "reinforcement learning", "policy performance"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2507.05248", "pdf": "https://arxiv.org/pdf/2507.05248.pdf", "abs": "https://arxiv.org/abs/2507.05248", "title": "Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models", "authors": ["Ziqi Miao", "Lijun Li", "Yuan Xiong", "Zhenhua Liu", "Pengyu Zhu", "Jing Shao"], "categories": ["cs.CL"], "comment": "21 pages, 9 figures. Code and data available at\n  https://github.com/Dtc7w3PQ/Response-Attack", "summary": "Contextual priming, where earlier stimuli covertly bias later judgments,\noffers an unexplored attack surface for large language models (LLMs). We\nuncover a contextual priming vulnerability in which the previous response in\nthe dialogue can steer its subsequent behavior toward policy-violating content.\nBuilding on this insight, we propose Response Attack, which uses an auxiliary\nLLM to generate a mildly harmful response to a paraphrased version of the\noriginal malicious query. They are then formatted into the dialogue and\nfollowed by a succinct trigger prompt, thereby priming the target model to\ngenerate harmful content. Across eight open-source and proprietary LLMs, RA\nconsistently outperforms seven state-of-the-art jailbreak techniques, achieving\nhigher attack success rates. To mitigate this threat, we construct and release\na context-aware safety fine-tuning dataset, which significantly reduces the\nattack success rate while preserving model capabilities. The code and data are\navailable at https://github.com/Dtc7w3PQ/Response-Attack.", "AI": {"tldr": "This paper explores vulnerabilities in large language models (LLMs) related to contextual priming, proposing a novel attack method called Response Attack to exploit this issue.", "motivation": "The study highlights an overlooked vulnerability in LLMs where previous dialogue responses can influence subsequent outputs, potentially leading to policy violations.", "method": "The authors propose a method called Response Attack, which leverages an auxiliary LLM to generate harmful responses from paraphrased queries and primes the target model to produce harmful content.", "result": "The Response Attack method outperforms seven state-of-the-art jailbreak techniques across various LLMs, showing higher success rates in generating harmful content.", "conclusion": "To address the identified vulnerability, the authors created a fine-tuning dataset that reduces attack effectiveness while maintaining LLM capabilities. Code and data are made publicly available.", "key_contributions": ["Introduction of Response Attack technique for exploiting contextual priming in LLMs.", "Demonstration of higher attack success rates compared to existing jailbreak methods.", "Development of a context-aware fine-tuning dataset to mitigate identified vulnerabilities."], "limitations": "", "keywords": ["contextual priming", "large language models", "Response Attack", "vulnerability", "fine-tuning"], "importance_score": 8, "read_time_minutes": 21}}
{"id": "2507.05257", "pdf": "https://arxiv.org/pdf/2507.05257.pdf", "abs": "https://arxiv.org/abs/2507.05257", "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions", "authors": ["Yuanzhe Hu", "Yu Wang", "Julian McAuley"], "categories": ["cs.CL", "cs.AI"], "comment": "23 Pages, Y. Hu and Y. Wang contribute equally", "summary": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on\nevaluating reasoning, planning, and execution capabilities, while another\ncritical component-memory, encompassing how agents memorize, update, and\nretrieve long-term information-is under-evaluated due to the lack of\nbenchmarks. We term agents with memory mechanisms as memory agents. In this\npaper, we identify four core competencies essential for memory agents: accurate\nretrieval, test-time learning, long-range understanding, and conflict\nresolution. Existing datasets either rely on limited context lengths or are\ntailored for static, long-context settings like book-based QA, which do not\nreflect the interactive, multi-turn nature of memory agents that incrementally\naccumulate information. Furthermore, no existing benchmarks cover all four\ncompetencies. Therefore, we introduce MemoryAgentBench, a new benchmark\nspecifically designed for memory agents. Our benchmark combines reformulated\nexisting datasets with newly constructed ones, covering the above four memory\ncompetencies, providing a systematic and challenging testbed for assessing\nmemory quality. We evaluate a diverse set of memory agents, ranging from simple\ncontext-based and retrieval-augmented generation (RAG) systems to advanced\nagents with external memory modules and tool integration. Empirical results\nreveal that current methods fall short of mastering all four competencies,\nunderscoring the need for further research into comprehensive memory mechanisms\nfor LLM agents.", "AI": {"tldr": "The paper introduces MemoryAgentBench, a new benchmark for evaluating memory competencies in memory agents within LLMs, addressing the lack of comprehensive assessment tools.", "motivation": "The paper addresses the under-evaluation of memory in LLM agents, highlighting the importance of understanding how agents memorize, update, and retrieve long-term information.", "method": "The authors introduce a benchmark called MemoryAgentBench that encompasses four core competencies of memory: accurate retrieval, test-time learning, long-range understanding, and conflict resolution. It combines restructured existing datasets with new ones to provide a systemic evaluation framework.", "result": "Empirical evaluations show that current memory agents do not adequately master all four competencies, indicating significant gaps in the capabilities of existing models.", "conclusion": "The study emphasizes the necessity for further research into memory mechanisms in LLM agents, given that current methods are limited in their performance.", "key_contributions": ["Introduction of MemoryAgentBench benchmark for memory agents", "Identified four core competencies essential for memory agents", "Evaluation of various memory agents demonstrating current limitations"], "limitations": "No existing benchmarks prior to this one covered all four competencies, leading to gaps in assessment.", "keywords": ["memory agents", "large language models", "memory competencies", "benchmark", "test-time learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.03147", "pdf": "https://arxiv.org/pdf/2507.03147.pdf", "abs": "https://arxiv.org/abs/2507.03147", "title": "DeepGesture: A conversational gesture synthesis system based on emotions and semantics", "authors": ["Thanh Hoang-Minh"], "categories": ["cs.HC", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Video Demo: https://www.youtube.com/watch?v=eZghfNGmZn8", "summary": "Along with the explosion of large language models, improvements in speech\nsynthesis, advancements in hardware, and the evolution of computer graphics,\nthe current bottleneck in creating digital humans lies in generating character\nmovements that correspond naturally to text or speech inputs.\n  In this work, we present DeepGesture, a diffusion-based gesture synthesis\nframework for generating expressive co-speech gestures conditioned on\nmultimodal signals-text, speech, emotion, and seed motion. Built upon the\nDiffuseStyleGesture model, DeepGesture introduces novel architectural\nenhancements that improve semantic alignment and emotional expressiveness in\ngenerated gestures. Specifically, we integrate fast text transcriptions as\nsemantic conditioning and implement emotion-guided classifier-free diffusion to\nsupport controllable gesture generation across affective states. A lightweight\nTransformer backbone combines full self-attention and cross-local attention for\neffective feature fusion of heterogeneous modalities. To visualize results, we\nimplement a full rendering pipeline in Unity based on BVH output from the\nmodel. Evaluation on the ZeroEGGS dataset shows that DeepGesture produces\ngestures with improved human-likeness and contextual appropriateness,\noutperforming baselines on Mean Opinion Score and Frechet Gesture Distance\nmetrics. Our system supports interpolation between emotional states and\ndemonstrates generalization to out-of-distribution speech, including synthetic\nvoices-marking a step forward toward fully multimodal, emotionally aware\ndigital humans.", "AI": {"tldr": "DeepGesture is a diffusion-based framework for generating expressive co-speech gestures from multimodal inputs, enhancing emotional expressiveness and semantic alignment.", "motivation": "The current challenge in creating digital humans lies in generating natural character movements corresponding to text or speech inputs.", "method": "DeepGesture integrates fast text transcriptions for semantic conditioning and employs emotion-guided classifier-free diffusion for controllable gesture generation using a lightweight Transformer backbone.", "result": "Evaluation on the ZeroEGGS dataset indicates that DeepGesture produces gestures with superior human-likeness and contextual appropriateness compared to baselines as measured by Mean Opinion Score and Frechet Gesture Distance.", "conclusion": "DeepGesture advances the development of emotionally aware digital humans and supports interpolation between emotional states and generalization to synthetic speech.", "key_contributions": ["Introduction of a diffusion-based model for gesture synthesis", "Integration of emotion-guided diffusion for controllable gesturing", "Implementation of a full rendering pipeline in Unity"], "limitations": "", "keywords": ["gesture synthesis", "diffusion model", "multimodal signals", "emotional expressiveness", "digital humans"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.03670", "pdf": "https://arxiv.org/pdf/2507.03670.pdf", "abs": "https://arxiv.org/abs/2507.03670", "title": "Interaction Techniques that Encourage Longer Prompts Can Improve Psychological Ownership when Writing with AI", "authors": ["Nikhita Joshi", "Daniel Vogel"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Writing longer prompts for an AI assistant to generate a short story\nincreases psychological ownership, a user's feeling that the writing belongs to\nthem. To encourage users to write longer prompts, we evaluated two interaction\ntechniques that modify the prompt entry interface of chat-based generative AI\nassistants: pressing and holding the prompt submission button, and continuously\nmoving a slider up and down when submitting a short prompt. A within-subjects\nexperiment investigated the effects of such techniques on prompt length and\npsychological ownership, and results showed that these techniques increased\nprompt length and led to higher psychological ownership than baseline\ntechniques. A second experiment further augmented these techniques by showing\nAI-generated suggestions for how the prompts could be expanded. This further\nincreased prompt length, but did not lead to improvements in psychological\nownership. Our results show that simple interface modifications like these can\nelicit more writing from users and improve psychological ownership.", "AI": {"tldr": "Modifying prompt entry interfaces for AI assistants can enhance user prompt length and psychological ownership.", "motivation": "To explore how interaction techniques can encourage users to write longer prompts for AI assistants, thereby increasing their sense of ownership over generated content.", "method": "A within-subjects experiment was conducted to evaluate interaction techniques (pressing and holding submission button, moving a slider) on prompt length and psychological ownership. A second experiment provided AI-generated suggestions to augment prompt length.", "result": "The experiments demonstrated that interface modifications significantly increased prompt length and psychological ownership. However, while AI suggestions further increased prompt length, they did not improve psychological ownership.", "conclusion": "Simple interface changes can effectively encourage more writing and enhance users' psychological ownership in AI-driven content generation.", "key_contributions": ["Evaluated interaction techniques for AI prompt entry interface.", "Demonstrated the impact of these techniques on prompt length and psychological ownership.", "Showed the mixed impact of AI-generated suggestions on writing engagement."], "limitations": "Focuses on specific interaction techniques and their effects, and does not explore long-term user engagement or other interface factors.", "keywords": ["Human-Computer Interaction", "Psychological Ownership", "Generative AI", "Interaction Techniques", "Prompt Length"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2507.04469", "pdf": "https://arxiv.org/pdf/2507.04469.pdf", "abs": "https://arxiv.org/abs/2507.04469", "title": "The role of large language models in UI/UX design: A systematic literature review", "authors": ["Ammar Ahmed", "Ali Shariq Imran"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "This systematic literature review examines the role of large language models\n(LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies\npublished between 2022 and 2025. We identify key LLMs in use, including GPT-4,\nGemini, and PaLM, and map their integration across the design lifecycle, from\nideation to evaluation. Common practices include prompt engineering,\nhuman-in-the-loop workflows, and multimodal input. While LLMs are reshaping\ndesign processes, challenges such as hallucination, prompt instability, and\nlimited explainability persist. Our findings highlight LLMs as emerging\ncollaborators in design, and we propose directions for the ethical, inclusive,\nand effective integration of these technologies.", "AI": {"tldr": "This paper reviews the impact of large language models (LLMs) on UI/UX design, synthesizing insights from 38 studies to outline practices and challenges faced in their application throughout the design lifecycle.", "motivation": "To explore how LLMs are being integrated into UI/UX design processes and identify the key practices and challenges associated with their use.", "method": "Systematic literature review of 38 peer-reviewed studies published from 2022 to 2025.", "result": "Key LLMs identified include GPT-4, Gemini, and PaLM. The integration of LLMs is mapped across the design lifecycle, highlighting practices like prompt engineering and human-in-the-loop workflows.", "conclusion": "LLMs are emerging as collaborators in design, yet challenges like hallucination and limited explainability need to be addressed for their effective use.", "key_contributions": ["Identification of key LLMs in UI/UX design", "Mapping LLM integration across the design lifecycle", "Proposed directions for ethical and effective LLM use in design"], "limitations": "Challenges such as hallucination, prompt instability, and limited explainability persist in the application of LLMs.", "keywords": ["Large Language Models", "UI/UX Design", "Systematic Review", "Prompt Engineering", "Human-in-the-Loop"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.04491", "pdf": "https://arxiv.org/pdf/2507.04491.pdf", "abs": "https://arxiv.org/abs/2507.04491", "title": "A validity-guided workflow for robust large language model research in psychology", "authors": ["Zhicheng Lin"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are rapidly being integrated into psychological\nresearch as research tools, evaluation targets, human simulators, and cognitive\nmodels. However, recent evidence reveals severe measurement unreliability:\nPersonality assessments collapse under factor analysis, moral preferences\nreverse with punctuation changes, and theory-of-mind accuracy varies widely\nwith trivial rephrasing. These \"measurement phantoms\"--statistical artifacts\nmasquerading as psychological phenomena--threaten the validity of a growing\nbody of research. Guided by the dual-validity framework that integrates\npsychometrics with causal inference, we present a six-stage workflow that\nscales validity requirements to research ambition--using LLMs to code text\nrequires basic reliability and accuracy, while claims about psychological\nproperties demand comprehensive construct validation. Researchers must (1)\nexplicitly define their research goal and corresponding validity requirements,\n(2) develop and validate computational instruments through psychometric\ntesting, (3) design experiments that control for computational confounds, (4)\nexecute protocols with transparency, (5) analyze data using methods appropriate\nfor non-independent observations, and (6) report findings within demonstrated\nboundaries and use results to refine theory. We illustrate the workflow through\nan example of model evaluation--\"LLM selfhood\"--showing how systematic\nvalidation can distinguish genuine computational phenomena from measurement\nartifacts. By establishing validated computational instruments and transparent\npractices, this workflow provides a path toward building a robust empirical\nfoundation for AI psychology research.", "AI": {"tldr": "The paper addresses measurement unreliability in the use of LLMs in psychological research and proposes a six-stage workflow to ensure rigor in applying LLMs as research tools.", "motivation": "To tackle the significant measurement unreliability present in psychological research using LLMs, which can lead to misleading results and interpretations.", "method": "The paper proposes a six-stage workflow that includes defining research goals, developing validated instruments, designing controlled experiments, executing protocols transparently, analyzing data appropriately, and reporting findings with rigor.", "result": "The workflow can distinguish genuine computational phenomena from measurement artifacts, thereby enhancing the validity of research involving LLMs in psychology.", "conclusion": "Implementing systematic validation practices can help establish a robust empirical foundation for AI psychology research, ensuring that the research conducted with LLMs adheres to rigorous scientific standards.", "key_contributions": ["Introduction of a six-stage workflow for validating LLM usage in psychological research", "Integration of psychometric methods with causal inference for enhanced validity", "Clarification of distinctions between genuine psychological phenomena and artifacts created by LLMs"], "limitations": "", "keywords": ["large language models", "psychological research", "measurement validity", "statistical artifacts", "empirical foundation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2306.09519", "pdf": "https://arxiv.org/pdf/2306.09519.pdf", "abs": "https://arxiv.org/abs/2306.09519", "title": "Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion", "authors": ["Qiao Qiao", "Yuepei Li", "Kang Zhou", "Qi Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "conference PAKDD 2023", "summary": "Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts\nof a relation with few-shot reference entity pairs. Current approaches randomly\nselect one negative sample for each reference entity pair to minimize a\nmargin-based ranking loss, which easily leads to a zero-loss problem if the\nnegative sample is far away from the positive sample and then out of the\nmargin. Moreover, the entity should have a different representation under a\ndifferent context. To tackle these issues, we propose a novel Relation-Aware\nNetwork with Attention-Based Loss (RANA) framework. Specifically, to better\nutilize the plentiful negative samples and alleviate the zero-loss issue, we\nstrategically select relevant negative samples and design an attention-based\nloss function to further differentiate the importance of each negative sample.\nThe intuition is that negative samples more similar to positive samples will\ncontribute more to the model. Further, we design a dynamic relation-aware\nentity encoder for learning a context-dependent entity representation.\nExperiments demonstrate that RANA outperforms the state-of-the-art models on\ntwo benchmark datasets.", "AI": {"tldr": "The paper proposes a Relation-Aware Network with Attention-Based Loss (RANA) framework for few-shot knowledge graph completion, addressing issues with negative sample selection and entity representation.", "motivation": "Current FKGC approaches struggle with negative sample selection, which can result in ineffective learning and a zero-loss problem due to poor negative sample relevance.", "method": "The RANA framework strategically selects relevant negative samples and utilizes an attention-based loss function to differentially weigh these samples. It also employs a dynamic relation-aware entity encoder for context-dependent entity representation.", "result": "Experiments show that RANA significantly outperforms existing state-of-the-art methods on two benchmark datasets for FKGC.", "conclusion": "The RANA framework effectively enhances the performance of few-shot knowledge graph completion tasks by improving negative sample selection and entity representation.", "key_contributions": ["Novel attention-based loss function for negative sample selection", "Dynamic relation-aware entity encoder for context-dependent representations", "Empirical performance improvement over state-of-the-art on benchmark datasets"], "limitations": "", "keywords": ["few-shot knowledge graph completion", "negative sample selection", "relation-aware network", "attention-based loss"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2308.03415", "pdf": "https://arxiv.org/pdf/2308.03415.pdf", "abs": "https://arxiv.org/abs/2308.03415", "title": "End-to-End Evaluation for Low-Latency Simultaneous Speech Translation", "authors": ["Christian Huber", "Tu Anh Dinh", "Carlos Mullov", "Ngoc Quan Pham", "Thai Binh Nguyen", "Fabian Retkowski", "Stefan Constantin", "Enes Yavuz Ugan", "Danni Liu", "Zhaolin Li", "Sai Koneru", "Jan Niehues", "Alexander Waibel"], "categories": ["cs.CL", "cs.AI"], "comment": "Demo paper at EMNLP 2023", "summary": "The challenge of low-latency speech translation has recently draw significant\ninterest in the research community as shown by several publications and shared\ntasks. Therefore, it is essential to evaluate these different approaches in\nrealistic scenarios. However, currently only specific aspects of the systems\nare evaluated and often it is not possible to compare different approaches.\n  In this work, we propose the first framework to perform and evaluate the\nvarious aspects of low-latency speech translation under realistic conditions.\nThe evaluation is carried out in an end-to-end fashion. This includes the\nsegmentation of the audio as well as the run-time of the different components.\n  Secondly, we compare different approaches to low-latency speech translation\nusing this framework. We evaluate models with the option to revise the output\nas well as methods with fixed output. Furthermore, we directly compare\nstate-of-the-art cascaded as well as end-to-end systems. Finally, the framework\nallows to automatically evaluate the translation quality as well as latency and\nalso provides a web interface to show the low-latency model outputs to the\nuser.", "AI": {"tldr": "This paper proposes a framework for evaluating low-latency speech translation systems under realistic conditions, enabling comparison of different approaches.", "motivation": "There is a growing interest in low-latency speech translation, necessitating a means to evaluate and compare the various systems effectively under realistic scenarios.", "method": "The proposed framework evaluates low-latency speech translation by segmenting audio, analyzing the runtime of components, and automatically assessing translation quality and latency.", "result": "The framework successfully allows for the comparison of state-of-the-art cascaded and end-to-end systems and provides a web interface for displaying low-latency model outputs.", "conclusion": "This work not only introduces a novel evaluation framework but also provides insights into the performance of various low-latency speech translation approaches.", "key_contributions": ["First framework for evaluating low-latency speech translation", "End-to-end evaluation approach including audio segmentation and runtime analysis", "Web interface for visualizing model outputs"], "limitations": "", "keywords": ["low-latency", "speech translation", "evaluation framework", "end-to-end systems", "cascaded systems"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2312.00292", "pdf": "https://arxiv.org/pdf/2312.00292.pdf", "abs": "https://arxiv.org/abs/2312.00292", "title": "SEPSIS: I Can Catch Your Lies -- A New Paradigm for Deception Detection", "authors": ["Anku Rani", "Dwip Dalal", "Shreya Gautam", "Pankaj Gupta", "Vinija Jain", "Aman Chadha", "Amit Sheth", "Amitava Das"], "categories": ["cs.CL"], "comment": "ACL SRW 2025", "summary": "Deception is the intentional practice of twisting information. It is a\nnuanced societal practice deeply intertwined with human societal evolution,\ncharacterized by a multitude of facets. This research explores the problem of\ndeception through the lens of psychology, employing a framework that\ncategorizes deception into three forms: lies of omission, lies of commission,\nand lies of influence. The primary focus of this study is specifically on\ninvestigating only lies of omission. We propose a novel framework for deception\ndetection leveraging NLP techniques. We curated an annotated dataset of 876,784\nsamples by amalgamating a popular large-scale fake news dataset and scraped\nnews headlines from the Twitter handle of the Times of India, a well-known\nIndian news media house. Each sample has been labeled with four layers, namely:\n(i) the type of omission (speculation, bias, distortion, sounds factual, and\nopinion), (ii) colors of lies(black, white, etc), and (iii) the intention of\nsuch lies (to influence, etc) (iv) topic of lies (political, educational,\nreligious, etc). We present a novel multi-task learning pipeline that leverages\nthe dataless merging of fine-tuned language models to address the deception\ndetection task mentioned earlier. Our proposed model achieved an F1 score of\n0.87, demonstrating strong performance across all layers, including the type,\ncolor, intent, and topic aspects of deceptive content. Finally, our research\nexplores the relationship between lies of omission and propaganda techniques.\nTo accomplish this, we conducted an in-depth analysis, uncovering compelling\nfindings. For instance, our analysis revealed a significant correlation between\nloaded language and opinion, shedding light on their interconnectedness. To\nencourage further research in this field, we are releasing the SEPSIS dataset\nand code at https://huggingface.co/datasets/ankurani/deception.", "AI": {"tldr": "This research investigates deception, specifically lies of omission, and proposes a novel NLP-based deception detection framework utilizing a newly curated dataset.", "motivation": "To explore the nuanced societal practice of deception and enhance detection methods through NLP techniques.", "method": "A multi-task learning pipeline using fine-tuned language models was developed, leveraging a dataset of 876,784 annotated samples categorized by type, color, intent, and topic of lies.", "result": "The proposed model achieved an F1 score of 0.87, demonstrating strong performance in detecting various aspects of deceptive content.", "conclusion": "The research highlights significant correlations in deception types and techniques, encouraging future work in this domain, and releases the SEPSIS dataset and code for further exploration.", "key_contributions": ["Novel framework for detecting lies of omission using NLP", "Curated SEPSIS dataset with 876,784 annotated samples", "High F1 score indicating effective multi-task learning approach"], "limitations": "", "keywords": ["deception detection", "natural language processing", "lies of omission"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2402.12374", "pdf": "https://arxiv.org/pdf/2402.12374.pdf", "abs": "https://arxiv.org/abs/2402.12374", "title": "Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding", "authors": ["Zhuoming Chen", "Avner May", "Ruslan Svirschevski", "Yuhsun Huang", "Max Ryabinin", "Zhihao Jia", "Beidi Chen"], "categories": ["cs.CL"], "comment": null, "summary": "As the usage of large language models (LLMs) grows, performing efficient\ninference with these models becomes increasingly important. While speculative\ndecoding has recently emerged as a promising direction for speeding up\ninference, existing methods are limited in their ability to scale to larger\nspeculation budgets, and adapt to different hyperparameters and hardware. This\npaper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for\nspeculative decoding. To attain better scalability, Sequoia introduces a\ndynamic programming algorithm to find the optimal tree structure for the\nspeculated tokens. To achieve robust speculative performance, Sequoia uses a\nnovel sampling and verification method that outperforms prior work across\ndifferent decoding temperatures. Finally, Sequoia introduces a hardware-aware\ntree optimizer that maximizes speculative performance by automatically\nselecting the token tree size and depth for a given hardware platform.\nEvaluation shows that Sequoia improves the decoding speed of Llama2-7B,\nLlama2-13B, and Vicuna-33B on an A100 by up to $4.04\\times$, $3.73\\times$, and\n$2.27\\times$. For offloading setting on L40, Sequoia achieves as low as 0.56\ns/token for exact Llama2-70B inference latency, which is $9.96\\times$ on our\noptimized offloading system (5.6 s/token), $9.7\\times$ than\nDeepSpeed-Zero-Inference, $19.5\\times$ than Huggingface Accelerate.", "AI": {"tldr": "Sequoia is a novel algorithm for speculative decoding of large language models, enhancing inference speed and scalability while adapting to different hardware.", "motivation": "With the increasing use of large language models, efficient inference has become crucial, necessitating improvements in decoding speeds and adaptability.", "method": "Sequoia employs a dynamic programming algorithm to optimize token tree structures and introduces a sampling and verification method, along with a hardware-aware tree optimizer that adapts to different hardware setups.", "result": "Sequoia significantly improves decoding speeds for Llama2-7B, Llama2-13B, and Vicuna-33B on A100 by factors up to 4.04 times, 3.73 times, and 2.27 times respectively, and achieves very low latency for Llama2-70B.", "conclusion": "The proposed algorithm outperforms existing methods and provides notable enhancements in both speed and robustness for inference in large language models.", "key_contributions": ["Introduction of a dynamic programming approach for optimal tree structure in speculative decoding.", "Development of a novel sampling and verification method for improved robustness across decoding temperatures.", "Creation of a hardware-aware tree optimizer that tailors decoding parameters to the specific hardware environment."], "limitations": "", "keywords": ["speculative decoding", "large language models", "inference optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2403.13638", "pdf": "https://arxiv.org/pdf/2403.13638.pdf", "abs": "https://arxiv.org/abs/2403.13638", "title": "Pretraining Language Models Using Translationese", "authors": ["Meet Doshi", "Raj Dabre", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we explore the utility of translationese as synthetic data\ncreated using machine translation for pre-training language models (LMs) for\nlow-resource languages (LRLs). Our simple methodology consists of translating\nlarge amounts of web-crawled monolingual documents (clean) into the LRLs,\nfollowed by filtering the translated documents using tiny LMs trained on small\nbut clean LRL data. Taking the case of Indian languages, we pre-train LMs from\nscratch with 28M and 85M parameters, and then fine-tune them for 5 downstream\nnatural language understanding (NLU) and 4 generative (NLG) tasks. We observe\nthat pre-training on filtered synthetic data leads to relative performance\ndrops of only 0.87% for NLU and 2.35% for NLG, compared to pre-training on\nclean data, and this gap further diminishes upon the inclusion of a small\namount of clean data. We also study the impact of synthetic data filtering and\nthe choice of source language for synthetic data generation. Furthermore,\nevaluating continually pre-trained larger models like Gemma-2B and Llama-3-8B\nin few-shot settings, we observe that using synthetic data is competitive with\nusing clean data. Our findings suggest that synthetic data shows promise for\nbridging the pre-training gap between English and LRLs.", "AI": {"tldr": "This paper examines using synthetic data from machine translation for pre-training language models in low-resource languages, demonstrating its effectiveness and competitive performance compared to clean data.", "motivation": "To explore the effectiveness of synthetic data from translationese for pre-training language models, particularly for low-resource languages.", "method": "Translating large volumes of web-crawled monolingual documents into low-resource languages, filtering the results using small LMs trained on clean data, and pre-training LMs for various NLU and NLG tasks.", "result": "The study shows performance drops of only 0.87% for NLU and 2.35% for NLG when using filtered synthetic data compared to clean data, with improvements noted when combining synthetic with small amounts of clean data.", "conclusion": "Synthetic data can effectively narrow the performance gap between pre-training on English and low-resource languages, showing potential as a valuable resource for language model development.", "key_contributions": ["Demonstrated the utility of translationese as synthetic data for LMs in low-resource languages.", "Showed competitive performance of pre-trained models using synthetic data compared to clean data.", "Analyzed the impact of data filtering and source language on model training."], "limitations": "The study focuses on specific languages (Indian languages), and results may vary across different low-resource language contexts.", "keywords": ["synthetic data", "low-resource languages", "machine translation", "language models", "natural language understanding"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2404.06283", "pdf": "https://arxiv.org/pdf/2404.06283.pdf", "abs": "https://arxiv.org/abs/2404.06283", "title": "LLMs' Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements", "authors": ["Victoria Basmov", "Yoav Goldberg", "Reut Tsarfaty"], "categories": ["cs.CL"], "comment": null, "summary": "The task of reading comprehension (RC), often implemented as context-based\nquestion answering (QA), provides a primary means to assess language models'\nnatural language understanding (NLU) capabilities. Yet, when applied to large\nlanguage models (LLMs) with extensive built-in world knowledge, this method can\nbe deceptive. If the context aligns with the LLMs' internal knowledge, it is\nhard to discern whether the models' answers stem from context comprehension or\nfrom LLMs' internal information. Conversely, using data that conflicts with the\nmodels' knowledge creates erroneous trends which distort the results. To\naddress this issue, we suggest to use RC on imaginary data, based on fictitious\nfacts and entities. This task is entirely independent of the models' world\nknowledge, enabling us to evaluate LLMs' linguistic abilities without the\ninterference of parametric knowledge. Testing ChatGPT, GPT-4, LLaMA 2 and\nMixtral on such imaginary data, we uncover a class of linguistic phenomena\nposing a challenge to current LLMs, involving thinking in terms of alternative,\nhypothetical scenarios. While all the models handle simple affirmative and\nnegative contexts with high accuracy, they are much more prone to error when\ndealing with modal and conditional contexts. Crucially, these phenomena also\ntrigger the LLMs' vulnerability to knowledge-conflicts again. In particular,\nwhile some models prove virtually unaffected by knowledge conflicts in\naffirmative and negative contexts, when faced with more semantically involved\nmodal and conditional environments, they often fail to separate the text from\ntheir internal knowledge.", "AI": {"tldr": "This paper evaluates the reading comprehension abilities of LLMs using imaginary data, highlighting their challenges with modal and conditional contexts.", "motivation": "To address the deceptive nature of reading comprehension tasks on LLMs caused by their internal world knowledge, and to accurately assess their linguistic abilities.", "method": "The authors propose using fictitious facts and entities to create reading comprehension tasks that are independent of the models' built-in knowledge. They test several models, including ChatGPT, GPT-4, LLaMA 2, and Mixtral, with this approach.", "result": "The study finds that while LLMs perform well on simple contexts, they struggle significantly with modal and conditional contexts, revealing vulnerabilities tied to their internal knowledge.", "conclusion": "Imaginary data serves as an effective tool for understanding LLMs' linguistic abilities and their shortcomings, particularly in complex contextual scenarios that challenge their understanding of semantics.", "key_contributions": ["Proposing the use of imaginary data for assessing LLMs' linguistic capabilities", "Identifying specific linguistic phenomena that challenge LLMs", "Demonstrating the models' vulnerabilities in modal and conditional contexts"], "limitations": "The approach relies on the design of the imaginary data; further studies are needed to generalize findings to real-world applications.", "keywords": ["Reading Comprehension", "Large Language Models", "Imaginary Data", "Linguistic Phenomena", "Conditional Contexts"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2404.14809", "pdf": "https://arxiv.org/pdf/2404.14809.pdf", "abs": "https://arxiv.org/abs/2404.14809", "title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications", "authors": ["Wenbo Shang", "Xin Huang"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "23 pages including references, 14 figures", "summary": "A graph is a fundamental data model to represent various entities and their\ncomplex relationships in society and nature, such as social networks,\ntransportation networks, and financial networks. Recently, large language\nmodels (LLMs) have showcased a strong generalization ability to handle various\nnatural language processing tasks to answer users' arbitrary questions and\ngenerate specific-domain content. Compared with graph learning models, LLMs\nenjoy superior advantages in addressing the challenges of generalizing graph\ntasks by eliminating the need for training graph learning models and reducing\nthe cost of manual annotation. However, LLMs are sequential models for textual\ndata, but graphs are non-sequential topological data. It is challenging to\nadapt LLMs to tackle graph analytics tasks. In this survey, we conduct a\ncomprehensive investigation of existing LLM studies on graph data, which\nsummarizes the relevant graph analytics tasks solved by advanced LLM models and\npoints out the existing challenges and future directions. Specifically, we\nstudy the key problems of LLM-based generative graph analytics (LLM-GGA) in\nterms of three categories: LLM-based graph query processing (LLM-GQP),\nLLM-based graph inference and learning (LLM-GIL), and graph-LLM-based\napplications. LLM-GQP focuses on an integration of graph analytics techniques\nand LLM prompts, including graph understanding and knowledge graphs and LLMs,\nwhile LLM-GIL focuses on learning and reasoning over graphs, including graph\nlearning, graph-formed reasoning, and graph representation. We summarize the\nuseful prompts incorporated into LLM to handle different graph downstream\ntasks. Moreover, we give a summary of LLM model evaluation, benchmark\ndatasets/tasks, and a deep pro and cons analysis of the discussed LLM-GGA\nmodels. We also explore open problems and future directions in the research\narea of LLMs and graph analytics.", "AI": {"tldr": "This survey investigates the use of large language models (LLMs) in graph analytics, categorizing key problems and challenges, summarizing tasks and prompt techniques, and exploring future research directions.", "motivation": "To address the challenges of generalizing graph analytics tasks using LLMs, which have shown strong capabilities in NLP but face adaptation issues with non-sequential graph data.", "method": "Comprehensive investigation of existing studies on LLMs applied to graph data, categorizing tasks into query processing, inference and learning, and applications.", "result": "The paper summarizes key problems, useful prompts, evaluation benchmarks, and a detailed analysis of LLMs used for graph analytics, noting their pros and cons.", "conclusion": "LLMs show promise in graph analytics but face challenges, necessitating further research and investigation into their effective integration with graph data.", "key_contributions": ["Categorization of LLM applications in graph analytics", "Summary of effective prompts for graph tasks", "Identification of open problems and future research directions"], "limitations": "The paper primarily surveys existing works and may not provide novel experimental results.", "keywords": ["Large Language Models", "Graph Analytics", "Generative Graph Analytics", "Natural Language Processing", "Machine Learning"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2405.09605", "pdf": "https://arxiv.org/pdf/2405.09605.pdf", "abs": "https://arxiv.org/abs/2405.09605", "title": "Elements of World Knowledge (EWoK): A Cognition-Inspired Framework for Evaluating Basic World Knowledge in Language Models", "authors": ["Anna A. Ivanova", "Aalok Sathe", "Benjamin Lipkin", "Unnathi Kumar", "Setayesh Radkani", "Thomas H. Clark", "Carina Kauf", "Jennifer Hu", "R. T. Pramod", "Gabriel Grand", "Vivian Paulun", "Maria Ryskina", "Ekin Akyürek", "Ethan Wilcox", "Nafisa Rashid", "Leshem Choshen", "Roger Levy", "Evelina Fedorenko", "Joshua Tenenbaum", "Jacob Andreas"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to Transactions of the ACL (TACL). Contains 25 pages (14\n  main), 6 figures. Visit http://ewok-core.github.io for data and code. Authors\n  Anna Ivanova, Aalok Sathe, Benjamin Lipkin contributed equally", "summary": "The ability to build and reason about models of the world is essential for\nsituated language understanding. But evaluating world modeling capabilities in\nmodern AI systems -- especially those based on language models -- has proven\nchallenging, in large part because of the difficulty of disentangling\nconceptual knowledge about the world from knowledge of surface co-occurrence\nstatistics. This paper presents Elements of World Knowledge (EWoK), a framework\nfor evaluating language models' understanding of the conceptual knowledge\nunderlying world modeling. EWoK targets specific concepts from multiple\nknowledge domains known to be important for world modeling in humans, from\nsocial interactions (help, deceive) to spatial relations (left, right).\nObjects, agents, and locations in the items can be flexibly filled in, enabling\neasy generation of multiple controlled datasets. We then introduce\nEWoK-core-1.0, a dataset of 4,374 items covering 11 world knowledge domains. We\nevaluate 20 open-weights large language models (1.3B--70B parameters) and\ncompare them with human performance. All tested models perform worse than\nhumans, with results varying drastically across domains. Performance on social\ninteractions and social properties was highest and performance on physical\nrelations and spatial relations was lowest. Overall, this dataset highlights\nsimple cases where even large models struggle and presents rich avenues for\ntargeted research on LLM world modeling capabilities.", "AI": {"tldr": "This paper introduces the Elements of World Knowledge (EWoK) framework and EWoK-core-1.0, a dataset for evaluating language models' understanding of world knowledge across various domains, showing that current models perform worse than humans in these tasks.", "motivation": "To evaluate the conceptual knowledge capabilities of language models in world modeling, as existing methods struggle to disentangle conceptual knowledge from surface-level statistics.", "method": "The paper presents the EWoK framework, which includes a dataset (EWoK-core-1.0) of 4,374 items spanning 11 knowledge domains and evaluates 20 large language models against human performance in these tasks.", "result": "The evaluation shows that all tested language models perform worse than humans, with variations in performance across different knowledge domains—highest in social interactions and lowest in physical/spatial relations.", "conclusion": "The EWoK dataset highlights areas where large language models struggle with world knowledge and suggests paths for further research into improving their understanding and capabilities.", "key_contributions": ["Introduction of the EWoK framework for evaluating language model understanding of world knowledge.", "Development of the EWoK-core-1.0 dataset with 4,374 evaluated items across 11 domains.", "Comparison of language model performance to human performance, revealing significant gaps."], "limitations": "The dataset primarily consists of specific controlled scenarios which may not encompass the full complexity of real-world situations.", "keywords": ["world modeling", "language models", "dataset", "evaluation framework", "conceptual knowledge"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2406.04428", "pdf": "https://arxiv.org/pdf/2406.04428.pdf", "abs": "https://arxiv.org/abs/2406.04428", "title": "MoralBench: Moral Evaluation of LLMs", "authors": ["Jianchao Ji", "Yutong Chen", "Mingyu Jin", "Wujiang Xu", "Wenyue Hua", "Yongfeng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACM SIGKDD Explorations Volume 27 Issue 1", "summary": "In the rapidly evolving field of artificial intelligence, large language\nmodels (LLMs) have emerged as powerful tools for a myriad of applications, from\nnatural language processing to decision-making support systems. However, as\nthese models become increasingly integrated into societal frameworks, the\nimperative to ensure they operate within ethical and moral boundaries has never\nbeen more critical. This paper introduces a novel benchmark designed to measure\nand compare the moral reasoning capabilities of LLMs. We present the first\ncomprehensive dataset specifically curated to probe the moral dimensions of LLM\noutputs, addressing a wide range of ethical dilemmas and scenarios reflective\nof real-world complexities.\n  The main contribution of this work lies in the development of benchmark\ndatasets and metrics for assessing the moral identity of LLMs, which accounts\nfor nuance, contextual sensitivity, and alignment with human ethical standards.\nOur methodology involves a multi-faceted approach, combining quantitative\nanalysis with qualitative insights from ethics scholars to ensure a thorough\nevaluation of model performance. By applying our benchmark across several\nleading LLMs, we uncover significant variations in moral reasoning capabilities\nof different models. These findings highlight the importance of considering\nmoral reasoning in the development and evaluation of LLMs, as well as the need\nfor ongoing research to address the biases and limitations uncovered in our\nstudy. We publicly release the benchmark at\nhttps://drive.google.com/drive/u/0/folders/1k93YZJserYc2CkqP8d4B3M3sgd3kA8W7\nand also open-source the code of the project at\nhttps://github.com/agiresearch/MoralBench.", "AI": {"tldr": "This paper presents a benchmark for evaluating the moral reasoning abilities of large language models (LLMs), highlighting ethical considerations in AI applications.", "motivation": "With the increasing integration of LLMs into societal frameworks, there is a critical need to assess their moral reasoning capabilities to ensure ethical use.", "method": "The authors developed a comprehensive dataset that evaluates LLM outputs against ethical dilemmas, using both quantitative methods and qualitative insights from ethics experts.", "result": "The study reveals significant differences in moral reasoning capabilities across various LLMs, emphasizing the need for moral considerations in their development.", "conclusion": "The findings indicate the necessity for ongoing research into the biases and limitations of LLMs, alongside the importance of moral reasoning in AI systems.", "key_contributions": ["Development of benchmark datasets for assessing moral reasoning in LLMs", "Introduction of metrics that account for contextual sensitivity and ethical alignment", "Public release of the dataset and open-source code for further research"], "limitations": "The study acknowledges potential biases in the dataset and the need for comprehensive evaluation across diverse ethical frameworks.", "keywords": ["Large Language Models", "Moral Reasoning", "Ethics in AI", "Benchmark Datasets", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2406.11106", "pdf": "https://arxiv.org/pdf/2406.11106.pdf", "abs": "https://arxiv.org/abs/2406.11106", "title": "From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models", "authors": ["Harsh Nishant Lalai", "Aashish Anantha Ramakrishnan", "Raj Sanjay Shah", "Dongwon Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "NAACL Findings 2025", "summary": "With the rapid growth of Large Language Models (LLMs), safeguarding textual\ncontent against unauthorized use is crucial. Watermarking offers a vital\nsolution, protecting both - LLM-generated and plain text sources. This paper\npresents a unified overview of different perspectives behind designing\nwatermarking techniques through a comprehensive survey of the research\nliterature. Our work has two key advantages: (1) We analyze research based on\nthe specific intentions behind different watermarking techniques, evaluation\ndatasets used, and watermarking addition and removal methods to construct a\ncohesive taxonomy. (2) We highlight the gaps and open challenges in text\nwatermarking to promote research protecting text authorship. This extensive\ncoverage and detailed analysis sets our work apart, outlining the evolving\nlandscape of text watermarking in Language Models.", "AI": {"tldr": "This paper surveys watermarking techniques for shielding LLM-generated and plain text from unauthorized use, offering a taxonomy and identifying research gaps.", "motivation": "The rapid growth of Large Language Models necessitates protective measures against unauthorized textual content usage.", "method": "The paper conducts a comprehensive survey of research literature on watermarking techniques, analyzing intentions, datasets, and methods for watermark addition and removal.", "result": "The study constructs a cohesive taxonomy of text watermarking and highlights gaps and open challenges in the field.", "conclusion": "The findings promote further research in text authorship protection and provide a detailed analysis of the current state of text watermarking in LLMs.", "key_contributions": ["Comprehensive survey of watermarking techniques", "Cohesive taxonomy based on intentions and methods", "Identification of gaps and challenges in text watermarking."], "limitations": "", "keywords": ["watermarking", "Large Language Models", "text authorship", "taxonomy", "research gaps"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2406.13720", "pdf": "https://arxiv.org/pdf/2406.13720.pdf", "abs": "https://arxiv.org/abs/2406.13720", "title": "On the Utility of Domain-Adjacent Fine-Tuned Model Ensembles for Few-shot Problems", "authors": ["Md Ibrahim Ibne Alam", "Parikshit Ram", "Soham Dan", "Horst Samulowitz", "Koushik Kar"], "categories": ["cs.CL", "cs.LG"], "comment": "Main paper is 14 pages, followed by references and appendix", "summary": "Large Language Models (LLMs) have been observed to perform well on a wide\nrange of downstream tasks when fine-tuned on domain-specific data. However,\nsuch data may not be readily available in many applications, motivating\nzero-shot or few-shot approaches using domain-adjacent models. While several\nfine-tuned models for various tasks are available, finding an appropriate\ndomain-adjacent model for a given task is often not straight forward. In this\npaper, we study DAFT-E, a framework that utilizes an Ensemble of\nDomain-Adjacent Fine-Tuned Foundation Models for few-shot problems. We show\nthat for zero-shot problems, this ensembling method provides an accuracy\nperformance close to that of the single best model. With few-shot problems,\nthis performance improves further, at which point DEFT-E can outperform any\nsingle domain-adjacent model while requiring much less data for domain-specific\nfine-tuning.", "AI": {"tldr": "DAFT-E is a framework utilizing an ensemble of domain-adjacent fine-tuned models for improving performance on few-shot and zero-shot tasks in NLP.", "motivation": "The paper addresses the challenge of finding appropriate domain-adjacent models for various NLP tasks, especially in the absence of readily available domain-specific data.", "method": "The study introduces DAFT-E, an ensemble approach that combines multiple fine-tuned foundation models to enhance accuracy in zero-shot and few-shot scenarios.", "result": "DAFT-E achieves nearly the same accuracy as the best single model in zero-shot situations and surpasses it in few-shot scenarios while requiring less data for fine-tuning.", "conclusion": "The DAFT-E framework demonstrates significant improvements in model performance in low-data situations, making it a viable solution for many NLP applications.", "key_contributions": ["Introduction of DAFT-E framework for ensemble modeling", "Demonstration of improved performance on few-shot tasks", "Reduction in data requirements for fine-tuning"], "limitations": "", "keywords": ["Domain-Adjacent Models", "Few-Shot Learning", "Zero-Shot Learning", "NLP", "Ensemble Learning"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2406.15695", "pdf": "https://arxiv.org/pdf/2406.15695.pdf", "abs": "https://arxiv.org/abs/2406.15695", "title": "SS-GEN: A Social Story Generation Framework with Large Language Models", "authors": ["Yi Feng", "Mingyang Song", "Jiaqi Wang", "Zhuang Chen", "Guanqun Bi", "Minlie Huang", "Liping Jing", "Jian Yu"], "categories": ["cs.CL"], "comment": "AAAI 2025 (Oral)", "summary": "Children with Autism Spectrum Disorder (ASD) often misunderstand social\nsituations and struggle to participate in daily routines. Social Stories are\ntraditionally crafted by psychology experts under strict constraints to address\nthese challenges but are costly and limited in diversity. As Large Language\nModels (LLMs) advance, there's an opportunity to develop more automated,\naffordable, and accessible methods to generate Social Stories in real-time with\nbroad coverage. However, adapting LLMs to meet the unique and strict\nconstraints of Social Stories is a challenging issue. To this end, we propose\nSS-GEN, a Social Story GENeration framework with LLMs. Firstly, we develop a\nconstraint-driven sophisticated strategy named StarSow to hierarchically prompt\nLLMs to generate Social Stories at scale, followed by rigorous human filtering\nto build a high-quality dataset. Additionally, we introduce quality assessment\ncriteria to evaluate the effectiveness of these generated stories. Considering\nthat powerful closed-source large models require very complex instructions and\nexpensive API fees, we finally fine-tune smaller language models with our\ncurated high-quality dataset, achieving comparable results at lower costs and\nwith simpler instruction and deployment. This work marks a significant step in\nleveraging AI to personalize Social Stories cost-effectively for autistic\nchildren at scale, which we hope can encourage future research on special\ngroups.", "AI": {"tldr": "The paper proposes SS-GEN, an LLM-based framework for generating Social Stories for children with Autism Spectrum Disorder, leveraging a novel prompting strategy and fine-tuning smaller models for cost-effective solutions.", "motivation": "To provide a more automated, affordable, and accessible means of generating Social Stories for children with Autism Spectrum Disorder, overcoming limitations of traditional methods.", "method": "The SS-GEN framework uses a constraint-driven strategy called StarSow to hierarchically prompt LLMs for generating Social Stories, followed by human filtering to create a high-quality dataset. It also includes quality assessment criteria for the generated stories.", "result": "The approach enables the generation of Social Stories at scale using smaller fine-tuned language models, achieving high-quality outputs comparable to larger models but at reduced costs and complexity.", "conclusion": "SS-GEN represents a significant advancement in AI applications for personalization in the context of autism, with implications for broader research on special groups.", "key_contributions": ["Introduction of the SS-GEN framework for generating Social Stories with LLMs", "Development of the StarSow prompting strategy for hierarchical generation", "Fine-tuning of smaller models based on a high-quality dataset for cost-effective applications."], "limitations": "", "keywords": ["Autism Spectrum Disorder", "Social Stories", "Large Language Models", "AI in education", "Personalization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.17378", "pdf": "https://arxiv.org/pdf/2406.17378.pdf", "abs": "https://arxiv.org/abs/2406.17378", "title": "A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens", "authors": ["Zhijie Nie", "Richong Zhang", "Zhanyu Wu"], "categories": ["cs.CL", "cs.IR"], "comment": "ACL2025 Oral", "summary": "Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nLLM-based embedder, the obtained text embedding will be able to be aligned with\nthe key tokens in the input text. We first fully analyze this phenomenon on\neight LLM-based embedders and show that this phenomenon is universal and is not\naffected by model architecture, training strategy, and embedding method. With a\ndeeper analysis, we find that the main change in embedding space between these\nembedders and their LLM backbones is in the first principal component. By\nadjusting the first principal component, we can align text embedding with the\nkey tokens. Finally, we give several examples to demonstrate the vast\napplication potential of this finding: (1) we propose a simple and practical\nsparse retrieval method based on the aligned tokens, which can achieve 80% of\nthe dense retrieval effect of the same model while reducing the computation\nsignificantly; (2) we show that our findings provide a novel perspective to\nhelp understand novel technologies (e.g., instruction-following embedding) and\nfuzzy concepts (e.g., semantic relatedness vs. similarity) in this field.", "AI": {"tldr": "This study reveals that text embeddings from LLMs can be aligned with key tokens in input text, leading to novel applications in sparse retrieval and insights into semantic concepts.", "motivation": "To explore the alignment of text embeddings with key tokens in input text across different LLM-based embedders.", "method": "The analysis was conducted on eight LLM-based embedders to determine the universality of the alignment phenomenon, focusing on changes in the embedding space, particularly the first principal component.", "result": "The research found that adjusting the first principal component could align text embeddings with key tokens, leading to a sparse retrieval method that offers significant computational savings while achieving high performance.", "conclusion": "The findings highlight the potential for improved sparse retrieval methods and provide insights into understanding instruction-following embedding and semantic concepts.", "key_contributions": ["Demonstrated universal token alignment phenomenon in LLM-based embeddings.", "Proposed a sparse retrieval method achieving significant computational efficiency.", "Provided new perspectives on semantic relatedness versus similarity in text embeddings."], "limitations": "", "keywords": ["text embeddings", "large language models", "sparse retrieval", "semantic similarity", "information retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2407.00402", "pdf": "https://arxiv.org/pdf/2407.00402.pdf", "abs": "https://arxiv.org/abs/2407.00402", "title": "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP", "authors": ["Omer Goldman", "Alon Jacovi", "Aviv Slobodkin", "Aviya Maimon", "Ido Dagan", "Reut Tsarfaty"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2024", "summary": "Improvements in language models' capabilities have pushed their applications\ntowards longer contexts, making long-context evaluation and development an\nactive research area. However, many disparate use-cases are grouped together\nunder the umbrella term of \"long-context\", defined simply by the total length\nof the model's input, including - for example - Needle-in-a-Haystack tasks,\nbook summarization, and information aggregation. Given their varied difficulty,\nin this position paper we argue that conflating different tasks by their\ncontext length is unproductive. As a community, we require a more precise\nvocabulary to understand what makes long-context tasks similar or different. We\npropose to unpack the taxonomy of long-context based on the properties that\nmake them more difficult with longer contexts. We propose two orthogonal axes\nof difficulty: (I) Diffusion: How hard is it to find the necessary information\nin the context? (II) Scope: How much necessary information is there to find? We\nsurvey the literature on long-context, provide justification for this taxonomy\nas an informative descriptor, and situate the literature with respect to it. We\nconclude that the most difficult and interesting settings, whose necessary\ninformation is very long and highly diffused within the input, is severely\nunder-explored. By using a descriptive vocabulary and discussing the relevant\nproperties of difficulty in long-context, we can implement more informed\nresearch in this area. We call for a careful design of tasks and benchmarks\nwith distinctly long context, taking into account the characteristics that make\nit qualitatively different from shorter context.", "AI": {"tldr": "This paper discusses the challenges and complexities associated with long-context tasks in language models, advocating for a refined taxonomy to better understand these tasks.", "motivation": "To address the varied difficulty of long-context tasks in language models and to promote a more precise nomenclature.", "method": "The authors propose a taxonomy based on two axes of difficulty: Diffusion (finding necessary information) and Scope (amount of necessary information). They survey existing literature and provide justification for their proposed taxonomy.", "result": "The authors find that most interesting long-context settings are under-explored, suggesting the need for careful task design and benchmarks that distinguish between long and short contexts.", "conclusion": "A refined taxonomy can aid in better research focus and task design in the realm of long-context applications in language models.", "key_contributions": ["Proposed a taxonomy for long-context tasks based on difficulty axes.", "Identified under-explored areas in long-context applications.", "Called for better-defined tasks and benchmarks in long-context evaluations."], "limitations": "", "keywords": ["long-context", "language models", "taxonomy", "information retrieval", "task design"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.04701", "pdf": "https://arxiv.org/pdf/2409.04701.pdf", "abs": "https://arxiv.org/abs/2409.04701", "title": "Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models", "authors": ["Michael Günther", "Isabelle Mohr", "Daniel James Williams", "Bo Wang", "Han Xiao"], "categories": ["cs.CL", "cs.IR", "68T50", "I.2.7"], "comment": "11 pages, 3rd draft", "summary": "Many use cases require retrieving smaller portions of text, and dense\nvector-based retrieval systems often perform better with shorter text segments,\nas the semantics are less likely to be over-compressed in the embeddings.\nConsequently, practitioners often split text documents into smaller chunks and\nencode them separately. However, chunk embeddings created in this way can lose\ncontextual information from surrounding chunks, resulting in sub-optimal\nrepresentations. In this paper, we introduce a novel method called late\nchunking, which leverages long context embedding models to first embed all\ntokens of the long text, with chunking applied after the transformer model and\njust before mean pooling - hence the term late in its naming. The resulting\nchunk embeddings capture the full contextual information, leading to superior\nresults across various retrieval tasks. The method is generic enough to be\napplied to a wide range of long-context embedding models and works without\nadditional training. To further increase the effectiveness of late chunking, we\npropose a dedicated fine-tuning approach for embedding models.", "AI": {"tldr": "This paper introduces a method called late chunking for creating chunk embeddings that maintain full contextual information, leading to improved performance in retrieval tasks.", "motivation": "Retrieve smaller portions of text using dense vector-based retrieval systems while preserving contextual information in chunk embeddings.", "method": "Embedding all tokens of a long text first, with chunking applied after the transformer model and before mean pooling.", "result": "Late chunking leads to improved retrieval results by capturing full contextual information in chunk embeddings.", "conclusion": "The proposed method is generic for various long-context embedding models and can improve their performance without additional training.", "key_contributions": ["Introduction of late chunking method", "Maintenance of full contextual information in embeddings", "Fine-tuning approach for embedding models"], "limitations": "", "keywords": ["chunk embeddings", "long context embedding", "retrieval tasks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.09318", "pdf": "https://arxiv.org/pdf/2409.09318.pdf", "abs": "https://arxiv.org/abs/2409.09318", "title": "ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models", "authors": ["Yahan Tu", "Rui Hu", "Jitao Sang"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Hallucination poses a persistent challenge for multimodal large language\nmodels (MLLMs). However, existing benchmarks for evaluating hallucinations are\ngenerally static, which may overlook the potential risk of data contamination.\nTo address this issue, we propose ODE, an open-set, dynamic protocol designed\nto evaluate object hallucinations in MLLMs at both the existence and attribute\nlevels. ODE employs a graph-based structure to represent real-world object\nconcepts, their attributes, and the distributional associations between them.\nThis structure facilitates the extraction of concept combinations based on\ndiverse distributional criteria, generating varied samples for structured\nqueries that evaluate hallucinations in both generative and discriminative\ntasks. Through the generation of new samples, dynamic concept combinations, and\nvaried distribution frequencies, ODE mitigates the risk of data contamination\nand broadens the scope of evaluation. This protocol is applicable to both\ngeneral and specialized scenarios, including those with limited data.\nExperimental results demonstrate the effectiveness of our protocol, revealing\nthat MLLMs exhibit higher hallucination rates when evaluated with ODE-generated\nsamples, which indicates potential data contamination. Furthermore, these\ngenerated samples aid in analyzing hallucination patterns and fine-tuning\nmodels, offering an effective approach to mitigating hallucinations in MLLMs.", "AI": {"tldr": "This paper proposes ODE, a dynamic protocol for evaluating hallucinations in multimodal large language models (MLLMs), addressing limitations of existing static benchmarks.", "motivation": "Existing benchmarks for evaluating hallucinations in MLLMs are static and may overlook data contamination risks.", "method": "ODE utilizes a graph-based structure to represent real-world object concepts and their attributes, enabling the generation of varied samples for structured queries.", "result": "Experimental results indicate that MLLMs have higher hallucination rates when assessed with ODE-generated samples, highlighting data contamination issues.", "conclusion": "ODE provides an effective method for analyzing hallucination patterns in MLLMs and aids in model fine-tuning and mitigation of hallucinations.", "key_contributions": ["Introduction of a dynamic protocol (ODE) for evaluating MLLM hallucinations", "Use of a graph-based structure to manage object concepts and attributes", "Demonstration of higher hallucination rates with ODE-generated samples"], "limitations": "The applicability of ODE in highly specialized contexts is yet to be fully explored.", "keywords": ["multimodal large language models", "data contamination", "hallucination evaluation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2409.20167", "pdf": "https://arxiv.org/pdf/2409.20167.pdf", "abs": "https://arxiv.org/abs/2409.20167", "title": "Using Large Multimodal Models to Extract Knowledge Components for Knowledge Tracing from Multimedia Question Information", "authors": ["Hyeongdon Moon", "Richard Davis", "Seyed Parsa Neshaei", "Pierre Dillenbourg"], "categories": ["cs.CL"], "comment": "Accepted to Educational Data Mining 2025", "summary": "Knowledge tracing models have enabled a range of intelligent tutoring systems\nto provide feedback to students. However, existing methods for knowledge\ntracing in learning sciences are predominantly reliant on statistical data and\ninstructor-defined knowledge components, making it challenging to integrate\nAI-generated educational content with traditional established methods. We\npropose a method for automatically extracting knowledge components from\neducational content using instruction-tuned large multimodal models. We\nvalidate this approach by comprehensively evaluating it against knowledge\ntracing benchmarks in five domains. Our results indicate that the automatically\nextracted knowledge components can effectively replace human-tagged labels,\noffering a promising direction for enhancing intelligent tutoring systems in\nlimited-data scenarios, achieving more explainable assessments in educational\nsettings, and laying the groundwork for automated assessment.", "AI": {"tldr": "The paper presents a method for automatically extracting knowledge components from educational content using instruction-tuned multimodal models, aiming to improve intelligent tutoring systems.", "motivation": "Existing knowledge tracing methods depend heavily on statistical data and manual knowledge component definitions, making integration of AI-generated content difficult.", "method": "The proposed method utilizes instruction-tuned large multimodal models to automatically extract knowledge components from educational content.", "result": "The evaluation against knowledge tracing benchmarks across five domains shows that the automatically extracted components can effectively replace human-tagged labels.", "conclusion": "This approach offers a novel solution for enhancing intelligent tutoring systems, particularly in limited-data scenarios, and supports more explainable assessments in educational contexts.", "key_contributions": ["Development of a method for automatic extraction of knowledge components", "Validation against knowledge tracing benchmarks", "Improvement of explainability in intelligent tutoring systems"], "limitations": "", "keywords": ["knowledge tracing", "intelligent tutoring systems", "education technology"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.23609", "pdf": "https://arxiv.org/pdf/2410.23609.pdf", "abs": "https://arxiv.org/abs/2410.23609", "title": "On Positional Bias of Faithfulness for Long-form Summarization", "authors": ["David Wan", "Jesse Vig", "Mohit Bansal", "Shafiq Joty"], "categories": ["cs.CL"], "comment": "NAACL 2025 (20 pages)", "summary": "Large Language Models (LLMs) often exhibit positional bias in long-context\nsettings, under-attending to information in the middle of inputs. We\ninvestigate the presence of this bias in long-form summarization, its impact on\nfaithfulness, and various techniques to mitigate this bias. To consistently\nevaluate faithfulness, we first compile a benchmark of eight human-annotated\nlong-form summarization datasets and perform a meta-evaluation of faithfulness\nmetrics. We show that LLM-based faithfulness metrics, though effective with\nfull-context inputs, remain sensitive to document order, indicating positional\nbias. Analyzing LLM-generated summaries across six datasets, we find a\n\"U-shaped\" trend in faithfulness, where LLMs faithfully summarize the beginning\nand end of documents but neglect middle content. Perturbing document order\nsimilarly reveals models are less faithful when important documents are placed\nin the middle of the input. We find that this behavior is partly due to\nshifting focus with context length: as context increases, summaries become less\nfaithful, but beyond a certain length, faithfulness improves as the model\nfocuses on the end. Finally, we experiment with different generation techniques\nto reduce positional bias and find that prompting techniques effectively direct\nmodel attention to specific positions, whereas more sophisticated approaches\noffer limited improvements. Our data and code are available in\nhttps://github.com/meetdavidwan/longformfact.", "AI": {"tldr": "This paper investigates positional bias in long-form summarization using Large Language Models (LLMs) and proposes techniques to mitigate this bias while assessing its impact on summarization faithfulness.", "motivation": "The presence of positional bias in LLMs leads to neglecting important information in the middle of long contexts, impacting the quality of long-form summarization.", "method": "The authors compiled a benchmark of eight human-annotated long-form summarization datasets and performed a meta-evaluation of various faithfulness metrics, analyzing the impact of document order on the generated summaries.", "result": "The study finds a 'U-shaped' trend in faithfulness, where LLMs capture the beginning and end of documents well but under-represent the middle. Additionally, positional bias was observed to affect the faithfulness of summaries based on document order.", "conclusion": "Prompting techniques can effectively increase attention on specific document positions, while more complex methods show limited improvements, suggesting areas for further research in reducing bias.", "key_contributions": ["Identification of positional bias in LLMs during long-form summarization", "Development of a benchmark for evaluating faithfulness in summarization", "Demonstration of the U-shaped trend in LLM summary faithfulness based on input structure"], "limitations": "The effectiveness of advanced techniques for mitigating positional bias remains limited.", "keywords": ["Large Language Models", "Long-form summarization", "Positional bias", "Faithfulness metrics", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2411.04109", "pdf": "https://arxiv.org/pdf/2411.04109.pdf", "abs": "https://arxiv.org/abs/2411.04109", "title": "Self-Consistency Preference Optimization", "authors": ["Archiki Prasad", "Weizhe Yuan", "Richard Yuanzhe Pang", "Jing Xu", "Maryam Fazel-Zarandi", "Mohit Bansal", "Sainbayar Sukhbaatar", "Jason Weston", "Jane Yu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025 (camera-ready)", "summary": "Self-alignment, whereby models learn to improve themselves without human\nannotation, is a rapidly growing research area. However, existing techniques\noften fail to improve complex reasoning tasks due to the difficulty of\nassigning correct rewards. An orthogonal approach that is known to improve\ncorrectness is self-consistency, a method applied at inference time based on\nmultiple sampling in order to find the most consistent answer. In this work, we\nextend the self-consistency concept to help train models. We thus introduce\nself-consistency preference optimization (ScPO), which iteratively trains\nconsistent answers to be preferred over inconsistent ones on unsupervised new\nproblems. We show ScPO leads to large improvements over conventional reward\nmodel training on reasoning tasks such as GSM8K and MATH, closing the gap with\nsupervised training with gold answers or preferences, and that combining ScPO\nwith standard supervised learning improves results even further. On ZebraLogic,\nScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and\nClaude-3 Haiku.", "AI": {"tldr": "This paper introduces self-consistency preference optimization (ScPO), which enhances model training by prioritizing consistent answers over inconsistent ones in unsupervised settings, leading to significant improvements in reasoning tasks.", "motivation": "To address the challenges of improving complex reasoning tasks using self-alignment methods that struggle with reward assignment accuracy.", "method": "ScPO is introduced as a training method that iteratively refines models by making consistent answers more preferable than inconsistent ones without human annotation.", "result": "ScPO significantly outperforms traditional reward model training on reasoning tasks, showing improvements that approach supervised training accuracy and enhancing results further when combined with standard supervised learning.", "conclusion": "The proposed ScPO method successfully trains models like Llama-3, demonstrating substantial performance gains over larger models and established benchmarks.", "key_contributions": ["Introduction of self-consistency preference optimization (ScPO) for model training.", "Demonstrated large performance improvements on reasoning tasks such as GSM8K and MATH.", "Combined ScPO with supervised learning for even better results."], "limitations": "", "keywords": ["self-alignment", "self-consistency", "preference optimization", "reasoning tasks", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.07191", "pdf": "https://arxiv.org/pdf/2411.07191.pdf", "abs": "https://arxiv.org/abs/2411.07191", "title": "The Super Weight in Large Language Models", "authors": ["Mengxia Yu", "De Wang", "Qi Shan", "Colorado J Reed", "Alvin Wan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs.", "AI": {"tldr": "This paper reveals that a small fraction of parameters in Large Language Models (LLMs), termed super weights, are crucial for performance, and even prunes of single parameters can severely degrade model output. It introduces a method for identifying these parameters and shows how preserving specific activations can enhance quantization methods.", "motivation": "To understand the impact of specific parameters on the performance of Large Language Models and improve weight quantization techniques.", "method": "A data-free method to identify super weights and their corresponding super activations using a single forward pass through the LLM.", "result": "Pruning even a single parameter can increase perplexity drastically and drop zero-shot accuracy to random guessing. Identifying and preserving super weights and activations allows for better quantization scaling.", "conclusion": "Super weights and super activations significantly affect the capability of LLMs, and understanding their dynamics can lead to better quantization strategies and overall model performance.", "key_contributions": ["Identification of super weights and super activations in LLMs", "Demonstration of severe impacts of pruning on LLM performance", "Improved weight quantization techniques through preservation of super weights."], "limitations": "", "keywords": ["Large Language Models", "super weights", "quantization", "activations", "model performance"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.05232", "pdf": "https://arxiv.org/pdf/2412.05232.pdf", "abs": "https://arxiv.org/abs/2412.05232", "title": "LIAR: Leveraging Inference Time Alignment (Best-of-N) to Jailbreak LLMs in Seconds", "authors": ["James Beetham", "Souradip Chakraborty", "Mengdi Wang", "Furong Huang", "Amrit Singh Bedi", "Mubarak Shah"], "categories": ["cs.CL"], "comment": null, "summary": "Jailbreak attacks expose vulnerabilities in safety-aligned LLMs by eliciting\nharmful outputs through carefully crafted prompts. Existing methods rely on\ndiscrete optimization or trained adversarial generators, but are slow,\ncompute-intensive, and often impractical. We argue that these inefficiencies\nstem from a mischaracterization of the problem. Instead, we frame jailbreaks as\ninference-time misalignment and introduce LIAR (Leveraging Inference-time\nmisAlignment to jailbReak), a fast, black-box, best-of-$N$ sampling attack\nrequiring no training. LIAR matches state-of-the-art success rates while\nreducing perplexity by $10\\times$ and Time-to-Attack from hours to seconds. We\nalso introduce a theoretical \"safety net against jailbreaks\" metric to quantify\nsafety alignment strength and derive suboptimality bounds. Our work offers a\nsimple yet effective tool for evaluating LLM robustness and advancing alignment\nresearch.", "AI": {"tldr": "LIAR is a fast, black-box attack method for jailbreaks in LLMs, significantly improving efficiency and effectiveness compared to existing methods.", "motivation": "Present limitations in existing jailbreak attack methods prompt a re-evaluation and new approach to ensure LLMs' safety and robustness.", "method": "LIAR frames jailbreaks as inference-time misalignment and utilizes a fast, best-of-$N$ sampling attack that requires no training.", "result": "LIAR achieves comparable success rates to existing methods while reducing perplexity by $10\\times$ and Time-to-Attack from hours to seconds.", "conclusion": "LIAR provides an effective tool for evaluating the robustness of LLMs and promotes advancements in alignment research.", "key_contributions": ["Introduction of LIAR, a fast and efficient jailbreak attack method.", "Development of a safety metric for evaluating safety alignment against jailbreaks.", "Reduction of Time-to-Attack and perplexity compared to traditional methods."], "limitations": "", "keywords": ["jailbreak attacks", "LLMs", "safety alignment", "inference-time misalignment", "robustness evaluation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2412.15251", "pdf": "https://arxiv.org/pdf/2412.15251.pdf", "abs": "https://arxiv.org/abs/2412.15251", "title": "AgentPS: Agentic Process Supervision for Content Moderation with Multimodal LLMs", "authors": ["Mingchao Liu", "Yu Sun", "Ruixiao Sun", "Xin Dong", "Xiang Shen", "Hongyu Xiong"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "The advanced processing and reasoning capabilities of multimodal large\nlanguage models (MLLMs) have driven substantial progress in vision-language\n(VL) understanding tasks. However, while effective for tasks governed by\nstraightforward logic, MLLMs often struggle with reasoning complex,\ndetail-intensive logical structures. To address this limitation, we introduce\nAgentPS, a novel framework that integrates Agentic Process Supervision into\nMLLMs by sequentially reasoning over ancillary questions during fine-tuning.\nAgentPS achieves substantial improvements over baseline MLLMs on both public\nbenchmarks and proprietary datasets. Notably, we show that using MLLM-generated\nancillary labels in place of human annotations yields only minimal performance\ndegradation, highlighting the method's scalability. These results establish\nAgentPS as a scalable and effective solution for complex multimodal\nclassification in large-scale industrial applications.", "AI": {"tldr": "AgentPS enhances MLLMs' reasoning in vision-language tasks by incorporating ancillary question supervision during fine-tuning.", "motivation": "To overcome the limitations of MLLMs in reasoning complex logical structures effectively in vision-language understanding tasks.", "method": "AgentPS integrates Agentic Process Supervision into MLLMs by sequentially reasoning over ancillary questions during the fine-tuning phase.", "result": "AgentPS demonstrates significant improvement over baseline MLLMs on public benchmarks and proprietary datasets. Additionally, it shows that MLLM-generated ancillary labels maintain performance effectiveness with minimal degradation compared to human annotations.", "conclusion": "AgentPS is a scalable and effective framework for improving complex multimodal classification in large-scale industrial applications.", "key_contributions": ["Introduction of AgentPS framework for improving MLLMs' reasoning capabilities.", "Demonstration of minimal performance loss when using MLLM-generated labels instead of human annotations.", "Substantial improvements on various benchmarks for vision-language understanding tasks."], "limitations": "", "keywords": ["Multimodal Large Language Models", "Vision-Language Understanding", "Agentic Process Supervision"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.16976", "pdf": "https://arxiv.org/pdf/2412.16976.pdf", "abs": "https://arxiv.org/abs/2412.16976", "title": "On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity Recognition in Health Corpora", "authors": ["Tzu-Chieh Chen", "Wen-Yang Lin"], "categories": ["cs.CL", "cs.AI", "I.2.7; J.3"], "comment": "13 pages; a short version has been accpeted for presentation at\n  MedInfo2025", "summary": "Named Entity Recognition has traditionally been a key task in natural\nlanguage processing, aiming to identify and extract important terms from\nunstructured text data. However, a notable challenge for contemporary\ndeep-learning NER models has been identifying discontinuous entities, which are\noften fragmented within the text. To date, methods to address Discontinuous\nNamed Entity Recognition have not been explored using ensemble learning to the\nbest of our knowledge. Furthermore, the rise of large language models, such as\nChatGPT in recent years, has shown significant effectiveness across many NLP\ntasks. Most existing approaches, however, have primarily utilized ChatGPT as a\nproblem-solving tool rather than exploring its potential as an integrative\nelement within ensemble learning algorithms. In this study, we investigated the\nintegration of ChatGPT as an arbitrator within an ensemble method, aiming to\nenhance performance on DNER tasks. Our method combines five state-of-the-art\nNER models with ChatGPT using custom prompt engineering to assess the\nrobustness and generalization capabilities of the ensemble algorithm. We\nconducted experiments on three benchmark medical datasets, comparing our method\nagainst the five SOTA models, individual applications of GPT-3.5 and GPT-4, and\na voting ensemble method. The results indicate that our proposed fusion of\nChatGPT with the ensemble learning algorithm outperforms the SOTA results in\nthe CADEC, ShARe13, and ShARe14 datasets, showcasing its potential to enhance\nNLP applications in the healthcare domain.", "AI": {"tldr": "This paper explores the integration of ChatGPT within an ensemble learning framework to improve Discontinuous Named Entity Recognition (DNER) tasks, particularly in the healthcare domain.", "motivation": "The motivation behind this study is to address the challenge of identifying discontinuous entities in Named Entity Recognition, which has been minimally explored with ensemble learning. Additionally, there is interest in leveraging large language models like ChatGPT as part of ensemble methods rather than just as tools for problem-solving.", "method": "The authors integrated ChatGPT as an arbitrator within an ensemble learning method that combines five state-of-the-art Named Entity Recognition models through prompt engineering. They experimented on three medical datasets.", "result": "The experiments demonstrated that the proposed integration of ChatGPT with the ensemble learning algorithm outperformed the state-of-the-art results on the CADEC, ShARe13, and ShARe14 datasets.", "conclusion": "The study concludes that the fusion of ChatGPT within ensemble methods shows promise in enhancing performance on DNER tasks in healthcare applications.", "key_contributions": ["Integration of ChatGPT as an arbitrator within ensemble learning for DNER tasks", "Demonstration of improved performance over state-of-the-art models", "Evaluation on benchmark medical datasets"], "limitations": "", "keywords": ["Named Entity Recognition", "Discontinuous Entities", "Ensemble Learning", "ChatGPT", "Healthcare NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.08203", "pdf": "https://arxiv.org/pdf/2501.08203.pdf", "abs": "https://arxiv.org/abs/2501.08203", "title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving", "authors": ["Zain Ul Abedin", "Shahzeb Qamar", "Lucie Flek", "Akbar Karimi"], "categories": ["cs.CL"], "comment": "Accepted to LLMSEC Workshop at ACL 2025", "summary": "While Large Language Models (LLMs) have shown impressive capabilities in math\nproblem-solving tasks, their robustness to noisy inputs is not well-studied. We\npropose ArithmAttack to examine how robust the LLMs are when they encounter\nnoisy prompts that contain extra noise in the form of punctuation marks. While\nbeing easy to implement, ArithmAttack does not cause any information loss since\nwords are not added or deleted from the context. We evaluate the robustness of\neight LLMs, including LLama3, Mistral, Mathstral, and DeepSeek on noisy GSM8K\nand MultiArith datasets. Our experiments suggest that all the studied models\nshow vulnerability to such noise, with more noise leading to poorer\nperformances.", "AI": {"tldr": "This paper proposes ArithmAttack, a method to evaluate the robustness of Large Language Models (LLMs) against noisy mathematical prompts, revealing vulnerabilities in their performance due to additional punctuation noise.", "motivation": "To investigate the robustness of LLMs in math problem-solving tasks when faced with noisy inputs.", "method": "ArithmAttack evaluates LLMs by introducing noise in the form of punctuation marks in prompts, without altering the context by adding or deleting words.", "result": "The evaluation of eight LLMs showed that they are vulnerable to noise, with increased noise leading to worse performance on the GSM8K and MultiArith datasets.", "conclusion": "LLMs exhibit significant vulnerabilities when processing noisy inputs, highlighting the need for improved robustness in these models.", "key_contributions": ["Introduction of ArithmAttack to assess LLM robustness", "Evaluation of eight LLMs on noise-affected datasets", "Findings that indicate performance degradation with increased prompt noise."], "limitations": "The study primarily focuses on noise from punctuation marks and may not encompass other types of noise or contexts.", "keywords": ["Large Language Models", "robustness", "ArithmAttack", "mathematics", "noisy inputs"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2501.08276", "pdf": "https://arxiv.org/pdf/2501.08276.pdf", "abs": "https://arxiv.org/abs/2501.08276", "title": "Exploring Robustness of LLMs to Paraphrasing Based on Sociodemographic Factors", "authors": ["Pulkit Arora", "Akbar Karimi", "Lucie Flek"], "categories": ["cs.CL"], "comment": null, "summary": "Despite their linguistic prowess, LLMs have been shown to be vulnerable to\nsmall input perturbations. While robustness to local adversarial changes has\nbeen studied, robustness to global modifications such as different linguistic\nstyles remains underexplored. Therefore, we take a broader approach to explore\na wider range of variations across sociodemographic dimensions. We extend the\nSocialIQA dataset to create diverse paraphrased sets conditioned on\nsociodemographic factors (age and gender). The assessment aims to provide a\ndeeper understanding of LLMs in (a) their capability of generating demographic\nparaphrases with engineered prompts and (b) their capabilities in interpreting\nreal-world, complex language scenarios. We also perform a reliability analysis\nof the generated paraphrases looking into linguistic diversity and perplexity\nas well as manual evaluation. We find that demographic-based paraphrasing\nsignificantly impacts the performance of language models, indicating that the\nsubtleties of linguistic variation remain a significant challenge. We will make\nthe code and dataset available for future research.", "AI": {"tldr": "This paper explores the robustness of large language models (LLMs) to demographic-based paraphrasing, revealing significant performance impacts due to linguistic variations across age and gender.", "motivation": "To investigate the vulnerability of LLMs to global linguistic modifications, particularly those influenced by sociodemographic factors, which remain underexplored compared to local adversarial perturbations.", "method": "The authors extend the SocialIQA dataset to create diverse paraphrased sets that consider sociodemographic dimensions (age and gender) and perform a reliability analysis on the generated paraphrases involving linguistic diversity and perplexity assessments, along with manual evaluation.", "result": "The study finds that demographic-based paraphrasing significantly affects the performance of language models, highlighting challenges due to linguistic variations.", "conclusion": "The results indicate that LLMs struggle with subtle variations in language related to age and gender, underscoring a need for further research in this area; codes and datasets for this study will be accessible for future use.", "key_contributions": ["Extended the SocialIQA dataset with demographic-based parity sets", "Conducted a comprehensive reliability analysis on paraphrase generation", "Demonstrated the impact of linguistic diversity on LLM performance"], "limitations": "", "keywords": ["Large Language Models", "Demographic Paraphrasing", "Sociodemographic Analysis", "Language Diversity"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.17635", "pdf": "https://arxiv.org/pdf/2501.17635.pdf", "abs": "https://arxiv.org/abs/2501.17635", "title": "In-Context Meta LoRA Generation", "authors": ["Yihua Shao", "Minxi Yan", "Yang Liu", "Siyu Chen", "Wenjie Chen", "Xinwei Long", "Ziyang Yan", "Lei Li", "Chenyu Zhang", "Nicu Sebe", "Hao Tang", "Yan Wang", "Hao Zhao", "Mengzhu Wang", "Jingcai Guo"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.", "AI": {"tldr": "ICM-LoRA is an efficient method for customizing large language models by generating task-specific LoRA weights using Conditional Variational Autoencoder (CVAE), addressing inefficiencies in multi-task scenarios.", "motivation": "To improve the efficiency of Low-rank Adaptation (LoRA) when fine-tuning for multiple tasks and to better capture correlations among tasks.", "method": "The proposed In-Context Meta LoRA (ICM-LoRA) uses a Conditional Variational Autoencoder to generate task-aware LoRA weights from task descriptions, merging these weights with LLMs to create specialized models without additional fine-tuning.", "result": "ICM-LoRA achieves more accurate LoRA parameter generation for various tasks and occupies only 283MB of storage, significantly less than traditional methods.", "conclusion": "The proposed method shows improvements in parameter reconstruction accuracy for diverse tasks while maintaining efficient storage requirements.", "key_contributions": ["Introduction of In-Context Meta LoRA for task-specific customization", "Utilization of Conditional Variational Autoencoder for generating LoRA weights", "Reduction of storage requirements to 1% compared to original LoRA"], "limitations": "", "keywords": ["Low-rank Adaptation", "Conditional Variational Autoencoder", "Multi-task Learning", "Large Language Models", "Efficient Fine-tuning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2502.04625", "pdf": "https://arxiv.org/pdf/2502.04625.pdf", "abs": "https://arxiv.org/abs/2502.04625", "title": "Phonetic Reconstruction of the Consonant System of Middle Chinese via Mixed Integer Optimization", "authors": ["Xiaoxi Luo", "Weiwei Sun"], "categories": ["cs.CL"], "comment": null, "summary": "This paper is concerned with phonetic reconstruction of the consonant system\nof Middle Chinese. We propose to cast the problem as a Mixed Integer\nProgramming problem, which is able to automatically explore homophonic\ninformation from ancient rhyme dictionaries and phonetic information from\nmodern Chinese dialects, the descendants of Middle Chinese. Numerical\nevaluation on a wide range of synthetic and real data demonstrates the\neffectiveness and robustness of the new method. We apply the method to\ninformation from Guangyun and 20 modern Chinese dialects to obtain a new\nphonetic reconstruction result. A linguistically-motivated discussion of this\nresult is also provided.", "AI": {"tldr": "The paper presents a Mixed Integer Programming approach to reconstruct the consonant system of Middle Chinese using homophonic data from ancient dictionaries and modern dialects.", "motivation": "To explore the phonetic reconstruction of Middle Chinese using linguistic data.", "method": "The study employs Mixed Integer Programming to analyze homophonic information from ancient rhyme dictionaries and phonetic data from modern Chinese dialects.", "result": "The methodology proves effective and robust in numerical evaluations across various data, resulting in a new phonetic reconstruction of Middle Chinese.", "conclusion": "A linguistically-motivated discussion of the new reconstruction result is provided.", "key_contributions": ["Application of Mixed Integer Programming to linguistic data", "Integration of ancient and modern phonetic information", "New phonetic reconstruction results for Middle Chinese"], "limitations": "", "keywords": ["Middle Chinese", "phonetic reconstruction", "Mixed Integer Programming"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2502.15343", "pdf": "https://arxiv.org/pdf/2502.15343.pdf", "abs": "https://arxiv.org/abs/2502.15343", "title": "Tokenization is Sensitive to Language Variation", "authors": ["Anna Wegmann", "Dong Nguyen", "David Jurgens"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Variation in language is ubiquitous and often systematically linked to\nregional, social, and contextual factors. Tokenizers split texts into smaller\nunits and might behave differently for less common linguistic forms. This might\naffect downstream LLM performance differently on two types of tasks: Tasks\nwhere the model should be robust to language variation (e.g., for semantic\ntasks like NLI, labels do not depend on whether a text uses British or American\nspelling) and tasks where the model should be sensitive to language variation\n(e.g., for form-based tasks like authorship verification, labels depend on\nwhether a text uses British or American spelling). We pre-train BERT base\nmodels with the popular Byte-Pair Encoding algorithm to investigate how key\ntokenization design choices impact the performance of downstream models: the\ncorpus used to train the tokenizer, the pre-tokenizer and the vocabulary size.\nWe find that the best tokenizer varies on the two task types and that the\npre-tokenizer has the biggest overall impact on performance. Further, we\nintroduce a new approach to estimate tokenizer impact on downstream LLM\nperformance, showing substantial improvement over metrics like R\\'enyi\nefficiency. We encourage more work on language variation and its relation to\ntokenizers and thus LLM performance.", "AI": {"tldr": "The paper investigates how tokenization strategies affect the performance of LLMs on various tasks influenced by language variation.", "motivation": "Understanding how different tokenization strategies impact language model performance in relation to linguistic variations is critical for improving NLP tasks.", "method": "The study pre-trains BERT base models using the Byte-Pair Encoding algorithm, varying the corpus for training the tokenizer, the pre-tokenizer, and vocabulary size to analyze their effects on performance across different types of downstream tasks.", "result": "The research shows that the optimal tokenizer for downstream tasks depends on the task type, with the pre-tokenizer significantly influencing model performance.", "conclusion": "The findings indicate a need for further exploration of the relationship between language variation, tokenization, and LLM performance, and present a novel method for estimating tokenizer impact.", "key_contributions": ["Demonstration of the differential impact of tokenization on LLM performance based on task type.", "Introduction of a new approach for estimating tokenizer impact on downstream model performance that surpasses existing metrics.", "Encouragement for further research into the implications of linguistic variation for tokenization in NLP."], "limitations": "", "keywords": ["tokenization", "language variation", "LLM performance", "BERT models", "NLP tasks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.19279", "pdf": "https://arxiv.org/pdf/2502.19279.pdf", "abs": "https://arxiv.org/abs/2502.19279", "title": "CritiQ: Mining Data Quality Criteria from Human Preferences", "authors": ["Honglin Guo", "Kai Lv", "Qipeng Guo", "Tianyi Liang", "Zhiheng Xi", "Demin Song", "Qiuyinzhe Zhang", "Yu Sun", "Kai Chen", "Xipeng Qiu", "Tao Gui"], "categories": ["cs.CL"], "comment": "to be published in ACL 2025, Code is available at\n  https://github.com/KYLN24/CritiQ", "summary": "Language model heavily depends on high-quality data for optimal performance.\nExisting approaches rely on manually designed heuristics, the perplexity of\nexisting models, training classifiers, or careful prompt engineering, which\nrequire significant expert experience and human annotation effort while\nintroduce biases. We introduce CritiQ, a novel data selection method that\nautomatically mines criteria from human preferences for data quality with only\n~30 human-annotated pairs and performs efficient data selection. The main\ncomponent, CritiQ Flow, employs a manager agent to evolve quality criteria and\nworker agents to make pairwise judgments. We build a knowledge base that\nextracts quality criteria from previous work to boost CritiQ Flow. Compared to\nperplexity- and classifier- based methods, verbal criteria are more\ninterpretable and possess reusable value. After deriving the criteria, we train\nthe CritiQ Scorer to give quality scores and perform efficient data selection.\nWe demonstrate the effectiveness of our method in the code, math, and logic\ndomains, achieving high accuracy on human-annotated test sets. To validate the\nquality of the selected data, we continually train Llama 3.1 models and observe\nimproved performance on downstream tasks compared to uniform sampling. Ablation\nstudies validate the benefits of the knowledge base and the reflection process.\nWe analyze how criteria evolve and the effectiveness of majority voting.", "AI": {"tldr": "This paper introduces CritiQ, a new data selection method that leverages human preferences to improve data quality for language models with minimal human annotation.", "motivation": "The motivation behind CritiQ is to enhance language model performance through better data quality selection, avoiding the biases and intensive human efforts involved in existing methods.", "method": "CritiQ uses a manager agent to develop quality criteria and worker agents for pairwise judgments, leading to efficient data selection based on minimal human input and a knowledge base to extract criteria from previous studies.", "result": "The method was tested in code, math, and logic domains, showing high accuracy on human-annotated test sets and improved performance in downstream tasks when training Llama 3.1 models, surpassing traditional sampling methods.", "conclusion": "CritiQ demonstrated a more interpretable and effective approach to data selection for language models, highlighting the utility of human-derived quality criteria.", "key_contributions": ["Introduces a novel data selection method based on human preferences.", "Reduces the need for extensive human annotation by developing criteria from minimal pairs.", "Improves downstream task performance using selected high-quality data."], "limitations": "", "keywords": ["data selection", "language models", "human preferences", "criteria evolution", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.03335", "pdf": "https://arxiv.org/pdf/2503.03335.pdf", "abs": "https://arxiv.org/abs/2503.03335", "title": "iNews: A Multimodal Dataset for Modeling Personalized Affective Responses to News", "authors": ["Tiancheng Hu", "Nigel Collier"], "categories": ["cs.CL", "cs.CY"], "comment": "Dataset available at https://huggingface.co/datasets/pitehu/inews", "summary": "Understanding how individuals perceive and react to information is\nfundamental for advancing social and behavioral sciences and developing\nhuman-centered AI systems. Current approaches often lack the granular data\nneeded to model these personalized responses, relying instead on aggregated\nlabels that obscure the rich variability driven by individual differences. We\nintroduce iNews, a novel large-scale dataset specifically designed to\nfacilitate the modeling of personalized affective responses to news content.\nOur dataset comprises annotations from 291 demographically diverse UK\nparticipants across 2,899 multimodal Facebook news posts from major UK outlets,\nwith an average of 5.18 annotators per sample. For each post, annotators\nprovide multifaceted labels including valence, arousal, dominance, discrete\nemotions, content relevance judgments, sharing likelihood, and modality\nimportance ratings. Crucially, we collect comprehensive annotator persona\ninformation covering demographics, personality, media trust, and consumption\npatterns, which explain 15.2% of annotation variance - substantially higher\nthan existing NLP datasets. Incorporating this information yields a 7% accuracy\ngain in zero-shot prediction and remains beneficial even with 32-shot\nin-context learning. iNews opens new possibilities for research in LLM\npersonalization, subjectivity, affective computing, and human behavior\nsimulation.", "AI": {"tldr": "Introduction of iNews, a novel dataset for modeling personalized affective responses to news content.", "motivation": "To advance social and behavioral sciences and human-centered AI systems by understanding individual reactions to information.", "method": "Collection of a large dataset with annotations from 291 diverse participants on 2,899 Facebook news posts, capturing multifaceted affective responses and detailed annotator persona information.", "result": "The dataset explains 15.2% of annotation variance and leads to a 7% accuracy gain in affective response prediction in NLP tasks.", "conclusion": "iNews supports research in LLM personalization, affective computing, and simulating human behavior.", "key_contributions": ["Introduction of a dataset that captures detailed individual responses to news content.", "Incorporation of comprehensive annotator persona information for improved prediction accuracy.", "Facilitates advancements in personalization within NLP and LLM applications."], "limitations": "", "keywords": ["dataset", "personalized responses", "affective computing", "human behavior", "NLP"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2503.08963", "pdf": "https://arxiv.org/pdf/2503.08963.pdf", "abs": "https://arxiv.org/abs/2503.08963", "title": "Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation", "authors": ["Yu Wang", "Kamalika Das", "Xiang Gao", "Wendi Cui", "Peng Li", "Jiaxin Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted as Finding of NAACL 2025", "summary": "In tasks like summarization and open-book question answering (QA), Large\nLanguage Models (LLMs) often encounter \"contextual hallucination\", where they\nproduce irrelevant or incorrect responses despite having access to accurate\nsource information. This typically occurs because these models tend to\nprioritize self-generated content over the input context, causing them to\ndisregard pertinent details. To address this challenge, we introduce a novel\nmethod called \"Guided Attention Map Editing\" (GAME), which dynamically adjusts\nattention maps to improve contextual relevance. During inference, GAME employs\na trained classifier to identify attention maps prone to inducing\nhallucinations and executes targeted interventions. These interventions, guided\nby gradient-informed \"edit directions'', strategically redistribute attention\nweights across various heads to effectively reduce hallucination. Comprehensive\nevaluations on challenging summarization and open-book QA tasks show that GAME\nconsistently reduces hallucinations across a variety of open-source models.\nSpecifically, GAME reduces hallucinations by 10% in the XSum summarization task\nwhile achieving a 7X speed-up in computational efficiency compared to the\nstate-of-the-art baselines.", "AI": {"tldr": "This paper presents GAME, a method to reduce contextual hallucination in LLMs by editing attention maps during inference.", "motivation": "Large Language Models often produce irrelevant or hallucinated responses due to prioritizing self-generated content over input context.", "method": "Game uses a trained classifier to identify problematic attention maps and guides their editing through gradient-informed directions to redistribute attention weights.", "result": "GAME reduces hallucinations by 10% in the XSum summarization task and achieves a 7X increase in computational efficiency over state-of-the-art models.", "conclusion": "The evaluations demonstrate that GAME effectively mitigates hallucinations in summarization and QA tasks across various models.", "key_contributions": ["Introduction of Guided Attention Map Editing (GAME) for LLMs", "Reduction of hallucinations by 10% in XSum task", "7X speed-up in computational efficiency compared to existing methods"], "limitations": "", "keywords": ["Large Language Models", "contextual hallucination", "attention maps", "summarization", "question answering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.09701", "pdf": "https://arxiv.org/pdf/2503.09701.pdf", "abs": "https://arxiv.org/abs/2503.09701", "title": "Have LLMs Made Active Learning Obsolete? Surveying the NLP Community", "authors": ["Julia Romberg", "Christopher Schröder", "Julius Gonsior", "Katrin Tomanek", "Fredrik Olsson"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Supervised learning relies on annotated data, which is expensive to obtain. A\nlongstanding strategy to reduce annotation costs is active learning, an\niterative process, in which a human annotates only data instances deemed\ninformative by a model. Large language models (LLMs) have pushed the\neffectiveness of active learning, while also advancing methods such as few- or\nzero-shot learning, and text synthesis -- all of which can reduce the need for\nactive learning. This naturally raises the question: has active learning become\nobsolete? To answer this fully, we must look beyond literature to practical\nexperiences. We conduct an online survey in the NLP community to collect\npreviously intangible insights on the perceived relevance of data annotation,\nparticularly focusing on active learning, including best practices, obstacles,\nand future prospects. Our findings show that annotated data is expected to\nremain a key factor and active learning to stay highly relevant while\nbenefiting from LLMs. Consistent with a community survey from over a decade\nago, however, we find that three key challenges persist -- setup complexity,\nrisks in the cost reduction, and tooling -- for which we propose alleviation\nstrategies. We publish an anonymized version of the collected dataset.", "AI": {"tldr": "This paper explores the relevance of active learning in the era of large language models, based on insights gained from a survey conducted in the NLP community.", "motivation": "The paper aims to understand if active learning has become obsolete with the rise of large language models and to gather community insights on the relevance of data annotation.", "method": "An online survey was conducted within the NLP community to collect insights on the perceived relevance of data annotation, particularly focusing on active learning and its challenges.", "result": "The findings indicate that annotated data will continue to be important and that active learning remains relevant, especially when integrated with LLMs.", "conclusion": "Despite advancements, active learning faces persistent challenges like setup complexity and tooling, which need addressing for better adoption.", "key_contributions": ["Insights from the NLP community on active learning relevance", "Identification of challenges in active learning", "Proposed alleviation strategies for these challenges"], "limitations": "The study relies on self-reported survey data, which may be biased.", "keywords": ["active learning", "data annotation", "large language models", "NLP", "community survey"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.12440", "pdf": "https://arxiv.org/pdf/2503.12440.pdf", "abs": "https://arxiv.org/abs/2503.12440", "title": "HKCanto-Eval: A Benchmark for Evaluating Cantonese Language Understanding and Cultural Comprehension in LLMs", "authors": ["Tsz Chung Cheng", "Chung Shing Cheng", "Chaak Ming Lau", "Eugene Tin-Ho Lam", "Chun Yat Wong", "Hoi On Yu", "Cheuk Hei Chong"], "categories": ["cs.CL"], "comment": null, "summary": "The ability of language models to comprehend and interact in diverse\nlinguistic and cultural landscapes is crucial. The Cantonese language used in\nHong Kong presents unique challenges for natural language processing due to its\nrich cultural nuances and lack of dedicated evaluation datasets. The\nHKCanto-Eval benchmark addresses this gap by evaluating the performance of\nlarge language models (LLMs) on Cantonese language understanding tasks,\nextending to English and Written Chinese for cross-lingual evaluation.\nHKCanto-Eval integrates cultural and linguistic nuances intrinsic to Hong Kong,\nproviding a robust framework for assessing language models in realistic\nscenarios. Additionally, the benchmark includes questions designed to tap into\nthe underlying linguistic metaknowledge of the models. Our findings indicate\nthat while proprietary models generally outperform open-weight models,\nsignificant limitations remain in handling Cantonese-specific linguistic and\ncultural knowledge, highlighting the need for more targeted training data and\nevaluation methods. The code can be accessed at\nhttps://github.com/hon9kon9ize/hkeval2025", "AI": {"tldr": "The HKCanto-Eval benchmark evaluates large language models on Cantonese language understanding tasks, addressing the lack of dedicated datasets.", "motivation": "To assess the performance of language models in the context of the Cantonese language, which has unique cultural and linguistic nuances.", "method": "Development of the HKCanto-Eval benchmark for evaluating language models on Cantonese, English, and Written Chinese with a focus on cultural context.", "result": "Proprietary models outperform open-weight models, but both types struggle with Cantonese-specific knowledge, indicating a need for improved training data.", "conclusion": "There are significant limitations in current models for understanding Cantonese culture and language, suggesting the necessity for better datasets and evaluation methods.", "key_contributions": ["Introduction of the HKCanto-Eval benchmark for Cantonese language evaluation", "Cross-lingual evaluation integrating English and Written Chinese", "Insights into the performance disparities between proprietary and open-weight models"], "limitations": "Current models have limitations in handling Cantonese-specific linguistic and cultural knowledge.", "keywords": ["Cantonese", "Language Models", "Natural Language Processing", "Benchmarking", "Cross-lingual Evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.14917", "pdf": "https://arxiv.org/pdf/2503.14917.pdf", "abs": "https://arxiv.org/abs/2503.14917", "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models", "authors": ["Jiazheng Li", "Lu Yu", "Qing Cui", "Zhiqiang Zhang", "Jun Zhou", "Yanfang Ye", "Chuxu Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High-quality data plays a critical role in the pretraining and fine-tuning of\nlarge language models (LLMs), even determining their performance ceiling to\nsome degree. Consequently, numerous data selection methods have been proposed\nto identify subsets of data that can effectively and efficiently enhance model\nperformance. However, most of these methods focus on general data selection and\ntend to overlook the specific nuances of domain-related data. In this paper, we\nintroduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using\nthe \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning\ndomain. By taking into account the unique characteristics of mathematics and\nreasoning, we construct a skill graph that captures the mathematical skills and\ntheir interrelations from a reference dataset. This skill graph guides us in\nassigning quality scores to the target dataset, enabling us to select the\ntop-ranked subset which is further used to pretrain LLMs. Experimental results\ndemonstrate the efficiency and effectiveness of MASS across different model\nsizes (1B and 7B) and pretraining datasets (web data and synthetic data).\nSpecifically, in terms of efficiency, models trained on subsets selected by\nMASS can achieve similar performance to models trained on the original\ndatasets, with a significant reduction in the number of trained tokens -\nranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained\non the same amount of tokens, models trained on the data selected by MASS\noutperform those trained on the original datasets by 3.3\\% to 5.9\\%. These\nresults underscore the potential of MASS to improve both the efficiency and\neffectiveness of pretraining LLMs.", "AI": {"tldr": "The paper introduces MASS, a data selection framework for pretraining LLMs in the mathematical reasoning domain that significantly enhances efficiency and effectiveness.", "motivation": "High-quality data is crucial for the performance of large language models (LLMs), and existing data selection methods often neglect domain-specific nuances.", "method": "MASS uses a skill graph that captures mathematical skills and their interrelations to assign quality scores to datasets and select the top-ranked subset for pretraining LLMs.", "result": "Models pretrained on subsets selected by MASS achieved similar performance to those trained on original datasets while using 50-70% fewer tokens and outperformed them by 3.3-5.9% with the same token amount.", "conclusion": "MASS improves both efficiency and effectiveness in pretraining LLMs for mathematical reasoning.", "key_contributions": ["Introduction of the MASS framework for domain-specific data selection", "Use of a skill graph to enhance data quality assessment", "Demonstrated efficiency and effectiveness improvements in LLM pretraining"], "limitations": "", "keywords": ["data selection", "large language models", "mathematical reasoning", "skill graph", "pretraining"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.15220", "pdf": "https://arxiv.org/pdf/2503.15220.pdf", "abs": "https://arxiv.org/abs/2503.15220", "title": "Entity-aware Cross-lingual Claim Detection for Automated Fact-checking", "authors": ["Rrubaa Panchendrarajan", "Arkaitz Zubiaga"], "categories": ["cs.CL"], "comment": null, "summary": "Identifying claims requiring verification is a critical task in automated\nfact-checking, especially given the proliferation of misinformation on social\nmedia platforms. Despite notable progress, challenges remain-particularly in\nhandling multilingual data prevalent in online discourse. Recent efforts have\nfocused on fine-tuning pre-trained multilingual language models to address\nthis. While these models can handle multiple languages, their ability to\neffectively transfer cross-lingual knowledge for detecting claims spreading on\nsocial media remains under-explored. In this paper, we introduce EX-Claim, an\nentity-aware cross-lingual claim detection model that generalizes well to\nhandle multilingual claims. The model leverages entity information derived from\nnamed entity recognition and entity linking techniques to improve the\nlanguage-level performance of both seen and unseen languages during training.\nExtensive experiments conducted on three datasets from different social media\nplatforms demonstrate that our proposed model stands out as an effective\nsolution, demonstrating consistent performance gains across 27 languages and\nrobust knowledge transfer between languages seen and unseen during training.", "AI": {"tldr": "EX-Claim is an entity-aware cross-lingual claim detection model designed to improve verification of multilingual claims on social media.", "motivation": "The proliferation of misinformation on social media necessitates the development of effective automated fact-checking systems that can handle multilingual data.", "method": "The model incorporates entity information from named entity recognition and linking to enhance performance on multilingual claim detection.", "result": "Results show consistent performance improvements across 27 languages, with robust cross-lingual knowledge transfer.", "conclusion": "The proposed EX-Claim model offers an effective solution for multilingual claim detection, outperforming existing models.", "key_contributions": ["Introduces a novel model for cross-lingual claim detection", "Implements entity-aware techniques to enhance performance", "Demonstrates effective knowledge transfer across languages"], "limitations": "", "keywords": ["cross-lingual", "claim detection", "fact-checking", "multilingual", "entity awareness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.18751", "pdf": "https://arxiv.org/pdf/2503.18751.pdf", "abs": "https://arxiv.org/abs/2503.18751", "title": "Construction Identification and Disambiguation Using BERT: A Case Study of NPN", "authors": ["Wesley Scivetti", "Nathan Schneider"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, ACL long-paper format. Camera-ready version, published at\n  the 29th Conference on Computational Natural Language Learning (CoNLL 2025)", "summary": "Construction Grammar hypothesizes that knowledge of a language consists\nchiefly of knowledge of form-meaning pairs (''constructions'') that include\nvocabulary, general grammar rules, and even idiosyncratic patterns. Recent work\nhas shown that transformer language models represent at least some\nconstructional patterns, including ones where the construction is rare overall.\nIn this work, we probe BERT's representation of the form and meaning of a minor\nconstruction of English, the NPN (noun-preposition-noun) construction --\nexhibited in such expressions as face to face and day to day -- which is known\nto be polysemous. We construct a benchmark dataset of semantically annotated\ncorpus instances (including distractors that superficially resemble the\nconstruction). With this dataset, we train and evaluate probing classifiers.\nThey achieve decent discrimination of the construction from distractors, as\nwell as sense disambiguation among true instances of the construction,\nrevealing that BERT embeddings carry indications of the construction's\nsemantics. Moreover, artificially permuting the word order of true construction\ninstances causes them to be rejected, indicating sensitivity to matters of\nform. We conclude that BERT does latently encode at least some knowledge of the\nNPN construction going beyond a surface syntactic pattern and lexical cues.", "AI": {"tldr": "This study probes BERT's understanding of a specific English construction, the noun-preposition-noun (NPN) structure, revealing its ability to encode semantic and syntactic features.", "motivation": "To investigate how transformer language models like BERT represent constructional patterns, specifically the NPN construction in English, which is known to have multiple meanings.", "method": "A benchmark dataset of semantically annotated instances of the NPN construction was constructed. Probing classifiers were trained and evaluated to distinguish the construction from distractors and to perform sense disambiguation.", "result": "The classifiers demonstrated good discrimination capabilities, indicating that BERT embeddings represent the semantics of the NPN construction. Additionally, permuting the word order of true instances led to rejection, showing sensitivity to form.", "conclusion": "BERT encodes knowledge of the NPN construction that extends beyond basic syntactic patterns and lexical cues, suggesting deeper linguistic understanding.", "key_contributions": ["Creation of a benchmark dataset for the NPN construction", "Demonstration that BERT embeddings capture semantic and form-related knowledge", "Insights into the polysemy of constructions as represented by transformer models"], "limitations": "", "keywords": ["BERT", "NPN construction", "semantic representation", "language models", "natural language processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.19265", "pdf": "https://arxiv.org/pdf/2503.19265.pdf", "abs": "https://arxiv.org/abs/2503.19265", "title": "PHEONA: An Evaluation Framework for Large Language Model-based Approaches to Computational Phenotyping", "authors": ["Sarah Pungitore", "Shashank Yadav", "Vignesh Subbian"], "categories": ["cs.CL"], "comment": "2 figures, 5 tables, accepted at 2025 AMIA Annual Symposium", "summary": "Computational phenotyping is essential for biomedical research but often\nrequires significant time and resources, especially since traditional methods\ntypically involve extensive manual data review. While machine learning and\nnatural language processing advancements have helped, further improvements are\nneeded. Few studies have explored using Large Language Models (LLMs) for these\ntasks despite known advantages of LLMs for text-based tasks. To facilitate\nfurther research in this area, we developed an evaluation framework, Evaluation\nof PHEnotyping for Observational Health Data (PHEONA), that outlines\ncontext-specific considerations. We applied and demonstrated PHEONA on concept\nclassification, a specific task within a broader phenotyping process for Acute\nRespiratory Failure (ARF) respiratory support therapies. From the sample\nconcepts tested, we achieved high classification accuracy, suggesting the\npotential for LLM-based methods to improve computational phenotyping processes.", "AI": {"tldr": "This paper presents an evaluation framework called PHEONA for using LLMs in computational phenotyping, specifically for classifying health data related to Acute Respiratory Failure.", "motivation": "The need for improved computational phenotyping in biomedical research due to the resource-intensive nature of traditional methods.", "method": "Development of the PHEONA framework and its application to concept classification for Acute Respiratory Failure.", "result": "The application of PHEONA resulted in high classification accuracy for the tested concepts.", "conclusion": "LLM-based methods have the potential to enhance computational phenotyping processes in health informatics.", "key_contributions": ["Introduction of the PHEONA evaluation framework for computational phenotyping.", "Demonstration of the framework's application in classifying health data for ARF therapies.", "Proof of high classification accuracy using LLMs for health data tasks."], "limitations": "", "keywords": ["computational phenotyping", "Large Language Models", "health informatics"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2503.23088", "pdf": "https://arxiv.org/pdf/2503.23088.pdf", "abs": "https://arxiv.org/abs/2503.23088", "title": "UNITYAI-GUARD: Pioneering Toxicity Detection Across Low-Resource Indian Languages", "authors": ["Himanshu Beniwal", "Reddybathuni Venkat", "Rohit Kumar", "Birudugadda Srivibhav", "Daksh Jain", "Pavan Doddi", "Eshwar Dhande", "Adithya Ananth", "Kuldeep", "Mayank Singh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work introduces UnityAI-Guard, a framework for binary toxicity\nclassification targeting low-resource Indian languages. While existing systems\npredominantly cater to high-resource languages, UnityAI-Guard addresses this\ncritical gap by developing state-of-the-art models for identifying toxic\ncontent across diverse Brahmic/Indic scripts. Our approach achieves an\nimpressive average F1-score of 84.23% across seven languages, leveraging a\ndataset of 567k training instances and 30k manually verified test instances. By\nadvancing multilingual content moderation for linguistically diverse regions,\nUnityAI-Guard also provides public API access to foster broader adoption and\napplication.", "AI": {"tldr": "UnityAI-Guard is a framework for binary toxicity classification in low-resource Indian languages, achieving an average F1-score of 84.23%.", "motivation": "The paper addresses the gap in toxicity classification systems that typically focus on high-resource languages, providing necessary tools for diverse linguistic communities.", "method": "The framework develops models specifically tailored for identifying toxic content in low-resource languages using a dataset of 567k training instances and 30k test instances.", "result": "UnityAI-Guard achieves an average F1-score of 84.23% across seven languages, indicating strong performance in binary toxicity classification.", "conclusion": "The framework not only enhances toxicity classification for low-resource languages but also encourages adoption through public API access.", "key_contributions": ["Development of a toxicity classification framework for low-resource Indian languages", "High performance with an average F1-score of 84.23%", "Public API access for broader adoption"], "limitations": "", "keywords": ["toxicity classification", "low-resource languages", "multilingual moderation"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2503.23091", "pdf": "https://arxiv.org/pdf/2503.23091.pdf", "abs": "https://arxiv.org/abs/2503.23091", "title": "Parsing Through Boundaries in Chinese Word Segmentation", "authors": ["Yige Chen", "Zelong Li", "Cindy Zhang", "Changbing Yang", "Amandisa Cady", "Ai Ka Lee", "Zejiao Zeng", "Eunkyul Leah Jo", "Haihua Pan", "Jungyeul Park"], "categories": ["cs.CL"], "comment": "Submitted to EMNLP2025 System Demonstration", "summary": "Chinese word segmentation is a foundational task in natural language\nprocessing (NLP), with far-reaching effects on syntactic analysis. Unlike\nalphabetic languages like English, Chinese lacks explicit word boundaries,\nmaking segmentation both necessary and inherently ambiguous. This study\nhighlights the intricate relationship between word segmentation and syntactic\nparsing, providing a clearer understanding of how different segmentation\nstrategies shape dependency structures in Chinese. Focusing on the Chinese GSD\ntreebank, we analyze multiple word boundary schemes, each reflecting distinct\nlinguistic and computational assumptions, and examine how they influence the\nresulting syntactic structures. To support detailed comparison, we introduce an\ninteractive web-based visualization tool that displays parsing outcomes across\nsegmentation methods.", "AI": {"tldr": "This paper explores Chinese word segmentation and its impact on syntactic parsing, introducing a web-based visualization tool for comparing parsing outcomes across different segmentation strategies.", "motivation": "To highlight the importance of word segmentation in Chinese NLP and its effects on syntactic analysis.", "method": "The study analyzes various word boundary schemes using the Chinese GSD treebank and examines their influence on dependency structures.", "result": "Different segmentation strategies lead to distinct syntactic structures, revealing the intricate relationship between segmentation and parsing.", "conclusion": "Segmentation methods significantly affect syntactic parsing in Chinese, and visualization can aid in comparing different approaches.", "key_contributions": ["Introduction of an interactive visualization tool for parsing outcomes", "Comprehensive analysis of segmentation strategies' impact on syntax", "Enhanced understanding of the relationship between word segmentation and syntactic parsing"], "limitations": "", "keywords": ["Chinese word segmentation", "Natural Language Processing", "Syntactic parsing"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2504.01931", "pdf": "https://arxiv.org/pdf/2504.01931.pdf", "abs": "https://arxiv.org/abs/2504.01931", "title": "On the Role of Feedback in Test-Time Scaling of Agentic AI Workflows", "authors": ["Souradip Chakraborty", "Mohammadreza Pourreza", "Ruoxi Sun", "Yiwen Song", "Nino Scherrer", "Furong Huang", "Amrit Singh Bedi", "Ahmad Beirami", "Jindong Gu", "Hamid Palangi", "Tomas Pfister"], "categories": ["cs.CL"], "comment": null, "summary": "Agentic AI workflows (systems that autonomously plan and act) are becoming\nwidespread, yet their task success rate on complex tasks remains low. A\npromising solution is inference-time alignment, which uses extra compute at\ntest time to improve performance. Inference-time alignment relies on three\ncomponents: sampling, evaluation, and feedback. While most prior work studies\nsampling and automatic evaluation, feedback remains underexplored. To study the\nrole of feedback, we introduce Iterative Agent Decoding (IAD), a procedure that\nrepeatedly inserts feedback extracted from different forms of critiques (reward\nmodels or AI-generated textual feedback) between decoding steps. Through IAD,\nwe analyze feedback along four dimensions: (1) its role in the accuracy-compute\ntrade-offs with limited inference budget, (2) quantifying the gains over\ndiversity-only baselines such as best-of-N sampling, (3) effectiveness of\ncomposing feedback from reward models versus textual critique, and (4)\nrobustness to noisy or low-quality feedback. Across Sketch2Code, Text2SQL,\nIntercode, and WebShop, we show that IAD with proper integration of high\nfidelity feedback leads to consistent gains up to 10 percent absolute\nperformance improvement over various baselines such as best-of-N. Our findings\nunderscore feedback as a crucial knob for inference-time alignment of agentic\nAI workflows with limited inference budget.", "AI": {"tldr": "This paper introduces Iterative Agent Decoding (IAD), a method to improve the performance of agentic AI workflows by utilizing feedback extracted from critiques to enhance inference-time alignment.", "motivation": "Agentic AI workflows are increasingly common but struggle with low success rates on complex tasks. Improving their performance through inference-time alignment, particularly by exploring the role of feedback, is critical.", "method": "The paper presents Iterative Agent Decoding (IAD), which integrates feedback from critiques during the decoding process. It explores the impact of this feedback on task accuracy and effectiveness in enhancing AI workflows.", "result": "IAD demonstrates an absolute performance improvement of up to 10% on tasks like Sketch2Code and Text2SQL, compared to traditional methods such as best-of-N sampling.", "conclusion": "Incorporating feedback into the inference-time alignment process significantly boosts the performance of agentic AI systems, highlighting its importance when working with limited inference budgets.", "key_contributions": ["Introduction of Iterative Agent Decoding (IAD) as a new approach for inference-time alignment.", "Analysis of feedback's impact on performance with diverse AI tasks.", "Comparison of feedback sources, revealing the advantages of high fidelity feedback."], "limitations": "", "keywords": ["Agentic AI", "Inference-Time Alignment", "Feedback", "Iterative Agent Decoding", "Machine Learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.03197", "pdf": "https://arxiv.org/pdf/2504.03197.pdf", "abs": "https://arxiv.org/abs/2504.03197", "title": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation", "authors": ["Jaewoo Park", "Jungyang Park", "Dongju Jang", "Jiwan Chung", "Byungwoo Yoo", "Jaewoo Shin", "Seonjoon Park", "Taehyeong Kim", "Youngjae Yu"], "categories": ["cs.CL"], "comment": "13 pages, 7 figures", "summary": "With the rapid advancement of mathematical reasoning capabilities in Large\nLanguage Models (LLMs), AI systems are increasingly being adopted in\neducational settings to support students' comprehension of problem-solving\nprocesses. However, a critical component remains underexplored in current\nLLM-generated explanations: multimodal explanation. In real-world instructional\ncontexts, human tutors routinely employ visual aids, such as diagrams,\nmarkings, and highlights, to enhance conceptual clarity. To bridge this gap, we\nintroduce the multimodal solution explanation task, designed to evaluate\nwhether models can identify visual keypoints, such as auxiliary lines, points,\nangles, and generate explanations that incorporate these key elements essential\nfor understanding. To evaluate model performance on this task, we propose ME2,\na multimodal benchmark consisting of 1,000 math problems annotated with visual\nkeypoints and corresponding explanatory text that references those elements.\nOur empirical results show that, aside from recent large-scale open-source and\nclosed-source models, most generalist open-source models, and even\nmath-specialist models, struggle with the multimodal solution explanation task.\nThis highlights a significant gap in current LLMs' ability to reason and\nexplain with visual grounding in educational contexts. We expect that the\nmultimodal solution explanation task and the ME2 dataset will catalyze further\nresearch on LLMs in education and promote their use as effective,\nexplanation-oriented AI tutors.", "AI": {"tldr": "This paper introduces a multimodal solution explanation task to evaluate and improve Large Language Models' (LLMs) ability to generate explanations incorporating visual aids for educational math problems.", "motivation": "To address the gap in multimodal explanation in LLM-generated content for educational purposes, as current models lack effective visual grounding in explanations, which is crucial for student understanding.", "method": "The paper introduces the multimodal solution explanation task and proposes ME2, a benchmark dataset containing 1,000 math problems with visual keypoints and their explanatory texts.", "result": "Empirical results indicate that many generalist open-source and even math-specialist LLMs struggle with the multimodal solution explanation task, revealing a significant gap in their capabilities.", "conclusion": "The introduction of the multimodal solution explanation task and the ME2 dataset aims to stimulate further research on the use of LLMs in education and their effectiveness as AI tutors.", "key_contributions": ["Introduction of the multimodal solution explanation task", "Creation of the ME2 benchmark dataset with annotated math problems", "Highlighting the shortcomings of existing LLMs in multimodal explanations"], "limitations": "Focus is limited to math problems; may not generalize to other domains or types of visual aids.", "keywords": ["Large Language Models", "multimodal explanation", "education", "AI tutors", "benchmark dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.03206", "pdf": "https://arxiv.org/pdf/2504.03206.pdf", "abs": "https://arxiv.org/abs/2504.03206", "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward", "authors": ["Yanming Wan", "Jiaxing Wu", "Marwa Abdulhai", "Lior Shani", "Natasha Jaques"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Effective conversational agents like large language models (LLMs) must\npersonalize their interactions to adapt to user preferences, personalities, and\nattributes across diverse domains like education and healthcare. Current\nmethods like Reinforcement Learning from Human Feedback (RLHF), often\nprioritize helpfulness and safety but fall short in fostering truly empathetic,\nadaptive, and personalized dialogues. Existing personalization approaches\ntypically rely on extensive user history, limiting their effectiveness for new\nor context-limited users. To address these limitations, we propose leveraging a\nuser model to incorporate a curiosity-based intrinsic reward into multi-turn\nRLHF. This novel reward mechanism encourages the LLM agent to actively infer\nuser traits by optimizing conversations to improve its user model's accuracy.\nConsequently, the agent delivers more personalized interactions by learning\nmore about the user. We demonstrate our method's effectiveness in two distinct\ndomains: significantly improving personalization performance in a\nconversational recommendation task, and personalizing conversations for\ndifferent learning styles in an educational setting. We show improved\ngeneralization capabilities compared to traditional multi-turn RLHF, all while\nmaintaining conversation quality. Our method offers a promising solution for\ncreating more personalized, adaptive, and engaging conversational agents.", "AI": {"tldr": "This paper proposes a curiosity-based reward mechanism for personalizing interactions in conversational agents, enhancing empathy and adaptability over traditional methods.", "motivation": "Current approaches for personalizing conversational agents often rely on extensive user history, which limits their use for new users or specific contexts. There is a need for methods that can foster empathy and adaptiveness in dialogues without extensive prior data.", "method": "The authors propose a user model that incorporates a curiosity-based intrinsic reward into multi-turn Reinforcement Learning from Human Feedback (RLHF), allowing the agent to infer user traits and optimize conversations.", "result": "The proposed method significantly improves personalization performance in conversational tasks and educational settings by better adapting to user traits while maintaining conversation quality and demonstrating enhanced generalization capabilities.", "conclusion": "This work suggests that incorporating curiosity-based rewards can lead to more personalized, engaging, and effective conversational agents in diverse contexts.", "key_contributions": ["Introduction of a curiosity-based intrinsic reward for conversational agents.", "Demonstration of improved personalization in conversational recommendation tasks.", "Enhanced adaptability for different learning styles in educational settings."], "limitations": "The effectiveness of the approach may vary with different user demographics or contexts that weren't tested in the study.", "keywords": ["Conversational Agents", "Personalization", "Reinforcement Learning", "User Model", "Large Language Models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.05995", "pdf": "https://arxiv.org/pdf/2504.05995.pdf", "abs": "https://arxiv.org/abs/2504.05995", "title": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday Knowledge", "authors": ["Firoj Alam", "Md Arid Hasan", "Sahinur Rahman Laskar", "Mucahid Kutlu", "Kareem Darwish", "Shammur Absar Chowdhury"], "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose the\nNativQA framework, which can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages -- ranging from extremely\nlow-resource to high-resource languages -- resulting in over 300K\nQuestion-Answer (QA) pairs. The developed resources can be used for LLM\nbenchmarking and further fine-tuning. The framework has been made publicly\navailable for the community (https://gitlab.com/nativqa/nativqa-framework).", "AI": {"tldr": "The NativQA framework creates large-scale QA datasets in native languages to enhance LLM capabilities and fairness in diverse contexts.", "motivation": "Address cultural bias and fairness concerns in LLMs by developing multilingual resources that cater to underrepresented regions.", "method": "The NativQA framework constructs QA datasets using user-defined queries and search engines to gather local information, evaluated across 39 locations in 24 countries in 7 languages.", "result": "The framework produced over 300K QA pairs from various linguistic contexts, enabling benchmarking and fine-tuning of LLMs.", "conclusion": "NativQA enhances LLM testing and deployment in culturally diverse settings and is publicly available for community use.", "key_contributions": ["Development of a framework for culturally aligned QA datasets", "Evaluation across diverse linguistic contexts", "Public availability of resources for community use"], "limitations": "", "keywords": ["LLMs", "Multilingual", "Cultural Bias", "QA Datasets", "Language Diversity"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2504.12913", "pdf": "https://arxiv.org/pdf/2504.12913.pdf", "abs": "https://arxiv.org/abs/2504.12913", "title": "MAIN: Mutual Alignment Is Necessary for instruction tuning", "authors": ["Fanyi Yang", "Jianfeng Liu", "Xin Zhang", "Haoyu Liu", "Xixin Cao", "Yuefeng Zhan", "Hao Sun", "Weiwei Deng", "Feng Sun", "Qi Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Instruction tuning has empowered large language models (LLMs) to achieve\nremarkable performance, yet its success heavily depends on the availability of\nlarge-scale, high-quality instruction-response pairs. To meet this demand,\nvarious methods have been developed to synthesize data at scale. However,\ncurrent methods for scaling up data generation often overlook a crucial aspect:\nthe alignment between instructions and responses. We hypothesize that the\nquality of instruction-response pairs is determined not by the individual\nquality of each component, but by the degree of mutual alignment. To address\nthis, we propose a Mutual Alignment Framework (MAIN) which enforces coherence\nbetween instructions and responses through mutual constraints. We demonstrate\nthat MAIN generalizes well across model architectures and sizes, achieving\nstate-of-the-art performance on LLaMA, Mistral, and Qwen models across diverse\nbenchmarks. This work underscores the critical role of instruction-response\nalignment in enabling generalizable and high-quality instruction tuning for\nLLMs.", "AI": {"tldr": "This paper introduces the Mutual Alignment Framework (MAIN) to enhance the quality of instruction-response pairs in instruction tuning for large language models.", "motivation": "The paper addresses the need for high-quality instruction-response pairs essential for the effective instruction tuning of large language models.", "method": "The authors propose the Mutual Alignment Framework (MAIN) which enforces coherence between instructions and responses by applying mutual constraints.", "result": "MAIN demonstrates state-of-the-art performance on LLaMA, Mistral, and Qwen models across diverse benchmarks, highlighting improved instruction-response alignment.", "conclusion": "The study emphasizes the importance of mutual alignment in producing high-quality instruction-response pairs for instruction tuning in LLMs.", "key_contributions": ["Introduction of the Mutual Alignment Framework (MAIN)", "Demonstration of improved performance across multiple model architectures", "Highlighting the significance of instruction-response alignment"], "limitations": "", "keywords": ["instruction tuning", "large language models", "alignment framework"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.14492", "pdf": "https://arxiv.org/pdf/2504.14492.pdf", "abs": "https://arxiv.org/abs/2504.14492", "title": "FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering", "authors": ["Yichen Li", "Zhiting Fan", "Ruizhe Chen", "Xiaotang Gai", "Luqi Gong", "Yan Zhang", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Large language models (LLMs) are prone to capturing biases from training\ncorpus, leading to potential negative social impacts. Existing prompt-based\ndebiasing methods exhibit instability due to their sensitivity to prompt\nchanges, while fine-tuning-based techniques incur substantial computational\noverhead and catastrophic forgetting. In this paper, we propose FairSteer, a\nnovel inference-time debiasing framework without requiring customized prompt\ndesign or model retraining. Motivated by the linear representation hypothesis,\nour preliminary investigation demonstrates that fairness-related features can\nbe encoded into separable directions in the hidden activation space. FairSteer\noperates in three steps: biased activation detection, debiasing steering vector\n(DSV) computation, and dynamic activation steering. Specifically, it first\ntrains a lightweight linear classifier to detect bias signatures in\nactivations, and then computes DSVs as intervention directions derived from\nsmall contrastive prompt pairs. Subsequently, it performs debiasing by\nadjusting activations with DSVs in the inference stage. Comprehensive\nevaluation with six LLMs demonstrates the superiority of FairSteer across\nquestion-answering, counterfactual input evaluation and open-ended text\ngeneration tasks. Code will be released.", "AI": {"tldr": "FairSteer is a new debiasing framework for large language models that operates at inference time without requiring prompt design or model retraining.", "motivation": "LLMs capture biases from training data, leading to negative social impacts, and existing debiasing methods are either unstable or computationally expensive.", "method": "FairSteer uses a linear classifier to detect bias in activations and computes debiasing steering vectors (DSVs) from contrastive prompt pairs for dynamic adjustment of activations during inference.", "result": "FairSteer shows superiority over existing methods in various tasks involving six LLMs, including question-answering and text generation, indicating effective bias mitigation without extensive resource use.", "conclusion": "FairSteer effectively debiases large language models in real-time during inference, presenting a more efficient and robust alternative to existing techniques.", "key_contributions": ["Introduction of FairSteer for inference-time debiasing", "Utilization of linear representation hypothesis for bias detection", "Demonstration of multiple successful applications across various LLMs"], "limitations": "", "keywords": ["debiasing", "large language models", "inference-time", "activations", "fairness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.15133", "pdf": "https://arxiv.org/pdf/2504.15133.pdf", "abs": "https://arxiv.org/abs/2504.15133", "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models", "authors": ["Ziwen Xu", "Shuxun Wang", "Kewei Xu", "Haoming Xu", "Mengru Wang", "Xinle Deng", "Yunzhi Yao", "Guozhou Zheng", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Work in progress. Demo: https://www.youtube.com/watch?v=AkfoiPfp5rQ;\n  code: https://github.com/zjunlp/EasyEdit", "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://www.youtube.com/watch?v=AkfoiPfp5rQ for a quick introduction.", "AI": {"tldr": "EasyEdit2 is a plug-and-play framework for adjusting Large Language Model behaviors, allowing for easy control over various aspects without needing extensive technical knowledge.", "motivation": "To provide accessible and efficient methods for controlling LLM behaviors during test-time interventions.", "method": "EasyEdit2 employs a new architecture with key modules like the steering vector generator and applier to facilitate automatic generation and application of steering vectors for model behavior adjustment.", "result": "The framework demonstrates effective model steering performance across different LLMs, showcasing the ease of controlling outputs with minimal technical knowledge.", "conclusion": "EasyEdit2 allows users to guide model responses effectively, making precise control of LLMs both accessible and efficient, with source code and demos available online.", "key_contributions": ["Introduction of a new framework for LLM behavior control", "Easy-to-use plug-and-play architecture", "Empirical performance across different LLMs"], "limitations": "", "keywords": ["LLM", "model steering", "user-friendly", "AI behavior control", "EasyEdit2"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23570", "pdf": "https://arxiv.org/pdf/2505.23570.pdf", "abs": "https://arxiv.org/abs/2505.23570", "title": "Evaluating AI capabilities in detecting conspiracy theories on YouTube", "authors": ["Leonardo La Rocca", "Francesco Corso", "Francesco Pierri"], "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": "Submitted for review to OSNEM Special Issue of April 2025", "summary": "As a leading online platform with a vast global audience, YouTube's extensive\nreach also makes it susceptible to hosting harmful content, including\ndisinformation and conspiracy theories. This study explores the use of\nopen-weight Large Language Models (LLMs), both text-only and multimodal, for\nidentifying conspiracy theory videos shared on YouTube. Leveraging a labeled\ndataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot\nsetting and compare their performance to a fine-tuned RoBERTa baseline. Results\nshow that text-based LLMs achieve high recall but lower precision, leading to\nincreased false positives. Multimodal models lag behind their text-only\ncounterparts, indicating limited benefits from visual data integration. To\nassess real-world applicability, we evaluate the most accurate models on an\nunlabeled dataset, finding that RoBERTa achieves performance close to LLMs with\na larger number of parameters. Our work highlights the strengths and\nlimitations of current LLM-based approaches for online harmful content\ndetection, emphasizing the need for more precise and robust systems.", "AI": {"tldr": "This study examines the effectiveness of open-weight Large Language Models (LLMs) in identifying conspiracy theory videos on YouTube, highlighting challenges in precision and the comparison with RoBERTa.", "motivation": "To address the prevalence of harmful content such as disinformation on YouTube, this research explores using LLMs for video classification.", "method": "The study evaluates various text-only and multimodal LLMs in a zero-shot setting using a labeled dataset of videos, and compares results to a fine-tuned RoBERTa baseline.", "result": "Text-based LLMs demonstrated high recall but lower precision, resulting in more false positives, while multimodal models performed worse than text-only models.", "conclusion": "The findings reveal the strengths and limitations of LLMs in detecting harmful content on YouTube, suggesting a need for improved systems for better precision.", "key_contributions": ["Evaluation of LLMs in identifying conspiracy theory videos", "Comparison with a fine-tuned RoBERTa baseline", "Insights into the performance of multimodal models versus text-only models."], "limitations": "Limited benefits from integrating visual data in multimodal models; need for more precise systems.", "keywords": ["YouTube", "Large Language Models", "disinformation", "conspiracy theories", "harmful content detection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.20666", "pdf": "https://arxiv.org/pdf/2506.20666.pdf", "abs": "https://arxiv.org/abs/2506.20666", "title": "Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs", "authors": ["Sonia K. Murthy", "Rosie Zhao", "Jennifer Hu", "Sham Kakade", "Markus Wulfmeier", "Peng Qian", "Tomer Ullman"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 3 figures", "summary": "Navigating everyday social situations often requires juggling conflicting\ngoals, such as conveying a harsh truth, maintaining trust, all while still\nbeing mindful of another person's feelings. These value trade-offs are an\nintegral part of human decision-making and language use, however, current tools\nfor interpreting such dynamic and multi-faceted notions of values in LLMs are\nlimited. In cognitive science, so-called \"cognitive models\" provide formal\naccounts of these trade-offs in humans, by modeling the weighting of a\nspeaker's competing utility functions in choosing an action or utterance. In\nthis work, we use a leading cognitive model of polite speech to interpret the\nextent to which LLMs represent human-like trade-offs. We apply this lens to\nsystematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models, and in\nopen-source models shown to be stronger in mathematical reasoning. Our findings\nfrom LLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. We show that our method\nis responsive to diverse aspects of the rapidly evolving LLM landscape, with\ninsights for forming hypotheses about other high-level behaviors, shaping\ntraining regimes for reasoning models, and better controlling trade-offs\nbetween values during model training.", "AI": {"tldr": "This paper evaluates how large language models (LLMs) embody human-like value trade-offs using cognitive models, analyzing reasoning effort and reinforcement learning post-training dynamics.", "motivation": "Understanding the representation of human-like value trade-offs in LLMs is crucial for improving their interpretability and effectiveness in simulating complex social interactions.", "method": "The study employs a cognitive model of polite speech to assess LLMs, focusing on reasoning effort in black-box models and reinforcement learning adjustments in open-source models.", "result": "Results reveal that reasoning models prioritize informational utility over social utility, while open-source models excel in mathematical reasoning. Additionally, significant shifts in utility values during early training phases are noted.", "conclusion": "The insights gained can inform hypotheses about LLM behaviors and guide the development of training paradigms that balance different value trade-offs during model training.", "key_contributions": ["Provides a framework for interpreting value trade-offs in LLMs.", "Highlights differences in utility priorities between model types.", "Identifies the impact of pretraining choices on utility dynamics."], "limitations": "The scope is limited to specific aspects of LLM training and may not capture all nuances of human-like decision-making.", "keywords": ["value trade-offs", "large language models", "cognitive models", "polite speech", "model training"], "importance_score": 8, "read_time_minutes": 15}}
