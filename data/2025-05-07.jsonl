{"id": "2505.03027", "pdf": "https://arxiv.org/pdf/2505.03027.pdf", "abs": "https://arxiv.org/abs/2505.03027", "title": "Revisiting Performance Models of Distal Pointing Tasks in Virtual Reality", "authors": ["Logan Lane", "Feiyu Lu", "Shakiba Davari", "Rob Teather", "Doug A. Bowman"], "categories": ["cs.HC"], "comment": null, "summary": "Performance models of interaction, such as Fitts Law, are important tools for\npredicting and explaining human motor performance and for designing\nhigh-performance user interfaces. Extensive prior work has proposed such models\nfor the 3D interaction task of distal pointing, in which the user points their\nhand or a device at a distant target in order to select it. However, there is\nno consensus on how to compute the index of difficulty for distal pointing\ntasks. We present a preliminary study suggesting that existing models may not\nbe sufficient to model distal pointing performance with current virtual reality\ntechnologies. Based on these results, we hypothesized that both the form of the\nmodel and the standard method for collecting empirical data for pointing tasks\nmight need to change in order to achieve a more accurate and valid distal\npointing model. In our main study, we used a new methodology to collect distal\npointing data and evaluated traditional models, purely ballistic models, and\ntwo-part models. Ultimately, we found that the best model used a simple\nFitts-Law-style index of difficulty with angular measures of amplitude and\nwidth."}
{"id": "2505.03073", "pdf": "https://arxiv.org/pdf/2505.03073.pdf", "abs": "https://arxiv.org/abs/2505.03073", "title": "Coupling the Heart to Musical Machines", "authors": ["Eric Easthope"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": null, "summary": "Biofeedback is being used more recently as a general control paradigm for\nhuman-computer interfaces (HCIs). While biofeedback especially from breath has\nseen increasing uptake as a controller for novel musical interfaces, new\ninterfaces for musical expression (NIMEs), the community has not given as much\nattention to the heart. The heart is just as intimate a part of music as breath\nand it is argued that the heart determines our perception of time and so\nindirectly our perception of music. Inspired by this I demonstrate a\nphotoplethysmogram (PPG)-based NIME controller using heart rate as a 1D control\nparameter to transform the qualities of sounds in real-time over a Bluetooth\nwireless HCI. I apply time scaling to \"warp\" audio buffers inbound to the sound\ncard, and play these transformed audio buffers back to the listener wearing the\nPPG sensor, creating a hypothetical perceptual biofeedback loop: changes in\nsound change heart rate to change PPG measurements to change sound. I discuss\nhow a sound-heart-PPG biofeedback loop possibly affords greater control and/or\nvariety of movements with a 1D controller, how controlling the space and/or\ntime scale of sound playback with biofeedback makes for possibilities in\nperformance ambience, and I briefly discuss generative latent spaces as a\npossible way to extend a 1D PPG control space."}
{"id": "2505.03105", "pdf": "https://arxiv.org/pdf/2505.03105.pdf", "abs": "https://arxiv.org/abs/2505.03105", "title": "Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation", "authors": ["Xule Lin"], "categories": ["cs.HC", "cs.AI", "cs.CY", "H.5.3; I.2.11; K.4.3; H.1.2; I.2.4"], "comment": "62 pages (31 appendix pages for guidance), 2 figures", "summary": "Scientific knowledge creation is fundamentally transforming as humans and AI\nsystems evolve beyond tool-user relationships into co-evolutionary epistemic\npartnerships. When AlphaFold revolutionized protein structure prediction,\nresearchers described engaging with an epistemic partner that reshaped how they\nconceptualized fundamental relationships. This article introduces Cognitio\nEmergens (CE), a framework addressing critical limitations in existing models\nthat focus on static roles or narrow metrics while failing to capture how\nscientific understanding emerges through recursive human-AI interaction over\ntime. CE integrates three components addressing these limitations: Agency\nConfigurations describing how authority distributes between humans and AI\n(Directed, Contributory, Partnership), with partnerships dynamically\noscillating between configurations rather than following linear progression;\nEpistemic Dimensions capturing six specific capabilities emerging through\ncollaboration across Discovery, Integration, and Projection axes, creating\ndistinctive \"capability signatures\" that guide development; and Partnership\nDynamics identifying forces shaping how these relationships evolve,\nparticularly the risk of epistemic alienation where researchers lose\ninterpretive control over knowledge they formally endorse. Drawing from\nautopoiesis theory, social systems theory, and organizational modularity, CE\nreveals how knowledge co-creation emerges through continuous negotiation of\nroles, values, and organizational structures. By reconceptualizing human-AI\nscientific collaboration as fundamentally co-evolutionary, CE offers a balanced\nperspective that neither uncritically celebrates nor unnecessarily fears AI's\nevolving role, instead providing conceptual tools for cultivating partnerships\nthat maintain meaningful human participation while enabling transformative\nscientific breakthroughs."}
{"id": "2505.03117", "pdf": "https://arxiv.org/pdf/2505.03117.pdf", "abs": "https://arxiv.org/abs/2505.03117", "title": "Do ATCOs Need Explanations, and Why? Towards ATCO-Centered Explainable AI for Conflict Resolution Advisories", "authors": ["Katherine Fennedy", "Brian Hilburn", "Thaivalappil N. M. Nadirsha", "Sameer Alam", "Khanh-Duy Le", "Hua Li"], "categories": ["cs.HC"], "comment": "2025 ATRD US-Europe Air Transportation Research & Development\n  Symposium", "summary": "Interest in explainable artificial intelligence (XAI) is surging. Prior\nresearch has primarily focused on systems' ability to generate explanations,\noften guided by researchers' intuitions rather than end-users' needs.\nUnfortunately, such approaches have not yielded favorable outcomes when\ncompared to a black-box baseline (i.e., no explanation). To address this gap,\nthis paper advocates a human-centered approach that shifts focus to air traffic\ncontrollers (ATCOs) by asking a fundamental yet overlooked question: Do ATCOs\nneed explanations, and if so, why? Insights from air traffic management (ATM),\nhuman-computer interaction, and the social sciences were synthesized to provide\na holistic understanding of XAI challenges and opportunities in ATM. Evaluating\n11 ATM operational goals revealed a clear need for explanations when ATCOs aim\nto document decisions and rationales for future reference or report generation.\nConversely, ATCOs are less likely to seek them when their conflict resolution\napproach align with the artificial intelligence (AI) advisory. While this is a\npreliminary study, the findings are expected to inspire broader and deeper\ninquiries into the design of ATCO-centric XAI systems, paving the way for more\neffective human-AI interaction in ATM."}
{"id": "2505.02847", "pdf": "https://arxiv.org/pdf/2505.02847.pdf", "abs": "https://arxiv.org/abs/2505.02847", "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models", "authors": ["Bang Zhang", "Ruotian Ma", "Qingxuan Jiang", "Peisong Wang", "Jiaqi Chen", "Zheng Xie", "Xingyu Chen", "Yue Wang", "Fanghua Ye", "Jian Li", "Yifan Yang", "Zhaopeng Tu", "Xiaolong Li"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents."}
{"id": "2505.03164", "pdf": "https://arxiv.org/pdf/2505.03164.pdf", "abs": "https://arxiv.org/abs/2505.03164", "title": "InfoVids: Reimagining the Viewer Experience with Alternative Visualization-Presenter Relationships", "authors": ["Ji Won Chung", "Tongyu Zhou", "Ivy Chen", "Kevin Hsu", "Ryan A. Rossi", "Alexa Siu", "Shunan Guo", "Franck Dernoncourt", "James Tompkin", "Jeff Huang"], "categories": ["cs.HC"], "comment": null, "summary": "Traditional data presentations typically separate the presenter and\nvisualization into two separate spaces--the 3D world and a 2D screen--enforcing\nvisualization-centric stories. To create a more human-centric viewing\nexperience, we establish a more equitable relationship between the\nvisualization and the presenter through our InfoVids. These\ninfographics-inspired informational videos are crafted to redefine\nrelationships between the presenter and visualizations. As we design InfoVids,\nwe explore how the use of layout, form, and interactions affects the viewer\nexperience. We compare InfoVids against their baseline 2D `slides' equivalents\nacross 9 metrics with 30 participants and provide practical, long-term insights\nfrom an autobiographical perspective. Our mixed methods analyses reveal that\nthis paradigm reduced viewer attention splitting, shifted the focus from the\nvisualization to the presenter, and led to more interactive, natural, and\nengaging full-body data performances for viewers. Ultimately, InfoVids helped\nviewers re-imagine traditional dynamics between the presenter and\nvisualizations."}
{"id": "2505.02850", "pdf": "https://arxiv.org/pdf/2505.02850.pdf", "abs": "https://arxiv.org/abs/2505.02850", "title": "Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Diksha Seth", "Ananya Thakur", "Deepak Subramani"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DB"], "comment": null, "summary": "Generating high-quality MCQs, especially those targeting diverse cognitive\nlevels and incorporating common misconceptions into distractor design, is\ntime-consuming and expertise-intensive, making manual creation impractical at\nscale. Current automated approaches typically generate questions at lower\ncognitive levels and fail to incorporate domain-specific misconceptions. This\npaper presents a hierarchical concept map-based framework that provides\nstructured knowledge to guide LLMs in generating MCQs with distractors. We\nchose high-school physics as our test domain and began by developing a\nhierarchical concept map covering major Physics topics and their\ninterconnections with an efficient database design. Next, through an automated\npipeline, topic-relevant sections of these concept maps are retrieved to serve\nas a structured context for the LLM to generate questions and distractors that\nspecifically target common misconceptions. Lastly, an automated validation is\ncompleted to ensure that the generated MCQs meet the requirements provided. We\nevaluate our framework against two baseline approaches: a base LLM and a\nRAG-based generation. We conducted expert evaluations and student assessments\nof the generated MCQs. Expert evaluation shows that our method significantly\noutperforms the baseline approaches, achieving a success rate of 75.20% in\nmeeting all quality criteria compared to approximately 37% for both baseline\nmethods. Student assessment data reveal that our concept map-driven approach\nachieved a significantly lower guess success rate of 28.05% compared to 37.10%\nfor the baselines, indicating a more effective assessment of conceptual\nunderstanding. The results demonstrate that our concept map-based approach\nenables robust assessment across cognitive levels and instant identification of\nconceptual gaps, facilitating faster feedback loops and targeted interventions\nat scale."}
{"id": "2505.03185", "pdf": "https://arxiv.org/pdf/2505.03185.pdf", "abs": "https://arxiv.org/abs/2505.03185", "title": "Behavioral Sensing and Intervention Paradigm: A Review of Closed-Loop Approaches for Ingestion Health", "authors": ["Jun Fang", "Yanuo Zhou", "Ka I Chan", "Jiajin Li", "Zeyi Sun", "Zhengnan Li", "Zicong Fu", "Hongjing Piao", "Haodong Xu", "Yuanchun Shi", "Yuntao Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Ingestive behavior plays a critical role in health, yet many existing\ninterventions remain limited to static guidance or manual self-tracking. With\nthe increasing integration of sensors and perceptual computing, recent systems\nhave begun to support closed-loop interventions that dynamically sense user\nbehavior and provide feedback during or around ingestion episodes. In this\nsurvey, we review 136 studies that leverage sensor-enabled or\ninteraction-mediated approaches to influence eating behavior. We propose a\nbehavioral closed-loop paradigm comprising three core components: target\nbehaviors, sensing modalities, and feedback strategies. A taxonomy of sensing\nand intervention modalities is presented, organized along human- and\nenvironment-based dimensions. Our analysis also examines evaluation methods and\ndesign trends across different modality-behavior pairings. This review reveals\nprevailing patterns and critical gaps, offering design insights for future\nadaptive and context-aware ingestion health interventions."}
{"id": "2505.02851", "pdf": "https://arxiv.org/pdf/2505.02851.pdf", "abs": "https://arxiv.org/abs/2505.02851", "title": "30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation", "authors": ["Franklin Zhang", "Sonya Zhang", "Alon Halevy"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; H.3.1; H.3.3"], "comment": "8 pages (main content), 4 figures. Submitted to ACL BEA2025", "summary": "In this paper, we present 30 Day Me, a habit formation application that\nleverages Large Language Models (LLMs) to help users break down their goals\ninto manageable, actionable steps and track their progress. Central to the app\nis the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced\nfrom over 15K webpages, and enables runtime search of challenge ideas aligned\nwith user-defined goals. We showcase how LLMs can be harnessed to rapidly\nconstruct domain specific content corpora for behavioral and educational\npurposes, and propose a practical pipeline that incorporates effective LLM\nenhanced approaches for content generation and semantic deduplication."}
{"id": "2505.03364", "pdf": "https://arxiv.org/pdf/2505.03364.pdf", "abs": "https://arxiv.org/abs/2505.03364", "title": "DroidRetriever: An Autonomous Navigation and Information Integration System Facilitating Mobile Sensemaking", "authors": ["Yiheng Bian", "Yunpeng Song", "Guiyu Ma", "Rongrong Zhu", "Zhongmin Cai"], "categories": ["cs.HC"], "comment": null, "summary": "Users regularly rely on mobile applications for their daily information\nneeds, and mobile sensemaking is prevalent in various domains such as\neducation, healthcare, business intelligence, and emergency response, where\ntimely and context-aware information-processing and decision-making is\ncritical. However, valuable information is often scattered across the closed\necosystems within various applications, posing challenges for traditional\nsearch engines to retrieve data openly and in real-time. Additionally, due to\nlimitations such as mobile device screen sizes, language differences, and\nunfamiliarity with specific applications and domain knowledge, users have to\nfrequently switch between multiple applications and spend substantial time\nlocating and integrating the information. To address these challenges, we\npresent DroidRetriever, a system for cross-application information retrieval to\nfacilitate mobile sensemaking. DroidRetriever can automatically navigate to\nrelevant interfaces based on users' natural language commands, capture\nscreenshots, extract and integrate information, and finally present the\nresults. Our experimental results demonstrate that DroidRetriever can extract\nand integrate information with near-human accuracy while significantly reducing\nprocessing time. Furthermore, with minimal user intervention, DroidRetriever\neffectively corrects and completes various information retrieval tasks,\nsubstantially reducing the user's workload. Our summary of the motivations for\nintervention and the discussion of their necessity provide valuable\nimplications for future research. We will open-source our code upon acceptance\nof the paper."}
{"id": "2505.02854", "pdf": "https://arxiv.org/pdf/2505.02854.pdf", "abs": "https://arxiv.org/abs/2505.02854", "title": "Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets", "authors": ["Masumi Morishige", "Ryo Koshihara"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 10 figures", "summary": "Reproducibility and reliability remain pressing challenges for generative AI\nsystems whose behavior can drift with each model update or prompt revision. We\nintroduce GPR-bench, a lightweight, extensible benchmark that operationalizes\nregression testing for general purpose use cases. GPR-bench couples an open,\nbilingual (English and Japanese) dataset covering eight task categories (e.g.,\ntext generation, code generation, and information retrieval) and 10 scenarios\nin each task categories (80 total test cases for each language) with an\nautomated evaluation pipeline that employs \"LLM-as-a-Judge\" scoring of\ncorrectness and conciseness. Experiments across three recent model versions -\ngpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default\nversus concise-writing instruction) reveal heterogeneous quality. Our results\nshow that newer models generally improve correctness, but the differences are\nmodest and not statistically significant, suggesting that GPR-bench may not be\nsufficiently challenging to differentiate between recent model versions. In\ncontrast, the concise-writing instruction significantly enhances conciseness\n(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with\nminimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of\nprompt engineering. Released under the MIT License, GPR- bench lowers the\nbarrier to initiating reproducibility monitoring and provides a foundation for\ncommunity-driven extensions, while also raising important considerations about\nbenchmark design for rapidly evolving language models."}
{"id": "2505.03423", "pdf": "https://arxiv.org/pdf/2505.03423.pdf", "abs": "https://arxiv.org/abs/2505.03423", "title": "AI-Based Feedback in Counselling Competence Training of Prospective Teachers", "authors": ["Tobias Hallmen", "Kathrin Gietl", "Karoline Hillesheim", "Moritz Bauermann", "Annemarie Friedrich", "Elisabeth André"], "categories": ["cs.HC"], "comment": null, "summary": "This study explores the use of AI-based feedback to enhance the counselling\ncompetence of prospective teachers. An iterative block seminar was designed,\nincorporating theoretical foundations, practical applications, and AI tools for\nanalysing verbal, paraverbal, and nonverbal communication. The seminar included\nrecorded simulated teacher-parent conversations, followed by AI-based feedback\nand qualitative interviews with students. The study investigated correlations\nbetween communication characteristics and conversation quality, student\nperceptions of AI-based feedback, and the training of AI models to identify\nconversation phases and techniques. Results indicated significant correlations\nbetween nonverbal and paraverbal features and conversation quality, and\nstudents positively perceived the AI feedback. The findings suggest that\nAI-based feedback can provide objective, actionable insights to improve teacher\ntraining programs. Future work will focus on refining verbal skill annotations,\nexpanding the dataset, and exploring additional features to enhance the\nfeedback system."}
{"id": "2505.02858", "pdf": "https://arxiv.org/pdf/2505.02858.pdf", "abs": "https://arxiv.org/abs/2505.02858", "title": "Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models", "authors": ["Henry Tari", "Nojus Sereiva", "Rishabh Kaushal", "Thales Bertaglia", "Adriana Iamnitchi"], "categories": ["cs.CL", "cs.CY"], "comment": "arXiv admin note: text overlap with arXiv:2407.08323", "summary": "Social media datasets are essential for research on a variety of topics, such\nas disinformation, influence operations, hate speech detection, or influencer\nmarketing practices. However, access to social media datasets is often\nconstrained due to costs and platform restrictions. Acquiring datasets that\nspan multiple platforms, which is crucial for understanding the digital\necosystem, is particularly challenging. This paper explores the potential of\nlarge language models to create lexically and semantically relevant social\nmedia datasets across multiple platforms, aiming to match the quality of real\ndata. We propose multi-platform topic-based prompting and employ various\nlanguage models to generate synthetic data from two real datasets, each\nconsisting of posts from three different social media platforms. We assess the\nlexical and semantic properties of the synthetic data and compare them with\nthose of the real data. Our empirical findings show that using large language\nmodels to generate synthetic multi-platform social media data is promising,\ndifferent language models perform differently in terms of fidelity, and a\npost-processing approach might be needed for generating high-fidelity synthetic\ndatasets for research. In addition to the empirical evaluation of three state\nof the art large language models, our contributions include new fidelity\nmetrics specific to multi-platform social media datasets."}
{"id": "2505.03440", "pdf": "https://arxiv.org/pdf/2505.03440.pdf", "abs": "https://arxiv.org/abs/2505.03440", "title": "manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality", "authors": ["Samuel Pantze", "Jean-Yves Tinevez", "Matthew McGinity", "Ulrik Günther"], "categories": ["cs.HC"], "comment": "7 pages, 6 figures, submitted to IEEE VIS 2025", "summary": "We propose manvr3d, a novel VR-ready platform for interactive\nhuman-in-the-loop cell tracking. We utilize VR controllers and eye-tracking\nhardware to facilitate rapid ground truth generation and proofreading for deep\nlearning-based cell tracking models. Life scientists reconstruct the\ndevelopmental history of organisms on the cellular level by analyzing 3D\ntime-lapse microscopy images acquired at high spatio-temporal resolution. The\nreconstruction of such cell lineage trees traditionally involves tracking\nindividual cells through all recorded time points, manually annotating their\npositions, and then linking them over time to create complete trajectories.\nDeep learning-based algorithms accelerate this process, yet depend heavily on\nmanually-annotated high-quality ground truth data and curation. Visual\nrepresentation of the image data in this process still relies primarily on 2D\nrenderings, which greatly limits spatial understanding and navigation. In this\nwork, we bridge the gap between deep learning-based cell tracking software and\n3D/VR visualization to create a human-in-the-loop cell tracking system. We lift\nthe incremental annotation, training and proofreading loop of the deep learning\nmodel into the 3rd dimension and apply natural user interfaces like hand\ngestures and eye tracking to accelerate the cell tracking workflow for life\nscientists."}
{"id": "2505.02859", "pdf": "https://arxiv.org/pdf/2505.02859.pdf", "abs": "https://arxiv.org/abs/2505.02859", "title": "Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI", "authors": ["Jonas Bokstaller", "Julia Altheimer", "Julian Dormehl", "Alina Buss", "Jasper Wiltfang", "Johannes Schneider", "Maximilian Röglinger"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Across various sectors applications of eXplainableAI (XAI) gained momentum as\nthe increasing black-boxedness of prevailing Machine Learning (ML) models\nbecame apparent. In parallel, Large Language Models (LLMs) significantly\ndeveloped in their abilities to understand human language and complex patterns.\nBy combining both, this paper presents a novel reference architecture for the\ninterpretation of XAI through an interactive chatbot powered by a fine-tuned\nLLM. We instantiate the reference architecture in the context of\nState-of-Health (SoH) prediction for batteries and validate its design in\nmultiple evaluation and demonstration rounds. The evaluation indicates that the\nimplemented prototype enhances the human interpretability of ML, especially for\nusers with less experience with XAI."}
{"id": "2505.03492", "pdf": "https://arxiv.org/pdf/2505.03492.pdf", "abs": "https://arxiv.org/abs/2505.03492", "title": "Augmenting Human Cognition through Everyday AR", "authors": ["Xiaoan Liu"], "categories": ["cs.HC", "cs.AI"], "comment": "3 pages, 4 figures. Position paper accepted to CHI'25 Workshop\n  'Everyday AR through AI-in-the-Loop'", "summary": "As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding."}
{"id": "2505.02862", "pdf": "https://arxiv.org/pdf/2505.02862.pdf", "abs": "https://arxiv.org/abs/2505.02862", "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs", "authors": ["Haoming Yang", "Ke Ma", "Xiaojun Jia", "Yingfei Sun", "Qianqian Xu", "Qingming Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies."}
{"id": "2505.03584", "pdf": "https://arxiv.org/pdf/2505.03584.pdf", "abs": "https://arxiv.org/abs/2505.03584", "title": "BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation", "authors": ["Lucas Anastasiou", "Anna De Liddo"], "categories": ["cs.HC", "cs.AI", "cs.CY", "I.2"], "comment": "5 pages, 3 figures", "summary": "Public deliberation, as in open discussion of issues of public concern, often\nsuffers from scattered and shallow discourse, poor sensemaking, and a\ndisconnect from actionable policy outcomes. This paper introduces BCause, a\ndiscussion system leveraging generative AI and human-machine collaboration to\ntransform unstructured dialogue around public issues (such as urban living,\npolicy changes, and current socio-economic transformations) into structured,\nactionable democratic processes. We present three innovations: (i) importing\nand transforming unstructured transcripts into argumentative discussions, (ii)\ngeo-deliberated problem-sensing via a Telegram bot for local issue reporting,\nand (iii) smart reporting with customizable widgets (e.g., summaries, topic\nmodelling, policy recommendations, clustered arguments). The system's human-AI\npartnership preserves critical human participation to ensure ethical oversight,\ncontextual relevance, and creative synthesis."}
{"id": "2505.02865", "pdf": "https://arxiv.org/pdf/2505.02865.pdf", "abs": "https://arxiv.org/abs/2505.02865", "title": "Accelerating Large Language Model Reasoning via Speculative Search", "authors": ["Zhihai Wang", "Jie Wang", "Jilai Pan", "Xilin Xia", "Huiling Zhen", "Mingxuan Yuan", "Jianye Hao", "Feng Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML2025", "summary": "Tree-search-based reasoning methods have significantly enhanced the reasoning\ncapability of large language models (LLMs) by facilitating the exploration of\nmultiple intermediate reasoning steps, i.e., thoughts. However, these methods\nsuffer from substantial inference latency, as they have to generate numerous\nreasoning thoughts, severely limiting LLM applicability. To address this\nchallenge, we propose a novel Speculative Search (SpecSearch) framework that\nsignificantly accelerates LLM reasoning by optimizing thought generation.\nSpecifically, SpecSearch utilizes a small model to strategically collaborate\nwith a large model at both thought and token levels, efficiently generating\nhigh-quality reasoning thoughts. The major pillar of SpecSearch is a novel\nquality-preserving rejection mechanism, which effectively filters out thoughts\nwhose quality falls below that of the large model's outputs. Moreover, we show\nthat SpecSearch preserves comparable reasoning quality to the large model.\nExperiments on both the Qwen and Llama models demonstrate that SpecSearch\nsignificantly outperforms state-of-the-art approaches, achieving up to\n2.12$\\times$ speedup with comparable reasoning quality."}
{"id": "2505.03618", "pdf": "https://arxiv.org/pdf/2505.03618.pdf", "abs": "https://arxiv.org/abs/2505.03618", "title": "Scalable Class-Centric Visual Interactive Labeling", "authors": ["Matthias Matt", "Jana Sedlakova", "Jürgen Bernard", "Matthias Zeppelzauer", "Manuela Waldner"], "categories": ["cs.HC"], "comment": null, "summary": "Large unlabeled datasets demand efficient and scalable data labeling\nsolutions, in particular when the number of instances and classes is large.\nThis leads to significant visual scalability challenges and imposes a high\ncognitive load on the users. Traditional instance-centric labeling methods,\nwhere (single) instances are labeled in each iteration struggle to scale\neffectively in these scenarios. To address these challenges, we introduce cVIL,\na Class-Centric Visual Interactive Labeling methodology designed for\ninteractive visual data labeling. By shifting the paradigm from\nassigning-classes-to-instances to assigning-instances-to-classes, cVIL reduces\nlabeling effort and enhances efficiency for annotators working with large,\ncomplex and class-rich datasets. We propose a novel visual analytics labeling\ninterface built on top of the conceptual cVIL workflow, enabling improved\nscalability over traditional visual labeling. In a user study, we demonstrate\nthat cVIL can improve labeling efficiency and user satisfaction over\ninstance-centric interfaces. The effectiveness of cVIL is further demonstrated\nthrough a usage scenario, showcasing its potential to alleviate cognitive load\nand support experts in managing extensive labeling tasks efficiently."}
{"id": "2505.02872", "pdf": "https://arxiv.org/pdf/2505.02872.pdf", "abs": "https://arxiv.org/abs/2505.02872", "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading", "authors": ["Cfir Avraham Hadar", "Omer Shubi", "Yoav Meiri", "Yevgeni Berzak"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements."}
{"id": "2505.03682", "pdf": "https://arxiv.org/pdf/2505.03682.pdf", "abs": "https://arxiv.org/abs/2505.03682", "title": "Evaluating Foveated Frame Rate Reduction in Virtual Reality for Head-Mounted Displays", "authors": ["Christopher Flöter", "Sergej Geringer", "Guido Reina", "Daniel Weiskopf", "Timo Ropinski"], "categories": ["cs.HC", "cs.GR", "I.3.6; I.3.7"], "comment": "Temporal resolution reduction, frame rate reduction, foveated\n  rendering, virtual reality", "summary": "Foveated rendering methods usually reduce spatial resolution in the periphery\nof the users' view. However, using foveated rendering to reduce temporal\nresolution, i.e., rendering frame rate, seems less explored. In this work, we\npresent the results of a user study investigating the perceptual effects of\nfoveated temporal resolution reduction, where only the temporal resolution\n(frame rate) is reduced in the periphery without affecting spatial quality\n(pixel density). In particular, we investigated the perception of temporal\nresolution artifacts caused by reducing the frame rate dependent on the\neccentricity of the user's gaze. Our user study with 15 participants was\nconducted in a virtual reality setting using a head-mounted display. Our\nresults indicate that it was possible to reduce average rendering costs, i.e.,\nthe number of rendered pixels, to a large degree before participants\nconsistently reported perceiving temporal artifacts."}
{"id": "2505.02983", "pdf": "https://arxiv.org/pdf/2505.02983.pdf", "abs": "https://arxiv.org/abs/2505.02983", "title": "Logits-Constrained Framework with RoBERTa for Ancient Chinese NER", "authors": ["Wenjie Hua", "Shenghan Xu"], "categories": ["cs.CL", "68T50", "I.2.7; I.5.1; I.5.4"], "comment": "5 pages, 2 figures, 6 tables. Accepted to EvaHan 2025 shared task on\n  Ancient Chinese NLP", "summary": "This paper presents a Logits-Constrained (LC) framework for Ancient Chinese\nNamed Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our\ntwo-stage model integrates GujiRoBERTa for contextual encoding and a\ndifferentiable decoding mechanism to enforce valid BMES label transitions.\nExperiments demonstrate that LC improves performance over traditional CRF and\nBiLSTM-based approaches, especially in high-label or large-data settings. We\nalso propose a model selection criterion balancing label complexity and dataset\nsize, providing practical guidance for real-world Ancient Chinese NLP tasks."}
{"id": "2505.02842", "pdf": "https://arxiv.org/pdf/2505.02842.pdf", "abs": "https://arxiv.org/abs/2505.02842", "title": "Evaluation of Coordination Strategies for Underground Automated Vehicle Fleets in Mixed Traffic", "authors": ["Olga Mironenko", "Hadi Banaee", "Amy Loutfi"], "categories": ["physics.soc-ph", "cs.HC", "cs.MA", "cs.RO", "cs.SY", "eess.SY", "I.2.9; I.2.11; H.1.2"], "comment": "Accepted for publication in the Proceedings of the 2025 IEEE\n  Intelligent Vehicles Symposium (IV 2025)", "summary": "This study investigates the efficiency and safety outcomes of implementing\ndifferent adaptive coordination models for automated vehicle (AV) fleets,\nmanaged by a centralized coordinator that dynamically responds to\nhuman-controlled vehicle behavior. The simulated scenarios replicate an\nunderground mining environment characterized by narrow tunnels with limited\nconnectivity. To address the unique challenges of such settings, we propose a\nnovel metric - Path Overlap Density (POD) - to predict efficiency and\npotentially the safety performance of AV fleets. The study also explores the\nimpact of map features on AV fleets performance. The results demonstrate that\nboth AV fleet coordination strategies and underground tunnel network\ncharacteristics significantly influence overall system performance. While map\nfeatures are critical for optimizing efficiency, adaptive coordination\nstrategies are essential for ensuring safe operations."}
{"id": "2505.03005", "pdf": "https://arxiv.org/pdf/2505.03005.pdf", "abs": "https://arxiv.org/abs/2505.03005", "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale", "authors": ["Daniel Goldstein", "Eric Alcaide", "Janna Lu", "Eugene Cheah"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper"}
{"id": "2505.02975", "pdf": "https://arxiv.org/pdf/2505.02975.pdf", "abs": "https://arxiv.org/abs/2505.02975", "title": "Navigating Privacy and Trust: AI Assistants as Social Support for Older Adults", "authors": ["Karina LaRubbio", "Malcolm Grba", "Diana Freed"], "categories": ["cs.CY", "cs.HC"], "comment": "7 pages. Proceedings of the CHI 2025 Workshop on Technology Mediated\n  Caregiving for Older Adults Aging in Place, April 27, 2025, Yokohama, Japan", "summary": "AI assistants are increasingly integrated into older adults' daily lives,\noffering new opportunities for social support and accessibility while raising\nimportant questions about privacy, autonomy, and trust. As these systems become\nembedded in caregiving and social networks, older adults must navigate\ntrade-offs between usability, data privacy, and personal agency across\ndifferent interaction contexts. Although prior work has explored AI assistants'\npotential benefits, further research is needed to understand how perceived\nusefulness and risk shape adoption and engagement. This paper examines these\ndynamics and advocates for participatory design approaches that position older\nadults as active decision makers in shaping AI assistant functionality. By\nadvancing a framework for privacy-aware, user-centered AI design, this work\ncontributes to ongoing discussions on developing ethical and transparent AI\nsystems that enhance well-being without compromising user control."}
{"id": "2505.03019", "pdf": "https://arxiv.org/pdf/2505.03019.pdf", "abs": "https://arxiv.org/abs/2505.03019", "title": "Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis", "authors": ["Albérick Euraste Djiré", "Abdoul Kader Kaboré", "Earl T. Barr", "Jacques Klein", "Tegawendé F. Bissyandé"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) achieve remarkable performance through\ntraining on massive datasets, they can exhibit concerning behaviors such as\nverbatim reproduction of training data rather than true generalization. This\nmemorization phenomenon raises significant concerns about data privacy,\nintellectual property rights, and the reliability of model evaluations. This\npaper introduces PEARL, a novel approach for detecting memorization in LLMs.\nPEARL assesses how sensitive an LLM's performance is to input perturbations,\nenabling memorization detection without requiring access to the model's\ninternals. We investigate how input perturbations affect the consistency of\noutputs, enabling us to distinguish between true generalization and\nmemorization. Our findings, following extensive experiments on the Pythia open\nmodel, provide a robust framework for identifying when the model simply\nregurgitates learned information. Applied on the GPT 4o models, the PEARL\nframework not only identified cases of memorization of classic texts from the\nBible or common code from HumanEval but also demonstrated that it can provide\nsupporting evidence that some data, such as from the New York Times news\narticles, were likely part of the training data of a given model."}
{"id": "2505.03033", "pdf": "https://arxiv.org/pdf/2505.03033.pdf", "abs": "https://arxiv.org/abs/2505.03033", "title": "Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes", "authors": ["George Xi Wang", "Jingying Deng", "Safinah Ali"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Independent learners often struggle with sustaining focus and emotional\nregulation in unstructured or distracting settings. Although some rely on\nambient aids such as music, ASMR, or visual backgrounds to support\nconcentration, these tools are rarely integrated into cohesive,\nlearner-centered systems. Moreover, existing educational technologies focus\nprimarily on content adaptation and feedback, overlooking the emotional and\nsensory context in which learning takes place. Large language models have\ndemonstrated powerful multimodal capabilities including the ability to generate\nand adapt text, audio, and visual content. Educational research has yet to\nfully explore their potential in creating personalized audiovisual learning\nenvironments. To address this gap, we introduce an AI-powered system that uses\nLLMs to generate personalized multisensory study environments. Users select or\ngenerate customized visual themes (e.g., abstract vs. realistic, static vs.\nanimated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs.\nnovel sounds) to create immersive settings aimed at reducing distraction and\nenhancing emotional stability. Our primary research question investigates how\ncombinations of personalized audiovisual elements affect learner cognitive load\nand engagement. Using a mixed-methods design that incorporates biometric\nmeasures and performance outcomes, this study evaluates the effectiveness of\nLLM-driven sensory personalization. The findings aim to advance emotionally\nresponsive educational technologies and extend the application of multimodal\nLLMs into the sensory dimension of self-directed learning."}
{"id": "2505.03025", "pdf": "https://arxiv.org/pdf/2505.03025.pdf", "abs": "https://arxiv.org/abs/2505.03025", "title": "A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts", "authors": ["Steven Bedrick", "A. Seza Doğruöz", "Sergiu Nisioi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Synthetic data sets are used across linguistic domains and NLP tasks,\nparticularly in scenarios where authentic data is limited (or even\nnon-existent). One such domain is that of clinical (healthcare) contexts, where\nthere exist significant and long-standing challenges (e.g., privacy,\nanonymization, and data governance) which have led to the development of an\nincreasing number of synthetic datasets. One increasingly important category of\nclinical dataset is that of clinical dialogues which are especially sensitive\nand difficult to collect, and as such are commonly synthesized.\n  While such synthetic datasets have been shown to be sufficient in some\nsituations, little theory exists to inform how they may be best used and\ngeneralized to new applications. In this paper, we provide an overview of how\nsynthetic datasets are created, evaluated and being used for dialogue related\ntasks in the medical domain. Additionally, we propose a novel typology for use\nin classifying types and degrees of data synthesis, to facilitate comparison\nand evaluation."}
{"id": "2505.03132", "pdf": "https://arxiv.org/pdf/2505.03132.pdf", "abs": "https://arxiv.org/abs/2505.03132", "title": "VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis", "authors": ["Xinyuan Yan", "Xiwei Xuan", "Jorge Piazentin Ono", "Jiajing Guo", "Vikram Mohanty", "Shekar Arvind Kumar", "Liang Gou", "Bei Wang", "Liu Ren"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Real-world machine learning models require rigorous evaluation before\ndeployment, especially in safety-critical domains like autonomous driving and\nsurveillance. The evaluation of machine learning models often focuses on data\nslices, which are subsets of the data that share a set of characteristics. Data\nslice finding automatically identifies conditions or data subgroups where\nmodels underperform, aiding developers in mitigating performance issues.\nDespite its popularity and effectiveness, data slicing for vision model\nvalidation faces several challenges. First, data slicing often needs additional\nimage metadata or visual concepts, and falls short in certain computer vision\ntasks, such as object detection. Second, understanding data slices is a\nlabor-intensive and mentally demanding process that heavily relies on the\nexpert's domain knowledge. Third, data slicing lacks a human-in-the-loop\nsolution that allows experts to form hypothesis and test them interactively. To\novercome these limitations and better support the machine learning operations\nlifecycle, we introduce VISLIX, a novel visual analytics framework that employs\nstate-of-the-art foundation models to help domain experts analyze slices in\ncomputer vision models. Our approach does not require image metadata or visual\nconcepts, automatically generates natural language insights, and allows users\nto test data slice hypothesis interactively. We evaluate VISLIX with an expert\nstudy and three use cases, that demonstrate the effectiveness of our tool in\nproviding comprehensive insights for validating object detection models."}
{"id": "2505.03030", "pdf": "https://arxiv.org/pdf/2505.03030.pdf", "abs": "https://arxiv.org/abs/2505.03030", "title": "UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output", "authors": ["Sicong Huang", "Jincheng He", "Shiyuan Huang", "Karthik Raja Anandan", "Arkajyoti Chakraborty", "Ian Lane"], "categories": ["cs.CL"], "comment": "6 pages, 1 figure", "summary": "Hallucinations pose a significant challenge for large language models when\nanswering knowledge-intensive queries. As LLMs become more widely adopted, it\nis crucial not only to detect if hallucinations occur but also to pinpoint\nexactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:\nMultilingual Shared-task on Hallucinations and Related Observable\nOvergeneration Mistakes, is a recent effort in this direction. This paper\ndescribes the UCSC system submission to the shared Mu-SHROOM task. We introduce\na framework that first retrieves relevant context, next identifies false\ncontent from the answer, and finally maps them back to spans in the LLM output.\nThe process is further enhanced by automatically optimizing prompts. Our system\nachieves the highest overall performance, ranking #1 in average position across\nall languages. We release our code and experiment results."}
{"id": "2505.03163", "pdf": "https://arxiv.org/pdf/2505.03163.pdf", "abs": "https://arxiv.org/abs/2505.03163", "title": "The Impact of Large Language Models on K-12 Education in Rural India: A Thematic Analysis of Student Volunteer's Perspectives", "authors": ["Harshita Goyal", "Garima Garg", "Prisha Mordia", "Veena Ramachandran", "Dhruv Kumar", "Jagat Sesh Challa"], "categories": ["cs.CY", "cs.HC"], "comment": "Under review, 23 pages, 1 figure, 1 table", "summary": "AI-driven education, particularly Large Language Models (LLMs), has the\npotential to address learning disparities in rural K-12 schools. However,\nresearch on AI adoption in rural India remains limited, with existing studies\nfocusing primarily on urban settings. This study examines the perceptions of\nvolunteer teachers on AI integration in rural education, identifying key\nchallenges and opportunities. Through semi-structured interviews with 23\nvolunteer educators in Rajasthan and Delhi, we conducted a thematic analysis to\nexplore infrastructure constraints, teacher preparedness, and digital literacy\ngaps. Findings indicate that while LLMs could enhance personalized learning and\nreduce teacher workload, barriers such as poor connectivity, lack of AI\ntraining, and parental skepticism hinder adoption. Despite concerns over\nover-reliance and ethical risks, volunteers emphasize that AI should be seen as\na complementary tool rather than a replacement for traditional teaching. Given\nthe potential benefits, LLM-based tutors merit further exploration in rural\nclassrooms, with structured implementation and localized adaptations to ensure\naccessibility and equity."}
{"id": "2505.03052", "pdf": "https://arxiv.org/pdf/2505.03052.pdf", "abs": "https://arxiv.org/abs/2505.03052", "title": "Teaching Models to Understand (but not Generate) High-risk Data", "authors": ["Ryan Wang", "Matthew Finlayson", "Luca Soldaini", "Swabha Swayamdipta", "Robin Jia"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language model developers typically filter out high-risk content -- such as\ntoxic or copyrighted text -- from their pre-training data to prevent models\nfrom generating similar outputs. However, removing such data altogether limits\nmodels' ability to recognize and appropriately respond to harmful or sensitive\ncontent. In this paper, we introduce Selective Loss to Understand but Not\nGenerate (SLUNG), a pre-training paradigm through which models learn to\nunderstand high-risk data without learning to generate it. Instead of uniformly\napplying the next-token prediction loss, SLUNG selectively avoids incentivizing\nthe generation of high-risk tokens while ensuring they remain within the\nmodel's context window. As the model learns to predict low-risk tokens that\nfollow high-risk ones, it is forced to understand the high-risk content.\nThrough our experiments, we show that SLUNG consistently improves models'\nunderstanding of high-risk data (e.g., ability to recognize toxic content)\nwithout increasing its generation (e.g., toxicity of model responses). Overall,\nour SLUNG paradigm enables models to benefit from high-risk text that would\notherwise be filtered out."}
{"id": "2505.03189", "pdf": "https://arxiv.org/pdf/2505.03189.pdf", "abs": "https://arxiv.org/abs/2505.03189", "title": "Patterns and Mechanisms of Contrastive Activation Engineering", "authors": ["Yixiong Hao", "Ayush Panda", "Stepan Shabalin", "Sheikh Abdur Raheem Ali"], "categories": ["cs.AI", "cs.HC"], "comment": "Published at the ICLR 2025 Bi-Align, HAIC, and Building Trust\n  workshops", "summary": "Controlling the behavior of Large Language Models (LLMs) remains a\nsignificant challenge due to their inherent complexity and opacity. While\ntechniques like fine-tuning can modify model behavior, they typically require\nextensive computational resources. Recent work has introduced a class of\ncontrastive activation engineering (CAE) techniques as promising approaches for\nsteering LLM outputs through targeted modifications to their internal\nrepresentations. Applied at inference-time with zero cost, CAE has the\npotential to introduce a new paradigm of flexible, task-specific LLM behavior\ntuning. We analyze the performance of CAE in in-distribution,\nout-of-distribution settings, evaluate drawbacks, and begin to develop\ncomprehensive guidelines for its effective deployment. We find that 1. CAE is\nonly reliably effective when applied to in-distribution contexts. 2. Increasing\nthe number of samples used to generate steering vectors has diminishing returns\nat around 80 samples. 3. Steering vectors are susceptible to adversarial inputs\nthat reverses the behavior that is steered for. 4. Steering vectors harm the\noverall model perplexity. 5. Larger models are more resistant to\nsteering-induced degradation."}
{"id": "2505.03053", "pdf": "https://arxiv.org/pdf/2505.03053.pdf", "abs": "https://arxiv.org/abs/2505.03053", "title": "Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text", "authors": ["Jennifer Healey", "Laurie Byrum", "Md Nadeem Akhtar", "Surabhi Bhargava", "Moumita Sinha"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, no figures, presented at CHI 2025 workshop for Human\n  Evaluation and Auditing of Language Models", "summary": "LLM evaluation is challenging even the case of base models. In real world\ndeployments, evaluation is further complicated by the interplay of task\nspecific prompts and experiential context. At scale, bias evaluation is often\nbased on short context, fixed choice benchmarks that can be rapidly evaluated,\nhowever, these can lose validity when the LLMs' deployed context differs. Large\nscale human evaluation is often seen as too intractable and costly. Here we\npresent our journey towards developing a semi-automated bias evaluation\nframework for free text responses that has human insights at its core. We\ndiscuss how we developed an operational definition of bias that helped us\nautomate our pipeline and a methodology for classifying bias beyond multiple\nchoice. We additionally comment on how human evaluation helped us uncover\nproblematic templates in a bias benchmark."}
{"id": "2505.03251", "pdf": "https://arxiv.org/pdf/2505.03251.pdf", "abs": "https://arxiv.org/abs/2505.03251", "title": "Chess variation entropy and engine relevance for humans", "authors": ["Marc Barthelemy"], "categories": ["physics.soc-ph", "cond-mat.dis-nn", "cs.HC"], "comment": "6 pages, 4 figures", "summary": "Modern chess engines significantly outperform human players and are essential\nfor evaluating positions and move quality. These engines assign a numerical\nevaluation $E$ to positions, indicating an advantage for either white or black,\nbut similar evaluations can mask varying levels of move complexity. While some\nmove sequences are straightforward, others demand near-perfect play, limiting\nthe practical value of these evaluations for most players. To quantify this\nproblem, we use entropy to measure the complexity of the principal variation\n(the sequence of best moves). Variations with forced moves have low entropy,\nwhile those with multiple viable alternatives have high entropy. Our results\nshow that, except for experts, most human players struggle with high-entropy\nvariations, especially when $|E|<100$ centipawns, which accounts for about\n$2/3$ of positions. This underscores the need for AI-generated evaluations to\nconvey the complexity of underlying move sequences, as they often exceed\ntypical human cognitive capabilities, reducing their practical utility."}
{"id": "2505.03059", "pdf": "https://arxiv.org/pdf/2505.03059.pdf", "abs": "https://arxiv.org/abs/2505.03059", "title": "Improving Model Alignment Through Collective Intelligence of Open-Source LLMS", "authors": ["Junlin Wang", "Roy Xie", "Shang Zhu", "Jue Wang", "Ben Athiwaratkun", "Bhuwan Dhingra", "Shuaiwen Leon Song", "Ce Zhang", "James Zou"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Building helpful and harmless large language models (LLMs) requires effective\nmodel alignment approach based on human instructions and feedback, which\nnecessitates high-quality human-labeled data. Constructing such datasets is\noften expensive and hard to scale, and may face potential limitations on\ndiversity and generalization. To address these challenges, we introduce Mixture\nof Agents Alignment (MoAA), that leverages the collective strengths of various\nlanguage models to provide high-quality data for model alignment. By employing\nMoAA, we enhance both supervised fine-tuning and preference optimization,\nleading to improved performance compared to using a single model alone to\ngenerate alignment data (e.g. using GPT-4o alone). Evaluation results show that\nour approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on\nArena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising\ndirection for model alignment through this new scalable and diverse synthetic\ndata recipe. Furthermore, we demonstrate that MoAA enables a self-improvement\npipeline, where models finetuned on MoA-generated data surpass their own\ninitial capabilities, providing evidence that our approach can push the\nfrontier of open-source LLMs without reliance on stronger external supervision.\nData and code will be released."}
{"id": "2505.03376", "pdf": "https://arxiv.org/pdf/2505.03376.pdf", "abs": "https://arxiv.org/abs/2505.03376", "title": "Tell Me the Good Stuff: User Preferences in Movie Recommendation Explanations", "authors": ["Juan Ahmad", "Jonas Hellgren", "Alan Said"], "categories": ["cs.IR", "cs.HC"], "comment": "Adjunct Proceedings of the 33rd ACM Conference on User Modeling,\n  Adaptation and Personalization", "summary": "Recommender systems play a vital role in helping users discover content in\nstreaming services, but their effectiveness depends on users understanding why\nitems are recommended. In this study, explanations were based solely on item\nfeatures rather than personalized data, simulating recommendation scenarios. We\ncompared user perceptions of one-sided (purely positive) and two-sided\n(positive and negative) feature-based explanations for popular movie\nrecommendations. Through an online study with 129 participants, we examined how\nexplanation style affected perceived trust, transparency, effectiveness, and\nsatisfaction. One-sided explanations consistently received higher ratings\nacross all dimensions. Our findings suggest that in low-stakes entertainment\ndomains such as popular movie recommendations, simpler positive explanations\nmay be more effective. However, the results should be interpreted with caution\ndue to potential confounding factors such as item familiarity and the placement\nof negative information in explanations. This work provides practical insights\nfor explanation design in recommender interfaces and highlights the importance\nof context in shaping user preferences."}
{"id": "2505.03229", "pdf": "https://arxiv.org/pdf/2505.03229.pdf", "abs": "https://arxiv.org/abs/2505.03229", "title": "Survey of Abstract Meaning Representation: Then, Now, Future", "authors": ["Behrooz Mansouri"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a survey of Abstract Meaning Representation (AMR), a\nsemantic representation framework that captures the meaning of sentences\nthrough a graph-based structure. AMR represents sentences as rooted, directed\nacyclic graphs, where nodes correspond to concepts and edges denote\nrelationships, effectively encoding the meaning of complex sentences. This\nsurvey investigates AMR and its extensions, focusing on AMR capabilities. It\nthen explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by\nshowing traditional, current, and possible futures approaches. It also reviews\nvarious applications of AMR including text generation, text classification, and\ninformation extraction and information seeking. By analyzing recent\ndevelopments and challenges in the field, this survey provides insights into\nfuture directions for research and the potential impact of AMR on enhancing\nmachine understanding of human language."}
{"id": "2505.03427", "pdf": "https://arxiv.org/pdf/2505.03427.pdf", "abs": "https://arxiv.org/abs/2505.03427", "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks", "authors": ["Mouath Abu Daoud", "Chaimae Abouzahir", "Leen Kharouf", "Walid Al-Eisawi", "Nizar Habash", "Farah E. Shamout"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages", "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare."}
{"id": "2505.03293", "pdf": "https://arxiv.org/pdf/2505.03293.pdf", "abs": "https://arxiv.org/abs/2505.03293", "title": "Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback", "authors": ["Shijing Zhu", "Zhuang Chen", "Guanqun Bi", "Binghang Li", "Yaxi Deng", "Dazhen Wan", "Libiao Peng", "Xiyao Xiao", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "FangFang Li", "Minlie Huang"], "categories": ["cs.CL"], "comment": "in progress", "summary": "Large language models (LLMs) have shown promise in providing scalable mental\nhealth support, while evaluating their counseling capability remains crucial to\nensure both efficacy and safety. Existing evaluations are limited by the static\nassessment that focuses on knowledge tests, the single perspective that centers\non user experience, and the open-loop framework that lacks actionable feedback.\nTo address these issues, we propose {\\Psi}-Arena, an interactive framework for\ncomprehensive assessment and optimization of LLM-based counselors, featuring\nthree key characteristics: (1) Realistic arena interactions that simulate\nreal-world counseling through multi-stage dialogues with psychologically\nprofiled NPC clients, (2) Tripartite evaluation that integrates assessments\nfrom the client, counselor, and supervisor perspectives, and (3) Closed-loop\noptimization that iteratively improves LLM counselors using diagnostic\nfeedback. Experiments across eight state-of-the-art LLMs show significant\nperformance variations in different real-world scenarios and evaluation\nperspectives. Moreover, reflection-based optimization results in up to a 141%\nimprovement in counseling performance. We hope PsychoArena provides a\nfoundational resource for advancing reliable and human-aligned LLM applications\nin mental healthcare."}
{"id": "2505.03568", "pdf": "https://arxiv.org/pdf/2505.03568.pdf", "abs": "https://arxiv.org/abs/2505.03568", "title": "Familiarizing with Music: Discovery Patterns for Different Music Discovery Needs", "authors": ["Marta Moscati", "Darius Afchar", "Markus Schedl", "Bruno Sguerra"], "categories": ["cs.IR", "cs.HC"], "comment": null, "summary": "Humans have the tendency to discover and explore. This natural tendency is\nreflected in data from streaming platforms as the amount of previously unknown\ncontent accessed by users. Additionally, in domains such as that of music\nstreaming there is evidence that recommending novel content improves users'\nexperience with the platform. Therefore, understanding users' discovery\npatterns, such as the amount to which and the way users access previously\nunknown content, is a topic of relevance for both the scientific community and\nthe streaming industry, particularly the music one. Previous works studied how\nmusic consumption differs for users of different traits and looked at\ndiversity, novelty, and consistency over time of users' music preferences.\nHowever, very little is known about how users discover and explore previously\nunknown music, and how this behavior differs for users of varying discovery\nneeds. In this paper we bridge this gap by analyzing data from a survey\nanswered by users of the major music streaming platform Deezer in combination\nwith their streaming data. We first address questions regarding whether users\nwho declare a higher interest in unfamiliar music listen to more diverse music,\nhave more stable music preferences over time, and explore more music within a\nsame time window, compared to those who declare a lower interest. We then\ninvestigate which type of music tracks users choose to listen to when they\nexplore unfamiliar music, identifying clear patterns of popularity and genre\nrepresentativeness that vary for users of different discovery needs.\n  Our findings open up possibilities to infer users' interest in unfamiliar\nmusic from streaming data as well as possibilities to develop recommender\nsystems that guide users in exploring music in a more natural way."}
{"id": "2505.03320", "pdf": "https://arxiv.org/pdf/2505.03320.pdf", "abs": "https://arxiv.org/abs/2505.03320", "title": "Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation", "authors": ["Junyu Ma", "Tianqing Fang", "Zhisong Zhang", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Mamba's theoretical infinite-context potential is limited in practice when\nsequences far exceed training lengths. This work explores unlocking Mamba's\nlong-context memory ability by a simple-yet-effective method, Recall with\nReasoning (RwR), by distilling chain-of-thought (CoT) summarization from a\nteacher model. Specifically, RwR prepends these summarization as CoT prompts\nduring fine-tuning, teaching Mamba to actively recall and reason over long\ncontexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's\nlong-context performance against comparable Transformer/hybrid baselines under\nsimilar pretraining conditions, while preserving short-context capabilities,\nall without architectural changes."}
{"id": "2409.10913", "pdf": "https://arxiv.org/pdf/2409.10913.pdf", "abs": "https://arxiv.org/abs/2409.10913", "title": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of Community Health Workers", "authors": ["Pragnya Ramjee", "Mehak Chhokar", "Bhuvan Sachdeva", "Mahendra Meena", "Hamid Abdullah", "Aditya Vashistha", "Ruchit Nagar", "Mohit Jain"], "categories": ["cs.HC"], "comment": null, "summary": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support."}
{"id": "2505.03406", "pdf": "https://arxiv.org/pdf/2505.03406.pdf", "abs": "https://arxiv.org/abs/2505.03406", "title": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation", "authors": ["Mohammad Shoaib Ansari", "Mohd Sohail Ali Khan", "Shubham Revankar", "Aditya Varma", "Anil S. Mokhade"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings."}
{"id": "2410.00196", "pdf": "https://arxiv.org/pdf/2410.00196.pdf", "abs": "https://arxiv.org/abs/2410.00196", "title": "Motion Design Principles for Accessible Video-based Learning: Addressing Cognitive Challenges for Deaf and Hard of Hearing Learners", "authors": ["Si Cheng", "Haocong Cheng", "Suzy Su", "Lu Ming", "Sarah Masud", "Qi Wang", "Yun Huang"], "categories": ["cs.HC"], "comment": null, "summary": "Deaf and Hard-of-Hearing (DHH) learners face unique challenges in video-based\nlearning due to the complex interplay between visual and auditory information\nin videos. Traditional approaches to making video content accessible primarily\nfocus on captioning, but these solutions often neglect the cognitive demands of\nprocessing both visual and textual information simultaneously. This paper\nintroduces a set of \\textit{Motion} design guidelines, aimed at mitigating\nthese cognitive challenges and improving video learning experiences for DHH\nlearners. Through a two-phase research, we identified five key challenges,\nincluding misaligned content and visual overload. We proposed five design\nprinciples accordingly. User study with 16 DHH participants showed that\nimproving visual-audio relevance and guiding visual attention significantly\nenhances the learning experience by reducing physical demand, alleviating\ntemporal pressure, and improving learning satisfaction. Our findings highlight\nthe potential of Motion design to transform educational content for DHH\nlearners, and we discuss implications for inclusive video learning tools."}
{"id": "2505.03427", "pdf": "https://arxiv.org/pdf/2505.03427.pdf", "abs": "https://arxiv.org/abs/2505.03427", "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks", "authors": ["Mouath Abu Daoud", "Chaimae Abouzahir", "Leen Kharouf", "Walid Al-Eisawi", "Nizar Habash", "Farah E. Shamout"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages", "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare."}
{"id": "2410.03032", "pdf": "https://arxiv.org/pdf/2410.03032.pdf", "abs": "https://arxiv.org/abs/2410.03032", "title": "CounterQuill: Investigating the Potential of Human-AI Collaboration in Online Counterspeech Writing", "authors": ["Xiaohan Ding", "Kaike Ping", "Uma Sushmitha Gunturi", "Buse Carik", "Sophia Stil", "Lance T Wilhelm", "Taufiq Daryanto", "James Hawdon", "Sang Won Lee", "Eugenia H Rho"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Online hate speech has become increasingly prevalent on social media\nplatforms, causing harm to individuals and society. While efforts have been\nmade to combat this issue through content moderation, the potential of\nuser-driven counterspeech as an alternative solution remains underexplored.\nExisting counterspeech methods often face challenges such as fear of\nretaliation and skill-related barriers. To address these challenges, we\nintroduce CounterQuill, an AI-mediated system that assists users in composing\neffective and empathetic counterspeech. CounterQuill provides a three-step\nprocess: (1) a learning session to help users understand hate speech and\ncounterspeech; (2) a brainstorming session that guides users in identifying key\nelements of hate speech and exploring counterspeech strategies; and (3) a\nco-writing session that enables users to draft and refine their counterspeech\nwith CounterQuill. We conducted a within-subjects user study with 20\nparticipants to evaluate CounterQuill in comparison to ChatGPT. Results show\nthat CounterQuill's guidance and collaborative writing process provided users a\nstronger sense of ownership over their co-authored counterspeech. Users\nperceived CounterQuill as a writing partner and thus were more willing to post\nthe co-written counterspeech online compared to the one written with ChatGPT."}
{"id": "2505.03452", "pdf": "https://arxiv.org/pdf/2505.03452.pdf", "abs": "https://arxiv.org/abs/2505.03452", "title": "An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation", "authors": ["Matan Orbach", "Ohad Eytan", "Benjamin Sznajder", "Ariel Gera", "Odellia Boni", "Yoav Kantor", "Gal Bloch", "Omri Levy", "Hadas Abraham", "Nitzan Barzilay", "Eyal Shnarch", "Michael E. Factor", "Shila Ofek-Koifman", "Paula Ta-Shma", "Assaf Toledo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a\ngiven use case can be complex and expensive. Motivated by this challenge,\nframeworks for RAG hyper-parameter optimization (HPO) have recently emerged,\nyet their effectiveness has not been rigorously benchmarked. To address this\ngap, we present a comprehensive study involving 5 HPO algorithms over 5\ndatasets from diverse domains, including a new one collected for this work on\nreal-world product documentation. Our study explores the largest HPO search\nspace considered to date, with two optimized evaluation metrics. Analysis of\nthe results shows that RAG HPO can be done efficiently, either greedily or with\niterative random search, and that it significantly boosts RAG performance for\nall datasets. For greedy HPO approaches, we show that optimizing models first\nis preferable to the prevalent practice of optimizing sequentially according to\nthe RAG pipeline order."}
{"id": "2503.16518", "pdf": "https://arxiv.org/pdf/2503.16518.pdf", "abs": "https://arxiv.org/abs/2503.16518", "title": "Advancing Human-Machine Teaming: Concepts, Challenges, and Applications", "authors": ["Dian Chen", "Han Jun Yoon", "Zelin Wan", "Nithin Alluru", "Sang Won Lee", "Richard He", "Terrence J. Moore", "Frederica F. Nelson", "Sunghyun Yoon", "Hyuk Lim", "Dan Dongseong Kim", "Jin-Hee Cho"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Human-Machine Teaming (HMT) is revolutionizing collaboration across domains\nsuch as defense, healthcare, and autonomous systems by integrating AI-driven\ndecision-making, trust calibration, and adaptive teaming. This survey presents\na comprehensive taxonomy of HMT, analyzing theoretical models, including\nreinforcement learning, instance-based learning, and interdependence theory,\nalongside interdisciplinary methodologies. Unlike prior reviews, we examine\nteam cognition, ethical AI, multi-modal interactions, and real-world evaluation\nframeworks. Key challenges include explainability, role allocation, and\nscalable benchmarking. We propose future research in cross-domain adaptation,\ntrust-aware AI, and standardized testbeds. By bridging computational and social\nsciences, this work lays a foundation for resilient, ethical, and scalable HMT\nsystems."}
{"id": "2505.03467", "pdf": "https://arxiv.org/pdf/2505.03467.pdf", "abs": "https://arxiv.org/abs/2505.03467", "title": "Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis", "authors": ["Shuang Zhou", "Jiashuo Wang", "Zidu Xu", "Song Wang", "David Brauer", "Lindsay Welton", "Jacob Cogan", "Yuen-Hei Chung", "Lei Tian", "Zaifu Zhan", "Yu Hou", "Mingquan Lin", "Genevieve B. Melton", "Rui Zhang"], "categories": ["cs.CL"], "comment": "22 pages, 8 figures", "summary": "Explainable disease diagnosis, which leverages patient information (e.g.,\nsigns and symptoms) and computational models to generate probable diagnoses and\nreasonings, offers clear clinical values. However, when clinical notes\nencompass insufficient evidence for a definite diagnosis, such as the absence\nof definitive symptoms, diagnostic uncertainty usually arises, increasing the\nrisk of misdiagnosis and adverse outcomes. Although explicitly identifying and\nexplaining diagnostic uncertainties is essential for trustworthy diagnostic\nsystems, it remains under-explored. To fill this gap, we introduce ConfiDx, an\nuncertainty-aware large language model (LLM) created by fine-tuning open-source\nLLMs with diagnostic criteria. We formalized the task and assembled richly\nannotated datasets that capture varying degrees of diagnostic ambiguity.\nEvaluating ConfiDx on real-world datasets demonstrated that it excelled in\nidentifying diagnostic uncertainties, achieving superior diagnostic\nperformance, and generating trustworthy explanations for diagnoses and\nuncertainties. To our knowledge, this is the first study to jointly address\ndiagnostic uncertainty recognition and explanation, substantially enhancing the\nreliability of automatic diagnostic systems."}
{"id": "2504.01153", "pdf": "https://arxiv.org/pdf/2504.01153.pdf", "abs": "https://arxiv.org/abs/2504.01153", "title": "Catch Me if You Search: When Contextual Web Search Results Affect the Detection of Hallucinations", "authors": ["Mahjabin Nahar", "Eun-Ju Lee", "Jin Won Park", "Dongwon Lee"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or `hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N = 560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts."}
{"id": "2505.03469", "pdf": "https://arxiv.org/pdf/2505.03469.pdf", "abs": "https://arxiv.org/abs/2505.03469", "title": "Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models", "authors": ["Bin Yu", "Hang Yuan", "Yuliang Wei", "Bailing Wang", "Weizhen Qi", "Kai Chen"], "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "Recent advances in large language models have demonstrated that Supervised\nFine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from\nlarge reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning\ncapabilities to non-reasoning models. However, models fine-tuned with this\napproach inherit the \"overthinking\" problem from teacher models, producing\nverbose and redundant reasoning chains during inference. To address this\nchallenge, we propose \\textbf{L}ong-\\textbf{S}hort Chain-of-Thought\n\\textbf{Mixture} \\textbf{S}upervised \\textbf{F}ine-\\textbf{T}uning\n(\\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their\nshort counterparts obtained through structure-preserved rewriting. Our\nexperiments demonstrate that models trained using the LS-Mixture SFT method,\ncompared to those trained with direct SFT, achieved an average accuracy\nimprovement of 2.3\\% across various benchmarks while substantially reducing\nmodel response length by approximately 47.61\\%. This work offers an approach to\nendow non-reasoning models with reasoning capabilities through supervised\nfine-tuning while avoiding the inherent overthinking problems inherited from\nteacher models, thereby enabling efficient reasoning in the fine-tuned models."}
{"id": "2504.09004", "pdf": "https://arxiv.org/pdf/2504.09004.pdf", "abs": "https://arxiv.org/abs/2504.09004", "title": "Exploring Families' Use and Mediation of Generative AI: A Multi-User Perspective", "authors": ["Shirley Zhang", "Bengisu Cagiltay", "Jennica Li", "Dakota Sullivan", "Bilge Mutlu", "Heather Kirkorian", "Kassem Fawaz"], "categories": ["cs.HC"], "comment": null, "summary": "Applications of Generative AI (GenAI), such as ChatGPT, have gained\npopularity among the public due to their ease of access, use, and support of\neducational and creative activities. Despite these benefits, GenAI poses unique\nrisks for families, such as lacking sufficient safeguards tailored to protect\nchildren under 16 years of age and not offering parental control features. This\nstudy explores families' use and co-use of GenAI, the perceived risks and\nopportunities of ChatGPT, and how parents mediate their children's use of\nGenAI. Through semi-structured interviews with 12 families, we identified ways\nfamilies used and mediated GenAI and factors that influenced parents' GenAI\nmediation strategies. We contextualize our findings with a modified model of\nfamily mediation strategies, drawing from previous family media and mediation\nframeworks. We provide insights for future research on family-GenAI\ninteractions and highlight the need for more robust protective measures on\nGenAI platforms for families."}
{"id": "2505.03473", "pdf": "https://arxiv.org/pdf/2505.03473.pdf", "abs": "https://arxiv.org/abs/2505.03473", "title": "Evaluation of LLMs on Long-tail Entity Linking in Historical Documents", "authors": ["Marta Boscariol", "Luana Bulla", "Lia Draetta", "Beatrice Fiumanò", "Emanuele Lenzi", "Leonardo Piano"], "categories": ["cs.CL"], "comment": null, "summary": "Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)\napplications, enabling the disambiguation of entity mentions by linking them to\ntheir corresponding entries in a reference knowledge base (KB). Thanks to their\ndeep contextual understanding capabilities, LLMs offer a new perspective to\ntackle EL, promising better results than traditional methods. Despite the\nimpressive generalization capabilities of LLMs, linking less popular, long-tail\nentities remains challenging as these entities are often underrepresented in\ntraining data and knowledge bases. Furthermore, the long-tail EL task is an\nunderstudied problem, and limited studies address it with LLMs. In the present\nwork, we assess the performance of two popular LLMs, GPT and LLama3, in a\nlong-tail entity linking scenario. Using MHERCL v0.1, a manually annotated\nbenchmark of sentences from domain-specific historical texts, we quantitatively\ncompare the performance of LLMs in identifying and linking entities to their\ncorresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity\nLinking and Relation Extraction framework. Our preliminary experiments reveal\nthat LLMs perform encouragingly well in long-tail EL, indicating that this\ntechnology can be a valuable adjunct in filling the gap between head and\nlong-tail EL."}
{"id": "2505.00821", "pdf": "https://arxiv.org/pdf/2505.00821.pdf", "abs": "https://arxiv.org/abs/2505.00821", "title": "Should AI Mimic People? Understanding AI-Supported Writing Technology Among Black Users", "authors": ["Jeffrey Basoah", "Jay L. Cunningham", "Erica Adams", "Alisha Bose", "Aditi Jain", "Kaustubh Yadav", "Zhengyang Yang", "Katharina Reinecke", "Daniela Rosner"], "categories": ["cs.HC"], "comment": "accepted to CSCW 2025", "summary": "AI-supported writing technologies (AISWT) that provide grammatical\nsuggestions, autocomplete sentences, or generate and rewrite text are now a\nregular feature integrated into many people's workflows. However, little is\nknown about how people perceive the suggestions these tools provide. In this\npaper, we investigate how Black American users perceive AISWT, motivated by\nprior findings in natural language processing that highlight how the underlying\nlarge language models can contain racial biases. Using interviews and\nobservational user studies with 13 Black American users of AISWT, we found a\nstrong tradeoff between the perceived benefits of using AISWT to enhance their\nwriting style and feeling like \"it wasn't built for us\". Specifically,\nparticipants reported AISWT's failure to recognize commonly used names and\nexpressions in African American Vernacular English, experiencing its\ncorrections as hurtful and alienating and fearing it might further minoritize\ntheir culture. We end with a reflection on the tension between AISWT that fail\nto include Black American culture and language, and AISWT that attempt to mimic\nit, with attention to accuracy, authenticity, and the production of social\ndifference."}
{"id": "2505.03481", "pdf": "https://arxiv.org/pdf/2505.03481.pdf", "abs": "https://arxiv.org/abs/2505.03481", "title": "Sentence Embeddings as an intermediate target in end-to-end summarisation", "authors": ["Maciej Zembrzuski", "Saad Mahamood"], "categories": ["cs.CL"], "comment": "10 pages, 1 figure, Year: 2019", "summary": "Current neural network-based methods to the problem of document summarisation\nstruggle when applied to datasets containing large inputs. In this paper we\npropose a new approach to the challenge of content-selection when dealing with\nend-to-end summarisation of user reviews of accommodations. We show that by\ncombining an extractive approach with externally pre-trained sentence level\nembeddings in an addition to an abstractive summarisation model we can\noutperform existing methods when this is applied to the task of summarising a\nlarge input dataset. We also prove that predicting sentence level embedding of\na summary increases the quality of an end-to-end system for loosely aligned\nsource to target corpora, than compared to commonly predicting probability\ndistributions of sentence selection."}
{"id": "2411.02179", "pdf": "https://arxiv.org/pdf/2411.02179.pdf", "abs": "https://arxiv.org/abs/2411.02179", "title": "CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality", "authors": ["Yiqin Zhao", "Mallesham Dasari", "Tian Guo"], "categories": ["cs.CV", "cs.GR", "cs.HC"], "comment": null, "summary": "High-quality environment lighting is essential for creating immersive mobile\naugmented reality (AR) experiences. However, achieving visually coherent\nestimation for mobile AR is challenging due to several key limitations in AR\ndevice sensing capabilities, including low camera FoV and limited pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address two key limitations of\ncontent quality and slow inference. In this work, we design and implement a\ngenerative lighting estimation system called CleAR that can produce\nhigh-quality, diverse environment maps in the format of 360{\\deg} HDR images.\nSpecifically, we design a two-step generation pipeline guided by AR environment\ncontext data to ensure the output aligns with the physical environment's visual\ncontext and color appearance. To improve the estimation robustness under\ndifferent lighting conditions, we design a real-time refinement component to\nadjust lighting estimation results on AR devices. Through a combination of\nquantitative and qualitative evaluations, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy,\nlatency, and robustness, and is rated by 31 participants as producing better\nrenderings for most virtual objects. For example, CleAR achieves 51% to 56%\naccuracy improvement on virtual object renderings across objects of three\ndistinctive types of materials and reflective properties. CleAR produces\nlighting estimates of comparable or better quality in just 3.2 seconds -- over\n110X faster than state-of-the-art methods."}
{"id": "2505.03531", "pdf": "https://arxiv.org/pdf/2505.03531.pdf", "abs": "https://arxiv.org/abs/2505.03531", "title": "Faster MoE LLM Inference for Extremely Large Models", "authors": ["Haoqi Yang", "Luohe Shi", "Qiwei Li", "Zuchao Li", "Ping Wang", "Bo Du", "Mengjia Shen", "Hai Zhao"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\nbecoming the mainstream approach for ultra-large-scale models. Existing\noptimization efforts for MoE models have focused primarily on coarse-grained\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\nmodels are gaining popularity, yet research on them remains limited. Therefore,\nwe want to discuss the efficiency dynamic under different service loads.\nAdditionally, fine-grained models allow deployers to reduce the number of\nrouted experts, both activated counts and total counts, raising the question of\nhow this reduction affects the trade-off between MoE efficiency and\nperformance. Our findings indicate that while deploying MoE models presents\ngreater challenges, it also offers significant optimization opportunities.\nReducing the number of activated experts can lead to substantial efficiency\nimprovements in certain scenarios, with only minor performance degradation.\nReducing the total number of experts provides limited efficiency gains but\nresults in severe performance degradation. Our method can increase throughput\nby at least 10\\% without any performance degradation. Overall, we conclude that\nMoE inference optimization remains an area with substantial potential for\nexploration and improvement."}
{"id": "2412.00814", "pdf": "https://arxiv.org/pdf/2412.00814.pdf", "abs": "https://arxiv.org/abs/2412.00814", "title": "VR-Doh: Hands-on 3D Modeling in Virtual Reality", "authors": ["Zhaofeng Luo", "Zhitong Cui", "Shijian Luo", "Mengyu Chu", "Minchen Li"], "categories": ["cs.GR", "cs.HC"], "comment": "12 pages", "summary": "We introduce VR-Doh, an open-source, hands-on 3D modeling system that enables\nintuitive creation and manipulation of elastoplastic objects in Virtual Reality\n(VR). By customizing the Material Point Method (MPM) for real-time simulation\nof hand-induced large deformations and enhancing 3D Gaussian Splatting for\nseamless rendering, VR-Doh provides an interactive and immersive 3D modeling\nexperience. Users can naturally sculpt, deform, and edit objects through both\ncontact- and gesture-based hand-object interactions. To achieve real-time\nperformance, our system incorporates localized simulation techniques,\nparticle-level collision handling, and the decoupling of physical and\nappearance representations, ensuring smooth and responsive interactions. VR-Doh\nsupports both object creation and editing, enabling diverse modeling tasks such\nas designing food items, characters, and interlocking structures, all resulting\nin simulation-ready assets. User studies with both novice and experienced\nparticipants highlight the system's intuitive design, immersive feedback, and\ncreative potential. Compared to existing geometric modeling tools, VR-Doh\noffers enhanced accessibility and natural interaction, making it a powerful\ntool for creative exploration in VR."}
{"id": "2505.03563", "pdf": "https://arxiv.org/pdf/2505.03563.pdf", "abs": "https://arxiv.org/abs/2505.03563", "title": "Say It Another Way: A Framework for User-Grounded Paraphrasing", "authors": ["Cléa Chataigner", "Rebecca Ma", "Prakhar Ganesh", "Afaf Taïk", "Elliot Creager", "Golnoosh Farnadi"], "categories": ["cs.CL"], "comment": null, "summary": "Small changes in how a prompt is worded can lead to meaningful differences in\nthe behavior of large language models (LLMs), raising concerns about the\nstability and reliability of their evaluations. While prior work has explored\nsimple formatting changes, these rarely capture the kinds of natural variation\nseen in real-world language use. We propose a controlled paraphrasing framework\nbased on a taxonomy of minimal linguistic transformations to systematically\ngenerate natural prompt variations. Using the BBQ dataset, we validate our\nmethod with both human annotations and automated checks, then use it to study\nhow LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our\nanalysis shows that even subtle prompt modifications can lead to substantial\nchanges in model behavior. These results highlight the need for robust,\nparaphrase-aware evaluation protocols."}
{"id": "2501.14917", "pdf": "https://arxiv.org/pdf/2501.14917.pdf", "abs": "https://arxiv.org/abs/2501.14917", "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach", "authors": ["Sara Abdali", "Can Goksen", "Saeed Amizadeh", "Julie E. Maybee", "Kazuhito Koishida"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance."}
{"id": "2505.03675", "pdf": "https://arxiv.org/pdf/2505.03675.pdf", "abs": "https://arxiv.org/abs/2505.03675", "title": "Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula G Allen-Meares", "Eulalia P Abril", "Olga Garcia-Bedoya", "Carolyn A Dickens", "Andrew D. Boyd"], "categories": ["cs.CL"], "comment": null, "summary": "We explore the potential of ChatGPT (3.5-turbo and 4) to generate\nconversations focused on self-care strategies for African-American heart\nfailure patients -- a domain with limited specialized datasets. To simulate\npatient-health educator dialogues, we employed four prompting strategies:\ndomain, African American Vernacular English (AAVE), Social Determinants of\nHealth (SDOH), and SDOH-informed reasoning. Conversations were generated across\nkey self-care domains of food, exercise, and fluid intake, with varying turn\nlengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as\nage, gender, neighborhood, and socioeconomic status. Our findings show that\neffective prompt design is essential. While incorporating SDOH and reasoning\nimproves dialogue quality, ChatGPT still lacks the empathy and engagement\nneeded for meaningful healthcare communication."}
{"id": "2503.10707", "pdf": "https://arxiv.org/pdf/2503.10707.pdf", "abs": "https://arxiv.org/abs/2503.10707", "title": "CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models", "authors": ["Zhiyuan Wang", "Katharine E. Daniel", "Laura E. Barnes", "Philip I. Chow"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries provide a promising method for tracking emotional\nstates, improving self-awareness, and promoting well-being outcome. This paper\naims to, through mobile diaries, understand cancer survivors' emotional states\nand key variables related to just-in-time intervention opportunities, including\nthe desire to regulate emotions and the availability to engage in\ninterventions. Although emotion analysis tools show potential for recognizing\nemotions from text, current methods lack the contextual understanding necessary\nto interpret brief mobile diary narratives. Our analysis of diary entries from\ncancer survivors (N=407) reveals systematic relationships between described\ncontexts and emotional states, with administrative and health-related contexts\nassociated with negative affect and regulation needs, while leisure activities\npromote positive emotions. We propose CALLM, a Context-Aware framework\nleveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) to analyze these brief entries by integrating retrieved peer experiences\nand personal diary history. CALLM demonstrates strong performance with balanced\naccuracies reaching 72.96% for positive affect, 73.29% for negative affect,\n73.72% for emotion regulation desire, and 60.09% for intervention availability,\noutperforming language model baselines. Post-hoc analysis reveals that model\nconfidence strongly predicts accuracy, with longer diary entries generally\nenhancing performance, and brief personalization periods yielding meaningful\nimprovements. Our findings demonstrate how contextual information in mobile\ndiaries can be effectively leveraged to understand emotional experiences,\npredict key states, and identify optimal intervention moments for personalized\njust-in-time support."}
{"id": "2505.03688", "pdf": "https://arxiv.org/pdf/2505.03688.pdf", "abs": "https://arxiv.org/abs/2505.03688", "title": "IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages", "authors": ["Sharvi Endait", "Ruturaj Ghatage", "Aditya Kulkarni", "Rajlaxmi Patil", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid progress in question-answering (QA) systems has predominantly\nbenefited high-resource languages, leaving Indic languages largely\nunderrepresented despite their vast native speaker base. In this paper, we\npresent IndicSQuAD, a comprehensive multi-lingual extractive QA dataset\ncovering nine major Indic languages, systematically derived from the SQuAD\ndataset. Building on previous work with MahaSQuAD for Marathi, our approach\nadapts and extends translation techniques to maintain high linguistic fidelity\nand accurate answer-span alignment across diverse languages. IndicSQuAD\ncomprises extensive training, validation, and test sets for each language,\nproviding a robust foundation for model development. We evaluate baseline\nperformances using language-specific monolingual BERT models and the\nmultilingual MuRIL-BERT. The results indicate some challenges inherent in\nlow-resource settings. Moreover, our experiments suggest potential directions\nfor future work, including expanding to additional languages, developing\ndomain-specific datasets, and incorporating multimodal data. The dataset and\nmodels are publicly shared at https://github.com/l3cube-pune/indic-nlp"}
{"id": "2505.03711", "pdf": "https://arxiv.org/pdf/2505.03711.pdf", "abs": "https://arxiv.org/abs/2505.03711", "title": "NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation", "authors": ["Baharul Islam", "Nasim Ahmad", "Ferdous Ahmed Barbhuiya", "Kuntal Dey"], "categories": ["cs.CL"], "comment": null, "summary": "We present our system submission for SemEval 2025 Task 5, which focuses on\ncross-lingual subject classification in the English and German academic\ndomains. Our approach leverages bilingual data during training, employing\nnegative sampling and a margin-based retrieval objective. We demonstrate that a\ndimension-as-token self-attention mechanism designed with significantly reduced\ninternal dimensions can effectively encode sentence embeddings for subject\nretrieval. In quantitative evaluation, our system achieved an average recall\nrate of 32.24% in the general quantitative setting (all subjects), 43.16% and\n31.53% of the general qualitative evaluation methods with minimal GPU usage,\nhighlighting their competitive performance. Our results demonstrate that our\napproach is effective in capturing relevant subject information under resource\nconstraints, although there is still room for improvement."}
{"id": "2505.03733", "pdf": "https://arxiv.org/pdf/2505.03733.pdf", "abs": "https://arxiv.org/abs/2505.03733", "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch", "authors": ["Zimu Lu", "Yunqiao Yang", "Houxing Ren", "Haotian Hou", "Han Xiao", "Ke Wang", "Weikang Shi", "Aojun Zhou", "Mingjie Zhan", "Hongsheng Li"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model."}
{"id": "2505.03739", "pdf": "https://arxiv.org/pdf/2505.03739.pdf", "abs": "https://arxiv.org/abs/2505.03739", "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model", "authors": ["Zuwei Long", "Yunhang Shen", "Chaoyou Fu", "Heting Gao", "Lijiang Li", "Peixian Chen", "Mengdan Zhang", "Hang Shao", "Jian Li", "Jinlong Peng", "Haoyu Cao", "Ke Li", "Rongrong Ji", "Xing Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio", "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks."}
{"id": "2505.02848", "pdf": "https://arxiv.org/pdf/2505.02848.pdf", "abs": "https://arxiv.org/abs/2505.02848", "title": "Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration", "authors": ["Kexin Ding", "Mu Zhou", "Akshay Chaudhari", "Shaoting Zhang", "Dimitris N. Metaxas"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "The wide exploration of large language models (LLMs) raises the awareness of\nalignment between healthcare stakeholder preferences and model outputs. This\nalignment becomes a crucial foundation to empower the healthcare workflow\neffectively, safely, and responsibly. Yet the varying behaviors of LLMs may not\nalways match with healthcare stakeholders' knowledge, demands, and values. To\nenable a human-AI alignment, healthcare stakeholders will need to perform\nessential roles in guiding and enhancing the performance of LLMs. Human\nprofessionals must participate in the entire life cycle of adopting LLM in\nhealthcare, including training data curation, model training, and inference. In\nthis review, we discuss the approaches, tools, and applications of alignments\nbetween healthcare stakeholders and LLMs. We demonstrate that LLMs can better\nfollow human values by properly enhancing healthcare knowledge integration,\ntask understanding, and human guidance. We provide outlooks on enhancing the\nalignment between humans and LLMs to build trustworthy real-world healthcare\napplications."}
{"id": "2505.02888", "pdf": "https://arxiv.org/pdf/2505.02888.pdf", "abs": "https://arxiv.org/abs/2505.02888", "title": "When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger", "authors": ["Rintaro Ando"], "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05, 68Q85", "I.2.0; I.2.3; I.2.6"], "comment": "20 pages, 4 figures, 3 tables. Code:\n  github.com/rintaro-ando-tech/n2m-rsi-demo (v1.0)", "summary": "We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal\nformal model showing that once an AI agent feeds its own outputs back as inputs\nand crosses an explicit information-integration threshold, its internal\ncomplexity will grow without bound under our assumptions. The framework unifies\nearlier ideas on self-prompting large language models, G\\\"odelian\nself-reference, and AutoML, yet remains implementation-agnostic. The model\nfurthermore scales naturally to interacting swarms of agents, hinting at\nsuper-linear effects once communication among instances is permitted. For\nsafety reasons, we omit system-specific implementation details and release only\na brief, model-agnostic toy prototype in Appendix C."}
{"id": "2505.02931", "pdf": "https://arxiv.org/pdf/2505.02931.pdf", "abs": "https://arxiv.org/abs/2505.02931", "title": "The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models", "authors": ["Fernando Vallecillos Ruiz", "Max Hort", "Leon Moonen"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in the research track of the 29th\n  International Conference on Evaluation and Assessment in Software Engineering\n  (EASE), 17-20 June 2025, Istanbul, T\\\"urkiye", "summary": "Automatic program repair (APR) aims to reduce the manual efforts required to\nidentify and fix errors in source code. Before the rise of LLM-based agents, a\ncommon strategy was to increase the number of generated patches, sometimes to\nthe thousands, to achieve better repair results on benchmarks. More recently,\nself-iterative capabilities enabled LLMs to refine patches over multiple rounds\nguided by feedback. However, literature often focuses on many iterations and\ndisregards different numbers of outputs.\n  We investigate an APR pipeline that balances these two approaches, the\ngeneration of multiple outputs and multiple rounds of iteration, while imposing\na limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs\n- DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR\ntask. We further fine-tune each model on an APR dataset with three sizes (1K,\n30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess\ntheir repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.\n  Our results show that by using only a fraction (<1%) of the fine-tuning\ndataset, we can achieve improvements of up to 78% in the number of plausible\npatches generated, challenging prior studies that reported limited gains using\nFull Fine-Tuning. However, we find that exceeding certain thresholds leads to\ndiminishing outcomes, likely due to overfitting. Moreover, we show that base\nmodels greatly benefit from creating patches in an iterative fashion rather\nthan generating them all at once. In addition, the benefit of iterative\nstrategies becomes more pronounced in complex benchmarks. Even fine-tuned\nmodels, while benefiting less from iterations, still gain advantages,\nparticularly on complex benchmarks. The research underscores the need for\nbalanced APR strategies that combine multi-output generation and iterative\nrefinement."}
{"id": "2505.02952", "pdf": "https://arxiv.org/pdf/2505.02952.pdf", "abs": "https://arxiv.org/abs/2505.02952", "title": "Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach", "authors": ["Fabrizio Marozzo"], "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR", "cs.LG"], "comment": null, "summary": "Generative AI systems have revolutionized human interaction by enabling\nnatural language-based coding and problem solving. However, the inherent\nambiguity of natural language often leads to imprecise instructions, forcing\nusers to iteratively test, correct, and resubmit their prompts. We propose an\niterative approach that systematically narrows down these ambiguities through a\nstructured series of clarification questions and alternative solution\nproposals, illustrated with input/output examples as well. Once every\nuncertainty is resolved, a final, precise solution is generated. Evaluated on a\ndiverse dataset spanning coding, data analysis, and creative writing, our\nmethod demonstrates superior accuracy, competitive resolution times, and higher\nuser satisfaction compared to conventional one-shot solutions, which typically\nrequire multiple manual iterations to achieve a correct output."}
{"id": "2505.03031", "pdf": "https://arxiv.org/pdf/2505.03031.pdf", "abs": "https://arxiv.org/abs/2505.03031", "title": "Radio: Rate-Distortion Optimization for Large Language Model Compression", "authors": ["Sean I. Young"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "In recent years, the compression of large language models (LLMs) has emerged\nas a key problem in facilitating LLM deployment on resource-limited devices,\nreducing compute costs, and mitigating the environmental footprint due to\nlarge-scale AI infrastructure. Here, we establish the foundations of LLM\nquantization from a rate-distortion theory perspective and propose a\nquantization technique based on simple rate-distortion optimization. Our\ntechnique scales to models containing hundreds of billions of weight parameters\nand offers users the flexibility to compress models, post-training, to a model\nsize or accuracy specified by the user."}
{"id": "2505.03054", "pdf": "https://arxiv.org/pdf/2505.03054.pdf", "abs": "https://arxiv.org/abs/2505.03054", "title": "BLAB: Brutally Long Audio Bench", "authors": ["Orevaoghene Ahia", "Martijn Bartelds", "Kabir Ahuja", "Hila Gonen", "Valentin Hofmann", "Siddhant Arora", "Shuyue Stella Li", "Vishal Puttagunta", "Mofetoluwa Adeyemi", "Charishma Buchireddy", "Ben Walls", "Noah Bennett", "Shinji Watanabe", "Noah A. Smith", "Yulia Tsvetkov", "Sachin Kumar"], "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Developing large audio language models (LMs) capable of understanding diverse\nspoken interactions is essential for accommodating the multimodal nature of\nhuman communication and can increase the accessibility of language technologies\nacross different user populations. Recent work on audio LMs has primarily\nevaluated their performance on short audio segments, typically under 30\nseconds, with limited exploration of long-form conversational speech segments\nthat more closely reflect natural user interactions with these models. We\nintroduce Brutally Long Audio Bench (BLAB), a challenging long-form audio\nbenchmark that evaluates audio LMs on localization, duration estimation,\nemotion, and counting tasks using audio segments averaging 51 minutes in\nlength. BLAB consists of 833+ hours of diverse, full-length audio clips, each\npaired with human-annotated, text-based natural language questions and answers.\nOur audio data were collected from permissively licensed sources and underwent\na human-assisted filtering process to ensure task compliance. We evaluate six\nopen-source and proprietary audio LMs on BLAB and find that all of them,\nincluding advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the\ntasks in BLAB. Our comprehensive analysis reveals key insights into the\ntrade-offs between task difficulty and audio duration. In general, we find that\naudio LMs struggle with long-form speech, with performance declining as\nduration increases. They perform poorly on localization, temporal reasoning,\ncounting, and struggle to understand non-phonemic information, relying more on\nprompts than audio content. BLAB serves as a challenging evaluation framework\nto develop audio LMs with robust long-form audio understanding capabilities."}
{"id": "2505.03273", "pdf": "https://arxiv.org/pdf/2505.03273.pdf", "abs": "https://arxiv.org/abs/2505.03273", "title": "SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation", "authors": ["Zhaoxi Mu", "Xinyu Yang", "Gang Wang"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Appears in IJCAI 2025", "summary": "While contemporary speech separation technologies adeptly process lengthy\nmixed audio waveforms, they are frequently challenged by the intricacies of\nreal-world environments, including noisy and reverberant settings, which can\nresult in artifacts or distortions in the separated speech. To overcome these\nlimitations, we introduce SepALM, a pioneering approach that employs audio\nlanguage models (ALMs) to rectify and re-synthesize speech within the text\ndomain following preliminary separation. SepALM comprises four core components:\na separator, a corrector, a synthesizer, and an aligner. By integrating an\nALM-based end-to-end error correction mechanism, we mitigate the risk of error\naccumulation and circumvent the optimization hurdles typically encountered in\nconventional methods that amalgamate automatic speech recognition (ASR) with\nlarge language models (LLMs). Additionally, we have developed Chain-of-Thought\n(CoT) prompting and knowledge distillation techniques to facilitate the\nreasoning and training processes of the ALM. Our experiments substantiate that\nSepALM not only elevates the precision of speech separation but also markedly\nbolsters adaptability in novel acoustic environments."}
{"id": "2505.03335", "pdf": "https://arxiv.org/pdf/2505.03335.pdf", "abs": "https://arxiv.org/abs/2505.03335", "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data", "authors": ["Andrew Zhao", "Yiran Wu", "Yang Yue", "Tong Wu", "Quentin Xu", "Yang Yue", "Matthieu Lin", "Shenzhi Wang", "Qingyun Wu", "Zilong Zheng", "Gao Huang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes."}
{"id": "2505.03414", "pdf": "https://arxiv.org/pdf/2505.03414.pdf", "abs": "https://arxiv.org/abs/2505.03414", "title": "Enhancing Target-unspecific Tasks through a Features Matrix", "authors": ["Fangming Cui", "Yonggang Zhang", "Xuan Wang", "Xinmei Tian", "Jun Yu"], "categories": ["cs.CV", "cs.CL"], "comment": "ICML 2025", "summary": "Recent developments in prompt learning of large vision-language models have\nsignificantly improved performance in target-specific tasks. However, these\nprompt optimizing methods often struggle to tackle the target-unspecific or\ngeneralizable tasks effectively. It may be attributed to the fact that\noverfitting training causes the model to forget its general knowledge having\nstrong promotion on target-unspecific tasks. To alleviate this issue, we\npropose a novel Features Matrix (FM) regularization approach designed to\nenhance these models on target-unspecific tasks. Our method extracts and\nleverages general knowledge, shaping a Features Matrix (FM). Specifically, the\nFM captures the semantics of diverse inputs from a deep and fine perspective,\npreserving essential general knowledge, which mitigates the risk of\noverfitting. Representative evaluations demonstrate that: 1) the FM is\ncompatible with existing frameworks as a generic and flexible module, and 2)\nthe FM significantly showcases its effectiveness in enhancing target-unspecific\ntasks, achieving state-of-the-art performance."}
{"id": "2505.03443", "pdf": "https://arxiv.org/pdf/2505.03443.pdf", "abs": "https://arxiv.org/abs/2505.03443", "title": "Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories", "authors": ["Valerio Bellandi"], "categories": ["cs.DC", "cs.AI", "cs.CL"], "comment": "This paper has been accepted at the 6th International Conference on\n  Recent Trends and Applications in Computer Science. It will appear in the\n  proceedings", "summary": "Centralized and distributed systems are two main approaches to organizing ICT\ninfrastructure, each with its pros and cons. Centralized systems concentrate\nresources in one location, making management easier but creating single points\nof failure. Distributed systems, on the other hand, spread resources across\nmultiple nodes, offering better scalability and fault tolerance, but requiring\nmore complex management. The choice between them depends on factors like\napplication needs, scalability, and data sensitivity. Centralized systems suit\napplications with limited scalability and centralized control, while\ndistributed systems excel in large-scale environments requiring high\navailability and performance. This paper explores a distributed document\nrepository system developed for the Italian Ministry of Justice, using edge\nrepositories to analyze textual data and metadata, enhancing semantic\nexploration capabilities."}
{"id": "2505.03501", "pdf": "https://arxiv.org/pdf/2505.03501.pdf", "abs": "https://arxiv.org/abs/2505.03501", "title": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models", "authors": ["Zihan Wang", "Hongwei Li", "Rui Zhang", "Wenbo Jiang", "Kangjie Chen", "Tianwei Zhang", "Qingchuan Zhao", "Guowen Xu"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "In this paper, we present a new form of backdoor attack against Large\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\nlingual-backdoor attacks is that the language itself serves as the trigger to\nhijack the infected LLMs to generate inflammatory speech. They enable the\nprecise targeting of a specific language-speaking group, exacerbating racial\ndiscrimination by malicious entities. We first implement a baseline\nlingual-backdoor attack, which is carried out by poisoning a set of training\ndata for specific downstream tasks through translation into the trigger\nlanguage. However, this baseline attack suffers from poor task generalization\nand is impractical in real-world settings. To address this challenge, we design\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\ndownstream tasks within the chat LLMs, regardless of the specific questions of\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\nGradient-based Search (PGCG) based adversarial training to expand the decision\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\nlingual-backdoor across various tasks. We perform extensive experiments to\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\nmultilingual capabilities and is expected to promote future research on the\npotential defenses to enhance the LLMs' robustness"}
{"id": "2505.03676", "pdf": "https://arxiv.org/pdf/2505.03676.pdf", "abs": "https://arxiv.org/abs/2505.03676", "title": "Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval", "authors": ["Arthur Satouf", "Gabriel Ben Zenou", "Benjamin Piwowarski", "Habiboulaye Amadou Boubacar", "Pablo Piantanida"], "categories": ["cs.IR", "cs.CL", "68P20, 68T50", "H.3"], "comment": "6 pages - 2 figures - conference: accepted at SIGIR 2025", "summary": "Current sparse neural information retrieval (IR) methods, and to a lesser\nextent more traditional models such as BM25, do not take into account the\ndocument collection and the complex interplay between different term weights\nwhen representing a single document. In this paper, we show how the Rational\nSpeech Acts (RSA), a linguistics framework used to minimize the number of\nfeatures to be communicated when identifying an object in a set, can be adapted\nto the IR case -- and in particular to the high number of potential features\n(here, tokens). RSA dynamically modulates token-document interactions by\nconsidering the influence of other documents in the dataset, better contrasting\ndocument representations. Experiments show that incorporating RSA consistently\nimproves multiple sparse retrieval models and achieves state-of-the-art\nperformance on out-of-domain datasets from the BEIR benchmark.\nhttps://github.com/arthur-75/Rational-Retrieval-Acts"}
{"id": "2401.16646", "pdf": "https://arxiv.org/pdf/2401.16646.pdf", "abs": "https://arxiv.org/abs/2401.16646", "title": "Incoherent Probability Judgments in Large Language Models", "authors": ["Jian-Qiao Zhu", "Thomas L. Griffiths"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autoregressive Large Language Models (LLMs) trained for next-word prediction\nhave demonstrated remarkable proficiency at producing coherent text. But are\nthey equally adept at forming coherent probability judgments? We use\nprobabilistic identities and repeated judgments to assess the coherence of\nprobability judgments made by LLMs. Our results show that the judgments\nproduced by these models are often incoherent, displaying human-like systematic\ndeviations from the rules of probability theory. Moreover, when prompted to\njudge the same event, the mean-variance relationship of probability judgments\nproduced by LLMs shows an inverted-U-shaped like that seen in humans. We\npropose that these deviations from rationality can be explained by linking\nautoregressive LLMs to implicit Bayesian inference and drawing parallels with\nthe Bayesian Sampler model of human probability judgments."}
{"id": "2406.14498", "pdf": "https://arxiv.org/pdf/2406.14498.pdf", "abs": "https://arxiv.org/abs/2406.14498", "title": "LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and Smartphone Sensors", "authors": ["Sheikh Asif Imran", "Mohammad Nur Hossain Khan", "Subrata Biswas", "Bashima Islam"], "categories": ["cs.CL"], "comment": null, "summary": "Wearables generate rich motion data, yet current systems only classify what\nhappened - failing to support natural questions about why it happened or what\nit means. We introduce LLaSA (Large Language and Sensor Assistant), a compact\n13B model that enables ask-anything, open-ended question answering grounded in\nraw IMU data. LLaSA supports conversational, context-aware reasoning -\nexplaining the causes of sensor-detected behaviors and answering free-form\nquestions in real-world scenarios. It is tuned for scientific accuracy,\ncoherence, and response reliability. To advance this new task of sensor-based\nQA, we release three large-scale datasets: SensorCaps, OpenSQA, and\nTune-OpenSQA. Together, these resources define a new benchmark for\nsensor-language models. LLaSA consistently produces interpretable, causal\nanswers and outperforms commercial LLMs across both public and real-world\nsettings. Our code repository and datasets can be found at\nhttps://github.com/BASHLab/LLaSA."}
{"id": "2408.01122", "pdf": "https://arxiv.org/pdf/2408.01122.pdf", "abs": "https://arxiv.org/abs/2408.01122", "title": "CFBench: A Comprehensive Constraints-Following Benchmark for LLMs", "authors": ["Tao Zhang", "Chenglin Zhu", "Yanjun Shen", "Wenjing Luo", "Yan Zhang", "Hao Liang", "Tao Zhang", "Fan Yang", "Mingan Lin", "Yujing Qiao", "Weipeng Chen", "Bin Cui", "Wentao Zhang", "Zenan Zhou"], "categories": ["cs.CL"], "comment": "15 pages, 10 figures", "summary": "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench"}
{"id": "2408.14307", "pdf": "https://arxiv.org/pdf/2408.14307.pdf", "abs": "https://arxiv.org/abs/2408.14307", "title": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing", "authors": ["Yayati Jadhav", "Peter Pak", "Amir Barati Farimani"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention."}
{"id": "2410.00153", "pdf": "https://arxiv.org/pdf/2410.00153.pdf", "abs": "https://arxiv.org/abs/2410.00153", "title": "Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution", "authors": ["Haiyan Zhao", "Heng Zhao", "Bo Shen", "Ali Payani", "Fan Yang", "Mengnan Du"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Probing learned concepts in large language models (LLMs) is crucial for\nunderstanding how semantic knowledge is encoded internally. Training linear\nclassifiers on probing tasks is a principle approach to denote the vector of a\ncertain concept in the representation space. However, the single vector\nidentified for a concept varies with both data and training, making it less\nrobust and weakening its effectiveness in real-world applications. To address\nthis challenge, we propose an approach to approximate the subspace representing\na specific concept. Built on linear probing classifiers, we extend the concept\nvectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's\neffectiveness through measuring its faithfulness and plausibility across\nmultiple LLMs with different sizes and architectures. Additionally, we use\nrepresentation intervention tasks to showcase its efficacy in real-world\napplications such as emotion steering. Experimental results indicate that GCS\nconcept vectors have the potential to balance steering performance and\nmaintaining the fluency in natural language generation tasks."}
{"id": "2410.09580", "pdf": "https://arxiv.org/pdf/2410.09580.pdf", "abs": "https://arxiv.org/abs/2410.09580", "title": "SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search", "authors": ["Hanwen Du", "Bo Peng", "Xia Ning"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Main Conference", "summary": "Conversational Recommender Systems (CRS) proactively engage users in\ninteractive dialogues to elicit user preferences and provide personalized\nrecommendations. Existing methods train Reinforcement Learning (RL)-based agent\nwith greedy action selection or sampling strategy, and may suffer from\nsuboptimal conversational planning. To address this, we present a novel Monte\nCarlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a\nconversational agent (S-agent) and a conversational planner (S-planner).\nS-planner builds a conversational search tree with MCTS based on the initial\nactions proposed by S-agent to find conversation plans. The best conversation\nplans from S-planner are used to guide the training of S-agent, creating a\nself-training loop where S-agent can iteratively improve its capability for\nconversational planning. Furthermore, we propose an efficient variant SAPIENT-e\nfor trade-off between training efficiency and performance. Extensive\nexperiments on four benchmark datasets validate the effectiveness of our\napproach, showing that SAPIENT outperforms the state-of-the-art baselines."}
{"id": "2411.00027", "pdf": "https://arxiv.org/pdf/2411.00027.pdf", "abs": "https://arxiv.org/abs/2411.00027", "title": "Personalization of Large Language Models: A Survey", "authors": ["Zhehao Zhang", "Ryan A. Rossi", "Branislav Kveton", "Yijia Shao", "Diyi Yang", "Hamed Zamani", "Franck Dernoncourt", "Joe Barrow", "Tong Yu", "Sungchul Kim", "Ruiyi Zhang", "Jiuxiang Gu", "Tyler Derr", "Hongjie Chen", "Junda Wu", "Xiang Chen", "Zichao Wang", "Subrata Mitra", "Nedim Lipka", "Nesreen Ahmed", "Yu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Personalization of Large Language Models (LLMs) has recently become\nincreasingly important with a wide range of applications. Despite the\nimportance and recent progress, most existing works on personalized LLMs have\nfocused either entirely on (a) personalized text generation or (b) leveraging\nLLMs for personalization-related downstream applications, such as\nrecommendation systems. In this work, we bridge the gap between these two\nseparate main directions for the first time by introducing a taxonomy for\npersonalized LLM usage and summarizing the key differences and challenges. We\nprovide a formalization of the foundations of personalized LLMs that\nconsolidates and expands notions of personalization of LLMs, defining and\ndiscussing novel facets of personalization, usage, and desiderata of\npersonalized LLMs. We then unify the literature across these diverse fields and\nusage scenarios by proposing systematic taxonomies for the granularity of\npersonalization, personalization techniques, datasets, evaluation methods, and\napplications of personalized LLMs. Finally, we highlight challenges and\nimportant open problems that remain to be addressed. By unifying and surveying\nrecent research using the proposed taxonomies, we aim to provide a clear guide\nto the existing literature and different facets of personalization in LLMs,\nempowering both researchers and practitioners."}
{"id": "2412.18135", "pdf": "https://arxiv.org/pdf/2412.18135.pdf", "abs": "https://arxiv.org/abs/2412.18135", "title": "LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment", "authors": ["Binrui Zeng", "Bin Ji", "Xiaodong Liu", "Jie Yu", "Shasha Li", "Jun Ma", "Xiaopeng Li", "Shangwen Wang", "Xinran Hong", "Yongtao Tang"], "categories": ["cs.CL"], "comment": "8 pages, 4 figures, accepted to IJCNN 2025", "summary": "As Large Language Models (LLMs) demonstrate exceptional performance across\nvarious domains, deploying LLMs on edge devices has emerged as a new trend.\nQuantization techniques, which reduce the size and memory requirements of LLMs,\nare effective for deploying LLMs on resource-limited edge devices. However,\nexisting one-size-fits-all quantization methods often fail to dynamically\nadjust the memory requirements of LLMs, limiting their applications to\npractical edge devices with various computation resources. To tackle this\nissue, we propose Layer-Specific Adaptive Quantization (LSAQ), a system for\nadaptive quantization and dynamic deployment of LLMs based on layer importance.\nSpecifically, LSAQ evaluates the importance of LLMs' neural layers by\nconstructing top-k token sets from the inputs and outputs of each layer and\ncalculating their Jaccard similarity. Based on layer importance, our system\nadaptively adjusts quantization strategies in real time according to the\ncomputation resource of edge devices, which applies higher quantization\nprecision to layers with higher importance, and vice versa. {Experimental\nresults show that LSAQ consistently outperforms the selected quantization\nbaselines in terms of perplexity and zero-shot tasks. Additionally, it can\ndevise appropriate quantization schemes for different usage scenarios to\nfacilitate the deployment of LLMs."}
{"id": "2501.14917", "pdf": "https://arxiv.org/pdf/2501.14917.pdf", "abs": "https://arxiv.org/abs/2501.14917", "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach", "authors": ["Sara Abdali", "Can Goksen", "Saeed Amizadeh", "Julie E. Maybee", "Kazuhito Koishida"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance."}
{"id": "2501.15858", "pdf": "https://arxiv.org/pdf/2501.15858.pdf", "abs": "https://arxiv.org/abs/2501.15858", "title": "Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech", "authors": ["Eunjung Yeo", "Julie Liss", "Visar Berisha", "David Mortensen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "15 pages, 2 figure, 2 tables", "summary": "Purpose: Speech intelligibility is a critical outcome in the assessment and\nmanagement of dysarthria, yet most research and clinical practices have focused\non English, limiting their applicability across languages. This commentary\nintroduces a conceptual framework--and a demonstration of how it can be\nimplemented--leveraging artificial intelligence (AI) to advance cross-language\nintelligibility assessment of dysarthric speech. Method: We propose a\ntwo-tiered conceptual framework consisting of a universal speech model that\nencodes dysarthric speech into acoustic-phonetic representations, followed by a\nlanguage-specific intelligibility assessment model that interprets these\nrepresentations within the phonological or prosodic structures of the target\nlanguage. We further identify barriers to cross-language intelligibility\nassessment of dysarthric speech, including data scarcity, annotation\ncomplexity, and limited linguistic insights into dysarthric speech, and outline\npotential AI-driven solutions to overcome these challenges. Conclusion:\nAdvancing cross-language intelligibility assessment of dysarthric speech\nnecessitates models that are both efficient and scalable, yet constrained by\nlinguistic rules to ensure accurate and language-sensitive assessment. Recent\nadvances in AI provide the foundational tools to support this integration,\nshaping future directions toward generalizable and linguistically informed\nassessment frameworks."}
{"id": "2502.00865", "pdf": "https://arxiv.org/pdf/2502.00865.pdf", "abs": "https://arxiv.org/abs/2502.00865", "title": "Predicting potentially abusive clauses in Chilean terms of services with natural language processing", "authors": ["Christoffer Loeffler", "Andrea Martínez Freile", "Tomás Rey Pizarro"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "39 pages, 2 figures, 8 tables, accepted for publication", "summary": "This study addresses the growing concern of information asymmetry in consumer\ncontracts, exacerbated by the proliferation of online services with complex\nTerms of Service that are rarely even read. Even though research on automatic\nanalysis methods is conducted, the problem is aggravated by the general focus\non English-language Machine Learning approaches and on major jurisdictions,\nsuch as the European Union. We introduce a new methodology and a substantial\ndataset addressing this gap. We propose a novel annotation scheme with four\ncategories and a total of 20 classes, and apply it on 50 online Terms of\nService used in Chile. Our evaluation of transformer-based models highlights\nhow factors like language- and/or domain-specific pre-training, few-shot sample\nsize, and model architecture affect the detection and classification of\npotentially abusive clauses. Results show a large variability in performance\nfor the different tasks and models, with the highest macro-F1 scores for the\ndetection task ranging from 79% to 89% and micro-F1 scores up to 96%, while\nmacro-F1 scores for the classification task range from 60% to 70% and micro-F1\nscores from 64% to 80%. Notably, this is the first Spanish-language multi-label\nclassification dataset for legal clauses, applying Chilean law and offering a\ncomprehensive evaluation of Spanish-language models in the legal domain. Our\nwork lays the ground for future research in method development for rarely\nconsidered legal analysis and potentially leads to practical applications to\nsupport consumers in Chile and Latin America as a whole."}
{"id": "2502.07963", "pdf": "https://arxiv.org/pdf/2502.07963.pdf", "abs": "https://arxiv.org/abs/2502.07963", "title": "Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?", "authors": ["Hye Sun Yun", "Karen Y. C. Zhang", "Ramez Kouzy", "Iain J. Marshall", "Junyi Jessy Li", "Byron C. Wallace"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 12 figures, 4 tables, CHIL 2025", "summary": "Medical research faces well-documented challenges in translating novel\ntreatments into clinical practice. Publishing incentives encourage researchers\nto present \"positive\" findings, even when empirical results are equivocal.\nConsequently, it is well-documented that authors often spin study results,\nespecially in article abstracts. Such spin can influence clinician\ninterpretation of evidence and may affect patient care decisions. In this\nstudy, we ask whether the interpretation of trial results offered by Large\nLanguage Models (LLMs) is similarly affected by spin. This is important since\nLLMs are increasingly being used to trawl through and synthesize published\nmedical evidence. We evaluated 22 LLMs and found that they are across the board\nmore susceptible to spin than humans. They might also propagate spin into their\noutputs: We find evidence, e.g., that LLMs implicitly incorporate spin into\nplain language summaries that they generate. We also find, however, that LLMs\nare generally capable of recognizing spin, and can be prompted in a way to\nmitigate spin's impact on LLM outputs."}
{"id": "2502.13685", "pdf": "https://arxiv.org/pdf/2502.13685.pdf", "abs": "https://arxiv.org/abs/2502.13685", "title": "MoM: Linear Sequence Modeling with Mixture-of-Memories", "authors": ["Jusen Du", "Weigao Sun", "Disen Lan", "Jiaxi Hu", "Yu Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical report, 16 pages", "summary": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE."}
{"id": "2502.14338", "pdf": "https://arxiv.org/pdf/2502.14338.pdf", "abs": "https://arxiv.org/abs/2502.14338", "title": "English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports", "authors": ["Avinash Patil", "Aryan Jadon"], "categories": ["cs.CL", "cs.SE"], "comment": "8 Pages, 4 Figures, 3 Tables", "summary": "Accurate translation of bug reports is critical for efficient collaboration\nin global software development. In this study, we conduct the first\ncomprehensive evaluation of machine translation (MT) performance on bug\nreports, analyzing the capabilities of DeepL, AWS Translate, and large language\nmodels such as ChatGPT, Claude, Gemini, LLaMA, and Mistral using data from the\nVisual Studio Code GitHub repository, specifically focusing on reports labeled\nwith the english-please tag. To assess both translation quality and source\nlanguage identification accuracy, we employ a range of MT evaluation\nmetrics-including BLEU, BERTScore, COMET, METEOR, and ROUGE-alongside\nclassification metrics such as accuracy, precision, recall, and F1-score. Our\nfindings reveal that while ChatGPT (gpt-4o) excels in semantic and lexical\ntranslation quality, it does not lead in source language identification. Claude\nand Mistral achieve the highest F1-scores (0.7182 and 0.7142, respectively),\nand Gemini records the best precision (0.7414). AWS Translate shows the highest\naccuracy (0.4717) in identifying source languages. These results highlight that\nno single system dominates across all tasks, reinforcing the importance of\ntask-specific evaluations. This study underscores the need for domain\nadaptation when translating technical content and provides actionable insights\nfor integrating MT into bug-triaging workflows. The code and dataset for this\npaper are available at GitHub-https://github.com/av9ash/English-Please"}
{"id": "2502.19187", "pdf": "https://arxiv.org/pdf/2502.19187.pdf", "abs": "https://arxiv.org/abs/2502.19187", "title": "BIG-Bench Extra Hard", "authors": ["Mehran Kazemi", "Bahare Fatemi", "Hritik Bansal", "John Palowitch", "Chrysovalantis Anastasiou", "Sanket Vaibhav Mehta", "Lalit K. Jain", "Virginia Aglietti", "Disha Jindal", "Peter Chen", "Nishanth Dikkala", "Gladys Tyen", "Xin Liu", "Uri Shalit", "Silvia Chiappa", "Kate Olszewska", "Yi Tay", "Vinh Q. Tran", "Quoc V. Le", "Orhan Firat"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh."}
{"id": "2503.10707", "pdf": "https://arxiv.org/pdf/2503.10707.pdf", "abs": "https://arxiv.org/abs/2503.10707", "title": "CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models", "authors": ["Zhiyuan Wang", "Katharine E. Daniel", "Laura E. Barnes", "Philip I. Chow"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries provide a promising method for tracking emotional\nstates, improving self-awareness, and promoting well-being outcome. This paper\naims to, through mobile diaries, understand cancer survivors' emotional states\nand key variables related to just-in-time intervention opportunities, including\nthe desire to regulate emotions and the availability to engage in\ninterventions. Although emotion analysis tools show potential for recognizing\nemotions from text, current methods lack the contextual understanding necessary\nto interpret brief mobile diary narratives. Our analysis of diary entries from\ncancer survivors (N=407) reveals systematic relationships between described\ncontexts and emotional states, with administrative and health-related contexts\nassociated with negative affect and regulation needs, while leisure activities\npromote positive emotions. We propose CALLM, a Context-Aware framework\nleveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) to analyze these brief entries by integrating retrieved peer experiences\nand personal diary history. CALLM demonstrates strong performance with balanced\naccuracies reaching 72.96% for positive affect, 73.29% for negative affect,\n73.72% for emotion regulation desire, and 60.09% for intervention availability,\noutperforming language model baselines. Post-hoc analysis reveals that model\nconfidence strongly predicts accuracy, with longer diary entries generally\nenhancing performance, and brief personalization periods yielding meaningful\nimprovements. Our findings demonstrate how contextual information in mobile\ndiaries can be effectively leveraged to understand emotional experiences,\npredict key states, and identify optimal intervention moments for personalized\njust-in-time support."}
{"id": "2503.13551", "pdf": "https://arxiv.org/pdf/2503.13551.pdf", "abs": "https://arxiv.org/abs/2503.13551", "title": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models", "authors": ["Teng Wang", "Zhangyi Jiang", "Zhenqi He", "Shenyang Tong", "Wenhan Yang", "Yanan Zheng", "Zeyu Li", "Zifan He", "Hailei Gong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate step.\nIn addition, the cost of annotating reasoning processes for reward modeling is\nhigh, making large-scale collection of high-quality data challenging. To\naddress this, we propose a novel reward model approach called the Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps at both fine-grained and coarse-grained levels. HRM excels at assessing\nmulti-step reasoning coherence, especially when flawed steps are later\ncorrected through self-reflection. To further reduce the cost of generating\ntraining data, we introduce a lightweight and effective data augmentation\nstrategy called Hierarchical Node Compression (HNC), which merges two\nconsecutive reasoning steps into one within the tree structure. By applying HNC\nto MCTS-generated reasoning trajectories, we enhance the diversity and\nrobustness of HRM training data while introducing controlled noise with minimal\ncomputational overhead. Empirical results on the PRM800K dataset show that HRM,\ntogether with HNC, provides more stable and reliable evaluations than PRM.\nFurthermore, cross-domain evaluations on the MATH500 and GSM8K datasets\ndemonstrate HRM's strong generalization and robustness across a variety of\nreasoning tasks."}
{"id": "2503.17279", "pdf": "https://arxiv.org/pdf/2503.17279.pdf", "abs": "https://arxiv.org/abs/2503.17279", "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement", "authors": ["Gaifan Zhang", "Yi Zhou", "Danushka Bollegala"], "categories": ["cs.CL"], "comment": null, "summary": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance."}
{"id": "2503.18991", "pdf": "https://arxiv.org/pdf/2503.18991.pdf", "abs": "https://arxiv.org/abs/2503.18991", "title": "HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment", "authors": ["Ruoxi Cheng", "Haoxuan Ma", "Weixin Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The three authors contributed equally to this work", "summary": "The alignment of large language models (LLMs) with human values remains\ncritical yet hindered by four key challenges: (1) scarcity of balanced safety\ndatasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to\nshallow alignment, and (4) inability to dynamically adapt rewards according to\ntask difficulty. To address these limitations, we introduce HAIR\n(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a\nnovel alignment approach inspired by shadow models in membership inference\nattacks. Our approach consists of two main components: (1) construction of a\nbalanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using\nstructured prompts that leverage the introspective reasoning capabilities of\nLLMs; and (2) training of category-specific reward models with Group Relative\nPolicy Optimization (GRPO), dynamically tuning optimization to task difficulty\nat both the data and model levels. Comprehensive experiments across four\nharmlessness and four usefulness benchmarks demonstrate that HAIR achieves\nstate-of-the-art performance, outperforming all baseline methods in safety\nwhile maintaining high levels of usefulness."}
{"id": "2503.20953", "pdf": "https://arxiv.org/pdf/2503.20953.pdf", "abs": "https://arxiv.org/abs/2503.20953", "title": "Clean & Clear: Feasibility of Safe LLM Clinical Guidance", "authors": ["Julia Ive", "Felix Jozsa", "Nick Jackson", "Paulina Bondaronek", "Ciaran Scott Hill", "Richard Dobson"], "categories": ["cs.CL"], "comment": null, "summary": "Background:\n  Clinical guidelines are central to safe evidence-based medicine in modern\nhealthcare, providing diagnostic criteria, treatment options and monitoring\nadvice for a wide range of illnesses. LLM-empowered chatbots have shown great\npromise in Healthcare Q&A tasks, offering the potential to provide quick and\naccurate responses to medical inquiries.\n  Our main objective was the development and preliminary assessment of an\nLLM-empowered chatbot software capable of reliably answering clinical guideline\nquestions using University College London Hospital (UCLH) clinical guidelines.\n  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant\ninformation from the UCLH guidelines to answer questions. Our approach\nhighlights the safety and reliability of referencing information over its\ninterpretation and response generation. Seven doctors from the ward assessed\nthe chatbot's performance by comparing its answers to the gold standard.\n  Results: Our chatbot demonstrates promising performance in terms of\nrelevance, with ~73% of its responses rated as very relevant, showcasing a\nstrong understanding of the clinical context. Importantly, our chatbot achieves\na recall of 1.00 for extracted guideline lines, substantially minimising the\nrisk of missing critical information. Approximately 78% of responses were rated\nsatisfactory in terms of completeness. A small portion (~14.5%) contained minor\nunnecessary information, indicating occasional lapses in precision. The\nchatbot' showed high efficiency, with an average completion time of 10 seconds,\ncompared to 30 seconds for human respondents. Evaluation of clinical reasoning\nshowed that 72% of the chatbot's responses were without flaws. Our chatbot\ndemonstrates significant potential to speed up and improve the process of\naccessing locally relevant clinical information for healthcare professionals."}
{"id": "2503.23895", "pdf": "https://arxiv.org/pdf/2503.23895.pdf", "abs": "https://arxiv.org/abs/2503.23895", "title": "Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement", "authors": ["Yuqiao Tan", "Shizhu He", "Huanxuan Liao", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "preprint. Code is available at https://github.com/Trae1ounG/DyPRAG", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG."}
{"id": "2504.07986", "pdf": "https://arxiv.org/pdf/2504.07986.pdf", "abs": "https://arxiv.org/abs/2504.07986", "title": "SEAL: Steerable Reasoning Calibration of Large Language Models for Free", "authors": ["Runjin Chen", "Zhenyu Zhang", "Junyuan Hong", "Souvik Kundu", "Zhangyang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated\ncompelling capabilities for complex reasoning tasks via the extended\nchain-of-thought (CoT) reasoning mechanism. However, recent studies reveal\nsubstantial redundancy in the CoT reasoning traces, which not only increases\ninference latency but also negatively impacts model performance by diverting\nattention to unnecessary reasoning paths. To address this issue, we investigate\nthe internal reasoning structures of LLMs and categorize them into three\nprimary thought types: execution, reflection, and transition thoughts.\nMoreover, our analysis reveals that excessive reflection and transition\nthoughts are strongly correlated with failure cases and these thought\ncategories exhibit clear separation in the latent space. Based on these, we\nintroduce SEAL (Steerable reasoning calibration), a training-free approach that\nseamlessly calibrates the CoT process, improving accuracy while demonstrating\nsignificant efficiency gains. SEAL consists of an offline stage for extracting\nthe reasoning steering vector in the latent space, followed by an on-the-fly\ncalibration of the reasoning trace through representation intervention using\nthe steering vector. Notably, the steering vector exhibits strong\ntransferability across various tasks. Extensive experiments across multiple\nmodels (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500,\nGSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11%\nimprovement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our\ncode is publicly available at https://github.com/VITA-Group/SEAL."}
{"id": "2504.17974", "pdf": "https://arxiv.org/pdf/2504.17974.pdf", "abs": "https://arxiv.org/abs/2504.17974", "title": "Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English", "authors": ["Sabur Butt", "Fazlourrahman Balouchzahi", "Ahmad Imam Amjad", "Maaz Amjad", "Hector G. Ceballos", "Salud Maria Jimenez-Zafra"], "categories": ["cs.CL"], "comment": null, "summary": "Hope is a complex and underexplored emotional state that plays a significant\nrole in education, mental health, and social interaction. Unlike basic\nemotions, hope manifests in nuanced forms ranging from grounded optimism to\nexaggerated wishfulness or sarcasm, making it difficult for Natural Language\nProcessing systems to detect accurately. This study introduces PolyHope V2, a\nmultilingual, fine-grained hope speech dataset comprising over 30,000 annotated\ntweets in English and Spanish. This resource distinguishes between four hope\nsubtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances\nexisting datasets by explicitly labeling sarcastic instances. We benchmark\nmultiple pretrained transformer models and compare them with large language\nmodels (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.\nOur findings show that fine-tuned transformers outperform prompt-based LLMs,\nespecially in distinguishing nuanced hope categories and sarcasm. Through\nqualitative analysis and confusion matrices, we highlight systematic challenges\nin separating closely related hope subtypes. The dataset and results provide a\nrobust foundation for future emotion recognition tasks that demand greater\nsemantic and contextual sensitivity across languages."}
{"id": "2504.18376", "pdf": "https://arxiv.org/pdf/2504.18376.pdf", "abs": "https://arxiv.org/abs/2504.18376", "title": "Pushing the boundary on Natural Language Inference", "authors": ["Pablo Miralles-González", "Javier Huertas-Tato", "Alejandro Martín", "David Camacho"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality."}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949.pdf", "abs": "https://arxiv.org/abs/2505.00949", "title": "Llama-Nemotron: Efficient Reasoning Models", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung", "Chris Alexiuk"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM."}
{"id": "2505.01877", "pdf": "https://arxiv.org/pdf/2505.01877.pdf", "abs": "https://arxiv.org/abs/2505.01877", "title": "Humans can learn to detect AI-generated texts, or at least learn when they can't", "authors": ["Jiří Milička", "Anna Marklová", "Ondřej Drobil", "Eva Pospíšilová"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates whether individuals can learn to accurately\ndiscriminate between human-written and AI-produced texts when provided with\nimmediate feedback, and if they can use this feedback to recalibrate their\nself-perceived competence. We also explore the specific criteria individuals\nrely upon when making these decisions, focusing on textual style and perceived\nreadability.\n  We used GPT-4o to generate several hundred texts across various genres and\ntext types comparable to Koditex, a multi-register corpus of human-written\ntexts. We then presented randomized text pairs to 255 Czech native speakers who\nidentified which text was human-written and which was AI-generated.\nParticipants were randomly assigned to two conditions: one receiving immediate\nfeedback after each trial, the other receiving no feedback until experiment\ncompletion. We recorded accuracy in identification, confidence levels, response\ntimes, and judgments about text readability along with demographic data and\nparticipants' engagement with AI technologies prior to the experiment.\n  Participants receiving immediate feedback showed significant improvement in\naccuracy and confidence calibration. Participants initially held incorrect\nassumptions about AI-generated text features, including expectations about\nstylistic rigidity and readability. Notably, without feedback, participants\nmade the most errors precisely when feeling most confident -- an issue largely\nresolved among the feedback group.\n  The ability to differentiate between human and AI-generated texts can be\neffectively learned through targeted training with explicit feedback, which\nhelps correct misconceptions about AI stylistic features and readability, as\nwell as potential other variables that were not explored, while facilitating\nmore accurate self-assessment. This finding might be particularly important in\neducational contexts."}
{"id": "2505.02156", "pdf": "https://arxiv.org/pdf/2505.02156.pdf", "abs": "https://arxiv.org/abs/2505.02156", "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents", "authors": ["Minzheng Wang", "Yongbin Li", "Haobo Wang", "Xinghua Zhang", "Nan Xu", "Bingli Wu", "Fei Huang", "Haiyang Yu", "Wenji Mao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in Progress. The code and data are available, see\n  https://github.com/MozerWang/AMPO", "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode\n$\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four\nthinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on\nreal-time context. Our framework's core innovation, the $\\textbf{A}$daptive\n$\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach."}
{"id": "2505.02579", "pdf": "https://arxiv.org/pdf/2505.02579.pdf", "abs": "https://arxiv.org/abs/2505.02579", "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning", "authors": ["Lingxiao Kong", "Cong Yang", "Susanne Neufang", "Oya Deniz Beyan", "Zeyd Boukhers"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 9 figures, submitted to SIGDIAL 2025 conference", "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives."}
{"id": "2404.05046", "pdf": "https://arxiv.org/pdf/2404.05046.pdf", "abs": "https://arxiv.org/abs/2404.05046", "title": "FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback", "authors": ["Liqiang Jing", "Xinya Du"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated proficiency in\ntackling a variety of visual-language tasks. However, current LVLMs suffer from\nmisalignment between text and image modalities which causes three kinds of\nhallucination problems, i.e., object existence, object attribute, and object\nrelationship. To tackle this issue, existing methods mainly utilize\nReinforcement Learning (RL) to align modalities in LVLMs. However, they still\nsuffer from three main limitations: (1) General feedback can not indicate the\nhallucination type contained in the response; (2) Sparse rewards only give the\nsequence-level reward for the whole response; and (3)Annotation cost is\ntime-consuming and labor-intensive. To handle these limitations, we propose an\ninnovative method to align modalities in LVLMs through Fine-Grained Artificial\nIntelligence Feedback (FGAIF), which mainly consists of three steps: AI-based\nFeedback Collection, Fine-grained Reward Model Training, and Reinforcement\nLearning with Fine-grained Reward. Specifically, We first utilize AI tools to\npredict the types of hallucination for each segment in the response and obtain\na collection of fine-grained feedback. Then, based on the collected reward\ndata, three specialized reward models are trained to produce dense rewards.\nFinally, a novel fine-grained feedback module is integrated into the Proximal\nPolicy Optimization (PPO) algorithm. Extensive experiments are conducted on\nhallucination and general benchmarks, demonstrating the superior performance of\nour proposed method. Notably, compared with previous models trained with the\nRL-based aligning method, our proposed method is effective even with fewer\nparameters."}
{"id": "2405.15025", "pdf": "https://arxiv.org/pdf/2405.15025.pdf", "abs": "https://arxiv.org/abs/2405.15025", "title": "OAC: Output-adaptive Calibration for Accurate Post-training Quantization", "authors": ["Ali Edalati", "Alireza Ghaffari", "Mahsa Ghazvini Nejad", "Lu Hou", "Boxing Chen", "Masoud Asgharian", "Vahid Partovi Nia"], "categories": ["cs.LG", "cs.CL"], "comment": "22 pages, 4 figures", "summary": "Deployment of Large Language Models (LLMs) has major computational costs, due\nto their rapidly expanding size. Compression of LLMs reduces the memory\nfootprint, latency, and energy required for their inference. Post-training\nQuantization (PTQ) techniques have been developed to compress LLMs while\navoiding expensive re-training. Most PTQ approaches formulate the quantization\nerror based on a layer-wise Euclidean loss, ignoring the model output. Then,\neach layer is calibrated using its layer-wise Hessian to update the weights\ntowards minimizing the quantization error. The Hessian is also used for\ndetecting the most salient weights to quantization. Such PTQ approaches are\nprone to accuracy drop in low-precision quantization. We propose\nOutput-adaptive Calibration (OAC) to incorporate the model output in the\ncalibration process. We formulate the quantization error based on the\ndistortion of the output cross-entropy loss. OAC approximates the\noutput-adaptive Hessian for each layer under reasonable assumptions to reduce\nthe computational complexity. The output-adaptive Hessians are used to update\nthe weight matrices and detect the salient weights towards maintaining the\nmodel output. Our proposed method outperforms the state-of-the-art baselines\nsuch as SpQR and BiLLM, especially, at extreme low-precision (2-bit and binary)\nquantization."}
{"id": "2405.19313", "pdf": "https://arxiv.org/pdf/2405.19313.pdf", "abs": "https://arxiv.org/abs/2405.19313", "title": "Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "categories": ["cs.AI", "cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "The observed similarities in the behavior of humans and Large Language Models\n(LLMs) have prompted researchers to consider the potential of using LLMs as\nmodels of human cognition. However, several significant challenges must be\naddressed before LLMs can be legitimately regarded as cognitive models. For\ninstance, LLMs are trained on far more data than humans typically encounter,\nand may have been directly trained on human data in specific cognitive tasks or\naligned with human preferences. Consequently, the origins of these behavioral\nsimilarities are not well understood. In this paper, we propose a novel way to\nenhance the utility of LLMs as cognitive models. This approach involves (i)\nleveraging computationally equivalent tasks that both an LLM and a rational\nagent need to master for solving a cognitive problem and (ii) examining the\nspecific task distributions required for an LLM to exhibit human-like\nbehaviors. We apply this approach to decision-making -- specifically risky and\nintertemporal choice -- where the key computationally equivalent task is the\narithmetic of expected value calculations. We show that an LLM pretrained on an\necologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts\nhuman behavior better than many traditional cognitive models. Pretraining LLMs\non ecologically valid arithmetic datasets is sufficient to produce a strong\ncorrespondence between these models and human decision-making. Our results also\nsuggest that LLMs used as cognitive models should be carefully investigated via\nablation studies of the pretraining data."}
{"id": "2406.16020", "pdf": "https://arxiv.org/pdf/2406.16020.pdf", "abs": "https://arxiv.org/abs/2406.16020", "title": "AudioBench: A Universal Benchmark for Audio Large Language Models", "authors": ["Bin Wang", "Xunlong Zou", "Geyu Lin", "Shuo Sun", "Zhuohan Liu", "Wenyu Zhang", "Zhengyuan Liu", "AiTi Aw", "Nancy F. Chen"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "v5 - Update acknowledgment; Code:\n  https://github.com/AudioLLMs/AudioBench", "summary": "We introduce AudioBench, a universal benchmark designed to evaluate Audio\nLarge Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26\ndatasets, among which, 7 are newly proposed datasets. The evaluation targets\nthree main aspects: speech understanding, audio scene understanding, and voice\nunderstanding (paralinguistic). Despite recent advancements, there lacks a\ncomprehensive benchmark for AudioLLMs on instruction following capabilities\nconditioned on audio signals. AudioBench addresses this gap by setting up\ndatasets as well as desired evaluation metrics. Besides, we also evaluated the\ncapabilities of five popular models and found that no single model excels\nconsistently across all tasks. We outline the research outlook for AudioLLMs\nand anticipate that our open-sourced evaluation toolkit, data, and leaderboard\nwill offer a robust testbed for future model developments."}
{"id": "2407.06606", "pdf": "https://arxiv.org/pdf/2407.06606.pdf", "abs": "https://arxiv.org/abs/2407.06606", "title": "Tailored Design of Audio-Visual Speech Recognition Models using Branchformers", "authors": ["David Gimeno-Gómez", "Carlos-D. Martínez-Hinarejos"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted in Computer Speech & Language journal of Elsevier", "summary": "Recent advances in Audio-Visual Speech Recognition (AVSR) have led to\nunprecedented achievements in the field, improving the robustness of this type\nof system in adverse, noisy environments. In most cases, this task has been\naddressed through the design of models composed of two independent encoders,\neach dedicated to a specific modality. However, while recent works have\nexplored unified audio-visual encoders, determining the optimal cross-modal\narchitecture remains an ongoing challenge. Furthermore, such approaches often\nrely on models comprising vast amounts of parameters and high computational\ncost training processes. In this paper, we aim to bridge this research gap by\nintroducing a novel audio-visual framework. Our proposed method constitutes, to\nthe best of our knowledge, the first attempt to harness the flexibility and\ninterpretability offered by encoder architectures, such as the Branchformer, in\nthe design of parameter-efficient AVSR systems. To be more precise, the\nproposed framework consists of two steps: first, estimating audio- and\nvideo-only systems, and then designing a tailored audio-visual unified encoder\nbased on the layer-level branch scores provided by the modality-specific\nmodels. Extensive experiments on English and Spanish AVSR benchmarks covering\nmultiple data conditions and scenarios demonstrated the effectiveness of our\nproposed method. Even when trained on a moderate scale of data, our models\nachieve competitive word error rates (WER) of approximately 2.5\\% for English\nand surpass existing approaches for Spanish, establishing a new benchmark with\nan average WER of around 9.1\\%. These results reflect how our tailored AVSR\nsystem is able to reach state-of-the-art recognition rates while significantly\nreducing the model complexity w.r.t. the prevalent approach in the field. Code\nand pre-trained models are available at\nhttps://github.com/david-gimeno/tailored-avsr."}
{"id": "2410.22086", "pdf": "https://arxiv.org/pdf/2410.22086.pdf", "abs": "https://arxiv.org/abs/2410.22086", "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate", "authors": ["Zhiqi Bu", "Xiaomeng Jin", "Bhanukiran Vinzamuri", "Anil Ramakrishna", "Kai-Wei Chang", "Volkan Cevher", "Mingyi Hong"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to NAACL 2025 main conference", "summary": "Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training."}
{"id": "2411.18915", "pdf": "https://arxiv.org/pdf/2411.18915.pdf", "abs": "https://arxiv.org/abs/2411.18915", "title": "MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications", "authors": ["Vishnou Vinayagame", "Gregory Senay", "Luis Martí"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Business documents often contain substantial tabular and textual information\nwith numerical values, requiring mathematical reasoning for effective document\nunderstanding. While Small Language Models (SLMs) still struggle at this task,\ntool-augmented multi-step agents perform better, at the cost of relying on\nclosed-source or larger models, external data, or extensive prompt-engineering.\nThis work introduces MATATA, a novel weakly supervised end-to-end approach to\ntrain multi-step reasoning language agents for document tabular applications.\nMATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B\nSLMs. During its two-stage training, MATATA uses the final outcome of the\nmulti-step reasoning chain as weak supervision. This approach avoids having to\nindividually supervise each intermediate agent in the reasoning chain. By\nemploying an adaptive planner and shared tools across different datasets,\nMATATA shows robust performance. Experiments demonstrate that MATATA achieves\nstate-of-the-art on FinQA, and on TAT-QA among reasoning methods based on\nopen-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based\nframeworks on TabMWP. This novel weakly supervised approach enables training an\nend-to-end multi-step reasoning agent without intermediate supervision,\nsupporting future developments of cost-effective powerful agentic systems."}
{"id": "2412.17739", "pdf": "https://arxiv.org/pdf/2412.17739.pdf", "abs": "https://arxiv.org/abs/2412.17739", "title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization", "authors": ["Ermo Hua", "Che Jiang", "Xingtai Lv", "Kaiyan Zhang", "Ning Ding", "Youbang Sun", "Biqing Qi", "Yuchen Fan", "Xuekai Zhu", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "Extending the context length of Language Models (LMs) by improving Rotary\nPosition Embedding (RoPE) has become a trend. While existing works mainly\naddress RoPE's limitations within attention mechanism, this paper provides an\nanalysis across nearly all parts of LMs, uncovering their adverse effects on\nlength generalization for RoPE-based attention. Using Discrete Signal\nProcessing theory, we show that RoPE enables periodic attention by implicitly\nachieving Non-Uniform Discrete Fourier Transform. However, this periodicity is\nundermined by the spectral damage caused by: 1) linear layers and activation\nfunctions outside of attention; 2) insufficiently trained frequency components\nbrought by time-domain truncation. Building on our observations, we propose\nFourier Position Embedding (FoPE), which enhances attention's frequency-domain\nproperties to improve both its periodic extension and length generalization.\nFoPE constructs Fourier Series and zero-outs the destructive frequency\ncomponents, increasing model robustness against the spectrum damage.\nExperiments across various model scales and benchmarks show that, within\nvarying context windows, FoPE maintains a more stable performance compared to\nRoPE and ALiBi. Several analyses and ablations bring further support to our\nmethod and theoretical modeling."}
{"id": "2502.07328", "pdf": "https://arxiv.org/pdf/2502.07328.pdf", "abs": "https://arxiv.org/abs/2502.07328", "title": "Music for All: Representational Bias and Cross-Cultural Adaptability of Music Generation Models", "authors": ["Atharva Mehta", "Shivam Chauhan", "Amirbek Djanibekov", "Atharva Kulkarni", "Gus Xia", "Monojit Choudhury"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "17 pages, 5 figures, accepted to NAACL'25", "summary": "The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning."}
{"id": "2503.02445", "pdf": "https://arxiv.org/pdf/2503.02445.pdf", "abs": "https://arxiv.org/abs/2503.02445", "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling", "authors": ["Hao Li", "Yuhao Huang", "Chang Xu", "Viktor Schlegel", "Renhe Jiang", "Riza Batista-Navarro", "Goran Nenadic", "Jiang Bian"], "categories": ["cs.LG", "cs.CL", "cs.MA"], "comment": "ICML 2025", "summary": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data."}
{"id": "2503.02950", "pdf": "https://arxiv.org/pdf/2503.02950.pdf", "abs": "https://arxiv.org/abs/2503.02950", "title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications", "authors": ["Danqing Zhang", "Balaji Rama", "Jingyi Ni", "Shiying He", "Fu Zhao", "Kunyu Chen", "Arnold Chen", "Junyu Cao"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "We introduce LiteWebAgent, an open-source suite for VLM-based web agent\napplications. Our framework addresses a critical gap in the web agent ecosystem\nwith a production-ready solution that combines minimal serverless backend\nconfiguration, intuitive user and browser interfaces, and extensible research\ncapabilities in agent planning, memory, and tree search. For the core\nLiteWebAgent agent framework, we implemented a simple yet effective baseline\nusing recursive function calling, providing with decoupled action generation\nand action grounding. In addition, we integrate advanced research components\nsuch as agent planning, agent workflow memory, and tree search in a modular and\nextensible manner. We then integrate the LiteWebAgent agent framework with\nfrontend and backend as deployed systems in two formats: (1) a production\nVercel-based web application, which provides users with an agent-controlled\nremote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control\nan existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent\nframework is available at https://github.com/PathOnAI/LiteWebAgent, with\ndeployed frontend at https://lite-web-agent.vercel.app/."}
{"id": "2503.15661", "pdf": "https://arxiv.org/pdf/2503.15661.pdf", "abs": "https://arxiv.org/abs/2503.15661", "title": "UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction", "authors": ["Shravan Nayak", "Xiangru Jian", "Kevin Qinghong Lin", "Juan A. Rodriguez", "Montek Kalsi", "Rabiul Awal", "Nicolas Chapados", "M. Tamer Özsu", "Aishwarya Agrawal", "David Vazquez", "Christopher Pal", "Perouz Taslakian", "Spandana Gella", "Sai Rajeswar"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "This paper has been accepted to the 41st International Conference on\n  Machine Learning (ICML 2025)", "summary": "Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks."}
{"id": "2504.11739", "pdf": "https://arxiv.org/pdf/2504.11739.pdf", "abs": "https://arxiv.org/abs/2504.11739", "title": "The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation", "authors": ["Bingjie Gao", "Xinyu Gao", "Xiaoxue Wu", "Yujie Zhou", "Yu Qiao", "Li Niu", "Xinyuan Chen", "Yaohui Wang"], "categories": ["cs.CV", "cs.CL"], "comment": "accepted by CVPR2025, Project website:\n  https://whynothaha.github.io/Prompt_optimizer/RAPO.html", "summary": "The evolution of Text-to-video (T2V) generative models, trained on\nlarge-scale datasets, has been marked by significant progress. However, the\nsensitivity of T2V generative models to input prompts highlights the critical\nrole of prompt design in influencing generative outcomes. Prior research has\npredominantly relied on Large Language Models (LLMs) to align user-provided\nprompts with the distribution of training prompts, albeit without tailored\nguidance encompassing prompt vocabulary and sentence structure nuances. To this\nend, we introduce RAPO, a novel Retrieval-Augmented Prompt Optimization\nframework. In order to address potential inaccuracies and ambiguous details\ngenerated by LLM-generated prompts. RAPO refines the naive prompts through dual\noptimization branches, selecting the superior prompt for T2V generation. The\nfirst branch augments user prompts with diverse modifiers extracted from a\nlearned relational graph, refining them to align with the format of training\nprompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive\nprompt using a pre-trained LLM following a well-defined instruction set.\nExtensive experiments demonstrate that RAPO can effectively enhance both the\nstatic and dynamic dimensions of generated videos, demonstrating the\nsignificance of prompt optimization for user-provided prompts."}
{"id": "2505.00926", "pdf": "https://arxiv.org/pdf/2505.00926.pdf", "abs": "https://arxiv.org/abs/2505.00926", "title": "How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias", "authors": ["Ruiquan Huang", "Yingbin Liang", "Jing Yang"], "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "accepted by ICML 2025", "summary": "Language recognition tasks are fundamental in natural language processing\n(NLP) and have been widely used to benchmark the performance of large language\nmodels (LLMs). These tasks also play a crucial role in explaining the working\nmechanisms of transformers. In this work, we focus on two representative tasks\nin the category of regular language recognition, known as `even pairs' and\n`parity check', the aim of which is to determine whether the occurrences of\ncertain subsequences in a given sequence are even. Our goal is to explore how a\none-layer transformer, consisting of an attention layer followed by a linear\nlayer, learns to solve these tasks by theoretically analyzing its training\ndynamics under gradient descent. While even pairs can be solved directly by a\none-layer transformer, parity check need to be solved by integrating\nChain-of-Thought (CoT), either into the inference stage of a transformer\nwell-trained for the even pairs task, or into the training of a one-layer\ntransformer. For both problems, our analysis shows that the joint training of\nattention and linear layers exhibits two distinct phases. In the first phase,\nthe attention layer grows rapidly, mapping data sequences into separable\nvectors. In the second phase, the attention layer becomes stable, while the\nlinear layer grows logarithmically and approaches in direction to a max-margin\nhyperplane that correctly separates the attention layer outputs into positive\nand negative samples, and the loss decreases at a rate of $O(1/t)$. Our\nexperiments validate those theoretical results."}
