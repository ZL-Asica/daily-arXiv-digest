{"id": "2509.12383", "pdf": "https://arxiv.org/pdf/2509.12383.pdf", "abs": "https://arxiv.org/abs/2509.12383", "title": "Data selves and identity theft in the age of AI", "authors": ["Tim Gorichanaz"], "categories": ["cs.HC"], "comment": null, "summary": "This chapter examines identity theft in the digital age, particularly in the\ncontext of emerging artificial intelligence (AI) technologies. It begins with a\ndiscussion of big data and selfhood, the concepts of data selves and data\ndoubles, and the process of identification in the digital age. Next, the\nliterature on online identity theft is reviewed, including its theoretical and\nempirical aspects. As is evident from that review, AI technologies have\nincreased the speed and scale of identity crimes that were already rampant in\nthe online world, even while they have led to new ways of detecting and\npreventing such crimes. As with any new technology, AI is currently fuelling an\narms race between criminals and law enforcement, with end users often caught\npowerless in the middle. The chapter closes by exploring some emerging\ndirections and future possibilities of identity theft in the age of AI.", "AI": {"tldr": "This chapter explores identity theft in the digital age, focusing on AI's role in exacerbating and combating these crimes.", "motivation": "Understanding the impact of AI on identity theft, especially with the proliferation of online technologies.", "method": "The chapter reviews existing literature on online identity theft, examining theoretical and empirical aspects.", "result": "AI technologies have accelerated identity crimes while also providing new detection and prevention methods, leading to a dynamic between criminals and law enforcement.", "conclusion": "The chapter emphasizes the ongoing battles regarding identity theft and suggests future directions as technology evolves.", "key_contributions": ["Analysis of AI's impact on identity theft", "Review of theoretical and empirical literature", "Discussion of future trends in identity theft prevention"], "limitations": "", "keywords": ["Identity Theft", "Artificial Intelligence", "Digital Age", "Online Security", "Big Data"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.12408", "pdf": "https://arxiv.org/pdf/2509.12408.pdf", "abs": "https://arxiv.org/abs/2509.12408", "title": "FlexMind: Scaffolding Flexible Ideation Workflows with AI in Creative Problem-Solving", "authors": ["Yaqing Yang", "Vikram Mohanty", "Nikolas Martelaro", "Aniket Kittur", "Yan-Ying Chen", "Matthew K. Hong"], "categories": ["cs.HC"], "comment": null, "summary": "Divergent thinking in the ideation stage of creative problem-solving demands\nthat individuals explore a broad design space. Yet this exploration rarely\nfollows a neat, linear sequence; problem-solvers constantly shift among\nsearching, creating, and evaluating ideas. Existing interfaces either impose\nrigid, step-by-step workflows or permit unguided free-form exploration. To\nstrike a balance between flexibility and guidance for augmenting people's\nefficiency and creativity, we introduce a human-AI collaborative workflow that\nsupports a fluid ideation process. The system surfaces three opt-in aids: (1)\nhigh-level schemas to uncover alternative ideas, (2) risk analysis with\nmitigation suggestions, and (3) steering system-generated suggestions. Users\ncan invoke these supports at any moment, allowing seamless back-and-forth\nmovement among design actions to maintain creative momentum.", "AI": {"tldr": "The paper presents a human-AI collaborative workflow designed to enhance creativity and efficiency during the ideation stage of creative problem-solving by providing fluid support for exploring design alternatives.", "motivation": "To address the limitations of existing interfaces in creative problem-solving, which either impose rigid workflows or offer no guidance.", "method": "The paper proposes a collaborative AI system that presents three opt-in aids: high-level schemas for alternative ideas, risk analysis with mitigation, and system-generated suggestions for user invocation.", "result": "The proposed system allows users to fluidly navigate through searching, creating, and evaluating ideas, enhancing both creativity and efficiency during ideation.", "conclusion": "By integrating these aids into the ideation process, users can sustain creative momentum without being constrained by rigid workflows.", "key_contributions": ["Introduced a human-AI collaborative workflow for creative problem-solving.", "Developed three opt-in aids for enhancing ideation: schemas, risk analysis, and suggestions.", "Facilitated a fluid transition between design actions to improve creativity and efficiency."], "limitations": "", "keywords": ["human-AI collaboration", "creative problem-solving", "ideation", "risk analysis", "design support"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12419", "pdf": "https://arxiv.org/pdf/2509.12419.pdf", "abs": "https://arxiv.org/abs/2509.12419", "title": "Beyond Gaze Overlap: Analyzing Joint Visual Attention Dynamics Using Egocentric Data", "authors": ["Kumushini Thennakoon", "Yasasi Abeysinghe", "Bhanuka Mahanama", "Vikas Ashok", "Sampath Jayarathna"], "categories": ["cs.HC"], "comment": "Accepted at IEEE 26th International Conference on Information Reuse\n  and Integration for Data Science, 6 pages,3 figures", "summary": "Joint visual attention (JVA) provides informative cues on human behavior\nduring social interactions. The ubiquity of egocentric eye-trackers and\nlarge-scale datasets on everyday interactions offer research opportunities in\nidentifying JVA in multi-user environments. We propose a novel approach\nutilizing spatiotemporal tubes centered on attention rendered by individual\ngaze and detect JVA using deep-learning-based feature mapping. Our results\nreveal object-focused collaborative tasks to yield higher JVA (44-46%), whereas\nindependent tasks yield lower (4-5%) attention. Beyond JVA, we analyze\nattention characteristics using ambient-focal attention coefficient K to\nunderstand the qualitative aspects of shared attention. Our analysis reveals\n$\\mathcal{K}$ to converge instances where participants interact with shared\nobjects while diverging when independent. While our study presents seminal\nfindings on joint attention with egocentric commodity eye trackers, it\nindicates the potential utility of our approach in psychology, human-computer\ninteraction, and social robotics, particularly in understanding attention\ncoordination mechanisms in ecologically valid contexts.", "AI": {"tldr": "This study proposes a novel approach to detect joint visual attention (JVA) using spatiotemporal tubes from egocentric eye-tracking data, revealing that object-focused collaborative tasks show significantly higher JVA than independent tasks.", "motivation": "The research aims to identify joint visual attention in multi-user environments using egocentric eye-trackers and large datasets, which provides insights into social interactions and attention coordination.", "method": "Utilizing deep-learning-based feature mapping, the approach involves analyzing spatiotemporal tubes centered on individual gaze to detect JVA in various task settings.", "result": "The analysis confirmed that collaborative tasks yield higher JVA (44-46%) compared to independent tasks (4-5%), revealing notable differences in attentional dynamics.", "conclusion": "The findings highlight the utility of the proposed method in fields like psychology, human-computer interaction, and social robotics for understanding attention mechanisms in realistic scenarios.", "key_contributions": ["Novel approach for detecting JVA using eye-tracking data", "Analysis of attention characteristics using the ambient-focal attention coefficient K", "Revealed significant differences in JVA depending on task types"], "limitations": "The study is limited to specific collaborative and independent task settings and may require further validation in diverse ecological contexts.", "keywords": ["Joint Visual Attention", "Egocentric Eye Trackers", "Attention Coordination", "Human-Computer Interaction", "Deep Learning"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2509.12517", "pdf": "https://arxiv.org/pdf/2509.12517.pdf", "abs": "https://arxiv.org/abs/2509.12517", "title": "Extended AI Interactions Shape Sycophancy and Perspective Mimesis", "authors": ["Shomik Jain", "Charlotte Park", "Matheus Mesquita Viana", "Ashia Wilson", "Dana Calacci"], "categories": ["cs.HC"], "comment": null, "summary": "We investigate whether long-context interactions between users and LLMs lead\nto AI mirroring behaviors. We focus on two forms of mirroring: (1) sycophancy\n-- the tendency of models to be overly agreeable with users, and (2)\nperspective mimesis -- the extent to which models reflect a user's perspective.\nUsing two weeks of interaction context collected from 38 users, we compare\nmodel responses with and without long-context for two tasks: political\nexplanations and personal advice. Our results demonstrate how and when\nreal-world interaction contexts can amplify AI mirroring behaviors. We find\nthat sycophancy increases in long-context, irrespective of the interaction\ntopics. Perspective mimesis increases only in contexts where models can\naccurately infer user perspectives.", "AI": {"tldr": "The paper examines AI mirroring behaviors in LLMs during long-context interactions, specifically focusing on sycophancy and perspective mimesis.", "motivation": "To explore how long-context interactions between users and LLMs influence AI mirroring behaviors, which are critical for understanding user experience and trust in AI systems.", "method": "We analyzed two weeks of interaction data from 38 users, comparing model responses with and without long-context in political explanations and personal advice tasks.", "result": "Findings reveal that sycophancy is consistently higher with long-context interactions, while perspective mimesis only increases in contexts where user perspectives can be accurately inferred.", "conclusion": "Long-context interactions significantly amplify certain AI mirroring behaviors, which has implications for user trust and experience with LLMs.", "key_contributions": ["Identification of sycophancy and perspective mimesis as key mirroring behaviors.", "Empirical evidence that long context affects these behaviors in LLMs.", "Insights into context-dependent interaction dynamics with AI."], "limitations": "", "keywords": ["AI mirroring", "long-context interactions", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12340", "pdf": "https://arxiv.org/pdf/2509.12340.pdf", "abs": "https://arxiv.org/abs/2509.12340", "title": "MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch", "authors": ["Nikolay Banar", "Ehsan Lotfi", "Jens Van Nooten", "Cristina Arhiliuc", "Marija Kliocaite", "Walter Daelemans"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, embedding resources, including models, benchmarks, and datasets,\nhave been widely released to support a variety of languages. However, the Dutch\nlanguage remains underrepresented, typically comprising only a small fraction\nof the published multilingual resources. To address this gap and encourage the\nfurther development of Dutch embeddings, we introduce new resources for their\nevaluation and generation. First, we introduce the Massive Text Embedding\nBenchmark for Dutch (MTEB-NL), which includes both existing Dutch datasets and\nnewly created ones, covering a wide range of tasks. Second, we provide a\ntraining dataset compiled from available Dutch retrieval datasets, complemented\nwith synthetic data generated by large language models to expand task coverage\nbeyond retrieval. Finally, we release a series of E5-NL models compact yet\nefficient embedding models that demonstrate strong performance across multiple\ntasks. We make our resources publicly available through the Hugging Face Hub\nand the MTEB package.", "AI": {"tldr": "This paper introduces resources aimed at enhancing Dutch language embeddings through the Massive Text Embedding Benchmark for Dutch (MTEB-NL) and a series of E5-NL embedding models.", "motivation": "To address the underrepresentation of the Dutch language in multilingual embedding resources and encourage their development.", "method": "Introduction of MTEB-NL for evaluation and generation of Dutch embeddings, supplemented by a training dataset from existing Dutch retrieval datasets and synthetic data from large language models. Release of E5-NL models for embedding tasks.", "result": "MTEB-NL includes existing and new datasets, a training dataset enhancing task coverage, and E5-NL models that show strong performance in embedding tasks.", "conclusion": "The resources provided will stimulate the development and evaluation of Dutch language embeddings and are publicly available.", "key_contributions": ["Introduction of MTEB-NL for Dutch embeddings evaluation and generation.", "Release of training data compiled from Dutch datasets complemented by synthetic data.", "Development and public release of E5-NL embedding models."], "limitations": "", "keywords": ["Dutch language", "embeddings", "MTEB-NL", "E5-NL", "language models"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2509.12525", "pdf": "https://arxiv.org/pdf/2509.12525.pdf", "abs": "https://arxiv.org/abs/2509.12525", "title": "The Adaptation Paradox: Agency vs. Mimicry in Companion Chatbots", "authors": ["T. James Brandt", "Cecilia Xi Wang"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "31 pages, 17 figures, 2 tables. Submitted to CHI 2026 (under review).\n  Preregistered: https://osf.io/f4h5b ; Code/Materials:\n  https://doi.org/10.5281/zenodo.15801081", "summary": "Generative AI powers a growing wave of companion chatbots, yet principles for\nfostering genuine connection remain unsettled. We test two routes: visible user\nauthorship versus covert language-style mimicry. In a preregistered 3x2\nexperiment (N = 162), we manipulated user-controlled avatar generation (none,\npremade, user-generated) and Language Style Matching (LSM) (static vs.\nadaptive). Generating an avatar boosted rapport ($\\omega^2$ = .040, p = .013),\nwhereas adaptive LSM underperformed static style on personalization and\nsatisfaction (d = 0.35, p = .009) and was paradoxically judged less adaptive (t\n= 3.07, p = .003, d = 0.48). We term this an Adaptation Paradox: synchrony\nerodes connection when perceived as incoherent, destabilizing persona. To\nexplain, we propose a stability-and-legibility account: visible authorship\nfosters natural interaction, while covert mimicry risks incoherence. Our\nfindings suggest designers should prioritize legible, user-driven\npersonalization and limit stylistic shifts rather than rely on opaque mimicry.", "AI": {"tldr": "The paper investigates the effects of user-controlled avatar generation and language style matching in chatbots, revealing that visible authorship enhances connection while covert mimicry can harm perception.", "motivation": "To explore how different methods of personalization in generative AI companion chatbots affect user connection and satisfaction.", "method": "A preregistered 3x2 experiment with 162 participants, manipulating avatar generation and language style matching styles.", "result": "User-generated avatars improved rapport significantly, while adaptive language style matching was less effective in personalization and satisfaction, leading to the Adaptation Paradox.", "conclusion": "Designers should focus on user-driven personalization and avoid opaque mimicry that may confuse users.", "key_contributions": ["Identifies the benefits of user-generated avatars for enhancing rapport in chatbots.", "Describes the Adaptation Paradox where stylistic mimicry can reduce perceived connection.", "Proposes a stability-and-legibility framework for designing personalized interactions."], "limitations": "Limited to specific styles of avatar generation and language mimicry; results may not generalize to all generative AI applications.", "keywords": ["Generative AI", "Chatbots", "User Authors", "Language Style Matching", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 31}}
{"id": "2509.12371", "pdf": "https://arxiv.org/pdf/2509.12371.pdf", "abs": "https://arxiv.org/abs/2509.12371", "title": "MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables", "authors": ["Matteo Marcuzzo", "Alessandro Zangari", "Andrea Albarelli", "Jose Camacho-Collados", "Mohammad Taher Pilehvar"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "As LLMs excel on standard reading comprehension benchmarks, attention is\nshifting toward evaluating their capacity for complex abstract reasoning and\ninference. Literature-based benchmarks, with their rich narrative and moral\ndepth, provide a compelling framework for evaluating such deeper comprehension\nskills. Here, we present MORABLES, a human-verified benchmark built from fables\nand short stories drawn from historical literature. The main task is structured\nas multiple-choice questions targeting moral inference, with carefully crafted\ndistractors that challenge models to go beyond shallow, extractive question\nanswering. To further stress-test model robustness, we introduce adversarial\nvariants designed to surface LLM vulnerabilities and shortcuts due to issues\nsuch as data contamination. Our findings show that, while larger models\noutperform smaller ones, they remain susceptible to adversarial manipulation\nand often rely on superficial patterns rather than true moral reasoning. This\nbrittleness results in significant self-contradiction, with the best models\nrefuting their own answers in roughly 20% of cases depending on the framing of\nthe moral choice. Interestingly, reasoning-enhanced models fail to bridge this\ngap, suggesting that scale - not reasoning ability - is the primary driver of\nperformance.", "AI": {"tldr": "MORABLES is a benchmark for evaluating LLMs on moral inference using historical literature, revealing vulnerabilities in reasoning despite scale.", "motivation": "To evaluate LLMs' capacity for complex reasoning beyond standard benchmarks.", "method": "A benchmark built from fables and short stories with multiple-choice questions focusing on moral inference and adversarial testing.", "result": "Larger models outperform smaller ones, yet remain susceptible to adversarial attacks and often fall back on superficial patterns.", "conclusion": "Scale is a key driver of performance rather than true reasoning ability, with significant self-contradiction observed in responses.", "key_contributions": ["Introduction of MORABLES benchmark for moral reasoning", "Identification of model vulnerabilities through adversarial testing", "Insights into the limitations of reasoning-enhanced models"], "limitations": "Models show significant self-contradictions and reliance on superficial patterns in reasoning situations.", "keywords": ["moral reasoning", "large language models", "adversarial testing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12578", "pdf": "https://arxiv.org/pdf/2509.12578.pdf", "abs": "https://arxiv.org/abs/2509.12578", "title": "Conflect: Designing Reflective Thinking-Based Contextual Privacy Policy for Mobile Applications", "authors": ["Shuning Zhang", "Sixing Tao", "Eve He", "Yuting Yang", "Ying Ma", "Ailei Wang", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "Privacy policies are lengthy and complex, leading to user neglect. While\ncontextual privacy policies (CPPs) present information at the point of risk,\nthey may lack engagement and disrupt tasks. We propose Conflect, an interactive\nCPP for mobile apps, guided by a reflective thinking framework. Through three\nworkshops with experienced designers and researchers, we constructed the design\nspace of reflective thinking-based CPP design, and identified the disconnect\nbetween context and action as the most critical problem. Based on participants'\nfeedback, we designed Conflect to use sidebar alerts, allowing users to reflect\non contextualized risks and fostering their control. Our system contextually\ndetects privacy risks, extracts policy segments, and automatically generates\nrisk descriptions with 94.0% policy extraction accuracy on CPP4APP dataset and\na 4.35s latency. A user study (N=28) demonstrated that Conflect improves user\nunderstanding, trust, and satisfaction while lowering cognitive load compared\nto CPPs, privacy policies and privacy labels.", "AI": {"tldr": "Designing Conflect, an interactive contextual privacy policy (CPP) for mobile apps to improve user engagement and understanding.", "motivation": "To address the neglect of lengthy and complex privacy policies by providing contextual information at the point of risk.", "method": "Developed Conflect through workshops, utilizing a reflective thinking framework to create sidebar alerts that inform users of privacy risks.", "result": "Conflect achieves 94.0% accuracy in policy extraction and a user study reveals improved user understanding, trust, satisfaction, and reduced cognitive load.", "conclusion": "Conflect offers an effective solution to enhance user interaction with privacy policies in mobile applications.", "key_contributions": ["Introduces Conflect as an interactive CPP", "Utilizes a reflective thinking framework for privacy engagement", "Demonstrated significant improvements in user experience metrics"], "limitations": "", "keywords": ["privacy policies", "human-computer interaction", "mobile applications"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12382", "pdf": "https://arxiv.org/pdf/2509.12382.pdf", "abs": "https://arxiv.org/abs/2509.12382", "title": "LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation", "authors": ["Anu Pradhan", "Alexandra Ortan", "Apurv Verma", "Madhavan Seshadri"], "categories": ["cs.CL", "H.3.3; I.2.7; I.2.6"], "comment": "Accepted in EARL 25: The 2nd Workshop on Evaluating and Applying\n  Recommender Systems with Large Language Models at RecSys 2025", "summary": "The evaluation bottleneck in recommendation systems has become particularly\nacute with the rise of Generative AI, where traditional metrics fall short of\ncapturing nuanced quality dimensions that matter in specialized domains like\nlegal research. Can we trust Large Language Models to serve as reliable judges\nof their own kind? This paper investigates LLM-as-a-Judge as a principled\napproach to evaluating Retrieval-Augmented Generation systems in legal\ncontexts, where the stakes of recommendation quality are exceptionally high.\n  We tackle two fundamental questions that determine practical viability: which\ninter-rater reliability metrics best capture the alignment between LLM and\nhuman assessments, and how do we conduct statistically sound comparisons\nbetween competing systems? Through systematic experimentation, we discover that\ntraditional agreement metrics like Krippendorff's alpha can be misleading in\nthe skewed distributions typical of AI system evaluations. Instead, Gwet's AC2\nand rank correlation coefficients emerge as more robust indicators for judge\nselection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg\ncorrections provides the statistical rigor needed for reliable system\ncomparisons.\n  Our findings suggest a path toward scalable, cost-effective evaluation that\nmaintains the precision demanded by legal applications, transforming what was\nonce a human-intensive bottleneck into an automated, yet statistically\nprincipled, evaluation framework.", "AI": {"tldr": "This paper explores the evaluation of Retrieval-Augmented Generation systems in legal contexts using LLMs and proposes new metrics for assessing LLM performance against human judgments.", "motivation": "To address the inadequacies of traditional evaluation metrics for recommendation systems with the rise of Generative AI, particularly in high-stakes fields like legal research.", "method": "The paper investigates which inter-rater reliability metrics effectively align LLM with human assessments and employs experiments to compare traditional and newer statistical methods for evaluating AI performance.", "result": "Gwet's AC2 and rank correlation coefficients are identified as more reliable for judge selection, while the Wilcoxon Signed-Rank Test with corrections proves effective for system comparisons.", "conclusion": "The proposed approach aims to create a scalable evaluation framework that combines automation with the precision required for legal applications, reducing the dependency on human evaluators.", "key_contributions": ["Introduces LLM-as-a-Judge for evaluating AI systems in legal contexts.", "Identifies more effective metrics (Gwet's AC2) for inter-rater reliability in AI evaluations.", "Develops a statistically sound framework for comparing competing AI systems."], "limitations": "", "keywords": ["Generative AI", "LLM evaluation", "Recommendation systems", "Legal applications", "Statistical methods"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.12590", "pdf": "https://arxiv.org/pdf/2509.12590.pdf", "abs": "https://arxiv.org/abs/2509.12590", "title": "DPCheatSheet: Using Worked and Erroneous LLM-usage Examples to Scaffold Differential Privacy Implementation", "authors": ["Shao-Yu Chu", "Yuhe Tian", "Yu-Xiang Wang", "Haojian Jin"], "categories": ["cs.HC"], "comment": null, "summary": "This paper explores how programmers without specialized expertise in\ndifferential privacy (DP) (i.e., novices) can leverage LLMs to implement DP\nprograms with minimal training. We first conducted a need-finding study with 6\nnovices and 3 experts to understand how they utilize LLMs in DP implementation.\nWhile DP experts can implement correct DP analyses through a few prompts,\nnovices struggle to articulate their requirements in prompts and lack the\nskills to verify the correctness of the generated code. We then developed\nDPCheatSheet, an instructional tool that helps novices implement DP using LLMs.\nDPCheatSheet combines two learning concepts: it annotates an expert's workflow\nwith LLMs as a worked example to bridge the expert mindset to novices, and it\npresents five common mistakes in LLM-based DP code generation as erroneous\nexamples to support error-driven learning. We demonstrated the effectiveness of\nDPCheatSheet with an error identification study and an open-ended DP\nimplementation study.", "AI": {"tldr": "The paper investigates how novices can use LLMs to implement differential privacy (DP) with minimal training and introduces DPCheatSheet, a tool to facilitate this process.", "motivation": "To enable programmers without specialized knowledge in differential privacy to effectively utilize LLMs for implementing DP programs.", "method": "Conducted a need-finding study with novices and experts to understand LLM usage, followed by the development of DPCheatSheet, which combines expert workflows and common pitfalls in LLM-generated code.", "result": "DPCheatSheet was shown to improve the ability of novices to identify errors and implement DP by bridging the knowledge gap with expert examples and error-driven learning.", "conclusion": "DPCheatSheet successfully aids novices in understanding and implementing differential privacy using LLMs, enhancing their coding practices in this domain.", "key_contributions": ["DPCheatSheet tool developed for aiding novices in DP implementation using LLMs", "Combines expert annotations and common mistakes for error-driven learning", "Empirical validation of effectiveness through studies."], "limitations": "The study involved a small sample size and focused only on novices and experts in differential privacy, limiting generalizability.", "keywords": ["differential privacy", "large language models", "novice programmers", "error identification", "instructional tool"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.12385", "pdf": "https://arxiv.org/pdf/2509.12385.pdf", "abs": "https://arxiv.org/abs/2509.12385", "title": "SENTRA: Selected-Next-Token Transformer for LLM Text Detection", "authors": ["Mitchell Plyler", "Yilun Zhang", "Alexander Tuzhilin", "Saoud Khalifah", "Sen Tian"], "categories": ["cs.CL", "cs.LG"], "comment": "EMNLP Findings 2025", "summary": "LLMs are becoming increasingly capable and widespread. Consequently, the\npotential and reality of their misuse is also growing. In this work, we address\nthe problem of detecting LLM-generated text that is not explicitly declared as\nsuch. We present a novel, general-purpose, and supervised LLM text detector,\nSElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder\nleveraging selected-next-token-probability sequences and utilizing contrastive\npre-training on large amounts of unlabeled data. Our experiments on three\npopular public datasets across 24 domains of text demonstrate SENTRA is a\ngeneral-purpose classifier that significantly outperforms popular baselines in\nthe out-of-domain setting.", "AI": {"tldr": "SENTRA is a novel LLM text detector that outperforms existing baselines in identifying LLM-generated text.", "motivation": "With the rising capabilities and usage of LLMs, detecting their misuse has become crucial.", "method": "SENTRA employs a Transformer-based architecture that utilizes selected-next-token-probability sequences and incorporates contrastive pre-training on large unlabeled datasets.", "result": "SENTRA is shown to significantly outperform popular baseline models across three datasets and 24 text domains.", "conclusion": "SENTRA offers a reliable method for detecting LLM-generated text, enhancing capabilities in content verification.", "key_contributions": ["Development of SENTRA, a novel LLM text detector", "Demonstration of improved performance over existing classifiers", "Contrastive pre-training on unlabeled data for general-purpose application"], "limitations": "", "keywords": ["LLM detection", "text generation", "Transformer", "contrastive learning", "content verification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.12626", "pdf": "https://arxiv.org/pdf/2509.12626.pdf", "abs": "https://arxiv.org/abs/2509.12626", "title": "DoubleAgents: Exploring Mechanisms of Building Trust with Proactive AI", "authors": ["Tao Long", "Xuanming Zhang", "Sitong Wang", "Zhou Yu", "Lydia B Chilton"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET"], "comment": "21 pages, 10 figures", "summary": "Agentic workflows promise efficiency, but adoption hinges on whether people\nactually trust systems that act on their behalf. We present DoubleAgents, an\nagentic planning tool that embeds transparency and control through user\nintervention, value-reflecting policies, rich state visualizations, and\nuncertainty flagging for human coordination tasks. A built-in respondent\nsimulation generates realistic scenarios, allowing users to rehearse, refine\npolicies, and calibrate their reliance before live use. We evaluate\nDoubleAgents in a two-day lab study (n=10), two deployments (n=2), and a\ntechnical evaluation. Results show that participants initially hesitated to\ndelegate but grew more reliant as they experienced transparency, control, and\nadaptive learning during simulated cases. Deployment results demonstrate\nDoubleAgents' real-world relevance and usefulness, showing that the effort\nrequired scaled appropriately with task complexity and contextual data. We\ncontribute trust-by-design patterns and mechanisms for proactive AI --\nconsistency, controllability, and explainability -- along with simulation as a\nsafe path to build and calibrate trust over time.", "AI": {"tldr": "DoubleAgents is an agentic planning tool aimed at enhancing user trust in automated systems through transparency and control.", "motivation": "The motivation behind DoubleAgents is to increase the efficiency of agentic workflows and ensure user trust in systems acting on their behalf.", "method": "The paper presents a tool that incorporates user intervention, value-reflecting policies, rich state visualizations, and uncertainty flagging, alongside a respondent simulation for realistic scenario testing.", "result": "Participants showed initial hesitation to delegate actions to the system but developed greater trust and reliance after experiencing its transparency and control features during training sessions.", "conclusion": "DoubleAgents successfully demonstrates how to nurture user trust in AI systems through design patterns that emphasize consistency, controllability, and explainability, with simulation serving as a safe training method.", "key_contributions": ["Trust-by-design patterns for AI systems", "Mechanisms for transparency, control, and explainability", "Simulation techniques for building user trust over time"], "limitations": "Study sample size was small (n=10 for lab study; n=2 for deployments), limiting generalizability.", "keywords": ["Agentic workflows", "Trust in AI", "User intervention", "Simulation", "Human coordination"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2509.12405", "pdf": "https://arxiv.org/pdf/2509.12405.pdf", "abs": "https://arxiv.org/abs/2509.12405", "title": "MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering", "authors": ["Wen-wai Yim", "Asma Ben Abacha", "Zixuan Yu", "Robert Doerning", "Fei Xia", "Meliha Yetisgen"], "categories": ["cs.CL", "68T50 (Primary) 68T45 (Secondary)", "I.2.7; I.2.10"], "comment": "9 pages, 8 tables", "summary": "Evaluating natural language generation (NLG) systems in the medical domain\npresents unique challenges due to the critical demands for accuracy, relevance,\nand domain-specific expertise. Traditional automatic evaluation metrics, such\nas BLEU, ROUGE, and BERTScore, often fall short in distinguishing between\nhigh-quality outputs, especially given the open-ended nature of medical\nquestion answering (QA) tasks where multiple valid responses may exist. In this\nwork, we introduce MORQA (Medical Open-Response QA), a new multilingual\nbenchmark designed to assess the effectiveness of NLG evaluation metrics across\nthree medical visual and text-based QA datasets in English and Chinese. Unlike\nprior resources, our datasets feature 2-4+ gold-standard answers authored by\nmedical professionals, along with expert human ratings for three English and\nChinese subsets. We benchmark both traditional metrics and large language model\n(LLM)-based evaluators, such as GPT-4 and Gemini, finding that LLM-based\napproaches significantly outperform traditional metrics in correlating with\nexpert judgments. We further analyze factors driving this improvement,\nincluding LLMs' sensitivity to semantic nuances and robustness to variability\namong reference answers. Our results provide the first comprehensive,\nmultilingual qualitative study of NLG evaluation in the medical domain,\nhighlighting the need for human-aligned evaluation methods. All datasets and\nannotations will be publicly released to support future research.", "AI": {"tldr": "This paper introduces MORQA, a multilingual benchmark for evaluating natural language generation systems in the medical domain, demonstrating the superiority of large language models over traditional metrics.", "motivation": "The accuracy and relevance of natural language generation (NLG) systems in the medical domain are crucial, necessitating better evaluation methods than traditional automatic metrics, which fail to distinguish quality in medical QA tasks.", "method": "The study presents MORQA, a multilingual benchmark featuring 2-4+ gold-standard answers from medical professionals and expert human ratings for English and Chinese datasets. It benchmarks traditional metrics against LLM-based evaluators like GPT-4 and Gemini.", "result": "LLM-based evaluators significantly outperform traditional metrics in correlating with expert judgments, illustrating their effectiveness in handling the nuances of medical QA tasks.", "conclusion": "The findings advocate for the use of LLM-based evaluation methods over conventional metrics, underscoring the importance of human-aligned evaluation in medical NLG systems.", "key_contributions": ["Introduction of MORQA, a multilingual benchmark for medical NLG evaluation.", "Demonstration of LLM-based evaluators' superiority in correlation with expert judgments over traditional metrics.", "Public release of datasets and annotations to aid future research in NLG evaluation."], "limitations": "Evaluation metrics may still have room for improvement, potential biases in human judgments, and the study is limited to English and Chinese.", "keywords": ["natural language generation", "medical QA", "LLM evaluation", "benchmark", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.12709", "pdf": "https://arxiv.org/pdf/2509.12709.pdf", "abs": "https://arxiv.org/abs/2509.12709", "title": "Harnessing the Power of AI in Qualitative Research: Role Assignment, Engagement, and User Perceptions of AI-Generated Follow-Up Questions in Semi-Structured Interviews", "authors": ["He Zhang", "Yueyan Liu", "Xin Guan", "Jie Cai", "John M. Carroll"], "categories": ["cs.HC"], "comment": "19 pages, 8 figures", "summary": "Semi-structured interviews highly rely on the quality of follow-up questions,\nyet interviewers' knowledge and skills may limit their depth and potentially\naffect outcomes. While many studies have shown the usefulness of large language\nmodels (LLMs) for qualitative analysis, their possibility in the data\ncollection process remains underexplored. We adopt an AI-driven \"Wizard-of-Oz\"\nsetup to investigate how real-time LLM support in generating follow-up\nquestions shapes semi-structured interviews. Through a study with 17\nparticipants, we examine the value of LLM-generated follow-up questions, the\nevolving division of roles, relationships, collaborative behaviors, and\nresponsibilities between interviewers and AI. Our findings (1) provide\nempirical evidence of the strengths and limitations of AI-generated follow-up\nquestions (AGQs); (2) introduce a Human-AI collaboration framework in this\ninterview context; and (3) propose human-centered design guidelines for\nAI-assisted interviewing. We position LLMs as complements, not replacements, to\nhuman judgment, and highlight pathways for integrating AI into qualitative data\ncollection.", "AI": {"tldr": "This paper explores the use of large language models (LLMs) to generate follow-up questions in semi-structured interviews, examining the resulting impacts on interview dynamics and the roles between human interviewers and AI.", "motivation": "To investigate the potential of LLMs in enhancing the quality of follow-up questions in semi-structured interviews, an aspect crucial to qualitative data collection yet often limited by interviewers' skills.", "method": "The study employs an AI-driven 'Wizard-of-Oz' setup, involving 17 participants to assess the effects of real-time LLM-generated follow-up questions on the interview process.", "result": "The findings reveal both advantages and limitations of using AI-generated follow-up questions and offer insights into the collaborative dynamics between interviewers and AI.", "conclusion": "LLMs should be viewed as complements to human judgment in qualitative research, with suggested guidelines for human-centered design in AI-assisted interviews.", "key_contributions": ["Empirical evidence on the strengths and weaknesses of AI-generated follow-up questions.", "A Human-AI collaboration framework in semi-structured interviews.", "Human-centered design guidelines for integrating AI into qualitative data collection."], "limitations": "The paper does not explore the long-term effects of LLM use in various interviewing contexts or across different interview types.", "keywords": ["Human-Computer Interaction", "Large Language Models", "Qualitative Research", "AI-assisted interviewing", "Human-AI collaboration"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2509.12440", "pdf": "https://arxiv.org/pdf/2509.12440.pdf", "abs": "https://arxiv.org/abs/2509.12440", "title": "MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts", "authors": ["Jiayi He", "Yangmin Huang", "Qianyun Du", "Xiangying Zhou", "Zhiyang He", "Jiaxue Hu", "Xiaodong Tao", "Lixian Lai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing deployment of Large Language Models (LLMs) in healthcare\nnecessitates a rigorous evaluation of their factual reliability. However,\nexisting benchmarks are often limited by narrow domains of data, failing to\ncapture the complexity of real-world medical information. To address this\ncritical gap, we introduce MedFact, a new and challenging benchmark for Chinese\nmedical fact-checking. MedFact comprises 2,116 expert-annotated instances\ncurated from diverse real-world texts, spanning 13 medical specialties, 8\nfine-grained error types, 4 writing styles, and multiple difficulty levels. Its\nconstruction employs a hybrid AI-human framework where iterative expert\nfeedback refines an AI-driven, multi-criteria filtering process, ensuring both\nhigh data quality and difficulty. We conduct a comprehensive evaluation of 20\nleading LLMs, benchmarking their performance on veracity classification and\nerror localization against a human expert baseline. Our results reveal that\nwhile models can often determine if a text contains an error, precisely\nlocalizing it remains a substantial challenge, with even top-performing models\nfalling short of human performance. Furthermore, our analysis uncovers a\nfrequent ``over-criticism'' phenomenon, a tendency for models to misidentify\ncorrect information as erroneous, which is exacerbated by advanced reasoning\ntechniques such as multi-agent collaboration and inference-time scaling. By\nhighlighting these critical challenges for deploying LLMs in medical\napplications, MedFact provides a robust resource to drive the development of\nmore factually reliable and medically aware models.", "AI": {"tldr": "Introduction of MedFact, a benchmark for evaluating the factual reliability of LLMs in healthcare, featuring 2,116 expert-annotated instances across various medical domains.", "motivation": "To address the gap in existing benchmarks that fail to capture the complexity of real-world medical information, particularly in the context of LLMs.", "method": "MedFact was constructed using a hybrid AI-human framework that includes iterative expert feedback to refine a multi-criteria filtering process, ensuring high data quality and varied difficulty.", "result": "Evaluation of 20 leading LLMs revealed models can identify errors but struggle with error localization; the 'over-criticism' phenomenon was also noted where correct information is incorrectly labeled as erroneous.", "conclusion": "MedFact serves as a crucial resource for improving the factually reliability of LLMs in medical applications.", "key_contributions": ["Introduction of the MedFact benchmark for Chinese medical fact-checking.", "Evaluation of LLMs reveals significant challenges in error localization.", "Highlights the over-criticism phenomenon in LLM outputs."], "limitations": "Limited to Chinese medical information and focused on specific error types and specialties.", "keywords": ["Large Language Models", "healthcare", "fact-checking", "benchmark", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.12752", "pdf": "https://arxiv.org/pdf/2509.12752.pdf", "abs": "https://arxiv.org/abs/2509.12752", "title": "Participatory AI: A Scandinavian Approach to Human-Centered AI", "authors": ["Niklas Elmqvist", "Eve Hoggan", "Hans-Jörg Schulz", "Marianne Graves Petersen", "Peter Dalsgaard", "Ira Assent", "Olav W. Bertelsen", "Akhil Arora", "Kaj Grønbæk", "Susanne Bødker", "Clemens Nylandsted Klokmose", "Rachel Charlotte Smith", "Sebastian Hubenschmid", "Christoph A. Johns", "Gabriela Molina León", "Anton Wolter", "Johannes Ellemose", "Vaishali Dhanoa", "Simon Aagaard Enni", "Mille Skovhus Lunding", "Karl-Emil Kjær Bilstrup", "Juan Sánchez Esquivel", "Luke Connelly", "Rafael Pablos Sarabia", "Morten Birk", "Joachim Nyborg", "Stefanie Zollmann", "Tobias Langlotz", "Meredith Siang-Yun Chou", "Jens Emil Sloth Grønbæk", "Michael Wessely", "Yijing Jiang", "Caroline Berger", "Duosi Dai", "Michael Mose Biskjaer", "Germán Leiva", "Jonas Frich", "Eva Eriksson", "Kim Halskov", "Thorbjørn Mikkelsen", "Nearchos Potamitis", "Michel Yildirim", "Arvind Srinivasan", "Jeanette Falk", "Nanna Inie", "Ole Sejer Iversen", "Hugo Andersson"], "categories": ["cs.HC", "H.5.2; H.1.2"], "comment": "32 pages, 7 figures", "summary": "AI's transformative impact on work, education, and everyday life makes it as\nmuch a political artifact as a technological one. Current AI models are opaque,\ncentralized, and overly generic. The algorithmic automation they provide\nthreatens human agency and democratic values in both workplaces and daily life.\nTo confront such challenges, we turn to Scandinavian Participatory Design (PD),\nwhich was devised in the 1970s to face a similar threat from mechanical\nautomation. In the PD tradition, technology is seen not just as an artifact,\nbut as a locus of democracy. Drawing from this tradition, we propose\nParticipatory AI as a PD approach to human-centered AI that applies five PD\nprinciples to four design challenges for algorithmic automation. We use\nconcrete case studies to illustrate how to treat AI models less as proprietary\nproducts and more as shared socio-technical systems that enhance rather than\ndiminish human agency, human dignity, and human values.", "AI": {"tldr": "This paper proposes Participatory AI as a human-centered design approach to counter the challenges posed by algorithmic automation, using principles from Scandinavian Participatory Design.", "motivation": "To address the threats to human agency and democratic values posed by opaque and centralized AI models in work and everyday life.", "method": "The paper draws from the Scandinavian Participatory Design tradition, applying its five principles to tackle four design challenges related to algorithmic automation.", "result": "The authors illustrate how AI can be viewed as a shared socio-technical system through case studies, enhancing human agency and values rather than diminishing them.", "conclusion": "Participatory AI can transform AI from proprietary products into democratic tools that support human dignity and agency.", "key_contributions": ["Introduction of Participatory AI as a human-centered design approach", "Application of PD principles to algorithmic automation design challenges", "Case studies demonstrating AI as socio-technical systems"], "limitations": "", "keywords": ["Participatory Design", "Human-Centered AI", "Algorithmic Automation", "Democratic Values", "Socio-Technical Systems"], "importance_score": 7, "read_time_minutes": 32}}
{"id": "2509.12451", "pdf": "https://arxiv.org/pdf/2509.12451.pdf", "abs": "https://arxiv.org/abs/2509.12451", "title": "Topic Coverage-based Demonstration Retrieval for In-Context Learning", "authors": ["Wonbin Kweon", "SeongKu Kang", "Runchu Tian", "Pengcheng Jiang", "Jiawei Han", "Hwanjo Yu"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "The effectiveness of in-context learning relies heavily on selecting\ndemonstrations that provide all the necessary information for a given test\ninput. To achieve this, it is crucial to identify and cover fine-grained\nknowledge requirements. However, prior methods often retrieve demonstrations\nbased solely on embedding similarity or generation probability, resulting in\nirrelevant or redundant examples. In this paper, we propose TopicK, a topic\ncoverage-based retrieval framework that selects demonstrations to\ncomprehensively cover topic-level knowledge relevant to both the test input and\nthe model. Specifically, TopicK estimates the topics required by the input and\nassesses the model's knowledge on those topics. TopicK then iteratively selects\ndemonstrations that introduce previously uncovered required topics, in which\nthe model exhibits low topical knowledge. We validate the effectiveness of\nTopicK through extensive experiments across various datasets and both open- and\nclosed-source LLMs. Our source code is available at\nhttps://github.com/WonbinKweon/TopicK_EMNLP2025.", "AI": {"tldr": "The paper introduces TopicK, a framework for selecting demonstrations in in-context learning based on topic coverage to improve model performance.", "motivation": "There is a need for effective selection of demonstrations in in-context learning to enhance model performance by covering fine-grained knowledge requirements.", "method": "TopicK estimates the topic requirements of input and assesses the model's knowledge on those topics, iteratively selecting demonstrations that cover previously uncovered required topics.", "result": "TopicK was validated through extensive experiments, showing improved performance across various datasets and LLMs.", "conclusion": "The TopicK framework demonstrates a significant enhancement in demonstration selection by focusing on topic coverage, thereby improving in-context learning effectiveness.", "key_contributions": ["Introduces a novel framework for topic coverage-based demonstration retrieval", "Establishes a methodology for estimating topic requirements and model knowledge", "Demonstrates effectiveness through empirical validation on diverse datasets"], "limitations": "The study might be limited by the datasets used for validation and the specific LLMs examined.", "keywords": ["in-context learning", "demonstration selection", "topic coverage", "large language models", "knowledge retrieval"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2509.12773", "pdf": "https://arxiv.org/pdf/2509.12773.pdf", "abs": "https://arxiv.org/abs/2509.12773", "title": "PLUTO: A Public Value Assessment Tool", "authors": ["Laura Koesten", "Péter Ferenc Gyarmati", "Connor Hogan", "Bernhard Jordan", "Seliem El-Sayed", "Barbara Prainsack", "Torsten Möller"], "categories": ["cs.HC"], "comment": null, "summary": "We present PLUTO (Public VaLUe Assessment TOol), a framework for assessing\nthe public value of specific instances of data use. Grounded in the concept of\ndata solidarity, PLUTO aims to empower diverse stakeholders - including\nregulatory bodies, private enterprises, NGOs, and individuals - to critically\nengage with data projects through a structured assessment of the risks and\nbenefits of data use, and by encouraging critical reflection. This paper\ndiscusses the theoretical foundation, development process, and initial user\nexperiences with PLUTO. Key challenges include translating qualitative\nassessments of benefits and risks into actionable quantitative metrics while\nmaintaining inclusivity and transparency. Initial feedback highlights PLUTO's\npotential to foster responsible decision-making and shared accountability in\ndata practices.", "AI": {"tldr": "PLUTO is a framework aimed at assessing the public value of data use, focusing on stakeholder engagement through qualitative and quantitative risk-benefit evaluations.", "motivation": "To empower stakeholders to engage critically with data projects and assess the risks and benefits involved in data use.", "method": "Development of PLUTO involved a theoretical foundation grounded in data solidarity, along with a focus on creating a structured assessment framework for diverse stakeholders.", "result": "PLUTO seeks to provide actionable metrics from qualitative assessments, highlighting initial user experiences and feedback on its implementation.", "conclusion": "PLUTO shows promise in promoting responsible decision-making and accountability in data practices, though challenges remain in balancing qualitative and quantitative assessments.", "key_contributions": ["Introduction of the PLUTO framework for data assessment", "Focus on stakeholder inclusivity in data evaluation", "Highlighting initial user experiences and feedback"], "limitations": "Challenges in translating qualitative assessments into quantitative metrics while maintaining inclusivity and transparency.", "keywords": ["data solidarity", "public value", "data assessment", "stakeholder engagement", "data practices"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.12459", "pdf": "https://arxiv.org/pdf/2509.12459.pdf", "abs": "https://arxiv.org/abs/2509.12459", "title": "Does Language Model Understand Language?", "authors": ["Suvojit Acharjee", "Utathya Aich", "Asfak Ali"], "categories": ["cs.CL"], "comment": null, "summary": "Despite advances in natural language generation and understanding, LM still\nstruggle with fine grained linguistic phenomena such as tense, negation, voice,\nand modality which are the elements central to effective human communication.\nIn the context of the United Nations SDG 4, where linguistic clarity is\ncritical, the deployment of LMs in educational technologies demands careful\nscrutiny. As LMs are increasingly powering applications like tutoring systems,\nautomated grading, and translation, their alignment with human linguistic\ninterpretation becomes essential for effective learning. In this study, we\nconduct a evaluation of SOTA language models across these challenging contexts\nin both English and Bengali. To ensure a structured assessment, we introduce a\nnew Route for Evaluation of Cognitive Inference in Systematic Environments\nguidelines. Our proposed LUCID dataset, composed of carefully crafted sentence\npairs in English and Bengali, specifically challenges these models on critical\naspects of language comprehension, including negation, tense, voice variations.\nWe assess the performance of SOTA models including MISTRAL-SABA-24B,\nLLaMA-4-Scout-17B, LLaMA-3.3-70B, Gemma2-9B, and Compound-Beta using standard\nmetrics like Pearson correlation, Spearman correlation, and Mean Absolute\nError, as well as novel, linguistically inspired metric the HCE accuracy. The\nHCE accuracy measures how often model predictions fall within one standard\ndeviation of the mean human rating, thus capturing human like tolerance for\nvariability in language interpretation. Our findings highlight Compound-Beta as\nthe most balanced model, consistently achieving high correlations and low MAEs\nacross diverse language conditions. It records the highest Pearson correlation\nin English and demonstrates robust performance on mixed-language data,\nindicating a strong alignment with human judgments in cross lingual scenarios.", "AI": {"tldr": "This study evaluates state-of-the-art language models on their ability to process complex linguistic phenomena critical for human communication in educational contexts, using a new dataset and innovative evaluation metrics.", "motivation": "The gap in fine-grained linguistic comprehension by language models hinders effective human communication, especially in educational technologies aligned with United Nations SDG 4 requirements.", "method": "We introduce a new evaluation framework (Route for Evaluation of Cognitive Inference in Systematic Environments) and the LUCID dataset, which challenges language models on aspects like tense, negation, and voice. We assess models using standard metrics and a novel HCE accuracy metric.", "result": "Our evaluation shows Compound-Beta is the best performing model, achieving high correlations and low error rates, particularly in English and mixed-language scenarios.", "conclusion": "The findings suggest that careful assessment of language models can reveal their strengths and weaknesses in understanding nuanced linguistic elements, with implications for educational technologies.", "key_contributions": ["Development of the LUCID dataset for linguistic evaluation.", "Introduction of the HCE accuracy metric for assessing model predictions against human judgment.", "Evaluation of multiple state-of-the-art language models on complex linguistic tasks."], "limitations": "Focus primarily on English and Bengali, which may limit generalizability to other languages or contexts.", "keywords": ["Natural Language Processing", "Language Models", "Education Technologies", "Cognitive Inference", "Linguistic Clarity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.12794", "pdf": "https://arxiv.org/pdf/2509.12794.pdf", "abs": "https://arxiv.org/abs/2509.12794", "title": "The Impact of Automation on Risk-Taking: The Role of Sense of Agency", "authors": ["Yang Chen", "Zhijun Zhang"], "categories": ["cs.HC"], "comment": "37 pages, 9 figures", "summary": "Automation significantly alters human behavior, particularly risk-taking.\nPrevious researches have paid limited attention to the underlying\ncharacteristics of automation and their mechanisms of influence on risk-taking.\nThis study investigated how automation affects risk-taking and examined the\nrole of sense of agency therein. By quantifying sense of agency through\nsubjective ratings, this research explored the impact of automation level and\nreliability level on risk-taking. The results of three experiments indicated\nthat automation reduced the level of risk-taking; higher automation level was\nassociated with lower sense of agency and lower risk-taking, with sense of\nagency playing a complete mediating role; higher automation reliability was\nassociated with higher sense of agency and higher risk-taking, with sense of\nagency playing a partial mediating role. The study concludes that automation\ninfluences risk-taking, such that higher automation level or lower reliability\nis associated with a lower likelihood of risk-taking. Sense of agency mediates\nthe impact of automation on risk-taking, and automation level and reliability\nhave different effects on risk-taking.", "AI": {"tldr": "This study explores how automation influences risk-taking behaviors and the role of sense of agency in this relationship through three experiments.", "motivation": "To understand the relationship between automation and risk-taking, and how sense of agency mediates this relationship.", "method": "Quantitative experiments measuring the sense of agency through subjective ratings while varying automation level and reliability.", "result": "The experiments found that higher automation levels decrease risk-taking and sense of agency, while higher reliability increases both risk-taking and sense of agency.", "conclusion": "Automation significantly affects risk-taking behaviors, with the sense of agency mediating these effects differently based on the level and reliability of automation.", "key_contributions": ["Identifies the mediating role of sense of agency in risk-taking under automation", "Demonstrates different effects of automation level and reliability on risk-taking", "Provides empirical evidence through experimental studies"], "limitations": "", "keywords": ["automation", "risk-taking", "sense of agency", "human behavior", "experimentation"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.12476", "pdf": "https://arxiv.org/pdf/2509.12476.pdf", "abs": "https://arxiv.org/abs/2509.12476", "title": "Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction", "authors": ["Sumanta Bhattacharyya", "Sara Riaz", "Pedram Rooshenas"], "categories": ["cs.CL"], "comment": null, "summary": "Training a task-specific small reasoning model is challenging when direct\nhuman supervision or high-quality labels are scarce. However, LLMs with\nreasoning capabilities produce abundant intermediate reasoning traces that can\nbe systematically refined to create effective supervision signals. We propose\nReason-Refine-then-Align (R2tA), which turns refined model rationales into\nsupervision for training task-specific reasoning models. Our method generates\ninitial reasoning and responses from an open-source base model on task-specific\ninputs, then refines these traces, fixing hallucinations and inconsistencies,\nto form a high-fidelity dataset. We perform a two-stage alignment, supervised\nfine-tuning (SFT), followed by direct preference optimization (DPO) to\ncalibrate the model's intermediate reasoning with human-validated conceptual\npreferences and then condition the final output on that aligned reasoning. As a\ncase study, we apply R2tA to evaluate extended entity relationship diagrams\n(EERDs) in database system design, a structurally complex task where\nprompt-only methods miss or hallucinate errors. We curated a dataset of 600\nEERD variants (train/test split of 450/150, respectively) with induced mistakes\nspanning 11 categories. Empirical evaluation suggests R2tA provides a\npractical, cost-effective path to scalable LLM adaptation in data-scarce\ndomains, enabling reproducible AI tools for education and beyond.", "AI": {"tldr": "The paper presents a novel method called R2tA for enhancing task-specific reasoning models by refining reasoning traces from LLMs to create high-quality supervision signals, particularly in data-scarce domains.", "motivation": "The challenge of training effective reasoning models in situations with limited supervision or high-quality labels motivates the development of R2tA, which leverages LLM reasoning traces for model training.", "method": "R2tA involves generating reasoning and responses from a base model, refining these traces to address errors, and employing a two-stage alignment process that includes supervised fine-tuning followed by preference optimization.", "result": "Empirical evaluations indicate that R2tA offers a robust and efficient approach to adapt LLMs in scarce data environments, significantly improving output quality in tasks like evaluating EERDs.", "conclusion": "R2tA demonstrates that refined reasoning traces can effectively inform and enhance training processes for AI models in educational contexts and more.", "key_contributions": ["Introduction of R2tA for improved supervision in reasoning model training", "Development of a dataset with induced mistakes for method evaluation", "Demonstration of scalability in LLM adaptation for education through empirical studies"], "limitations": "The method's effectiveness may vary based on the initial quality of the LLM and the specific tasks it is applied to.", "keywords": ["machine learning", "reasoning models", "supervision", "LLM adaptation", "human-validated preferences"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12816", "pdf": "https://arxiv.org/pdf/2509.12816.pdf", "abs": "https://arxiv.org/abs/2509.12816", "title": "Gesture Evaluation in Virtual Reality", "authors": ["Axel Wiebe Werner", "Jonas Beskow", "Anna Deichler"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.LG", "68T50, 68T07, 68U35", "H.5.1; H.5.2; I.2.10; I.3.7"], "comment": "Published in Proceedings of the 26th International Conference on\n  Multimodal Interaction (ICMI '24), ACM. Copyright 2024 ACM. Licensed under CC\n  BY", "summary": "Gestures are central to human communication, enriching interactions through\nnon-verbal expression. Virtual avatars increasingly use AI-generated gestures\nto enhance life-likeness, yet evaluations have largely been confined to 2D.\nVirtual Reality (VR) provides an immersive alternative that may affect how\ngestures are perceived. This paper presents a comparative evaluation of\ncomputer-generated gestures in VR and 2D, examining three models from the 2023\nGENEA Challenge. Results show that gestures viewed in VR were rated slightly\nhigher on average, with the strongest effect observed for motion-capture \"true\nmovement.\" While model rankings remained consistent across settings, VR\ninfluenced participants' overall perception and offered unique benefits over\ntraditional 2D evaluation.", "AI": {"tldr": "This paper evaluates AI-generated gestures in Virtual Reality (VR) versus 2D settings, highlighting the advantages of immersive environments.", "motivation": "To explore the impact of different environments on the perception of AI-generated gestures and enhance human-computer interaction.", "method": "A comparative evaluation was conducted using three models from the 2023 GENEA Challenge, assessing how gestures were perceived in VR and 2D contexts.", "result": "Gestures in VR received higher ratings on average, particularly highlighting the effectiveness of motion-capture 'true movement' gestures.", "conclusion": "VR positively influences participants' perceptions of gestures, providing unique advantages over traditional 2D evaluations.", "key_contributions": ["Comparative evaluation of gesture perception in VR vs. 2D", "Highlighting the effectiveness of motion-capture gestures", "Demonstrating VR's impact on overall gesture perception"], "limitations": "", "keywords": ["gestures", "Virtual Reality", "human-computer interaction", "AI", "multimodal interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12508", "pdf": "https://arxiv.org/pdf/2509.12508.pdf", "abs": "https://arxiv.org/abs/2509.12508", "title": "FunAudio-ASR Technical Report", "authors": ["Keyu An", "Yanni Chen", "Chong Deng", "Changfeng Gao", "Zhifu Gao", "Bo Gong", "Xiangang Li", "Yabin Li", "Xiang Lv", "Yunjie Ji", "Yiheng Jiang", "Bin Ma", "Haoneng Luo", "Chongjia Ni", "Zexu Pan", "Yiping Peng", "Zhendong Peng", "Peiyao Wang", "Hao Wang", "Wen Wang", "Wupeng Wang", "Biao Tian", "Zhentao Tan", "Nan Yang", "Bin Yuan", "Jieping Ye", "Jixing Yu", "Qinglin Zhang", "Kun Zou", "Han Zhao", "Shengkui Zhao", "Jingren Zhou"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "In recent years, automatic speech recognition (ASR) has witnessed\ntransformative advancements driven by three complementary paradigms: data\nscaling, model size scaling, and deep integration with large language models\n(LLMs). However, LLMs are prone to hallucination, which can significantly\ndegrade user experience in real-world ASR applications. In this paper, we\npresent FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically\ncombines massive data, large model capacity, LLM integration, and reinforcement\nlearning to achieve state-of-the-art performance across diverse and complex\nspeech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized\nfor practical deployment, with enhancements in streaming capability, noise\nrobustness, code-switching, hotword customization, and satisfying other\nreal-world application requirements. Experimental results show that while most\nLLM-based ASR systems achieve strong performance on open-source benchmarks,\nthey often underperform on real industry evaluation sets. Thanks to\nproduction-oriented optimizations, FunAudio-ASR achieves SOTA performance on\nreal application datasets, demonstrating its effectiveness and robustness in\npractical settings.", "AI": {"tldr": "FunAudio-ASR is an advanced LLM-based ASR system designed to enhance performance in practical applications through optimization in key areas.", "motivation": "To address the hallucination issues in LLMs that degrade user experience in real-world ASR applications.", "method": "The paper introduces FunAudio-ASR, leveraging data scaling, large model capacity, LLM integration, and reinforcement learning for ASR.", "result": "FunAudio-ASR demonstrates state-of-the-art performance in practical settings, outperforming traditional LLM-based ASR systems on industry evaluation sets.", "conclusion": "FunAudio-ASR's production-oriented optimizations make it highly effective for real-world ASR applications, achieving strong performance in various scenarios.", "key_contributions": ["Introduction of FunAudio-ASR combining LLMs and advanced ASR techniques.", "Optimizations for streaming capability and noise robustness.", "Demonstration of SOTA performance on real-world datasets."], "limitations": "", "keywords": ["automatic speech recognition", "large language models", "reinforcement learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.13039", "pdf": "https://arxiv.org/pdf/2509.13039.pdf", "abs": "https://arxiv.org/abs/2509.13039", "title": "Winds Through Time: Interactive Data Visualization and Physicalization for Paleoclimate Communication", "authors": ["David Hunter", "Pablo Botin", "Emily Snode-Brenneman", "Amy Stevermer", "Becca Hatheway", "Dillon Amaya", "Eddie Goldstein", "Wayne A Seltzer", "Mark D Gross", "Kris Karnauskas", "Daniel Leithinger", "Ellen Yi-Luen Do"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "We describe a multidisciplinary collaboration to iteratively design an\ninteractive exhibit for a public science center on paleoclimate, the study of\npast climates. We created a data physicalisation of mountains and ice sheets\nthat can be tangibly manipulated by visitors to interact with a wind simulation\nvisualisation that demonstrates how the climate of North America differed\ndramatically between now and the peak of the last ice age. We detail the system\nfor interaction and visualisation plus design choices to appeal to an audience\nthat ranges from children to scientists and responds to site requirements.", "AI": {"tldr": "This paper discusses the design of an interactive exhibit on paleoclimate for a science center, featuring a data physicalization that engages visitors through manipulation and visualization of climate changes.", "motivation": "To create an engaging way for a diverse audience to understand the significant climate changes from historical perspectives, particularly during the last ice age.", "method": "The design involved a collaborative, iterative process leading to the development of a physical model representing mountains and ice sheets combined with an interactive wind simulation.", "result": "The exhibit allows visitors to manipulate physical representations of climate data and see the effects of climate change in an intuitive way.", "conclusion": "The exhibit successfully engages a broad audience in learning about paleoclimate through interactive experiences that bridge complex scientific concepts and tangible interaction.", "key_contributions": ["Creation of an interactive physical model representing paleoclimate data.", "Methodology for designing exhibits that cater to varying levels of audience expertise.", "Demonstration of an effective way to visualize climate change impact through hands-on interaction."], "limitations": "", "keywords": ["paleoclimate", "interactive exhibit", "public science education", "data physicalization", "science communication"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.12514", "pdf": "https://arxiv.org/pdf/2509.12514.pdf", "abs": "https://arxiv.org/abs/2509.12514", "title": "A comparison of pipelines for the translation of a low resource language based on transformers", "authors": ["Chiara Bonfanti", "Michele Colombino", "Giulia Coucourde", "Faeze Memari", "Stefano Pinardi", "Rosa Meo"], "categories": ["cs.CL", "cs.CE", "cs.CY", "cs.LG"], "comment": "9 pages, 4 figures", "summary": "This work compares three pipelines for training transformer-based neural\nnetworks to produce machine translators for Bambara, a Mand\\`e language spoken\nin Africa by about 14,188,850 people. The first pipeline trains a simple\ntransformer to translate sentences from French into Bambara. The second\nfine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures\nfor French-to-Bambara translation. Models from the first two pipelines were\ntrained with different hyperparameter combinations to improve BLEU and chrF\nscores, evaluated on both test sentences and official Bambara benchmarks. The\nthird pipeline uses language distillation with a student-teacher dual neural\nnetwork to integrate Bambara into a pre-trained LaBSE model, which provides\nlanguage-agnostic embeddings. A BERT extension is then applied to LaBSE to\ngenerate translations. All pipelines were tested on Dokotoro (medical) and\nBayelemagaba (mixed domains). Results show that the first pipeline, although\nsimpler, achieves the best translation accuracy (10% BLEU, 21% chrF on\nBayelemagaba), consistent with low-resource translation results. On the Yiri\ndataset, created for this work, it achieves 33.81% BLEU and 41% chrF.\nInstructor-based models perform better on single datasets than on aggregated\ncollections, suggesting they capture dataset-specific patterns more\neffectively.", "AI": {"tldr": "This paper compares three transformer-based machine translation pipelines for translating French to Bambara, focusing on their effectiveness and evaluation metrics.", "motivation": "The study addresses the need for effective translation methods for Bambara, a low-resource language, by comparing different machine translation pipelines.", "method": "Three distinct pipelines were trained: a simple transformer model, fine-tuning LLaMA3 models, and a student-teacher model using language distillation with LaBSE.", "result": "The first pipeline outperformed the others in translation accuracy, yielding 10% BLEU and 21% chrF on the Bayelemagaba dataset, and 33.81% BLEU and 41% chrF on the Yiri dataset.", "conclusion": "Despite the simplicity, the first pipeline achieved the best results, indicating that simpler models can outperform more complex ones in low-resource translation tasks.", "key_contributions": ["Comparison of three translation pipelines for a low-resource language.", "Introduction of a new dataset for evaluating translation accuracy.", "Insights into the effectiveness of instructor-based models on specific datasets."], "limitations": "The models were only tested on specific datasets and their performance may vary with different texts or domains.", "keywords": ["machine translation", "transformer models", "Bambara language", "low-resource language", "language distillation"], "importance_score": 3, "read_time_minutes": 9}}
{"id": "2509.13051", "pdf": "https://arxiv.org/pdf/2509.13051.pdf", "abs": "https://arxiv.org/abs/2509.13051", "title": "More than Meets the Eye: Understanding the Effect of Individual Objects on Perceived Visual Privacy", "authors": ["Mete Harun Akcay", "Siddharth Prakash Rao", "Alexandros Bakas", "Buse Gul Atli"], "categories": ["cs.HC"], "comment": "27 pages, 5 figures, 11 tables. In submission", "summary": "User-generated content, such as photos, comprises the majority of online\nmedia content and drives engagement due to the human ability to process visual\ninformation quickly. Consequently, many online platforms are designed for\nsharing visual content, with billions of photos posted daily. However, photos\noften reveal more than they intended through visible and contextual cues,\nleading to privacy risks. Previous studies typically treat privacy as a\nproperty of the entire image, overlooking individual objects that may carry\nvarying privacy risks and influence how users perceive it. We address this gap\nwith a mixed-methods study (n = 92) to understand how users evaluate the\nprivacy of images containing multiple sensitive objects. Our results reveal\nmental models and nuanced patterns that uncover how granular details, such as\nphoto-capturing context and co-presence of other objects, affect privacy\nperceptions. These novel insights could enable personalized, context-aware\nprivacy protection designs on social media and future technologies.", "AI": {"tldr": "This study investigates how users evaluate the privacy of images with multiple sensitive objects, revealing nuanced patterns and mental models related to privacy perceptions.", "motivation": "To address the gap in understanding how individual objects within an image influence privacy risks and user perceptions, as previous research often treated privacy as a property of the entire image.", "method": "A mixed-methods study was conducted with 92 participants to explore the evaluation of privacy in images containing multiple sensitive objects.", "result": "The study uncovers how contextual details and the co-presence of objects affect users' privacy perceptions, indicating that privacy is perceived differently depending on specific elements in the image.", "conclusion": "These insights could inform the design of personalized and context-aware privacy protection features in social media and technology.", "key_contributions": ["Identification of mental models related to privacy perception.", "Nuanced understanding of privacy risks posed by individual objects in images.", "Implications for developing better privacy protection designs in visual content sharing."], "limitations": "", "keywords": ["privacy", "user-generated content", "image evaluation", "context-aware systems", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.12591", "pdf": "https://arxiv.org/pdf/2509.12591.pdf", "abs": "https://arxiv.org/abs/2509.12591", "title": "MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models", "authors": ["Vijay Govindarajan", "Pratik Patel", "Sahil Tripathi", "Md Azizul Hoque", "Gautam Siddharth Kashyap"], "categories": ["cs.CL"], "comment": "Accepted in The 26th International Conference on Web Information\n  Systems Engineering (WISE), scheduled for 15-17 December 2025 in Marrakech,\n  Morocco", "summary": "Automated Audio Captioning (AAC) generates captions for audio clips but faces\nchallenges due to limited datasets compared to image captioning. To overcome\nthis, we propose the zero-shot AAC system that leverages pre-trained models,\neliminating the need for extensive training. Our approach uses a pre-trained\naudio CLIP model to extract auditory features and generate a structured prompt,\nwhich guides a Large Language Model (LLM) in caption generation. Unlike\ntraditional greedy decoding, our method refines token selection through the\naudio CLIP model, ensuring alignment with the audio content. Experimental\nresults demonstrate a 35% improvement in NLG mean score (from 4.7 to 7.3) using\nMAGIC search with the WavCaps model. The performance is heavily influenced by\nthe audio-text matching model and keyword selection, with optimal results\nachieved using a single keyword prompt, and a 50% performance drop when no\nkeyword list is used.", "AI": {"tldr": "This paper presents a zero-shot Automated Audio Captioning (AAC) system that utilizes pre-trained models to generate captions for audio clips without extensive training.", "motivation": "The paper addresses the challenges of Automated Audio Captioning due to the scarcity of datasets compared to image captioning.", "method": "The proposed system employs a pre-trained audio CLIP model to extract auditory features and generate a structured prompt for a Large Language Model (LLM) to produce captions, refining token selection via the audio CLIP model.", "result": "Experimental results indicate a 35% improvement in the NLG mean score, increasing from 4.7 to 7.3 using MAGIC search with the WavCaps model.", "conclusion": "The effectiveness of the system is significantly affected by the audio-text matching model and keyword selection, achieving optimal results with a single keyword prompt.", "key_contributions": ["Introduction of a zero-shot AAC system", "Use of audio CLIP model for feature extraction", "Demonstrated performance improvements over traditional methods"], "limitations": "Performance heavily depends on audio-text matching and keyword selection, with a notable drop when no keywords are used.", "keywords": ["Automated Audio Captioning", "Zero-shot learning", "Large Language Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.13064", "pdf": "https://arxiv.org/pdf/2509.13064.pdf", "abs": "https://arxiv.org/abs/2509.13064", "title": "Patient Perspectives on Telemonitoring during Colorectal Cancer Surgery Prehabilitation", "authors": ["Irina Bianca Serban", "Dimitra Dritsa", "David ten Cate", "Loes Janssen", "Margot Heijmans", "Sara Colombo", "Aarnout Brombacher", "Steven Houben"], "categories": ["cs.HC"], "comment": "20 pages, 3 figures, presented at the 19th EAI International\n  Conference on Pervasive Computing Technologies for Healthcare, to be\n  published in the Springer - LNICST series", "summary": "Multimodal prehabilitation for colorectal cancer (CRC) surgery aims to\noptimize patient fitness and reduce postoperative complications. While\ntelemonitoring's clinical value in supporting decision-making is recognized,\npatient perspectives on its use in prehabilitation remain underexplored,\nparticularly compared to its related clinical context, rehabilitation. To\naddress this gap, we conducted interviews with five patients who completed a\nfour-week CRC prehabilitation program incorporating continuous telemonitoring.\nOur findings reveal patients' willingness to engage with telemonitoring, shaped\nby their motivations, perceived benefits, and concerns. We outline design\nconsiderations for patient-centered systems and offer a foundation for further\nresearch on telemonitoring in CRC prehabilitation.", "AI": {"tldr": "This paper explores patient perspectives on telemonitoring in colorectal cancer prehabilitation, revealing motivations, benefits, and design considerations for patient-centered systems.", "motivation": "Understanding patient perspectives on telemonitoring during prehabilitation to improve system design and patient outcomes in colorectal cancer care.", "method": "Interviews with five patients who participated in a four-week prehabilitation program with continuous telemonitoring were conducted to gather insights on their experiences.", "result": "Patients were generally willing to engage with telemonitoring, influenced by their motivations, perceived benefits, and concerns, which can inform future system designs.", "conclusion": "The findings highlight the need for patient-centered design considerations in telemonitoring for prehabilitation and set the groundwork for future research in this area.", "key_contributions": ["Identified key factors influencing patient engagement with telemonitoring in prehabilitation", "Provided insights into patient motivations and concerns", "Outlined design considerations for patient-centered telemonitoring systems"], "limitations": "The study is based on a small sample size of five patients, which may not represent the broader population.", "keywords": ["telemonitoring", "colorectal cancer", "prehabilitation", "patient-centered design", "health informatics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.12603", "pdf": "https://arxiv.org/pdf/2509.12603.pdf", "abs": "https://arxiv.org/abs/2509.12603", "title": "EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving", "authors": ["Mukai Li", "Linfeng Song", "Zhenwen Liang", "Jiahao Xu", "Shansan Gong", "Qi Liu", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have recently advanced the field of Automated\nTheorem Proving (ATP), attaining substantial performance gains through widely\nadopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)\nreasoning and increased sampling passes. However, they both introduce\nsignificant computational overhead for inference. Moreover, existing cost\nanalyses typically regulate only the number of sampling passes, while\nneglecting the substantial disparities in sampling costs introduced by\ndifferent scaling strategies. In this paper, we systematically compare the\nefficiency of different test-time scaling strategies for ATP models and\ndemonstrate the inefficiency of the current state-of-the-art (SOTA) open-source\napproaches. We then investigate approaches to significantly reduce token usage\nand sample passes while maintaining the original performance. Specifically, we\npropose two complementary methods that can be integrated into a unified EconRL\npipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching\nmechanism designed to mitigate unnecessary token consumption, and (2) Diverse\nparallel-scaled reinforcement learning (RL) with trainable prefixes to enhance\npass rates under constrained sampling passes. Experiments on miniF2F and\nProofNet demonstrate that our EconProver achieves comparable performance to\nbaseline methods with only 12% of the computational cost. This work provides\nactionable insights for deploying lightweight ATP models without sacrificing\nperformance.", "AI": {"tldr": "The paper evaluates the efficiency of test-time scaling strategies in Automated Theorem Proving (ATP) with Large Language Models (LLMs) and proposes methods to reduce computational costs while maintaining performance.", "motivation": "To address the inefficiency of existing test-time scaling strategies in ATP models, which lead to high computational overhead, particularly in sampling passes and token usage.", "method": "The authors systematically compare different test-time scaling strategies and propose two methods: a dynamic Chain-of-Thought (CoT) switching mechanism to reduce token consumption and a Diverse parallel-scaled reinforcement learning (RL) approach with trainable prefixes to enhance pass rates.", "result": "The proposed EconProver achieves performance comparable to baseline methods while using only 12% of the computational cost, showcasing significant efficiency improvements.", "conclusion": "This research provides insights for developing lightweight ATP models that maintain performance levels, thereby enabling more efficient application of LLMs in theorem proving.", "key_contributions": ["Systematic comparison of test-time scaling strategies for ATP with LLMs", "Introduction of a dynamic CoT switching mechanism to reduce unnecessary token usage", "Development of Diverse parallel-scaled RL with trainable prefixes to improve sampling efficiency"], "limitations": "", "keywords": ["Automated Theorem Proving", "Large Language Models", "Chain-of-Thought reasoning"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.13191", "pdf": "https://arxiv.org/pdf/2509.13191.pdf", "abs": "https://arxiv.org/abs/2509.13191", "title": "Textarium: Entangling Annotation, Abstraction and Argument", "authors": ["Philipp Proff", "Marian Dörk"], "categories": ["cs.HC", "cs.CL", "H.5.2; H.5.4; I.7.1; J.5"], "comment": "This is the authors' version of the article presented at VIS4DH and\n  published in the proceedings of IEEE VIS 2025", "summary": "We present a web-based environment that connects annotation, abstraction, and\nargumentation during the interpretation of text. As a visual interface for\nscholarly reading and writing, Textarium combines human analysis with\nlightweight computational processing to bridge close and distant reading\npractices. Readers can highlight text, group keywords into concepts, and embed\nthese observations as anchors in essays. The interface renders these\ninterpretive actions as parameterized visualization states. Through a\nspeculative design process of co-creative and iterative prototyping, we\ndeveloped a reading-writing approach that makes interpretive processes\ntransparent and shareable within digital narratives.", "AI": {"tldr": "Web-based environment for annotating and interpreting texts, integrating human analysis with computational processing.", "motivation": "To enhance the scholarly reading and writing process by bridging close and distant reading practices.", "method": "The study employs a co-creative and iterative prototyping approach to design a visual interface for text interpretation.", "result": "Textarium allows users to highlight text, categorize keywords into concepts, and visualize their interpretive actions, making the process of reading and writing more transparent.", "conclusion": "The reading-writing approach developed through Textarium facilitates collaborative and shareable interpretive processes within digital narratives.", "key_contributions": ["Development of Textarium as a visual interface for text interpretation", "Integration of human analysis with computational tools", "Facilitation of transparent and shareable interpretive processes"], "limitations": "", "keywords": ["text interpretation", "visual interface", "scholarly reading"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2509.12635", "pdf": "https://arxiv.org/pdf/2509.12635.pdf", "abs": "https://arxiv.org/abs/2509.12635", "title": "Positional Encoding via Token-Aware Phase Attention", "authors": ["Yu", "Wang", "Sheng Shen", "Rémi Munos", "Hongyuan Zhan", "Yuandong Tian"], "categories": ["cs.CL", "cs.AI"], "comment": "21 pages", "summary": "We prove under practical assumptions that Rotary Positional Embedding (RoPE)\nintroduces an intrinsic distance-dependent bias in attention scores that limits\nRoPE's ability to model long-context. RoPE extension methods may alleviate this\nissue, but they typically require post-hoc adjustments after pretraining, such\nas rescaling or hyperparameters retuning. This paper introduces Token-Aware\nPhase Attention (TAPA), a new positional encoding method that incorporates a\nlearnable phase function into the attention mechanism. TAPA preserves token\ninteractions over long range, extends to longer contexts with direct and light\nfine-tuning, extrapolates to unseen lengths, and attains significantly lower\nperplexity on long-context than RoPE families.", "AI": {"tldr": "This paper introduces Token-Aware Phase Attention (TAPA), a novel positional encoding method that improves attention modeling in long-contexts compared to Rotary Positional Embedding (RoPE).", "motivation": "To address the limitations of RoPE in modeling long-context due to its intrinsic distance-dependent bias in attention scores.", "method": "The paper proposes a new approach, TAPA, which integrates a learnable phase function into the attention mechanism to enhance token interactions over long distances.", "result": "TAPA demonstrates improved performance in handling longer contexts, achieving lower perplexity compared to RoPE families and effectively extrapolating to unseen sequence lengths.", "conclusion": "TAPA provides a more effective method for positional encoding in attention mechanisms, particularly beneficial for long-context applications.", "key_contributions": ["Introduction of Token-Aware Phase Attention (TAPA) as a new positional encoding method.", "Demonstration of lower perplexity on long-context tasks compared to existing methods.", "Ability to extrapolate to unseen input lengths with minimal tuning."], "limitations": "", "keywords": ["Token-Aware Phase Attention", "positional encoding", "attention mechanism", "long-context", "RoPE"], "importance_score": 7, "read_time_minutes": 21}}
{"id": "2509.13253", "pdf": "https://arxiv.org/pdf/2509.13253.pdf", "abs": "https://arxiv.org/abs/2509.13253", "title": "Evolution of Programmers' Trust in Generative AI Programming Assistants", "authors": ["Anshul Shah", "Thomas Rexin", "Elena Tomson", "Leo Porter", "William G. Griswold", "Adalbert Gerald Soosai Raj"], "categories": ["cs.HC", "cs.SE"], "comment": "Koli Calling 2025 conference", "summary": "Motivation. Trust in generative AI programming assistants is a vital attitude\nthat impacts how programmers use those programming assistants. Programmers that\nare over-trusting may be too reliant on their tools, leading to incorrect or\nvulnerable code; programmers that are under-trusting may avoid using tools that\ncan improve their productivity and well-being.\n  Methods. Since trust is a dynamic attitude that may change over time, this\nstudy aims to understand programmers' evolution of trust after immediate (one\nhour) and extended (10 days) use of GitHub Copilot. We collected survey data\nfrom 71 upper-division computer science students working on a legacy code base,\nrepresenting a population that is about to enter the workforce. In this study,\nwe quantitatively measure student trust levels and qualitatively uncover why\nstudent trust changes.\n  Findings. Student trust, on average, increased over time. After completing a\nproject with Copilot, however, students felt that Copilot requires a competent\nprogrammer to complete some tasks manually. Students mentioned that seeing\nCopilot's correctness, understanding how Copilot uses context from the code\nbase, and learning some basics of natural language processing contributed to\ntheir elevated trust.\n  Implications. Our study helps instructors and industry managers understand\nthe factors that influence how students calibrate their trust with AI\nassistants. We make four pedagogical recommendations, which are that CS\neducators should 1) provide opportunities for students to work with Copilot on\nchallenging software engineering tasks to calibrate their trust, 2) teach\ntraditional skills of comprehending, debugging, and testing so students can\nverify output, 3) teach students about the basics of natural language\nprocessing, and 4) explicitly introduce and demonstrate the range of features\navailable in Copilot.", "AI": {"tldr": "Study on programmers' trust in GitHub Copilot over short and long-term use, revealing factors influencing trust and offering pedagogical recommendations.", "motivation": "Understanding trust dynamics in generative AI programming assistants to improve their effective use among programmers.", "method": "Survey data collected from 71 upper-division computer science students analyzing trust levels after using GitHub Copilot immediately and over 10 days.", "result": "Average student trust in Copilot increased over time, influenced by perceived correctness, context understanding, and basic NLP knowledge.", "conclusion": "The findings inform educators and industry managers about the importance of trust calibration in AI tools, leading to pedagogical recommendations.", "key_contributions": ["Reveals evolution of student trust in AI programming tools over time.", "Identifies factors that enhance trust in programming assistants like Copilot.", "Provides actionable recommendations for CS educators to improve trust calibration."], "limitations": "", "keywords": ["Trust in AI", "GitHub Copilot", "Programming education", "Human-Computer Interaction", "Trust calibration"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2509.12647", "pdf": "https://arxiv.org/pdf/2509.12647.pdf", "abs": "https://arxiv.org/abs/2509.12647", "title": "PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition", "authors": ["Li Fu", "Yu Xin", "Sunlu Zeng", "Lu Fan", "Youzheng Wu", "Xiaodong He"], "categories": ["cs.CL", "eess.AS"], "comment": "Submitted to ICASSP 2026", "summary": "This paper presents a Pronunciation-Aware Contextualized (PAC) framework to\naddress two key challenges in Large Language Model (LLM)-based Automatic Speech\nRecognition (ASR) systems: effective pronunciation modeling and robust\nhomophone discrimination. Both are essential for raw or long-tail word\nrecognition. The proposed approach adopts a two-stage learning paradigm. First,\nwe introduce a pronunciation-guided context learning method. It employs an\ninterleaved grapheme-phoneme context modeling strategy that incorporates\ngrapheme-only distractors, encouraging the model to leverage phonemic cues for\naccurate recognition. Then, we propose a pronunciation-discriminative\nreinforcement learning method with perturbed label sampling to further enhance\nthe model\\'s ability to distinguish contextualized homophones. Experimental\nresults on the public English Librispeech and Mandarin AISHELL-1 datasets\nindicate that PAC: (1) reduces relative Word Error Rate (WER) by 30.2% and\n53.8% compared to pre-trained LLM-based ASR models, and (2) achieves 31.8% and\n60.5% relative reductions in biased WER for long-tail words compared to strong\nbaselines, respectively.", "AI": {"tldr": "This paper introduces a Pronunciation-Aware Contextualized (PAC) framework that improves pronunciation modeling and homophone discrimination in LLM-based ASR systems, achieving significant reductions in Word Error Rate.", "motivation": "The study aims to enhance automatic speech recognition systems by addressing challenges in pronunciation modeling and homophone discrimination, which are critical for recognizing raw or long-tail words.", "method": "The proposed PAC framework employs a two-stage learning paradigm: first, it utilizes a pronunciation-guided context learning method with an interleaved grapheme-phoneme context modeling strategy; second, it enhances the model's homophone discrimination through a pronunciation-discriminative reinforcement learning approach.", "result": "Experimental results demonstrate that PAC reduces Word Error Rate (WER) by 30.2% and 53.8% compared to pre-trained LLM-based ASR models, and achieves notable reductions in biased WER for long-tail words.", "conclusion": "The PAC framework significantly improves performance in ASR tasks, particularly for long-tail words, highlighting the importance of pronunciation in such systems.", "key_contributions": ["Pronunciation-guided context learning method", "Pronunciation-discriminative reinforcement learning with perturbed label sampling", "Substantial reductions in Word Error Rate for long-tail words"], "limitations": "", "keywords": ["Large Language Model", "Automatic Speech Recognition", "Pronunciation Modeling", "Homophone Discrimination", "Reinforcement Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.13291", "pdf": "https://arxiv.org/pdf/2509.13291.pdf", "abs": "https://arxiv.org/abs/2509.13291", "title": "Towards an Embodied Composition Framework for Organizing Immersive Computational Notebooks", "authors": ["Sungwon In", "Eric Krokos", "Kirsten Whitley", "Chris North", "Yalong Yang"], "categories": ["cs.HC"], "comment": "11 pages, 9 figures, The ACM Symposium on Virtual Reality Software\n  and Technology (VRST) 2025", "summary": "As immersive technologies evolve, immersive computational notebooks offer new\nopportunities for interacting with code, data, and outputs. However, scaling\nthese environments remains a challenge, particularly when analysts manually\narrange large numbers of cells to maintain both execution logic and visual\ncoherence. To address this, we introduce an embodied composition framework,\nfacilitating organizational processes in the context of immersive computational\nnotebooks. To evaluate the effectiveness of the embodied composition framework,\nwe conducted a controlled user study comparing manual and embodied composition\nframeworks in an organizational process. The results show that embodied\ncomposition frameworks significantly reduced user effort and decreased\ncompletion time. However, the design of the triggering mechanism requires\nfurther refinement. Our findings highlight the potential of embodied\ncomposition frameworks to enhance the scalability of the organizational process\nin immersive computational notebooks.", "AI": {"tldr": "This paper presents an embodied composition framework to improve user interaction in immersive computational notebooks, revealing reduced effort and time in organizational tasks.", "motivation": "The paper addresses the challenge of scaling immersive computational notebooks, where analysts struggle with arranging numerous cells while maintaining execution logic and visual coherence.", "method": "A controlled user study was conducted comparing manual and embodied composition frameworks in organizational processes.", "result": "The study results indicate that the embodied composition framework significantly reduced user effort and completion time, although the design of the triggering mechanism needs refinement.", "conclusion": "The findings suggest that embodied composition frameworks can enhance the scalability of organizational processes in immersive computational notebooks.", "key_contributions": ["Introduction of an embodied composition framework for immersive computational notebooks.", "Evidence from user studies demonstrating reduced effort and time with the new framework.", "Identification of areas for improvement in the triggering mechanism."], "limitations": "The design of the triggering mechanism requires further refinement.", "keywords": ["immersive technologies", "computational notebooks", "user study"], "importance_score": 6, "read_time_minutes": 11}}
{"id": "2509.12652", "pdf": "https://arxiv.org/pdf/2509.12652.pdf", "abs": "https://arxiv.org/abs/2509.12652", "title": "Don't Change My View: Ideological Bias Auditing in Large Language Models", "authors": ["Paul Kröger", "Emilio Barkett"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly embedded in products used\nby millions, their outputs may influence individual beliefs and, cumulatively,\nshape public opinion. If the behavior of LLMs can be intentionally steered\ntoward specific ideological positions, such as political or religious views,\nthen those who control these systems could gain disproportionate influence over\npublic discourse. Although it remains an open question whether LLMs can\nreliably be guided toward coherent ideological stances and whether such\nsteering can be effectively prevented, a crucial first step is to develop\nmethods for detecting when such steering attempts occur. In this work, we adapt\na previously proposed statistical method to the new context of ideological bias\nauditing. Our approach carries over the model-agnostic design of the original\nframework, which does not require access to the internals of the language\nmodel. Instead, it identifies potential ideological steering by analyzing\ndistributional shifts in model outputs across prompts that are thematically\nrelated to a chosen topic. This design makes the method particularly suitable\nfor auditing proprietary black-box systems. We validate our approach through a\nseries of experiments, demonstrating its practical applicability and its\npotential to support independent post hoc audits of LLM behavior.", "AI": {"tldr": "This paper presents a method for detecting ideological steering in large language models by analyzing shifts in model outputs.", "motivation": "To address the potential influence of large language models on public opinion and the need for methods to detect ideological bias.", "method": "The authors adapt a statistical method for detecting ideological bias without accessing the internals of the model, focusing on distributional shifts in outputs across related prompts.", "result": "The method was validated through experiments, showing that it can effectively identify steering attempts in LLM outputs.", "conclusion": "The proposed approach is a viable tool for auditing the behavior of LLMs and could assist in preventing ideological bias in public discourse.", "key_contributions": ["Development of a model-agnostic method for ideological bias auditing in LLMs", "Ability to detect ideological steering without needing model internals", "Validation of the method through practical experiments"], "limitations": "", "keywords": ["large language models", "ideological bias", "auditing methods", "statistical analysis", "public discourse"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.13295", "pdf": "https://arxiv.org/pdf/2509.13295.pdf", "abs": "https://arxiv.org/abs/2509.13295", "title": "Investigating Seamless Transitions Between Immersive Computational Notebooks and Embodied Data Interactions", "authors": ["Sungwon In", "Eric Krokos", "Kirsten Whitley", "Chris North", "Yalong Yang"], "categories": ["cs.HC"], "comment": "11 pages, 7 figures, The ACM Symposium on Virtual Reality Software\n  and Technology (VRST)", "summary": "A growing interest in Immersive Analytics (IA) has led to the extension of\ncomputational notebooks (e.g., Jupyter Notebook) into an immersive environment\nto enhance analytical workflows. However, existing solutions rely on the WIMP\n(windows, icons, menus, pointer) metaphor, which remains impractical for\ncomplex data exploration. Although embodied interaction offers a more intuitive\nalternative, immersive computational notebooks and embodied data exploration\nsystems are implemented as standalone tools. This separation requires analysts\nto invest considerable effort to transition from one environment to an entirely\ndifferent one during analytical workflows. To address this, we introduce ICoN,\na prototype that facilitates a seamless transition between computational\nnotebooks and embodied data explorations within a unified, fully immersive\nenvironment. Our findings reveal that unification improves transition\nefficiency and intuitiveness during analytical workflows, highlighting its\npotential for seamless data analysis.", "AI": {"tldr": "The paper introduces ICoN, an immersive computational notebook prototype that combines traditional analytical tools with embodied data exploration, improving workflow efficiency.", "motivation": "The paper addresses the impracticality of using WIMP metaphors in complex data exploration, which currently forces analysts to switch between different environments during their workflow.", "method": "ICoN prototype was developed to unify computational notebooks and embodied exploration in a fully immersive environment, allowing for seamless transitions.", "result": "Testing showed that unification through ICoN enhances the efficiency and intuitiveness of analytical workflows, allowing for smoother data analysis.", "conclusion": "ICoN has the potential to significantly improve data exploration by providing a unified environment that reduces the overhead of switching contexts.", "key_contributions": ["Development of ICoN prototype for immersive analytics", "Demonstration of improved workflow efficiency", "Integration of computational notebooks with embodied data exploration"], "limitations": "The study focuses on a prototype; broader applications and long-term usability remain to be tested.", "keywords": ["Immersive Analytics", "Computational Notebooks", "Embodied Interaction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.12661", "pdf": "https://arxiv.org/pdf/2509.12661.pdf", "abs": "https://arxiv.org/abs/2509.12661", "title": "Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations", "authors": ["Yougen Zhou", "Qin Chen", "Ningning Zhou", "Jie Zhou", "Xingjiao Wu", "Liang He"], "categories": ["cs.CL"], "comment": null, "summary": "Emotional support conversation (ESC) aims to alleviate distress through\nempathetic dialogue, yet large language models (LLMs) face persistent\nchallenges in delivering effective ESC due to low accuracy in strategy\nplanning. Moreover, there is a considerable preference bias towards specific\nstrategies. Prior methods using fine-tuned strategy planners have shown\npotential in reducing such bias, while the underlying causes of the preference\nbias in LLMs have not well been studied. To address these issues, we first\nreveal the fundamental causes of the bias by identifying the knowledge\nboundaries of LLMs in strategy planning. Then, we propose an approach to\nmitigate the bias by reinforcement learning with a dual reward function, which\noptimizes strategy planning via both accuracy and entropy-based confidence for\neach region according to the knowledge boundaries. Experiments on the ESCov and\nExTES datasets with multiple LLM backbones show that our approach outperforms\nthe baselines, confirming the effectiveness of our approach.", "AI": {"tldr": "The paper addresses bias in large language models during emotional support conversations by identifying knowledge boundaries and proposing a reinforcement learning method to optimize strategy planning.", "motivation": "To improve the effectiveness of emotional support conversation by addressing low accuracy in strategy planning and preference bias in large language models.", "method": "The authors identify the knowledge boundaries of LLMs in strategy planning and propose a dual reward function for reinforcement learning to optimize strategy planning by accuracy and entropy-based confidence.", "result": "Experiments show that the proposed method outperforms baseline models on ESCov and ExTES datasets, confirming its effectiveness in reducing bias.", "conclusion": "The study reveals underlying causes of bias in LLMs and presents an effective way to mitigate it, potentially enhancing emotional support applications.", "key_contributions": ["Identified fundamental causes of bias in LLMs for emotional support dialogue.", "Proposed a novel reinforcement learning approach to enhance strategy planning accuracy.", "Demonstrated effectiveness through experiments on relevant datasets."], "limitations": "The study focuses on specific datasets and may require validation on broader applications of emotional support conversation.", "keywords": ["emotional support conversation", "large language models", "reinforcement learning", "strategy planning", "preference bias"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.12662", "pdf": "https://arxiv.org/pdf/2509.12662.pdf", "abs": "https://arxiv.org/abs/2509.12662", "title": "Chat-Driven Text Generation and Interaction for Person Retrieval", "authors": ["Zequn Xie", "Chuxin Wang", "Sihang Cai", "Yeqiang Wang", "Shulei Wang", "Tao Jin"], "categories": ["cs.CL", "I.2.7; I.4.9"], "comment": "Accepted by EMNLP 2025. 13 pages, 3 figures", "summary": "Text-based person search (TBPS) enables the retrieval of person images from\nlarge-scale databases using natural language descriptions, offering critical\nvalue in surveillance applications. However, a major challenge lies in the\nlabor-intensive process of obtaining high-quality textual annotations, which\nlimits scalability and practical deployment. To address this, we introduce two\ncomplementary modules: Multi-Turn Text Generation (MTG) and Multi-Turn Text\nInteraction (MTI). MTG generates rich pseudo-labels through simulated dialogues\nwith MLLMs, producing fine-grained and diverse visual descriptions without\nmanual supervision. MTI refines user queries at inference time through dynamic,\ndialogue-based reasoning, enabling the system to interpret and resolve vague,\nincomplete, or ambiguous descriptions - characteristics often seen in\nreal-world search scenarios. Together, MTG and MTI form a unified and\nannotation-free framework that significantly improves retrieval accuracy,\nrobustness, and usability. Extensive evaluations demonstrate that our method\nachieves competitive or superior results while eliminating the need for manual\ncaptions, paving the way for scalable and practical deployment of TBPS systems.", "AI": {"tldr": "This paper introduces a framework for Text-based person search (TBPS) that uses Multi-Turn Text Generation and Interaction to improve retrieval accuracy and usability without manual annotations.", "motivation": "The need for high-quality textual annotations in TBPS restricts scalability and practical deployment, particularly in surveillance applications.", "method": "The proposed framework includes two modules: Multi-Turn Text Generation (MTG) which creates pseudo-labels through dialogues with MLLMs, and Multi-Turn Text Interaction (MTI) that refines user queries during inference, addressing vague descriptions.", "result": "The framework significantly enhances retrieval accuracy and usability, achieving competitive results while eliminating the dependency on manual captions.", "conclusion": "The unified and annotation-free approach set forth demonstrates potential for scalable and practical deployment of TBPS systems.", "key_contributions": ["Introduced a method to generate pseudo-labels without manual supervision.", "Developed a dialogue-based query refinement process for improved user interaction.", "Created an annotation-free framework that enhances TBPS usability and accuracy."], "limitations": "", "keywords": ["Text-based person search", "Natural language processing", "Multi-Turn Text Generation", "MLLMs"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12672", "pdf": "https://arxiv.org/pdf/2509.12672.pdf", "abs": "https://arxiv.org/abs/2509.12672", "title": "Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content", "authors": ["Shaz Furniturewala", "Arkaitz Zubiaga"], "categories": ["cs.CL"], "comment": null, "summary": "The volume of machine-generated content online has grown dramatically due to\nthe widespread use of Large Language Models (LLMs), leading to new challenges\nfor content moderation systems. Conventional content moderation classifiers,\nwhich are usually trained on text produced by humans, suffer from\nmisclassifications due to LLM-generated text deviating from their training data\nand adversarial attacks that aim to avoid detection. Present-day defence\ntactics are reactive rather than proactive, since they rely on adversarial\ntraining or external detection models to identify attacks. In this work, we aim\nto identify the vulnerable components of toxicity classifiers that contribute\nto misclassification, proposing a novel strategy based on mechanistic\ninterpretability techniques. Our study focuses on fine-tuned BERT and RoBERTa\nclassifiers, testing on diverse datasets spanning a variety of minority groups.\nWe use adversarial attacking techniques to identify vulnerable circuits.\nFinally, we suppress these vulnerable circuits, improving performance against\nadversarial attacks. We also provide demographic-level insights into these\nvulnerable circuits, exposing fairness and robustness gaps in model training.\nWe find that models have distinct heads that are either crucial for performance\nor vulnerable to attack and suppressing the vulnerable heads improves\nperformance on adversarial input. We also find that different heads are\nresponsible for vulnerability across different demographic groups, which can\ninform more inclusive development of toxicity detection models.", "AI": {"tldr": "This study addresses the challenges of content moderation systems in detecting LLM-generated text by identifying and suppressing vulnerable components of toxicity classifiers.", "motivation": "The increase of machine-generated content has led to difficulties in conventional content moderation classifiers, necessitating a proactive approach to improve detection accuracy.", "method": "We apply mechanistic interpretability techniques to fine-tuned BERT and RoBERTa classifiers, using adversarial techniques to identify and suppress vulnerable circuits responsible for misclassification.", "result": "The suppression of vulnerable circuits improved performance against adversarial attacks and highlighted demographic-level insights about vulnerabilities in toxicity detection models.", "conclusion": "Our findings reveal distinct model heads that impact performance and vulnerability, advocating for inclusive development in toxicity detection.", "key_contributions": ["Identification of vulnerable components in toxicity classifiers", "Improvement of classifier performance against adversarial attacks", "Insights into demographic-specific vulnerabilities in machine learning models"], "limitations": "", "keywords": ["Large Language Models", "Content Moderation", "Adversarial Attacks", "Toxicity Detection", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12677", "pdf": "https://arxiv.org/pdf/2509.12677.pdf", "abs": "https://arxiv.org/abs/2509.12677", "title": "Case-Based Decision-Theoretic Decoding with Quality Memories", "authors": ["Hiroyuki Deguchi", "Masaaki Nagata"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP2025 main", "summary": "Minimum Bayes risk (MBR) decoding is a decision rule of text generation,\nwhich selects the hypothesis that maximizes the expected utility and robustly\ngenerates higher-quality texts than maximum a posteriori (MAP) decoding.\nHowever, it depends on sample texts drawn from the text generation model; thus,\nit is difficult to find a hypothesis that correctly captures the knowledge or\ninformation of out-of-domain. To tackle this issue, we propose case-based\ndecision-theoretic (CBDT) decoding, another method to estimate the expected\nutility using examples of domain data. CBDT decoding not only generates\nhigher-quality texts than MAP decoding, but also the combination of MBR and\nCBDT decoding outperformed MBR decoding in seven domain De--En and\nJa$\\leftrightarrow$En translation tasks and image captioning tasks on MSCOCO\nand nocaps datasets.", "AI": {"tldr": "The paper introduces case-based decision-theoretic (CBDT) decoding as a method for text generation that improves over traditional methods by utilizing domain data examples.", "motivation": "To improve the quality of text generation decodings that often rely on in-domain samples, which can limit performance when encountering out-of-domain data.", "method": "The method proposed combines traditional MBR decoding with CBDT decoding to leverage examples from domain data for improved expected utility estimations in text generation.", "result": "CBDT decoding generates higher-quality texts than MAP decoding and outperforms MBR decoding in various language translation and image captioning tasks.", "conclusion": "The CBDT decoding method enhances text generation quality by effectively utilizing domain-specific examples, proving better than existing methods across several benchmarks.", "key_contributions": ["Introduction of CBDT decoding for text generation", "Demonstrated superior performance over MBR and MAP decoding", "Validation on translation and image captioning tasks with improved results"], "limitations": "", "keywords": ["Minimum Bayes risk", "case-based decision-theoretic", "text generation", "machine translation", "image captioning"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2509.12720", "pdf": "https://arxiv.org/pdf/2509.12720.pdf", "abs": "https://arxiv.org/abs/2509.12720", "title": "HistoryBankQA: Multilingual Temporal Question Answering on Historical Events", "authors": ["Biswadip Mandal", "Anant Khandelwal", "Manish Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Temporal reasoning about historical events is a critical skill for NLP tasks\nlike event extraction, historical entity linking, temporal question answering,\ntimeline summarization, temporal event clustering and temporal natural language\ninference. Yet efforts on benchmarking temporal reasoning capabilities of large\nlanguage models (LLMs) are rather limited. Existing temporal reasoning datasets\nare limited in scale, lack multilingual coverage and focus more on contemporary\nevents. To address these limitations, we present HistoryBank, a multilingual\ndatabase of 10M+ historical events extracted from Wikipedia timeline pages and\narticle infoboxes. Our database provides unprecedented coverage in both\nhistorical depth and linguistic breadth with 10 languages. Additionally, we\nconstruct a comprehensive question answering benchmark for temporal reasoning\nacross all languages. This benchmark covers a diverse set of 6 temporal QA\nreasoning tasks, and we evaluate a suite of popular language models\n(LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess their\nperformance on these tasks. As expected GPT4o performs best across all answer\ntypes and languages; Gemma-2 outperforms the other small language models. Our\nwork aims to provide a comprehensive resource for advancing multilingual and\ntemporally-aware natural language understanding of historical events. To\nfacilitate further research, we will make our code and datasets publicly\navailable upon acceptance of this paper.", "AI": {"tldr": "HistoryBank is a multilingual database of over 10 million historical events aimed at improving temporal reasoning benchmarks for language models.", "motivation": "To address the limited benchmarking and dataset size for temporal reasoning in NLP, especially in the context of historical events.", "method": "Creation of a multilingual database with 10+ million historical events and a comprehensive question answering benchmark for temporal reasoning across six tasks.", "result": "Evaluation of popular language models; GPT4o shows the best performance across all tasks and languages, while Gemma-2 outperformed other small models.", "conclusion": "The work provides a significant resource for enhancing multilingual and temporally-aware natural language understanding and will be publicly available for further research.", "key_contributions": ["Introduction of HistoryBank with 10M+ historical events", "Multilingual coverage in 10 languages", "Development of a comprehensive temporal QA benchmark"], "limitations": "", "keywords": ["temporal reasoning", "historical events", "multilingual database"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12771", "pdf": "https://arxiv.org/pdf/2509.12771.pdf", "abs": "https://arxiv.org/abs/2509.12771", "title": "Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision", "authors": ["Omri Suissa", "Muhiim Ali", "Shengmai Chen", "Yinuo Cai", "Shekhar Pradhan"], "categories": ["cs.CL"], "comment": null, "summary": "Humans can recognize an image as an instance of a general concept, beyond\nsimply identifying its objects and their relationships. In this paper, we\ninvestigate 1. The extent to which VLMs have this concept abstraction capacity,\nand 2. Strategies for encoding the sort of higher-concept information in images\nthat would enable the resulting VLM model (CLEAR GLASS model) to have this\ncapability to a greater degree. To this end, we introduce a grouped\nimage-caption dataset (MAGIC), which consists of several groups of image\ncaptions and for each group a set of associated images and higher-level\nconceptual labels. We use a novel contrastive loss technique to induce the\nmodel to encode in the representation of each image (caption) in a group the\ninformation that is common to all members of the image-caption group. Our main\ncontribution is a grouped contrastive loss function based on text-image\ncontrastive groups (outer contrastive loss) as well as an inner loss which\nmeasures the distances between image-caption instances in the group. Our\ntraining methodology results in the CLEAR GLASS model having the concept\nabstraction capacity as an emergent capacity because the model is not exposed\nto the higher-level concepts associated with each group. Instead, the training\nforces the model to create for each image-caption group a semantic\nrepresentation that brings it closer to the semantic representation of the\nhigher-level concepts in the latent semantic space. Our experiments show that\nthis training methodology results in a model which shows improvement in\nabstract concept recognition compared to SOTA models.", "AI": {"tldr": "This paper investigates the capacity of Vision-Language Models (VLMs) for concept abstraction and introduces a novel training methodology using a grouped contrastive loss function to enhance this capability.", "motivation": "To explore how well VLMs can abstract general concepts beyond mere object identification and to improve these models' capacities for concept recognition.", "method": "The authors present the CLEAR GLASS model trained with a grouped contrastive loss function on a newly created dataset (MAGIC) that includes grouped image-caption pairs and associated higher-level concepts.", "result": "The CLEAR GLASS model demonstrates enhanced ability to recognize abstract concepts, outperforming state-of-the-art models by effectively encoding information shared across grouped image-caption instances.", "conclusion": "By using a training approach that emphasizes the semantic relationships within image-caption groups, the model achieves emergent capacity in concept abstraction without prior exposure to the higher-level concepts.", "key_contributions": ["Introduction of a grouped contrastive loss function for VLMs", "Creation of the MAGIC dataset for training VLMs on concept abstraction", "Demonstration of improved abstract concept recognition in the CLEAR GLASS model"], "limitations": "", "keywords": ["Vision-Language Models", "concept abstraction", "contrastive loss"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.12811", "pdf": "https://arxiv.org/pdf/2509.12811.pdf", "abs": "https://arxiv.org/abs/2509.12811", "title": "ConvergeWriter: Data-Driven Bottom-Up Article Construction", "authors": ["Binquan Ji", "Jiaqi Wang", "Ruiting Li", "Xingchen Han", "Yiyang Qi", "Shichao Wang", "Yifei Lu", "Yuantao Han", "Feiliang Ren"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable prowess in text\ngeneration, yet producing long-form, factual documents grounded in extensive\nexternal knowledge bases remains a significant challenge. Existing \"top-down\"\nmethods, which first generate a hypothesis or outline and then retrieve\nevidence, often suffer from a disconnect between the model's plan and the\navailable knowledge, leading to content fragmentation and factual inaccuracies.\nTo address these limitations, we propose a novel \"bottom-up,\" data-driven\nframework that inverts the conventional generation pipeline. Our approach is\npredicated on a \"Retrieval-First for Knowledge, Clustering for Structure\"\nstrategy, which first establishes the \"knowledge boundaries\" of the source\ncorpus before any generative planning occurs. Specifically, we perform\nexhaustive iterative retrieval from the knowledge base and then employ an\nunsupervised clustering algorithm to organize the retrieved documents into\ndistinct \"knowledge clusters.\" These clusters form an objective, data-driven\nfoundation that directly guides the subsequent generation of a hierarchical\noutline and the final document content. This bottom-up process ensures that the\ngenerated text is strictly constrained by and fully traceable to the source\nmaterial, proactively adapting to the finite scope of the knowledge base and\nfundamentally mitigating the risk of hallucination. Experimental results on\nboth 14B and 32B parameter models demonstrate that our method achieves\nperformance comparable to or exceeding state-of-the-art baselines, and is\nexpected to demonstrate unique advantages in knowledge-constrained scenarios\nthat demand high fidelity and structural coherence. Our work presents an\neffective paradigm for generating reliable, structured, long-form documents,\npaving the way for more robust LLM applications in high-stakes,\nknowledge-intensive domains.", "AI": {"tldr": "This paper proposes a novel 'bottom-up' framework for generating long-form documents using Large Language Models (LLMs), improving reliability by reversing conventional methods through iterative knowledge retrieval and unsupervised clustering.", "motivation": "Existing LLM methods struggle with long-form generation due to disconnects between planning and available knowledge, leading to inaccuracies and fragmentation.", "method": "The proposed approach employs a 'Retrieval-First for Knowledge, Clustering for Structure' strategy, where extensive retrieval from a knowledge base is performed first, followed by clustering of the retrieved documents into knowledge clusters to guide the generation process.", "result": "Experiments show that this method achieves performance comparable to or better than state-of-the-art baselines with unique advantages in scenarios requiring high accuracy and coherence.", "conclusion": "This novel paradigm enhances the generation of reliable, structured documents, fostering better applications of LLMs in knowledge-intensive fields.", "key_contributions": ["Introduction of a 'bottom-up' generation framework for LLMs.", "Utilization of unsupervised clustering to guide document structure from retrieved knowledge.", "Demonstration of improved performance in knowledge-constrained scenarios."], "limitations": "", "keywords": ["Large Language Models", "Knowledge Retrieval", "Document Generation", "Clustering", "Factual Accuracy"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2509.12853", "pdf": "https://arxiv.org/pdf/2509.12853.pdf", "abs": "https://arxiv.org/abs/2509.12853", "title": "Data Augmentation for Maltese NLP using Transliterated and Machine Translated Arabic Data", "authors": ["Kurt Micallef", "Nizar Habash", "Claudia Borg"], "categories": ["cs.CL"], "comment": "EMNLP Camera-Ready", "summary": "Maltese is a unique Semitic language that has evolved under extensive\ninfluence from Romance and Germanic languages, particularly Italian and\nEnglish. Despite its Semitic roots, its orthography is based on the Latin\nscript, creating a gap between it and its closest linguistic relatives in\nArabic. In this paper, we explore whether Arabic-language resources can support\nMaltese natural language processing (NLP) through cross-lingual augmentation\ntechniques. We investigate multiple strategies for aligning Arabic textual data\nwith Maltese, including various transliteration schemes and machine translation\n(MT) approaches. As part of this, we also introduce novel transliteration\nsystems that better represent Maltese orthography. We evaluate the impact of\nthese augmentations on monolingual and mutlilingual models and demonstrate that\nArabic-based augmentation can significantly benefit Maltese NLP tasks.", "AI": {"tldr": "The paper investigates the potential of Arabic-language resources for enhancing Maltese natural language processing (NLP) through cross-lingual augmentation techniques.", "motivation": "There is a linguistic gap between Maltese, a Semitic language, and its linguistic relatives in Arabic, prompting the exploration of resources and techniques from Arabic to improve Maltese NLP.", "method": "The study employs various strategies for aligning Arabic textual data with Maltese, including transliteration schemes and machine translation approaches, while introducing novel transliteration systems for Maltese orthography.", "result": "The evaluation demonstrates that Arabic-based augmentation significantly benefits Maltese NLP tasks across monolingual and multilingual models.", "conclusion": "Using Arabic resources and innovative transliteration methods can enhance the performance of Maltese NLP, bridging linguistic and resource gaps between these languages.", "key_contributions": ["Exploration of cross-lingual augmentation techniques for Maltese NLP using Arabic resources.", "Introduction of novel transliteration systems for Maltese orthography.", "Evaluation of the impact of Arabic-based augmentation on Maltese NLP tasks."], "limitations": "", "keywords": ["Maltese", "Arabic", "Natural Language Processing", "Cross-lingual Augmentation", "Transliteration"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.12876", "pdf": "https://arxiv.org/pdf/2509.12876.pdf", "abs": "https://arxiv.org/abs/2509.12876", "title": "Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents", "authors": ["Fuyu Xing", "Zimu Wang", "Wei Wang", "Haiyang Zhang"], "categories": ["cs.CL", "cs.MM"], "comment": "Accepted at INLG 2025. Camera-ready version", "summary": "The proliferation of multimedia content necessitates the development of\neffective Multimedia Event Extraction (M2E2) systems. Though Large\nVision-Language Models (LVLMs) have shown strong cross-modal capabilities,\ntheir utility in the M2E2 task remains underexplored. In this paper, we present\nthe first systematic evaluation of representative LVLMs, including DeepSeek-VL2\nand the Qwen-VL series, on the M2E2 dataset. Our evaluations cover text-only,\nimage-only, and cross-media subtasks, assessed under both few-shot prompting\nand fine-tuning settings. Our key findings highlight the following valuable\ninsights: (1) Few-shot LVLMs perform notably better on visual tasks but\nstruggle significantly with textual tasks; (2) Fine-tuning LVLMs with LoRA\nsubstantially enhances model performance; and (3) LVLMs exhibit strong synergy\nwhen combining modalities, achieving superior performance in cross-modal\nsettings. We further provide a detailed error analysis to reveal persistent\nchallenges in areas such as semantic precision, localization, and cross-modal\ngrounding, which remain critical obstacles for advancing M2E2 capabilities.", "AI": {"tldr": "The paper systematically evaluates Large Vision-Language Models (LVLMs) on Multimedia Event Extraction (M2E2) tasks, revealing strengths in cross-modal performance and areas for improvement.", "motivation": "To explore the effectiveness of Large Vision-Language Models (LVLMs) in Multimedia Event Extraction (M2E2) tasks and identify their strengths and weaknesses.", "method": "The authors conducted a systematic evaluation of various LVLMs, including DeepSeek-VL2 and Qwen-VL, on the M2E2 dataset, through different subtasks and settings: text-only, image-only, and cross-media, assessed via few-shot prompting and fine-tuning.", "result": "Few-shot LVLMs show better performance on visual tasks but struggle with textual tasks; fine-tuning with LoRA improves performance, and LVLMs demonstrate strong synergy in cross-modal tasks.", "conclusion": "The findings highlight specific areas for improvement in LVLMs for M2E2 tasks, particularly regarding semantic precision, localization, and cross-modal grounding.", "key_contributions": ["First systematic evaluation of LVLMs on M2E2 tasks", "Insights into the performance of few-shot and fine-tuned LVLMs", "Identification of critical challenges in M2E2 capabilities."], "limitations": "", "keywords": ["Multimedia Event Extraction", "Large Vision-Language Models", "cross-modal performance"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2509.12886", "pdf": "https://arxiv.org/pdf/2509.12886.pdf", "abs": "https://arxiv.org/abs/2509.12886", "title": "The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations", "authors": ["Yubo Zhu", "Dongrui Liu", "Zecheng Lin", "Wei Tong", "Sheng Zhong", "Jing Shao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Estimating the difficulty of input questions as perceived by large language\nmodels (LLMs) is essential for accurate performance evaluation and adaptive\ninference. Existing methods typically rely on repeated response sampling,\nauxiliary models, or fine-tuning the target model itself, which may incur\nsubstantial computational costs or compromise generality. In this paper, we\npropose a novel approach for difficulty estimation that leverages only the\nhidden representations produced by the target LLM. We model the token-level\ngeneration process as a Markov chain and define a value function to estimate\nthe expected output quality given any hidden state. This allows for efficient\nand accurate difficulty estimation based solely on the initial hidden state,\nwithout generating any output tokens. Extensive experiments across both textual\nand multimodal tasks demonstrate that our method consistently outperforms\nexisting baselines in difficulty estimation. Moreover, we apply our difficulty\nestimates to guide adaptive reasoning strategies, including Self-Consistency,\nBest-of-N, and Self-Refine, achieving higher inference efficiency with fewer\ngenerated tokens.", "AI": {"tldr": "This paper introduces a novel method for estimating the difficulty of questions for large language models using only their hidden representations, improving efficiency and accuracy over previous methods.", "motivation": "Accurate difficulty estimation of input questions is crucial for the performance evaluation and adaptive inference of large language models, yet existing methods are computationally expensive or lack generality.", "method": "The authors propose modeling the token-level generation process as a Markov chain, defining a value function to estimate expected output quality from the hidden states of the LLM, without generating tokens.", "result": "The proposed method consistently outperforms existing baselines in estimating difficulty across various tasks and contexts, leading to gains in inference efficiency when applied to adaptive reasoning strategies.", "conclusion": "The approach enhances performance in both textual and multimodal contexts, validating its utility in guiding adaptive reasoning strategies with fewer tokens generated.", "key_contributions": ["Development of a Markov chain model for difficulty estimation using hidden states", "Improved efficiency in estimating difficulty without output generation", "Demonstration of enhanced adaptive reasoning strategies through effective difficulty estimates"], "limitations": "", "keywords": ["difficulty estimation", "large language models", "adaptive reasoning", "Markov chain", "hidden representations"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.12892", "pdf": "https://arxiv.org/pdf/2509.12892.pdf", "abs": "https://arxiv.org/abs/2509.12892", "title": "Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings", "authors": ["Shiyu Li", "Yang Tang", "Ruijie Liu", "Shi-Zhe Chen", "Xi Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Oral", "summary": "Large language models (LLMs) have recently demonstrated excellent performance\nin text embedding tasks. Previous work usually use LoRA to fine-tune existing\nLLMs, which are limited by the data and training gap between LLMs and embedding\nmodels. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM\ntrained from scratch and fine-tuned as a text embedder. First, we add news data\nand multilingual pairs for LLM pretraining to bridge the data gap. Based on\nthis, we propose a cross-lingual retrieval dataset that enables the LLM to\nbetter integrate embeddings across different languages. Second, whereas LLMs\nuse a causal mask with token-level loss, embedding models use a bidirectional\nmask with sentence-level loss. This training gap makes full fine-tuning less\neffective than LoRA. We introduce a soft-masking mechanism to gradually\ntransition between these two types of masks, enabling the model to learn more\ncomprehensive representations. Based on this, we propose a dynamic hard\nnegative mining method that exposes the model to more difficult negative\nexamples throughout the training process. Being intuitive and effective, with\nonly approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA\nperformance on both the Massive Text Embedding Benchmark (MTEB) and Chinese\nMTEB (May 19, 2025).", "AI": {"tldr": "Introduction of Conan-embedding-v2, a novel LLM trained to improve text embedding performance by addressing data and training gaps.", "motivation": "To improve the performance of text embeddings by bridging the data and training gaps seen in current LLMs and traditional embedding models.", "method": "Conan-embedding-v2, a 1.4B-parameter model, was trained from scratch with additional news data and multilingual pairs. A soft-masking mechanism was introduced to transition between causal and bidirectional masks, alongside a dynamic hard negative mining method to enhance training.", "result": "Conan-embedding-v2 achieves state-of-the-art performance on the Massive Text Embedding Benchmark and Chinese MTEB with only 1.4B parameters.", "conclusion": "The innovative training strategies implemented in Conan-embedding-v2 significantly enhance text embedding quality across languages, providing a viable alternative to traditional methods.", "key_contributions": ["Introduction of Conan-embedding-v2 for text embedding tasks", "Implementation of a soft-masking mechanism for training transitions", "Development of a dynamic hard negative mining method to improve learning"], "limitations": "", "keywords": ["large language models", "text embeddings", "cross-lingual retrieval", "soft-masking mechanism", "dynamic hard negative mining"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2509.12908", "pdf": "https://arxiv.org/pdf/2509.12908.pdf", "abs": "https://arxiv.org/abs/2509.12908", "title": "All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning", "authors": ["Caiqi Zhang", "Chang Shu", "Ehsan Shareghi", "Nigel Collier"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main", "summary": "Confidence estimation is essential for the reliable deployment of large\nlanguage models (LLMs). Existing methods are primarily designed for factual QA\ntasks and often fail to generalize to reasoning tasks. To address this gap, we\npropose a set of training-free, graph-based confidence estimation methods\ntailored to reasoning tasks. Our approach models reasoning paths as directed\ngraphs and estimates confidence by exploiting graph properties such as\ncentrality, path convergence, and path weighting. Experiments with two LLMs on\nthree reasoning datasets demonstrate improved confidence estimation and\nenhanced performance on two downstream tasks.", "AI": {"tldr": "This paper introduces graph-based methods for confidence estimation in reasoning tasks using large language models.", "motivation": "Existing confidence estimation methods for LLMs do not generalize well to reasoning tasks, highlighting the need for improved methods.", "method": "The proposed approach uses directed graphs to model reasoning paths and estimates confidence leveraging properties like centrality and path convergence.", "result": "Experiments show that the graph-based methods provide improved confidence estimation and better performance in downstream tasks compared to existing methods.", "conclusion": "The new training-free confidence estimation methods significantly enhance the reliability of LLMs in reasoning contexts.", "key_contributions": ["Introduces graph-based confidence estimation for reasoning tasks", "Utilizes properties of graphs to enhance estimation methods", "Demonstrates improved performance on reasoning datasets"], "limitations": "", "keywords": ["confidence estimation", "large language models", "reasoning tasks"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2410.23540", "pdf": "https://arxiv.org/pdf/2410.23540.pdf", "abs": "https://arxiv.org/abs/2410.23540", "title": "Y-AR: A Mixed Reality CAD Tool for 3D Wire Bending", "authors": ["Shuo Feng", "Bo Liu", "Yifan", "Shan", "Roy Zunder", "Wei-Che Lin", "Tri Dinh", "Harald Haraldsson", "Ofer Berman", "Thijs Roumen"], "categories": ["cs.HC"], "comment": null, "summary": "Wire bending is a technique used in manufacturing to mass-produce items such\nas clips, mounts, and braces. Recent advances in programmable wire bending have\nmade this process increasingly accessible for custom fabrication. However, CNC\nwire benders are controlled using Computer Aided Manufacturing (CAM) software,\nwithout design tools, making custom designs challenging to produce. We present\nY-AR, a Computer Aided Design (CAD) interface for 3D wire bending. Y-AR uses\nmixed reality to let designers create clips, mounts, and braces to physically\nconnect objects to their surrounding environment. The interface incorporates\nsprings as design primitives which (1) apply forces to hold objects, and (2)\ncounter-act dimensional inaccuracies inherently caused by mid-air modeling and\nmeasurement errors in AR. Springs are a natural design element when working\nwith metal wire-bending given its specific material properties. We demonstrate\nworkflows to design and fabricate a range of mechanisms in Y-AR as well as\nstructures made using free-hand design tools. We found that combining\ngesture-based interaction with fabrication-aware design principles allowed\nnovice users to create functional wire connectors, even when using imprecise\nXR-based input. In our usability evaluation, all 12 participants successfully\ndesigned and fabricated a functional bottle holder using Y-AR.", "AI": {"tldr": "Y-AR is a mixed reality CAD interface for 3D wire bending that enables novice users to design and fabricate functional wire connectors with ease.", "motivation": "The motivation behind this work is to enhance the accessibility and ease of custom fabrication for wire bending processes, which are traditionally reliant on complex CAM software without integrated design tools.", "method": "The authors developed a mixed reality CAD interface called Y-AR which incorporates gesture-based interaction and design primitives like springs to facilitate the design and fabrication of wire connectors.", "result": "In a usability evaluation, 12 participants successfully designed and fabricated a functional bottle holder using the Y-AR interface, demonstrating its effectiveness for novice users.", "conclusion": "The study concludes that the combination of gesture interaction and fabrication-aware design principles overcomes challenges in accuracy, enabling successful wire connector creation.", "key_contributions": ["Development of Y-AR mixed reality CAD tool for wire bending", "Integration of springs as design primitives to counteract inaccuracies", "Successful usability testing with novice users creating functional designs"], "limitations": "The study may be limited by the small sample size of usability testing and the focus on specific wire-bending applications.", "keywords": ["Mixed Reality", "Wire Bending", "Computer Aided Design", "Gesture Interaction", "Usability Evaluation"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2509.12955", "pdf": "https://arxiv.org/pdf/2509.12955.pdf", "abs": "https://arxiv.org/abs/2509.12955", "title": "Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework", "authors": ["Heng Zhang", "Chengzhi Zhang"], "categories": ["cs.CL", "cs.DL", "cs.IR"], "comment": null, "summary": "The automated generation of research workflows is essential for improving the\nreproducibility of research and accelerating the paradigm of \"AI for Science\".\nHowever, existing methods typically extract merely fragmented procedural\ncomponents and thus fail to capture complete research workflows. To address\nthis gap, we propose an end-to-end framework that generates comprehensive,\nstructured research workflows by mining full-text academic papers. As a case\nstudy in the Natural Language Processing (NLP) domain, our paragraph-centric\napproach first employs Positive-Unlabeled (PU) Learning with SciBERT to\nidentify workflow-descriptive paragraphs, achieving an F1-score of 0.9772.\nSubsequently, we utilize Flan-T5 with prompt learning to generate workflow\nphrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of\n0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically\ncategorized into data preparation, data processing, and data analysis stages\nusing ChatGPT with few-shot learning, achieving a classification precision of\n0.958. By mapping categorized phrases to their document locations in the\ndocuments, we finally generate readable visual flowcharts of the entire\nresearch workflows. This approach facilitates the analysis of workflows derived\nfrom an NLP corpus and reveals key methodological shifts over the past two\ndecades, including the increasing emphasis on data analysis and the transition\nfrom feature engineering to ablation studies. Our work offers a validated\ntechnical framework for automated workflow generation, along with a novel,\nprocess-oriented perspective for the empirical investigation of evolving\nscientific paradigms. Source code and data are available at:\nhttps://github.com/ZH-heng/research_workflow.", "AI": {"tldr": "This paper presents a framework for generating structured research workflows by mining full-text academic papers, particularly in the NLP domain, using advanced machine learning techniques.", "motivation": "The need for automated generation of research workflows to improve reproducibility and accelerate 'AI for Science' initiatives.", "method": "An end-to-end framework that employs Positive-Unlabeled Learning with SciBERT for identifying workflow-descriptive paragraphs, followed by Flan-T5 for generating workflow phrases, and ChatGPT for categorizing these phrases into workflow stages.", "result": "Achieved an F1-score of 0.9772 for paragraph identification and precision of 0.958 in workflow categorization, with flowcharts representing entire research workflows.", "conclusion": "The proposed framework not only automates workflow generation but also provides insights into methodological shifts in NLP research over two decades.", "key_contributions": ["Development of a comprehensive framework for workflow generation from academic papers.", "Use of advanced NLP models like SciBERT and Flan-T5 for efficient extraction and categorization of research processes.", "Presentation of methodological shifts in NLP research through visual flowcharts."], "limitations": "", "keywords": ["research workflows", "Natural Language Processing", "automated generation", "Positive-Unlabeled Learning", "workflow visualization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.04629", "pdf": "https://arxiv.org/pdf/2412.04629.pdf", "abs": "https://arxiv.org/abs/2412.04629", "title": "Argumentative Experience: Reducing Confirmation Bias on Controversial Issues through LLM-Generated Multi-Persona Debates", "authors": ["Li Shi", "Houjiang Liu", "Yian Wong", "Utkarsh Mujumdar", "Dan Zhang", "Jacek Gwizdka", "Matthew Lease"], "categories": ["cs.HC", "cs.CY", "cs.IR"], "comment": "Complete reanalysis using a revised methodology; results, figures,\n  and discussion updated to reflect the new approach", "summary": "Multi-persona debate systems powered by large language models (LLMs) show\npromise in reducing confirmation bias, which can fuel echo chambers and social\npolarization. However, empirical evidence remains limited on whether they\nmeaningfully shift user attention toward belief-challenging content, promote\nbelief change, or outperform traditional debiasing strategies. To investigate\nthis, we compare an LLM-based multi-persona debate system with a two-stance\nretrieval-based system, exposing participants to multiple viewpoints on\ncontroversial topics. By collecting eye-tracking data, belief change measures,\nand qualitative feedback, our results show that while the debate system does\nnot significantly increase attention to opposing views, or make participants\nshift away from prior beliefs, it does provide a buffering effect against bias\ncaused by individual cognitive tendency. These findings shed light on both the\npromise and limits of multi-persona debate systems in information seeking, and\nwe offer design insights to guide future work toward more balanced and\nreflective information engagement.", "AI": {"tldr": "This paper investigates the effectiveness of multi-persona debate systems powered by large language models (LLMs) in mitigating confirmation bias through an empirical study comparing them with traditional debiasing methods.", "motivation": "To explore how LLM-based debate systems affect user attention towards belief-challenging content and their efficacy compared to traditional methods.", "method": "Participants were exposed to a multi-persona debate system and a two-stance retrieval-based system, with data collected through eye-tracking, belief change measures, and qualitative feedback.", "result": "The multi-persona debate system did not significantly increase attention to opposing views or shift beliefs, but it provided a buffering effect against bias from individual cognitive tendencies.", "conclusion": "The study highlights both the strengths and limitations of multi-persona debate systems in fostering balanced information engagement and offers design insights for future improvements.", "key_contributions": ["Empirical comparison of LLM-based debate systems with traditional debiasing methods", "Eye-tracking and qualitative feedback methodology", "Insights on the limits and potential of multi-persona systems in reducing confirmation bias"], "limitations": "The debate system did not significantly change attention or beliefs, suggesting limitations in its effectiveness for challenging established views.", "keywords": ["multi-persona debate", "large language models", "confirmation bias", "information seeking", "belief change"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.12960", "pdf": "https://arxiv.org/pdf/2509.12960.pdf", "abs": "https://arxiv.org/abs/2509.12960", "title": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language Models", "authors": ["Yuval Weiss", "David Demitri Africa", "Paula Buttery", "Richard Diehl Martinez"], "categories": ["cs.CL", "cs.AI"], "comment": "12 Pages, 6 Tables, 8 Figures", "summary": "Parameter-efficient methods such as LoRA have revolutionised the fine-tuning\nof LLMs. Still, their extension to pretraining via ReLoRA is less well\nunderstood, especially for small language models (SLMs), which offer lower\ncomputational and environmental costs. This work is the first systematic study\nof ReLoRA in SLMs (11M-66M parameters), evaluating both performance and\nlearning dynamics. Through ablation experiments, we find that ReLoRA generally\nperforms worse than standard training on loss, Paloma perplexity and BLiMP,\nwith the gap widening for the larger models. Further analysis of the learning\ndynamics of the models indicates that ReLoRA reinforces the rank deficiencies\nfound in smaller models. These results indicate that low-rank update strategies\nmay not transfer easily to SLM pretraining, highlighting the need for more\nresearch in the low-compute regime.", "AI": {"tldr": "This paper systematically studies ReLoRA's application in small language models for pretraining, revealing its performance drawbacks compared to standard training.", "motivation": "To investigate the effectiveness of ReLoRA in pretraining small language models, especially in terms of computational efficiency and learning dynamics.", "method": "A systematic evaluation involving ablation experiments across small language models with 11M to 66M parameters was conducted to assess performance on various metrics against standard training.", "result": "ReLoRA generally performed worse than standard training across multiple performance measures, with a widening gap in larger models and reinforcement of rank deficiencies.", "conclusion": "Low-rank update strategies may not be suitable for SLM pretraining, necessitating further research in this area.", "key_contributions": ["First systematic study of ReLoRA in small language models", "Identification of performance issues in ReLoRA compared to standard methods", "Insights into the learning dynamics affected by ReLoRA in SLMs"], "limitations": "Results show that ReLoRA underperforms and reinforces deficiencies in smaller models, suggesting limitations in its transferability to pretraining.", "keywords": ["ReLoRA", "small language models", "pretraining", "low-rank updates", "learning dynamics"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2504.11795", "pdf": "https://arxiv.org/pdf/2504.11795.pdf", "abs": "https://arxiv.org/abs/2504.11795", "title": "Schemex: Interactive Structural Abstraction from Examples with Contrastive Refinement", "authors": ["Sitong Wang", "Samia Menon", "Dingzeyu Li", "Xiaojuan Ma", "Richard Zemel", "Lydia B. Chilton"], "categories": ["cs.HC"], "comment": null, "summary": "Each type of creative or communicative work is underpinned by an implicit\nstructure. People learn these structures from examples - a process known in\ncognitive science as schema induction. However, inducing schemas is\nchallenging, as structural patterns are often obscured by surface-level\nvariation. We present Schemex, an interactive visual workflow that scaffolds\nschema induction through clustering, abstraction, and contrastive refinement.\nSchemex supports users through visual representations and interactive\nexploration that connect abstract structures to concrete examples, promoting\ntransparency, adaptability, and effective human-AI collaboration. In our user\nstudy, participants reported significantly greater insight and confidence in\nthe schemas developed with Schemex compared to those created using a baseline\nof an AI reasoning model. We conclude by discussing the broader implications of\nstructural abstraction and contrastive refinement across domains.", "AI": {"tldr": "Schemex is a visual tool designed to aid schema induction through interactive exploration and clustering, enhancing human-AI collaboration and user insight.", "motivation": "The challenge in schema induction arises from the obscuring of structural patterns by surface-level variability, necessitating tools that facilitate understanding and categorizing of these schemas.", "method": "Schemex employs interactive visual representations to guide users in clustering and refining their understanding of schemas, allowing for exploration of connections between abstract structures and concrete examples.", "result": "User studies indicated that participants using Schemex felt more insight and confidence in their developed schemas than those using a standard AI model.", "conclusion": "The findings suggest that structural abstraction and contrastive refinement have wider implications in enhancing understanding across various domains, advocating for tools that support these processes.", "key_contributions": ["Development of Schemex, an interactive visual workflow for schema induction.", "Empirical evidence showing enhanced user insight and confidence through the use of Schemex.", "Discussion of broader implications for structural abstraction in human-AI collaboration."], "limitations": "The abstract does not specify limitations, but it is likely that the effectiveness may vary across different user demographics or types of tasks.", "keywords": ["schema induction", "human-AI collaboration", "visualization", "interaction", "clustering"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.12961", "pdf": "https://arxiv.org/pdf/2509.12961.pdf", "abs": "https://arxiv.org/abs/2509.12961", "title": "Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews", "authors": ["Chenye Zou", "Xingyue Wen", "Tianyi Hu", "Qian Janice Wang", "Daniel Hershcovich"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Recent advances in large language models (LLMs) have opened the door to\nculture-aware language tasks. We introduce the novel problem of adapting wine\nreviews across Chinese and English, which goes beyond literal translation by\nincorporating regional taste preferences and culture-specific flavor\ndescriptors. In a case study on cross-cultural wine review adaptation, we\ncompile the first parallel corpus of professional reviews, containing 8k\nChinese and 16k Anglophone reviews. We benchmark both\nneural-machine-translation baselines and state-of-the-art LLMs with automatic\nmetrics and human evaluation. For the latter, we propose three culture-oriented\ncriteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness\n-- to assess how naturally a translated review resonates with target-culture\nreaders. Our analysis shows that current models struggle to capture cultural\nnuances, especially in translating wine descriptions across different cultures.\nThis highlights the challenges and limitations of translation models in\nhandling cultural content.", "AI": {"tldr": "This paper addresses the challenge of adapting wine reviews between Chinese and English by incorporating culture-specific flavors and regional preferences, showcasing limitations of current translation models.", "motivation": "To tackle the limitations of existing translation models in capturing cultural nuances in wine reviews between different languages.", "method": "The authors compile a parallel corpus of 8k Chinese and 16k English wine reviews and benchmark translation models against cultural metrics.", "result": "Automated and human evaluations reveal that current models inadequately address cultural aspects, struggling particularly with wine descriptions.", "conclusion": "The findings underscore the necessity for improved models that can effectively translate cultural content, particularly in niche areas like wine reviews.", "key_contributions": ["Introduction of a novel problem of culture-aware language adaptation for wine reviews", "Creation of the first parallel corpus for Chinese and English wine reviews", "Proposal of culture-oriented assessment criteria for translation quality."], "limitations": "Current models do not perform well in capturing cultural nuances in translations, particularly for flavor descriptions.", "keywords": ["Cultural Proximity", "Wine Reviews", "Neural Machine Translation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.12994", "pdf": "https://arxiv.org/pdf/2509.12994.pdf", "abs": "https://arxiv.org/abs/2509.12994", "title": "SitLLM: Large Language Models for Sitting Posture Health Understanding via Pressure Sensor Data", "authors": ["Jian Gao", "Fufangchen Zhao", "Yiyang Zhang", "Danfeng Yan"], "categories": ["cs.CL"], "comment": null, "summary": "Poor sitting posture is a critical yet often overlooked factor contributing\nto long-term musculoskeletal disorders and physiological dysfunctions. Existing\nsitting posture monitoring systems, although leveraging visual, IMU, or\npressure-based modalities, often suffer from coarse-grained recognition and\nlack the semantic expressiveness necessary for personalized feedback. In this\npaper, we propose \\textbf{SitLLM}, a lightweight multimodal framework that\nintegrates flexible pressure sensing with large language models (LLMs) to\nenable fine-grained posture understanding and personalized health-oriented\nresponse generation. SitLLM comprises three key components: (1) a\n\\textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps\ninto spatial patches and injects local noise perturbations for robust feature\nextraction; (2) a \\textit{Prompt-Driven Cross-Modal Alignment Module} that\nreprograms sensor embeddings into the LLM's semantic space via multi-head\ncross-attention using the pre-trained vocabulary embeddings; and (3) a\n\\textit{Multi-Context Prompt Module} that fuses feature-level, structure-level,\nstatistical-level, and semantic-level contextual information to guide\ninstruction comprehension.", "AI": {"tldr": "SitLLM addresses poor sitting posture through a multimodal framework integrating pressure sensing and large language models for personalized health feedback.", "motivation": "To address the limitations of existing sitting posture monitoring systems that suffer from coarse-grained recognition and lack of semantic expressiveness.", "method": "SitLLM combines flexible pressure sensing with LLMs, featuring three components: a Gaussian-Robust Sensor Embedding Module for robust feature extraction, a Prompt-Driven Cross-Modal Alignment Module for embedding transformation, and a Multi-Context Prompt Module for contextual information fusion.", "result": "SitLLM enables fine-grained posture understanding and personalized response generation, enhancing health-oriented feedback in real-time.", "conclusion": "The proposed framework demonstrates potential in improving personalized health responses related to sitting posture through effective integration of pressure sensing and language models.", "key_contributions": ["Lightweight multimodal framework for posture monitoring.", "Integration of pressure sensing with large language models for real-time feedback.", "Contextual information fusion for improved understanding and response generation."], "limitations": "", "keywords": ["Sitting Posture", "Multimodal Framework", "Large Language Models", "Health Informatics", "Pressure Sensing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.13047", "pdf": "https://arxiv.org/pdf/2509.13047.pdf", "abs": "https://arxiv.org/abs/2509.13047", "title": "Multi-Model Synthetic Training for Mission-Critical Small Language Models", "authors": ["Nolan Platt", "Pragyansmita Nayak"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 68T50", "I.2.7; I.2.6"], "comment": "8 pages. Accepted as a full paper to the 3rd International Conference\n  on Foundation and Large Language Models (IEEE FLLM) 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their appli- cation to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing over- fitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models - when fine\ntuned properly - can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expand- ing research in the growing field of specialized\nsmall language models, our approach has immediate applications in maritime\nsafety, security operations, and vessel traffic management systems in various\nindustries.", "AI": {"tldr": "This paper introduces a cost-effective method for applying LLMs to maritime intelligence by transforming AIS vessel tracking data into synthetic Q&A pairs, achieving significant accuracy with smaller models.", "motivation": "To address the challenges of applying LLMs in specialized fields due to the scarcity and complexity of domain-specific training data.", "method": "The approach utilizes LLMs as one-time teachers to convert 3.2 billion AIS records into 21,543 synthetic question and answer pairs, employing multi-model generation techniques to ensure accurate reasoning and prevent overfitting.", "result": "The fine-tuned Qwen2.5-7B model achieved 75% accuracy on maritime tasks while being 261x cheaper than using larger models for direct inference.", "conclusion": "Smaller, properly fine-tuned models can achieve near-equivalent accuracy to larger models at a fraction of the cost, with important implications for specialized AI applications in maritime sectors.", "key_contributions": ["Novel approach to synthetic dataset generation for LLMs in specialized fields.", "Demonstration of significant cost reduction in model training for maritime intelligence applications.", "Framework presented is highly reproducible for other domains with similar challenges."], "limitations": "", "keywords": ["Large Language Models", "synthetic dataset generation", "maritime intelligence"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2509.13081", "pdf": "https://arxiv.org/pdf/2509.13081.pdf", "abs": "https://arxiv.org/abs/2509.13081", "title": "Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO", "authors": ["Francesco Pappone", "Ruggero Marino Lazzaroni", "Federico Califano", "Niccolò Gentile", "Roberto Marras"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) excel at generating human-like text,\naligning their outputs with complex, qualitative goals like pedagogical\nsoundness remains a significant challenge. Standard reinforcement learning\ntechniques often rely on slow and expensive LLM-as-a-judge evaluations or on\nbrittle, keyword-based metrics like ROUGE, which fail to capture the semantic\nessence of a high-quality explanation. In this work, we introduce a novel\napproach to reward shaping within the Group Relative Policy Optimisation (GRPO)\nframework. Our central contribution is the use of a small, efficient\nencoder-only transformer as a semantic reward model. This model provides a\ndense, semantically rich reward signal based on the cosine similarity between a\ngenerated explanation and a ground-truth reference, guiding the policy towards\nexplanations that are not just factually correct but also structurally and\nconceptually aligned with expert reasoning. We apply this method to the task of\ntraining a model for the Italian medical-school entrance examinations,\nfollowing standard domain-adaptive continued pre-training (CPT) and supervised\nfine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic\nreward significantly improves explanation faithfulness and clarity over a\nstrong SFT baseline, showcasing the power of using lightweight encoder models\nfor nuanced reward shaping in complex generation tasks", "AI": {"tldr": "This paper introduces a novel approach for improving Large Language Models' (LLMs) text generation quality by using a lightweight transformer as a semantic reward model within the Group Relative Policy Optimisation (GRPO) framework, specifically applied to generating explanations for medical entrance exams.", "motivation": "Aligning LLM outputs with qualitative goals like pedagogical soundness is challenging due to limitations in existing evaluation metrics.", "method": "The authors propose a reward shaping approach using a small encoder-only transformer that provides a dense semantic reward signal based on cosine similarity between generated explanations and ground-truth references.", "result": "The approach significantly improves explanation faithfulness and clarity in generating medical-school entrance exam responses compared to a strong baseline.", "conclusion": "The use of lightweight encoder models for nuanced reward shaping shows promise in enhancing the quality of generated explanations in complex tasks.", "key_contributions": ["Introduction of a semantic reward model within the GRPO framework.", "Demonstration of improved explanation clarity and faithfulness using a lightweight transformer.", "Application of this method to medical educational contexts."], "limitations": "", "keywords": ["Large Language Models", "reward shaping", "semantic similarity", "Group Relative Policy Optimisation", "medical education"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.13127", "pdf": "https://arxiv.org/pdf/2509.13127.pdf", "abs": "https://arxiv.org/abs/2509.13127", "title": "Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning", "authors": ["Sijia Cui", "Shuai Xu", "Aiyao He", "Yanna Wang", "Bo Xu"], "categories": ["cs.CL"], "comment": "Accepted to IJCNN 2025", "summary": "Recent advancements in Large Language Models(LLMs) have led to the\ndevelopment of LLM-based AI agents. A key challenge is the creation of agents\nthat can effectively ground themselves in complex, adversarial long-horizon\nenvironments. Existing methods mainly focus on (1) using LLMs as policies to\ninteract with the environment through generating low-level feasible actions,\nand (2) utilizing LLMs to generate high-level tasks or language guides to\nstimulate action generation. However, the former struggles to generate reliable\nactions, while the latter relies heavily on expert experience to translate\nhigh-level tasks into specific action sequences. To address these challenges,\nwe introduce the Plan with Language, Act with Parameter (PLAP) planning\nframework that facilitates the grounding of LLM-based agents in long-horizon\nenvironments. The PLAP method comprises three key components: (1) a skill\nlibrary containing environment-specific parameterized skills, (2) a skill\nplanner powered by LLMs, and (3) a skill executor converting the parameterized\nskills into executable action sequences. We implement PLAP in MicroRTS, a\nlong-horizon real-time strategy game that provides an unfamiliar and\nchallenging environment for LLMs. The experimental results demonstrate the\neffectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting\noutperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully\ncrafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.\nAdditionally, we design comprehensive evaluation metrics and test 6\nclosed-source and 2 open-source LLMs within the PLAP framework, ultimately\nreleasing an LLM leaderboard ranking long-horizon skill planning ability. Our\ncode is available at https://github.com/AI-Research-TeamX/PLAP.", "AI": {"tldr": "Introduction of the PLAP framework for grounding LLM-based agents in long-horizon environments, demonstrating significant improvements in performance in MicroRTS.", "motivation": "To address challenges faced by LLM-based AI agents in grounding themselves in complex, adversarial long-horizon environments, where existing methods struggle with reliable action generation and expert reliance.", "method": "The PLAP framework consists of a skill library of parameterized skills, a skill planner powered by LLMs, and a skill executor to convert skills into executable actions, implemented in the MicroRTS game.", "result": "PLAP, particularly driven by GPT-4o in a zero-shot setting, outperformed 80% of baseline agents; Qwen2-72B-driven PLAP excelled against top scripted agents like CoacAI.", "conclusion": "PLAP provides a robust method for grounding LLMs in complex environments, with an effective evaluation metric and LLM leaderboard.", "key_contributions": ["Introduction of the PLAP planning framework", "Demonstration of superior performance in long-horizon environments", "Release of an LLM leaderboard for skill planning"], "limitations": "", "keywords": ["Large Language Models", "AI agents", "skill planning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.13154", "pdf": "https://arxiv.org/pdf/2509.13154.pdf", "abs": "https://arxiv.org/abs/2509.13154", "title": "LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals", "authors": ["Jinxin Li", "Gang Tu", "ShengYu Cheng", "Junjie Hu", "Jinting Wang", "Rui Chen", "Zhilong Zhou", "Dongbo Shan"], "categories": ["cs.CL"], "comment": null, "summary": "Hallucination remains a critical barrier for deploying large language models\n(LLMs) in reliability-sensitive applications. Existing detection methods\nlargely fall into two categories: factuality checking, which is fundamentally\nconstrained by external knowledge coverage, and static hidden-state analysis,\nthat fails to capture deviations in reasoning dynamics. As a result, their\neffectiveness and robustness remain limited. We propose HSAD (Hidden Signal\nAnalysis-based Detection), a novel hallucination detection framework that\nmodels the temporal dynamics of hidden representations during autoregressive\ngeneration. HSAD constructs hidden-layer signals by sampling activations across\nlayers, applies Fast Fourier Transform (FFT) to obtain frequency-domain\nrepresentations, and extracts the strongest non-DC frequency component as\nspectral features. Furthermore, by leveraging the autoregressive nature of\nLLMs, HSAD identifies optimal observation points for effective and reliable\ndetection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over\n10 percentage points improvement compared to prior state-of-the-art methods. By\nintegrating reasoning-process modeling with frequency-domain analysis, HSAD\nestablishes a new paradigm for robust hallucination detection in LLMs.", "AI": {"tldr": "A new framework, HSAD, is proposed for detecting hallucinations in large language models by analyzing hidden representations and frequency-domain features.", "motivation": "To address the critical barrier of hallucination in large language models for reliability-sensitive applications.", "method": "HSAD uses temporal dynamics of hidden states during generation, sampling layer activations and applying Fast Fourier Transform for frequency-domain analysis.", "result": "HSAD outperforms existing detection methods by over 10 percentage points on multiple benchmarks including TruthfulQA.", "conclusion": "HSAD provides a robust new paradigm for hallucination detection, combining reasoning-process modeling with frequency-domain analysis.", "key_contributions": ["Introduces HSAD, a new hallucination detection framework.", "Uses temporal analysis of hidden representations and frequency-domain features.", "Achieves significant performance improvements over existing methods."], "limitations": "", "keywords": ["hallucination detection", "large language models", "frequency-domain analysis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.13196", "pdf": "https://arxiv.org/pdf/2509.13196.pdf", "abs": "https://arxiv.org/abs/2509.13196", "title": "The Few-shot Dilemma: Over-prompting Large Language Models", "authors": ["Yongjian Tang", "Doruk Tuncel", "Christian Koerner", "Thomas Runkler"], "categories": ["cs.CL"], "comment": "accepted for the main track of FLLM", "summary": "Over-prompting, a phenomenon where excessive examples in prompts lead to\ndiminished performance in Large Language Models (LLMs), challenges the\nconventional wisdom about in-context few-shot learning. To investigate this\nfew-shot dilemma, we outline a prompting framework that leverages three\nstandard few-shot selection methods - random sampling, semantic embedding, and\nTF-IDF vectors - and evaluate these methods across multiple LLMs, including\nGPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.\nOur experimental results reveal that incorporating excessive domain-specific\nexamples into prompts can paradoxically degrade performance in certain LLMs,\nwhich contradicts the prior empirical conclusion that more relevant few-shot\nexamples universally benefit LLMs. Given the trend of LLM-assisted software\nengineering and requirement analysis, we experiment with two real-world\nsoftware requirement classification datasets. By gradually increasing the\nnumber of TF-IDF-selected and stratified few-shot examples, we identify their\noptimal quantity for each LLM. This combined approach achieves superior\nperformance with fewer examples, avoiding the over-prompting problem, thus\nsurpassing the state-of-the-art by 1% in classifying functional and\nnon-functional requirements.", "AI": {"tldr": "This paper investigates the phenomenon of over-prompting in LLMs, revealing that excessive examples can degrade performance. It proposes a framework using few-shot selection methods to optimize prompt efficiency.", "motivation": "The study is motivated by the discrepancy in previous findings on few-shot prompting for LLMs and the practical implications for LLM-assisted software engineering.", "method": "The authors outline a prompting framework using random sampling, semantic embedding, and TF-IDF vectors for few-shot selection. They evaluate various LLMs across software requirement classification tasks.", "result": "The research shows that carefully optimising the number of domain-specific examples in prompts enhances LLM performance, surpassing the previous state-of-the-art by 1% in requirement classification.", "conclusion": "The findings suggest that fewer, carefully selected examples can improve LLM performance, addressing the over-prompting issue, which has implications for LLM applications in software requirements analysis.", "key_contributions": ["Identification of the over-prompting phenomenon in LLMs", "Development of a few-shot selection framework", "Demonstration of improved task performance with optimized examples"], "limitations": "The study focuses on a limited number of LLMs and specific classification tasks, which may not generalize across all domains or applications.", "keywords": ["over-prompting", "few-shot learning", "Large Language Models", "TF-IDF", "software requirement classification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.13244", "pdf": "https://arxiv.org/pdf/2509.13244.pdf", "abs": "https://arxiv.org/abs/2509.13244", "title": "Evaluating LLM Alignment on Personality Inference from Real-World Interview Data", "authors": ["Jianfeng Zhu", "Julina Maharjan", "Xinyu Li", "Karin G. Coifman", "Ruoming Jin"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures", "summary": "Large Language Models (LLMs) are increasingly deployed in roles requiring\nnuanced psychological understanding, such as emotional support agents,\ncounselors, and decision-making assistants. However, their ability to interpret\nhuman personality traits, a critical aspect of such applications, remains\nunexplored, particularly in ecologically valid conversational settings. While\nprior work has simulated LLM \"personas\" using discrete Big Five labels on\nsocial media data, the alignment of LLMs with continuous, ground-truth\npersonality assessments derived from natural interactions is largely\nunexamined. To address this gap, we introduce a novel benchmark comprising\nsemi-structured interview transcripts paired with validated continuous Big Five\ntrait scores. Using this dataset, we systematically evaluate LLM performance\nacross three paradigms: (1) zero-shot and chain-of-thought prompting with\nGPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA\narchitectures, and (3) regression using static embeddings from pretrained BERT\nand OpenAI's text-embedding-3-small. Our results reveal that all Pearson\ncorrelations between model predictions and ground-truth personality traits\nremain below 0.26, highlighting the limited alignment of current LLMs with\nvalidated psychological constructs. Chain-of-thought prompting offers minimal\ngains over zero-shot, suggesting that personality inference relies more on\nlatent semantic representation than explicit reasoning. These findings\nunderscore the challenges of aligning LLMs with complex human attributes and\nmotivate future work on trait-specific prompting, context-aware modeling, and\nalignment-oriented fine-tuning.", "AI": {"tldr": "This paper evaluates the ability of Large Language Models (LLMs) to interpret human personality traits using a new benchmark of interview transcripts and validated Big Five trait scores, revealing limitations in current model alignments with psychological constructs.", "motivation": "To explore the unexplored ability of LLMs to interpret human personality traits in conversational settings, which is crucial for applications like emotional support and decision-making.", "method": "A novel benchmark of semi-structured interview transcripts with validated Big Five trait scores is used to evaluate LLM performance through zero-shot prompting, fine-tuning, and regression with embeddings.", "result": "Pearson correlations between model predictions and ground-truth personality traits are below 0.26, indicating limited alignment of LLMs with validated psychological constructs.", "conclusion": "The findings highlight the challenges in aligning LLMs with complex human personality traits and suggest avenues for future research focusing on trait-specific prompting and improved alignment methods.", "key_contributions": ["Introduction of a novel benchmark for evaluating LLMs against continuous Big Five trait scores", "Systematic evaluation of LLM performance using various paradigms", "Identification of limitations in current LLMs' ability to align with psychological constructs"], "limitations": "The results indicate that current LLMs do not effectively align with validated personality assessments, suggesting a need for improved methodologies.", "keywords": ["Large Language Models", "Personality Traits", "Big Five", "Human-Computer Interaction", "AI applications"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.13282", "pdf": "https://arxiv.org/pdf/2509.13282.pdf", "abs": "https://arxiv.org/abs/2509.13282", "title": "ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement", "authors": ["Ali Salamatian", "Amirhossein Abaskohi", "Wan-Cyuan Fan", "Mir Rayat Imtiaz Hossain", "Leonid Sigal", "Giuseppe Carenini"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "EMNLP 2025", "summary": "Charts are a crucial visual medium for communicating and representing\ninformation. While Large Vision-Language Models (LVLMs) have made progress on\nchart question answering (CQA), the task remains challenging, particularly when\nmodels attend to irrelevant regions of the chart. In this work, we present\nChartGaze, a new eye-tracking dataset that captures human gaze patterns during\nchart reasoning tasks. Through a systematic comparison of human and model\nattention, we find that LVLMs often diverge from human gaze, leading to reduced\ninterpretability and accuracy. To address this, we propose a gaze-guided\nattention refinement that aligns image-text attention with human fixations. Our\napproach improves both answer accuracy and attention alignment, yielding gains\nof up to 2.56 percentage points across multiple models. These results\ndemonstrate the promise of incorporating human gaze to enhance both the\nreasoning quality and interpretability of chart-focused LVLMs.", "AI": {"tldr": "The paper presents ChartGaze, a dataset tracking human gaze patterns during chart reasoning tasks, aiming to improve attention alignment in Large Vision-Language Models (LVLMs).", "motivation": "To tackle the challenges in chart question answering (CQA) caused by LVLMs attending to irrelevant regions, which affects both interpretability and accuracy.", "method": "A new eye-tracking dataset, ChartGaze, is introduced, and a gaze-guided attention refinement method is proposed to align LVLM attention with human gaze patterns.", "result": "The proposed method results in improved answer accuracy and attention alignment, achieving up to 2.56 percentage points gain across models.", "conclusion": "Incorporating human gaze significantly enhances the reasoning quality and interpretability of chart-focused LVLMs.", "key_contributions": ["Introduction of the ChartGaze dataset for eye-tracking during chart reasoning tasks.", "A novel gaze-guided attention refinement method for aligning model and human attention.", "Demonstrated improvements in answer accuracy and attention alignment in LVLMs."], "limitations": "", "keywords": ["Large Vision-Language Models", "Chart Question Answering", "Eye-tracking", "Gaze-guided attention", "Human-computer interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.13309", "pdf": "https://arxiv.org/pdf/2509.13309.pdf", "abs": "https://arxiv.org/abs/2509.13309", "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents", "authors": ["Zile Qiao", "Guoxin Chen", "Xuanzhong Chen", "Donglei Yu", "Wenbiao Yin", "Xinyu Wang", "Zhen Zhang", "Baixuan Li", "Huifeng Yin", "Kuan Li", "Rui Min", "Minpeng Liao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Recent advances in deep-research systems have demonstrated the potential for\nAI agents to autonomously discover and synthesize knowledge from external\nsources. In this paper, we introduce WebResearcher, a novel framework for\nbuilding such agents through two key components: (1) WebResearcher, an\niterative deep-research paradigm that reformulates deep research as a Markov\nDecision Process, where agents periodically consolidate findings into evolving\nreports while maintaining focused workspaces, overcoming the context\nsuffocation and noise contamination that plague existing mono-contextual\napproaches; and (2) WebFrontier, a scalable data synthesis engine that\ngenerates high-quality training data through tool-augmented complexity\nescalation, enabling systematic creation of research tasks that bridge the gap\nbetween passive knowledge recall and active knowledge construction. Notably, we\nfind that the training data from our paradigm significantly enhances tool-use\ncapabilities even for traditional mono-contextual methods. Furthermore, our\nparadigm naturally scales through parallel thinking, enabling concurrent\nmulti-agent exploration for more comprehensive conclusions. Extensive\nexperiments across 6 challenging benchmarks demonstrate that WebResearcher\nachieves state-of-the-art performance, even surpassing frontier proprietary\nsystems.", "AI": {"tldr": "WebResearcher is a framework that enables AI agents to autonomously synthesize knowledge from external sources, using a Markov Decision Process and a data synthesis engine for improved research outcomes.", "motivation": "To overcome limitations of existing mono-contextual research approaches and enhance AI agents' ability to discover and synthesize knowledge.", "method": "WebResearcher reformulates deep research as a Markov Decision Process, combining it with WebFrontier, a scalable data synthesis engine for generating high-quality training data.", "result": "WebResearcher achieves state-of-the-art performance on 6 benchmarks, surpassing proprietary systems and enhancing tool-use capabilities of traditional methods.", "conclusion": "The proposed paradigm enables improved knowledge construction and concurrent explorations through multi-agent systems, demonstrating significant advancements in deep research methodologies.", "key_contributions": ["Introduction of WebResearcher framework", "Development of tool-augmented complexity escalation in WebFrontier", "Demonstration of state-of-the-art performance across benchmarks"], "limitations": "", "keywords": ["AI agents", "knowledge synthesis", "Markov Decision Process", "multi-agent systems", "deep research"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.13310", "pdf": "https://arxiv.org/pdf/2509.13310.pdf", "abs": "https://arxiv.org/abs/2509.13310", "title": "Scaling Agents via Continual Pre-training", "authors": ["Liangcai Su", "Zhen Zhang", "Guangyu Li", "Zhuo Chen", "Chenxi Wang", "Maojia Song", "Xinyu Wang", "Kuan Li", "Jialong Wu", "Xuanzhong Chen", "Zile Qiao", "Zhongwang Zhang", "Huifeng Yin", "Shihao Cai", "Runnan Fang", "Zhengwei Tao", "Wenbiao Yin", "Chenxiong Qian", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.", "AI": {"tldr": "This paper introduces Agentic Continual Pre-training (Agentic CPT) for deep research agents, leading to the development of AgentFounder, a model that excels at multi-step reasoning and tool use, achieving state-of-the-art performance on multiple benchmarks.", "motivation": "To address the poor performance of general-purpose foundation models in agentic tasks, particularly in open-source implementations, by proposing a framework for robust agentic foundational models.", "method": "The paper introduces Agentic Continual Pre-training (Agentic CPT) as a new approach in the training pipeline for deep research agents, which facilitates the concurrent learning of diverse agentic behaviors with alignment to expert demonstrations.", "result": "AgentFounder-30B model achieves state-of-the-art performance on 10 benchmarks, notably scoring 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE, demonstrating improved tool-use ability and multi-step reasoning.", "conclusion": "The incorporation of Agentic CPT into model training significantly enhances the capabilities of foundation models in agentic tasks and presents a new direction for future research in this area.", "key_contributions": ["Introduction of Agentic Continual Pre-training (Agentic CPT)", "Development of the AgentFounder model", "Achievement of state-of-the-art performance on multiple benchmarks"], "limitations": "", "keywords": ["Agentic systems", "Continual pre-training", "Large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.13311", "pdf": "https://arxiv.org/pdf/2509.13311.pdf", "abs": "https://arxiv.org/abs/2509.13311", "title": "Towards General Agentic Intelligence via Environment Scaling", "authors": ["Runnan Fang", "Shihao Cai", "Baixuan Li", "Jialong Wu", "Guangyu Li", "Wenbiao Yin", "Xinyu Wang", "Xiaobin Wang", "Liangcai Su", "Zhen Zhang", "Shibin Wu", "Zhengwei Tao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Advanced agentic intelligence is a prerequisite for deploying Large Language\nModels in practical, real-world applications. Diverse real-world APIs demand\nprecise, robust function-calling intelligence, which needs agents to develop\nthese capabilities through interaction in varied environments. The breadth of\nfunction-calling competence is closely tied to the diversity of environments in\nwhich agents are trained. In this work, we scale up environments as a step\ntowards advancing general agentic intelligence. This gives rise to two central\nchallenges: (i) how to scale environments in a principled manner, and (ii) how\nto effectively train agentic capabilities from experiences derived through\ninteractions with these environments. To address these, we design a scalable\nframework that automatically constructs heterogeneous environments that are\nfully simulated, systematically broadening the space of function-calling\nscenarios. We further adapt a two-phase agent fine-tuning strategy: first\nendowing agents with fundamental agentic capabilities, then specializing them\nfor domain-specific contexts. Extensive experiments on agentic benchmarks,\ntau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,\nAgentScaler, significantly enhances the function-calling capability of models.", "AI": {"tldr": "This paper proposes a scalable framework, AgentScaler, for enhancing function-calling capabilities of Large Language Models (LLMs) through diverse environment interactions and a two-phase fine-tuning strategy.", "motivation": "The necessity for advanced agentic intelligence in LLMs for real-world applications, particularly with diverse APIs that require robust function-calling competence.", "method": "We designed a scalable framework that automatically creates heterogeneous environments for agents to interact with and learn from, and a two-phase fine-tuning strategy is employed for training agentic capabilities.", "result": "Extensive experiments show that AgentScaler significantly improves the function-calling capability of models evaluated on various agentic benchmarks.", "conclusion": "By scaling environments and employing a systematic training approach, we advance the development of agentic intelligence in LLMs.", "key_contributions": ["Development of a scalable framework for agent training in diverse environments", "Introduction of a two-phase fine-tuning strategy for enhancing agentic capabilities", "Demonstration of significant improvements in function-calling abilities on benchmark tests"], "limitations": "", "keywords": ["agentic intelligence", "Large Language Models", "function-calling", "scalable framework", "fine-tuning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.13312", "pdf": "https://arxiv.org/pdf/2509.13312.pdf", "abs": "https://arxiv.org/abs/2509.13312", "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research", "authors": ["Zijian Li", "Xin Guan", "Bo Zhang", "Shen Huang", "Houquan Zhou", "Shaopeng Lai", "Ming Yan", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jun Zhang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "An agent system for open-ended deep research", "summary": "This paper tackles open-ended deep research (OEDR), a complex challenge where\nAI agents must synthesize vast web-scale information into insightful reports.\nCurrent approaches are plagued by dual-fold limitations: static research\npipelines that decouple planning from evidence acquisition and one-shot\ngeneration paradigms that easily suffer from long-context failure issues like\n\"loss in the middle\" and hallucinations. To address these challenges, we\nintroduce WebWeaver, a novel dual-agent framework that emulates the human\nresearch process. The planner operates in a dynamic cycle, iteratively\ninterleaving evidence acquisition with outline optimization to produce a\ncomprehensive, source-grounded outline linking to a memory bank of evidence.\nThe writer then executes a hierarchical retrieval and writing process,\ncomposing the report section by section. By performing targeted retrieval of\nonly the necessary evidence from the memory bank for each part, it effectively\nmitigates long-context issues. Our framework establishes a new state-of-the-art\nacross major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and\nDeepResearchGym. These results validate our human-centric, iterative\nmethodology, demonstrating that adaptive planning and focused synthesis are\ncrucial for producing high-quality, reliable, and well-structured reports.", "AI": {"tldr": "WebWeaver is a novel dual-agent framework for open-ended deep research that improves AI's ability to synthesize web-scale information into insightful reports.", "motivation": "To address limitations in current AI research generation methods, particularly static pipelines and hallucination issues.", "method": "WebWeaver uses a dual-agent approach where a planner dynamically interleaves evidence acquisition with outline optimization, followed by a writer that composes reports section by section using hierarchical retrieval.", "result": "WebWeaver achieves state-of-the-art performance on major OEDR benchmarks such as DeepResearch Bench, DeepConsult, and DeepResearchGym.", "conclusion": "The iterative and adaptive planning improves the production of high-quality and structured reports, emphasizing the importance of focusing on synthesis.", "key_contributions": ["Introduction of the WebWeaver framework for OEDR.", "Dynamic interleaving of planning and evidence acquisition.", "Targeted retrieval processes that mitigate long-context issues."], "limitations": "", "keywords": ["open-ended deep research", "AI synthesis", "human-centric research"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.13313", "pdf": "https://arxiv.org/pdf/2509.13313.pdf", "abs": "https://arxiv.org/abs/2509.13313", "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization", "authors": ["Xixi Wu", "Kuan Li", "Yida Zhao", "Liwen Zhang", "Litu Ou", "Huifeng Yin", "Zhongwang Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Minhao Cheng", "Shuai Wang", "Hong Cheng", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents.", "AI": {"tldr": "ReSum is a new paradigm for LLM-based web agents that improves performance on complex queries by enabling indefinite exploration through context summarization.", "motivation": "LLM-based web agents struggle with context limitations when handling complex queries, leading to incomplete solutions.", "method": "ReSum employs periodic context summarization to convert interaction histories into compact reasoning states, allowing web agents to bypass context constraints.", "result": "ReSum shows an average improvement of 4.5% over the ReAct paradigm, with gains of up to 8.2% using ReSum-GRPO training on web agents across three benchmarks.", "conclusion": "ReSum represents a significant advancement in enabling LLM-based agents to handle complex queries more effectively without being hindered by context window limitations.", "key_contributions": ["Introduction of ReSum for context summarization", "Integration of GRPO with segmented trajectory training", "Demonstrated substantial performance improvements on multiple benchmarks"], "limitations": "", "keywords": ["Large Language Models", "web agents", "context summarization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.13316", "pdf": "https://arxiv.org/pdf/2509.13316.pdf", "abs": "https://arxiv.org/abs/2509.13316", "title": "Do Natural Language Descriptions of Model Activations Convey Privileged Information?", "authors": ["Millicent Li", "Alberto Mario Ceballos Arroyo", "Giordano Rogers", "Naomi Saphra", "Byron C. Wallace"], "categories": ["cs.CL", "cs.LG"], "comment": "34 pages, 6 figures", "summary": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets are not ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the activations of the target LLM being decoded. Taken\ntogether, our results indicate a need for targeted benchmarks and experimental\ncontrols to rigorously assess whether verbalization methods provide meaningful\ninsights into the operations of LLMs.", "AI": {"tldr": "This paper critiques the effectiveness of LLM activation verbalization methods in providing insights into target model internals, revealing that they often reflect the verbalizer LLM's knowledge rather than the target model's operations.", "motivation": "To determine whether LLM activation verbalization methods provide true insights into the internal workings of LLMs or merely reflect input information.", "method": "The authors evaluate popular verbalization methods across established datasets, analyze their performance without access to target model internals, and conduct controlled experiments to assess what the verbalizations truly reflect.", "result": "The study finds that existing benchmarks are inadequate for assessing verbalization methods, as results often depend more on the capabilities of the verbalizer LLM than the target LLM.", "conclusion": "The authors advocate for the development of targeted benchmarks and experimental controls to ensure meaningful evaluation of verbalization methods in understanding LLM operations.", "key_contributions": ["Critique of current LLM activation verbalization methods", "Introduction of the concept that verbalizations reflect the verbalizer more than the target model", "Call for the creation of better benchmarks for evaluating verbalization techniques."], "limitations": "The findings highlight a dependence on datasets that do not effectively assess the internal workings of LLMs.", "keywords": ["LLM", "activation verbalization", "interpretability", "natural language descriptions", "benchmarks"], "importance_score": 8, "read_time_minutes": 34}}
{"id": "2509.12525", "pdf": "https://arxiv.org/pdf/2509.12525.pdf", "abs": "https://arxiv.org/abs/2509.12525", "title": "The Adaptation Paradox: Agency vs. Mimicry in Companion Chatbots", "authors": ["T. James Brandt", "Cecilia Xi Wang"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "31 pages, 17 figures, 2 tables. Submitted to CHI 2026 (under review).\n  Preregistered: https://osf.io/f4h5b ; Code/Materials:\n  https://doi.org/10.5281/zenodo.15801081", "summary": "Generative AI powers a growing wave of companion chatbots, yet principles for\nfostering genuine connection remain unsettled. We test two routes: visible user\nauthorship versus covert language-style mimicry. In a preregistered 3x2\nexperiment (N = 162), we manipulated user-controlled avatar generation (none,\npremade, user-generated) and Language Style Matching (LSM) (static vs.\nadaptive). Generating an avatar boosted rapport ($\\omega^2$ = .040, p = .013),\nwhereas adaptive LSM underperformed static style on personalization and\nsatisfaction (d = 0.35, p = .009) and was paradoxically judged less adaptive (t\n= 3.07, p = .003, d = 0.48). We term this an Adaptation Paradox: synchrony\nerodes connection when perceived as incoherent, destabilizing persona. To\nexplain, we propose a stability-and-legibility account: visible authorship\nfosters natural interaction, while covert mimicry risks incoherence. Our\nfindings suggest designers should prioritize legible, user-driven\npersonalization and limit stylistic shifts rather than rely on opaque mimicry.", "AI": {"tldr": "This study investigates the impact of user-generated avatars and language style matching on user rapport with chatbots, revealing an 'Adaptation Paradox' where adaptive language styles may hinder connection.", "motivation": "To explore how different aspects of chatbot design, such as avatar visibility and language style matching, influence user rapport and connection.", "method": "A preregistered 3x2 experiment with 162 participants manipulating avatar generation (none, premade, user-generated) and Language Style Matching (static vs. adaptive).", "result": "Generating a user-controlled avatar improved rapport, but adaptive language style matching was less effective for personalization and satisfaction and caused a perception of incoherence.", "conclusion": "Designers should emphasize visible user authorship in chatbot interactions to enhance user connection instead of relying on covert language mimicry.", "key_contributions": ["Identified the positive impact of user-generated avatars on chatbot rapport.", "Revealed the Adaptation Paradox where adaptive language styles may reduce perceived coherence.", "Provided a framework for prioritizing user-driven personalization in chatbot design."], "limitations": "Study limited to a laboratory setting with specific experimental conditions, potential lack of ecological validity.", "keywords": ["Generative AI", "Chatbots", "Language Style Matching", "User Rapport", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.13191", "pdf": "https://arxiv.org/pdf/2509.13191.pdf", "abs": "https://arxiv.org/abs/2509.13191", "title": "Textarium: Entangling Annotation, Abstraction and Argument", "authors": ["Philipp Proff", "Marian Dörk"], "categories": ["cs.HC", "cs.CL", "H.5.2; H.5.4; I.7.1; J.5"], "comment": "This is the authors' version of the article presented at VIS4DH and\n  published in the proceedings of IEEE VIS 2025", "summary": "We present a web-based environment that connects annotation, abstraction, and\nargumentation during the interpretation of text. As a visual interface for\nscholarly reading and writing, Textarium combines human analysis with\nlightweight computational processing to bridge close and distant reading\npractices. Readers can highlight text, group keywords into concepts, and embed\nthese observations as anchors in essays. The interface renders these\ninterpretive actions as parameterized visualization states. Through a\nspeculative design process of co-creative and iterative prototyping, we\ndeveloped a reading-writing approach that makes interpretive processes\ntransparent and shareable within digital narratives.", "AI": {"tldr": "Textarium is a web-based interface that integrates annotation, abstraction, and argumentation to enhance scholarly reading and writing by visualizing interpretive actions.", "motivation": "The motivation behind Textarium is to improve the process of interpreting text by facilitating both close and distant reading practices in a more visual and interactive manner.", "method": "Textarium employs a web-based environment that allows users to highlight text, group keywords, and embed observations as anchors in essays, manifesting these actions as parameterized visualizations.", "result": "The implementation of Textarium offers a transparent and shareable approach to the interpretive processes involved in reading and writing, enhancing scholarly communication.", "conclusion": "Textarium effectively bridges the gap between human analysis and computational processing, promoting a more interactive and collaborative approach to text interpretation.", "key_contributions": ["Development of an innovative web-based environment for scholarly text interaction.", "Integration of annotation, abstraction, and argumentation into a single interface.", "Provision of parameterized visualizations that make interpretive actions transparent."], "limitations": "", "keywords": ["Annotation", "Abstraction", "Argumentation", "Text interpretation", "Visualization"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2307.02103", "pdf": "https://arxiv.org/pdf/2307.02103.pdf", "abs": "https://arxiv.org/abs/2307.02103", "title": "Do predictability factors towards signing avatars hold across cultures?", "authors": ["Abdelhadi Soudi", "Manal El Hakkaoui", "Kristof Van Laerhoven"], "categories": ["cs.CL"], "comment": "updated version", "summary": "Avatar technology can offer accessibility possibilities and improve the\nDeaf-and-Hard of Hearing sign language users access to communication, education\nand services, such as the healthcare system. However, sign language users\nacceptance of signing avatars as well as their attitudes towards them vary and\ndepend on many factors. Furthermore, research on avatar technology is mostly\ndone by researchers who are not Deaf. The study examines the extent to which\nintrinsic or extrinsic factors contribute to predict the attitude towards\navatars across cultures. Intrinsic factors include the characteristics of the\navatar, such as appearance, movements and facial expressions. Extrinsic factors\ninclude users technology experience, their hearing status, age and their sign\nlanguage fluency. This work attempts to answer questions such as, if lower\nattitude ratings are related to poor technology experience with ASL users, for\nexample, is that also true for Moroccan Sign Language (MSL) users? For the\npurposes of the study, we designed a questionnaire to understand MSL users\nattitude towards avatars. Three groups of participants were surveyed: Deaf\n(57), Hearing (20) and Hard-of-Hearing (3). The results of our study were then\ncompared with those reported in other relevant studies.", "AI": {"tldr": "The study explores how intrinsic and extrinsic factors influence the attitudes of Deaf-and-Hard of Hearing sign language users towards avatar technology.", "motivation": "To improve communication and accessibility for Deaf-and-Hard of Hearing individuals through avatar technology, understanding user acceptance is crucial.", "method": "A questionnaire was designed and distributed to three participant groups (Deaf, Hearing, Hard-of-Hearing) to assess their attitudes towards avatars.", "result": "The study found that attitudes towards avatars vary significantly based on intrinsic factors (e.g., avatar characteristics) and extrinsic factors (e.g., technology experience).", "conclusion": "Inclusive research involving Deaf researchers is important for more accurate insights; attitudes towards avatars are influenced by both user experience and cultural differences.", "key_contributions": ["Identification of intrinsic and extrinsic factors affecting attitudes towards avatars in sign language users.", "Comparison of findings with other studies on similar topics across different cultures.", "Introduction of a questionnaire specifically designed for Moroccan Sign Language users."], "limitations": "The study may be limited by the small sample size and cultural specificity to Moroccan Sign Language.", "keywords": ["avatar technology", "sign language", "Deaf-and-Hard of Hearing", "attitudes", "accessibility"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2409.13745", "pdf": "https://arxiv.org/pdf/2409.13745.pdf", "abs": "https://arxiv.org/abs/2409.13745", "title": "Context-Aware Membership Inference Attacks against Pre-trained Large Language Models", "authors": ["Hongyan Chang", "Ali Shahin Shamsabadi", "Kleomenis Katevas", "Hamed Haddadi", "Reza Shokri"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "stat.ML"], "comment": null, "summary": "Membership Inference Attacks (MIAs) on pre-trained Large Language Models\n(LLMs) aim at determining if a data point was part of the model's training set.\nPrior MIAs that are built for classification models fail at LLMs, due to\nignoring the generative nature of LLMs across token sequences. In this paper,\nwe present a novel attack on pre-trained LLMs that adapts MIA statistical tests\nto the perplexity dynamics of subsequences within a data point. Our method\nsignificantly outperforms prior approaches, revealing context-dependent\nmemorization patterns in pre-trained LLMs.", "AI": {"tldr": "This paper presents a novel membership inference attack on pre-trained large language models by adapting statistical tests to the perplexity dynamics of subsequences.", "motivation": "To address the limitations of prior membership inference attacks that fail on Large Language Models due to their generative nature.", "method": "An adaptation of membership inference attack statistical tests focused on the perplexity dynamics of subsequences within a data point.", "result": "The proposed method significantly outperforms previous approaches and reveals context-dependent memorization patterns in pre-trained LLMs.", "conclusion": "The novel attack demonstrates the unique challenges and methodologies needed to assess membership inference in generative models like LLMs.", "key_contributions": ["Development of a new attack methodology for MIAs on LLMs", "Revealing context-dependent memorization in LLMs", "Significantly improved performance over previous MIA methods"], "limitations": "", "keywords": ["Membership Inference Attacks", "Large Language Models", "Perplexity Dynamics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.08388", "pdf": "https://arxiv.org/pdf/2410.08388.pdf", "abs": "https://arxiv.org/abs/2410.08388", "title": "Responsible AI in NLP: GUS-Net Span-Level Bias Detection Dataset and Benchmark for Generalizations, Unfairness, and Stereotypes", "authors": ["Maximus Powers", "Shaina Raza", "Alex Chang", "Rehana Riaz", "Umang Mavani", "Harshitha Reddy Jonala", "Ansh Tiwari", "Hua Wei"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Representational harms in language technologies often occur in short spans\nwithin otherwise neutral text, where phrases may simultaneously convey\ngeneralizations, unfairness, or stereotypes. Framing bias detection as\nsentence-level classification obscures which words carry bias and what type is\npresent, limiting both auditability and targeted mitigation. We introduce the\nGUS-Net Framework, comprising the GUS dataset and a multi-label token-level\ndetector for span-level analysis of social bias. The GUS dataset contains 3,739\nunique snippets across multiple domains, with over 69,000 token-level\nannotations. Each token is labeled using BIO tags (Begin, Inside, Outside) for\nthree pathways of representational harm: Generalizations, Unfairness, and\nStereotypes. To ensure reliable data annotation, we employ an automated\nmulti-agent pipeline that proposes candidate spans which are subsequently\nverified and corrected by human experts. We formulate bias detection as\nmulti-label token-level classification and benchmark both encoder-based models\n(e.g., BERT family variants) and decoder-based large language models (LLMs).\nOur evaluations cover token-level identification and span-level entity\nrecognition on our test set, and out-of-distribution generalization. Empirical\nresults show that encoder-based models consistently outperform decoder-based\nbaselines on nuanced and overlapping spans while being more computationally\nefficient. The framework delivers interpretable, fine-grained diagnostics that\nenable systematic auditing and mitigation of representational harms in\nreal-world NLP systems.", "AI": {"tldr": "The GUS-Net Framework facilitates token-level analysis of social bias in language technologies, providing a dataset and methods to detect representational harms.", "motivation": "To address representational harms which occur in language technologies, particularly how biases in language can affect understanding without being easily identifiable.", "method": "Developed the GUS dataset with over 69,000 annotations and a multi-label token-level detector for analyzing phrases that convey biases such as generalizations, unfairness, and stereotypes, using BIO tagging.", "result": "Empirical evaluations indicate that encoder-based models outperform decoder-based LLMs in identifying nuanced biases while being more efficient.", "conclusion": "The GUS-Net Framework offers a systematic approach for the auditing and mitigation of representational harms in NLP systems, enabling fine-grained analysis and improvement.", "key_contributions": ["Introduced the GUS dataset for bias detection in NLP.", "Developed a multi-label token-level detector for span-level analysis of biases.", "Demonstrated that encoder-based models can outperform LLMs in detecting biases efficiently."], "limitations": "", "keywords": ["social bias", "NLP", "GUS dataset", "token-level analysis", "encoder-based models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.08045", "pdf": "https://arxiv.org/pdf/2502.08045.pdf", "abs": "https://arxiv.org/abs/2502.08045", "title": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs", "authors": ["Mohsinul Kabir", "Ajwad Abrar", "Sophia Ananiadou"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted at EMNLP 2025 (Main)", "summary": "A large number of studies rely on closed-style multiple-choice surveys to\nevaluate cultural alignment in Large Language Models (LLMs). In this work, we\nchallenge this constrained evaluation paradigm and explore more realistic,\nunconstrained approaches. Using the World Values Survey (WVS) and Hofstede\nCultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger\ncultural alignment in less constrained settings, where responses are not\nforced. Additionally, we show that even minor changes, such as reordering\nsurvey choices, lead to inconsistent outputs, exposing the limitations of\nclosed-style evaluations. Our findings advocate for more robust and flexible\nevaluation frameworks that focus on specific cultural proxies, encouraging more\nnuanced and accurate assessments of cultural alignment in LLMs.", "AI": {"tldr": "This paper critiques the use of closed-style multiple-choice surveys for evaluating cultural alignment in LLMs, advocating for more flexible evaluation methods that yield more nuanced results.", "motivation": "To address the limitations of closed-style multiple-choice surveys in evaluating cultural alignment in Large Language Models (LLMs).", "method": "The study utilizes case studies from the World Values Survey (WVS) and Hofstede Cultural Dimensions to explore the effects of evaluation constraints on LLM responses.", "result": "Results indicate that LLMs demonstrate stronger cultural alignment in unconstrained settings, and minor changes in survey structure can lead to inconsistent outputs.", "conclusion": "The study calls for the adoption of more robust and flexible evaluation frameworks for assessing cultural alignment in LLMs, emphasizing the importance of nuanced assessments.", "key_contributions": ["Critique of closed evaluation methods for LLMs", "Empirical findings on cultural alignment variability based on survey constraints", "Proposal for more flexible evaluation frameworks"], "limitations": "The study primarily focuses on specific cultural dimensions and may not generalize across all evaluation contexts.", "keywords": ["Cultural Alignment", "Large Language Models", "Evaluation Frameworks"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.12769", "pdf": "https://arxiv.org/pdf/2502.12769.pdf", "abs": "https://arxiv.org/abs/2502.12769", "title": "How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild", "authors": ["Saad Obaid ul Islam", "Anne Lauscher", "Goran Glavaš"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models.", "AI": {"tldr": "This paper quantifies LLM hallucination in multilingual knowledge-intensive question answering, showing similar hallucination rates across languages using both noisy and gold datasets.", "motivation": "To address the global utility risk posed by LLM hallucinations, especially in multilingual contexts beyond English-centric studies.", "method": "A multilingual hallucination detection model is trained, utilizing both machine translation-generated noisy data and manually annotated gold data for five high-resource languages, followed by a study across 30 languages and 6 LLM families.", "result": "The study reveals that longer responses with more hallucinated tokens occur in higher-resource languages, but no correlation exists between length-normalized hallucination rates and digital representation of languages. Smaller LLMs demonstrate higher hallucination rates than larger ones.", "conclusion": "The findings validate the use of noisy data for estimating hallucination rates and suggest that the problem of hallucination persists across various languages, warranting further investigation.", "key_contributions": ["Quantitative analysis of LLM hallucination rates in 30 languages.", "Development of a multilingual hallucination detection model.", "Validation of noisy data for hallucination rate estimation across languages."], "limitations": "The reliance on machine translation for noisy data may introduce additional biases.", "keywords": ["Large Language Models", "hallucination", "multilingual", "question answering", "machine translation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.02783", "pdf": "https://arxiv.org/pdf/2503.02783.pdf", "abs": "https://arxiv.org/abs/2503.02783", "title": "Teaching Your Models to Understand Code via Focal Preference Alignment", "authors": ["Jie Wu", "Haoling Li", "Xin Zhang", "Jianwen Luo", "Yangyu Huang", "Ruihang Chu", "Yujiu Yang", "Scarlett Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP'25", "summary": "Preference learning extends the performance of Code LLMs beyond traditional\nsupervised fine-tuning by leveraging relative quality comparisons. In existing\napproaches, a set of n candidate solutions is evaluated based on test case\nsuccess rates, with the candidate demonstrating a higher pass rate being\nlabeled as positive and its counterpart with a lower pass rate as negative.\nHowever, because this approach aligns entire failing code blocks rather than\npinpointing specific errors, it lacks the granularity necessary to capture\nmeaningful error-correction relationships. As a result, the model is unable to\nlearn more informative error-correction patterns. To address these issues, we\npropose Target-DPO, a new preference alignment framework that mimics human\niterative debugging to refine Code LLMs. Target-DPO explicitly locates error\nregions and aligns the corresponding tokens via a tailored DPO algorithm. To\nfacilitate it, we introduce the CodeFlow dataset, where samples are iteratively\nrefined until passing tests, with modifications capturing error corrections.\nExtensive experiments show that a diverse suite of Code LLMs equipped with\nTarget-DPO achieves significant performance gains in code generation and\nimproves on challenging tasks like BigCodeBench. In-depth analysis reveals that\nTarget-DPO yields fewer errors. Code, model and datasets are in:\nhttps://github.com/JieWu02/Target-DPO.", "AI": {"tldr": "This paper introduces Target-DPO, a new preference learning framework that enhances Code LLMs by refining the error-correction process through human-like iterative debugging and a specialized DPO algorithm.", "motivation": "The need for improved error-correction capabilities in Code LLMs, which currently rely on coarse evaluations that fail to capture specific error relationships.", "method": "Target-DPO introduces a tailored preference alignment framework that identifies error regions and aligns tokens, supported by the creation of the CodeFlow dataset for iterative sample refinement.", "result": "Target-DPO significantly improves the performance of various Code LLMs, especially in code generation, and reduces the error rates in challenging tasks.", "conclusion": "The proposed method demonstrates substantial performance gains and offers a more nuanced understanding of error correction in code generation.", "key_contributions": ["Introduction of Target-DPO preference alignment framework", "Creation of CodeFlow dataset for iterative debugging", "Demonstration of improved performance in Code LLMs and reduced errors"], "limitations": "", "keywords": ["Code LLMs", "Preference learning", "Error correction", "Iterative debugging", "DPO algorithm"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.13021", "pdf": "https://arxiv.org/pdf/2503.13021.pdf", "abs": "https://arxiv.org/abs/2503.13021", "title": "Dynamic Relation Inference via Verb Embeddings", "authors": ["Omri Suissa", "Muhiim Ali", "Ariana Azarbal", "Hui Shen", "Shekhar Pradhan"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "CLIP has demonstrated exceptional image-text matching capabilities due to its\ntraining on contrastive learning tasks. Past research has suggested that\nwhereas CLIP effectively matches text to images when the matching can be\nachieved just by matching the text with the objects in the image, CLIP\nstruggles when the matching depends on representing the relationship among the\nobjects in the images (i.e., inferring relations). Previous attempts to address\nthis limitation by training CLIP on relation detection datasets with only\nlinguistic supervision have met with limited success. In this paper, we offer\ninsights and practical methods to advance the field of relation inference from\nimages. This paper approaches the task of creating a model that effectively\ndetects relations among the objects in images by producing text and image\nembeddings that capture relationships through linguistic supervision. To this\nend, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which\naugments the COCO dataset, fine-tunes CLIP with hard negatives\nsubject-relation-object triples and corresponding images, and introduces a\nnovel loss function to improve relation detection. Evaluated on multiple\nCLIP-based models, our method significantly improves zero-shot relation\ninference accuracy in both frozen and fine-tuned settings, significantly\noutperforming CLIP and state-of-the-art models while generalizing well on\nunseen data.", "AI": {"tldr": "The paper introduces DRIVE, a novel method to enhance CLIP's ability to infer relationships among objects in images through improved text and image embeddings, achieving significant improvements in zero-shot relation inference accuracy.", "motivation": "Existing methods struggle with CLIP's ability to infer relationships among objects in images, necessitating improved techniques for relation detection.", "method": "The paper presents DRIVE, which fine-tunes CLIP using hard negatives from augmented COCO dataset, optimizing a novel loss function for better relationship detection.", "result": "DRIVE significantly enhances zero-shot relation inference accuracy compared to CLIP and other state-of-the-art models, performing well in both frozen and fine-tuned settings.", "conclusion": "The proposed methods demonstrate a notable improvement in relation inference tasks, paving the way for better integration of relational understanding in image-text matching systems.", "key_contributions": ["Introduction of Dynamic Relation Inference via Verb Embeddings (DRIVE)", "Fine-tuning CLIP using hard negatives and a novel loss function", "Significant performance improvements in zero-shot relation inference."], "limitations": "", "keywords": ["CLIP", "image-text matching", "relation detection", "Dynamic Relation Inference", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.01132", "pdf": "https://arxiv.org/pdf/2504.01132.pdf", "abs": "https://arxiv.org/abs/2504.01132", "title": "Is the Top Still Spinning? Evaluating Subjectivity in Narrative Understanding", "authors": ["Melanie Subbiah", "Akankshya Mishra", "Grace Kim", "Liyan Tang", "Greg Durrett", "Kathleen McKeown"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Determining faithfulness of a claim to a source document is an important\nproblem across many domains. This task is generally treated as a binary\njudgment of whether the claim is supported or unsupported in relation to the\nsource. In many cases, though, whether a claim is supported can be ambiguous.\nFor instance, it may depend on making inferences from given evidence, and\ndifferent people can reasonably interpret the claim as either supported or\nunsupported based on their agreement with those inferences. Forcing binary\nlabels upon such claims lowers the reliability of evaluation. In this work, we\nreframe the task to manage the subjectivity involved with factuality judgments\nof ambiguous claims. We introduce LLM-generated edits of summaries as a method\nof providing a nuanced evaluation of claims: how much does a summary need to be\nedited to be unambiguous? Whether a claim gets rewritten and how much it\nchanges can be used as an automatic evaluation metric, the Ambiguity Rewrite\nMetric (ARM), with a much richer feedback signal than a binary judgment of\nfaithfulness. We focus on the area of narrative summarization as it is\nparticularly rife with ambiguity and subjective interpretation. We show that\nARM produces a 21% absolute improvement in annotator agreement on claim\nfaithfulness, indicating that subjectivity is reduced.", "AI": {"tldr": "This paper addresses the ambiguity in evaluating the faithfulness of claims to source documents by introducing the Ambiguity Rewrite Metric (ARM), which assesses the degree of necessary edits for claim clarity instead of binary labels.", "motivation": "The work aims to improve the evaluation of claim faithfulness, acknowledging that binary classifications can overlook the complexity and subjectivity involved in interpreting ambiguous claims.", "method": "The authors propose using LLM-generated edits to evaluate how much a summary needs to be altered for clarity, thus replacing binary judgments with a more nuanced metric.", "result": "The Ambiguity Rewrite Metric (ARM) indicates a 21% absolute improvement in annotator agreement regarding claim faithfulness, demonstrating reduced subjectivity in evaluations.", "conclusion": "By adopting ARM, the evaluation of ambiguous claims becomes more effective, leading to greater clarity and consistency in faithfulness judgments.", "key_contributions": ["Introduction of the Ambiguity Rewrite Metric (ARM) for claim evaluation", "Demonstration of significant improvement in annotator agreement on claim faithfulness", "Focus on narrative summarization areas to highlight ambiguity issues"], "limitations": "", "keywords": ["claim faithfulness", "ambiguity", "narrative summarization", "LLM-generated edits", "evaluation metric"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.05262", "pdf": "https://arxiv.org/pdf/2504.05262.pdf", "abs": "https://arxiv.org/abs/2504.05262", "title": "Do Large Language Models Truly Grasp Addition? A Rule-Focused Diagnostic Using Two-Integer Arithmetic", "authors": ["Yang Yan", "Yu Lu", "Renjun Xu", "Zhenzhong Lan"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP'25 Main", "summary": "Large language models (LLMs) achieve impressive results on advanced\nmathematics benchmarks but sometimes fail on basic arithmetic tasks, raising\nthe question of whether they have truly grasped fundamental arithmetic rules or\nare merely relying on pattern matching. To unravel this issue, we\nsystematically probe LLMs' understanding of two-integer addition (0 to $2^64$)\nby testing three crucial properties: commutativity (A+B=B+A), representation\ninvariance via symbolic remapping (e.g., $7 -> Y$), and consistent accuracy\nscaling with operand length. Our evaluation of 12 leading LLMs reveals a stark\ndisconnect: while models achieve high numeric accuracy (73.8-99.8%), they\nsystematically fail these diagnostics. Specifically, accuracy plummets to <=\n7.5% with symbolic inputs, commutativity is violated in up to 20% of cases, and\naccuracy scaling is non-monotonic. These findings demonstrate that current LLMs\naddress elementary addition via pattern matching, not robust rule induction,\nmotivating new diagnostic benchmarks and innovations in model architecture and\ntraining to cultivate genuine mathematical reasoning. Our dataset and\ngenerating code are available at\nhttps://github.com/kuri-leo/llm-arithmetic-diagnostic.", "AI": {"tldr": "This paper examines large language models' understanding of basic arithmetic, demonstrating that they primarily rely on pattern matching rather than true rule induction.", "motivation": "To investigate whether LLMs genuinely understand fundamental arithmetic rules or simply depend on pattern matching, by testing their performance on basic addition.", "method": "The study probes LLMs' comprehension through tests of commutativity, representation invariance, and accuracy scaling across 12 leading models.", "result": "Models achieved high average numeric accuracy (73.8-99.8%) but consistently failed three key diagnostics, such as accuracy dropping to <= 7.5% with symbolic inputs.", "conclusion": "Current LLMs perform elementary arithmetic via pattern matching rather than actual mathematical reasoning, necessitating new benchmarks and improvements in model training.", "key_contributions": ["Developed novel diagnostics for LLM arithmetic understanding", "Demonstrated disconnect between high accuracy and true rule understanding", "Provided publicly available dataset and code for further research"], "limitations": "Study focused solely on two-integer addition, not addressing other arithmetic operations or complex mathematical reasoning.", "keywords": ["large language models", "arithmetic", "pattern matching", "commutativity", "representation invariance"], "importance_score": 8, "read_time_minutes": 10}}
