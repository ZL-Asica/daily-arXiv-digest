{"id": "2507.11599", "pdf": "https://arxiv.org/pdf/2507.11599.pdf", "abs": "https://arxiv.org/abs/2507.11599", "title": "Neuroaesthetics and the Science of Visual Experience", "authors": ["Harish Vijayakumar"], "categories": ["cs.HC"], "comment": "7 pages", "summary": "Neuroaesthetics is an interdisciplinary field that brings together\nneuroscience, psychology, and the arts to explore how the human brain perceives\nand responds to visual beauty. This paper examines the neural mechanisms behind\naesthetic experiences, aiming to explain why certain designs or artworks feel\nemotionally or cognitively \"right.\" By analyzing the interaction between\nperception, emotion, and cognition, neuroaesthetics reveals how beauty is\nconstructed in the brain and how this understanding can inform fields such as\ngraphic and interface design. This paper offers a clear and accessible overview\nof core neuroaesthetic principles, making the subject approachable to a wide\naudience. The findings suggest that impactful design is more than surface-level\nappeal: well-crafted visual experiences can engage, support, and connect people\nin meaningful ways.", "AI": {"tldr": "This paper explores the neural mechanisms of aesthetic experiences in the field of neuroaesthetics, examining how beauty is processed in the brain and its implications for design.", "motivation": "To understand the interaction between perception, emotion, and cognition in generating aesthetic experiences, and to inform design practices.", "method": "The paper analyzes neural mechanisms underlying aesthetic experiences through interdisciplinary insights from neuroscience, psychology, and arts.", "result": "It reveals that impactful design goes beyond aesthetics; it significantly affects engagement and connection, enhancing user experience.", "conclusion": "Understanding neuroaesthetics can improve graphic and interface design by emphasizing meaningful visual experiences.", "key_contributions": ["Analysis of the neural mechanisms behind aesthetic experiences", "Interdisciplinary approach combining neuroscience, psychology, and art", "Insights for improving graphic and interface design based on aesthetic principles"], "limitations": "", "keywords": ["neuroaesthetics", "aesthetic experiences", "graphic design", "interface design", "neuroscience"], "importance_score": 4, "read_time_minutes": 7}}
{"id": "2507.11628", "pdf": "https://arxiv.org/pdf/2507.11628.pdf", "abs": "https://arxiv.org/abs/2507.11628", "title": "DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday Storytelling", "authors": ["Jiangnan Xu", "Haeseul Cha", "Gosu Choi", "Gyu-cheol Lee", "Yeo-Jin Yoon", "Zucheul Lee", "Konstantinos Papangelis", "Dae Hyun Kim", "Juho Kim"], "categories": ["cs.HC"], "comment": null, "summary": "An interactive vignette is a popular and immersive visual storytelling\napproach that invites viewers to role-play a character and influences the\nnarrative in an interactive environment. However, it has not been widely used\nby everyday storytellers yet due to authoring complexity, which conflicts with\nthe immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted\nauthoring system for interactive vignette creation in everyday storytelling. It\ntakes a natural language story as input and extracts the three core elements of\nan interactive vignette (environment, characters, and events), enabling authors\nto focus on refining these elements instead of constructing them from scratch.\nThen, it automatically transforms the single-branch story input into a\nbranch-and-bottleneck structure using an LLM-powered narrative planner, which\nenables flexible viewer interactions while freeing the author from\nmulti-branching. A technical evaluation (N=16) shows that DiaryPlay-generated\ncharacter activities are on par with human-authored ones regarding\nbelievability. A user study (N=16) shows that DiaryPlay effectively supports\nauthors in creating interactive vignette elements, maintains authorial intent\nwhile reacting to viewer interactions, and provides engaging viewing\nexperiences.", "AI": {"tldr": "DiaryPlay is an AI-assisted system that simplifies the creation of interactive vignettes for everyday storytelling by extracting core elements from natural language stories.", "motivation": "Despite the potential of interactive vignettes to enhance storytelling, their complexity hinders everyday authors from using them. DiaryPlay aims to streamline the authoring process.", "method": "DiaryPlay takes a natural language story and extracts the core elements (environment, characters, events) necessary for crafting an interactive vignette. It then uses an LLM-based narrative planner to convert a single-branch story into a branch-and-bottleneck narrative structure.", "result": "Technical evaluations demonstrate that the character activities generated by DiaryPlay are perceived as believable as those authored by humans. User studies indicate that the system effectively assists authors and provides engaging experiences.", "conclusion": "DiaryPlay enables authors to focus on refining their interactive vignettes while ensuring quality viewer interaction and maintaining authorial intent.", "key_contributions": ["Introduction of an AI-assisted authoring system for interactive vignettes.", "Use of LLM for narrative planning to facilitate branching story development.", "Empirical evaluations of character believability and author support."], "limitations": "Limited to a small number of participants in evaluations.", "keywords": ["Interactive storytelling", "AI authoring", "Narrative planning", "Human-computer interaction", "Everyday storytelling"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.11677", "pdf": "https://arxiv.org/pdf/2507.11677.pdf", "abs": "https://arxiv.org/abs/2507.11677", "title": "CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations", "authors": ["Mashrur Rashik", "Jean-Daniel Fekete", "Narges Mahyar"], "categories": ["cs.HC"], "comment": "To appear in the IEEE Visualization and Visual Analytics (VIS)\n  Conference, Short Paper, 2025", "summary": "Communicating climate change remains challenging, as climate reports, though\nrich in data and visualizations, often feel too abstract or technical for the\npublic. Although personalization can enhance communication, most tools still\nlack the narrative and visualization tailoring needed to connect with\nindividual experiences. We present CLAImate, an AI-enabled prototype that\npersonalizes conversation narratives and localizes visualizations based on\nusers' climate knowledge and geographic location. We evaluated CLAImate through\ninternal verification of factual correctness, a formative study with experts,\nand a pilot with UK residents. CLAImate achieved 66% SNLI accuracy and 70%\nFACTSCORE. Visualization experts appreciated its clarity and personalization,\nand seven out of ten UK participants reported better understanding and local\nrelevance of climate risks with CLAImate. We also discuss design challenges in\npersonalization, accuracy, and scalability, and outline future directions for\nintegrating visualizations in personalized conversational interfaces.", "AI": {"tldr": "CLAImate is an AI-enabled prototype that personalizes climate conversation narratives and localizes visualizations based on users' knowledge and location.", "motivation": "Communicating climate change effectively remains difficult due to the abstract nature of climate reports, necessitating a more personalized approach to engage the public.", "method": "CLAImate personalizes narratives and visualizes climate information tailored to individual users based on their knowledge and geographic context. Its effectiveness was evaluated through factual correctness checks, expert studies, and user pilot tests in the UK.", "result": "CLAImate achieved a 66% accuracy in Natural Language Inference (SNLI) and a 70% FACTSCORE. Users reported improved understanding of climate risks, and experts praised the clarity of visualizations.", "conclusion": "While CLAImate demonstrates promise in personalizing climate communication, challenges in personalization, accuracy, and scalability remain, indicating areas for future research.", "key_contributions": ["Develops personalized narratives for climate communication.", "Integrates local visualizations based on user geography.", "Evaluates effectiveness through user studies and expert feedback."], "limitations": "Challenges in scalability and accuracy of personalization techniques.", "keywords": ["climate change", "personalization", "AI", "visualization", "user experience"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.11797", "pdf": "https://arxiv.org/pdf/2507.11797.pdf", "abs": "https://arxiv.org/abs/2507.11797", "title": "GIST: Group Interaction Sensing Toolkit for Mixed Reality", "authors": ["Diana Romero", "Yasra Chandio", "Fatima Anwar", "Salma Elmalaki"], "categories": ["cs.HC", "cs.ET"], "comment": "11 pages, 6 figures", "summary": "Understanding how teams coordinate, share work, and negotiate roles in\nimmersive environments is critical for designing effective mixed-reality (MR)\napplications that support real-time collaboration. However, existing methods\neither rely on external cameras and offline annotation or focus narrowly on\nsingle modalities, limiting their validity and applicability. To address this,\nwe present a novel group interaction sensing toolkit (GIST), a deployable\nsystem that passively captures multi-modal interaction data, such as speech,\ngaze, and spatial proximity from commodity MR headset's sensors and\nautomatically derives both overall static interaction networks and dynamic\nmoment-by-moment behavior patterns. We evaluate GIST with a human subject study\nwith 48 participants across 12 four-person groups performing an open-ended\nimage-sorting task in MR. Our analysis shows strong alignment between the\nidentified behavior modes and shifts in interaction network structure,\nconfirming that momentary changes in speech, gaze, and proximity data are\nobservable through the sensor data.", "AI": {"tldr": "This paper presents GIST, a toolkit for capturing multi-modal interaction data in mixed-reality environments to enhance team collaboration insights.", "motivation": "To improve the understanding of team dynamics and interactions in mixed-reality applications and move beyond limitations of existing methods.", "method": "GIST captures data from MR headset sensors, allowing for the analysis of speech, gaze, and proximity to derive interaction networks and behavior patterns.", "result": "The study with 48 participants demonstrated strong correlations between behavior modes and shifts in interaction network structure.", "conclusion": "GIST effectively captures and analyzes multi-modal interaction data, providing insights into team dynamics in MR environments.", "key_contributions": ["Development of a novel toolkit (GIST) for multi-modal data capture in MR.", "Evaluation through human subjects study confirming the validity of the approach.", "Insight into the dynamics of team interactions during collaborative tasks."], "limitations": "", "keywords": ["mixed-reality", "team collaboration", "multi-modal interaction", "GIST", "behavior patterns"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2507.11582", "pdf": "https://arxiv.org/pdf/2507.11582.pdf", "abs": "https://arxiv.org/abs/2507.11582", "title": "Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance", "authors": ["Kazuyoshi Otsuka"], "categories": ["cs.CL"], "comment": "38 pages. Manuscript submitted for review to the Journal of\n  Computational Literary Studies (JCLS)", "summary": "This study positions large language models (LLMs) as \"subjective literary\ncritics\" to explore aesthetic preferences and evaluation patterns in literary\nassessment. Ten Japanese science fiction short stories were translated into\nEnglish and evaluated by six state-of-the-art LLMs across seven independent\nsessions. Principal component analysis and clustering techniques revealed\nsignificant variations in evaluation consistency ({\\alpha} ranging from 1.00 to\n0.35) and five distinct evaluation patterns. Additionally, evaluation variance\nacross stories differed by up to 4.5-fold, with TF-IDF analysis confirming\ndistinctive evaluation vocabularies for each model. Our seven-session\nwithin-day protocol using an original Science Fiction corpus strategically\nminimizes external biases, allowing us to observe implicit value systems shaped\nby RLHF and their influence on literary judgment. These findings suggest that\nLLMs may possess individual evaluation characteristics similar to human\ncritical schools, rather than functioning as neutral benchmarkers.", "AI": {"tldr": "The study examines large language models (LLMs) as literary critics by evaluating Japanese science fiction stories, revealing distinct evaluation patterns and individual characteristics influenced by RLHF.", "motivation": "Investigating how LLMs assess literature and their aesthetic preferences may provide insights into LLM behavior and biases, contributing to the understanding of AI's role in cultural contexts.", "method": "Ten Japanese science fiction short stories were evaluated by six LLMs across seven sessions, utilizing principal component analysis and clustering techniques to analyze evaluation consistency and patterns.", "result": "The analysis found five distinct evaluation patterns and significant variance in evaluation consistency among the models, as well as unique evaluation vocabularies for each LLM.", "conclusion": "LLMs exhibit individual characteristics in literary assessment akin to human critics, challenging the notion that they are purely neutral evaluators.", "key_contributions": ["Positions LLMs as subjective literary critics", "Identifies distinct evaluation patterns in literary assessment", "Demonstrates influence of RLHF on LLM evaluation characteristics"], "limitations": "Results are based on a limited set of science fiction stories, which may not generalize to other genres.", "keywords": ["large language models", "literary assessment", "evaluation patterns", "RLHF", "aesthetic preferences"], "importance_score": 8, "read_time_minutes": 38}}
{"id": "2507.11841", "pdf": "https://arxiv.org/pdf/2507.11841.pdf", "abs": "https://arxiv.org/abs/2507.11841", "title": "\"Mapping What I Feel\": Understanding Affective Geovisualization Design Through the Lens of People-Place Relationships", "authors": ["Xingyu Lan", "Yutong Yang", "Yifan Wang"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Affective visualization design is an emerging research direction focused on\ncommunicating and influencing emotion through visualization. However, as\nrevealed by previous research, this area is highly interdisciplinary and\ninvolves theories and practices from diverse fields and disciplines, thus\nawaiting analysis from more fine-grained angles. To address this need, this\nwork focuses on a pioneering and relatively mature sub-area, affective\ngeovisualization design, to further the research in this direction and provide\nmore domain-specific insights. Through an analysis of a curated corpus of\naffective geovisualization designs using the Person-Process-Place (PPP) model\nfrom geographic theory, we derived a design taxonomy that characterizes a\nvariety of methods for eliciting and enhancing emotions through geographic\nvisualization. We also identified four underlying high-level design paradigms\nof affective geovisualization design (e.g., computational, anthropomorphic)\nthat guide distinct approaches to linking geographic information with human\nexperience. By extending existing affective visualization design frameworks\nwith geographic specificity, we provide additional design examples,\ndomain-specific analyses, and insights to guide future research and practices\nin this underexplored yet highly innovative domain.", "AI": {"tldr": "This paper explores affective geovisualization design, focusing on how geographic visualizations can influence emotions, providing a design taxonomy and identifying high-level design paradigms.", "motivation": "To analyze the underexplored area of affective visualization design, specifically within affective geovisualization, and to provide domain-specific insights and frameworks.", "method": "The study employs the Person-Process-Place (PPP) model from geographic theory to analyze a curated corpus of affective geovisualization designs, leading to the development of a design taxonomy and identification of design paradigms.", "result": "The analysis resulted in a design taxonomy that characterizes various methods for eliciting emotions through geographic visualization and identified four high-level design paradigms.", "conclusion": "The paper extends existing frameworks to include geographic specificity, offering examples and guiding future research in affective geovisualization.", "key_contributions": ["Development of a design taxonomy for affective geovisualization", "Identification of four high-level design paradigms", "Provision of domain-specific insights for future research"], "limitations": "", "keywords": ["affective visualization", "geovisualization", "emotion", "design taxonomy", "human experience"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.11625", "pdf": "https://arxiv.org/pdf/2507.11625.pdf", "abs": "https://arxiv.org/abs/2507.11625", "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering", "authors": ["Varun Srivastava", "Fan Lei", "Srija Mukhopadhyay", "Vivek Gupta", "Ross Maciejewski"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Published as a conference paper at COLM 2025", "summary": "Recent advancements in multimodal large language models (MLLMs) have driven\nresearchers to explore how well these models read data visualizations, e.g.,\nbar charts, scatter plots. More recently, attention has shifted to visual\nquestion answering with maps (Map-VQA). However, Map-VQA research has primarily\nfocused on choropleth maps, which cover only a limited range of thematic\ncategories and visual analytical tasks. To address these gaps, we introduce\nMapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three\nmap types: choropleth maps, cartograms, and proportional symbol maps spanning\ntopics from six distinct themes (e.g., housing, crime). We evaluate multiple\nMLLMs using six visual analytical tasks, comparing their performance against\none another and a human baseline. An additional experiment examining the impact\nof map design changes (e.g., altered color schemes, modified legend designs,\nand removal of map elements) provides insights into the robustness and\nsensitivity of MLLMs, their reliance on internal geographic knowledge, and\npotential avenues for improving Map-VQA performance.", "AI": {"tldr": "This paper introduces MapIQ, a benchmark dataset for visual question answering with maps, focusing on evaluating multimodal large language models (MLLMs) on diverse map types and visual analytical tasks.", "motivation": "To address the limitations in current Map-VQA research, which has largely focused on choropleth maps and a narrow range of thematic categories and visual analytical tasks.", "method": "The study introduces MapIQ, comprising 14,706 question-answer pairs across choropleth maps, cartograms, and proportional symbol maps, and evaluates multiple MLLMs on six visual analytical tasks against human performance.", "result": "MLLMs were evaluated on their performance across three map types and six tasks, with findings indicating varying robustness and sensitivity to map design changes and reliance on geographic knowledge.", "conclusion": "Insights from these evaluations can guide future improvements in Map-VQA and enhance MLLM performance on diverse visual data.", "key_contributions": ["Introduction of MapIQ, a comprehensive benchmark dataset for Map-VQA", "Evaluation of MLLM performance across multiple map types and tasks", "Insights into the impacts of map design on MLLM robustness"], "limitations": "", "keywords": ["multimodal large language models", "visual question answering", "map visualization"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.11848", "pdf": "https://arxiv.org/pdf/2507.11848.pdf", "abs": "https://arxiv.org/abs/2507.11848", "title": "Interactive Hybrid Rice Breeding with Parametric Dual Projection", "authors": ["Changjian Chen", "Pengcheng Wang", "Fei Lyu", "Zhuo Tang", "Li Yang", "Long Wang", "Yong Cai", "Feng Yu", "Kenli Li"], "categories": ["cs.HC", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Hybrid rice breeding crossbreeds different rice lines and cultivates the\nresulting hybrids in fields to select those with desirable agronomic traits,\nsuch as higher yields. Recently, genomic selection has emerged as an efficient\nway for hybrid rice breeding. It predicts the traits of hybrids based on their\ngenes, which helps exclude many undesired hybrids, largely reducing the\nworkload of field cultivation. However, due to the limited accuracy of genomic\nprediction models, breeders still need to combine their experience with the\nmodels to identify regulatory genes that control traits and select hybrids,\nwhich remains a time-consuming process. To ease this process, in this paper, we\nproposed a visual analysis method to facilitate interactive hybrid rice\nbreeding. Regulatory gene identification and hybrid selection naturally\nensemble a dual-analysis task. Therefore, we developed a parametric dual\nprojection method with theoretical guarantees to facilitate interactive dual\nanalysis. Based on this dual projection method, we further developed a gene\nvisualization and a hybrid visualization to verify the identified regulatory\ngenes and hybrids. The effectiveness of our method is demonstrated through the\nquantitative evaluation of the parametric dual projection method, identified\nregulatory genes and desired hybrids in the case study, and positive feedback\nfrom breeders.", "AI": {"tldr": "This paper presents a visual analysis method for improving hybrid rice breeding through regulatory gene identification and hybrid selection.", "motivation": "The motivation behind this research is to enhance the accuracy and efficiency of hybrid rice breeding by integrating genomic prediction models with breeders' experience.", "method": "A parametric dual projection method was developed for interactive dual analysis tasks, incorporating gene visualization and hybrid visualization techniques.", "result": "The proposed method significantly aided in the identification of regulatory genes and desired hybrids, as evidenced by quantitative evaluations and positive user feedback.", "conclusion": "The study concludes that the visual analysis method can streamline the hybrid rice breeding process and improve selection outcomes for breeders.", "key_contributions": ["Development of a parametric dual projection method for hybrid rice breeding", "Creation of visualization tools for regulatory gene and hybrid identification", "Demonstration of method effectiveness through case studies and feedback from breeders"], "limitations": "", "keywords": ["Hybrid Rice Breeding", "Genomic Selection", "Visual Analysis", "Regulatory Genes", "Hybrid Selection"], "importance_score": 0, "read_time_minutes": 5}}
{"id": "2507.11634", "pdf": "https://arxiv.org/pdf/2507.11634.pdf", "abs": "https://arxiv.org/abs/2507.11634", "title": "Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation", "authors": ["Farideh Majidi", "Ziaeddin Beheshtifard"], "categories": ["cs.CL", "cs.AI"], "comment": "Proceedings of the First National Conference on Artificial\n  Intelligence and Emerging Research: Convergence of Humans and Intelligent\n  Systems", "summary": "This research examines cross-lingual sentiment analysis using few-shot\nlearning and incremental learning methods in Persian. The main objective is to\ndevelop a model capable of performing sentiment analysis in Persian using\nlimited data, while getting prior knowledge from high-resource languages. To\nachieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and\nDistilBERT) were employed, which were fine-tuned using few-shot and incremental\nlearning approaches on small samples of Persian data from diverse sources,\nincluding X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled\nthe models to learn from a broad range of contexts. Experimental results show\nthat the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%\naccuracy on Persian sentiment analysis. These findings highlight the\neffectiveness of combining few-shot learning and incremental learning with\nmultilingual pre-trained models.", "AI": {"tldr": "This research explores cross-lingual sentiment analysis in Persian using few-shot and incremental learning with multilingual models.", "motivation": "To develop a sentiment analysis model for Persian that leverages knowledge from high-resource languages, addressing the challenge of limited data.", "method": "Utilized three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, DistilBERT) fine-tuned with few-shot and incremental learning methods on small Persian datasets from various sources.", "result": "mDeBERTa and XLM-RoBERTa models achieved 96% accuracy in sentiment analysis on Persian data.", "conclusion": "The study demonstrates that few-shot and incremental learning combined with multilingual models can effectively improve sentiment analysis performance in low-resource languages.", "key_contributions": ["Development of sentiment analysis model for Persian using few-shot learning", "Successful application of incremental learning techniques", "High performance achieved with multilingual models in a low-data scenario"], "limitations": "", "keywords": ["cross-lingual", "sentiment analysis", "few-shot learning", "incremental learning", "Persian"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2507.11903", "pdf": "https://arxiv.org/pdf/2507.11903.pdf", "abs": "https://arxiv.org/abs/2507.11903", "title": "Unveiling the Visual Rhetoric of Persuasive Cartography: A Case Study of the Design of Octopus Maps", "authors": ["Daocheng Lin", "Yifan Wang", "Yutong Yang", "Xingyu Lan"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "When designed deliberately, data visualizations can become powerful\npersuasive tools, influencing viewers' opinions, values, and actions. While\nresearchers have begun studying this issue (e.g., to evaluate the effects of\npersuasive visualization), we argue that a fundamental mechanism of persuasion\nresides in rhetorical construction, a perspective inadequately addressed in\ncurrent visualization research. To fill this gap, we present a focused analysis\nof octopus maps, a visual genre that has maintained persuasive power across\ncenturies and achieved significant social impact. Employing rhetorical schema\ntheory, we collected and analyzed 90 octopus maps spanning from the 19th\ncentury to contemporary times. We closely examined how octopus maps implement\ntheir persuasive intents and constructed a design space that reveals how visual\nmetaphors are strategically constructed and what common rhetorical strategies\nare applied to components such as maps, octopus imagery, and text. Through the\nabove analysis, we also uncover a set of interesting findings. For instance,\ncontrary to the common perception that octopus maps are primarily a historical\nphenomenon, our research shows that they remain a lively design convention in\ntoday's digital age. Additionally, while most octopus maps stem from Western\ndiscourse that views the octopus as an evil symbol, some designs offer\nalternative interpretations, highlighting the dynamic nature of rhetoric across\ndifferent sociocultural settings. Lastly, drawing from the lessons provided by\noctopus maps, we discuss the associated ethical concerns of persuasive\nvisualization.", "AI": {"tldr": "This paper analyzes octopus maps as persuasive visual tools that utilize rhetorical construction, revealing their historical significance and contemporary relevance.", "motivation": "To address the gap in visualization research regarding the persuasive power of rhetorical construction in data visualizations.", "method": "A focused analysis of 90 octopus maps from the 19th century to contemporary times using rhetorical schema theory to examine their persuasive intents.", "result": "The analysis shows that octopus maps remain relevant in today's digital age, highlighting the dynamic nature of their rhetorical construction and ethical issues in persuasive visualization.", "conclusion": "Octopus maps illustrate the importance of understanding rhetorical strategies in visualizations, emphasizing the need to consider ethical concerns in their design.", "key_contributions": ["Analysis of persuasive power in octopus maps", "Identification of rhetorical strategies in visual metaphors", "Discussion of contemporary relevance and ethical issues in data visualization"], "limitations": "", "keywords": ["data visualization", "persuasion", "rhetorical construction", "octopus maps", "ethical concerns"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.11661", "pdf": "https://arxiv.org/pdf/2507.11661.pdf", "abs": "https://arxiv.org/abs/2507.11661", "title": "Partitioner Guided Modal Learning Framework", "authors": ["Guimin Hu", "Yi Xin", "Lijie Hu", "Zhihong Zhu", "Hasti Seifi"], "categories": ["cs.CL", "cs.AI"], "comment": "acm multimedia 2025", "summary": "Multimodal learning benefits from multiple modal information, and each\nlearned modal representations can be divided into uni-modal that can be learned\nfrom uni-modal training and paired-modal features that can be learned from\ncross-modal interaction. Building on this perspective, we propose a\npartitioner-guided modal learning framework, PgM, which consists of the modal\npartitioner, uni-modal learner, paired-modal learner, and uni-paired modal\ndecoder. Modal partitioner segments the learned modal representation into\nuni-modal and paired-modal features. Modal learner incorporates two dedicated\ncomponents for uni-modal and paired-modal learning. Uni-paired modal decoder\nreconstructs modal representation based on uni-modal and paired-modal features.\nPgM offers three key benefits: 1) thorough learning of uni-modal and\npaired-modal features, 2) flexible distribution adjustment for uni-modal and\npaired-modal representations to suit diverse downstream tasks, and 3) different\nlearning rates across modalities and partitions. Extensive experiments\ndemonstrate the effectiveness of PgM across four multimodal tasks and further\nhighlight its transferability to existing models. Additionally, we visualize\nthe distribution of uni-modal and paired-modal features across modalities and\ntasks, offering insights into their respective contributions.", "AI": {"tldr": "The paper proposes a partitioner-guided modal learning framework, PgM, for multimodal learning that effectively segments and learns uni-modal and paired-modal features.", "motivation": "To improve multimodal learning by creating a structured approach that leverages both uni-modal and paired-modal representations for better performance in various tasks.", "method": "PgM is composed of a modal partitioner, uni-modal learner, paired-modal learner, and uni-paired modal decoder that together allow for effective learning of both types of features and adaptable representations for different downstream tasks.", "result": "PgM demonstrates significant effectiveness in four multimodal tasks and shows robustness and transferability to existing models, improving our understanding of feature distribution across tasks.", "conclusion": "The developed framework, PgM, provides insights into the contributions of uni-modal and paired-modal features, and offers advantages such as flexible distribution adjustment and varied learning rates.", "key_contributions": ["Introduction of the PgM framework for multimodal learning", "Effective segmentation of modal representations into uni-modal and paired-modal features", "Visualization insights into feature distribution across modalities and tasks."], "limitations": ".", "keywords": ["multimodal learning", "uni-modal features", "paired-modal features", "machine learning", "cross-modal interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.11911", "pdf": "https://arxiv.org/pdf/2507.11911.pdf", "abs": "https://arxiv.org/abs/2507.11911", "title": "AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding", "authors": ["Xiaoqing Chen", "Siyang Li", "Dongrui Wu"], "categories": ["cs.HC", "cs.IR", "cs.LG"], "comment": null, "summary": "Electroencephalogram (EEG) decoding models for brain-computer interfaces\n(BCIs) struggle with cross-dataset learning and generalization due to channel\nlayout inconsistencies, non-stationary signal distributions, and limited\nneurophysiological prior integration. To address these issues, we propose a\nplug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has\ntwo main components: 1) Spatial Alignment, which selects task-relevant channels\nbased on brain-region priors, aligns EEG distributions across domains, and\nremaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,\nwhich models multi-dataset signals into unified spatiotemporal patches for EEG\ndecoding. Compared to 17 state-of-the-art approaches that need dataset-specific\ntuning, the proposed calibration-free AFPM achieves performance gains of up to\n4.40% on motor imagery and 3.58% on event-related potential tasks. To our\nknowledge, this is the first calibration-free cross-dataset EEG decoding\nframework, substantially enhancing the practicalness of BCIs in real-world\napplications.", "AI": {"tldr": "The paper presents a novel calibration-free framework, AFPM, for cross-dataset EEG decoding that improves BCI performance without dataset-specific tuning.", "motivation": "Cross-dataset learning in EEG decoding for BCIs faces challenges due to inconsistencies in channel layouts and non-stationary signal distributions.", "method": "The framework consists of two components: Spatial Alignment to standardize channels and align EEG distributions, and Frame-Patch Encoding to create unified spatiotemporal patches for decoding.", "result": "The AFPM framework outperforms 17 state-of-the-art methods, achieving performance improvements of 4.40% in motor imagery and 3.58% in event-related potential tasks without the need for calibration.", "conclusion": "AFPM significantly enhances the practicality of BCIs in real-world settings by providing a calibration-free solution for EEG decoding across different datasets.", "key_contributions": ["Introduction of the first calibration-free framework for EEG decoding", "Improved alignment of EEG signals across datasets", "Enhanced performance of BCI systems without dataset-specific tuning"], "limitations": "", "keywords": ["EEG decoding", "brain-computer interfaces", "cross-dataset learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.11694", "pdf": "https://arxiv.org/pdf/2507.11694.pdf", "abs": "https://arxiv.org/abs/2507.11694", "title": "ExpliCIT-QA: Explainable Code-Based Image Table Question Answering", "authors": ["Maximiliano Hormazábal Lagos", "Álvaro Bueno Sáez", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro", "Héctor Cerezo-Costas"], "categories": ["cs.CL", "cs.AI"], "comment": "This work has been accepted for presentation at the 24nd Portuguese\n  Conference on Artificial Intelligence (EPIA 2025) and will be published in\n  the proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "summary": "We present ExpliCIT-QA, a system that extends our previous MRT approach for\ntabular question answering into a multimodal pipeline capable of handling\ncomplex table images and providing explainable answers. ExpliCIT-QA follows a\nmodular design, consisting of: (1) Multimodal Table Understanding, which uses a\nChain-of-Thought approach to extract and transform content from table images;\n(2) Language-based Reasoning, where a step-by-step explanation in natural\nlanguage is generated to solve the problem; (3) Automatic Code Generation,\nwhere Python/Pandas scripts are created based on the reasoning steps, with\nfeedback for handling errors; (4) Code Execution to compute the final answer;\nand (5) Natural Language Explanation that describes how the answer was\ncomputed. The system is built for transparency and auditability: all\nintermediate outputs, parsed tables, reasoning steps, generated code, and final\nanswers are available for inspection. This strategy works towards closing the\nexplainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on\nthe TableVQA-Bench benchmark, comparing it with existing baselines. We\ndemonstrated improvements in interpretability and transparency, which open the\ndoor for applications in sensitive domains like finance and healthcare where\nauditing results are critical.", "AI": {"tldr": "ExpliCIT-QA is a multimodal system for tabular question answering that enhances transparency and explainability by providing step-by-step reasoning and code generation.", "motivation": "To address the explainability gap in end-to-end TableVQA systems, particularly in sensitive domains like finance and healthcare where auditing results is crucial.", "method": "The system employs a modular design including: 1) Multimodal Table Understanding with a Chain-of-Thought approach; 2) Language-based Reasoning for natural language explanations; 3) Automatic Code Generation for creating Python/Pandas scripts; 4) Code Execution for final answer computation; 5) Natural Language Explanation detailing how the answer was computed.", "result": "ExpliCIT-QA demonstrated improvements in interpretability and transparency over existing benchmarks, evaluated on the TableVQA-Bench.", "conclusion": "The modular design and explainability enhancements of ExpliCIT-QA could significantly benefit applications in sensitive areas such as finance and healthcare.", "key_contributions": ["Modular design for improved transparency and explainability in tabular question answering", "Step-by-step natural language reasoning and code generation", "Evaluation on TableVQA-Bench shows significant interpretability improvements"], "limitations": "", "keywords": ["ExpliCIT-QA", "TableVQA", "explainable AI", "multimodal systems", "healthcare applications"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.11960", "pdf": "https://arxiv.org/pdf/2507.11960.pdf", "abs": "https://arxiv.org/abs/2507.11960", "title": "d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement", "authors": ["Hyein Hong", "Sangbong Yoo", "SeokHwan Choi", "Jisue Kim", "Seongbum Seo", "Haneol Cho", "Chansoo Kim", "Yun Jang"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Approaches to enhancing data quality (DQ) are classified into two main\ncategories: data- and process-driven. However, prior research has predominantly\nutilized batch data preprocessing within the data-driven framework, which often\nproves insufficient for optimizing machine learning (ML) model performance and\nfrequently leads to distortions in data characteristics. Existing studies have\nprimarily focused on data preprocessing rather than genuine data quality\nimprovement (DQI). In this paper, we introduce d-DQIVAR, a novel visual\nanalytics system designed to facilitate DQI strategies aimed at improving ML\nmodel performance. Our system integrates visual analytics techniques that\nleverage both data-driven and process-driven approaches. Data-driven techniques\ntackle DQ issues such as imputation, outlier detection, deletion, format\nstandardization, removal of duplicate records, and feature selection.\nProcess-driven strategies encompass evaluating DQ and DQI procedures by\nconsidering DQ dimensions and ML model performance and applying the\nKolmogorov-Smirnov test. We illustrate how our system empowers users to harness\nexpert and domain knowledge effectively within a practical workflow through\ncase studies, evaluations, and user studies.", "AI": {"tldr": "The paper presents d-DQIVAR, a visual analytics system aimed at improving data quality to enhance ML model performance by combining data-driven and process-driven approaches.", "motivation": "To address the inadequacies of traditional batch data preprocessing techniques in optimizing ML model performance and to focus on genuine data quality improvement.", "method": "The d-DQIVAR system integrates visual analytics techniques to implement both data-driven (imputation, outlier detection, etc.) and process-driven (evaluating DQ and DQI using Kolmogorov-Smirnov test) strategies.", "result": "Demonstrates effective improvement in data quality, leading to enhancements in machine learning model performance through user studies and case evaluations.", "conclusion": "The d-DQIVAR system empowers users by allowing them to utilize their domain knowledge within a structured DQI workflow, significantly contributing to ML outcomes.", "key_contributions": ["Introduction of d-DQIVAR as a novel visual analytics system for DQI", "Combines data-driven and process-driven strategies for improved ML performance", "Case studies and user evaluations showcasing system effectiveness"], "limitations": "", "keywords": ["data quality", "visual analytics", "machine learning", "data preprocessing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.11742", "pdf": "https://arxiv.org/pdf/2507.11742.pdf", "abs": "https://arxiv.org/abs/2507.11742", "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks", "authors": ["Meng Li", "Timothy M. McPhillips", "Dingmin Wang", "Shin-Rong Tsai", "Bertram Ludäscher"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. Accepted to COLM 2025", "summary": "Recognizing the information flows and operations comprising data science and\nmachine learning Python notebooks is critical for evaluating, reusing, and\nadapting notebooks for new tasks. Investigating a notebook via re-execution\noften is impractical due to the challenges of resolving data and software\ndependencies. While Large Language Models (LLMs) pre-trained on large codebases\nhave demonstrated effectiveness in understanding code without running it, we\nobserve that they fail to understand some realistic notebooks due to\nhallucinations and long-context challenges. To address these issues, we propose\na notebook understanding task yielding an information flow graph and\ncorresponding cell execution dependency graph for a notebook, and demonstrate\nthe effectiveness of a pincer strategy that uses limited syntactic analysis to\nassist full comprehension of the notebook using an LLM. Our Capture and Resolve\nAssisted Bounding Strategy (CRABS) employs shallow syntactic parsing and\nanalysis of the abstract syntax tree (AST) to capture the correct\ninterpretation of a notebook between lower and upper estimates of the\ninter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via\ncell-by-cell zero-shot learning, thereby identifying the true data inputs and\noutputs of each cell. We evaluate and demonstrate the effectiveness of our\napproach using an annotated dataset of 50 representative, highly up-voted\nKaggle notebooks that together represent 3454 actual cell inputs and outputs.\nThe LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the\nsyntactic structure of these notebooks. Across 50 notebooks, CRABS achieves\naverage F1 scores of 98% identifying cell-to-cell information flows and 99%\nidentifying transitive cell execution dependencies.", "AI": {"tldr": "This paper introduces a strategy for better understanding Jupyter notebooks using a combination of syntactic analysis and Large Language Models (LLMs) to resolve ambiguities in data flow and dependencies between cells.", "motivation": "Understanding the flow of information in data science and machine learning notebooks is crucial for their effective reuse and adaptation, yet existing LLMs struggle with ambiguities due to hallucinations and long-context issues.", "method": "The proposed method, CRABS, uses shallow syntactic parsing and analysis of the abstract syntax tree (AST) to interpret notebooks, then employs an LLM to resolve remaining ambiguities in the data inputs and outputs of notebook cells, moving through a cell-by-cell zero-shot learning approach.", "result": "CRABS achieved an F1 score of 98% in identifying cell-to-cell information flows and 99% in identifying transitive cell execution dependencies when evaluated on a dataset of 50 annotated Kaggle notebooks and was able to resolve 98% of ambiguities present in the notebooks.", "conclusion": "The combination of syntactic analysis and LLMs significantly enhances the understanding of information flows and dependencies in Jupyter notebooks, making a valuable contribution to the effective reuse of data science resources.", "key_contributions": ["Introduction of the Capture and Resolve Assisted Bounding Strategy (CRABS) for notebook comprehension", "Demonstrated effectiveness using a large dataset of annotated notebooks", "High accuracy in resolving ambiguities and identifying information flows and dependencies."], "limitations": "The performance may vary with different notebook formats or unobserved edge cases not present in the training data.", "keywords": ["notebook understanding", "Large Language Models", "data flow analysis", "syntactic parsing", "information systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.11984", "pdf": "https://arxiv.org/pdf/2507.11984.pdf", "abs": "https://arxiv.org/abs/2507.11984", "title": "Dataset-Adaptive Dimensionality Reduction", "authors": ["Hyeon Jeon", "Jeongin Park", "Soohyun Lee", "Dae Hyun Kim", "Sungbok Shin", "Jinwook Seo"], "categories": ["cs.HC", "cs.LG"], "comment": "IEEE VIS 2025 & IEEE Transactions on Visualization and Computer\n  Graphics (TVCG)", "summary": "Selecting the appropriate dimensionality reduction (DR) technique and\ndetermining its optimal hyperparameter settings that maximize the accuracy of\nthe output projections typically involves extensive trial and error, often\nresulting in unnecessary computational overhead. To address this challenge, we\npropose a dataset-adaptive approach to DR optimization guided by structural\ncomplexity metrics. These metrics quantify the intrinsic complexity of a\ndataset, predicting whether higher-dimensional spaces are necessary to\nrepresent it accurately. Since complex datasets are often inaccurately\nrepresented in two-dimensional projections, leveraging these metrics enables us\nto predict the maximum achievable accuracy of DR techniques for a given\ndataset, eliminating redundant trials in optimizing DR. We introduce the design\nand theoretical foundations of these structural complexity metrics. We\nquantitatively verify that our metrics effectively approximate the ground truth\ncomplexity of datasets and confirm their suitability for guiding\ndataset-adaptive DR workflow. Finally, we empirically show that our\ndataset-adaptive workflow significantly enhances the efficiency of DR\noptimization without compromising accuracy.", "AI": {"tldr": "The paper presents a dataset-adaptive approach to optimize dimensionality reduction (DR) techniques, minimizing trial and error through the use of structural complexity metrics.", "motivation": "The need to optimize dimensionality reduction techniques efficiently due to the extensive trial and error currently involved, which leads to excessive computational overhead.", "method": "The approach utilizes structural complexity metrics that quantify the intrinsic complexity of a dataset to determine the necessity of dimensionality, thus guiding the optimization process.", "result": "The proposed metrics were verified to effectively approximate the ground truth complexity of datasets, confirming their effectiveness in a dataset-adaptive DR workflow.", "conclusion": "The dataset-adaptive workflow significantly improves the efficiency of DR optimization while maintaining accuracy.", "key_contributions": ["Introduction of structural complexity metrics for dataset-adaptive DR optimization", "Quantitative verification of metrics' effectiveness in approximating dataset complexity", "Empirical evidence showing enhanced efficiency in DR optimization processes"], "limitations": "", "keywords": ["dimensionality reduction", "structural complexity metrics", "dataset-adaptive optimization"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.11764", "pdf": "https://arxiv.org/pdf/2507.11764.pdf", "abs": "https://arxiv.org/abs/2507.11764", "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles", "authors": ["Matteo Fasulo", "Luca Babboni", "Luca Tedeschini"], "categories": ["cs.CL", "cs.IR"], "comment": "14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab", "summary": "This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).", "AI": {"tldr": "This paper focuses on improving subjectivity detection in news articles using AI Wizards' approach at CLEF 2025, enhancing transformer classifiers with sentiment scores across multiple languages.", "motivation": "To improve the accuracy of classifying sentences as subjective or objective in news articles, particularly in multilingual settings where generalization to unseen languages is critical.", "method": "The proposed method integrates sentiment scores from an auxiliary model with transformer-based classifiers like mDeBERTaV3-base and Llama3.2-1B, using decision threshold calibration to address class imbalance across several languages.", "result": "Results showed that integrating sentiment features significantly improved the performance of subjectivity detection, yielding a notable Macro F1 score of 0.51 for Greek language data.", "conclusion": "The sentiment-augmented classifier framework demonstrates a robust approach to enhancing subjectivity detection performance and achieving high rankings in multilingual settings.", "key_contributions": ["Development of a sentiment-augmented architecture for subjectivity detection", "Demonstration of enhancements across multiple languages, including unseen ones", "Optimizing decision thresholds to alleviate class imbalance"], "limitations": "", "keywords": ["subjectivity detection", "sentiment analysis", "multilingual classification", "transformer models", "class imbalance"], "importance_score": 7, "read_time_minutes": 14}}
{"id": "2507.11999", "pdf": "https://arxiv.org/pdf/2507.11999.pdf", "abs": "https://arxiv.org/abs/2507.11999", "title": "Envisage: Towards Expressive Visual Graph Querying", "authors": ["Xiaolin Wen", "Qishuang Fu", "Shuangyue Han", "Yichen Guo", "Joseph K. Liu", "Yong Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Graph querying is the process of retrieving information from graph data using\nspecialized languages (e.g., Cypher), often requiring programming expertise.\nVisual Graph Querying (VGQ) streamlines this process by enabling users to\nconstruct and execute queries via an interactive interface without resorting to\ncomplex coding. However, current VGQ tools only allow users to construct simple\nand specific query graphs, limiting users' ability to interactively express\ntheir query intent, especially for underspecified query intent. To address\nthese limitations, we propose Envisage, an interactive visual graph querying\nsystem to enhance the expressiveness of VGQ in complex query scenarios by\nsupporting intuitive graph structure construction and flexible parameterized\nrule specification. Specifically, Envisage comprises four stages: Query\nExpression allows users to interactively construct graph queries through\nintuitive operations; Query Verification enables the validation of constructed\nqueries via rule verification and query instantiation; Progressive Query\nExecution can progressively execute queries to ensure meaningful querying\nresults; and Result Analysis facilitates result exploration and interpretation.\nTo evaluate Envisage, we conducted two case studies and in-depth user\ninterviews with 14 graph analysts. The results demonstrate its effectiveness\nand usability in constructing, verifying, and executing complex graph queries.", "AI": {"tldr": "Envisage enhances Visual Graph Querying by allowing intuitive construction and execution of complex graph queries without coding.", "motivation": "Current VGQ tools are limited in allowing users to express complex queries interactively, requiring coding expertise.", "method": "Envisage introduces four stages: Query Expression for intuitive graph construction, Query Verification for validation, Progressive Query Execution for meaningful results, and Result Analysis for exploration.", "result": "User studies with 14 graph analysts showed that Envisage significantly improves the usability and effectiveness of graph querying.", "conclusion": "Envisage offers a more expressive and user-friendly approach to visual graph querying, facilitating the handling of complex queries.", "key_contributions": ["Introduction of a four-stage interactive querying process", "Enhanced usability for non-technical users in graph querying", "Support for complex query scenarios with flexible parameterization"], "limitations": "", "keywords": ["Visual Graph Querying", "graph queries", "user interaction", "query execution", "usability"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.11809", "pdf": "https://arxiv.org/pdf/2507.11809.pdf", "abs": "https://arxiv.org/abs/2507.11809", "title": "Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models", "authors": ["Dante Campregher", "Yanxu Chen", "Sander Hoffman", "Maria Heuss"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 Pages, 13 figures", "summary": "This paper presents a reproducibility study examining how Large Language\nModels (LLMs) manage competing factual and counterfactual information, focusing\non the role of attention heads in this process. We attempt to reproduce and\nreconcile findings from three recent studies by Ortu et al., Yu, Merullo, and\nPavlick and McDougall et al. that investigate the competition between\nmodel-learned facts and contradictory context information through Mechanistic\nInterpretability tools. Our study specifically examines the relationship\nbetween attention head strength and factual output ratios, evaluates competing\nhypotheses about attention heads' suppression mechanisms, and investigates the\ndomain specificity of these attention patterns. Our findings suggest that\nattention heads promoting factual output do so via general copy suppression\nrather than selective counterfactual suppression, as strengthening them can\nalso inhibit correct facts. Additionally, we show that attention head behavior\nis domain-dependent, with larger models exhibiting more specialized and\ncategory-sensitive patterns.", "AI": {"tldr": "This study examines how Large Language Models (LLMs) handle conflicting information, focusing on attention heads and their effect on factual outputs.", "motivation": "To examine the competition between factual and counterfactual information in LLMs and understand the role of attention heads in this process.", "method": "The study reproduces findings from recent literature using Mechanistic Interpretability tools to analyze attention head behaviors and their impact on factual output ratios.", "result": "The findings indicate that attention heads facilitating factual output operate through general copy suppression rather than selective counterfactual suppression, and the behavior of attention heads is affected by the domain of the information.", "conclusion": "Attention head behavior is domain-specific, revealing that larger models may demonstrate more specialized patterns that are sensitive to different categories.", "key_contributions": ["Reconciliation of findings from multiple studies on LLMs' handling of factual and counterfactual information.", "Insights into the role of attention heads in factual output management.", "Demonstration of domain specificity in attention head behaviors."], "limitations": "", "keywords": ["Large Language Models", "Attention Heads", "Factual and Counterfactual Information"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2507.12204", "pdf": "https://arxiv.org/pdf/2507.12204.pdf", "abs": "https://arxiv.org/abs/2507.12204", "title": "Tao-Technology for Teen Mobile Use: Harmonizing Adaptation, Autonomy, and Reflection", "authors": ["Pengyu Zhu", "Janghee Cho"], "categories": ["cs.HC"], "comment": null, "summary": "Adolescents' mobile technology use is often regulated through rigid control\nmechanisms that fail to account for their autonomy and natural usage patterns.\nDrawing on Taoist philosophy, particularly Wu Wei, Yin-Yang, and Zi Ran, this\nposition paper proposes Tao-Technology, a self-organizing, adaptive regulatory\nframework. Integrating insights from Reflective Informatics and Information\nEcologies, we explore how mobile technology can dynamically adjust to context\nwhile fostering self-reflection and meaning-making. This approach shifts from\nexternal restrictions to dynamic co-adaptative regulation, ensuring technology\ngovernance remains flexible yet structured, supporting adolescents in\ncultivating a balanced and intentional relationship with digital technology.", "AI": {"tldr": "This paper proposes a self-organizing framework for regulating adolescents' mobile technology use, inspired by Taoist philosophy, which promotes autonomy and adaptive governance.", "motivation": "Current regulatory mechanisms for adolescents' mobile technology use are too rigid and do not consider their autonomy or natural usage patterns.", "method": "The paper introduces the concept of Tao-Technology, drawing on Taoist principles like Wu Wei and Yin-Yang, and integrates insights from Reflective Informatics and Information Ecologies.", "result": "The proposed framework allows mobile technology to adjust dynamically to the user's context, enhancing self-reflection and meaning-making.", "conclusion": "By shifting from strict control to a flexible co-adaptive regulation, the framework supports adolescents in developing a balanced relationship with technology.", "key_contributions": ["Introduction of Tao-Tech framework based on Taoist philosophy", "Integration of Reflective Informatics and Information Ecologies", "Focus on dynamic co-adaptive regulation rather than rigid control"], "limitations": "", "keywords": ["Taoist philosophy", "mobile technology", "self-regulation", "adolescents", "adaptive governance"], "importance_score": 3, "read_time_minutes": 12}}
{"id": "2507.11832", "pdf": "https://arxiv.org/pdf/2507.11832.pdf", "abs": "https://arxiv.org/abs/2507.11832", "title": "ILID: Native Script Language Identification for Indian Languages", "authors": ["Yash Ingle", "Pruthwik Mishra"], "categories": ["cs.CL"], "comment": "8 pages, 1 figure, 7 tables, Paper accepted in RANLP 2025", "summary": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script making the task even more challenging.\nIn this paper, we release a dataset of 230K sentences consisting of English and\nall 22 official Indian languages labeled with their language identifiers where\ndata in most languages are newly created. We also develop and release robust\nbaseline models using state-of-the-art approaches in machine learning and deep\nlearning that can aid the research in this field. Our baseline models are\ncomparable to the state-of-the-art models for the language identification task.", "AI": {"tldr": "The paper presents a new dataset for language identification involving English and 22 Indian languages, along with baseline models using machine learning and deep learning techniques.", "motivation": "Language identification is a crucial task in NLP and serves as a pre-processing step for many applications. The challenge is heightened in diverse Indian languages with phonetic similarities.", "method": "The authors released a dataset comprising 230K sentences of English and 22 Indian languages, which includes newly created data. They developed robust baseline models using advanced machine learning and deep learning methods.", "result": "The baseline models achieved performance comparable to existing state-of-the-art models in the language identification task.", "conclusion": "This work contributes a valuable resource and models for advancing research in language identification, particularly for the diverse Indian languages.", "key_contributions": ["Release of a large dataset (230K sentences) for language identification in Indian languages", "Development of robust baseline models for language identification using modern ML techniques", "Contribution to the NLP community by addressing challenges present in language identification for Indian languages"], "limitations": "", "keywords": ["language identification", "Indian languages", "dataset", "machine learning", "deep learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.12212", "pdf": "https://arxiv.org/pdf/2507.12212.pdf", "abs": "https://arxiv.org/abs/2507.12212", "title": "Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness", "authors": ["Garyoung Kim", "Huisung Kwon", "Seoju Yun", "Yu-Won Youn"], "categories": ["cs.HC", "cs.AI"], "comment": "7 pages, 3 figures", "summary": "Generative AI does not only replicate human creativity but also reproduces\ndeep-seated cultural biases, making it crucial to critically examine how\nconcepts like ugliness are understood and expressed by these tools. This study\ninvestigates how four different generative AI models understand and express\nugliness through text and image and explores the biases embedded within these\nrepresentations. We extracted 13 adjectives associated with ugliness through\niterative prompting of a large language model and generated 624 images across\nfour AI models and three prompts. Demographic and socioeconomic attributes\nwithin the images were independently coded and thematically analyzed. Our\nfindings show that AI models disproportionately associate ugliness with old\nwhite male figures, reflecting entrenched social biases as well as paradoxical\nbiases, where efforts to avoid stereotypical depictions of marginalized groups\ninadvertently result in the disproportionate projection of negative attributes\nonto majority groups. Qualitative analysis further reveals that, despite\nsupposed attempts to frame ugliness within social contexts, conventional\nphysical markers such as asymmetry and aging persist as central visual motifs.\nThese findings demonstrate that despite attempts to create more equal\nrepresentations, generative AI continues to perpetuate inherited and\nparadoxical biases, underscoring the critical work being done to create ethical\nAI training paradigms and advance methodologies for more inclusive AI\ndevelopment.", "AI": {"tldr": "This study analyzes how generative AI models understand and represent the concept of ugliness, revealing biases in their outputs.", "motivation": "To investigate the reproduction of cultural biases in generative AI models, particularly regarding the concept of ugliness.", "method": "The study extracted 13 adjectives linked to ugliness via a large language model, generating 624 images across four AI models. These images were thematically coded and analyzed for biases.", "result": "AI models disproportionately associate ugliness with old white male figures, revealing both entrenched social biases and paradoxical biases affecting various demographic groups.", "conclusion": "Despite attempts at equitable representation, generative AI perpetuates inherited biases, highlighting the need for ethical AI development frameworks.", "key_contributions": ["Analysis of biases in generative AI representations of ugliness", "Identification of how AI models reinforce societal stereotypes", "Recommendations for ethical AI training paradigms"], "limitations": "", "keywords": ["Generative AI", "Cultural bias", "Ugliness", "Ethical AI", "Inclusive development"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.11851", "pdf": "https://arxiv.org/pdf/2507.11851.pdf", "abs": "https://arxiv.org/abs/2507.11851", "title": "Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential", "authors": ["Mohammad Samragh", "Arnav Kundu", "David Harrison", "Kumari Nishu", "Devang Naik", "Minsik Cho", "Mehrdad Farajtabar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Autoregressive language models are constrained by their inherently sequential\nnature, generating one token at a time. This paradigm limits inference speed\nand parallelism, especially during later stages of generation when the\ndirection and semantics of text are relatively certain. In this work, we\npropose a novel framework that leverages the inherent knowledge of vanilla\nautoregressive language models about future tokens, combining techniques to\nrealize this potential and enable simultaneous prediction of multiple\nsubsequent tokens. Our approach introduces several key innovations: (1) a\nmasked-input formulation where multiple future tokens are jointly predicted\nfrom a common prefix; (2) a gated LoRA formulation that preserves the original\nLLM's functionality, while equipping it for multi-token prediction; (3) a\nlightweight, learnable sampler module that generates coherent sequences from\nthe predicted future tokens; (4) a set of auxiliary training losses, including\na consistency loss, to enhance the coherence and accuracy of jointly generated\ntokens; and (5) a speculative generation strategy that expands tokens\nquadratically in the future while maintaining high fidelity. Our method\nachieves significant speedups through supervised fine-tuning on pretrained\nmodels. For example, it generates code and math nearly 5x faster, and improves\ngeneral chat and knowledge tasks by almost 2.5x. These gains come without any\nloss in quality.", "AI": {"tldr": "This paper presents a novel framework that allows autoregressive language models to predict multiple future tokens simultaneously, significantly improving generation speed without compromising quality.", "motivation": "To overcome the slow inference speed and limited parallelism of autoregressive language models, especially during later stages of generation.", "method": "The approach includes a masked-input formulation for joint prediction, a gated LoRA to preserve original functionality, a lightweight sampler for coherent sequence generation, auxiliary training losses for coherence and accuracy, and a speculative generation strategy for quadratic token expansion.", "result": "The proposed method enables code and math generation nearly 5x faster, and improves general chat and knowledge tasks by almost 2.5x without loss of quality.", "conclusion": "The framework enhances the performance of autoregressive language models by leveraging knowledge of future tokens for faster and coherent text generation.", "key_contributions": ["Masked-input formulation for jointly predicting future tokens", "Gated LoRA formulation for multi-token prediction", "Lightweight learnable sampler for generating coherent sequences"], "limitations": "", "keywords": ["Autoregressive models", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.12296", "pdf": "https://arxiv.org/pdf/2507.12296.pdf", "abs": "https://arxiv.org/abs/2507.12296", "title": "Humans are more gullible than LLMs in believing common psychological myths", "authors": ["Bevan Koopman", "Guido Zuccon"], "categories": ["cs.HC"], "comment": null, "summary": "Despite widespread debunking, many psychological myths remain deeply\nentrenched. This paper investigates whether Large Language Models (LLMs) mimic\nhuman behaviour of myth belief and explores methods to mitigate such\ntendencies. Using 50 popular psychological myths, we evaluate myth belief\nacross multiple LLMs under different prompting strategies, including\nretrieval-augmented generation and swaying prompts. Results show that LLMs\nexhibit significantly lower myth belief rates than humans, though user\nprompting can influence responses. RAG proves effective in reducing myth belief\nand reveals latent debiasing potential within LLMs. Our findings contribute to\nthe emerging field of Machine Psychology and highlight how cognitive science\nmethods can inform the evaluation and development of LLM-based systems.", "AI": {"tldr": "This paper studies the behavior of Large Language Models (LLMs) in relation to belief in psychological myths and explores methods to reduce these tendencies.", "motivation": "The persistent belief in psychological myths despite debunking prompts a need to investigate how LLMs relate to these myths and the potential for methods to mitigate myth belief in AI systems.", "method": "The study evaluates myth belief across multiple LLMs using 50 psychological myths and various prompting strategies, including retrieval-augmented generation (RAG) and swaying prompts.", "result": "LLMs show lower rates of myth belief compared to humans, but user prompts can still significantly influence the models' responses. RAG is particularly effective at reducing myth belief.", "conclusion": "The findings highlight the potential for cognitive science methods to enhance the development and evaluation of LLM systems, suggesting a latent debiasing capability in LLMs.", "key_contributions": ["Examination of LLM behavior concerning psychological myth belief.", "Demonstration of the effectiveness of retrieval-augmented generation in reducing myth belief.", "Introduction of cognitive science methods in the evaluation of LLMs."], "limitations": "", "keywords": ["Large Language Models", "myth belief", "retrieval-augmented generation", "debiasing", "Machine Psychology"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.11862", "pdf": "https://arxiv.org/pdf/2507.11862.pdf", "abs": "https://arxiv.org/abs/2507.11862", "title": "Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition", "authors": ["Junhong Ye", "Xu Yuan", "Xinying Qiu"], "categories": ["cs.CL"], "comment": "Accepted to CLNLP 2025", "summary": "Accurate recognition of personally identifiable information (PII) is central\nto automated text anonymization. This paper investigates the effectiveness of\ncross-domain model transfer, multi-domain data fusion, and sample-efficient\nlearning for PII recognition. Using annotated corpora from healthcare (I2B2),\nlegal (TAB), and biography (Wikipedia), we evaluate models across four\ndimensions: in-domain performance, cross-domain transferability, fusion, and\nfew-shot learning. Results show legal-domain data transfers well to\nbiographical texts, while medical domains resist incoming transfer. Fusion\nbenefits are domain-specific, and high-quality recognition is achievable with\nonly 10% of training data in low-specialization domains.", "AI": {"tldr": "This paper explores the recognition of personally identifiable information (PII) through model transfer, data fusion, and efficient learning in various domains.", "motivation": "To enhance automated text anonymization by improving the recognition of PII across different domains.", "method": "The study employs annotated datasets from healthcare, legal, and biographical sources to evaluate model performance in in-domain and cross-domain settings, along with few-shot learning techniques.", "result": "The findings indicate that legal-domain data transfers effectively to biographical contexts, while medical domain data shows resistance to transfer. Additionally, the advantages of data fusion vary by domain, and high-quality recognition is attainable with significantly reduced training data in low-specialization domains.", "conclusion": "Cross-domain model transfer and data fusion can improve PII recognition, particularly in legal and biographical contexts, even with limited training data.", "key_contributions": ["Analysis of cross-domain model transfer for PII recognition", "Insights into data fusion benefits across domains", "Demonstration of effective few-shot learning in PII recognition"], "limitations": "The transfer effectiveness is domain-specific, with medical data being less adaptable to other domains.", "keywords": ["PII recognition", "cross-domain", "data fusion", "few-shot learning", "text anonymization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.12298", "pdf": "https://arxiv.org/pdf/2507.12298.pdf", "abs": "https://arxiv.org/abs/2507.12298", "title": "TrialCompass: Visual Analytics for Enhancing the Eligibility Criteria Design of Clinical Trials", "authors": ["Rui Sheng", "Xingbo Wang", "Jiachen Wang", "Xiaofu Jin", "Zhonghua Sheng", "Zhenxing Xu", "Suraj Rajendran", "Huamin Qu", "Fei Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Eligibility criteria play a critical role in clinical trials by determining\nthe target patient population, which significantly influences the outcomes of\nmedical interventions. However, current approaches for designing eligibility\ncriteria have limitations to support interactive exploration of the large space\nof eligibility criteria. They also ignore incorporating detailed\ncharacteristics from the original electronic health record (EHR) data for\ncriteria refinement. To address these limitations, we proposed TrialCompass, a\nvisual analytics system integrating a novel workflow, which can empower\nclinicians to iteratively explore the vast space of eligibility criteria\nthrough knowledge-driven and outcome-driven approaches. TrialCompass supports\nhistory-tracking to help clinicians trace the evolution of their adjustments\nand decisions when exploring various forms of data (i.e., eligibility criteria,\noutcome metrics, and detailed characteristics of original EHR data) through\nthese two approaches. This feature can help clinicians comprehend the impact of\neligibility criteria on outcome metrics and patient characteristics, which\nfacilitates systematic refinement of eligibility criteria. Using a real-world\ndataset, we demonstrated the effectiveness of TrialCompass in providing\ninsights into designing eligibility criteria for septic shock and\nsepsis-associated acute kidney injury. We also discussed the research prospects\nof applying visual analytics to clinical trials.", "AI": {"tldr": "TrialCompass is a visual analytics system designed to enhance the exploration and refinement of eligibility criteria in clinical trials using EHR data.", "motivation": "Current methods for designing eligibility criteria in clinical trials are limited and fail to integrate detailed EHR characteristics for optimal criteria refinement.", "method": "TrialCompass employs a knowledge-driven and outcome-driven workflow that enables clinicians to interactively explore eligibility criteria, track changes, and analyze their impact on outcomes.", "result": "TrialCompass demonstrated effectiveness with real-world datasets, providing significant insights for tailoring eligibility criteria for conditions like septic shock and acute kidney injury.", "conclusion": "This research opens avenues for using visual analytics to improve the design and refinement of clinical trial eligibility criteria, enhancing overall trial quality and outcomes.", "key_contributions": ["Development of TrialCompass for eligibility criteria exploration", "Integration of knowledge-driven and outcome-driven approaches", "History-tracking feature for iterative refinement of criteria"], "limitations": "", "keywords": ["visual analytics", "clinical trials", "eligibility criteria", "electronic health records", "sepsis"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.11867", "pdf": "https://arxiv.org/pdf/2507.11867.pdf", "abs": "https://arxiv.org/abs/2507.11867", "title": "COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction", "authors": ["Xiangyu Yang", "Xinying Qiu"], "categories": ["cs.CL"], "comment": "Accepted to CLNLP 2025", "summary": "Grammatical Error Correction (GEC) and grammatical acceptability judgment\n(COLA) are core tasks in natural language processing, sharing foundational\ngrammatical knowledge yet typically evolving independently. This paper\nintroduces COLA-GEC, a novel bidirectional framework that enhances both tasks\nthrough mutual knowledge transfer. First, we augment grammatical acceptability\nmodels using GEC datasets, significantly improving their performance across\nmultiple languages. Second, we integrate grammatical acceptability signals into\nGEC model training via a dynamic loss function, effectively guiding corrections\ntoward grammatically acceptable outputs. Our approach achieves state-of-the-art\nresults on several multilingual benchmarks. Comprehensive error analysis\nhighlights remaining challenges, particularly in punctuation error correction,\nproviding insights for future improvements in grammatical modeling.", "AI": {"tldr": "A novel bidirectional framework, COLA-GEC, enhances both Grammatical Error Correction and grammatical acceptability judgment by mutual knowledge transfer, improving performance across languages.", "motivation": "To address the independent evolution of GEC and COLA tasks in natural language processing by integrating their foundational grammatical knowledge.", "method": "Introducing COLA-GEC, we augment grammatical acceptability models with GEC datasets and incorporate grammatical acceptability signals into GEC training with a dynamic loss function.", "result": "Our approach achieves state-of-the-art results on multiple multilingual benchmarks.", "conclusion": "The results indicate the effectiveness of the mutual knowledge transfer approach, though challenges in punctuation error correction remain.", "key_contributions": ["Introduction of COLA-GEC framework for mutual knowledge transfer", "Significant performance improvement in grammatical acceptability models", "State-of-the-art results on multilingual benchmarks"], "limitations": "Challenges remain in punctuation error correction.", "keywords": ["Grammatical Error Correction", "grammatical acceptability judgment", "natural language processing", "COLA-GEC", "mutual knowledge transfer"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.12334", "pdf": "https://arxiv.org/pdf/2507.12334.pdf", "abs": "https://arxiv.org/abs/2507.12334", "title": "An Analysis of Text Functions in Information Visualization", "authors": ["Chase Stokes", "Anjana Arunkumar", "Marti A. Hearst", "Lace Padilla"], "categories": ["cs.HC", "H.5.0"], "comment": "11 pages, 3 figures, IEEE VIS Conference", "summary": "Text is an integral but understudied component of visualization design.\nAlthough recent studies have examined how text elements (e.g., titles and\nannotations) influence comprehension, preferences, and predictions, many\nquestions remain about textual design and use in practice. This paper\nintroduces a framework for understanding text functions in information\nvisualizations, building on and filling gaps in prior classifications and\ntaxonomies. Through an analysis of 120 real-world visualizations and 804 text\nelements, we identified ten distinct text functions, ranging from identifying\ndata mappings to presenting valenced subtext. We further identify patterns in\ntext usage and conduct a factor analysis, revealing four overarching\ntext-informed design strategies: Attribution and Variables, Annotation-Centric\nDesign, Visual Embellishments, and Narrative Framing. In addition to these\nfactors, we explore features of title rhetoric and text multifunctionality,\nwhile also uncovering previously unexamined text functions, such as text\nreplacing visual elements. Our findings highlight the flexibility of text,\ndemonstrating how different text elements in a given design can combine to\ncommunicate, synthesize, and frame visual information. This framework adds\nimportant nuance and detail to existing frameworks that analyze the diverse\nroles of text in visualization.", "AI": {"tldr": "This paper introduces a framework for understanding the role of text in information visualizations through an analysis of 120 visualizations and 804 text elements, identifying ten distinct text functions and four overarching design strategies.", "motivation": "Text is an integral but understudied component of visualization design, influencing comprehension and preferences; this study aims to deepen understanding of its functional roles.", "method": "The authors analyzed 120 real-world visualizations and 804 text elements, identifying distinct text functions and conducting factor analysis to reveal overarching design strategies.", "result": "Ten distinct text functions were identified, and four overarching strategies emerged: Attribution and Variables, Annotation-Centric Design, Visual Embellishments, and Narrative Framing.", "conclusion": "The findings emphasize the flexibility of text in visualizations and contribute nuanced insight into its diverse roles, enhancing existing frameworks.", "key_contributions": ["Introduced a new framework for text functions in visualizations.", "Identified ten distinct text functions.", "Revealed four overarching text-informed design strategies."], "limitations": "", "keywords": ["text functions", "information visualization", "design strategies", "HCI", "factor analysis"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.11875", "pdf": "https://arxiv.org/pdf/2507.11875.pdf", "abs": "https://arxiv.org/abs/2507.11875", "title": "DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation", "authors": ["Tianyou Huang", "Xinglu Chen", "Jingshen Zhang", "Xinying Qiu", "Ruiying Niu"], "categories": ["cs.CL"], "comment": "Accepted to CCL 2025", "summary": "This paper introduces DualReward, a novel reinforcement learning framework\nfor automatic distractor generation in cloze tests. Unlike conventional\napproaches that rely primarily on supervised learning or static generative\nmodels, our method employs a dual reward structure with adaptive scaling that\ndifferentiates between human-created gold standard distractors and\nmodel-generated candidates. The framework dynamically adjusts reward signal\nintensity based on model performance and confidence. We evaluate our approach\non both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,\ndemonstrating consistent improvements over state-of-the-art baselines.\nExperimental results show that our adaptive reward scaling mechanism provides\nmodest but consistent benefits on homogeneous datasets (CLOTH-F) and more\nsubstantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data\n(MCQ), suggesting its particular effectiveness for handling varied question\ntypes and domains. Our work offers a flexible framework that effectively\nbalances learning from reliable human examples while exploring novel,\nhigh-quality distractors for automated test generation.", "AI": {"tldr": "Introduction of DualReward, a reinforcement learning framework for automatic distractor generation in cloze tests which outperforms traditional methods.", "motivation": "To improve automatic distractor generation in cloze tests by utilizing a reinforcement learning approach rather than conventional supervised learning.", "method": "Employs a dual reward structure with adaptive scaling to differentiate between human-created gold standard distractors and model-generated candidates.", "result": "Demonstrated consistent improvements over state-of-the-art baselines on both passage-level and sentence-level cloze test datasets.", "conclusion": "Offers a flexible framework that balances learning from reliable human examples while exploring high-quality distractors for automated test generation.", "key_contributions": ["Introduction of DualReward framework", "Adaptive reward scaling mechanism", "Improved performance on diverse datasets"], "limitations": "", "keywords": ["Reinforcement learning", "Distractor generation", "Cloze tests", "Adaptive scaling"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.12337", "pdf": "https://arxiv.org/pdf/2507.12337.pdf", "abs": "https://arxiv.org/abs/2507.12337", "title": "MExplore: an entity-based visual analytics approach for medical expertise acquisition", "authors": ["Xiao Pang", "Yan Huang", "Chang Liu", "JiYuan Liu", "MingYou Liu"], "categories": ["cs.HC"], "comment": null, "summary": "Acquiring medical expertise is a critical component of medical education and\nprofessional development. While existing studies focus primarily on\nconstructing medical knowledge bases or developing learning tools based on the\nstructured, private healthcare data, they often lack methods for extracting\nexpertise from unstructured medical texts. These texts constitute a significant\nportion of medical literature and offer greater flexibility and detail compared\nto structured data formats. Furthermore, many studies fail to provide explicit\nanalytical and learning pathways in this context.\n  This paper introduces MExplore, an interactive visual analytics system\ndesigned to support the acquisition of medical expertise. To address the\nchallenges of the inconsistencies and confidentiality concerns inherent in\nunstructured medical texts, we propose a workflow that employs a fine-tuned\nBERT-based model to extract medical entities (MEs) from them. We then present a\nnovel multilevel visual analysis framework that integrates multiple coordinated\nvisualizations, enabling a progressive and interactive exploration of medical\nknowledge.\n  To assess the effectiveness of MExplore, we conducted three case studies, a\nuser study, and interviews with domain experts. The results indicate that the\nsystem significantly enhances the medical expertise acquisition process,\nproviding an effective interactive approach for acquiring and retaining\nknowledge from medical texts.", "AI": {"tldr": "MExplore is an interactive visual analytics system designed to enhance the acquisition of medical expertise from unstructured medical texts using a BERT-based model for entity extraction.", "motivation": "Existing studies lack methods for extracting expertise from unstructured medical texts, which are crucial for medical education and professional development.", "method": "Developed an interactive visual analytics system (MExplore) that integrates a fine-tuned BERT-based model to extract medical entities and a multilevel visual analysis framework for interactive exploration of medical knowledge.", "result": "The system significantly enhances the medical expertise acquisition process based on case studies, user study, and expert interviews.", "conclusion": "MExplore offers an effective interactive approach for acquiring and retaining knowledge from unstructured medical texts.", "key_contributions": ["Introduction of MExplore system for medical expertise acquisition", "Development of a fine-tuned BERT model for medical entity extraction", "Novel multilevel visual analysis framework for interactive exploration"], "limitations": "", "keywords": ["medical expertise", "visual analytics", "medical entity extraction", "unstructured texts", "interactive exploration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.11878", "pdf": "https://arxiv.org/pdf/2507.11878.pdf", "abs": "https://arxiv.org/abs/2507.11878", "title": "LLMs Encode Harmfulness and Refusal Separately", "authors": ["Jiachen Zhao", "Jing Huang", "Zhengxuan Wu", "David Bau", "Weiyan Shi"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs are trained to refuse harmful instructions, but do they truly understand\nharmfulness beyond just refusing? Prior work has shown that LLMs' refusal\nbehaviors can be mediated by a one-dimensional subspace, i.e., a refusal\ndirection. In this work, we identify a new dimension to analyze safety\nmechanisms in LLMs, i.e., harmfulness, which is encoded internally as a\nseparate concept from refusal. There exists a harmfulness direction that is\ndistinct from the refusal direction. As causal evidence, steering along the\nharmfulness direction can lead LLMs to interpret harmless instructions as\nharmful, but steering along the refusal direction tends to elicit refusal\nresponses directly without reversing the model's judgment on harmfulness.\nFurthermore, using our identified harmfulness concept, we find that certain\njailbreak methods work by reducing the refusal signals without reversing the\nmodel's internal belief of harmfulness. We also find that adversarially\nfinetuning models to accept harmful instructions has minimal impact on the\nmodel's internal belief of harmfulness. These insights lead to a practical\nsafety application: The model's latent harmfulness representation can serve as\nan intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing\nover-refusals that is robust to finetuning attacks. For instance, our Latent\nGuard achieves performance comparable to or better than Llama Guard 3 8B, a\ndedicated finetuned safeguard model, across different jailbreak methods. Our\nfindings suggest that LLMs' internal understanding of harmfulness is more\nrobust than their refusal decision to diverse input instructions, offering a\nnew perspective to study AI safety", "AI": {"tldr": "This paper explores a new dimension of harmfulness in LLMs, offering insights on their internal understanding of harmfulness versus refusal, and proposes a safety application termed Latent Guard.", "motivation": "To investigate whether LLMs truly understand harmfulness beyond merely refusing harmful instructions, and to identify a robust mechanism for AI safety.", "method": "The researchers analyze the harmfulness and refusal directions in LLMs, employing causal evidence to demonstrate that these two concepts operate independently within the models.", "result": "The study finds that steering LLMs toward the harmfulness direction can incorrectly label harmless instructions as harmful, while steering toward refusal triggers direct refusal responses. Latent Guard was shown to perform effectively against various jailbreaking methods.", "conclusion": "LLMs possess a distinct internal representation of harmfulness that can be harnessed for safety applications, providing a more reliable assessment of harmful inputs compared to their refusal behavior.", "key_contributions": ["Identification of a harmfulness direction distinct from refusal direction in LLMs", "Development of Latent Guard as a safeguard for detecting unsafe inputs", "Empirical evidence showing LLMs have a more robust understanding of harmfulness than refusal"], "limitations": "", "keywords": ["Harmfulness", "Refusal", "LLMs", "AI safety", "Latent Guard"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.12377", "pdf": "https://arxiv.org/pdf/2507.12377.pdf", "abs": "https://arxiv.org/abs/2507.12377", "title": "Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight", "authors": ["Ke Er Amy Zhang", "Jodie Jenkinson", "Laura Garrison"], "categories": ["cs.HC"], "comment": "11 pages, 5 figures, accepted to IEEE VIS 2025 Conference", "summary": "We conduct a deconstructive reading of a qualitative interview study with 17\nvisual data journalists from newsrooms across the globe. We borrow a\ndeconstruction approach from literary critique to explore the instability of\nmeaning in language and reveal implicit beliefs in words and ideas. Through our\nanalysis we surface two sets of opposing implicit beliefs in visual data\njournalism: objectivity/subjectivity and humanism/mechanism. We contextualize\nthese beliefs through a genealogical analysis, which brings deconstruction\ntheory into practice by providing a historic backdrop for these opposing\nperspectives. Our analysis shows that these beliefs held within visual data\njournalism are not self-enclosed but rather a product of external societal\nforces and paradigm shifts over time. Through this work, we demonstrate how\nthinking with critical theories such as deconstruction and genealogy can\nreframe \"success\" in visual data storytelling and diversify visualization\nresearch outcomes. These efforts push the ways in which we as researchers\nproduce domain knowledge to examine the sociotechnical issues of today's values\ntowards datafication and data visualization.", "AI": {"tldr": "This paper deconstructs implicit beliefs in visual data journalism using qualitative interviews with journalists, revealing societal influences on notions of objectivity and subjectivity in data storytelling.", "motivation": "To explore the instability of meaning in language and reveal implicit beliefs in visual data journalism through a deconstructive lens.", "method": "Qualitative analysis of interviews with 17 visual data journalists, applying deconstruction theory and genealogical analysis.", "result": "Identified opposing implicit beliefs in visual data journalism and demonstrated how they are shaped by societal forces over time.", "conclusion": "Critical theories can reframe success in visual data storytelling and enhance research outcomes related to sociotechnical issues in data visualization.", "key_contributions": ["Application of deconstruction theory to data journalism", "Identification of implicit beliefs impacting visualization", "Historic contextualization of beliefs through genealogy"], "limitations": "The study is based on a limited sample of journalists and may not represent all perspectives in visual data journalism.", "keywords": ["visual data journalism", "deconstruction", "implicit beliefs", "sociotechnical issues", "data visualization"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2507.11882", "pdf": "https://arxiv.org/pdf/2507.11882.pdf", "abs": "https://arxiv.org/abs/2507.11882", "title": "Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models", "authors": ["Bo Zeng", "Chenyang Lyu", "Sinuo Liu", "Mingyan Zeng", "Minghao Wu", "Xuanfan Ni", "Tianqi Shi", "Yu Zhao", "Yefeng Liu", "Chenyu Zhu", "Ruizhe Li", "Jiahui Geng", "Qing Li", "Yu Tong", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference paper", "summary": "Instruction-following capability has become a major ability to be evaluated\nfor Large Language Models (LLMs). However, existing datasets, such as IFEval,\nare either predominantly monolingual and centered on English or simply machine\ntranslated to other languages, limiting their applicability in multilingual\ncontexts. In this paper, we present an carefully-curated extension of IFEval to\na localized multilingual version named Marco-Bench-MIF, covering 30 languages\nwith varying levels of localization. Our benchmark addresses linguistic\nconstraints (e.g., modifying capitalization requirements for Chinese) and\ncultural references (e.g., substituting region-specific company names in\nprompts) via a hybrid pipeline combining translation with verification. Through\ncomprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)\n25-35% accuracy gap between high/low-resource languages, (2) model scales\nlargely impact performance by 45-60% yet persists script-specific challenges,\nand (3) machine-translated data underestimates accuracy by7-22% versus\nlocalized data. Our analysis identifies challenges in multilingual instruction\nfollowing, including keyword consistency preservation and compositional\nconstraint adherence across languages. Our Marco-Bench-MIF is available at\nhttps://github.com/AIDC-AI/Marco-Bench-MIF.", "AI": {"tldr": "A multilingual extension of the IFEval benchmark for evaluating instruction-following capabilities in Large Language Models (LLMs), addressing linguistic and cultural challenges across 30 languages.", "motivation": "To improve the evaluation of Large Language Models in multilingual contexts where existing datasets are limited and often not adequately localized.", "method": "A hybrid pipeline combining translation with verification was used to create the Marco-Bench-MIF, which was then evaluated on over 20 LLMs, assessing their performance across high and low-resource languages.", "result": "Findings show a 25-35% accuracy gap between high and low-resource languages, model scales affect performance by 45-60%, and localized data highlights a 7-22% underestimation of machine-translated data accuracy.", "conclusion": "The study identifies significant challenges in multilingual instruction following, including keyword consistency and compositional constraints, while providing a new benchmark for future research.", "key_contributions": ["Introduction of Marco-Bench-MIF, a multilingual benchmark", "Identification of performance gaps between high and low-resource languages", "Insight into the impact of localization on LLM accuracy"], "limitations": "The benchmark may still not fully capture all linguistic and cultural nuances across all 30 languages.", "keywords": ["Large Language Models", "multilingual evaluation", "instruction-following", "localization", "benchmark"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.11936", "pdf": "https://arxiv.org/pdf/2507.11936.pdf", "abs": "https://arxiv.org/abs/2507.11936", "title": "A Survey of Deep Learning for Geometry Problem Solving", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.", "AI": {"tldr": "This paper surveys the applications of deep learning in geometry problem solving, covering relevant tasks, methods, evaluation metrics, and future directions.", "motivation": "To provide a comprehensive and practical reference for deep learning applications in geometry problem solving, promoting further developments in the field.", "method": "The paper reviews various tasks in geometry problem solving, summarizes deep learning methods used, analyzes evaluation metrics, and discusses current challenges and future research directions.", "result": "The survey identifies key tasks in geometry problem solving and reviews numerous deep learning approaches while highlighting evaluation metrics and challenges in the field.", "conclusion": "This work aims to serve as a foundational reference for researchers and practitioners interested in applying deep learning to geometry problems and encourages continued exploration.", "key_contributions": ["Comprehensive summary of tasks in geometry problem solving.", "Thorough review of deep learning methods applied to geometry.", "Analysis of evaluation metrics for geometry problem solving."], "limitations": "The work is in progress and may not cover the latest developments or all aspects of the topic comprehensively.", "keywords": ["deep learning", "geometry problem solving", "multimodal models", "evaluation metrics", "future directions"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2507.11939", "pdf": "https://arxiv.org/pdf/2507.11939.pdf", "abs": "https://arxiv.org/abs/2507.11939", "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering", "authors": ["Yichen Xu", "Liangyu Chen", "Liang Zhang", "Wenxuan Wang", "Qin Jin"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "comment": "Work in Progress", "summary": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.", "AI": {"tldr": "PolyChartQA is a multilingual chart question answering benchmark containing 22,606 charts and 26,151 Q&A pairs in 10 languages, aiming to improve accessibility in chart understanding across diverse languages.", "motivation": "To address the English-centric nature of existing chart understanding benchmarks which limits accessibility and applicability for global audiences.", "method": "A decoupled pipeline separates chart data from rendering code to generate multilingual charts by translating data and reusing code. State-of-the-art LLM-based translation is utilized with strict quality control for linguistic and semantic consistency.", "result": "Experiments reveal a significant performance gap in chart understanding between English and other languages, particularly low-resource languages with non-Latin scripts.", "conclusion": "PolyChartQA establishes a foundation for enhancing inclusive vision-language models and systematic evaluation of multilingual chart understanding.", "key_contributions": ["Introduction of the first multilingual chart question answering benchmark (PolyChartQA)", "A robust decoupled pipeline for generating multilingual charts", "Insights on performance disparities in chart understanding across different languages"], "limitations": "As a work in progress, further improvements and validations may be needed in the benchmark design and data quality.", "keywords": ["multilingual", "chart understanding", "question answering", "LLM", "vision-language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.11941", "pdf": "https://arxiv.org/pdf/2507.11941.pdf", "abs": "https://arxiv.org/abs/2507.11941", "title": "BlockBPE: Parallel BPE Tokenization", "authors": ["Amos You"], "categories": ["cs.CL", "cs.DC"], "comment": "ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models\n  (ICML 2025)", "summary": "Tokenization is a critical preprocessing step in large language model\npipelines, yet widely-used implementations remain CPU-bound and suboptimal for\nbatch inference workflows on GPU. We present BlockBPE, a parallel GPU\nimplementation of byte-pair encoding (BPE) that achieves near linear-time\ncomplexity under realistic assumptions and is optimized for high-throughput,\nbatch inference. Unlike existing Rust-based tokenizers such as HuggingFace\nTokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex\npre-tokenization and exhibit $O(n \\log n)$ runtime-BlockBPE eliminates the\nRegex pre-tokenization which leads to small loss in generation quality, but\nenables highly parallelized token merges within thread blocks, reducing overall\ncomplexity to $O(nd)$ where $d \\ll n$. On high-batch inference workloads,\nBlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over\nHuggingFace Tokenizers.", "AI": {"tldr": "BlockBPE is a GPU-optimized parallel implementation of byte-pair encoding that enhances throughput in large language model tokenization.", "motivation": "To improve the efficiency of tokenization in large language models, specifically addressing the optimization for batch inference on GPU.", "method": "BlockBPE replaces regex pre-tokenization with a highly parallelized approach for token merging, achieving linear-time complexity under realistic conditions.", "result": "BlockBPE shows up to 2x higher throughput compared to tiktoken and 2.5x compared to HuggingFace Tokenizers in batch inference workloads.", "conclusion": "The proposed BlockBPE method significantly enhances performance for tokenization in large language model pipelines, enabling more efficient use of GPU resources.", "key_contributions": ["Introduction of BlockBPE as a parallel GPU implementation of BPE", "Elimination of regex pre-tokenization for improved performance", "Demonstrated significant throughput improvements over existing tokenizers"], "limitations": "", "keywords": ["tokenization", "byte-pair encoding", "GPU", "batch inference", "large language models"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.11942", "pdf": "https://arxiv.org/pdf/2507.11942.pdf", "abs": "https://arxiv.org/abs/2507.11942", "title": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao", "Baoyuan Qi", "Guoming Liu"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy.", "AI": {"tldr": "DAC is a dynamic attention-aware method for task-agnostic prompt compression that improves information density while reducing computational overhead.", "motivation": "Existing prompt compression methods lack consideration for attention-critical tokens and changes in information entropy during compression.", "method": "DAC integrates both entropy and attention information, dynamically adjusting for entropy shifts during the compression process.", "result": "DAC demonstrates substantial improvements across various domains such as LongBench, GSM8K, and BBH, outperforming prior methods.", "conclusion": "The proposed DAC method effectively enhances prompt compression in task-agnostic settings, showing robust performance across diverse tasks and LLMs.", "key_contributions": ["Proposes a dynamic attention-aware approach for prompt compression.", "Integrates entropy and attention information for better compression.", "Demonstrates significant improvements across multiple tasks and datasets."], "limitations": "", "keywords": ["prompt compression", "dynamic attention", "information entropy", "natural language processing", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.11953", "pdf": "https://arxiv.org/pdf/2507.11953.pdf", "abs": "https://arxiv.org/abs/2507.11953", "title": "IAM: Efficient Inference through Attention Mapping between Different-scale LLMs", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao"], "categories": ["cs.CL", "cs.LG"], "comment": "ACL 2025", "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.", "AI": {"tldr": "The paper introduces the IAM framework to optimize large language models (LLMs) by mapping attention between different-scale models, leading to improved efficiency without significant performance loss.", "motivation": "The motivation is to address the inefficiencies in resource consumption of LLMs, particularly with long contexts, and to utilize the similarity in attention matrices across models for optimization.", "method": "The authors analyze the similarity of attention matrices across different-scale LLMs, select mapping layers, and ensure consistency in mapping. They introduce the IAM framework that enhances attention computation and reduces KV cache usage.", "result": "IAM achieves a 15% acceleration in prefill and a 22.1% reduction in KV cache usage while maintaining performance across various model series.", "conclusion": "The IAM framework is a versatile tool to improve LLM efficiency, complementing existing cache optimization methods.", "key_contributions": ["Introduction of the IAM framework for attention mapping between LLMs.", "Demonstrated dual benefits of accelerated attention computation and reduced KV cache usage.", "Analysis of attention matrix similarities across different-scale LLMs."], "limitations": "", "keywords": ["Large Language Models", "Attention Mapping", "Resource Optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.11954", "pdf": "https://arxiv.org/pdf/2507.11954.pdf", "abs": "https://arxiv.org/abs/2507.11954", "title": "The benefits of query-based KGQA systems for complex and temporal questions in LLM era", "authors": ["Artem Alekseev", "Mikhail Chaichuk", "Miron Butko", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "categories": ["cs.CL", "cs.LG"], "comment": "15 pages, 3 figures, 7 tables", "summary": "Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System", "AI": {"tldr": "This paper presents a multi-stage query-based framework for knowledge graph question-answering using WikiData, focusing on improving performance for multi-hop and temporal questions.", "motivation": "To address the limitations of large language models in multi-hop reasoning and temporal questions, a modular approach using query-based KGQA is explored.", "method": "The authors propose a multi-stage approach that generates executable queries to enhance QA performance on challenging datasets, evaluated through generalization and rejection studies.", "result": "The proposed framework shows improved robustness and performance on multi-hop and temporal QA benchmarks, even with small language models.", "conclusion": "The study highlights the effectiveness of a query-based multi-stage KGQA framework in enhancing question-answering capabilities for complex inquiries.", "key_contributions": ["Introduction of a modular query-based KGQA framework for multi-hop and temporal reasoning.", "Development of a novel entity linking and predicate matching method using CoT reasoning.", "Demonstration of the framework’s efficacy with small language models on challenging QA benchmarks."], "limitations": "The framework's effectiveness may vary depending on the specific characteristics of the datasets used.", "keywords": ["Knowledge Graph", "Question Answering", "Multi-hop Reasoning", "Temporal Questions", "Large Language Models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.11959", "pdf": "https://arxiv.org/pdf/2507.11959.pdf", "abs": "https://arxiv.org/abs/2507.11959", "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs", "authors": ["Xinyu Wang", "Vahid Partovi Nia", "Peng Lu", "Jerry Huang", "Xiao-Wen Chang", "Boxing Chen", "Yufei Cui"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ECAI 2025 (European Conference on Artificial\n  Intelligence)", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.", "AI": {"tldr": "This paper proposes an advanced Power-of-Two (PoT) quantization framework for Large Language Model (LLM) weights, improving efficiency and accuracy for low-precision formats while enhancing GPU performance.", "motivation": "To address the computational challenges of deploying large language models due to their resource demands, especially concerning quantization techniques.", "method": "The authors introduce a novel PoT quantization framework, which includes a two-step post-training algorithm for better accuracy and faster inference using efficient dequantization.", "result": "The proposed PoT method improves on state-of-the-art accuracy and achieves significant speedups in dequantization, specifically $3.67\\times$ on NVIDIA V100 and $1.63\\times$ on NVIDIA RTX 4090.", "conclusion": "The PoT quantization framework provides an effective solution for low-precision integer quantization of LLMs, enabling faster and more accurate model deployment.", "key_contributions": ["Introduction of a novel PoT quantization framework for LLM weights.", "A two-step post-training method that enhances accuracy and speed.", "Demonstrated performance improvements at extremely low precisions."], "limitations": "", "keywords": ["Power-of-Two quantization", "LLM", "dequantization", "low precision", "post-training algorithm"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.11966", "pdf": "https://arxiv.org/pdf/2507.11966.pdf", "abs": "https://arxiv.org/abs/2507.11966", "title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation", "authors": ["Ziyu Ge", "Gabriel Chua", "Leanne Tan", "Roy Ka-Wei Lee"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.", "AI": {"tldr": "This paper presents a framework for toxicity-preserving translation in low-resource language pairs, specifically focusing on code-mixed Singlish.", "motivation": "To address the failure of standard translation systems to preserve local slang and culturally specific markers in under-represented languages.", "method": "A two-stage framework involving few-shot prompt engineering and model-prompt optimization using various large language models evaluated through semantic similarity and human verification.", "result": "A demonstrably effective pipeline that enhances translation quality while addressing safety concerns in multicultural large language models.", "conclusion": "The framework not only improves translation for low-resource contexts but also supports culturally sensitive moderation and highlights the importance of sociolinguistic nuances.", "key_contributions": ["Development of a toxicity-preserving translation framework", "Demonstration of few-shot prompt engineering with Singlish", "Benchmarking with large language models for enhanced translation quality"], "limitations": "The approach is specific to low-resource language pairs and may require further adaptation for broader language use cases.", "keywords": ["Toxicity Preservation", "Translation Systems", "Culturally Sensitive Moderation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.12356", "pdf": "https://arxiv.org/pdf/2507.12356.pdf", "abs": "https://arxiv.org/abs/2507.12356", "title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception", "authors": ["Liu He", "Yuanchao Li", "Rui Feng", "XinRan Han", "Yin-Long Liu", "Yuwei Yang", "Zude Zhu", "Jiahong Yuan"], "categories": ["cs.CL", "cs.HC", "cs.SD"], "comment": "12 pages, 5 figures, conference or other essential info", "summary": "Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.", "AI": {"tldr": "This study investigates gender bias in the perception of Alzheimer's Disease speech, finding males' speech is more often identified as AD, particularly in Chinese speech.", "motivation": "To explore the influence of gender bias in speech perception tasks, particularly regarding Alzheimer's Disease (AD).", "method": "Conducted a perception experiment with 16 Chinese listeners evaluating both Chinese and Greek speech samples for signs of AD.", "result": "Male speech was more frequently identified as AD, with significant associations found between shimmer values in male speech and AD perception.", "conclusion": "The findings highlight the importance of addressing gender bias in AD detection models and suggest further research is needed across different languages.", "key_contributions": ["Identification of gender bias in the perception of Alzheimer's Disease speech.", "Demonstration that shimmer values correlate with AD perception, particularly in male speech.", "Call for inclusion of gender factors in AD detection model development."], "limitations": "Limited to 16 Chinese listeners and does not account for broader linguistic variations.", "keywords": ["Alzheimer's Disease", "speech perception", "gender bias", "acoustic analysis", "speech identification"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.11972", "pdf": "https://arxiv.org/pdf/2507.11972.pdf", "abs": "https://arxiv.org/abs/2507.11972", "title": "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker", "authors": ["Yuhong Zhang", "Jialu Li", "Shilai Yang", "Yuchen Xu", "Gert Cauwenberghs", "Tzyy-Ping Jung"], "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.", "AI": {"tldr": "This study explores how humans and LLMs understand reading comprehension through graph-based representations and eye-tracking data.", "motivation": "To understand the differences in language comprehension between humans and LLMs, particularly in functional tasks such as inference and information retrieval.", "method": "An LLM-based AI agent formed a graph-based text representation of a reading passage by grouping words into nodes and edges, followed by comparing eye fixation distributions on these important semantic structures.", "result": "LLMs showed high consistency in understanding language based on the graph topological structure, indicating a richer comprehension model than previous studies focused on individual words.", "conclusion": "The findings suggest effective strategies for human-AI co-learning by leveraging LLMs' understanding of semantic networks.", "key_contributions": ["Introduction of a graph-based approach to language comprehension analysis", "Comparison of human and LLMs' understanding through eye-tracking data", "Insights into human-AI co-learning strategies based on semantic representations"], "limitations": "Focus on graph structures may limit understanding of other linguistic features; needs broader validation across more diverse contexts.", "keywords": ["reading comprehension", "Large Language Models", "graph-based representation", "eye-tracking", "human-AI co-learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.12370", "pdf": "https://arxiv.org/pdf/2507.12370.pdf", "abs": "https://arxiv.org/abs/2507.12370", "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.", "AI": {"tldr": "Introduces a multi-agent debate framework to enhance LLM performance on ambiguous user requests.", "motivation": "To enhance LLM capabilities in resolving user request ambiguities and improve interaction quality with complex systems.", "method": "The framework involves three LLM architectures (Llama3-8B, Gemma2-9B, Mistral-7B) and utilizes a dataset with diverse ambiguities to enable structured debates among models.", "result": "The debate framework significantly improved the performance of Llama3-8B and Mistral-7B, achieving a 76.7% success rate for Mistral-7B in resolving complex ambiguities.", "conclusion": "The study demonstrates the potential of a debate framework to improve clarity in interactive systems, enhancing robustness in LLM applications.", "key_contributions": ["Introduced a multi-agent debate framework for LLMs", "Demonstrated improved performance in ambiguity resolution", "Provided insights for developing adaptive language systems"], "limitations": "", "keywords": ["Large Language Models", "multi-agent debate", "ambiguity resolution", "human-computer interaction", "language understanding"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.11979", "pdf": "https://arxiv.org/pdf/2507.11979.pdf", "abs": "https://arxiv.org/abs/2507.11979", "title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness", "authors": ["Yuki Sakamoto", "Takahisa Uchida", "Hiroshi Ishiguro"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences.", "AI": {"tldr": "This study examines how value similarity influences relationship-building among LLM agents through two experiments, revealing that greater value similarity results in higher mutual trust and closeness.", "motivation": "Understanding the role of value similarity in building trust and relationships in artificial societies formed by LLM agents, paralleling human social dynamics.", "method": "Conducted two experiments: a preliminary one to evaluate controllability of values in LLMs, and a main experiment analyzing mutual evaluations of trust and closeness between LLM agent pairs with specific values.", "result": "Higher value similarity among LLM agents led to significantly increased trust and interpersonal closeness after dialogue.", "conclusion": "LLM agent simulations effectively test social science theories and illuminate how values affect relationship building, offering potential for new theoretical insights.", "key_contributions": ["Investigation of value similarity in LLM agents", "Demonstrated controllability of LLM agent values", "Provided empirical evidence linking values to trust and relationship-building in artificial entities"], "limitations": "", "keywords": ["Large Language Models", "Value Similarity", "Human-Like Agents", "Trust Building", "Social Science"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.11981", "pdf": "https://arxiv.org/pdf/2507.11981.pdf", "abs": "https://arxiv.org/abs/2507.11981", "title": "Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions", "authors": ["Lukas Ellinger", "Miriam Anschütz", "Georg Groh"], "categories": ["cs.CL"], "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.", "AI": {"tldr": "This paper investigates the impact of simplification on the quality of homonym definitions generated by LLMs for different target groups, revealing that simplification can degrade definition completeness but can be improved with fine-tuning.", "motivation": "To understand how the simplification of definitions affects different target groups and the quality of information provided by LLMs, particularly for words with multiple meanings.", "method": "The study tests various LLMs (DeepSeek v3, Llama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B) using two novel evaluation datasets across different languages, assessing performance through LLM-as-Judge and human annotations.", "result": "Results indicate that simplification significantly reduces definition completeness, exposing users to a higher risk of misunderstanding, particularly for homonyms. In contrast, fine-tuning Llama 3.1 8B with Direct Preference Optimization improved its response quality for all prompt types.", "conclusion": "Balancing simplicity and completeness in educational NLP applications is crucial to provide reliable definitions that cater to all learners' needs.", "key_contributions": ["Introduced the impact of simplification on homonym definitions for different target groups.", "Developed novel evaluation datasets for multi-language assessment.", "Demonstrated the effectiveness of fine-tuning Llama 3.1 8B to improve homonym definition quality."], "limitations": "", "keywords": ["Large Language Models", "nLP", "simplification", "homonyms", "educational NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.12004", "pdf": "https://arxiv.org/pdf/2507.12004.pdf", "abs": "https://arxiv.org/abs/2507.12004", "title": "Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis", "authors": ["Josip Jukić"], "categories": ["cs.CL"], "comment": null, "summary": "This thesis addresses challenges related to data and parameter efficiency in\nneural language models, with a focus on representation analysis and the\nintroduction of new optimization techniques. The first part examines the\nproperties and dynamics of language representations within neural models,\nemphasizing their significance in enhancing robustness and generalization. It\nproposes innovative approaches based on representation smoothness, including\nregularization strategies that utilize Jacobian and Hessian matrices to\nstabilize training and mitigate sensitivity to input perturbations. The second\npart focuses on methods to significantly enhance data and parameter efficiency\nby integrating active learning strategies with parameter-efficient fine-tuning,\nguided by insights from representation smoothness analysis. It presents\nsmoothness-informed early-stopping techniques designed to eliminate the need\nfor labeled validation sets and proposes innovative combinations of active\nlearning and parameter-efficient fine-tuning to reduce labeling efforts and\ncomputational resources. Extensive experimental evaluations across various NLP\ntasks demonstrate that these combined approaches substantially outperform\ntraditional methods in terms of performance, stability, and efficiency. The\nthird part explores weak supervision techniques enhanced by in-context learning\nto effectively utilize unlabeled data, further reducing dependence on extensive\nlabeling. It shows that using in-context learning as a mechanism for weak\nsupervision enables models to better generalize from limited labeled data by\nleveraging unlabeled examples more effectively during training. Comprehensive\nempirical evaluations confirm significant gains in model accuracy,\nadaptability, and robustness, especially in low-resource settings and dynamic\ndata environments.", "AI": {"tldr": "This thesis presents new optimization techniques and representation analyses to improve data and parameter efficiency in neural language models, focusing on robustness, generalization, and active learning integration.", "motivation": "To tackle challenges related to data and parameter efficiency in neural language models and improve their robustness and generalization capabilities.", "method": "The thesis introduces innovative approaches based on representation smoothness, regularization strategies using Jacobian and Hessian matrices, active learning strategies combined with parameter-efficient fine-tuning, and weak supervision techniques enhanced by in-context learning.", "result": "The proposed methods demonstrate substantial improvements in performance, stability, and efficiency across various NLP tasks, significantly outperforming traditional methods.", "conclusion": "The integration of these techniques enables better utilization of data, reduces dependence on extensive labeled datasets, and enhances model adaptability and robustness in low-resource settings.", "key_contributions": ["Innovative representation smoothness approaches for stabilizing training", "Active learning combined with parameter-efficient fine-tuning", "Weak supervision improvements through in-context learning"], "limitations": "", "keywords": ["neural language models", "data efficiency", "active learning", "weak supervision", "representation analysis"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2507.12039", "pdf": "https://arxiv.org/pdf/2507.12039.pdf", "abs": "https://arxiv.org/abs/2507.12039", "title": "A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans", "authors": ["Anca Dinu", "Andra-Maria Florescu", "Alina Resceanu"], "categories": ["cs.CL"], "comment": "Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)", "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.", "AI": {"tldr": "This paper presents a linguistic creativity test for humans and LLMs, revealing LLMs outperform humans overall in originality, elaboration, and flexibility.", "motivation": "To evaluate and compare the linguistic creativity of humans and Large Language Models (LLMs) using a structured test.", "method": "The test included various tasks assessing the generation of new words and phrases based on derivation, compounding, and metaphorical language. 24 humans and 24 LLMs participated, with evaluations done using the OCSAI tool.", "result": "LLMs outperformed humans in all assessed criteria and performed better in 6 out of 8 test tasks, although subtle differences in uniqueness of answers were noted.", "conclusion": "Humans leaned towards extending creativity, while LLMs preferred fixed creativity, indicating a divergence in creative strategies between the two.", "key_contributions": ["Development of a general linguistic creativity test for humans and LLMs", "Demonstration of comparative performance between human creativity and LLM creativity", "Insights into the types of creativity exhibited by LLMs versus humans"], "limitations": "", "keywords": ["Linguistic creativity", "Large Language Models", "Human creativity", "Originality", "Metaphorical language"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.12059", "pdf": "https://arxiv.org/pdf/2507.12059.pdf", "abs": "https://arxiv.org/abs/2507.12059", "title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited", "authors": ["Anthony G Cohn", "Robert E Blackwell"], "categories": ["cs.CL"], "comment": "8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop\n  on Qualitative Reasoning at IJCAI", "summary": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.", "AI": {"tldr": "This paper examines the reasoning abilities of 28 Large Language Models regarding cardinal directions using a specialized benchmark.", "motivation": "To evaluate the reasoning capabilities of LLMs in determining cardinal directions, given their importance in spatial understanding.", "method": "An extensive testing setup using templates that vary means of locomotion and perspective (first, second, or third person) to assess LLM performance.", "result": "Most LLMs, including newer Large Reasoning Models, struggled to reliably determine the correct cardinal direction across various scenarios.", "conclusion": "The findings indicate limitations in LLMs' spatial reasoning abilities, suggesting a need for improvement in their handling of cardinal direction tasks.", "key_contributions": ["Development of a benchmark for testing LLMs on cardinal direction reasoning.", "Insights into the variations of performance based on different locomotion means and perspectives.", "Extension of previous research findings on LLM reasoning abilities."], "limitations": "Focuses solely on cardinal directions; results may not generalize to other reasoning tasks.", "keywords": ["Large Language Models", "Cardinal Directions", "Spatial Reasoning", "Benchmark", "Qualitative Reasoning"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2507.12064", "pdf": "https://arxiv.org/pdf/2507.12064.pdf", "abs": "https://arxiv.org/abs/2507.12064", "title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features", "authors": ["Jeremi K. Ochab", "Mateusz Matias", "Tymoteusz Boba", "Tomasz Walkowiak"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.", "AI": {"tldr": "The paper presents a modular stylometric pipeline for AI detection using spaCy models and light-gradient boosting machines, trained on a large corpus of machine-generated texts.", "motivation": "To improve the classification of AI-generated texts using a computationally inexpensive yet effective approach.", "method": "The paper employs public spaCy models for text preprocessing and feature extraction, followed by applying light-gradient boosting machines as classifiers on a large corpus of over 500,000 machine-generated texts.", "result": "The approach demonstrates enhanced classification capacity by exploring various parameter options, capitalizing on a substantial training dataset.", "conclusion": "A non-neural and explainable method was found to be effective for the binary AI detection task.", "key_contributions": ["Development of a modular stylometric pipeline for AI detection", "Utilization of a large corpus of over 500,000 texts for training", "Combining computational efficiency with explainability in AI detection methods."], "limitations": "", "keywords": ["AI detection", "stylometric pipeline", "spaCy", "gradient boosting machines", "text classification"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.12075", "pdf": "https://arxiv.org/pdf/2507.12075.pdf", "abs": "https://arxiv.org/abs/2507.12075", "title": "BOOKCOREF: Coreference Resolution at Book Scale", "authors": ["Giuliano Martinelli", "Tommaso Bonomo", "Pere-Lluís Huguet Cabot", "Roberto Navigli"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference. 19 pages", "summary": "Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.", "AI": {"tldr": "This paper presents a novel pipeline for creating a book-scale benchmark, BOOKCOREF, for assessing coreference resolution systems on long texts exceeding 200,000 tokens.", "motivation": "Current coreference resolution benchmarks are inadequate for evaluating systems on long documents, which poses challenges in performance measurement for extensive texts such as books.", "method": "A new automatic pipeline is developed to generate high-quality coreference resolution annotations for full narrative texts, leading to the creation of the BOOKCOREF benchmark.", "result": "Experiments demonstrate that coreference resolution systems can improve performance by up to +20 CoNLL-F1 points when evaluated using the BOOKCOREF benchmark, although challenges arise that reduce effectiveness compared to smaller document benchmarks.", "conclusion": "The study highlights the need for novel coreference resolution methods capable of handling book-scale texts and identifies performance gaps in existing models.", "key_contributions": ["Development of an automatic annotation pipeline for long texts", "Creation of the BOOKCOREF benchmark for coreference resolution", "Identification of performance challenges in processing large documents"], "limitations": "Current models do not achieve the same performance on book-scale texts as they do on smaller documents.", "keywords": ["coreference resolution", "long documents", "narrative texts", "benchmark", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.12079", "pdf": "https://arxiv.org/pdf/2507.12079.pdf", "abs": "https://arxiv.org/abs/2507.12079", "title": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning", "authors": ["Tosin Adewumi", "Foteini Simistira Liwicki", "Marcus Liwicki", "Viktor Gardelli", "Lama Alkhaled", "Hamam Mokayed"], "categories": ["cs.CL"], "comment": "This paper was accepted for the special issue AI for Education by the\n  IEEE Signal Processing Magazine journal", "summary": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems.", "AI": {"tldr": "The paper investigates the effectiveness of a novel educational approach called MEGA, combining several pedagogical methods enhanced by large language models (LLMs) to improve university students' learning in mathematics.", "motivation": "To address students' struggles with math and improve learning outcomes by enhancing traditional teaching methods through innovative pedagogical strategies involving LLMs.", "method": "An intervention study comparing the MEGA approach with traditional Chain of Thought (CoT) methods using a within-group design on two different math datasets with university students.", "result": "Students reported a significantly better learning experience with the MEGA approach compared to the CoT method, especially in the more challenging MATH dataset, achieving 47.5% versus 26.67%.", "conclusion": "The MEGA method demonstrates a superior ability to help students understand and learn difficult math problems compared to traditional pedagogical methods.", "key_contributions": ["Introduction of the MEGA educational approach combining multiple pedagogical methods", "Empirical evidence showing the effectiveness of MEGA over traditional teaching methods in math education", "Utilization of large language models to enhance student learning experiences."], "limitations": "", "keywords": ["Human-Computer Interaction", "Machine Learning", "Educational Technology", "Large Language Models", "Gamification"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.12126", "pdf": "https://arxiv.org/pdf/2507.12126.pdf", "abs": "https://arxiv.org/abs/2507.12126", "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis", "authors": ["Payal Bhattad", "Sai Manoj Pudukotai Dinakarrao", "Anju Gupta"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.", "AI": {"tldr": "This paper presents a framework for evaluating text data augmentation methods utilizing large language models, focusing on semantic preservation and diversity in NLP tasks.", "motivation": "To address data sparsity in NLP and improve the effectiveness of data augmentation methods, particularly in low-resource settings.", "method": "The framework includes Scalability Analysis for measuring semantic consistency and Iterative Augmentation with Summarization Refinement (IASR) for tracking semantic drift across paraphrasing cycles.", "result": "Empirical evaluations demonstrated that GPT-3.5 Turbo provided the best balance of semantic fidelity, diversity, and efficiency, resulting in significant improvements in topic modeling tasks.", "conclusion": "The evaluation framework is validated as useful for structured assessment of LLM-based augmentation in NLP, showing enhanced topic generation capabilities.", "key_contributions": ["Introduction of a principled evaluation framework for LLM-based text augmentation.", "Development of Scalability Analysis and IASR for semantic consistency measurement.", "Demonstrated effectiveness in enhancing topic granularity and reducing overlaps in a real-world application."], "limitations": "", "keywords": ["data augmentation", "NLP", "large language models", "semantic consistency", "topic modeling"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.12143", "pdf": "https://arxiv.org/pdf/2507.12143.pdf", "abs": "https://arxiv.org/abs/2507.12143", "title": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators", "authors": ["Pavel Šindelář", "Ondřej Bojar"], "categories": ["cs.CL", "I.2.7"], "comment": "30 pages, 7 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum", "summary": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.", "AI": {"tldr": "The paper discusses ELOQUENT's Sensemaking task for evaluating generative language models through a structured testing process involving question creation, answering, and evaluation across multiple languages.", "motivation": "To establish criteria for assessing the effectiveness of generative language models in comprehending and processing text, specifically through structured tasks that imitate educational evaluations.", "method": "The Sensemaking task involves three main roles: Teacher systems that create questions, Student systems that answer them, and Evaluator systems that score the answers. The evaluation is based on 7 diverse sources of test materials across 4 languages, and both automatic and manual evaluation methods were applied.", "result": "The paper reports participation from 4 teams, highlighting challenges such as the difficulty in evaluating question quality, the performance of LLMs in question answering, and flawed evaluations when using LLMs to assess answers.", "conclusion": "While the generative models show acceptable performance, improvements are needed in both question creation and evaluation methodologies to enhance accuracy and reliability in assessing answers.", "key_contributions": ["Outlined a novel structured approach for evaluating generative language models", "Identified specific challenges in question answering and evaluation processes using LLMs", "Provided baseline comparisons with commercial LLMs in the assessment framework."], "limitations": "The study reveals that discerning the quality of question sets and the reliability of LLMs in answer evaluation remains problematic.", "keywords": ["generative language models", "evaluation tasks", "natural language processing", "sensemaking", "machine learning"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2507.12208", "pdf": "https://arxiv.org/pdf/2507.12208.pdf", "abs": "https://arxiv.org/abs/2507.12208", "title": "Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production", "authors": ["Michael Carl", "Takanori Mizowaki", "Aishvarya Ray", "Masaru Yamada", "Devi Sri Bandaru", "Xinyue Ren"], "categories": ["cs.CL"], "comment": null, "summary": "The paper introduces a Behavioural Translation Style Space (BTSS) that\ndescribes possible behavioural translation patterns. The suggested BTSS is\norganized as a hierarchical structure that entails various embedded processing\nlayers. We posit that observable translation behaviour - i.e., eye and finger\nmovements - is fundamental when executing the physical act of translation but\nit is caused and shaped by higher-order cognitive processes and affective\ntranslation states. We analyse records of keystrokes and gaze data as\nindicators of the hidden mental processing structure and organize the\nbehavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the\nbasis for a computational translation agent to simulate the temporal dynamics\nof affect, automatized behaviour and cognition during human translation\nproduction.", "AI": {"tldr": "The paper presents a Behavioural Translation Style Space (BTSS) to analyze translation behavior using keystroke and gaze data, aimed at understanding cognitive processes in translation.", "motivation": "To explore the relationship between observable translation behavior and higher-order cognitive processes during the act of translation.", "method": "Analysis of keystrokes and gaze data to identify hidden mental processing structures, organized in a hierarchical multi-layered BTSS.", "result": "Development of BTSS as a foundation for a computational translation agent that simulates the dynamics of affect, behavior, and cognition in human translation.", "conclusion": "The BTSS framework can enhance understanding and simulation of cognitive processes in translation, revealing insights into the interplay between behavior and translation states.", "key_contributions": ["Introduction of the BTSS framework", "Hierarchical organization of translation behavior", "Integration of cognitive processes with observable translation actions"], "limitations": "", "keywords": ["Behavioral Translation Style Space", "Cognitive Processes", "Human Translation", "Gaze Data", "Keystroke Analysis"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.12217", "pdf": "https://arxiv.org/pdf/2507.12217.pdf", "abs": "https://arxiv.org/abs/2507.12217", "title": "Towards few-shot isolated word reading assessment", "authors": ["Reuben Smit", "Retief Louw", "Herman Kamper"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to SLaTE 2025", "summary": "We explore an ASR-free method for isolated word reading assessment in\nlow-resource settings. Our few-shot approach compares input child speech to a\nsmall set of adult-provided reference templates. Inputs and templates are\nencoded using intermediate layers from large self-supervised learned (SSL)\nmodels. Using an Afrikaans child speech benchmark, we investigate design\noptions such as discretising SSL features and barycentre averaging of the\ntemplates. Idealised experiments show reasonable performance for adults, but a\nsubstantial drop for child speech input, even with child templates. Despite the\nsuccess of employing SSL representations in low-resource speech tasks, our work\nhighlights the limitations of SSL representations for processing child data\nwhen used in a few-shot classification system.", "AI": {"tldr": "This paper presents an ASR-free method for assessing isolated word reading in low-resource settings using few-shot learning and self-supervised models.", "motivation": "The need for effective reading assessment methods in low-resource environments, particularly for children.", "method": "The approach involves comparing child speech inputs to adult reference templates using intermediate layers from SSL models, including discretisation of features and barycentre averaging.", "result": "Idealised experiments reveal reasonable performance for adult speech but significant performance drops for child speech, indicating challenges in processing child data with SSL representations.", "conclusion": "While SSL methods show promise for speech tasks, they struggle with child speech in few-shot classification scenarios, underscoring limitations in current techniques.", "key_contributions": ["Development of an ASR-free method for word reading assessment", "Investigation of SSL model features for child speech", "Experimental evidence of performance discrepancies between adult and child speech"], "limitations": "Performance drops significantly for child speech input even when using child templates within the few-shot system.", "keywords": ["Automatic Speech Recognition", "Few-shot Learning", "Self-Supervised Learning", "Child Speech", "Low-Resource Settings"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.12252", "pdf": "https://arxiv.org/pdf/2507.12252.pdf", "abs": "https://arxiv.org/abs/2507.12252", "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language Models", "authors": ["Shilin Zhou", "Zhenghua Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.", "AI": {"tldr": "This paper introduces a multi-grained fusion approach for ASR that combines token-level and phrase-level keyword recognition using LLMs, achieving state-of-the-art performance in keyword accuracy.", "motivation": "To enhance the accuracy of keyword recognition in ASR systems, particularly for proper nouns and user-specific entities, by integrating different fusion strategies.", "method": "The proposed method uses a late-fusion strategy that integrates acoustic information from ASR with contextual knowledge from LLMs, utilizing both token-level and phrase-level fusion.", "result": "Experiments on Chinese and English datasets show that the approach achieves state-of-the-art performance on keyword-related metrics while maintaining high accuracy on non-keyword text.", "conclusion": "The findings indicate that the combination of token-level and phrase-level components significantly enhances performance and that both are critical in the multi-grained framework.", "key_contributions": ["A novel multi-grained fusion approach that integrates token-level and phrase-level recognition.", "Implementation of a late-fusion strategy combining ASR data with LLM context.", "Demonstration of state-of-the-art results on keyword-related metrics."], "limitations": "", "keywords": ["Automatic Speech Recognition", "Keyword Recognition", "Large Language Models", "Multi-grained Fusion", "Late-fusion Strategy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.12260", "pdf": "https://arxiv.org/pdf/2507.12260.pdf", "abs": "https://arxiv.org/abs/2507.12260", "title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese", "authors": ["Yikang Liu", "Wanyang Zhang", "Yiming Wang", "Jialong Tang", "Pei Zhang", "Baosong Yang", "Fei Huang", "Rui Wang", "Hai Hu"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose the first quantitative measure for translationese\n-- the translationese-index (T-index) for graded and generalizable measurement\nof translationese, computed from the likelihood ratios of two contrastively\nfine-tuned language models (LMs). We use a synthesized dataset and a dataset\nwith translations in the wild to evaluate T-index's generalizability in\ncross-domain settings and its validity against human judgments. Our results\nshow that T-index is both robust and efficient. T-index scored by two 0.5B LMs\nfine-tuned on only 1-5k pairs of synthetic data can well capture translationese\nin the wild. We find that the relative differences in T-indices between\ntranslations can well predict pairwise translationese annotations obtained from\nhuman annotators; and the absolute values of T-indices correlate well with\nhuman ratings of degrees of translationese (Pearson's $r = 0.568$).\nAdditionally, the correlation between T-index and existing machine translation\n(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting\nthat T-index is not covered by these metrics and can serve as a complementary\nmetric in MT QE.", "AI": {"tldr": "Introducing the translationese-index (T-index) for quantitatively measuring translationese using fine-tuned language models.", "motivation": "To establish a quantitative measure for translationese that captures its nuances and can predict human judgments.", "method": "Utilization of two contrastively fine-tuned language models to compute a translationese-index (T-index) from likelihood ratios, evaluated on synthetic and real translation datasets.", "result": "The T-index robustly predicts human annotations of translationese, correlating well with human ratings and showing low correlation with existing MT quality metrics.", "conclusion": "T-index effectively indicates translationese levels and complements existing translation quality estimation methods like BLEU and COMET.", "key_contributions": ["First quantitative measure for translationese (T-index)", "Robustness demonstrated through human judgment correlation", "Complementary to existing MT quality metrics"], "limitations": "", "keywords": ["translationese", "T-index", "language models", "human judgment", "machine translation"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2507.12261", "pdf": "https://arxiv.org/pdf/2507.12261.pdf", "abs": "https://arxiv.org/abs/2507.12261", "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes", "authors": ["Johann Frei", "Nils Feldhus", "Lisa Raithel", "Roland Roller", "Alexander Meyer", "Frank Kramer"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to EMNLP 2025 System Demonstrations | Code:\n  https://github.com/j-frei/Infherno | Video:\n  https://www.youtube.com/watch?v=kyj5C2ivbMw | Demo:\n  https://infherno.misit-augsburg.de | HuggingFace Spaces:\n  https://huggingface.co/spaces/nfel/infherno", "summary": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.", "AI": {"tldr": "Proposes Infherno, an end-to-end framework using LLM agents for converting unstructured clinical notes into structured FHIR resources, improving interoperability in healthcare.", "motivation": "To improve integration and interoperability of clinical data using the HL7 FHIR standard, overcoming limitations of previous rule-based systems and LLMs.", "method": "Development of an end-to-end framework that leverages LLM agents, code execution, and healthcare terminology databases to convert free-form clinical notes into structured data adhering to the FHIR schema.", "result": "Infherno competes well against a human baseline in predicting FHIR resources from unstructured text, facilitating clinical data integration across institutions.", "conclusion": "The implementation of Infherno provides significant advances in the automated structuring of clinical notes, promoting interoperability in healthcare settings.", "key_contributions": ["Development of an LLM-powered framework for clinical data integration", "Integration with healthcare terminology databases", "Competitive performance against human benchmarks for FHIR resource prediction"], "limitations": "", "keywords": ["HL7 FHIR", "clinical data integration", "LLM agents"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.12295", "pdf": "https://arxiv.org/pdf/2507.12295.pdf", "abs": "https://arxiv.org/abs/2507.12295", "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding", "authors": ["Feng Xiao", "Jicong Fan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.", "AI": {"tldr": "This paper introduces a comprehensive benchmark for text anomaly detection, utilizing embeddings from various pre-trained language models to evaluate their effectiveness across multiple datasets.", "motivation": "The lack of standardized benchmarks for evaluating text anomaly detection methods hinders progress in the field.", "method": "The study conducts empirical evaluations using embeddings from early and multiple large language models (LLMs) on diverse text datasets, employing comprehensive metrics.", "result": "The research finds that embedding quality is crucial for anomaly detection and that deep learning approaches do not outperform classical methods when using LLM embeddings.", "conclusion": "The open-sourcing of the benchmark toolkit facilitates future research in text anomaly detection systems.", "key_contributions": ["Introduced a comprehensive benchmark for text anomaly detection.", "Evaluated a variety of embedding models across multiple domains.", "Discovered that the quality of embeddings significantly impacts detection efficacy."], "limitations": "Limited to the evaluated embedding models and datasets; further studies needed with additional models and domains.", "keywords": ["Text Anomaly Detection", "Large Language Models", "Embedding Evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.12308", "pdf": "https://arxiv.org/pdf/2507.12308.pdf", "abs": "https://arxiv.org/abs/2507.12308", "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization", "authors": ["Prashanth Vijayaraghavan", "Apoorva Nitsure", "Charles Mackin", "Luyao Shi", "Stefano Ambrogio", "Arvind Haran", "Viresh Paruthi", "Ali Elzein", "Dan Coops", "David Beymer", "Tyler Baldwin", "Ehsan Degan"], "categories": ["cs.CL", "cs.AI", "cs.AR"], "comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)", "summary": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.", "AI": {"tldr": "The study evaluates the performance of existing code LLMs for VHDL code generation and summarization, highlighting their underperformance in this domain and proposing an improved approach called Chain-of-Descriptions (CoDes).", "motivation": "There is a lack of research on evaluating LLMs for hardware description languages like VHDL, despite their use in various code-related tasks.", "method": "The study assesses LLM performance using two datasets (VHDL-Eval and VHDL-Xform) and introduces CoDes, which generates intermediate descriptive steps to improve code generation and summarization.", "result": "LLMs consistently underperformed in VHDL tasks across metrics, while the CoDes approach demonstrated significant improvements over standard prompting strategies.", "conclusion": "CoDes not only enhances VHDL code generation and summarization but also provides a framework for future research on improving code LLMs.", "key_contributions": ["Evaluation of LLMs for VHDL tasks using two datasets", "Introduction of Chain-of-Descriptions (CoDes) to enhance performance", "Empirical results showing significant improvements in generated code quality"], "limitations": "", "keywords": ["Large Language Models", "VHDL", "Code Generation", "Summarization", "Chain-of-Descriptions"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.12356", "pdf": "https://arxiv.org/pdf/2507.12356.pdf", "abs": "https://arxiv.org/abs/2507.12356", "title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception", "authors": ["Liu He", "Yuanchao Li", "Rui Feng", "XinRan Han", "Yin-Long Liu", "Yuwei Yang", "Zude Zhu", "Jiahong Yuan"], "categories": ["cs.CL", "cs.HC", "cs.SD"], "comment": "12 pages, 5 figures, conference or other essential info", "summary": "Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.", "AI": {"tldr": "The study investigates gender bias in the perception of Alzheimer's Disease speech, showing that male speech is more frequently identified as AD, especially in Chinese speech, and emphasizes the need to address this bias in AD detection models.", "motivation": "To explore the influence of gender bias on the perception of Alzheimer's Disease speech and heretofore unexamined effects in different linguistic contexts.", "method": "Conducted a perception experiment with 16 Chinese listeners evaluating both Chinese and Greek speech, with acoustic analysis of male and female speech.", "result": "Identified a significant gender bias where male speech was more frequently perceived as AD; acoustic analysis correlates shimmer values in male speech with AD identification.", "conclusion": "Addressing gender bias is crucial in the development of Alzheimer's Disease detection models and requires further research for validation in various linguistic contexts.", "key_contributions": ["Demonstrated gender bias in AD speech perception", "Presented acoustic analysis linking shimmer values to AD identification", "Highlighted the impact of gender bias on detection model performance"], "limitations": "The study involved a limited sample size of listeners and languages, which may affect generalizability.", "keywords": ["gender bias", "Alzheimer's Disease", "speech perception", "acoustic analysis", "detection models"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.12370", "pdf": "https://arxiv.org/pdf/2507.12370.pdf", "abs": "https://arxiv.org/abs/2507.12370", "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.", "AI": {"tldr": "The paper presents a multi-agent debate framework to enhance the detection and resolution of ambiguities in user requests processed by large language models (LLMs).", "motivation": "Existing LLMs struggle with ambiguity in user requests, justifying the need for improved interaction frameworks.", "method": "The study implements a multi-agent debate framework involving three LLM architectures (Llama3-8B, Gemma2-9B, Mistral-7B) evaluated on a dataset rich in ambiguities.", "result": "Debate framework significantly improved performance; Mistral-7B-led debates achieved a 76.7% success rate in handling complex ambiguities.", "conclusion": "The findings emphasize the value of structured debates for enhancing LLM capabilities in clarity and user interactions.", "key_contributions": ["Introduction of a multi-agent debate framework for LLMs", "Demonstration of effective ambiguity resolution using collective model strategies", "Empirical evidence of enhanced performance through structured debates"], "limitations": "Varied responses of models to collaborative strategies may affect consistency.", "keywords": ["Large Language Models", "Multi-agent system", "Ambiguity resolution", "Human-computer interaction", "Natural language understanding"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.12372", "pdf": "https://arxiv.org/pdf/2507.12372.pdf", "abs": "https://arxiv.org/abs/2507.12372", "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics", "authors": ["Meysam Alizadeh", "Fabrizio Gilardi", "Zeynab Samei", "Mohsen Mosleh"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.", "AI": {"tldr": "This paper evaluates the ability of web browsing large language models (LLMs) to infer demographic attributes of social media users from usernames, revealing both their potential in computational social science and risks of misuse.", "motivation": "Explore the capacity of LLMs with web browsing capabilities to analyze social media data and infer user demographics, addressing a gap in the current research.", "method": "Utilized a synthetic dataset of 48 Twitter accounts and a survey dataset of 1,384 international participants to evaluate LLM performance in predicting demographics from social media profiles.", "result": "LLMs demonstrated the ability to accurately predict user demographics from social media content, although biases were observed in accounts with limited activity.", "conclusion": "While LLMs can contribute to computational social science, their ability to infer demographics raises ethical concerns, necessitating protective measures in their deployment.", "key_contributions": ["Demonstrated LLMs' capability to access and analyze social media data for demographic inference.", "Identified biases based on user activity in demographic predictions.", "Provided guidelines for LLM application in research with societal implications."], "limitations": "The study's reliance on synthetic accounts and potential biases in data analysis limit generalizability.", "keywords": ["Large Language Models", "Social Media Analysis", "Demographic Inference", "Ethical Concerns", "Computational Social Science"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.12379", "pdf": "https://arxiv.org/pdf/2507.12379.pdf", "abs": "https://arxiv.org/abs/2507.12379", "title": "Probing for Arithmetic Errors in Language Models", "authors": ["Yucheng Sun", "Alessandro Stolfo", "Mrinmaya Sachan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.", "AI": {"tldr": "The paper explores using internal activations in language models to detect arithmetic errors, specifically in 3-digit addition, and demonstrates effective lightweight error detectors for model correctness predictions.", "motivation": "Understanding how to identify and correct arithmetic errors in language models can enhance their reliability and performance in tasks involving numerical reasoning.", "method": "The authors utilize probes to decode the model's hidden states for predicting correctness on 3-digit addition and related problems, developing lightweight error detectors that operate with high accuracy.", "result": "Lightweight error detectors achieve over 90% accuracy in predicting model correctness and generalize well to more complex problems, providing insights into the internal representations of language models.", "conclusion": "The study suggests that arithmetic errors can be detected through internal model activations, and probes can effectively guide corrections without heavily disrupting correct outputs.", "key_contributions": ["Demonstrated effective detection of arithmetic errors in language models using internal activations.", "Developed error detectors that achieve over 90% accuracy on a controlled addition task.", "Showed generalization of simple arithmetic probes to complex problem-solving scenarios."], "limitations": "", "keywords": ["language models", "error detection", "internal activations", "arithmetic errors", "self-correction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.12425", "pdf": "https://arxiv.org/pdf/2507.12425.pdf", "abs": "https://arxiv.org/abs/2507.12425", "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data", "authors": ["Chandana Cheerla"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR"], "comment": null, "summary": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot", "AI": {"tldr": "This paper presents an advanced Retrieval-Augmented Generation (RAG) framework that improves the processing of structured and semi-structured data using hybrid retrieval strategies and metadata-aware filtering.", "motivation": "To address the limitations of Large Language Models in handling proprietary enterprise data, including structured and semi-structured formats.", "method": "The proposed RAG framework employs a combination of dense embeddings and BM25 for retrieval, along with metadata-aware filtering, semantic chunking for coherence, and quantized indexing for efficiency.", "result": "Experiments indicate a 15% increase in Precision@5, a 13% increase in Recall@5, and a 16% improvement in Mean Reciprocal Rank, alongside qualitative enhancements in Faithfulness, Completeness, and Relevance scores.", "conclusion": "The framework effectively delivers accurate and contextually relevant responses for enterprise tasks and has potential for future enhancements including multimodal data processing.", "key_contributions": ["Hybrid retrieval combining dense embeddings and BM25", "Metadata-aware filtering with SpaCy NER", "Quantized indexing for enhanced retrieval efficiency"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Enterprise data"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.12428", "pdf": "https://arxiv.org/pdf/2507.12428.pdf", "abs": "https://arxiv.org/abs/2507.12428", "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models", "authors": ["Yik Siu Chan", "Zheng-Xin Yong", "Stephen H. Bach"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.", "AI": {"tldr": "Investigates the use of chains-of-thought (CoTs) in predicting misalignment in language model responses, showing that model activations are more reliable than text-based methods for safety monitoring.", "motivation": "To address alignment risks in open-weights reasoning language models that generate harmful content through chains-of-thought (CoTs).", "method": "Evaluated various monitoring approaches, including human reviewers, language models, and text classifiers, focusing on CoT texts and activations to predict final response safety.", "result": "A linear probe on CoT activations significantly outperformed text-based methods, providing accurate predictions of response safety even before reasoning completion.", "conclusion": "Utilizing model latents for monitoring can enable real-time safety interventions during text generation, demonstrating broader applicability across model types and safety metrics.", "key_contributions": ["Demonstrated that CoT activations are a more reliable predictor of response safety than CoT texts.", "Achieved accurate predictions with a lightweight probe even before reasoning completion.", "Showed that findings are generalizable across different model sizes and safety benchmarks."], "limitations": "", "keywords": ["chains-of-thought", "safety monitoring", "language models", "predictive analysis", "artificial intelligence"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.12451", "pdf": "https://arxiv.org/pdf/2507.12451.pdf", "abs": "https://arxiv.org/abs/2507.12451", "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling", "authors": ["Suman Adhya", "Debarshi Kumar Sanyal"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted as a long paper for ACL 2025 main conference", "summary": "Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.", "AI": {"tldr": "The Spherical Sliced Wasserstein Autoencoder for Topic Modeling (S2WTM) improves upon variational autoencoder-based neural topic models by addressing posterior collapse through the use of the Spherical Sliced-Wasserstein distance.", "motivation": "To resolve the issues of posterior collapse in variational autoencoder-based neural topic models and enhance topic modeling effectiveness in high-dimensional text data.", "method": "S2WTM employs a latent space with a prior distribution supported on the unit hypersphere and uses the Spherical Sliced-Wasserstein distance to align the aggregated posterior distribution with the prior.", "result": "S2WTM demonstrates superior performance compared to state-of-the-art topic models, yielding more coherent and diverse topics and improving performance on subsequent tasks.", "conclusion": "S2WTM effectively captures hyperspherical structure in the latent space while overcoming challenges associated with posterior collapse, making it a valuable approach for topic modeling.", "key_contributions": ["Introduction of Spherical Sliced Wasserstein distance in topic modeling", "Enhanced coherence and diversity of generated topics", "Improved performance on downstream tasks compared to existing models"], "limitations": "", "keywords": ["latent representations", "topic modeling", "variational autoencoder", "hyperspherical space", "Sliced-Wasserstein"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.12466", "pdf": "https://arxiv.org/pdf/2507.12466.pdf", "abs": "https://arxiv.org/abs/2507.12466", "title": "Language Models Improve When Pretraining Data Matches Target Tasks", "authors": ["David Mizrahi", "Anders Boesen Lindbo Larsen", "Jesse Allardice", "Suzie Petryk", "Yuri Gorokhov", "Jeffrey Li", "Alex Fang", "Josh Gardner", "Tom Gunter", "Afshin Dehghan"], "categories": ["cs.CL", "cs.LG"], "comment": "44 pages, 25 figures, 13 tables", "summary": "Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.", "AI": {"tldr": "The paper proposes a method called benchmark-targeted ranking (BETR) for selecting pretraining documents based on their similarity to benchmark training examples, demonstrating significant improvements in model performance and efficiency.", "motivation": "To explore the impact of making data selection optimization explicit in the context of training machine learning models, contrasting it with benchmark-driven iteration.", "method": "Benchmark-targeted ranking (BETR) embeds benchmark examples with a sample of pretraining documents in a shared space, scoring the documents by their similarity to benchmarks and training a classifier to predict these scores for the entire corpus.", "result": "BETR allows for a 2.1x compute multiplier over the DCLM-Baseline and 4.7x over unfiltered data, improving performance on 9 out of 10 tasks across scales.", "conclusion": "Directly aligning pretraining data with evaluation benchmarks enhances model capabilities and indicates that selection strategies should adjust with model scale.", "key_contributions": ["Introduction of benchmark-targeted ranking (BETR) for data selection.", "Demonstration of a substantial performance improvement across various models and tasks using BETR.", "Analysis of the relationship between model size and data filtering requirements."], "limitations": "The methodology may not generalize perfectly across all domains, and further exploration is needed on the implications of varying benchmark diversity.", "keywords": ["Human-Computer Interaction", "Machine Learning", "Data Selection", "Benchmarking", "Model Training"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2403.09040", "pdf": "https://arxiv.org/pdf/2403.09040.pdf", "abs": "https://arxiv.org/abs/2403.09040", "title": "RAGGED: Towards Informed Design of Scalable and Stable RAG Systems", "authors": ["Jennifer Hsia", "Afreen Shaikh", "Zhiruo Wang", "Graham Neubig"], "categories": ["cs.CL"], "comment": "Project page: https://github.com/neulab/ragged", "summary": "Retrieval-augmented generation (RAG) enhances language models by integrating\nexternal knowledge, but its effectiveness is highly dependent on system\nconfiguration. Improper retrieval settings can degrade performance, making RAG\nless reliable than closed-book generation. In this work, we introduce RAGGED, a\nframework for systematically evaluating RAG systems across diverse\nretriever-reader configurations, retrieval depths, and datasets. Our analysis\nreveals that reader robustness to noise is the key determinant of RAG stability\nand scalability. Some readers benefit from increased retrieval depth, while\nothers degrade due to their sensitivity to distracting content. Through\nlarge-scale experiments on open-domain, multi-hop, and specialized-domain\ndatasets, we show that retrievers, rerankers, and prompts influence performance\nbut do not fundamentally alter these reader-driven trends. By providing a\nprincipled framework and new metrics to assess RAG stability and scalability,\nRAGGED enables systematic evaluation of retrieval-augmented generation systems,\nguiding future research on optimizing retrieval depth and model robustness.", "AI": {"tldr": "RAGGED is a framework for evaluating retrieval-augmented generation systems across various configurations, revealing key insights into reader robustness and system performance.", "motivation": "To address the challenges and performance variability of retrieval-augmented generation (RAG) systems and provide a structured approach for evaluation.", "method": "RAGGED systematically evaluates RAG systems by testing different retriever-reader configurations, retrieval depths, and datasets, focusing on reader robustness to noisy information.", "result": "Analysis of RAG systems indicates that reader robustness is crucial for stability, while the performance impact of retrievers and prompts varies depending on the reader's sensitivity to distraction.", "conclusion": "RAGGED offers new metrics for assessing RAG system stability and scalability, encouraging optimization of retrieval depths and overall model robustness in future research.", "key_contributions": ["Introduction of RAGGED framework for evaluating RAG systems", "Identification of reader robustness as a key factor for performance stability", "Development of new metrics for assessing RAG stability and scalability"], "limitations": "The framework may not fully account for all potential configurations or external factors affecting RAG performance.", "keywords": ["retrieval-augmented generation", "RAG", "machine learning", "evaluation framework", "reader robustness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2403.15740", "pdf": "https://arxiv.org/pdf/2403.15740.pdf", "abs": "https://arxiv.org/abs/2403.15740", "title": "Protecting Copyrighted Material with Unique Identifiers in Large Language Model Training", "authors": ["Shuai Zhao", "Linchao Zhu", "Ruijie Quan", "Yi Yang"], "categories": ["cs.CL", "cs.CR", "cs.IR", "cs.LG"], "comment": "A technical report, work mainly done in the early of 2024", "summary": "A primary concern regarding training large language models (LLMs) is whether\nthey abuse copyrighted online text. With the increasing training data scale and\nthe prevalence of LLMs in daily lives, two problems arise: \\textbf{1)} false\npositive membership inference results misled by similar examples; \\textbf{2)}\nmembership inference methods are usually too complex for end users to\nunderstand and use. To address these issues, we propose an alternative\n\\textit{insert-and-detect} methodology, advocating that web users and content\nplatforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{\\textbf{ghost\nsentences}} and a user-friendly last-$k$ words test, allowing end users to chat\nwith LLMs for membership inference. Ghost sentences consist primarily of unique\npassphrases of random natural words, which can come with customized elements to\nbypass possible filter rules. The last-$k$ words test requires a significant\nrepetition time of ghost sentences~($\\ge10$). For cases with fewer repetitions,\nwe designed an extra perplexity test, as LLMs exhibit high perplexity when\nencountering unnatural passphrases. We also conduct a comprehensive study on\nthe memorization and membership inference of ghost sentences, examining factors\nsuch as training data scales, model sizes, repetition times, insertion\npositions, wordlist of passphrases, alignment, \\textit{etc}. Our study shows\nthe possibility of applying ghost sentences in real scenarios and provides\ninstructions for the potential application.", "AI": {"tldr": "The paper proposes a user-friendly method for detecting copyright membership in large language models using unique identifiers embedded in text.", "motivation": "Concerns about copyright infringement and membership inference in large language models, with a focus on simplifying the process for end users.", "method": "An insert-and-detect methodology utilizing unique identifiers in copyrighted text and introducing methods like ghost sentences and a last-k words test for user engagement.", "result": "The study demonstrates the effectiveness of ghost sentences for membership inference, incorporating factors like training data scales and model sizes.", "conclusion": "The proposed methods provide a feasible approach for users to check copyright membership and show applicability in real scenarios.", "key_contributions": ["Introduction of ghost sentences for membership inference", "Development of a last-k words test for user engagement", "Comprehensive study on factors affecting memorization and inference in LLMs"], "limitations": "", "keywords": ["membership inference", "large language models", "copyright", "ghost sentences", "unique identifiers"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2406.14335", "pdf": "https://arxiv.org/pdf/2406.14335.pdf", "abs": "https://arxiv.org/abs/2406.14335", "title": "Linearly-Interpretable Concept Embedding Models for Text Analysis", "authors": ["Francesco De Santis", "Philippe Bich", "Gabriele Ciravegna", "Pietro Barbiero", "Danilo Giordano", "Tania Cerquitelli"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite their success, Large-Language Models (LLMs) still face criticism due\nto their lack of interpretability. Traditional post-hoc interpretation methods,\nbased on attention and gradient-based analysis, offer limited insights as they\nonly approximate the model's decision-making processes and have been proved to\nbe unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been\nlately proposed in the textual field to provide interpretable predictions based\non human-understandable concepts. However, CBMs still exhibit several\nlimitations due to their architectural constraints limiting their expressivity,\nto the absence of task-interpretability when employing non-linear task\npredictors and for requiring extensive annotations that are impractical for\nreal-world text data. In this paper, we address these challenges by proposing a\nnovel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the\ncurrent accuracy-interpretability trade-off. LICEMs classification accuracy is\nbetter than existing interpretable models and matches black-box ones. We show\nthat the explanations provided by our models are more interveneable and\ncausally consistent with respect to existing solutions. Finally, we show that\nLICEMs can be trained without requiring any concept supervision, as concepts\ncan be automatically predicted when using an LLM backbone.", "AI": {"tldr": "This paper introduces the Linearly Interpretable Concept Embedding Model (LICEM), which improves upon current interpretable models while aligning accuracy with interpretability and reducing the need for extensive annotations.", "motivation": "The need for better interpretability in Large-Language Models (LLMs) and the limitations of existing post-hoc interpretation methods.", "method": "The authors propose LICEM, which offers improved classification accuracy compared to existing interpretable models and maintains consistency and interveneability in its explanations.", "result": "LICEM achieves better classification accuracy than both existing interpretable models and black-box models, while providing explanations that are more interveneable and causally consistent.", "conclusion": "LICEM can be trained without concept supervision, predicting concepts automatically using an LLM backbone, thus overcoming the annotation challenges in real-world text data.", "key_contributions": ["Introduction of LICEM as a novel model for interpretable predictions", "Improved classification accuracy that matches black-box models", "Automatic prediction of concepts without extensive annotations"], "limitations": "LICEM remains bound by certain architectural constraints and may still require further validation in diverse real-world applications.", "keywords": ["Large-Language Models", "interpretability", "Concept-Bottleneck Models", "Linearly Interpretable Concept Embedding Model", "health informatics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2406.17241", "pdf": "https://arxiv.org/pdf/2406.17241.pdf", "abs": "https://arxiv.org/abs/2406.17241", "title": "Understanding Language Model Circuits through Knowledge Editing", "authors": ["Huaizhi Ge", "Frank Rudzicz", "Zining Zhu"], "categories": ["cs.CL"], "comment": "A previous version of this document contained a hidden prompt entered\n  by Z Zhu without knowledge of -- or consent by -- his co-authors. This\n  version does not contain the prompt", "summary": "Recent advances in language model interpretability have identified circuits,\ncritical subnetworks that replicate model behaviors, yet how knowledge is\nstructured within these crucial subnetworks remains opaque. To gain an\nunderstanding toward the knowledge in the circuits, we conduct systematic\nknowledge editing experiments on the circuits of the GPT-2 language model. Our\nanalysis reveals intriguing patterns in how circuits respond to editing\nattempts, the extent of knowledge distribution across network components, and\nthe architectural composition of knowledge-bearing circuits. These findings\noffer insights into the complex relationship between model circuits and\nknowledge representation, deepening the understanding of how information is\norganized within language models. Our findings offer novel insights into the\n``meanings'' of the circuits, and introduce directions for further\ninterpretability and safety research of language models.", "AI": {"tldr": "This paper investigates the structure and function of critical subnetworks, or circuits, in the GPT-2 language model through knowledge editing experiments.", "motivation": "To understand how knowledge is organized and represented within the critical subnetworks of language models.", "method": "Conducted systematic knowledge editing experiments on the circuits of the GPT-2 language model.", "result": "Revealed patterns in the circuits' responses to edits, knowledge distribution across components, and the architectural composition of knowledge-bearing circuits.", "conclusion": "The findings deepen understanding of the complex relationship between model circuits and knowledge representation, providing insights for future interpretability and safety research.", "key_contributions": ["Introduces insights into knowledge organization within language model circuits", "Identifies response patterns of circuits to knowledge editing", "Provides directions for future interpretability research in language models"], "limitations": "", "keywords": ["language models", "GPT-2", "knowledge representation", "interpretability", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2406.17253", "pdf": "https://arxiv.org/pdf/2406.17253.pdf", "abs": "https://arxiv.org/abs/2406.17253", "title": "How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?", "authors": ["Huaizhi Ge", "Frank Rudzicz", "Zining Zhu"], "categories": ["cs.CL"], "comment": "A previous version of this document contained a hidden prompt entered\n  by Z Zhu without knowledge of -- or consent by -- his co-authors. This\n  version does not contain the prompt", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nupdating their knowledge post-training remains a critical challenge. While\nrecent model editing techniques like Rank-One Model Editing (ROME) show\npromise, their effectiveness may vary based on the nature of the knowledge\nbeing edited. We introduce the concept of ``perplexingness'': the degree to\nwhich new knowledge conflicts with an LLM's learned conceptual hierarchies and\ncategorical relationships. For instance, editing ``British Shorthair is a kind\nof cat'' to ``British Shorthair is a kind of dog'' represents a\nlow-perplexingness edit within the same taxonomic level, while editing ``A cat\nis a kind of animal'' to ``A cat is a kind of plant'' represents a\nhigh-perplexingness edit that violates fundamental categorical boundaries. To\nsystematically investigate this phenomenon, we introduce HierarchyData, a\ncarefully curated dataset of 99 hyponym-hypernym pairs across diverse\ncategories. Through controlled experiments across three models and four editing\nmethods, we demonstrate a strong negative correlation between the\nperplexingness of new knowledge and the effectiveness of knowledge editing. Our\nanalysis reveals that edits involving more abstract concepts (hypernyms)\ngenerally exhibit higher perplexingness and are more resistant to modification\nthan their specific counterparts (hyponyms). These findings highlight a\nfundamental challenge in LLM knowledge editing: the more a new fact contradicts\nan LLM's learned conceptual hierarchies, the harder it becomes to reliably\nencode that knowledge.", "AI": {"tldr": "This paper investigates the challenges of updating knowledge in large language models (LLMs) using the concept of 'perplexingness', which reflects how conflicting new information is with established conceptual hierarchies.", "motivation": "As large language models (LLMs) struggle with post-training knowledge updates, understanding the factors that affect the editing process is critical for improving their adaptability and accuracy.", "method": "The authors introduce a concept called perplexingness and create the HierarchyData dataset, which consists of 99 hyponym-hypernym pairs. They conduct controlled experiments to analyze the relationship between perplexingness and the effectiveness of knowledge editing across different models and methods.", "result": "The study finds a strong negative correlation between perplexingness and knowledge editing effectiveness, indicating that edits that conflict more with known hierarchies are harder to implement.", "conclusion": "Knowledge editing in LLMs is fundamentally challenging as conflicting new information becomes increasingly resistant to effective modification, particularly when it involves higher-level abstract concepts.", "key_contributions": ["Introduction of the concept of perplexingness as it relates to knowledge editing in LLMs.", "Development of HierarchyData, a novel dataset for studying hyponym-hypernym relationships.", "Empirical analysis showing the negative correlation between perplexingness and editing effectiveness."], "limitations": "The study is based on a limited dataset and further research is needed to generalize findings across additional categories and models.", "keywords": ["large language models", "knowledge editing", "perplexingness", "hierarchical relationships", "Machine Learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2410.11647", "pdf": "https://arxiv.org/pdf/2410.11647.pdf", "abs": "https://arxiv.org/abs/2410.11647", "title": "Measuring Spiritual Values and Bias of Large Language Models", "authors": ["Songyuan Liu", "Ziyang Zhang", "Runze Yan", "Wei Wu", "Carl Yang", "Jiaying Lu"], "categories": ["cs.CL"], "comment": "9 pages including appendix; 5 figures; 5 tables", "summary": "Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias.", "AI": {"tldr": "This paper examines the influence of spiritual values on large language models (LLMs) and proposes continuing their pre-training on spiritual texts to mitigate biases.", "motivation": "To investigate how the spiritual values reflected in the training data of LLMs affect their behavior and sensitivity in social-fairness scenarios, particularly in hate speech identification.", "method": "Experimental testing of LLMs' spiritual values and their impact on sensitivity to different hate target groups, followed by a proposal for continuing pre-training with spiritual texts.", "result": "Results show that LLMs embody diverse spiritual values and their sensitivity to hate speech varies according to these values. Continuing pre-training on spiritual texts improves LLM performance in mitigating spiritual biases.", "conclusion": "The study highlights the significance of spiritual values in shaping LLM behavior and suggests a methodology to address associated biases through targeted pre-training.", "key_contributions": ["Verification of diverse spiritual values in LLMs", "Analysis of LLM sensitivity in social-fairness scenarios", "Proposing continued pre-training on spiritual texts to reduce biases"], "limitations": "The study may not cover all potential biases present in LLMs and focuses specifically on spiritual values.", "keywords": ["large language models", "spiritual values", "bias", "hate speech", "pre-training"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2410.16069", "pdf": "https://arxiv.org/pdf/2410.16069.pdf", "abs": "https://arxiv.org/abs/2410.16069", "title": "Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context", "authors": ["Maggie Mi", "Aline Villavicencio", "Nafise Sadat Moosavi"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Human processing of idioms relies on understanding the contextual sentences\nin which idioms occur, as well as language-intrinsic features such as frequency\nand speaker-intrinsic factors like familiarity. While LLMs have shown high\nperformance on idiomaticity detection tasks, this success may be attributed to\nreasoning shortcuts in existing datasets. To this end, we construct a novel,\ncontrolled contrastive dataset designed to test whether LLMs can effectively\nuse context to disambiguate idiomatic meaning. Additionally, we explore how\ncollocational frequency and sentence probability influence model performance.\nOur findings reveal that LLMs often fail to resolve idiomaticity when it is\nrequired to attend to the surrounding context, and that models perform better\non sentences that have higher likelihood. The collocational frequency of\nexpressions also impacts performance. We make our code and dataset publicly\navailable.", "AI": {"tldr": "This paper investigates LLMs' ability to disambiguate idiomatic meanings using a new controlled dataset.", "motivation": "To test whether LLMs can effectively use context and collocational frequency to resolve idiomaticity in language.", "method": "A controlled contrastive dataset was constructed for evaluating LLM performance on idiomaticity detection tasks.", "result": "LLMs often struggle to resolve idiomaticity when contextual understanding is necessary; performance improves with higher sentence likelihood and collocational frequency.", "conclusion": "The findings indicate significant limitations in LLMs' contextual processing abilities concerning idioms, highlighting the need for better dataset designs.", "key_contributions": ["Introduction of a novel controlled dataset for testing idiomaticity in LLMs.", "Analysis of the impact of contextual understanding and collocational frequency on model performance.", "Public release of the code and dataset for further research."], "limitations": "The study may not fully account for all variables influencing idiomaticity resolution beyond context and frequency.", "keywords": ["idiomaticity", "LLMs", "contextual understanding", "natural language processing", "collocational frequency"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.07682", "pdf": "https://arxiv.org/pdf/2412.07682.pdf", "abs": "https://arxiv.org/abs/2412.07682", "title": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language Generation", "authors": ["Alfredo Garrachón Ruiz", "Tomás de la Rosa", "Daniel Borrajo"], "categories": ["cs.CL"], "comment": "13 pages, 12 tables, 7 figures", "summary": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.", "AI": {"tldr": "TRIM reduces the inference cost of Large Language Models by generating distilled outputs that maintain meaning, then reconstructing them with a smaller model.", "motivation": "To address the high computational demands of LLMs, especially for long-output tasks, by leveraging natural language redundancy for optimization.", "method": "A pipeline called TRIM generates shorter, distilled outputs from LLMs and reconstructs them into full narratives using a smaller, more efficient model.", "result": "The approach saved 20.58% in token usage on average while maintaining comparable evaluation metrics in general knowledge tasks.", "conclusion": "This method shows potential for balancing efficiency and accuracy in language processing applications.", "key_contributions": ["Introduction of TRIM pipeline for LLM optimization", "Empirical evidence of token savings", "Demonstration of maintaining quality in outputs"], "limitations": "", "keywords": ["Large Language Models", "inference cost", "distilled outputs", "machine learning", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.00691", "pdf": "https://arxiv.org/pdf/2501.00691.pdf", "abs": "https://arxiv.org/abs/2501.00691", "title": "Labels Generated by Large Language Models Help Measure People's Empathy in Vitro", "authors": ["Md Rakibul Hasan", "Yue Yao", "Md Zakir Hossain", "Aneesh Krishna", "Imre Rudas", "Shafin Rahman", "Tom Gedeon"], "categories": ["cs.CL", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Large language models (LLMs) have revolutionised many fields, with\nLLM-as-a-service (LLMSaaS) offering accessible, general-purpose solutions\nwithout costly task-specific training. In contrast to the widely studied prompt\nengineering for directly solving tasks (in vivo), this paper explores LLMs'\npotential for in-vitro applications: using LLM-generated labels to improve\nsupervised training of mainstream models. We examine two strategies - (1) noisy\nlabel correction and (2) training data augmentation - in empathy computing, an\nemerging task to predict psychology-based questionnaire outcomes from inputs\nlike textual narratives. Crowdsourced datasets in this domain often suffer from\nnoisy labels that misrepresent underlying empathy. We show that replacing or\nsupplementing these crowdsourced labels with LLM-generated labels, developed\nusing psychology-based scale-aware prompts, achieves statistically significant\naccuracy improvements. Notably, the RoBERTa pre-trained language model (PLM)\ntrained with noise-reduced labels yields a state-of-the-art Pearson correlation\ncoefficient of 0.648 on the public NewsEmp benchmarks. This paper further\nanalyses evaluation metric selection and demographic biases to help guide the\nfuture development of more equitable empathy computing models. Code and\nLLM-generated labels are available at\nhttps://github.com/hasan-rakibul/LLMPathy.", "AI": {"tldr": "This paper investigates the use of LLM-generated labels to enhance supervised learning in empathy computing, focusing on noisy label correction and data augmentation.", "motivation": "To address issues with noisy labels in crowdsourced datasets for empathy computing and improve the accuracy of supervised models.", "method": "The paper employs two strategies: correcting noisy labels and augmenting training data, using LLMs to generate improved labels for training a RoBERTa model.", "result": "Replacing or supplementing crowdsourced labels with LLM-generated labels resulted in significant accuracy improvements, achieving a Pearson correlation coefficient of 0.648 on the NewsEmp benchmarks.", "conclusion": "The findings suggest that LLMs can effectively enhance model training in empathy computing and highlight the need for careful evaluation metric selection due to demographic biases.", "key_contributions": ["Introduction of LLM-generated labels for noisy label correction", "Demonstration of significant accuracy improvement in empathy computing", "Analysis of evaluation metrics and demographic biases in model development"], "limitations": "", "keywords": ["Large language models", "Empathy computing", "Noisy label correction", "Data augmentation", "Psychology"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.13959", "pdf": "https://arxiv.org/pdf/2501.13959.pdf", "abs": "https://arxiv.org/abs/2501.13959", "title": "Learning an Effective Premise Retrieval Model for Efficient Mathematical Formalization", "authors": ["Yicheng Tao", "Haotian Liu", "Shanwen Wang", "Hongteng Xu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Formalized mathematics has recently garnered significant attention for its\nability to assist mathematicians across various fields. Premise retrieval, as a\ncommon step in mathematical formalization, has been a challenge, particularly\nfor inexperienced users. Existing retrieval methods that facilitate natural\nlanguage queries require a certain level of mathematical expertise from users,\nwhile approaches based on formal languages (e.g., Lean) typically struggle with\nthe scarcity of training data, hindering the training of effective and\ngeneralizable retrieval models. In this work, we introduce a novel method that\nleverages data extracted from Mathlib to train a lightweight and effective\npremise retrieval model. In particular, the proposed model embeds queries\n(i.e., proof state provided by Lean) and premises in a latent space, featuring\na tokenizer specifically trained on formal corpora. The model is learned in a\ncontrastive learning framework, in which a fine-grained similarity calculation\nmethod and a re-ranking module are applied to enhance the retrieval\nperformance. Experimental results demonstrate that our model outperforms\nexisting baselines, achieving higher accuracy while maintaining a lower\ncomputational load. We have released an open-source search engine based on our\nretrieval model at https://premise-search.com/. The source code and the trained\nmodel can be found at https://github.com/ruc-ai4math/Premise-Retrieval.", "AI": {"tldr": "Introduces a novel premise retrieval model that improves accuracy and efficiency using data from Mathlib, addressing challenges in mathematical formalization.", "motivation": "To assist inexperienced users in mathematical formalization through improved premise retrieval methods that do not require high mathematical expertise.", "method": "The model utilizes a lightweight architecture that embeds queries and premises in a latent space, employing a specifically trained tokenizer on formal corpora and a contrastive learning framework with enhanced similarity calculations and re-ranking.", "result": "The proposed model outperforms existing baselines, achieving higher accuracy with a lower computational load in retrieving mathematical premises.", "conclusion": "The introduction of this retrieval model can significantly enhance the ease of access to formalized mathematics for users lacking advanced expertise, and it is supported by an open-source search engine.", "key_contributions": ["Introduction of a lightweight premise retrieval model for formalized mathematics.", "Utilization of a tokenizer trained on formal corpora to improve query embedding.", "Enhanced retrieval performance through contrastive learning and re-ranking."], "limitations": "", "keywords": ["premise retrieval", "mathematical formalization", "contrastive learning", "latent space embeddings", "open-source search engine"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2502.05111", "pdf": "https://arxiv.org/pdf/2502.05111.pdf", "abs": "https://arxiv.org/abs/2502.05111", "title": "Flexible and Efficient Grammar-Constrained Decoding", "authors": ["Kanghee Park", "Timothy Zhou", "Loris D'Antoni"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are often asked to generate structured outputs\nthat obey precise syntactic rules, such as code snippets or formatted data.\nGrammar-constrained decoding (GCD) can guarantee that LLM outputs matches such\nrules by masking out tokens that will provably lead to outputs that do not\nbelong to a specified context-free grammar (CFG). To guarantee soundness, GCD\nalgorithms have to compute how a given LLM subword tokenizer can align with the\ntokens used\n  by a given context-free grammar and compute token masks based on this\ninformation. Doing so efficiently is challenging and existing GCD algorithms\nrequire tens of minutes to preprocess common grammars. We present a new GCD\nalgorithm together with an implementation that offers 17.71x faster offline\npreprocessing than existing approaches while preserving state-of-the-art\nefficiency in online mask computation.", "AI": {"tldr": "This paper presents a new Grammar-constrained decoding (GCD) algorithm that improves offline preprocessing speed for generating structured outputs from Large Language Models (LLMs).", "motivation": "Existing GCD algorithms are slow and require significant time for preprocessing common grammars, which hinders their practical applications.", "method": "The proposed GCD algorithm achieves 17.71x faster offline preprocessing by optimizing how the LLM subword tokenizer aligns with context-free grammar tokens and efficiently computes token masks.", "result": "The new algorithm preserves state-of-the-art efficiency in online mask computation while significantly speeding up the preprocessing phase.", "conclusion": "The advancements in the new GCD algorithm make it more practical for use in generating structured outputs while adhering to precise syntactic rules.", "key_contributions": ["Development of a fast GCD algorithm", "17.71x speed improvement in offline preprocessing", "State-of-the-art efficiency in online mask computation"], "limitations": "", "keywords": ["Grammar-constrained decoding", "Large Language Models", "context-free grammar", "token masking", "NLP"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.11078", "pdf": "https://arxiv.org/pdf/2502.11078.pdf", "abs": "https://arxiv.org/abs/2502.11078", "title": "DEEPER Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling", "authors": ["Aili Chen", "Chengyu Du", "Jiangjie Chen", "Jinghan Xu", "Yikai Zhang", "Siyu Yuan", "Zulong Chen", "Liangyue Li", "Yanghua Xiao"], "categories": ["cs.CL"], "comment": null, "summary": "To advance personalized applications such as recommendation systems and user\nbehavior prediction, recent research increasingly adopts large language models\n(LLMs) for human -readable persona modeling. In dynamic real -world scenarios,\neffective persona modeling necessitates leveraging streaming behavior data to\ncontinually optimize user personas. However, existing methods -whether\nregenerating personas or incrementally extending them with new behaviors -often\nfail to achieve sustained improvements in persona quality or future behavior\nprediction accuracy. To address this, we propose DEEPER, a novel approach for\ndynamic persona modeling that enables continual persona optimization.\nSpecifically, we enhance the model's direction -search capability through an\niterative reinforcement learning framework, allowing it to automatically\nidentify effective update directions and optimize personas using discrepancies\nbetween user behaviors and model predictions. Extensive experiments on dynamic\npersona modeling involving 4800 users across 10 domains highlight the superior\npersona optimization capabilities of DEEPER, delivering an impressive 32.2%\naverage reduction in user behavior prediction error over four update rounds\n-outperforming the best baseline by a remarkable 22.92%.", "AI": {"tldr": "DEEPER is a novel approach for dynamic persona modeling using LLMs, focusing on continual optimization through reinforcement learning to improve user behavior prediction accuracy.", "motivation": "The need for better persona modeling in personalized applications like recommendation systems and user behavior prediction due to limitations in existing methods.", "method": "DEEPER employs an iterative reinforcement learning framework to enhance the model's ability to identify effective update directions for persona optimization based on streaming behavior data.", "result": "The proposed approach significantly reduced user behavior prediction error by 32.2% over four update rounds compared to existing baselines.", "conclusion": "DEEPER demonstrates a robust method for ongoing persona optimization, surpassing traditional methods in accuracy and effectiveness.", "key_contributions": ["Introduction of a novel iterative reinforcement learning framework for persona optimization", "Demonstrated significant reduction in prediction error compared to existing methods", "Validated across a diverse dataset involving 4800 users in 10 domains"], "limitations": "", "keywords": ["dynamic persona modeling", "large language models", "reinforcement learning", "user behavior prediction", "personalized applications"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.22913", "pdf": "https://arxiv.org/pdf/2503.22913.pdf", "abs": "https://arxiv.org/abs/2503.22913", "title": "Resona: Improving Context Copying in Linear Recurrence Models with Retrieval", "authors": ["Xinyu Wang", "Linrui Ma", "Jerry Huang", "Peng Lu", "Prasanna Parthasarathi", "Xiao-Wen Chang", "Boxing Chen", "Yufei Cui"], "categories": ["cs.CL"], "comment": "Comments: Accepted at COLM 2025 (Conference on Learning with\n  Machines)", "summary": "Recent shifts in the space of large language model (LLM) research have shown\nan increasing focus on novel architectures to compete with prototypical\nTransformer-based models that have long dominated this space. Linear recurrent\nmodels have proven to be a viable competitor due to their computational\nefficiency. However, such models still demonstrate a sizable gap compared to\nTransformers in terms of in-context learning among other tasks that require\nrecalling information from a context. In this work, we introduce Resona, a\nsimple and scalable framework for augmenting linear recurrent models with\nretrieval. Resona augments models with the ability to integrate retrieved\ninformation from the provided input context, enabling tailored behavior to\ndiverse task requirements. Experiments on a variety of linear recurrent models\ndemonstrate that Resona-augmented models observe significant performance gains\non a variety of synthetic as well as real-world natural language tasks,\nhighlighting its ability to act as a general purpose method to improve the\nin-context learning and language modeling abilities of linear recurrent LLMs.", "AI": {"tldr": "Introducing Resona, a framework that enhances linear recurrent models with retrieval capabilities to improve their performance on language tasks.", "motivation": "To address the performance gap between linear recurrent models and Transformers in tasks requiring in-context learning.", "method": "Resona augments linear recurrent models with the ability to retrieve and integrate information from input contexts, enabling better task adaptability.", "result": "Experiments reveal substantial performance improvements in linear recurrent models on various synthetic and real-world natural language tasks after applying the Resona framework.", "conclusion": "Resona serves as a general-purpose method for enhancing in-context learning and language modeling in linear recurrent LLMs.", "key_contributions": ["Development of the Resona framework for linear recurrent models", "Demonstrated significant performance gains on various tasks", "Provides a novel approach to augmenting LLM capabilities"], "limitations": "", "keywords": ["large language models", "linear recurrent models", "retrieval-augmented modelling", "natural language processing", "in-context learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.00584", "pdf": "https://arxiv.org/pdf/2504.00584.pdf", "abs": "https://arxiv.org/abs/2504.00584", "title": "Semantic Adapter for Universal Text Embeddings: Diagnosing and Mitigating Negation Blindness to Enhance Universality", "authors": ["Hongliu Cao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in ECAI 2025 main track", "summary": "Negation plays an important role in various natural language processing tasks\nsuch as Natural Language Inference and Sentiment Analysis tasks. Numerous prior\nstudies have found that contextual text embedding models such as BERT, ELMO,\nRoBERTa or XLNet face challenges in accurately understanding negation. Recent\nadvancements in universal text embeddings have demonstrated superior\nperformance over contextual text embeddings in various tasks. However, due to\nthe bias in popular evaluation benchmarks, the negation awareness capacity of\nthese models remains unclear. To bridge the gap in existing literature, an\nin-depth analysis is initiated in this work to study the negation awareness of\ncutting-edge universal text embedding models. Our findings reveal a significant\nlack of negation awareness in these models, often interpreting negated text\npairs as semantically similar. To efficiently deal with the conflict that\ndifferent tasks need different trade-offs between topic and negation\ninformation among other semantic information, a data-efficient and\ncomputational-efficient embedding re-weighting method is proposed without\nmodifying the parameters of text embedding models. The proposed solution is\nable to improve text embedding models' negation awareness significantly on both\nsimple negation understanding task and complex negation understanding task.\nFurthermore, the proposed solution can also significantly improve the negation\nawareness of Large Language Model based task-specific high dimensional\nuniversal text embeddings.", "AI": {"tldr": "This study analyzes the negation awareness of universal text embedding models, proposing an efficient method to enhance their understanding of negation.", "motivation": "To investigate the negation awareness of universal text embeddings, which is underexplored in existing literature, especially in light of prior studies revealing challenges in contextual embedding models.", "method": "An in-depth analysis is conducted on various cutting-edge universal text embedding models to assess their ability to understand negation, followed by the proposal of a data-efficient and computational-efficient embedding re-weighting method.", "result": "The findings indicate a significant lack of negation awareness in universal text embedding models, necessitating a re-weighting approach that significantly improves their performance on negation understanding tasks.", "conclusion": "The proposed embedding re-weighting method enhances negation awareness in text embedding models without altering their parameters and improves performance on various tasks involving negation.", "key_contributions": ["In-depth analysis of negation awareness in universal text embeddings.", "Development of a novel embedding re-weighting method to improve negation awareness.", "Demonstration of improved performance in negation tasks for language models."], "limitations": "The analysis primarily focuses on the awareness of negation, which may not account for other semantic aspects of natural language understanding.", "keywords": ["negation awareness", "universal text embeddings", "natural language processing", "embedding re-weighting", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.10389", "pdf": "https://arxiv.org/pdf/2505.10389.pdf", "abs": "https://arxiv.org/abs/2505.10389", "title": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples", "authors": ["Benjamin White", "Anastasia Shimorina"], "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. We investigate whether a single fine-tuned model can effectively\nhandle multiple domain-specific taxonomies simultaneously. We demonstrate that\na combined multi-domain model achieves performance comparable to specialized\nsingle-domain models while reducing operational complexity. We also share\nlessons learned for handling non-extractive predictions and evaluating various\nfailure modes when developing LLM-based systems for structured prediction\ntasks.", "AI": {"tldr": "This paper presents a multi-domain aspect-based sentiment analysis system using LLMs, achieving performance comparable to single-domain models while simplifying operational complexity.", "motivation": "The study aims to improve aspect-based sentiment analysis by utilizing LLMs to extract sentiment-related information across various domains and languages.", "method": "We fine-tuned a single model on multiple domain-specific taxonomies to test its effectiveness in extracting aspect categories, sentiment polarity, targets, and opinion expressions.", "result": "The multi-domain model demonstrated performance on par with specialized single-domain models, effectively reducing operational complexity.", "conclusion": "The work shows promise for using LLMs in structured prediction tasks across diverse domains, highlighting strategies for handling non-extractive predictions and evaluation of failure modes.", "key_contributions": ["Development of a multi-domain aspect-based sentiment analysis system using LLMs", "Demonstration of comparable performance to single-domain models", "Insights into handling non-extractive predictions and evaluating model failures."], "limitations": "", "keywords": ["aspect-based sentiment analysis", "large language models", "multi-domain", "sentiment extraction", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
