{"id": "2508.11770", "pdf": "https://arxiv.org/pdf/2508.11770.pdf", "abs": "https://arxiv.org/abs/2508.11770", "title": "FairVizARD: A Visualization System for Assessing Multi-Party Fairness of Ride-Sharing Matching Algorithms", "authors": ["Ashwin Kumar", "Sanket Shah", "Meghna Lowalekar", "Pradeep Varakantham", "Alvitta Ottley", "William Yeoh"], "categories": ["cs.HC"], "comment": null, "summary": "There is growing interest in algorithms that match passengers with drivers in\nride-sharing problems and their fairness for the different parties involved\n(passengers, drivers, and ride-sharing companies). Researchers have proposed\nvarious fairness metrics for matching algorithms, but it is often unclear how\none should balance the various parties' fairness, given that they are often in\nconflict. We present FairVizARD, a visualization-based system that aids users\nin evaluating the fairness of ride-sharing matching algorithms. FairVizARD\npresents the algorithms' results by visualizing relevant spatio-temporal\ninformation using animation and aggregated information in charts. FairVizARD\nalso employs efficient techniques for visualizing a large amount of information\nin a user friendly manner, which makes it suitable for real-world settings. We\nconduct our experiments on a real-world large-scale taxi dataset and, through\nuser studies and an expert interview, we show how users can use FairVizARD not\nonly to evaluate the fairness of matching algorithms but also to expand on\ntheir notions of fairness."}
{"id": "2508.11778", "pdf": "https://arxiv.org/pdf/2508.11778.pdf", "abs": "https://arxiv.org/abs/2508.11778", "title": "XR-First Design for Productivity: A Conceptual Framework for Enabling Efficient Task Switching in XR", "authors": ["Matt Gottsacker", "Yahya Hmaiti", "Mykola Maslych", "Gerd Bruder", "Joseph J. LaViola Jr.", "Gregory F. Welch"], "categories": ["cs.HC"], "comment": "IEEE ISMAR xrWORKS Workshop", "summary": "A core component of completing tasks efficiently in computer-supported\nknowledge work is the ability for users to rapidly switch their focus (and\ninteraction) across different applications using various shortcuts and\ngestures. This feature set has been explored in research, and several modern\nconsumer extended reality (XR) headsets now support loading multiple\napplications windows at once. However, many XR applications that are useful for\nknowledge work involve rich spatial information, which window-based metaphors\ndo not sufficiently represent nor afford appropriate interaction. In modern XR\nheadsets, such immersive applications run as siloed experiences, requiring the\nuser to fully exit one before starting another. We present a vision for\nachieving an XR-first, user-centric paradigm for efficient context switching in\nXR to encourage and guide future research and development of XR context- and\ntask-switching interfaces."}
{"id": "2508.11781", "pdf": "https://arxiv.org/pdf/2508.11781.pdf", "abs": "https://arxiv.org/abs/2508.11781", "title": "Behavioral and Symbolic Fillers as Delay Mitigation for Embodied Conversational Agents in Virtual Reality", "authors": ["Denmar Mojan Gonzales", "Snehanjali Kalamkar", "Sophie JÃ¶rg", "Jens Grubert"], "categories": ["cs.HC"], "comment": "Accepted to IEEE Transactions on Visualization and Computer Graphics", "summary": "When communicating with embodied conversational agents (ECAs) in virtual\nreality, there might be delays in the responses of the agents lasting several\nseconds, for example, due to more extensive computations of the answers when\nlarge language models are used. Such delays might lead to unnatural or\nfrustrating interactions. In this paper, we investigate filler types to\nmitigate these effects and lead to a more positive experience and perception of\nthe agent. In a within-subject study, we asked 24 participants to communicate\nwith ECAs in virtual reality, comparing four strategies displayed during the\ndelays: a multimodal behavioral filler consisting of conversational and\ngestural fillers, a base condition with only idle motions, and two symbolic\nindicators with progress bars, one embedded as a badge on the agent, the other\none external and visualized as a thinking bubble. Our results indicate that the\nbehavioral filler improved perceived response time, three subscales of\npresence, humanlikeness, and naturalness. Participants looked away from the\nface more often when symbolic indicators were displayed, but the visualizations\ndid not lead to a more positive impression of the agent or to increased\npresence. The majority of participants preferred the behavioral fillers, only\n12.5% and 4.2% favored the symbolic embedded and external conditions,\nrespectively."}
{"id": "2508.11788", "pdf": "https://arxiv.org/pdf/2508.11788.pdf", "abs": "https://arxiv.org/abs/2508.11788", "title": "Two Sides to Every Story: Exploring Hybrid Design Teams' Perceptions of Psychological Safety on Slack", "authors": ["Marjan Naghshbandi", "Sharon Ferguson", "Alison Olechowski"], "categories": ["cs.HC"], "comment": "To be published in the Proceedings of the ACM on Human-Computer\n  Interaction, Vol. 9, No. 7", "summary": "While the unique challenges of hybrid work can compromise collaboration and\nteam dynamics, hybrid teams can thrive with well-informed strategies and tools\nthat nurture interpersonal engagements. To inform future supports, we pursue a\nmixed-methods study of hybrid engineering design capstone teams' Psychological\nSafety (PS) (i.e., their climate of interpersonal risk-taking and mutual\nrespect) to understand how the construct manifests in teams engaged in\ninnovation. Using interviews, we study six teams' perceptions of PS indicators\nand how they present differently on Slack (when compared to in-person\ninteractions). We then leverage the interview insights to design Slack-based PS\nindicators. We present five broad facets of PS in hybrid teams, four perceived\ndifferences of PS on Slack compared to in-person, and 15 Slack-based, PS\nindicators--the groundwork for future automated PS measurement on\ninstant-messaging platforms. These insights produce three design implications\nand illustrative design examples for ways instant-messaging platforms can\nsupport Psychologically Safe hybrid teams, and best practices for hybrid teams\nto support interpersonal risk-taking and build mutual respect."}
{"id": "2508.11676", "pdf": "https://arxiv.org/pdf/2508.11676.pdf", "abs": "https://arxiv.org/abs/2508.11676", "title": "Deep Language Geometry: Constructing a Metric Space from LLM Weights", "authors": ["Maksym Shamrai", "Vladyslav Hamolia"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 pages, accepted to RANLP 2025", "summary": "We introduce a novel framework that utilizes the internal weight activations\nof modern Large Language Models (LLMs) to construct a metric space of\nlanguages. Unlike traditional approaches based on hand-crafted linguistic\nfeatures, our method automatically derives high-dimensional vector\nrepresentations by computing weight importance scores via an adapted pruning\nalgorithm. Our approach captures intrinsic language characteristics that\nreflect linguistic phenomena. We validate our approach across diverse datasets\nand multilingual LLMs, covering 106 languages. The results align well with\nestablished linguistic families while also revealing unexpected inter-language\nconnections that may indicate historical contact or language evolution. The\nsource code, computed language latent vectors, and visualization tool are made\npublicly available at https://github.com/mshamrai/deep-language-geometry."}
{"id": "2508.11892", "pdf": "https://arxiv.org/pdf/2508.11892.pdf", "abs": "https://arxiv.org/abs/2508.11892", "title": "RPKT: Learning What You Don't -- Know Recursive Prerequisite Knowledge Tracing in Conversational AI Tutors for Personalized Learning", "authors": ["Jinwen Tang", "Qiming Guo", "Zhicheng Tang"], "categories": ["cs.HC"], "comment": null, "summary": "Educational systems often assume learners can identify their knowledge gaps,\nyet research consistently shows that students struggle to recognize what they\ndon't know they need to learn-the \"unknown unknowns\" problem. This paper\npresents a novel Recursive Prerequisite Knowledge Tracing (RPKT) system that\naddresses this challenge through dynamic prerequisite discovery using large\nlanguage models. Unlike existing adaptive learning systems that rely on\npre-defined knowledge graphs, our approach recursively traces prerequisite\nconcepts in real-time until reaching a learner's actual knowledge boundary. The\nsystem employs LLMs for intelligent prerequisite extraction, implements binary\nassessment interfaces for cognitive load reduction, and provides personalized\nlearning paths based on identified knowledge gaps. Demonstration across\ncomputer science domains shows the system can discover multiple nested levels\nof prerequisite dependencies, identify cross-domain mathematical foundations,\nand generate hierarchical learning sequences without requiring pre-built\ncurricula. Our approach shows great potential for advancing personalized\neducation technology by enabling truly adaptive learning across any academic\ndomain."}
{"id": "2508.11758", "pdf": "https://arxiv.org/pdf/2508.11758.pdf", "abs": "https://arxiv.org/abs/2508.11758", "title": "Can we Evaluate RAGs with Synthetic Data?", "authors": ["Jonas van Elburg", "Peter van der Putten", "Maarten Marx"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for the SynDAiTE workshop at the European Conference on\n  Machine Learning and Principles and Practice of Knowledge Discovery in\n  Databases (ECML-PKDD 2025), September 15, 2025 - Porto, Portugal", "summary": "We investigate whether synthetic question-answer (QA) data generated by large\nlanguage models (LLMs) can serve as an effective proxy for human-labeled\nbenchmarks when such data is unavailable. We assess the reliability of\nsynthetic benchmarks across two experiments: one varying retriever parameters\nwhile keeping the generator fixed, and another varying the generator with fixed\nretriever parameters. Across four datasets, of which two open-domain and two\nproprietary, we find that synthetic benchmarks reliably rank the RAGs varying\nin terms of retriever configuration, aligning well with human-labeled benchmark\nbaselines. However, they fail to produce consistent RAG rankings when comparing\ngenerator architectures. The breakdown possibly arises from a combination of\ntask mismatch between the synthetic and human benchmarks, and stylistic bias\nfavoring certain generators."}
{"id": "2508.12192", "pdf": "https://arxiv.org/pdf/2508.12192.pdf", "abs": "https://arxiv.org/abs/2508.12192", "title": "Playing telephone with generative models: \"verification disability,\" \"compelled reliance,\" and accessibility in data visualization", "authors": ["Frank Elavsky", "Cindy Xiong Bearfield"], "categories": ["cs.HC", "H.5.2; I.2.6; I.3.3; J.4; K.4.2; I.2.10"], "comment": "11 pages, 7 figures, IEEE VIS Accessibility Workshop, position paper", "summary": "This paper is a collaborative piece between two worlds of expertise in the\nfield of data visualization: accessibility and bias. In particular, the rise of\ngenerative models playing a role in accessibility is a worrying trend for data\nvisualization. These models are increasingly used to help author visualizations\nas well as generate descriptions of existing visualizations for people who are\nblind, low vision, or use assistive technologies such as screen readers.\nSighted human-to-human bias has already been established as an area of concern\nfor theory, research, and design in data visualization. But what happens when\nsomeone is unable to verify the model output or adequately interrogate\nalgorithmic bias, such as a context where a blind person asks a model to\ndescribe a chart for them? In such scenarios, trust from the user is not\nearned, rather reliance is compelled by the model-to-human relationship. In\nthis work, we explored the dangers of AI-generated descriptions for\naccessibility, playing a game of telephone between models, observing bias\nproduction in model interpretation, and re-interpretation of a data\nvisualization. We unpack ways that model failure in visualization is especially\nproblematic for users with visual impairments, and suggest directions forward\nfor three distinct readers of this piece: technologists who build\nmodel-assisted interfaces for end users, users with disabilities leveraging\nmodels for their own purposes, and researchers concerned with bias,\naccessibility, or visualization."}
{"id": "2508.11767", "pdf": "https://arxiv.org/pdf/2508.11767.pdf", "abs": "https://arxiv.org/abs/2508.11767", "title": "Limitation Learning: Catching Adverse Dialog with GAIL", "authors": ["Noah Kasmanoff", "Rahul Zalkikar"], "categories": ["cs.CL", "cs.LG"], "comment": "Paper from 2021", "summary": "Imitation learning is a proven method for creating a policy in the absence of\nrewards, by leveraging expert demonstrations. In this work, we apply imitation\nlearning to conversation. In doing so, we recover a policy capable of talking\nto a user given a prompt (input state), and a discriminator capable of\nclassifying between expert and synthetic conversation. While our policy is\neffective, we recover results from our discriminator that indicate the\nlimitations of dialog models. We argue that this technique can be used to\nidentify adverse behavior of arbitrary data models common for dialog oriented\ntasks."}
{"id": "2508.12268", "pdf": "https://arxiv.org/pdf/2508.12268.pdf", "abs": "https://arxiv.org/abs/2508.12268", "title": "iTrace: Click-Based Gaze Visualization on the Apple Vision Pro", "authors": ["Esra Mehmedova", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "categories": ["cs.HC", "cs.CV"], "comment": "Paper submitted to ACM SIGGRAPH Motion, Interaction and Games 2025\n  (MIG 2025)", "summary": "The Apple Vision Pro is equipped with accurate eye-tracking capabilities, yet\nthe privacy restrictions on the device prevent direct access to continuous user\ngaze data. This study introduces iTrace, a novel application that overcomes\nthese limitations through click-based gaze extraction techniques, including\nmanual methods like a pinch gesture, and automatic approaches utilizing dwell\ncontrol or a gaming controller. We developed a system with a client-server\narchitecture that captures the gaze coordinates and transforms them into\ndynamic heatmaps for video and spatial eye tracking. The system can generate\nindividual and averaged heatmaps, enabling analysis of personal and collective\nattention patterns.\n  To demonstrate its effectiveness and evaluate the usability and performance,\na study was conducted with two groups of 10 participants, each testing\ndifferent clicking methods. The 8BitDo controller achieved higher average data\ncollection rates at 14.22 clicks/s compared to 0.45 clicks/s with dwell\ncontrol, enabling significantly denser heatmap visualizations. The resulting\nheatmaps reveal distinct attention patterns, including concentrated focus in\nlecture videos and broader scanning during problem-solving tasks. By allowing\ndynamic attention visualization while maintaining a high gaze precision of 91\n%, iTrace demonstrates strong potential for a wide range of applications in\neducational content engagement, environmental design evaluation, marketing\nanalysis, and clinical cognitive assessment. Despite the current gaze data\nrestrictions on the Apple Vision Pro, we encourage developers to use iTrace\nonly in research settings."}
{"id": "2508.11771", "pdf": "https://arxiv.org/pdf/2508.11771.pdf", "abs": "https://arxiv.org/abs/2508.11771", "title": "Investigating Transcription Normalization in the Faetar ASR Benchmark", "authors": ["Leo Peckham", "Michael Ong", "Naomi Nagy", "Ewan Dunbar"], "categories": ["cs.CL"], "comment": null, "summary": "We examine the role of transcription inconsistencies in the Faetar Automatic\nSpeech Recognition benchmark, a challenging low-resource ASR benchmark. With\nthe help of a small, hand-constructed lexicon, we conclude that find that,\nwhile inconsistencies do exist in the transcriptions, they are not the main\nchallenge in the task. We also demonstrate that bigram word-based language\nmodelling is of no added benefit, but that constraining decoding to a finite\nlexicon can be beneficial. The task remains extremely difficult."}
{"id": "2508.12333", "pdf": "https://arxiv.org/pdf/2508.12333.pdf", "abs": "https://arxiv.org/abs/2508.12333", "title": "Sketchar: Supporting Character Design and Illustration Prototyping Using Generative AI", "authors": ["Long Ling", "Xinyi Chen", "Ruoyu Wen", "Toby Jia-Jun Li", "Ray LC"], "categories": ["cs.HC"], "comment": "Accepted at CHI PLAY 2024", "summary": "Character design in games involves interdisciplinary collaborations,\ntypically between designers who create the narrative content, and illustrators\nwho realize the design vision. However, traditional workflows face challenges\nin communication due to the differing backgrounds of illustrators and\ndesigners, the latter with limited artistic abilities. To overcome these\nchallenges, we created Sketchar, a Generative AI (GenAI) tool that allows\ndesigners to prototype game characters and generate images based on conceptual\ninput, providing visual outcomes that can give immediate feedback and enhance\ncommunication with illustrators' next step in the design cycle. We conducted a\nmixed-method study to evaluate the interaction between game designers and\nSketchar. We showed that the reference images generated in co-creating with\nSketchar fostered refinement of design details and can be incorporated into\nreal-world workflows. Moreover, designers without artistic backgrounds found\nthe Sketchar workflow to be more expressive and worthwhile. This research\ndemonstrates the potential of GenAI in enhancing interdisciplinary\ncollaboration in the game industry, enabling designers to interact beyond their\nown limited expertise."}
{"id": "2508.11779", "pdf": "https://arxiv.org/pdf/2508.11779.pdf", "abs": "https://arxiv.org/abs/2508.11779", "title": "A Multi-Task Evaluation of LLMs' Processing of Academic Text Input", "authors": ["Tianyi Li", "Yu Qin", "Olivia R. Liu Sheng"], "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "How much large language models (LLMs) can aid scientific discovery, notably\nin assisting academic peer review, is in heated debate. Between a literature\ndigest and a human-comparable research assistant lies their practical\napplication potential. We organize individual tasks that computer science\nstudies employ in separate terms into a guided and robust workflow to evaluate\nLLMs' processing of academic text input. We employ four tasks in the\nassessment: content reproduction/comparison/scoring/reflection, each demanding\na specific role of the LLM (oracle/judgmental arbiter/knowledgeable\narbiter/collaborator) in assisting scholarly works, and altogether testing LLMs\nwith questions that increasingly require intellectual capabilities towards a\nsolid understanding of scientific texts to yield desirable solutions. We\nexemplify a rigorous performance evaluation with detailed instructions on the\nprompts. Adopting first-rate Information Systems articles at three top journals\nas the input texts and an abundant set of text metrics, we record a compromised\nperformance of the leading LLM - Google's Gemini: its summary and paraphrase of\nacademic text is acceptably reliable; using it to rank texts through pairwise\ntext comparison is faintly scalable; asking it to grade academic texts is prone\nto poor discrimination; its qualitative reflection on the text is\nself-consistent yet hardly insightful to inspire meaningful research. This\nevidence against an endorsement of LLMs' text-processing capabilities is\nconsistent across metric-based internal (linguistic assessment), external\n(comparing to the ground truth), and human evaluation, and is robust to the\nvariations of the prompt. Overall, we do not recommend an unchecked use of LLMs\nin constructing peer reviews."}
{"id": "2508.12385", "pdf": "https://arxiv.org/pdf/2508.12385.pdf", "abs": "https://arxiv.org/abs/2508.12385", "title": "System-driven Interactive Design Support for Cloud Architecture: A Qualitative User Experience Study with Novice Engineers", "authors": ["Ryosuke Kohita", "Akira Kasuga"], "categories": ["cs.HC", "cs.SE"], "comment": null, "summary": "Cloud architecture design presents significant challenges due to the\nnecessity of clarifying ambiguous requirements and systematically addressing\ncomplex trade-offs, especially for novice engineers with limited cloud\nexperience. While recent advances in the use of AI tools have broadened\navailable options, system-driven approaches that offer explicit guidance and\nstep-by-step information management may be especially effective in supporting\nnovices during the design process. This study qualitatively examines the\nexperiences of 60 novice engineers using such a system-driven cloud design\nsupport tool. The findings indicate that structured and proactive system\nguidance helps novices engage more effectively in architectural design,\nespecially when addressing tasks where knowledge and experience gaps are most\ncritical. For example, participants found it easier to create initial\narchitectures and did not need to craft prompts themselves. In addition,\nparticipants reported that the ability to simulate and compare multiple\narchitecture options enabled them to deepen their understanding of cloud design\nprinciples and trade-offs, demonstrating the educational value of system-driven\nsupport. The study also identifies areas for improvement, including more\nadaptive information delivery tailored to user expertise, mechanisms for\nvalidating system outputs, and better integration with implementation workflows\nsuch as infrastructure-as-code generation and deployment guidance. Addressing\nthese aspects can further enhance the educational and practical value of\nsystem-driven support tools for cloud architecture design."}
{"id": "2508.11816", "pdf": "https://arxiv.org/pdf/2508.11816.pdf", "abs": "https://arxiv.org/abs/2508.11816", "title": "LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText", "authors": ["Krishna Chaitanya Marturi", "Heba H. Elwazzan"], "categories": ["cs.CL"], "comment": "Text Simplification, hallucination detection, LLMs, CLEF 2025,\n  SimpleText, CEUR-WS", "summary": "In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,\nwhich addresses both sentence-level and document-level scientific text\nsimplification. For sentence-level simplification, our methodology employs\nlarge language models (LLMs) to first generate a structured plan, followed by\nplan-driven simplification of individual sentences. At the document level, we\nleverage LLMs to produce concise summaries and subsequently guide the\nsimplification process using these summaries. This two-stage, LLM-based\nframework enables more coherent and contextually faithful simplifications of\nscientific text."}
{"id": "2508.12388", "pdf": "https://arxiv.org/pdf/2508.12388.pdf", "abs": "https://arxiv.org/abs/2508.12388", "title": "When motivation can be more than a message: designing agents to boost physical activity", "authors": ["Alessandro Silacci", "Maurizio Caon", "Mauro Cherubini"], "categories": ["cs.HC"], "comment": "This is a pre-peer-review version of a paper with the same title\n  accepted at 20th IFIP TC13 International Conference on Human-Computer\n  Interaction (INTERACT 2025)", "summary": "Virtual agents are commonly used in physical activity interventions to\nsupport behavior change, often taking the role of coaches that deliver\nencouragement and feedback. While effective for compliance, this role typically\nlacks relational depth. This pilot study explores how such agents might be\nperceived not just as instructors, but as co-participants: entities that appear\nto exert effort alongside users. Drawing on thematic analysis of\nsemi-structured interviews with 12 participants from a prior physical activity\nintervention, we examine how users interpret and evaluate agent effort in\nsocial comparison contexts. Our findings reveal a recurring tension between\nperceived performance and authenticity. Participants valued social features\nwhen they believed others were genuinely trying. In contrast, ambiguous or\nimplausible activity levels undermined trust and motivation. Many participants\nexpressed skepticism toward virtual agents unless their actions reflected\nvisible effort or were grounded in relatable human benchmarks. Based on these\ninsights, we propose early design directions for fostering co-experienced\nexertion in agents, including behavioral cues, narrative grounding, and\npersonalized performance. These insights contribute to the design of more\nengaging, socially resonant agents capable of supporting co-experienced\nphysical activity."}
{"id": "2508.11823", "pdf": "https://arxiv.org/pdf/2508.11823.pdf", "abs": "https://arxiv.org/abs/2508.11823", "title": "Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText", "authors": ["Krishna Chaitanya Marturi", "Heba H. Elwazzan"], "categories": ["cs.CL"], "comment": "Text Simplification, hallucination detection, LLMs, CLEF 2025,\n  SimpleText, CEUR-WS", "summary": "In this paper, we describe our methodology for the CLEF 2025 SimpleText Task\n2, which focuses on detecting and evaluating creative generation and\ninformation distortion in scientific text simplification. Our solution\nintegrates multiple strategies: we construct an ensemble framework that\nleverages BERT-based classifier, semantic similarity measure, natural language\ninference model, and large language model (LLM) reasoning. These diverse\nsignals are combined using meta-classifiers to enhance the robustness of\nspurious and distortion detection. Additionally, for grounded generation, we\nemploy an LLM-based post-editing system that revises simplifications based on\nthe original input texts."}
{"id": "2508.12416", "pdf": "https://arxiv.org/pdf/2508.12416.pdf", "abs": "https://arxiv.org/abs/2508.12416", "title": "fCrit: A Visual Explanation System for Furniture Design Creative Support", "authors": ["Vuong Nguyen", "Gabriel Vigliensoni"], "categories": ["cs.HC", "cs.AI", "H.5.2"], "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "summary": "We introduce fCrit, a dialogue-based AI system designed to critique furniture\ndesign with a focus on explainability. Grounded in reflective learning and\nformal analysis, fCrit employs a multi-agent architecture informed by a\nstructured design knowledge base. We argue that explainability in the arts\nshould not only make AI reasoning transparent but also adapt to the ways users\nthink and talk about their designs. We demonstrate how fCrit supports this\nprocess by tailoring explanations to users' design language and cognitive\nframing. This work contributes to Human-Centered Explainable AI (HCXAI) in\ncreative practice, advancing domain-specific methods for situated, dialogic,\nand visually grounded AI support."}
{"id": "2508.11828", "pdf": "https://arxiv.org/pdf/2508.11828.pdf", "abs": "https://arxiv.org/abs/2508.11828", "title": "A Survey of Idiom Datasets for Psycholinguistic and Computational Research", "authors": ["Michael Flor", "Xinyi Liu", "Anna Feldman"], "categories": ["cs.CL"], "comment": "KONVENS 2025. To appear", "summary": "Idioms are figurative expressions whose meanings often cannot be inferred\nfrom their individual words, making them difficult to process computationally\nand posing challenges for human experimental studies. This survey reviews\ndatasets developed in psycholinguistics and computational linguistics for\nstudying idioms, focusing on their content, form, and intended use.\nPsycholinguistic resources typically contain normed ratings along dimensions\nsuch as familiarity, transparency, and compositionality, while computational\ndatasets support tasks like idiomaticity detection/classification,\nparaphrasing, and cross-lingual modeling. We present trends in annotation\npractices, coverage, and task framing across 53 datasets. Although recent\nefforts expanded language coverage and task diversity, there seems to be no\nrelation yet between psycholinguistic and computational research on idioms."}
{"id": "2508.12498", "pdf": "https://arxiv.org/pdf/2508.12498.pdf", "abs": "https://arxiv.org/abs/2508.12498", "title": "Say It, See It: A Systematic Evaluation on Speech-Based 3D Content Generation Methods in Augmented Reality", "authors": ["Yanming Xiu", "Joshua Chilukuri", "Shunav Sen", "Maria Gorlatova"], "categories": ["cs.HC"], "comment": "Accepted to ISMAR 2025 UNAI Workshop", "summary": "As augmented reality (AR) applications increasingly require 3D content,\ngenerative pipelines driven by natural input such as speech offer an\nalternative to manual asset creation. In this work, we design a modular,\nedge-assisted architecture that supports both direct text-to-3D and\ntext-image-to-3D pathways, enabling interchangeable integration of\nstate-of-the-art components and systematic comparison of their performance in\nAR settings. Using this architecture, we implement and evaluate four\nrepresentative pipelines through an IRB-approved user study with 11\nparticipants, assessing six perceptual and usability metrics across three\nobject prompts. Overall, text-image-to-3D pipelines deliver higher generation\nquality: the best-performing pipeline, which used FLUX for image generation and\nTrellis for 3D generation, achieved an average satisfaction score of 4.55 out\nof 5 and an intent alignment score of 4.82 out of 5. In contrast, direct\ntext-to-3D pipelines excel in speed, with the fastest, Shap-E, completing\ngeneration in about 20 seconds. Our results suggest that perceptual quality has\na greater impact on user satisfaction than latency, with users tolerating\nlonger generation times when output quality aligns with expectations. We\ncomplement subjective ratings with system-level metrics and visual analysis,\nproviding practical insights into the trade-offs of current 3D generation\nmethods for real-world AR deployment."}
{"id": "2508.11829", "pdf": "https://arxiv.org/pdf/2508.11829.pdf", "abs": "https://arxiv.org/abs/2508.11829", "title": "Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions", "authors": ["Leigh Levinson", "Christopher J. Agostino"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "9 pages, 1 figure, submitted to NeurIPS Creative AI track", "summary": "Despite significant advances, AI systems struggle with the frame problem:\ndetermining what information is contextually relevant from an exponentially\nlarge possibility space. We hypothesize that biological rhythms, particularly\nhormonal cycles, serve as natural relevance filters that could address this\nfundamental challenge. We develop a framework that embeds simulated menstrual\nand circadian cycles into Large Language Models through system prompts\ngenerated from periodic functions modeling key hormones including estrogen,\ntestosterone, and cortisol. Across multiple state-of-the-art models, linguistic\nanalysis reveals emotional and stylistic variations that track biological\nphases; sadness peaks during menstruation while happiness dominates ovulation\nand circadian patterns show morning optimism transitioning to nocturnal\nintrospection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates\nsubtle but consistent performance variations aligning with biological\nexpectations, including optimal function in moderate rather than extreme\nhormonal ranges. This methodology provides a novel approach to contextual AI\nwhile revealing how societal biases regarding gender and biology are embedded\nwithin language models."}
{"id": "2508.12504", "pdf": "https://arxiv.org/pdf/2508.12504.pdf", "abs": "https://arxiv.org/abs/2508.12504", "title": "Organization Matters: A Qualitative Study of Organizational Dynamics in Red Teaming Practices For Generative AI", "authors": ["Bixuan Ren", "EunJeong Cheon", "Jianghui Li"], "categories": ["cs.HC"], "comment": null, "summary": "The rapid integration of generative artificial intelligence (GenAI) across\ndiverse fields underscores the critical need for red teaming efforts to\nproactively identify and mitigate associated risks. While previous research\nprimarily addresses technical aspects, this paper highlights organizational\nfactors that hinder the effectiveness of red teaming in real-world settings.\nThrough qualitative analysis of 15 semi-structured interviews with red teamers\nfrom various organizations, we uncover challenges such as the marginalization\nof vulnerable red teamers, the invisibility of nuanced AI risks to vulnerable\nusers until post-deployment, and a lack of user-centered red teaming\napproaches. These issues often arise from underlying organizational dynamics,\nincluding organizational resistance, organizational inertia, and organizational\nmediocracy. To mitigate these dynamics, we discuss the implications of user\nresearch for red teaming and the importance of embedding red teaming throughout\nthe entire development cycle of GenAI systems."}
{"id": "2508.11831", "pdf": "https://arxiv.org/pdf/2508.11831.pdf", "abs": "https://arxiv.org/abs/2508.11831", "title": "When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection", "authors": ["Julia Sammartino", "Libby Barak", "Jing Peng", "Anna Feldman"], "categories": ["cs.CL", "cs.AI"], "comment": "RANLP 2025", "summary": "Euphemisms are culturally variable and often ambiguous, posing challenges for\nlanguage models, especially in low-resource settings. This paper investigates\nhow cross-lingual transfer via sequential fine-tuning affects euphemism\ndetection across five languages: English, Spanish, Chinese, Turkish, and\nYoruba. We compare sequential fine-tuning with monolingual and simultaneous\nfine-tuning using XLM-R and mBERT, analyzing how performance is shaped by\nlanguage pairings, typological features, and pretraining coverage. Results show\nthat sequential fine-tuning with a high-resource L1 improves L2 performance,\nespecially for low-resource languages like Yoruba and Turkish. XLM-R achieves\nlarger gains but is more sensitive to pretraining gaps and catastrophic\nforgetting, while mBERT yields more stable, though lower, results. These\nfindings highlight sequential fine-tuning as a simple yet effective strategy\nfor improving euphemism detection in multilingual models, particularly when\nlow-resource languages are involved."}
{"id": "2508.12518", "pdf": "https://arxiv.org/pdf/2508.12518.pdf", "abs": "https://arxiv.org/abs/2508.12518", "title": "Towards Adaptive External Communication in Autonomous Vehicles: A Conceptual Design Framework", "authors": ["Tram Thi Minh Tran", "Judy Kay", "Stewart Worrall", "Marius Hoggenmueller", "Callum Parker", "Xinyan Yu", "Julie Stephany Berrio Perez", "Mao Shan", "Martin Tomitsch"], "categories": ["cs.HC"], "comment": null, "summary": "External Human-Machine Interfaces (eHMIs) are key to facilitating interaction\nbetween autonomous vehicles and external road actors, yet most remain reactive\nand do not account for scalability and inclusivity. This paper introduces a\nconceptual design framework for adaptive eHMIs-interfaces that dynamically\nadjust communication as road actors vary and context shifts. Using the\ncyber-physical system as a structuring lens, the framework comprises three\nlayers: Input (what the system detects), Processing (how the system decides),\nand Output (how the system communicates). Developed through theory-led\nabstraction and expert discussion, the framework helps researchers and\ndesigners think systematically about adaptive eHMIs and provides a structured\ntool to design, analyse, and assess adaptive communication strategies. We show\nhow such systems may resolve longstanding limitations in eHMI research while\nraising new ethical and technical considerations."}
{"id": "2508.11857", "pdf": "https://arxiv.org/pdf/2508.11857.pdf", "abs": "https://arxiv.org/abs/2508.11857", "title": "SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance", "authors": ["Andrei-Valentin TÄnase", "Elena Pelican"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Tokenization remains a fundamental yet underexplored bottleneck in natural\nlanguage processing, with strategies largely static despite remarkable progress\nin model architectures. We present SupraTok, a novel tokenization architecture\nthat reimagines subword segmentation through three innovations: cross-boundary\npattern learning that discovers multi-word semantic units, entropy-driven data\ncuration that optimizes training corpus quality, and multi-phase curriculum\nlearning for stable convergence. Our approach extends Byte-Pair Encoding by\nlearning \"superword\" tokens, coherent multi-word expressions that preserve\nsemantic unity while maximizing compression efficiency. SupraTok achieves 31%\nimprovement in English tokenization efficiency (5.91 versus 4.51 characters per\ntoken) compared to OpenAI's o200k tokenizer and 30% improvement over Google's\nGemma 3 tokenizer (256k vocabulary), while maintaining competitive performance\nacross 38 languages. When integrated with a GPT-2 scale model (124M parameters)\ntrained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%\nimprovement on HellaSWAG and 9.5% on MMLU benchmarks without architectural\nmodifications. While these results are promising at this scale, further\nvalidation at larger model scales is needed. These findings suggest that\nefficient tokenization can complement architectural innovations as a path to\nimproved language model performance."}
{"id": "2508.12579", "pdf": "https://arxiv.org/pdf/2508.12579.pdf", "abs": "https://arxiv.org/abs/2508.12579", "title": "The Future of Tech Labor: How Workers are Organizing and Transforming the Computing Industry", "authors": ["Cella M. Sum", "Anna Konvicka", "Mona Wang", "Sarah E. Fox"], "categories": ["cs.HC"], "comment": null, "summary": "The tech industry's shifting landscape and the growing precarity of its labor\nforce have spurred unionization efforts among tech workers. These workers turn\nto collective action to improve their working conditions and to protest\nunethical practices within their workplaces. To better understand this\nmovement, we interviewed 44 U.S.-based tech worker-organizers to examine their\nmotivations, strategies, challenges, and future visions for labor organizing.\nThese workers included engineers, product managers, customer support\nspecialists, QA analysts, logistics workers, gig workers, and union staff\norganizers. Our findings reveal that, contrary to popular narratives of\nprestige and privilege within the tech industry, tech workers face fragmented\nand unstable work environments which contribute to their disempowerment and\nhinder their organizing efforts. Despite these difficulties, organizers are\nlaying the groundwork for a more resilient tech worker movement through\ncommunity building and expanding political consciousness. By situating these\ndynamics within broader structural and ideological forces, we identify ways for\nthe CSCW community to build solidarity with tech workers who are materially\ntransforming our field through their organizing efforts."}
{"id": "2508.11889", "pdf": "https://arxiv.org/pdf/2508.11889.pdf", "abs": "https://arxiv.org/abs/2508.11889", "title": "In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning", "authors": ["Hui Ma", "Bo Zhang", "Jinpeng Hu", "Zenglin Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Emotion recognition in conversation (ERC) aims to identify the emotion of\neach utterance in a conversation, playing a vital role in empathetic artificial\nintelligence. With the growing of large language models (LLMs), instruction\ntuning has emerged as a critical paradigm for ERC. Existing studies mainly\nfocus on multi-stage instruction tuning, which first endows LLMs with speaker\ncharacteristics, and then conducts context-aware instruction tuning to\ncomprehend emotional states. However, these methods inherently constrains the\ncapacity to jointly capture the dynamic interaction between speaker\ncharacteristics and conversational context, resulting in weak alignment among\nspeaker identity, contextual cues, and emotion states within a unified\nframework. In this paper, we propose InitERC, a simple yet effective one-stage\nin-context instruction tuning framework for ERC. InitERC adapts LLMs to learn\nspeaker-context-emotion alignment from context examples via in-context\ninstruction tuning. Specifically, InitERC comprises four components, i.e.,\ndemonstration pool construction, in-context example selection, prompt template\ndesign, and in-context instruction tuning. To explore the impact of in-context\nexamples, we conduct a comprehensive study on three key factors: retrieval\nstrategy, example ordering, and the number of examples. Extensive experiments\non three widely used datasets demonstrate that our proposed InitERC achieves\nsubstantial improvements over the state-of-the-art baselines."}
{"id": "2508.13047", "pdf": "https://arxiv.org/pdf/2508.13047.pdf", "abs": "https://arxiv.org/abs/2508.13047", "title": "Using AI for User Representation: An Analysis of 83 Persona Prompts", "authors": ["Joni Salminen", "Danial Amin", "Bernard Jansen"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at AICCSA-2025", "summary": "We analyzed 83 persona prompts from 27 research articles that used large\nlanguage models (LLMs) to generate user personas. Findings show that the\nprompts predominantly generate single personas. Several prompts express a\ndesire for short or concise persona descriptions, which deviates from the\ntradition of creating rich, informative, and rounded persona profiles. Text is\nthe most common format for generated persona attributes, followed by numbers.\nText and numbers are often generated together, and demographic attributes are\nincluded in nearly all generated personas. Researchers use up to 12 prompts in\na single study, though most research uses a small number of prompts. Comparison\nand testing multiple LLMs is rare. More than half of the prompts require the\npersona output in a structured format, such as JSON, and 74% of the prompts\ninsert data or dynamic variables. We discuss the implications of increased use\nof computational personas for user representation."}
{"id": "2508.11915", "pdf": "https://arxiv.org/pdf/2508.11915.pdf", "abs": "https://arxiv.org/abs/2508.11915", "title": "CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures", "authors": ["Punya Syon Pandey", "Yongjin Yang", "Jiarui Liu", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Game-theoretic interactions between agents with Large Language Models (LLMs)\nhave revealed many emergent capabilities, yet the linguistic diversity of these\ninteractions has not been sufficiently quantified. In this paper, we present\nthe Conversational Robustness Evaluation Score: CORE, a metric to quantify the\neffectiveness of language use within multi-agent systems across different\ngame-theoretic interactions. CORE integrates measures of cluster entropy,\nlexical repetition, and semantic similarity, providing a direct lens of dialog\nquality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,\nand neutral settings, further grounding our analysis in Zipf's and Heaps' Laws\nto characterize word frequency distributions and vocabulary growth. Our\nfindings show that cooperative settings exhibit both steeper Zipf distributions\nand higher Heap exponents, indicating more repetition alongside greater\nvocabulary expansion. In contrast, competitive interactions display lower Zipf\nand Heaps exponents, reflecting less repetition and more constrained\nvocabularies. These results provide new insights into how social incentives\ninfluence language adaptation, and highlight CORE as a robust diagnostic for\nmeasuring linguistic robustness in multi-agent LLM systems. Our code is\navailable at https://github.com/psyonp/core."}
{"id": "2508.13074", "pdf": "https://arxiv.org/pdf/2508.13074.pdf", "abs": "https://arxiv.org/abs/2508.13074", "title": "Ashes or Breath: Exploring Moral Dilemmas of Life and Cultural Legacy through Mixed Reality Gaming", "authors": ["Black Sun", "Ge Kacy Fu", "Shichao Guo"], "categories": ["cs.HC"], "comment": null, "summary": "Traditional approaches to teaching moral dilemmas often rely on abstract,\ndisembodied scenarios that limit emotional engagement and reflective depth. To\naddress this gap, we developed \\textit{Ashes or Breath}, a Mixed Reality game\ndelivered via head-mounted displays(MR-HMDs). This places players in an ethical\ncrisis: they must save a living cat or a priceless cultural artifact during a\nmuseum fire. Designed through an iterative, values-centered process, the\nexperience leverages embodied interaction and spatial immersion to heighten\nemotional stakes and provoke ethical reflection. Players face irreversible,\nemotionally charged choices followed by narrative consequences in a reflective\nroom, exploring diverse perspectives and societal implications. Preliminary\nevaluations suggest that embedding moral dilemmas into everyday environments\nvia MR-HMDs intensifies empathy, deepens introspection, and encourages users to\nreconsider their moral assumptions. This work contributes to ethics-based\nexperiential learning in HCI, positioning augmented reality not merely as a\nmedium of interaction but as a stage for ethical encounter."}
{"id": "2508.11927", "pdf": "https://arxiv.org/pdf/2508.11927.pdf", "abs": "https://arxiv.org/abs/2508.11927", "title": "LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese", "authors": ["Jie Lu", "Du Jin", "Hitomi Yanaka"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "Unlike English, which uses distinct forms (e.g., had, has, will have) to mark\nthe perfect aspect across tenses, Chinese and Japanese lack separate\ngrammatical forms for tense within the perfect aspect, which complicates\nNatural Language Inference (NLI). Focusing on the perfect aspect in these\nlanguages, we construct a linguistically motivated, template-based NLI dataset\n(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle\nwith temporal inference, particularly in detecting subtle tense and\nreference-time shifts. These findings highlight model limitations and\nunderscore the need for cross-linguistic evaluation in temporal semantics. Our\ndataset is available at https://github.com/Lujie2001/CrossNLI."}
{"id": "2508.13095", "pdf": "https://arxiv.org/pdf/2508.13095.pdf", "abs": "https://arxiv.org/abs/2508.13095", "title": "At the Speed of the Heart: Evaluating Physiologically-Adaptive Visualizations for Supporting Engagement in Biking Exergaming in Virtual Reality", "authors": ["Oliver Hein", "Sandra Wackerl", "Changkun Ou", "Florian Alt", "Francesco Chiossi"], "categories": ["cs.HC"], "comment": null, "summary": "Many exergames face challenges in keeping users within safe and effective\nintensity levels during exercise. Meanwhile, although wearable devices\ncontinuously collect physiological data, this information is seldom leveraged\nfor real-time adaptation or to encourage user reflection. We designed and\nevaluated a VR cycling simulator that dynamically adapts based on users' heart\nrate zones. First, we conducted a user study (N=50) comparing eight\nvisualization designs to enhance engagement and exertion control, finding that\ngamified elements like non-player characters (NPCs) were promising for feedback\ndelivery. Based on these findings, we implemented a physiology-adaptive\nexergame that adjusts visual feedback to keep users within their target heart\nrate zones. A lab study (N=18) showed that our system has potential to help\nusers maintain their target heart rate zones. Subjective ratings of exertion,\nenjoyment, and motivation remained largely unchanged between conditions. Our\nfindings suggest that real-time physiological adaptation through NPC\nvisualizations can improve workout regulation in exergaming."}
{"id": "2508.11933", "pdf": "https://arxiv.org/pdf/2508.11933.pdf", "abs": "https://arxiv.org/abs/2508.11933", "title": "CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection", "authors": ["Yue Wang", "Liesheng Wei", "Yuxiang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting machine-generated text (MGT) from contemporary Large Language\nModels (LLMs) is increasingly crucial amid risks like disinformation and\nthreats to academic integrity. Existing zero-shot detection paradigms, despite\ntheir practicality, often exhibit significant deficiencies. Key challenges\ninclude: (1) superficial analyses focused on limited textual attributes, and\n(2) a lack of investigation into consistency across linguistic dimensions such\nas style, semantics, and logic. To address these challenges, we introduce the\n\\textbf{C}ollaborative \\textbf{A}dversarial \\textbf{M}ulti-agent\n\\textbf{F}ramework (\\textbf{CAMF}), a novel architecture using multiple\nLLM-based agents. CAMF employs specialized agents in a synergistic three-phase\nprocess: \\emph{Multi-dimensional Linguistic Feature Extraction},\n\\emph{Adversarial Consistency Probing}, and \\emph{Synthesized Judgment\nAggregation}. This structured collaborative-adversarial process enables a deep\nanalysis of subtle, cross-dimensional textual incongruities indicative of\nnon-human origin. Empirical evaluations demonstrate CAMF's significant\nsuperiority over state-of-the-art zero-shot MGT detection techniques."}
{"id": "2508.13116", "pdf": "https://arxiv.org/pdf/2508.13116.pdf", "abs": "https://arxiv.org/abs/2508.13116", "title": "Choosing the Right Engine in the Virtual Reality Landscape", "authors": ["Santiago Berrezueta-Guzman", "Stefan Wagner"], "categories": ["cs.HC"], "comment": "Pre-print", "summary": "Virtual reality (VR) development relies on game engines to provide real-time\nrendering, physics simulation, and interaction systems. Among the most widely\nused game engines, Unreal Engine and Unity dominate the industry, offering\ndistinct advantages in graphics rendering, performance optimization, usability,\nresource requirements, and scalability. This study presents a comprehensive\ncomparative analysis of both engines, evaluating their capabilities and\ntrade-offs through empirical assessments and real-world case studies of\nlarge-scale VR projects. The findings highlight key factors such as rendering\nfidelity, computational efficiency, cross-platform compatibility, and\ndevelopment workflows. These provide practical insights for selecting the most\nsuitable engine based on project-specific needs. Furthermore, emerging trends\nin artificial intelligence (AI)-driven enhancements, including Deep Learning\nSuper Sampling (DLSS) and large language models (LLMs), are explored to assess\ntheir impact on VR development workflows. By aligning engine capabilities with\ntechnical and creative requirements, developers can overcome performance\nbottlenecks, enhance immersion, and streamline optimization techniques.\n  This study serves as a valuable resource for VR developers, researchers, and\nindustry professionals, offering data-driven recommendations to navigate the\nevolving landscape of VR technology."}
{"id": "2508.12031", "pdf": "https://arxiv.org/pdf/2508.12031.pdf", "abs": "https://arxiv.org/abs/2508.12031", "title": "Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases", "authors": ["Shaozhe Yin", "Jinyu Guo", "Kai Shuang", "Xia Liu", "Ruize Ou"], "categories": ["cs.CL"], "comment": null, "summary": "Continual Relation Extraction (CRE) aims to continually learn new emerging\nrelations while avoiding catastrophic forgetting. Existing CRE methods mainly\nuse memory replay and contrastive learning to mitigate catastrophic forgetting.\nHowever, these methods do not attach importance to the error cases that can\nreveal the model's cognitive biases more effectively. To address this issue, we\npropose an instruction-based continual contrastive tuning approach for Large\nLanguage Models (LLMs) in CRE. Different from existing CRE methods that\ntypically handle the training and memory data in a unified manner, this\napproach splits the training and memory data of each task into two parts\nrespectively based on the correctness of the initial responses and treats them\ndifferently through dual-task fine-tuning. In addition, leveraging the\nadvantages of LLM's instruction-following ability, we propose a novel\ninstruction-based contrastive tuning strategy for LLM to continuously correct\ncurrent cognitive biases with the guidance of previous data in an\ninstruction-tuning manner, which mitigates the gap between old and new\nrelations in a more suitable way for LLMs. We experimentally evaluate our model\non TACRED and FewRel, and the results show that our model achieves new\nstate-of-the-art CRE performance with significant improvements, demonstrating\nthe importance of specializing in exploiting error cases."}
{"id": "2508.13138", "pdf": "https://arxiv.org/pdf/2508.13138.pdf", "abs": "https://arxiv.org/abs/2508.13138", "title": "Human Digital Twin: Data, Models, Applications, and Challenges", "authors": ["Rong Pan", "Hongyue Sun", "Xiaoyu Chen", "Giulia Pedrielli", "Jiapeng Huang"], "categories": ["cs.HC"], "comment": null, "summary": "Human digital twins (HDTs) are dynamic, data-driven virtual representations\nof individuals, continuously updated with multimodal data to simulate, monitor,\nand predict health trajectories. By integrating clinical, physiological,\nbehavioral, and environmental inputs, HDTs enable personalized diagnostics,\ntreatment planning, and anomaly detection. This paper reviews current\napproaches to HDT modeling, with a focus on statistical and machine learning\ntechniques, including recent advances in anomaly detection and failure\nprediction. It also discusses data integration, computational methods, and\nethical, technological, and regulatory challenges in deploying HDTs for\nprecision healthcare."}
{"id": "2508.12040", "pdf": "https://arxiv.org/pdf/2508.12040.pdf", "abs": "https://arxiv.org/abs/2508.12040", "title": "Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation", "authors": ["Jinyi Han", "Tingyun Li", "Shisong Chen", "Jie Shi", "Xinyi Wang", "Guanglei Yue", "Jiaqing Liang", "Xin Lin", "Liqian Wen", "Zulong Chen", "Yanghua Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": "The initial versin was made in August 2024", "summary": "While large language models (LLMs) have demonstrated remarkable performance\nacross diverse tasks, they fundamentally lack self-awareness and frequently\nexhibit overconfidence, assigning high confidence scores to incorrect\npredictions. Accurate confidence estimation is therefore critical for enhancing\nthe trustworthiness and reliability of LLM-generated outputs. However, existing\napproaches suffer from coarse-grained scoring mechanisms that fail to provide\nfine-grained, continuous confidence estimates throughout the generation\nprocess. To address these limitations, we introduce FineCE, a novel confidence\nestimation method that delivers accurate, fine-grained confidence scores during\ntext generation. Specifically, we first develop a comprehensive pipeline for\nconstructing training data that effectively captures the underlying\nprobabilistic distribution of LLM responses, and then train a model to predict\nconfidence scores for arbitrary text sequences in a supervised manner.\nFurthermore, we propose a Backward Confidence Integration (BCI) strategy that\nleverages information from the subsequent text to enhance confidence estimation\nfor the current sequence during inference. We also introduce three strategies\nfor identifying optimal positions to perform confidence estimation within the\ngeneration process. Extensive experiments on multiple benchmark datasets\ndemonstrate that FineCE consistently outperforms existing classical confidence\nestimation methods. Our code and all baselines used in the paper are available\non GitHub."}
{"id": "2508.11640", "pdf": "https://arxiv.org/pdf/2508.11640.pdf", "abs": "https://arxiv.org/abs/2508.11640", "title": "Vibe2Spike: Batteryless Wireless Tags for Vibration Sensing with Event Cameras and Spiking Networks", "authors": ["Danny Scott", "William LaForest", "Hritom Das", "Ioannis Polykretis", "Catherine D. Schuman", "Charles Rizzo", "James Plank", "Sai Swaminathan"], "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG"], "comment": "International Conference on Neuromorphic Systems (ICONS) 2025 9\n  pages, 7 images", "summary": "The deployment of dense, low-cost sensors is critical for realizing\nubiquitous smart environments. However, existing sensing solutions struggle\nwith the energy, scalability, and reliability trade-offs imposed by battery\nmaintenance, wireless transmission overhead, and data processing complexity. In\nthis work, we present Vibe2Spike, a novel battery-free, wireless sensing\nframework that enables vibration-based activity recognition using visible light\ncommunication (VLC) and spiking neural networks (SNNs). Our system uses\nultra-low-cost tags composed only of a piezoelectric disc, a Zener diode, and\nan LED, which harvest vibration energy and emit sparse visible light spikes\nwithout requiring batteries or RF radios. These optical spikes are captured by\nevent cameras and classified using optimized SNN models evolved via the EONS\nframework. We evaluate Vibe2Spike across five device classes, achieving 94.9\\%\naverage classification fitness while analyzing the latency-accuracy trade-offs\nof different temporal binning strategies. Vibe2Spike demonstrates a scalable,\nand energy-efficient approach for enabling intelligent environments in a\nbatteryless manner."}
{"id": "2508.12086", "pdf": "https://arxiv.org/pdf/2508.12086.pdf", "abs": "https://arxiv.org/abs/2508.12086", "title": "J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs", "authors": ["Yao Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 90C29, 62F07", "I.2.7; I.2.6; G.1.6"], "comment": "9 pages, 3 tables, 1 algorithm", "summary": "In large language model (LLM) adaptation, balancing multiple optimization\nobjectives such as improving factuality (heat) and increasing confidence (via\nlow entropy) poses a fundamental challenge, especially when prompt parameters\n(e.g., hidden-layer insertions h and embedding modifications w) interact in\nnon-trivial ways. Existing multi-objective optimization strategies often rely\non scalar gradient aggregation, ignoring the deeper geometric structure between\nobjectives and parameters. We propose J6, a structured Jacobian-based method\nthat decomposes the gradient interaction matrix into six interpretable\ncomponents. This decomposition enables both hard decision-making (e.g.,\nchoosing the dominant update direction via argmax) and soft strategies (e.g.,\nattention-style weighting via softmax over J6), forming a dynamic update\nframework that adapts to local conflict and synergy. Moreover, the\ninterpretable structure of J6 provides insight into parameter attribution, task\ninterference, and geometry-aligned adaptation. Our work introduces a principled\nand extensible mechanism for conflict-aware prompt optimization, and opens a\nnew avenue for incorporating structured Jacobian reasoning into multi-objective\nneural tuning."}
{"id": "2508.11662", "pdf": "https://arxiv.org/pdf/2508.11662.pdf", "abs": "https://arxiv.org/abs/2508.11662", "title": "Generative AI in Training and Coaching: Redefining the Design Process of Learning Materials", "authors": ["Alexander Komar", "Marc-AndrÃ© Heidelmann", "Kristina Schaaff"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Generative artificial intelligence (GenAI) is transforming education,\nredefining the role of trainers and coaches in learning environments. In our\nstudy, we explore how AI integrates into the design process of learning\nmaterials, assessing its impact on efficiency, pedagogical quality, and the\nevolving role of human trainers and coaches. Through qualitative interviews\nwith professionals in education and corporate training, we identify the\nfollowing key topics: trainers and coaches increasingly act as facilitators and\ncontent moderators rather than primary creators, efficiency gains allow for a\nstronger strategic focus but at the same time the new tools require new skills.\nAdditionally, we analyze how the anthropomorphism of AI shapes user trust and\nexpectations. From these insights, we derive how tools based on GenAI can\nsuccessfully be implemented for trainers and coaches on an individual,\norganizational, systemic, and strategic level."}
{"id": "2508.12096", "pdf": "https://arxiv.org/pdf/2508.12096.pdf", "abs": "https://arxiv.org/abs/2508.12096", "title": "STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples", "authors": ["Haiquan Hu", "Jiazhi Jiang", "Shiyou Xu", "Ruhan Zeng", "Tian Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submit to AAAI 2026", "summary": "Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs."}
{"id": "2508.11704", "pdf": "https://arxiv.org/pdf/2508.11704.pdf", "abs": "https://arxiv.org/abs/2508.11704", "title": "Next-Gen Education: Enhancing AI for Microlearning", "authors": ["Suman Saha", "Fatemeh Rahbari", "Farhan Sadique", "Sri Krishna Chaitanya Velamakanni", "Mahfuza Farooque", "William J. Rothwell"], "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC", "cs.MM"], "comment": "Published and presented in 2025 ASEE Annual Conference and\n  Exposition, 22 pages, 6 figures", "summary": "This paper explores integrating microlearning strategies into university\ncurricula, particularly in computer science education, to counteract the\ndecline in class attendance and engagement in US universities after COVID. As\nstudents increasingly opt for remote learning and recorded lectures,\ntraditional educational approaches struggle to maintain engagement and\neffectiveness. Microlearning, which breaks complex subjects into manageable\nunits, is proposed to address shorter attention spans and enhance educational\noutcomes. It uses interactive formats such as videos, quizzes, flashcards, and\nscenario-based exercises, which are especially beneficial for topics like\nalgorithms and programming logic requiring deep understanding and ongoing\npractice. Adoption of microlearning is often limited by the effort needed to\ncreate such materials. This paper proposes leveraging AI tools, specifically\nChatGPT, to reduce the workload for educators by automating the creation of\nsupplementary materials. While AI can automate certain tasks, educators remain\nessential in guiding and shaping the learning process. This AI-enhanced\napproach ensures course content is kept current with the latest research and\ntechnology, with educators providing context and insights. By examining AI\ncapabilities in microlearning, this study shows the potential to transform\neducational practices and outcomes in computer science, offering a practical\nmodel for combining advanced technology with established teaching methods."}
{"id": "2508.12140", "pdf": "https://arxiv.org/pdf/2508.12140.pdf", "abs": "https://arxiv.org/abs/2508.12140", "title": "Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality", "authors": ["Ziqian Bi", "Lu Chen", "Junhao Song", "Hongying Luo", "Enze Ge", "Junmin Huang", "Tianyang Wang", "Keyu Chen", "Chia Xin Liang", "Zihan Wei", "Huafeng Liu", "Chunjie Tian", "Jibin Guan", "Joe Yeong", "Yongzhi Xu", "Peng Wang", "Junfeng Hao"], "categories": ["cs.CL"], "comment": null, "summary": "This study presents the first comprehensive evaluation of thinking budget\nmechanisms in medical reasoning tasks, revealing fundamental scaling laws\nbetween computational resources and reasoning quality. We systematically\nevaluated two major model families, Qwen3 (1.7B to 235B parameters) and\nDeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning\ndiverse specialties and difficulty levels. Through controlled experiments with\nthinking budgets ranging from zero to unlimited tokens, we establish\nlogarithmic scaling relationships where accuracy improvements follow a\npredictable pattern with both thinking budget and model size. Our findings\nidentify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)\nsuitable for real-time applications, balanced (256 to 512 tokens) offering\noptimal cost-performance tradeoffs for routine clinical support, and\nhigh-accuracy (above 512 tokens) justified only for critical diagnostic tasks.\nNotably, smaller models demonstrate disproportionately larger benefits from\nextended thinking, with 15 to 20% improvements compared to 5 to 10% for larger\nmodels, suggesting a complementary relationship where thinking budget provides\ngreater relative benefits for capacity-constrained models. Domain-specific\npatterns emerge clearly, with neurology and gastroenterology requiring\nsignificantly deeper reasoning processes than cardiovascular or respiratory\nmedicine. The consistency between Qwen3 native thinking budget API and our\nproposed truncation method for DeepSeek-R1 validates the generalizability of\nthinking budget concepts across architectures. These results establish thinking\nbudget control as a critical mechanism for optimizing medical AI systems,\nenabling dynamic resource allocation aligned with clinical needs while\nmaintaining the transparency essential for healthcare deployment."}
{"id": "2508.11873", "pdf": "https://arxiv.org/pdf/2508.11873.pdf", "abs": "https://arxiv.org/abs/2508.11873", "title": "SimInterview: Transforming Business Education through Large Language Model-Based Simulated Multilingual Interview Training System", "authors": ["Truong Thanh Hung Nguyen", "Tran Diem Quynh Nguyen", "Hoang Loc Cao", "Thi Cam Thanh Tran", "Thi Cam Mai Truong", "Hung Cao"], "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.MM"], "comment": "Published as a conference paper at ICEFM 2025", "summary": "Business interview preparation demands both solid theoretical grounding and\nrefined soft skills, yet conventional classroom methods rarely deliver the\nindividualized, culturally aware practice employers currently expect. This\npaper introduces SimInterview, a large language model (LLM)-based simulated\nmultilingual interview training system designed for business professionals\nentering the AI-transformed labor market. Our system leverages an LLM agent and\nsynthetic AI technologies to create realistic virtual recruiters capable of\nconducting personalized, real-time conversational interviews. The framework\ndynamically adapts interview scenarios using retrieval-augmented generation\n(RAG) to match individual resumes with specific job requirements across\nmultiple languages. Built on LLMs (OpenAI o3, Llama 4 Maverick, Gemma 3),\nintegrated with Whisper speech recognition, GPT-SoVITS voice synthesis, Ditto\ndiffusion-based talking head generation model, and ChromaDB vector databases,\nour system significantly improves interview readiness across English and\nJapanese markets. Experiments with university-level candidates show that the\nsystem consistently aligns its assessments with job requirements, faithfully\npreserves resume content, and earns high satisfaction ratings, with the\nlightweight Gemma 3 model producing the most engaging conversations.\nQualitative findings revealed that the standardized Japanese resume format\nimproved document retrieval while diverse English resumes introduced additional\nvariability, and they highlighted how cultural norms shape follow-up\nquestioning strategies. Finally, we also outlined a contestable AI design that\ncan explain, detect bias, and preserve human-in-the-loop to meet emerging\nregulatory expectations."}
{"id": "2508.12158", "pdf": "https://arxiv.org/pdf/2508.12158.pdf", "abs": "https://arxiv.org/abs/2508.12158", "title": "LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data", "authors": ["Stephen Meisenbacher", "Alexandra Klymenko", "Florian Matthes"], "categories": ["cs.CL"], "comment": "13 pages, 3 figures, 4 tables. Accepted to HAIPS @ CCS 2025", "summary": "Despite advances in the field of privacy-preserving Natural Language\nProcessing (NLP), a significant challenge remains the accurate evaluation of\nprivacy. As a potential solution, using LLMs as a privacy evaluator presents a\npromising approach $\\unicode{x2013}$ a strategy inspired by its success in\nother subfields of NLP. In particular, the so-called $\\textit{LLM-as-a-Judge}$\nparadigm has achieved impressive results on a variety of natural language\nevaluation tasks, demonstrating high agreement rates with human annotators.\nRecognizing that privacy is both subjective and difficult to define, we\ninvestigate whether LLM-as-a-Judge can also be leveraged to evaluate the\nprivacy sensitivity of textual data. Furthermore, we measure how closely LLM\nevaluations align with human perceptions of privacy in text. Resulting from a\nstudy involving 10 datasets, 13 LLMs, and 677 human survey participants, we\nconfirm that privacy is indeed a difficult concept to measure empirically,\nexhibited by generally low inter-human agreement rates. Nevertheless, we find\nthat LLMs can accurately model a global human privacy perspective, and through\nan analysis of human and LLM reasoning patterns, we discuss the merits and\nlimitations of LLM-as-a-Judge for privacy evaluation in textual data. Our\nfindings pave the way for exploring the feasibility of LLMs as privacy\nevaluators, addressing a core challenge in solving pressing privacy issues with\ninnovative technical solutions."}
{"id": "2508.11887", "pdf": "https://arxiv.org/pdf/2508.11887.pdf", "abs": "https://arxiv.org/abs/2508.11887", "title": "Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards", "authors": ["Yousra Shleibik", "Jordan Sinclair", "Kerstin Haring"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "The advent of autonomous driving systems promises to transform transportation\nby enhancing safety, efficiency, and comfort. As these technologies evolve\ntoward higher levels of autonomy, the need for integrated systems that\nseamlessly support human involvement in decision-making becomes increasingly\ncritical. Certain scenarios necessitate human involvement, including those\nwhere the vehicle is unable to identify an object or element in the scene, and\nas such cannot take independent action. Therefore, situational awareness is\nessential to mitigate potential risks during a takeover, where a driver must\nassume control and autonomy from the vehicle. The need for driver attention is\nimportant to avoid collisions with external agents and ensure a smooth\ntransition during takeover operations. This paper explores the integration of\nattention redirection techniques, such as gaze manipulation through targeted\nvisual and auditory cues, to help drivers maintain focus on emerging hazards\nand reduce target fixation in semi-autonomous driving scenarios. We propose a\nconceptual framework that combines real-time gaze tracking, context-aware\nsaliency analysis, and synchronized visual and auditory alerts to enhance\nsituational awareness, proactively address potential hazards, and foster\neffective collaboration between humans and autonomous systems."}
{"id": "2508.12227", "pdf": "https://arxiv.org/pdf/2508.12227.pdf", "abs": "https://arxiv.org/abs/2508.12227", "title": "Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges", "authors": ["Abdelhamid Haouhat", "Slimane Bellaouar", "Attia Nehar", "Hadda Cherroun", "Ahmed Abdelali"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Machine Learning (MML) aims to integrate and analyze information\nfrom diverse modalities, such as text, audio, and visuals, enabling machines to\naddress complex tasks like sentiment analysis, emotion recognition, and\nmultimedia retrieval. Recently, Arabic MML has reached a certain level of\nmaturity in its foundational development, making it time to conduct a\ncomprehensive survey. This paper explores Arabic MML by categorizing efforts\nthrough a novel taxonomy and analyzing existing research. Our taxonomy\norganizes these efforts into four key topics: datasets, applications,\napproaches, and challenges. By providing a structured overview, this survey\noffers insights into the current state of Arabic MML, highlighting areas that\nhave not been investigated and critical research gaps. Researchers will be\nempowered to build upon the identified opportunities and address challenges to\nadvance the field."}
{"id": "2508.11944", "pdf": "https://arxiv.org/pdf/2508.11944.pdf", "abs": "https://arxiv.org/abs/2508.11944", "title": "CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs", "authors": ["Hongtao Liu", "Zhicheng Du", "Zihe Wang", "Weiran Shen"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Game-playing ability serves as an indicator for evaluating the strategic\nreasoning capability of large language models (LLMs). While most existing\nstudies rely on utility performance metrics, which are not robust enough due to\nvariations in opponent behavior and game structure. To address this limitation,\nwe propose \\textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation\nframework inspired by the cognitive hierarchy models from behavioral economics.\nWe hypothesize that agents have bounded rationality -- different agents behave\nat varying reasoning depths/levels. We evaluate LLMs' strategic reasoning\nthrough a three-phase systematic framework, utilizing behavioral data from six\nstate-of-the-art LLMs across fifteen carefully selected normal-form games.\nExperiments show that LLMs exhibit consistent strategic reasoning levels across\ndiverse opponents, confirming the framework's robustness and generalization\ncapability. We also analyze the effects of two key mechanisms (Chat Mechanism\nand Memory Mechanism) on strategic reasoning performance. Results indicate that\nthe Chat Mechanism significantly degrades strategic reasoning, whereas the\nMemory Mechanism enhances it. These insights position CHBench as a promising\ntool for evaluating LLM capabilities, with significant potential for future\nresearch and practical applications."}
{"id": "2508.12243", "pdf": "https://arxiv.org/pdf/2508.12243.pdf", "abs": "https://arxiv.org/abs/2508.12243", "title": "SEA-BED: Southeast Asia Embedding Benchmark", "authors": ["Wuttikorn Ponwitayarat", "Raymond Ng", "Jann Railey Montalan", "Thura Aung", "Jian Gang Ngui", "Yosephine Susanto", "William Tjhi", "Panuthep Tasawong", "Erik Cambria", "Ekapol Chuangsuwanich", "Sarana Nutanong", "Peerat Limkonchotiwat"], "categories": ["cs.CL"], "comment": null, "summary": "Sentence embeddings are essential for NLP tasks such as semantic search,\nre-ranking, and textual similarity. Although multilingual benchmarks like MMTEB\nbroaden coverage, Southeast Asia (SEA) datasets are scarce and often\nmachine-translated, missing native linguistic properties. With nearly 700\nmillion speakers, the SEA region lacks a region-specific embedding benchmark.\nWe introduce SEA-BED, the first large-scale SEA embedding benchmark with 169\ndatasets across 9 tasks and 10 languages, where 71% are formulated by humans,\nnot machine generation or translation. We address three research questions: (1)\nwhich SEA languages and tasks are challenging, (2) whether SEA languages show\nunique performance gaps globally, and (3) how human vs. machine translations\naffect evaluation. We evaluate 17 embedding models across six studies,\nanalyzing task and language challenges, cross-benchmark comparisons, and\ntranslation trade-offs. Results show sharp ranking shifts, inconsistent model\nperformance among SEA languages, and the importance of human-curated datasets\nfor low-resource languages like Burmese."}
{"id": "2508.12075", "pdf": "https://arxiv.org/pdf/2508.12075.pdf", "abs": "https://arxiv.org/abs/2508.12075", "title": "Into the Wild: When Robots Are Not Welcome", "authors": ["Shaul Ashkenazi", "Gabriel Skantze", "Jane Stuart-Smith", "Mary Ellen Foster"], "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025 (paper PubRob-Fails/2025/4)", "summary": "Social robots are increasingly being deployed in public spaces, where they\nface not only technological difficulties and unexpected user utterances, but\nalso objections from stakeholders who may not be comfortable with introducing a\nrobot into those spaces. We describe our difficulties with deploying a social\nrobot in two different public settings: 1) Student services center; 2) Refugees\nand asylum seekers drop-in service. Although this is a failure report, in each\nuse case we eventually managed to earn the trust of the staff and form a\nrelationship with them, allowing us to deploy our robot and conduct our\nstudies."}
{"id": "2508.12255", "pdf": "https://arxiv.org/pdf/2508.12255.pdf", "abs": "https://arxiv.org/abs/2508.12255", "title": "What do Speech Foundation Models Learn? Analysis and Applications", "authors": ["Ankita Pasad"], "categories": ["cs.CL", "eess.AS"], "comment": "Ph.D. Thesis", "summary": "Speech foundation models (SFMs) are designed to serve as general-purpose\nrepresentations for a wide range of speech-processing tasks. The last five\nyears have seen an influx of increasingly successful self-supervised and\nsupervised pre-trained models with impressive performance on various downstream\ntasks.\n  Although the zoo of SFMs continues to grow, our understanding of the\nknowledge they acquire lags behind. This thesis presents a lightweight analysis\nframework using statistical tools and training-free tasks to investigate the\nacoustic and linguistic knowledge encoded in SFM layers. We conduct a\ncomparative study across multiple SFMs and statistical tools. Our study also\nshows that the analytical insights have concrete implications for downstream\ntask performance.\n  The effectiveness of an SFM is ultimately determined by its performance on\nspeech applications. Yet it remains unclear whether the benefits extend to\nspoken language understanding (SLU) tasks that require a deeper understanding\nthan widely studied ones, such as speech recognition. The limited exploration\nof SLU is primarily due to a lack of relevant datasets. To alleviate that, this\nthesis contributes tasks, specifically spoken named entity recognition (NER)\nand named entity localization (NEL), to the Spoken Language Understanding\nEvaluation benchmark. We develop SFM-based approaches for NER and NEL, and find\nthat end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded\n(speech recognition followed by a text model) approaches. Further, we evaluate\nE2E SLU models across SFMs and adaptation strategies to assess the impact on\ntask performance.\n  Collectively, this thesis tackles previously unanswered questions about SFMs,\nproviding tools and datasets to further our understanding and to enable the\ncommunity to make informed design choices for future model development and\nadoption."}
{"id": "2508.12163", "pdf": "https://arxiv.org/pdf/2508.12163.pdf", "abs": "https://arxiv.org/abs/2508.12163", "title": "RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis", "authors": ["Wenqing Wang", "Yun Fu"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "I.4; I.3; I.2"], "comment": "Accepted to the ICCV 2025 Workshop on Artificial Social Intelligence", "summary": "Emotion is a critical component of artificial social intelligence. However,\nwhile current methods excel in lip synchronization and image quality, they\noften fail to generate accurate and controllable emotional expressions while\npreserving the subject's identity. To address this challenge, we introduce\nRealTalk, a novel framework for synthesizing emotional talking heads with high\nemotion accuracy, enhanced emotion controllability, and robust identity\npreservation. RealTalk employs a variational autoencoder (VAE) to generate 3D\nfacial landmarks from driving audio, which are concatenated with emotion-label\nembeddings using a ResNet-based landmark deformation model (LDM) to produce\nemotional landmarks. These landmarks and facial blendshape coefficients jointly\ncondition a novel tri-plane attention Neural Radiance Field (NeRF) to\nsynthesize highly realistic emotional talking heads. Extensive experiments\ndemonstrate that RealTalk outperforms existing methods in emotion accuracy,\ncontrollability, and identity preservation, advancing the development of\nsocially intelligent AI systems."}
{"id": "2508.12257", "pdf": "https://arxiv.org/pdf/2508.12257.pdf", "abs": "https://arxiv.org/abs/2508.12257", "title": "Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework", "authors": ["Zheye Deng", "Chunkit Chan", "Tianshi Zheng", "Wei Fan", "Weiqi Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "The evolution of AI systems toward agentic operation and context-aware\nretrieval necessitates transforming unstructured text into structured formats\nlike tables, knowledge graphs, and charts. While such conversions enable\ncritical applications from summarization to data mining, current research lacks\na comprehensive synthesis of methodologies, datasets, and metrics. This\nsystematic review examines text-to-structure techniques and the encountered\nchallenges, evaluates current datasets and assessment criteria, and outlines\npotential directions for future research. We also introduce a universal\nevaluation framework for structured outputs, establishing text-to-structure as\nfoundational infrastructure for next-generation AI systems."}
{"id": "2508.12285", "pdf": "https://arxiv.org/pdf/2508.12285.pdf", "abs": "https://arxiv.org/abs/2508.12285", "title": "\"My productivity is boosted, but ...\" Demystifying Users' Perception on AI Coding Assistants", "authors": ["Yunbo Lyu", "Zhou Yang", "Jieke Shi", "Jianming Chang", "Yue Liu", "David Lo"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "13 pages, Camera-Ready Version that will appear in ASE 2025", "summary": "This paper aims to explore fundamental questions in the era when AI coding\nassistants like GitHub Copilot are widely adopted: what do developers truly\nvalue and criticize in AI coding assistants, and what does this reveal about\ntheir needs and expectations in real-world software development? Unlike\nprevious studies that conduct observational research in controlled and\nsimulated environments, we analyze extensive, first-hand user reviews of AI\ncoding assistants, which capture developers' authentic perspectives and\nexperiences drawn directly from their actual day-to-day work contexts. We\nidentify 1,085 AI coding assistants from the Visual Studio Code Marketplace.\nAlthough they only account for 1.64% of all extensions, we observe a surge in\nthese assistants: over 90% of them are released within the past two years. We\nthen manually analyze the user reviews sampled from 32 AI coding assistants\nthat have sufficient installations and reviews to construct a comprehensive\ntaxonomy of user concerns and feedback about these assistants. We manually\nannotate each review's attitude when mentioning certain aspects of coding\nassistants, yielding nuanced insights into user satisfaction and\ndissatisfaction regarding specific features, concerns, and overall tool\nperformance. Built on top of the findings-including how users demand not just\nintelligent suggestions but also context-aware, customizable, and\nresource-efficient interactions-we propose five practical implications and\nsuggestions to guide the enhancement of AI coding assistants that satisfy user\nneeds."}
{"id": "2508.12265", "pdf": "https://arxiv.org/pdf/2508.12265.pdf", "abs": "https://arxiv.org/abs/2508.12265", "title": "Fast, Slow, and Tool-augmented Thinking for LLMs: A Review", "authors": ["Xinda Jia", "Jinpeng Li", "Zezhong Wang", "Jingjing Li", "Xingshan Zeng", "Yasheng Wang", "Weinan Zhang", "Yong Yu", "Weiwen Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\nreasoning across diverse domains. However, effective reasoning in real-world\ntasks requires adapting the reasoning strategy to the demands of the problem,\nranging from fast, intuitive responses to deliberate, step-by-step reasoning\nand tool-augmented thinking. Drawing inspiration from cognitive psychology, we\npropose a novel taxonomy of LLM reasoning strategies along two knowledge\nboundaries: a fast/slow boundary separating intuitive from deliberative\nprocesses, and an internal/external boundary distinguishing reasoning grounded\nin the model's parameters from reasoning augmented by external tools. We\nsystematically survey recent work on adaptive reasoning in LLMs and categorize\nmethods based on key decision factors. We conclude by highlighting open\nchallenges and future directions toward more adaptive, efficient, and reliable\nLLMs."}
{"id": "2508.12571", "pdf": "https://arxiv.org/pdf/2508.12571.pdf", "abs": "https://arxiv.org/abs/2508.12571", "title": "Cyber Risks to Next-Gen Brain-Computer Interfaces: Analysis and Recommendations", "authors": ["Tyler Schroder", "Renee Sirbu", "Sohee Park", "Jessica Morley", "Sam Street", "Luciano Floridi"], "categories": ["cs.CR", "cs.CY", "cs.ET", "cs.HC", "cs.NE"], "comment": null, "summary": "Brain-computer interfaces (BCIs) show enormous potential for advancing\npersonalized medicine. However, BCIs also introduce new avenues for\ncyber-attacks or security compromises. In this article, we analyze the problem\nand make recommendations for device manufacturers to better secure devices and\nto help regulators understand where more guidance is needed to protect patient\nsafety and data confidentiality. Device manufacturers should implement the\nprior suggestions in their BCI products. These recommendations help protect BCI\nusers from undue risks, including compromised personal health and genetic\ninformation, unintended BCI-mediated movement, and many other cybersecurity\nbreaches. Regulators should mandate non-surgical device update methods, strong\nauthentication and authorization schemes for BCI software modifications,\nencryption of data moving to and from the brain, and minimize network\nconnectivity where possible. We also design a hypothetical, average-case threat\nmodel that identifies possible cybersecurity threats to BCI patients and\npredicts the likeliness of risk for each category of threat. BCIs are at less\nrisk of physical compromise or attack, but are vulnerable to remote attack; we\nfocus on possible threats via network paths to BCIs and suggest technical\ncontrols to limit network connections."}
{"id": "2508.12277", "pdf": "https://arxiv.org/pdf/2508.12277.pdf", "abs": "https://arxiv.org/abs/2508.12277", "title": "The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution", "authors": ["Elon Ezra", "Ariel Weizman", "Amos Azaria"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 9 figures", "summary": "Large language models (LLMs) are commonly evaluated on tasks that test their\nknowledge or reasoning abilities. In this paper, we explore a different type of\nevaluation: whether an LLM can predict aspects of its own responses. Since LLMs\nlack the ability to execute themselves, we introduce the Self-Execution\nBenchmark, which measures a model's ability to anticipate properties of its\noutput, such as whether a question will be difficult for it, whether it will\nrefuse to answer, or what kinds of associations it is likely to produce. Our\nexperiments show that models generally perform poorly on this benchmark, and\nthat increased model size or capability does not consistently lead to better\nperformance. These results suggest a fundamental limitation in how LLMs\nrepresent and reason about their own behavior."}
{"id": "2508.12614", "pdf": "https://arxiv.org/pdf/2508.12614.pdf", "abs": "https://arxiv.org/abs/2508.12614", "title": "Towards SISO Bistatic Sensing for ISAC", "authors": ["Zhongqin Wang", "J. Andrew Zhang", "Kai Wu", "Min Xu", "Y. Jay Guo"], "categories": ["eess.SP", "cs.HC", "cs.LG"], "comment": null, "summary": "Integrated Sensing and Communication (ISAC) is a key enabler for\nnext-generation wireless systems. However, real-world deployment is often\nlimited to low-cost, single-antenna transceivers. In such bistatic Single-Input\nSingle-Output (SISO) setup, clock asynchrony introduces random phase offsets in\nChannel State Information (CSI), which cannot be mitigated using conventional\nmulti-antenna methods. This work proposes WiDFS 3.0, a lightweight bistatic\nSISO sensing framework that enables accurate delay and Doppler estimation from\ndistorted CSI by effectively suppressing Doppler mirroring ambiguity. It\noperates with only a single antenna at both the transmitter and receiver,\nmaking it suitable for low-complexity deployments. We propose a\nself-referencing cross-correlation (SRCC) method for SISO random phase removal\nand employ delay-domain beamforming to resolve Doppler ambiguity. The resulting\nunambiguous delay-Doppler-time features enable robust sensing with compact\nneural networks. Extensive experiments show that WiDFS 3.0 achieves accurate\nparameter estimation, with performance comparable to or even surpassing that of\nprior multi-antenna methods, especially in delay estimation. Validated under\nsingle- and multi-target scenarios, the extracted ambiguity-resolved features\nshow strong sensing accuracy and generalization. For example, when deployed on\nthe embedded-friendly MobileViT-XXS with only 1.3M parameters, WiDFS 3.0\nconsistently outperforms conventional features such as CSI amplitude, mirrored\nDoppler, and multi-receiver aggregated Doppler."}
{"id": "2508.12281", "pdf": "https://arxiv.org/pdf/2508.12281.pdf", "abs": "https://arxiv.org/abs/2508.12281", "title": "Legal$Î$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain", "authors": ["Xin Dai", "Buqiang Xu", "Zhenghao Liu", "Yukun Yan", "Huiyuan Xie", "Xiaoyuan Yi", "Shuo Wang", "Ge Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Legal Artificial Intelligence (LegalAI) has achieved notable advances in\nautomating judicial decision-making with the support of Large Language Models\n(LLMs). However, existing legal LLMs still struggle to generate reliable and\ninterpretable reasoning processes. They often default to fast-thinking behavior\nby producing direct answers without explicit multi-step reasoning, limiting\ntheir effectiveness in complex legal scenarios that demand rigorous\njustification. To address this challenge, we propose Legal$\\Delta$, a\nreinforcement learning framework designed to enhance legal reasoning through\nchain-of-thought guided information gain. During training, Legal$\\Delta$\nemploys a dual-mode input setup-comprising direct answer and\nreasoning-augmented modes-and maximizes the information gain between them. This\nencourages the model to acquire meaningful reasoning patterns rather than\ngenerating superficial or redundant explanations. Legal$\\Delta$ follows a\ntwo-stage approach: (1) distilling latent reasoning capabilities from a\npowerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning\nquality via differential comparisons, combined with a multidimensional reward\nmechanism that assesses both structural coherence and legal-domain specificity.\nExperimental results on multiple legal reasoning tasks demonstrate that\nLegal$\\Delta$ outperforms strong baselines in both accuracy and\ninterpretability. It consistently produces more robust and trustworthy legal\njudgments without relying on labeled preference data. All code and data will be\nreleased at https://github.com/NEUIR/LegalDelta."}
{"id": "2508.12730", "pdf": "https://arxiv.org/pdf/2508.12730.pdf", "abs": "https://arxiv.org/abs/2508.12730", "title": "Unlearning Comparator: A Visual Analytics System for Comparative Evaluation of Machine Unlearning Methods", "authors": ["Jaeung Lee", "Suhyeon Yu", "Yurim Jang", "Simon S. Woo", "Jaemin Jo"], "categories": ["cs.CR", "cs.HC", "cs.LG", "H.5.2; I.3.6"], "comment": "Submitted to IEEE Transactions on Visualization and Computer Graphics\n  (TVCG), under review. 15 pages. This work has been submitted to the IEEE for\n  possible publication", "summary": "Machine Unlearning (MU) aims to remove target training data from a trained\nmodel so that the removed data no longer influences the model's behavior,\nfulfilling \"right to be forgotten\" obligations under data privacy laws. Yet, we\nobserve that researchers in this rapidly emerging field face challenges in\nanalyzing and understanding the behavior of different MU methods, especially in\nterms of three fundamental principles in MU: accuracy, efficiency, and privacy.\nConsequently, they often rely on aggregate metrics and ad-hoc evaluations,\nmaking it difficult to accurately assess the trade-offs between methods. To\nfill this gap, we introduce a visual analytics system, Unlearning Comparator,\ndesigned to facilitate the systematic evaluation of MU methods. Our system\nsupports two important tasks in the evaluation process: model comparison and\nattack simulation. First, it allows the user to compare the behaviors of two\nmodels, such as a model generated by a certain method and a retrained baseline,\nat class-, instance-, and layer-levels to better understand the changes made\nafter unlearning. Second, our system simulates membership inference attacks\n(MIAs) to evaluate the privacy of a method, where an attacker attempts to\ndetermine whether specific data samples were part of the original training set.\nWe evaluate our system through a case study visually analyzing prominent MU\nmethods and demonstrate that it helps the user not only understand model\nbehaviors but also gain insights that can inform the improvement of MU methods."}
{"id": "2508.12282", "pdf": "https://arxiv.org/pdf/2508.12282.pdf", "abs": "https://arxiv.org/abs/2508.12282", "title": "A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation", "authors": ["Ziyang Chen", "Erxue Min", "Xiang Zhao", "Yunxin Li", "Xin Jia", "Jinzhi Liao", "Jichao Li", "Shuaiqiang Wang", "Baotian Hu", "Dawei Yin"], "categories": ["cs.CL", "cs.IR", "68T50, 68P20", "I.2.7; H.3.3"], "comment": "10 pages, 5 figures", "summary": "We introduce ChronoQA, a large-scale benchmark dataset for Chinese question\nanswering, specifically designed to evaluate temporal reasoning in\nRetrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over\n300,000 news articles published between 2019 and 2024, and contains 5,176\nhigh-quality questions covering absolute, aggregate, and relative temporal\ntypes with both explicit and implicit time expressions. The dataset supports\nboth single- and multi-document scenarios, reflecting the real-world\nrequirements for temporal alignment and logical consistency. ChronoQA features\ncomprehensive structural annotations and has undergone multi-stage validation,\nincluding rule-based, LLM-based, and human evaluation, to ensure data quality.\nBy providing a dynamic, reliable, and scalable resource, ChronoQA enables\nstructured evaluation across a wide range of temporal tasks, and serves as a\nrobust benchmark for advancing time-sensitive retrieval-augmented question\nanswering systems."}
{"id": "2508.12854", "pdf": "https://arxiv.org/pdf/2508.12854.pdf", "abs": "https://arxiv.org/abs/2508.12854", "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model", "authors": ["Ronghao Lin", "Shuai Shen", "Weipeng Hu", "Qiaolin He", "Aolin Xiong", "Li Huang", "Haifeng Hu", "Yap-peng Tan"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MM"], "comment": "Accepted at ACM MM 2025 Grand Challenge", "summary": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG."}
{"id": "2508.12286", "pdf": "https://arxiv.org/pdf/2508.12286.pdf", "abs": "https://arxiv.org/abs/2508.12286", "title": "Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction", "authors": ["Qinghua Wang", "Xu Zhang", "Lingyan Yang", "Rui Shao", "Bonan Wang", "Fang Wang", "Cunquan Qu"], "categories": ["cs.CL"], "comment": null, "summary": "Probation is a crucial institution in modern criminal law, embodying the\nprinciples of fairness and justice while contributing to the harmonious\ndevelopment of society. Despite its importance, the current Intelligent\nJudicial Assistant System (IJAS) lacks dedicated methods for probation\nprediction, and research on the underlying factors influencing probation\neligibility remains limited. In addition, probation eligibility requires a\ncomprehensive analysis of both criminal circumstances and remorse. Much of the\nexisting research in IJAS relies primarily on data-driven methodologies, which\noften overlooks the legal logic underpinning judicial decision-making. To\naddress this gap, we propose a novel approach that integrates legal logic into\ndeep learning models for probation prediction, implemented in three distinct\nstages. First, we construct a specialized probation dataset that includes fact\ndescriptions and probation legal elements (PLEs). Second, we design a distinct\nprobation prediction model named the Multi-Task Dual-Theory Probation\nPrediction Model (MT-DT), which is grounded in the legal logic of probation and\nthe \\textit{Dual-Track Theory of Punishment}. Finally, our experiments on the\nprobation dataset demonstrate that the MT-DT model outperforms baseline models,\nand an analysis of the underlying legal logic further validates the\neffectiveness of the proposed approach."}
{"id": "2508.12896", "pdf": "https://arxiv.org/pdf/2508.12896.pdf", "abs": "https://arxiv.org/abs/2508.12896", "title": "Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption", "authors": ["Faruk Alpay", "Taylan Alpay"], "categories": ["cs.AI", "cs.HC", "stat.ME", "62M10, 62J02, 62F12, 62P20, 91B16"], "comment": "17 pages, 7 figures, 4 tables", "summary": "We formalize three design axioms for sustained adoption of agent-centric AI\nsystems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed >\nDestination; (A3) Agency > Chat. We model adoption as a sum of a decaying\nnovelty term and a growing utility term and derive the phase conditions for\ntroughs/overshoots with full proofs. We introduce: (i) an\nidentifiability/confounding analysis for $(\\alpha,\\beta,N_0,U_{\\max})$ with\ndelta-method gradients; (ii) a non-monotone comparator\n(logistic-with-transient-bump) evaluated on the same series to provide\nadditional model comparison; (iii) ablations over hazard families $h(\\cdot)$\nmapping $\\Delta V \\to \\beta$; (iv) a multi-series benchmark (varying trough\ndepth, noise, AR structure) reporting coverage (type-I error, power); (v)\ncalibration of friction proxies against time-motion/survey ground truth with\nstandard errors; (vi) residual analyses (autocorrelation and\nheteroskedasticity) for each fitted curve; (vii) preregistered windowing\nchoices for pre/post estimation; (viii) Fisher information & CRLB for\n$(\\alpha,\\beta)$ under common error models; (ix) microfoundations linking\n$\\mathcal{T}$ to $(N_0,U_{\\max})$; (x) explicit comparison to bi-logistic,\ndouble-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$\nheterogeneity. Figures and tables are reflowed for readability, and the\nbibliography restores and extends non-logistic/Bass adoption references\n(Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All\ncode and logs necessary to reproduce the synthetic analyses are embedded as\nLaTeX listings."}
{"id": "2508.12301", "pdf": "https://arxiv.org/pdf/2508.12301.pdf", "abs": "https://arxiv.org/abs/2508.12301", "title": "CarelessWhisper: Turning Whisper into a Causal Streaming Model", "authors": ["Tomer Krichli", "Bhiksha Raj", "Joseph Keshet"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "17 pages, 7 Figures, This work has been submitted to the IEEE for\n  possible publication", "summary": "Automatic Speech Recognition (ASR) has seen remarkable progress, with models\nlike OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)\nperformance in offline transcription. However, these models are not designed\nfor streaming (online or real-time) transcription, due to limitations in their\narchitecture and training methodology. We propose a method to turn the\ntransformer encoder-decoder model into a low-latency streaming model that is\ncareless about future context. We present an analysis explaining why it is not\nstraightforward to convert an encoder-decoder transformer to a low-latency\nstreaming model. Our proposed method modifies the existing (non-causal) encoder\nto a causal encoder by fine-tuning both the encoder and decoder using Low-Rank\nAdaptation (LoRA) and a weakly aligned dataset. We then propose an updated\ninference mechanism that utilizes the fine-tune causal encoder and decoder to\nyield greedy and beam-search decoding, and is shown to be locally optimal.\nExperiments on low-latency chunk sizes (less than 300 msec) show that our\nfine-tuned model outperforms existing non-fine-tuned streaming approaches in\nmost cases, while using a lower complexity. Additionally, we observe that our\ntraining process yields better alignment, enabling a simple method for\nextracting word-level timestamps. We release our training and inference code,\nalong with the fine-tuned models, to support further research and development\nin streaming ASR."}
{"id": "2508.12925", "pdf": "https://arxiv.org/pdf/2508.12925.pdf", "abs": "https://arxiv.org/abs/2508.12925", "title": "Deformation of the panoramic sphere into an ellipsoid to induce self-motion in telepresence users", "authors": ["Eetu Laukka", "Evan G. Center", "Timo Ojala", "Steven M. LaValle", "Matti Pouke"], "categories": ["cs.RO", "cs.HC"], "comment": "2025 IEEE Conference on Telepresence", "summary": "Mobile telepresence robots allow users to feel present and explore remote\nenvironments using technology. Traditionally, these systems are implemented\nusing a camera onboard a mobile robot that can be controlled. Although\nhigh-immersion technologies, such as 360-degree cameras, can increase\nsituational awareness and presence, they also introduce significant challenges.\nAdditional processing and bandwidth requirements often result in latencies of\nup to seconds. The current delay with a 360-degree camera streaming over the\ninternet makes real-time control of these systems difficult. Working with\nhigh-latency systems requires some form of assistance to the users.\n  This study presents a novel way to utilize optical flow to create an illusion\nof self-motion to the user during the latency period between user sending\nmotion commands to the robot and seeing the actual motion through the\n360-camera stream. We find no significant benefit of using the self-motion\nillusion to performance or accuracy of controlling a telepresence robot with a\nlatency of 500 ms, as measured by the task completion time and collisions into\nobjects. Some evidence is shown that the method might increase virtual reality\n(VR) sickness, as measured by the simulator sickness questionnaire (SSQ). We\nconclude that further adjustments are necessary in order to render the method\nviable."}
{"id": "2508.12355", "pdf": "https://arxiv.org/pdf/2508.12355.pdf", "abs": "https://arxiv.org/abs/2508.12355", "title": "Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering", "authors": ["Eviatar Nachshoni", "Arie Cattan", "Shmuel Amar", "Ori Shapira", "Ido Dagan"], "categories": ["cs.CL"], "comment": "no comments", "summary": "Large Language Models (LLMs) have demonstrated strong performance in question\nanswering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a\nquestion may have several valid answers, remains challenging. Traditional QA\nsettings often assume consistency across evidences, but MAQA can involve\nconflicting answers. Constructing datasets that reflect such conflicts is\ncostly and labor-intensive, while existing benchmarks often rely on synthetic\ndata, restrict the task to yes/no questions, or apply unverified automated\nannotation. To advance research in this area, we extend the conflict-aware MAQA\nsetting to require models not only to identify all valid answers, but also to\ndetect specific conflicting answer pairs, if any. To support this task, we\nintroduce a novel cost-effective methodology for leveraging fact-checking\ndatasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware\nMAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate\neight high-end LLMs on NATCONFQA, revealing their fragility in handling various\ntypes of conflicts and the flawed strategies they employ to resolve them."}
{"id": "2508.12946", "pdf": "https://arxiv.org/pdf/2508.12946.pdf", "abs": "https://arxiv.org/abs/2508.12946", "title": "Insights from Interviews with Teachers and Students on the Use of a Social Robot in Computer Science Class in Sixth Grade", "authors": ["Ann-Sophie Schenk", "Stefan Schiffer", "Heqiu Song"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "In this paper we report on first insights from interviews with teachers and\nstudents on using social robots in computer science class in sixth grade. Our\nfocus is on learning about requirements and potential applications. We are\nparticularly interested in getting both perspectives, the teachers' and the\nlearners' view on how robots could be used and what features they should or\nshould not have. Results show that teachers as well as students are very open\nto robots in the classroom. However, requirements are partially quite\nheterogeneous among the groups. This leads to complex design challenges which\nwe discuss at the end of this paper."}
{"id": "2508.12387", "pdf": "https://arxiv.org/pdf/2508.12387.pdf", "abs": "https://arxiv.org/abs/2508.12387", "title": "ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models", "authors": ["Yuanfeng Xu", "Zehui Dai", "Jian Liang", "Jiapeng Guan", "Guangrun Wang", "Liang Lin", "Xiaohui Lv"], "categories": ["cs.CL"], "comment": "16pages, 3 figures", "summary": "Small Language Models (SLMs) are a cost-effective alternative to Large\nLanguage Models (LLMs), but often struggle with complex reasoning due to their\nlimited capacity and a tendency to produce mistakes or inconsistent answers\nduring multi-step reasoning. Existing efforts have improved SLM performance,\nbut typically at the cost of one or more of three key aspects: (1) reasoning\ncapability, due to biased supervision that filters out negative reasoning paths\nand limits learning from errors; (2) autonomy, due to over-reliance on\nexternally generated reasoning signals; and (3) generalization, which suffers\nwhen models overfit to teacher-specific patterns. In this paper, we introduce\nReaLM, a reinforcement learning framework for robust and self-sufficient\nreasoning in vertical domains. To enhance reasoning capability, we propose\nMulti-Route Process Verification (MRPV), which contrasts both positive and\nnegative reasoning paths to extract decisive patterns. To reduce reliance on\nexternal guidance and improve autonomy, we introduce Enabling Autonomy via\nAsymptotic Induction (EAAI), a training strategy that gradually fades external\nsignals. To improve generalization, we apply guided chain-of-thought\ndistillation to encode domain-specific rules and expert knowledge into SLM\nparameters, making them part of what the model has learned. Extensive\nexperiments on both vertical and general reasoning tasks demonstrate that ReaLM\nsignificantly improves SLM performance across aspects (1)-(3) above."}
{"id": "2508.13051", "pdf": "https://arxiv.org/pdf/2508.13051.pdf", "abs": "https://arxiv.org/abs/2508.13051", "title": "Investigating VR Accessibility Reviews for Users with Disabilities: A Qualitative Analysis", "authors": ["Yi Wang", "Chetan Arora", "Xiao Liu", "Thuong Hoang", "ZHengxin Zhang", "Henry Been Lirn Duh", "John Grundy"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Accessibility reviews provide valuable insights into both the limitations and\nbenefits experienced by users with disabilities when using virtual reality (VR)\napplications. However, a comprehensive investigation into VR accessibility for\nusers with disabilities is still lacking. To fill this gap, this study analyzes\nuser reviews from the Meta and Steam stores of VR apps, focusing on the\nreported issues affecting users with disabilities. We applied selection\ncriteria to 1,367,419 reviews from the top 40, the 20 most popular, and the 40\nlowest-rated VR applications on both platforms. In total, 1,076 (0.078%) VR\naccessibility reviews referenced various disabilities across 100 VR\napplications. These applications were categorized into Action, Sports, Social,\nPuzzle, Horror, and Simulation, with Action receiving the highest number of\naccessibility related-reviews. We identified 16 different types of disabilities\nacross six categories. Furthermore, we examined the causes of accessibility\nissues as reported by users with disabilities. Overall, VR accessibility\nreviews were predominantly under-supported."}
{"id": "2508.12393", "pdf": "https://arxiv.org/pdf/2508.12393.pdf", "abs": "https://arxiv.org/abs/2508.12393", "title": "MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph", "authors": ["Duzhen Zhang", "Zixiao Wang", "Zhong-Zhi Li", "Yahan Yu", "Shuncheng Jia", "Jiahua Dong", "Haotian Xu", "Xing Wu", "Yingying Zhang", "Tielin Zhang", "Jie Yang", "Xiuying Chen", "Le Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid expansion of medical literature presents growing challenges for\nstructuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)\noffer a promising solution by enabling efficient retrieval, automated\nreasoning, and knowledge discovery. However, current KG construction methods\noften rely on supervised pipelines with limited generalizability or naively\naggregate outputs from Large Language Models (LLMs), treating biomedical\ncorpora as static and ignoring the temporal dynamics and contextual uncertainty\nof evolving knowledge. To address these limitations, we introduce MedKGent, a\nLLM agent framework for constructing temporally evolving medical KGs.\nLeveraging over 10 million PubMed abstracts published between 1975 and 2023, we\nsimulate the emergence of biomedical knowledge via a fine-grained daily time\nseries. MedKGent incrementally builds the KG in a day-by-day manner using two\nspecialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor\nAgent identifies knowledge triples and assigns confidence scores via\nsampling-based estimation, which are used to filter low-confidence extractions\nand inform downstream processing. The Constructor Agent incrementally\nintegrates the retained triples into a temporally evolving graph, guided by\nconfidence scores and timestamps to reinforce recurring knowledge and resolve\nconflicts. The resulting KG contains 156,275 entities and 2,971,384 relational\ntriples. Quality assessments by two SOTA LLMs and three domain experts\ndemonstrate an accuracy approaching 90\\%, with strong inter-rater agreement. To\nevaluate downstream utility, we conduct RAG across seven medical question\nanswering benchmarks using five leading LLMs, consistently observing\nsignificant improvements over non-augmented baselines. Case studies further\ndemonstrate the KG's value in literature-based drug repurposing via\nconfidence-aware causal inference."}
{"id": "2508.13088", "pdf": "https://arxiv.org/pdf/2508.13088.pdf", "abs": "https://arxiv.org/abs/2508.13088", "title": "Seeing the Many: Exploring Parameter Distributions Conditioned on Features in Surrogates", "authors": ["Xiaohan Wang", "Zhimin Li", "Joshua A. Levine", "Matthew Berger"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Recently, neural surrogate models have emerged as a compelling alternative to\ntraditional simulation workflows. This is accomplished by modeling the\nunderlying function of scientific simulations, removing the need to run\nexpensive simulations. Beyond just mapping from input parameter to output,\nsurrogates have also been shown useful for inverse problems: output to input\nparameters. Inverse problems can be understood as search, where we aim to find\nparameters whose surrogate outputs contain a specified feature. Yet finding\nthese parameters can be costly, especially for high-dimensional parameter\nspaces. Thus, existing surrogate-based solutions primarily focus on finding a\nsmall set of matching parameters, in the process overlooking the broader\npicture of plausible parameters. Our work aims to model and visualize the\ndistribution of possible input parameters that produce a given output feature.\nTo achieve this goal, we aim to address two challenges: (1) the approximation\nerror inherent in the surrogate model and (2) forming the parameter\ndistribution in an interactive manner. We model error via density estimation,\nreporting high density only if a given parameter configuration is close to\ntraining parameters, measured both over the input and output space. Our density\nestimate is used to form a prior belief on parameters, and when combined with a\nlikelihood on features, gives us an efficient way to sample plausible parameter\nconfigurations that generate a target output feature. We demonstrate the\nusability of our solution through a visualization interface by performing\nfeature-driven parameter analysis over the input parameter space of three\nsimulation datasets. Source code is available at\nhttps://github.com/matthewberger/seeing-the-many"}
{"id": "2508.12405", "pdf": "https://arxiv.org/pdf/2508.12405.pdf", "abs": "https://arxiv.org/abs/2508.12405", "title": "Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing", "authors": ["Zilong Bai", "Zihan Xu", "Cong Sun", "Chengxi Zang", "H. Timothy Bunnell", "Catherine Sinfield", "Jacqueline Rutter", "Aaron Thomas Martinez", "L. Charles Bailey", "Mark Weiner", "Thomas R. Campion", "Thomas Carton", "Christopher B. Forrest", "Rainu Kaushal", "Fei Wang", "Yifan Peng"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication in npj Health Systems", "summary": "Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)\nremains challenging due to its myriad symptoms that evolve over long- and\nvariable-time intervals. To address this issue, we developed a hybrid natural\nlanguage processing pipeline that integrates rule-based named entity\nrecognition with BERT-based assertion detection modules for PASC-symptom\nextraction and assertion detection from clinical notes. We developed a\ncomprehensive PASC lexicon with clinical specialists. From 11 health systems of\nthe RECOVER initiative network across the U.S., we curated 160 intake progress\nnotes for model development and evaluation, and collected 47,654 progress notes\nfor a population-level prevalence study. We achieved an average F1 score of\n0.82 in one-site internal validation and 0.76 in 10-site external validation\nfor assertion detection. Our pipeline processed each note at $2.448\\pm 0.812$\nseconds on average. Spearman correlation tests showed $\\rho >0.83$ for positive\nmentions and $\\rho >0.72$ for negative ones, both with $P <0.0001$. These\ndemonstrate the effectiveness and efficiency of our models and their potential\nfor improving PASC diagnosis."}
{"id": "2312.12399", "pdf": "https://arxiv.org/pdf/2312.12399.pdf", "abs": "https://arxiv.org/abs/2312.12399", "title": "Development and User Experiences of a Novel Virtual Reality Task for Poststroke Visuospatial Neglect: An Exploratory Pilot Study", "authors": ["Andrew Danso", "Patti Nijhuis", "Alessandro Ansani", "Martin Hartmann", "Gulnara Minkkinen", "Geoff Luck", "Joshua S. Bamford", "Sarah Faber", "Kat R. Agres", "Solange Glasser", "Teppo SÃ¤rkÃ¤mÃ¶", "Rebekah Rousi", "Marc R. Thompson"], "categories": ["cs.HC"], "comment": "19 pages, 5 figures, 4 tables", "summary": "Background: Visuospatial neglect (VSN) affects spatial awareness, leading to\nfunctional and motor challenges. This case study explores virtual reality (VR)\nas a potential complementary tool for VSN rehabilitation.\n  Objective: Specifically, we aim to explore the initial experiences of\npatients and physiotherapists engaging with a novel protocol, using an\naudiovisual cue task to support VSN rehabilitation.\n  Methods: A preliminary VR task integrating audiovisual cues was co-designed\nwith 2 physiotherapists. The task was then tested with 2 patients with VSN over\n12 sessions. The intervention focused on engaging neglected spatial areas, with\nphysiotherapists adapting the task to individual needs and monitoring\nresponses.\n  Results: Initial testing with 2 trainee physiotherapists indicated high\nusability, engagement, and perceived safety. Two patients with VSN completed 12\nVR sessions. For Patient A, completion times increased following the\nintroduction of an audio cue, though modeling indicated a nonsignificant linear\ntrend (beta = 0.08; P = .33) and a marginally significant downward curvature\n(beta = -0.001; P = .08). In contrast, Patient B showed a significant linear\ndecrease in completion times (beta = -0.53; P = .009), with a quadratic trend\nindicating a performance minimum around session 10 (B = 0.007; P = .04).\nIntraweek variability also decreased. Motor scores (Box and Block Test and\n9-Hole Peg Test) remained stable, and subjective feedback indicated improved\nmobility confidence and positive task engagement.\n  Conclusions: Further research with larger cohorts is needed to confirm the VR\ntask's utility and refine the intervention."}
{"id": "2508.12407", "pdf": "https://arxiv.org/pdf/2508.12407.pdf", "abs": "https://arxiv.org/abs/2508.12407", "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads", "authors": ["Zhuorui Liu", "Chen Zhang", "Dawei Song"], "categories": ["cs.CL"], "comment": "5 pages, 4 figures", "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance."}
{"id": "2410.12268", "pdf": "https://arxiv.org/pdf/2410.12268.pdf", "abs": "https://arxiv.org/abs/2410.12268", "title": "VisAnatomy: An SVG Chart Corpus with Fine-Grained Semantic Labels", "authors": ["Chen Chen", "Hannah K. Bako", "Peihong Yu", "John Hooker", "Jeffrey Joyal", "Simon C. Wang", "Samuel Kim", "Jessica Wu", "Aoxue Ding", "Lara Sandeep", "Alex Chen", "Chayanika Sinha", "Zhicheng Liu"], "categories": ["cs.HC"], "comment": "Will appear at IEEE VIS 2025 conference and TVCG", "summary": "Chart corpora, which comprise data visualizations and their semantic labels,\nare crucial for advancing visualization research. However, the labels in most\nexisting corpora are high-level (e.g., chart types), hindering their utility\nfor broader applications in the era of AI. In this paper, we contribute\nVISANATOMY, a corpus containing 942 real-world SVG charts produced by over 50\ntools, encompassing 40 chart types and featuring structural and stylistic\ndesign variations. Each chart is augmented with multi-level fine-grained labels\non its semantic components, including each graphical element's type, role, and\nposition, hierarchical groupings of elements, group layouts, and visual\nencodings. In total, VISANATOMY provides labels for more than 383k graphical\nelements. We demonstrate the richness of the semantic labels by comparing\nVISANATOMY with existing corpora. We illustrate its usefulness through four\napplications: semantic role inference for SVG elements, chart semantic\ndecomposition, chart type classification, and content navigation for\naccessibility. Finally, we discuss research opportunities to further improve\nVISANATOMY."}
{"id": "2508.12411", "pdf": "https://arxiv.org/pdf/2508.12411.pdf", "abs": "https://arxiv.org/abs/2508.12411", "title": "The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases", "authors": ["Emanuel Z. Fenech-Borg", "Tilen P. Meznaric-Kos", "Milica D. Lekovic-Bojovic", "Arni J. Hentze-Djurhuus"], "categories": ["cs.CL", "I.2.7; K.4.1; H.3.3"], "comment": "10 pages, 5 figures, IEEE conference format, submitted to [Conference\n  Name]", "summary": "Large language models (LLMs) are deployed globally, yet their underlying\ncultural and ethical assumptions remain underexplored. We propose the notion of\na \"cultural gene\" -- a systematic value orientation that LLMs inherit from\ntheir training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200\nprompts targeting two classic cross-cultural dimensions:\nIndividualism-Collectivism (IDV) and Power Distance (PDI). Using standardized\nzero-shot prompts, we compare a Western-centric model (GPT-4) and an\nEastern-centric model (ERNIE Bot). Human annotation shows significant and\nconsistent divergence across both dimensions. GPT-4 exhibits individualistic\nand low-power-distance tendencies (IDV score approx 1.21; PDI score approx\n-1.05), while ERNIE Bot shows collectivistic and higher-power-distance\ntendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically\nsignificant (p < 0.001). We further compute a Cultural Alignment Index (CAI)\nagainst Hofstede's national scores and find GPT-4 aligns more closely with the\nUSA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns\nmore closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative\nanalyses of dilemma resolution and authority-related judgments illustrate how\nthese orientations surface in reasoning. Our results support the view that LLMs\nfunction as statistical mirrors of their cultural corpora and motivate\nculturally aware evaluation and deployment to avoid algorithmic cultural\nhegemony."}
{"id": "2504.12488", "pdf": "https://arxiv.org/pdf/2504.12488.pdf", "abs": "https://arxiv.org/abs/2504.12488", "title": "Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process", "authors": ["Mohi Reza", "Jeb Thomas-Mitchell", "Peter Dushniku", "Nathan Laundry", "Joseph Jay Williams", "Anastasia Kuzminykh"], "categories": ["cs.HC", "cs.AI", "H.5.2; I.2.7; I.2.6; I.7.2"], "comment": null, "summary": "As generative AI tools like ChatGPT become integral to everyday writing,\ncritical questions arise about how to preserve writers' sense of agency and\nownership when using these tools. Yet, a systematic understanding of how AI\nassistance affects different aspects of the writing process - and how this\nshapes writers' agency - remains underexplored. To address this gap, we\nconducted a systematic review of 109 HCI papers using the PRISMA approach. From\nthis literature, we identify four overarching design strategies for AI writing\nsupport: structured guidance, guided exploration, active co-writing, and\ncritical feedback - mapped across the four key cognitive processes in writing:\nplanning, translating, reviewing, and monitoring. We complement this analysis\nwith interviews of 15 writers across diverse domains. Our findings reveal that\nwriters' desired levels of AI intervention vary across the writing process:\ncontent-focused writers (e.g., academics) prioritize ownership during planning,\nwhile form-focused writers (e.g., creatives) value control over translating and\nreviewing. Writers' preferences are also shaped by contextual goals, values,\nand notions of originality and authorship. By examining when ownership matters,\nwhat writers want to own, and how AI interactions shape agency, we surface both\nalignment and gaps between research and user needs. Our findings offer\nactionable design guidance for developing human-centered writing tools for\nco-writing with AI, on human terms."}
{"id": "2508.12448", "pdf": "https://arxiv.org/pdf/2508.12448.pdf", "abs": "https://arxiv.org/abs/2508.12448", "title": "Uncovering Emergent Physics Representations Learned In-Context by Large Language Models", "authors": ["Yeongwoo Song", "Jaeyong Bae", "Dong-Kyum Kim", "Hawoong Jeong"], "categories": ["cs.CL", "cs.LG"], "comment": "17 pages, 10 figures", "summary": "Large language models (LLMs) exhibit impressive in-context learning (ICL)\nabilities, enabling them to solve wide range of tasks via textual prompts\nalone. As these capabilities advance, the range of applicable domains continues\nto expand significantly. However, identifying the precise mechanisms or\ninternal structures within LLMs that allow successful ICL across diverse,\ndistinct classes of tasks remains elusive. Physics-based tasks offer a\npromising testbed for probing this challenge. Unlike synthetic sequences such\nas basic arithmetic or symbolic equations, physical systems provide\nexperimentally controllable, real-world data based on structured dynamics\ngrounded in fundamental principles. This makes them particularly suitable for\nstudying the emergent reasoning behaviors of LLMs in a realistic yet tractable\nsetting. Here, we mechanistically investigate the ICL ability of LLMs,\nespecially focusing on their ability to reason about physics. Using a dynamics\nforecasting task in physical systems as a proxy, we evaluate whether LLMs can\nlearn physics in context. We first show that the performance of dynamics\nforecasting in context improves with longer input contexts. To uncover how such\ncapability emerges in LLMs, we analyze the model's residual stream activations\nusing sparse autoencoders (SAEs). Our experiments reveal that the features\ncaptured by SAEs correlate with key physical variables, such as energy. These\nfindings demonstrate that meaningful physical concepts are encoded within LLMs\nduring in-context learning. In sum, our work provides a novel case study that\nbroadens our understanding of how LLMs learn in context."}
{"id": "2507.20655", "pdf": "https://arxiv.org/pdf/2507.20655.pdf", "abs": "https://arxiv.org/abs/2507.20655", "title": "CoGrader: Transforming Instructors' Assessment of Project Reports through Collaborative LLM Integration", "authors": ["Zixin Chen", "Jiachen Wang", "Yumeng Li", "Haobo Li", "Chuhan Shi", "Rong Zhang", "Huamin Qu"], "categories": ["cs.HC"], "comment": null, "summary": "Grading project reports are increasingly significant in today's educational\nlandscape, where they serve as key assessments of students' comprehensive\nproblem-solving abilities. However, it remains challenging due to the\nmultifaceted evaluation criteria involved, such as creativity and\npeer-comparative achievement. Meanwhile, instructors often struggle to maintain\nfairness throughout the time-consuming grading process. Recent advances in AI,\nparticularly large language models, have demonstrated potential for automating\nsimpler grading tasks, such as assessing quizzes or basic writing quality.\nHowever, these tools often fall short when it comes to complex metrics, like\ndesign innovation and the practical application of knowledge, that require an\ninstructor's educational insights into the class situation. To address this\nchallenge, we conducted a formative study with six instructors and developed\nCoGrader, which introduces a novel grading workflow combining human-LLM\ncollaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader\nwas found effective in improving grading efficiency and consistency while\nproviding reliable peer-comparative feedback to students. We also discuss\ndesign insights and ethical considerations for the development of human-AI\ncollaborative grading systems."}
{"id": "2508.12458", "pdf": "https://arxiv.org/pdf/2508.12458.pdf", "abs": "https://arxiv.org/abs/2508.12458", "title": "M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following", "authors": ["Ruirui Gao", "Emily Johnson", "Bowen Tan", "Yanfei Qian"], "categories": ["cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) hold immense potential for complex\nmultimodal instruction following, yet their development is often hindered by\nthe high cost and inconsistency of human annotation required for effective\nfine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)\nand existing preference optimization methods like RLHF and DPO frequently\nstruggle to efficiently leverage the model's own generation space to identify\nhighly informative \"hard negative\" samples. To address these challenges, we\npropose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and\ndata-efficient method designed to enhance LVLMs' capabilities in visual\ninstruction following. M3PO intelligently selects the most \"learning-valuable\"\npreference sample pairs from a diverse pool of LVLM-generated candidates. This\nselection is driven by a sophisticated mechanism that integrates two crucial\nsignals: a Multimodal Alignment Score (MAS) to assess external quality and the\nmodel's Self-Consistency / Confidence (log-probability) to gauge internal\nbelief. These are combined into a novel M3P-Score, which specifically\nidentifies preferred responses and challenging dispreferred responses that the\nmodel might confidently generate despite being incorrect. These high-quality\npreference pairs are then used for efficient Direct Preference Optimization\n(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our\nextensive experiments demonstrate that M3PO consistently outperforms strong\nbaselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a\ncomprehensive suite of multimodal instruction following benchmarks (MME-Bench,\nPOPE, IFT, Human Pref. Score)."}
{"id": "2508.01235", "pdf": "https://arxiv.org/pdf/2508.01235.pdf", "abs": "https://arxiv.org/abs/2508.01235", "title": "NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration", "authors": ["Yaxin Hu", "Arissa J. Sato", "Jingxin Du", "Chenming Ye", "Anjun Zhu", "Pragathi Praveena", "Bilge Mutlu"], "categories": ["cs.HC", "cs.RO", "68"], "comment": null, "summary": "Robotic telepresence enables users to navigate and experience remote\nenvironments. However, effective navigation and situational awareness depend on\nusers' prior knowledge of the environment, limiting the usefulness of these\nsystems for exploring unfamiliar places. We explore how integrating\nlocation-aware LLM-based narrative capabilities into a mobile robot can support\nremote exploration. We developed a prototype system, called NarraGuide, that\nprovides narrative guidance for users to explore and learn about a remote place\nthrough a dialogue-based interface. We deployed our prototype in a geology\nmuseum, where remote participants (n=20) used the robot to tour the museum. Our\nfindings reveal how users perceived the robot's role, engaged in dialogue in\nthe tour, and expressed preferences for bystander encountering. Our work\ndemonstrates the potential of LLM-enabled robotic capabilities to deliver\nlocation-aware narrative guidance and enrich the experience of exploring remote\nenvironments."}
{"id": "2508.12459", "pdf": "https://arxiv.org/pdf/2508.12459.pdf", "abs": "https://arxiv.org/abs/2508.12459", "title": "LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages", "authors": ["Alham Fikri Aji", "Trevor Cohn"], "categories": ["cs.CL"], "comment": null, "summary": "As one of the world's most populous countries, with 700 languages spoken,\nIndonesia is behind in terms of NLP progress. We introduce LoraxBench, a\nbenchmark that focuses on low-resource languages of Indonesia and covers 6\ndiverse tasks: reading comprehension, open-domain QA, language inference,\ncausal reasoning, translation, and cultural QA. Our dataset covers 20\nlanguages, with the addition of two formality registers for three languages. We\nevaluate a diverse set of multilingual and region-focused LLMs and found that\nthis benchmark is challenging. We note a visible discrepancy between\nperformance in Indonesian and other languages, especially the low-resource\nones. There is no clear lead when using a region-specific model as opposed to\nthe general multilingual model. Lastly, we show that a change in register\naffects model performance, especially with registers not commonly found in\nsocial media, such as high-level politeness `Krama' Javanese."}
{"id": "2508.02133", "pdf": "https://arxiv.org/pdf/2508.02133.pdf", "abs": "https://arxiv.org/abs/2508.02133", "title": "Hierarchical MoE: Continuous Multimodal Emotion Recognition with Incomplete and Asynchronous Inputs", "authors": ["Yitong Zhu", "Lei Han", "Guanxuan Jiang", "PengYuan Zhou", "Yuyang Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Multimodal emotion recognition (MER) is crucial for human-computer\ninteraction, yet real-world challenges like dynamic modality incompleteness and\nasynchrony severely limit its robustness. Existing methods often assume\nconsistently complete data or lack dynamic adaptability. To address these\nlimitations, we propose a novel Hi-MoE~(Hierarchical Mixture-of-Experts)\nframework for robust continuous emotion prediction. This framework employs a\ndual-layer expert structure. A Modality Expert Bank utilizes soft routing to\ndynamically handle missing modalities and achieve robust information fusion. A\nsubsequent Emotion Expert Bank leverages differential-attention routing to\nflexibly attend to emotional prototypes, enabling fine-grained emotion\nrepresentation. Additionally, a cross-modal alignment module explicitly\naddresses temporal shifts and semantic inconsistencies between modalities.\nExtensive experiments on benchmark datasets DEAP and DREAMER demonstrate our\nmodel's state-of-the-art performance in continuous emotion regression,\nshowcasing exceptional robustness under challenging conditions such as dynamic\nmodality absence and asynchronous sampling. This research significantly\nadvances the development of intelligent emotion systems adaptable to complex\nreal-world environments."}
{"id": "2508.12461", "pdf": "https://arxiv.org/pdf/2508.12461.pdf", "abs": "https://arxiv.org/abs/2508.12461", "title": "Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models", "authors": ["Ziqian Bi", "Keyu Chen", "Chiung-Yi Tseng", "Danyang Zhang", "Tianyang Wang", "Hongying Luo", "Lu Chen", "Junming Huang", "Jibin Guan", "Junfeng Hao", "Junhao Song"], "categories": ["cs.CL"], "comment": null, "summary": "In August 2025, OpenAI released GPT-OSS models, its first open weight large\nlanguage models since GPT-2 in 2019, comprising two mixture of experts\narchitectures with 120B and 20B parameters. We evaluated both variants against\nsix contemporary open source large language models ranging from 14.7B to 235B\nparameters, representing both dense and sparse designs, across ten benchmarks\ncovering general knowledge, mathematical reasoning, code generation,\nmultilingual understanding, and conversational ability. All models were tested\nin unquantised form under standardised inference settings, with statistical\nvalidation using McNemars test and effect size analysis. Results show that\ngpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such\nas HumanEval and MMLU, despite requiring substantially less memory and energy\nper response. Both models demonstrate mid-tier overall performance within the\ncurrent open source landscape, with relative strength in code generation and\nnotable weaknesses in multilingual tasks. These findings provide empirical\nevidence that scaling in sparse architectures may not yield proportional\nperformance gains, underscoring the need for further investigation into\noptimisation strategies and informing more efficient model selection for future\nopen source deployments."}
{"id": "2508.06801", "pdf": "https://arxiv.org/pdf/2508.06801.pdf", "abs": "https://arxiv.org/abs/2508.06801", "title": "Understanding Pedestrian Gesture Misrecognition: Insights from Vision-Language Model Reasoning", "authors": ["Tram Thi Minh Tran", "Xinyan Yu", "Callum Parker", "Julie Stephany Berrio Perez", "Stewart Worrall", "Martin Tomitsch"], "categories": ["cs.HC"], "comment": null, "summary": "Pedestrian gestures play an important role in traffic communication,\nparticularly in interactions with autonomous vehicles (AVs), yet their subtle,\nambiguous, and context-dependent nature poses persistent challenges for machine\ninterpretation. This study investigates these challenges by using GPT-4V, a\nvision-language model, not as a performance benchmark but as a diagnostic tool\nto reveal patterns and causes of gesture misrecognition. We analysed a public\ndataset of pedestrian-vehicle interactions, combining manual video review with\nthematic analysis of the model's qualitative reasoning. This dual approach\nsurfaced recurring factors influencing misrecognition, including gesture\nvisibility, pedestrian behaviour, interaction context, and environmental\nconditions. The findings suggest practical considerations for gesture design,\nincluding the value of salience and contextual redundancy, and highlight\nopportunities to improve AV recognition systems through richer context\nmodelling and uncertainty-aware interpretations. While centred on AV-pedestrian\ninteraction, the method and insights are applicable to other domains where\nmachines interpret human gestures, such as wearable AR and assistive\ntechnologies."}
{"id": "2508.12482", "pdf": "https://arxiv.org/pdf/2508.12482.pdf", "abs": "https://arxiv.org/abs/2508.12482", "title": "The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping", "authors": ["Xiaomeng Zhu", "R. Thomas McCoy", "Robert Frank"], "categories": ["cs.CL"], "comment": null, "summary": "Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use\nthe syntactic environments in which a verb occurs to learn its meaning. In this\npaper, we examine whether large language models exhibit a similar behavior. We\ndo this by training RoBERTa and GPT-2 on perturbed datasets where syntactic\ninformation is ablated. Our results show that models' verb representation\ndegrades more when syntactic cues are removed than when co-occurrence\ninformation is removed. Furthermore, the representation of mental verbs, for\nwhich syntactic bootstrapping has been shown to be particularly crucial in\nhuman verb learning, is more negatively impacted in such training regimes than\nphysical verbs. In contrast, models' representation of nouns is affected more\nwhen co-occurrences are distorted than when syntax is distorted. In addition to\nreinforcing the important role of syntactic bootstrapping in verb learning, our\nresults demonstrated the viability of testing developmental hypotheses on a\nlarger scale through manipulating the learning environments of large language\nmodels."}
{"id": "2508.08128", "pdf": "https://arxiv.org/pdf/2508.08128.pdf", "abs": "https://arxiv.org/abs/2508.08128", "title": "Fuzzy Ontology Embeddings and Visual Query Building for Ontology Exploration", "authors": ["Vladimir Zhurov", "John Kausch", "Kamran Sedig", "Mostafa Milani"], "categories": ["cs.HC"], "comment": "Journal submission", "summary": "Ontologies play a central role in structuring knowledge across domains,\nsupporting tasks such as reasoning, data integration, and semantic search.\nHowever, their large size and complexity, particularly in fields such as\nbiomedicine, computational biology, law, and engineering, make them difficult\nfor non-experts to navigate. Formal query languages such as SPARQL offer\nexpressive access but require users to understand the ontology's structure and\nsyntax. In contrast, visual exploration tools and basic keyword-based search\ninterfaces are easier to use but often lack flexibility and expressiveness. We\nintroduce FuzzyVis, a proof-of-concept system that enables intuitive and\nexpressive exploration of complex ontologies. FuzzyVis integrates two key\ncomponents: a fuzzy logic-based querying model built on fuzzy ontology\nembeddings, and an interactive visual interface for building and interpreting\nqueries. Users can construct new composite concepts by selecting and combining\nexisting ontology concepts using logical operators such as conjunction,\ndisjunction, and negation. These composite concepts are matched against the\nontology using fuzzy membership-based embeddings, which capture degrees of\nmembership and support approximate, concept-level similarity search. The visual\ninterface supports browsing, query composition, and partial search without\nrequiring formal syntax. By combining fuzzy semantics with embedding-based\nreasoning, FuzzyVis enables flexible interpretation, efficient computation, and\nexploratory learning. Case studies demonstrate how FuzzyVis supports subtle\ninformation needs and helps users uncover relevant concepts in large, complex\nontologies."}
{"id": "2508.12495", "pdf": "https://arxiv.org/pdf/2508.12495.pdf", "abs": "https://arxiv.org/abs/2508.12495", "title": "Mitigating Hallucinations in Large Language Models via Causal Reasoning", "authors": ["Yuangang Li", "Yiqing Shen", "Yi Nian", "Jiechao Gao", "Ziyi Wang", "Chenxiao Yu", "Shawn Li", "Jie Wang", "Xiyang Hu", "Yue Zhao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) exhibit logically inconsistent hallucinations\nthat appear coherent yet violate reasoning principles, with recent research\nsuggesting an inverse relationship between causal reasoning capabilities and\nsuch hallucinations. However, existing reasoning approaches in LLMs, such as\nChain-of-Thought (CoT) and its graph-based variants, operate at the linguistic\ntoken level rather than modeling the underlying causal relationships between\nvariables, lacking the ability to represent conditional independencies or\nsatisfy causal identification assumptions. To bridge this gap, we introduce\ncausal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning\nframework that trains LLMs to explicitly construct variable-level directed\nacyclic graph (DAG) and then perform reasoning over it. Moreover, we present a\ndataset comprising 25,368 samples (CausalDR), where each sample includes an\ninput question, explicit causal DAG, graph-based reasoning trace, and validated\nanswer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves\nthe causal reasoning capability with the state-of-the-art 95.33% accuracy on\nCLADDER (surpassing human performance of 94.8% for the first time) and reduces\nthe hallucination on HaluEval with 10% improvements. It demonstrates that\nexplicit causal structure modeling in LLMs can effectively mitigate logical\ninconsistencies in LLM outputs. Code is available at\nhttps://github.com/MrLYG/CDCR-SFT."}
{"id": "2508.10286", "pdf": "https://arxiv.org/pdf/2508.10286.pdf", "abs": "https://arxiv.org/abs/2508.10286", "title": "Artificial Emotion: A Survey of Theories and Debates on Realising Emotion in Artificial Intelligence", "authors": ["Yupei Li", "Qiyang Sun", "Michelle Schlicher", "Yee Wen Lim", "BjÃ¶rn W. Schuller"], "categories": ["cs.HC"], "comment": null, "summary": "Affective Computing (AC) has enabled Artificial Intelligence (AI) systems to\nrecognise, interpret, and respond to human emotions - a capability also known\nas Artificial Emotional Intelligence (AEI). It is increasingly seen as an\nimportant component of Artificial General Intelligence (AGI). We discuss\nwhether in order to peruse this goal, AI benefits from moving beyond emotion\nrecognition and synthesis to develop internal emotion-like states, which we\nterm as Artificial Emotion (AE). This shift potentially allows AI to benefit\nfrom the paradigm of `inner emotions' in ways we - as humans - do. Although\nrecent research shows early signs that AI systems may exhibit AE-like\nbehaviours, a clear framework for how emotions can be realised in AI remains\nunderexplored. In this paper, we discuss potential advantages of AE in AI,\nreview current manifestations of AE in machine learning systems, examine\nemotion-modulated architectures, and summarise mechanisms for modelling and\nintegrating AE into future AI. We also explore the ethical implications and\nsafety risks associated with `emotional' AGI, while concluding with our opinion\non how AE could be beneficial in the future."}
{"id": "2508.12535", "pdf": "https://arxiv.org/pdf/2508.12535.pdf", "abs": "https://arxiv.org/abs/2508.12535", "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection", "authors": ["Seonglae Cho", "Zekun Wu", "Adriano Koshiyama"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "42 pages, 9 tables", "summary": "Sparse Autoencoders (SAEs) can extract interpretable features from large\nlanguage models (LLMs) without supervision. However, their effectiveness in\ndownstream steering tasks is limited by the requirement for contrastive\ndatasets or large activation storage. To address these limitations, we propose\nCorrSteer, which selects features by correlating sample correctness with SAE\nactivations from generated tokens at inference time. This approach uses only\ninference-time activations to extract more relevant features, thereby avoiding\nspurious correlations. It also obtains steering coefficients from average\nactivations, automating the entire pipeline. Our method shows improved task\nperformance on QA, bias mitigation, jailbreaking prevention, and reasoning\nbenchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%\nimprovement in MMLU performance and a +22.9% improvement in HarmBench with only\n4000 samples. Selected features demonstrate semantically meaningful patterns\naligned with each task's requirements, revealing the underlying capabilities\nthat drive performance. Our work establishes correlationbased selection as an\neffective and scalable approach for automated SAE steering across language\nmodel applications."}
{"id": "2501.01212", "pdf": "https://arxiv.org/pdf/2501.01212.pdf", "abs": "https://arxiv.org/abs/2501.01212", "title": "Towards Consumer-Grade Cybersickness Prediction: Multi-Model Alignment for Real-Time Vision-Only Inference", "authors": ["Yitong Zhu", "Zhuowen Liang", "Yiming Wu", "Tangyao Li", "Yuyang Wang"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Cybersickness remains a major obstacle to the widespread adoption of\nimmersive virtual reality (VR), particularly in consumer-grade environments.\nWhile prior methods rely on invasive signals such as electroencephalography\n(EEG) for high predictive accuracy, these approaches require specialized\nhardware and are impractical for real-world applications. In this work, we\npropose a scalable, deployable framework for personalized cybersickness\nprediction leveraging only non-invasive signals readily available from\ncommercial VR headsets, including head motion, eye tracking, and physiological\nresponses. Our model employs a modality-specific graph neural network enhanced\nwith a Difference Attention Module to extract temporal-spatial embeddings\ncapturing dynamic changes across modalities. A cross-modal alignment module\njointly trains the video encoder to learn personalized traits by aligning video\nfeatures with sensor-derived representations. Consequently, the model\naccurately predicts individual cybersickness using only video input during\ninference. Experimental results show our model achieves 88.4\\% accuracy,\nclosely matching EEG-based approaches (89.16\\%), while reducing deployment\ncomplexity. With an average inference latency of 90ms, our framework supports\nreal-time applications, ideal for integration into consumer-grade VR platforms\nwithout compromising personalization or performance. The code will be relesed\nat https://github.com/U235-Aurora/PTGNN."}
{"id": "2508.12591", "pdf": "https://arxiv.org/pdf/2508.12591.pdf", "abs": "https://arxiv.org/abs/2508.12591", "title": "Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning", "authors": ["Yu-Hsuan Fang", "Tien-Hong Lo", "Yao-Ting Sung", "Berlin Chen"], "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": "Accepted at IEEE ASRU 2025", "summary": "Traditional Automated Speaking Assessment (ASA) systems exhibit inherent\nmodality limitations: text-based approaches lack acoustic information while\naudio-based methods miss semantic context. Multimodal Large Language Models\n(MLLM) offer unprecedented opportunities for comprehensive ASA by\nsimultaneously processing audio and text within unified frameworks. This paper\npresents a very first systematic study of MLLM for comprehensive ASA,\ndemonstrating the superior performance of MLLM across the aspects of content\nand language use . However, assessment on the delivery aspect reveals unique\nchallenges, which is deemed to require specialized training strategies. We thus\npropose Speech-First Multimodal Training (SFMT), leveraging a curriculum\nlearning principle to establish more robust modeling foundations of speech\nbefore cross-modal synergetic fusion. A series of experiments on a benchmark\ndataset show MLLM-based systems can elevate the holistic assessment performance\nfrom a PCC value of 0.783 to 0.846. In particular, SFMT excels in the\nevaluation of the delivery aspect, achieving an absolute accuracy improvement\nof 4% over conventional training approaches, which also paves a new avenue for\nASA."}
{"id": "2504.01911", "pdf": "https://arxiv.org/pdf/2504.01911.pdf", "abs": "https://arxiv.org/abs/2504.01911", "title": "Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning", "authors": ["Yinggan Xu", "Hana Kimlee", "Yijia Xiao", "Di Luo"], "categories": ["cs.AI", "cs.CL", "cs.HC", "physics.comp-ph"], "comment": "ICML 2025 Workshop on MAS", "summary": "Large Language Models (LLMs) are playing an increasingly important role in\nphysics research by assisting with symbolic manipulation, numerical\ncomputation, and scientific reasoning. However, ensuring the reliability,\ntransparency, and interpretability of their outputs remains a major challenge.\nIn this work, we introduce a novel multi-agent LLM physicist framework that\nfosters collaboration between AI and human scientists through three key\nmodules: a reasoning module, an interpretation module, and an AI-scientist\ninteraction module. Recognizing that effective physics reasoning demands\nlogical rigor, quantitative accuracy, and alignment with established\ntheoretical models, we propose an interpretation module that employs a team of\nspecialized LLM agents-including summarizers, model builders, visualization\ntools, and testers-to systematically structure LLM outputs into transparent,\nphysically grounded science models. A case study demonstrates that our approach\nsignificantly improves interpretability, enables systematic validation, and\nenhances human-AI collaboration in physics problem-solving and discovery. Our\nwork bridges free-form LLM reasoning with interpretable, executable models for\nscientific analysis, enabling more transparent and verifiable AI-augmented\nresearch."}
{"id": "2508.12630", "pdf": "https://arxiv.org/pdf/2508.12630.pdf", "abs": "https://arxiv.org/abs/2508.12630", "title": "Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context", "authors": ["Maitreyi Chatterjee", "Devansh Agarwal"], "categories": ["cs.CL"], "comment": "Paper is currently in peer review", "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and task\ncompetence in conversational settings. However, their effectiveness in\nmulti-session and long-term interactions is hindered by limited memory\npersistence. Typical retrieval-augmented generation (RAG) systems store\ndialogue history as dense vectors, which capture semantic similarity but\nneglect finer linguistic structures such as syntactic dependencies, discourse\nrelations, and coreference links. We propose Semantic Anchoring, a hybrid\nagentic memory architecture that enriches vector-based storage with explicit\nlinguistic cues to improve recall of nuanced, context-rich exchanges. Our\napproach combines dependency parsing, discourse relation tagging, and\ncoreference resolution to create structured memory entries. Experiments on\nadapted long-term dialogue datasets show that semantic anchoring improves\nfactual recall and discourse coherence by up to 18% over strong RAG baselines.\nWe further conduct ablation studies, human evaluations, and error analysis to\nassess robustness and interpretability."}
{"id": "2504.10905", "pdf": "https://arxiv.org/pdf/2504.10905.pdf", "abs": "https://arxiv.org/abs/2504.10905", "title": "InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation", "authors": ["Yukang Lin", "Yan Hong", "Zunnan Xu", "Xindi Li", "Chao Xu", "Chuanbiao Song", "Ronghui Li", "Haoxing Chen", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang", "Xiu Li"], "categories": ["cs.CV", "cs.HC"], "comment": "Accept to ACMMM 2025", "summary": "Recent video generation research has focused heavily on isolated actions,\nleaving interactive motions-such as hand-face interactions-largely unexamined.\nThese interactions are essential for emerging biometric authentication systems,\nwhich rely on interactive motion-based anti-spoofing approaches. From a\nsecurity perspective, there is a growing need for large-scale, high-quality\ninteractive videos to train and strengthen authentication models. In this work,\nwe introduce a novel paradigm for animating realistic hand-face interactions.\nOur approach simultaneously learns spatio-temporal contact dynamics and\nbiomechanically plausible deformation effects, enabling natural interactions\nwhere hand movements induce anatomically accurate facial deformations while\nmaintaining collision-free contact. To facilitate this research, we present\nInterHF, a large-scale hand-face interaction dataset featuring 18 interaction\npatterns and 90,000 annotated videos. Additionally, we propose InterAnimate, a\nregion-aware diffusion model designed specifically for interaction animation.\nInterAnimate leverages learnable spatial and temporal latents to effectively\ncapture dynamic interaction priors and integrates a region-aware interaction\nmechanism that injects these priors into the denoising process. To the best of\nour knowledge, this work represents the first large-scale effort to\nsystematically study human hand-face interactions. Qualitative and quantitative\nresults show InterAnimate produces highly realistic animations, setting a new\nbenchmark. Code and data will be made public to advance research."}
{"id": "2508.12631", "pdf": "https://arxiv.org/pdf/2508.12631.pdf", "abs": "https://arxiv.org/abs/2508.12631", "title": "Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing", "authors": ["Yiqun Zhang", "Hao Li", "Jianhao Chen", "Hangfan Zhang", "Peng Ye", "Lei Bai", "Shuyue Hu"], "categories": ["cs.CL"], "comment": "Ongoing work", "summary": "Balancing performance and efficiency is a central challenge in large language\nmodel (LLM) advancement. GPT-5 addresses this with test-time routing,\ndynamically assigning queries to either an efficient or a high-capacity model\nduring inference. In this work, we present Avengers-Pro, a test-time routing\nframework that ensembles LLMs of varying capacities and efficiencies, providing\na unified solution for all performance-efficiency tradeoffs. The Avengers-Pro\nembeds and clusters incoming queries, then routes each to the most suitable\nmodel based on a performance-efficiency score. Across 6 challenging benchmarks\nand 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and\nClaude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a\nperformance-efficiency trade-off parameter, it can surpass the strongest single\nmodel (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the\naverage accuracy of the strongest single model at 27% lower cost, and reach\n~90% of that performance at 63% lower cost. Last but not least, it achieves a\nPareto frontier, consistently yielding the highest accuracy for any given cost,\nand the lowest cost for any given accuracy, among all single models. Code is\navailable at https://github.com/ZhangYiqun018/AvengersPro."}
{"id": "2506.24039", "pdf": "https://arxiv.org/pdf/2506.24039.pdf", "abs": "https://arxiv.org/abs/2506.24039", "title": "Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data", "authors": ["Shubhabrata Mukherjee", "Jack Lang", "Obeen Kwon", "Iryna Zenyuk", "Valerie Brogden", "Adam Weber", "Daniela Ushizima"], "categories": ["cs.CV", "cs.HC"], "comment": "This paper has been accepted for presentation at the 59th\n  International Conference on Parallel Processing (ICPP 2025), DRAI workshop", "summary": "Zero-shot and prompt-based models have excelled at visual reasoning tasks by\nleveraging large-scale natural image corpora, but they often fail on sparse and\ndomain-specific scientific image data. We introduce Zenesis, a no-code\ninteractive computer vision platform designed to reduce data readiness\nbottlenecks in scientific imaging workflows. Zenesis integrates lightweight\nmultimodal adaptation for zero-shot inference on raw scientific data,\nhuman-in-the-loop refinement, and heuristic-based temporal enhancement. We\nvalidate our approach on Focused Ion Beam Scanning Electron Microscopy\n(FIB-SEM) datasets of catalyst-loaded membranes. Zenesis outperforms baselines,\nachieving an average accuracy of 0.947, Intersection over Union (IoU) of 0.858,\nand Dice score of 0.923 on amorphous catalyst samples; and 0.987 accuracy,\n0.857 IoU, and 0.923 Dice on crystalline samples. These results represent a\nsignificant performance gain over conventional methods such as Otsu\nthresholding and standalone models like the Segment Anything Model (SAM).\nZenesis enables effective image segmentation in domains where annotated\ndatasets are limited, offering a scalable solution for scientific discovery."}
{"id": "2508.12632", "pdf": "https://arxiv.org/pdf/2508.12632.pdf", "abs": "https://arxiv.org/abs/2508.12632", "title": "Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection", "authors": ["Chi Wang", "Min Gao", "Zongwei Wang", "Junwei Yin", "Kai Shu", "Chenghua Lin"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid development of large language models, the generation of fake\nnews has become increasingly effortless, posing a growing societal threat and\nunderscoring the urgent need for reliable detection methods. Early efforts to\nidentify LLM-generated fake news have predominantly focused on the textual\ncontent itself; however, because much of that content may appear coherent and\nfactually consistent, the subtle traces of falsification are often difficult to\nuncover. Through distributional divergence analysis, we uncover prompt-induced\nlinguistic fingerprints: statistically distinct probability shifts between\nLLM-generated real and fake news when maliciously prompted. Based on this\ninsight, we propose a novel method named Linguistic Fingerprints Extraction\n(LIFE). By reconstructing word-level probability distributions, LIFE can find\ndiscriminative patterns that facilitate the detection of LLM-generated fake\nnews. To further amplify these fingerprint patterns, we also leverage\nkey-fragment techniques that accentuate subtle linguistic differences, thereby\nimproving detection reliability. Our experiments show that LIFE achieves\nstate-of-the-art performance in LLM-generated fake news and maintains high\nperformance in human-written fake news. The code and data are available at\nhttps://anonymous.4open.science/r/LIFE-E86A."}
{"id": "2507.19196", "pdf": "https://arxiv.org/pdf/2507.19196.pdf", "abs": "https://arxiv.org/abs/2507.19196", "title": "Towards Multimodal Social Conversations with Robots: Using Vision-Language Models", "authors": ["Ruben Janssens", "Tony Belpaeme"], "categories": ["cs.RO", "cs.CL", "cs.HC"], "comment": "Accepted at the workshop \"Human - Foundation Models Interaction: A\n  Focus On Multimodal Information\" (FoMo-HRI) at IEEE RO-MAN 2025 (Camera-ready\n  version)", "summary": "Large language models have given social robots the ability to autonomously\nengage in open-domain conversations. However, they are still missing a\nfundamental social skill: making use of the multiple modalities that carry\nsocial interactions. While previous work has focused on task-oriented\ninteractions that require referencing the environment or specific phenomena in\nsocial interactions such as dialogue breakdowns, we outline the overall needs\nof a multimodal system for social conversations with robots. We then argue that\nvision-language models are able to process this wide range of visual\ninformation in a sufficiently general manner for autonomous social robots. We\ndescribe how to adapt them to this setting, which technical challenges remain,\nand briefly discuss evaluation practices."}
{"id": "2508.12662", "pdf": "https://arxiv.org/pdf/2508.12662.pdf", "abs": "https://arxiv.org/abs/2508.12662", "title": "Breaking Language Barriers: Equitable Performance in Multilingual Language Models", "authors": ["Tanay Nagar", "Grigorii Khvatskii", "Anna Sokol", "Nitesh V. Chawla"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as a non-archival work-in-progress paper at the NAACL 2025\n  Student Research Workshop", "summary": "Cutting-edge LLMs have emerged as powerful tools for multilingual\ncommunication and understanding. However, LLMs perform worse in Common Sense\nReasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi\nor Swahili compared to high-resource languages (HRLs) like English. Equalizing\nthis inconsistent access to quality LLM outputs is crucial to ensure fairness\nfor speakers of LRLs and across diverse linguistic communities. In this paper,\nwe propose an approach to bridge this gap in LLM performance. Our approach\ninvolves fine-tuning an LLM on synthetic code-switched text generated using\ncontrolled language-mixing methods. We empirically demonstrate that fine-tuning\nLLMs on synthetic code-switched datasets leads to substantial improvements in\nLRL model performance while preserving or enhancing performance in HRLs.\nAdditionally, we present a new dataset of synthetic code-switched text derived\nfrom the CommonSenseQA dataset, featuring three distinct language ratio\nconfigurations."}
{"id": "2507.19855", "pdf": "https://arxiv.org/pdf/2507.19855.pdf", "abs": "https://arxiv.org/abs/2507.19855", "title": "Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning", "authors": ["Aditya Sharma", "Ananya Gupta", "Chengyu Wang", "Chiamaka Adebayo", "Jakub Kowalski"], "categories": ["cs.LG", "cs.HC", "68T05, 68T07, 68T40", "I.2.6; I.2.9; I.2.7; I.2.10; H.5.2"], "comment": "12 pages, 4 figures,", "summary": "Large Language Models (LLMs), despite their advanced linguistic capabilities,\nfundamentally lack an intuitive understanding of physical dynamics, which\nlimits their effectiveness in real-world scenarios that require causal\nreasoning. In this paper, we introduce Causal World Model Induction (CWMI), a\nnovel framework designed to embed an explicit model of causal physics within an\nLLM. Our approach incorporates a dedicated Causal Physics Module (CPM) and a\nnew training objective called Causal Intervention Loss, encouraging the model\nto learn cause-and-effect relationships from multimodal data. By training the\nmodel to predict the outcomes of hypothetical interventions instead of merely\ncapturing statistical correlations, CWMI develops a robust internal\nrepresentation of physical laws. Experimental results show that CWMI\nsignificantly outperforms state-of-the-art LLMs on zero-shot physical reasoning\ntasks, including the PIQA benchmark and our newly proposed PhysiCa-Bench\ndataset. These findings demonstrate that inducing a causal world model is a\ncritical step toward more reliable and generalizable AI systems."}
{"id": "2508.12669", "pdf": "https://arxiv.org/pdf/2508.12669.pdf", "abs": "https://arxiv.org/abs/2508.12669", "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery", "authors": ["Bishanka Seal", "Rahul Seetharaman", "Aman Bansal", "Abhilash Nandy"], "categories": ["cs.CL", "cs.CY"], "comment": "14 pages, 4 tables", "summary": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub"}
{"id": "2508.07501", "pdf": "https://arxiv.org/pdf/2508.07501.pdf", "abs": "https://arxiv.org/abs/2508.07501", "title": "FormCoach: Lift Smarter, Not Harder", "authors": ["Xiaoye Zuo", "Nikos Athanasiou", "Ginger Delmas", "Yiming Huang", "Xingyu Fu", "Lingjie Liu"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Good form is the difference between strength and strain, yet for the\nfast-growing community of at-home fitness enthusiasts, expert feedback is often\nout of reach. FormCoach transforms a simple camera into an always-on,\ninteractive AI training partner, capable of spotting subtle form errors and\ndelivering tailored corrections in real time, leveraging vision-language models\n(VLMs). We showcase this capability through a web interface and benchmark\nstate-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference\nvideo pairs spanning 22 strength and mobility exercises. To accelerate research\nin AI-driven coaching, we release both the dataset and an automated,\nrubric-based evaluation pipeline, enabling standardized comparison across\nmodels. Our benchmarks reveal substantial gaps compared to human-level\ncoaching, underscoring both the challenges and opportunities in integrating\nnuanced, context-aware movement analysis into interactive AI systems. By\nframing form correction as a collaborative and creative process between humans\nand machines, FormCoach opens a new frontier in embodied AI."}
{"id": "2508.12685", "pdf": "https://arxiv.org/pdf/2508.12685.pdf", "abs": "https://arxiv.org/abs/2508.12685", "title": "ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction", "authors": ["Xingshan Zeng", "Weiwen Liu", "Lingzhi Wang", "Liangyou Li", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Agentic task-solving with Large Language Models (LLMs) requires multi-turn,\nmulti-step interactions, often involving complex function calls and dynamic\nuser-agent exchanges. Existing simulation-based data generation methods for\nsuch scenarios rely heavily on costly autoregressive interactions between\nmultiple LLM agents, thereby limiting real-world performance of agentic tasks.\nIn this paper, we propose a novel Non-Autoregressive Iterative Generation\nframework, called ToolACE-MT, for constructing high-quality multi-turn agentic\ndialogues. ToolACE-MT generates full conversational trajectories through three\nstages: coarse-grained initialization, iterative refinement, and offline\nverification. The initialization phase builds a structurally complete yet\nsemantically coarse dialogue skeleton; the iterative refinement phase\nintroduces realistic complexities and continued refinement via mask-and-fill\noperations; and the offline verification phase ensures correctness and\ncoherence via rule- and model-based checks. Experiments demonstrate that\nToolACE-MT enables efficient, effective and generalizable agentic data\ngeneration, offering a new paradigm for high-quality data construction in\ntool-augmented LLM scenarios."}
{"id": "2508.10071", "pdf": "https://arxiv.org/pdf/2508.10071.pdf", "abs": "https://arxiv.org/abs/2508.10071", "title": "Advancing Data Equity: Practitioner Responsibility and Accountability in NLP Data Practices", "authors": ["Jay L. Cunningham", "Kevin Zhongyang Shao", "Rock Yuren Pang", "Nathaniel Mengist"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "10 pages, 6 Pages (References and Appendices). The archival version\n  has been accepted to AAAI (AIES 2025) without the extended Appendices. This\n  extended version includes Appendices", "summary": "While research has focused on surfacing and auditing algorithmic bias to\nensure equitable AI development, less is known about how NLP practitioners -\nthose directly involved in dataset development, annotation, and deployment -\nperceive and navigate issues of NLP data equity. This study is among the first\nto center practitioners' perspectives, linking their experiences to a\nmulti-scalar AI governance framework and advancing participatory\nrecommendations that bridge technical, policy, and community domains. Drawing\non a 2024 questionnaire and focus group, we examine how U.S.-based NLP data\npractitioners conceptualize fairness, contend with organizational and systemic\nconstraints, and engage emerging governance efforts such as the U.S. AI Bill of\nRights. Findings reveal persistent tensions between commercial objectives and\nequity commitments, alongside calls for more participatory and accountable data\nworkflows. We critically engage debates on data diversity and diversity\nwashing, arguing that improving NLP equity requires structural governance\nreforms that support practitioner agency and community consent."}
{"id": "2508.12726", "pdf": "https://arxiv.org/pdf/2508.12726.pdf", "abs": "https://arxiv.org/abs/2508.12726", "title": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning", "authors": ["Weize Liu", "Yongchi Zhao", "Yijia Luo", "Mingyu Xu", "Jiaheng Liu", "Yanan Li", "Xiguo Hu", "Yuchi Xu", "Wenbo Su", "Bo Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage tasks but still struggle with complex, multi-step reasoning,\nparticularly across diverse disciplines. Existing reasoning datasets often\neither lack disciplinary breadth or the structural depth necessary to elicit\nrobust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd\nReasoning data synthesis pipeline that leverages naturally available, extensive\nraw documents (book corpus and web corpus) to generate multidisciplinary\nchallenging questions. A core innovation of our approach is the introduction of\na Design Logic concept, which mimics the question-creation process of human\neducators. We use LLMs to reverse-engineer and abstract over 120,000 design\nlogics from existing questions across various disciplines. By matching these\ndesign logics with disciplinary source materials, we are able to create\nreasoning questions that far surpass the difficulty and diversity of existing\ndatasets. Based on this pipeline, we synthesized two large-scale reasoning\ndatasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),\ncontaining 3.04 million challenging questions synthesized from the book corpus,\nand Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging\nquestions from the web corpus. Our data analysis demonstrates that the\nquestions synthesized by our method exhibit substantially greater difficulty\nand diversity than those in the baseline datasets. We validate the\neffectiveness of these datasets by conducting SFT experiments on the\nQwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset\nsignificantly outperforms existing multidisciplinary datasets of the same\nvolume. Training with the full datasets further enables the models to surpass\nthe multidisciplinary reasoning performance of the official Qwen3-8B and\nQwen3-4B models."}
{"id": "2508.12733", "pdf": "https://arxiv.org/pdf/2508.12733.pdf", "abs": "https://arxiv.org/abs/2508.12733", "title": "LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models", "authors": ["Zhiyuan Ning", "Tianle Gu", "Jiaxin Song", "Shixin Hong", "Lingyu Li", "Huacan Liu", "Jie Li", "Yixu Wang", "Meng Lingyu", "Yan Teng", "Yingchun Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "7pages, 5 figures", "summary": "The widespread adoption and increasing prominence of large language models\n(LLMs) in global technologies necessitate a rigorous focus on ensuring their\nsafety across a diverse range of linguistic and cultural contexts. The lack of\na comprehensive evaluation and diverse data in existing multilingual safety\nevaluations for LLMs limits their effectiveness, hindering the development of\nrobust multilingual safety alignment. To address this critical gap, we\nintroduce LinguaSafe, a comprehensive multilingual safety benchmark crafted\nwith meticulous attention to linguistic authenticity. The LinguaSafe dataset\ncomprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated\nusing a combination of translated, transcreated, and natively-sourced data, our\ndataset addresses the critical need for multilingual safety evaluations of\nLLMs, filling the void in the safety evaluation of LLMs across diverse\nunder-represented languages from Hungarian to Malay. LinguaSafe presents a\nmultidimensional and fine-grained evaluation framework, with direct and\nindirect safety assessments, including further evaluations for oversensitivity.\nThe results of safety and helpfulness evaluations vary significantly across\ndifferent domains and different languages, even in languages with similar\nresource levels. Our benchmark provides a comprehensive suite of metrics for\nin-depth safety evaluation, underscoring the critical importance of thoroughly\nassessing multilingual safety in LLMs to achieve more balanced safety\nalignment. Our dataset and code are released to the public to facilitate\nfurther research in the field of multilingual LLM safety."}
{"id": "2508.12769", "pdf": "https://arxiv.org/pdf/2508.12769.pdf", "abs": "https://arxiv.org/abs/2508.12769", "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description", "authors": ["Shaoming Duan", "Zirui Wang", "Chuanyi Liu", "Zhibin Zhu", "Yuhao Zhang", "Peiyi Han", "Liang Yan", "Zewu Penge"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git"}
{"id": "2508.12774", "pdf": "https://arxiv.org/pdf/2508.12774.pdf", "abs": "https://arxiv.org/abs/2508.12774", "title": "From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task", "authors": ["Javier Garcia Gilabert", "Xixian Liao", "Severino Da Dalt", "Ella Bohman", "Audrey Mash", "Francesca De Luca Fornaciari", "Irene Baucells", "Joan Llop", "Miguel Claramunt Argote", "Carlos Escolano", "Maite Melero"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we present the SALAMANDRATA family of models, an improved\niteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically\ntrained to achieve strong performance in translation-related tasks for 38\nEuropean languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For\nboth versions, we applied the same training recipe with a first step of\ncontinual pre-training on parallel data, and a second step of supervised\nfine-tuning on high-quality instructions. The BSC submission to the WMT25\nGeneral Machine Translation shared task is based on the 7B variant of\nSALAMANDRATA. We first adapted the model vocabulary to support the additional\nnon-European languages included in the task. This was followed by a second\nphase of continual pre-training and supervised fine-tuning, carefully designed\nto optimize performance across all translation directions for this year's\nshared task. For decoding, we employed two quality-aware strategies: Minimum\nBayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI\nrespectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,\nalong with the newer SALAMANDRATA-V2 model, on Hugging Face1"}
{"id": "2508.12778", "pdf": "https://arxiv.org/pdf/2508.12778.pdf", "abs": "https://arxiv.org/abs/2508.12778", "title": "HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks", "authors": ["Zhe Chen", "Yusheng Liao", "Shuyang Jiang", "Zhiyuan Zhu", "Haolin Li", "Yanfeng Wang", "Yu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Medical large vision-language Models (Med-LVLMs) have shown promise in\nclinical applications but suffer from factual inaccuracies and unreliable\noutputs, posing risks in real-world diagnostics. While retrieval-augmented\ngeneration has emerged as a potential solution, current medical multimodal RAG\nsystems are unable to perform effective retrieval across heterogeneous sources.\nThe irrelevance of retrieved reports affects the factuality of analysis, while\ninsufficient knowledge affects the credibility of clinical decision-making. To\nbridge the gap, we construct MedAtlas, which includes extensive multimodal\nreport repositories and diverse text corpora. Based on it, we present\nHeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous\nknowledge sources. The framework introduces Modality-specific CLIPs for\neffective report retrieval and a Multi-corpora Query Generator for dynamically\nconstructing queries for diverse corpora. Incorporating knowledge from such\nmultifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge\nPreference Tuning to achieve cross-modality and multi-source knowledge\nalignment. Extensive experiments across 12 datasets and 3 modalities\ndemonstrate that the proposed HeteroRAG achieves state-of-the-art performance\nin most medical vision language benchmarks, significantly improving factual\naccuracy and reliability of Med-LVLMs."}
{"id": "2508.12800", "pdf": "https://arxiv.org/pdf/2508.12800.pdf", "abs": "https://arxiv.org/abs/2508.12800", "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward", "authors": ["Yong Deng", "Guoqing Wang", "Zhenzhe Ying", "Xiaofeng Wu", "Jinzhen Lin", "Wenwen Xiong", "Yuqin Dai", "Shuo Yang", "Zhanwei Zhang", "Qiwen Wang", "Yang Qin", "Changhua Meng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns."}
{"id": "2508.12803", "pdf": "https://arxiv.org/pdf/2508.12803.pdf", "abs": "https://arxiv.org/abs/2508.12803", "title": "When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models", "authors": ["Ahmed Elshabrawy", "Hour Kaing", "Haiyue Song", "Alham Fikri Aji", "Hideki Tanaka", "Masao Utiyama", "Raj Dabre"], "categories": ["cs.CL"], "comment": null, "summary": "Alignment with high-resource standard languages is often assumed to aid the\nmodeling of related low-resource varieties. We challenge this assumption by\ndemonstrating that excessive representational entanglement with a dominant\nvariety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,\ncan actively hinder generative modeling. We present the first comprehensive\ncausal study of this phenomenon by analyzing and directly intervening in the\ninternal representation geometry of large language models (LLMs). Our key\ncontribution is an online variational probing framework that continuously\nestimates the subspace of the standard variety during fine-tuning, enabling\nprojection-based decoupling from this space. While our study uses Arabic as a\ncase due to its unusually rich parallel resources across 25 dialects, the\nbroader motivation is methodological: dialectal MT serves as a controlled proxy\nfor generative tasks where comparable multi-variety corpora are unavailable.\nAcross 25 dialects, our intervention improves generation quality by up to +4.9\nchrF++ and +2.0 on average compared to standard fine-tuning, despite a measured\ntradeoff in standard-language performance. These results provide causal\nevidence that subspace dominance by high-resource varieties can restrict\ngenerative capacity for related varieties. More generally, we unify geometric\nand information-theoretic probing with subspace-level causal interventions,\noffering practical tools for improving generative modeling in closely related\nlanguage families and, more broadly, for controlling representational\nallocation in multilingual and multi-domain LLMs. Code will be released."}
{"id": "2508.12819", "pdf": "https://arxiv.org/pdf/2508.12819.pdf", "abs": "https://arxiv.org/abs/2508.12819", "title": "ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue", "authors": ["Jeongwoo Kang", "Maria Boritchev", "Maximin Coavoux"], "categories": ["cs.CL"], "comment": "Accepted at IWCS 2025", "summary": "We present our work to build a French semantic corpus by annotating French\ndialogue in Abstract Meaning Representation (AMR). Specifically, we annotate\nthe DinG corpus, consisting of transcripts of spontaneous French dialogues\nrecorded during the board game Catan. As AMR has insufficient coverage of the\ndynamics of spontaneous speech, we extend the framework to better represent\nspontaneous speech and sentence structures specific to French. Additionally, to\nsupport consistent annotation, we provide an annotation guideline detailing\nthese extensions. We publish our corpus under a free license (CC-SA-BY). We\nalso train and evaluate an AMR parser on our data. This model can be used as an\nassistance annotation tool to provide initial annotations that can be refined\nby human annotators. Our work contributes to the development of semantic\nresources for French dialogue."}
{"id": "2508.12828", "pdf": "https://arxiv.org/pdf/2508.12828.pdf", "abs": "https://arxiv.org/abs/2508.12828", "title": "Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection", "authors": ["Raneem Alharthi", "Rajwa Alharthi", "Aiqi Jiang", "Arkaitz Zubiaga"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abusive language detection has become an increasingly important task as a\nmeans to tackle this type of harmful content in social media. There has been a\nsubstantial body of research developing models for determining if a social\nmedia post is abusive or not; however, this research has primarily focused on\nexploiting social media posts individually, overlooking additional context that\ncan be derived from surrounding posts. In this study, we look at conversational\nexchanges, where a user replies to an earlier post by another user (the parent\ntweet). We ask: does leveraging context from the parent tweet help determine if\na reply post is abusive or not, and what are the features that contribute the\nmost? We study a range of content-based and account-based features derived from\nthe context, and compare this to the more widely studied approach of only\nlooking at the features from the reply tweet. For a more generalizable study,\nwe test four different classification models on a dataset made of\nconversational exchanges (parent-reply tweet pairs) with replies labeled as\nabusive or not. Our experiments show that incorporating contextual features\nleads to substantial improvements compared to the use of features derived from\nthe reply tweet only, confirming the importance of leveraging context. We\nobserve that, among the features under study, it is especially the\ncontent-based features (what is being posted) that contribute to the\nclassification performance rather than account-based features (who is posting\nit). While using content-based features, it is best to combine a range of\ndifferent features to ensure improved performance over being more selective and\nusing fewer features. Our study provides insights into the development of\ncontextualized abusive language detection models in realistic settings\ninvolving conversations."}
{"id": "2508.12830", "pdf": "https://arxiv.org/pdf/2508.12830.pdf", "abs": "https://arxiv.org/abs/2508.12830", "title": "It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae", "authors": ["Jan Maliszewski"], "categories": ["cs.CL"], "comment": null, "summary": "While the indirect evidence suggests that already in the early scholastic\nperiod the literary production based on records of oral teaching (so-called\nreportationes) was not uncommon, there are very few sources commenting on the\npractice. This paper details the design of a study applying stylometric\ntechniques of authorship attribution to a collection developed from\nreportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover\nlayers of editorial work and thus validate some hypotheses regarding the\ncollection's formation. Following Camps, Cl\\'erice, and Pinche (2021), I\ndiscuss the implementation of an HTR pipeline and stylometric analysis based on\nthe most frequent words, POS tags, and pseudo-affixes. The proposed study will\noffer two methodological gains relevant to computational research on the\nscholastic tradition: it will directly compare performance on manually composed\nand automatically extracted data, and it will test the validity of\ntransformer-based OCR and automated transcription alignment for workflows\napplied to scholastic Latin corpora. If successful, this study will provide an\neasily reusable template for the exploratory analysis of collaborative literary\nproduction stemming from medieval universities."}
{"id": "2508.12863", "pdf": "https://arxiv.org/pdf/2508.12863.pdf", "abs": "https://arxiv.org/abs/2508.12863", "title": "Word Meanings in Transformer Language Models", "authors": ["Jumbly Grindrod", "Peter Grindrod"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We investigate how word meanings are represented in the transformer language\nmodels. Specifically, we focus on whether transformer models employ something\nanalogous to a lexical store - where each word has an entry that contains\nsemantic information. To do this, we extracted the token embedding space of\nRoBERTa-base and k-means clustered it into 200 clusters. In our first study, we\nthen manually inspected the resultant clusters to consider whether they are\nsensitive to semantic information. In our second study, we tested whether the\nclusters are sensitive to five psycholinguistic measures: valence,\nconcreteness, iconicity, taboo, and age of acquisition. Overall, our findings\nwere very positive - there is a wide variety of semantic information encoded\nwithin the token embedding space. This serves to rule out certain \"meaning\neliminativist\" hypotheses about how transformer LLMs process semantic\ninformation."}
{"id": "2508.12868", "pdf": "https://arxiv.org/pdf/2508.12868.pdf", "abs": "https://arxiv.org/abs/2508.12868", "title": "An LLM Agent-Based Complex Semantic Table Annotation Approach", "authors": ["Yilin Geng", "Shujing Wang", "Chuan Wang", "Keqing He", "Yanfei Lv", "Ying Wang", "Zaiwen Feng", "Xiaoying Bai"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "The Semantic Table Annotation (STA) task, which includes Column Type\nAnnotation (CTA) and Cell Entity Annotation (CEA), maps table contents to\nontology entities and plays important roles in various semantic applications.\nHowever, complex tables often pose challenges such as semantic loss of column\nnames or cell values, strict ontological hierarchy requirements, homonyms,\nspelling errors, and abbreviations, which hinder annotation accuracy. To\naddress these issues, this paper proposes an LLM-based agent approach for CTA\nand CEA. We design and implement five external tools with tailored prompts\nbased on the ReAct framework, enabling the STA agent to dynamically select\nsuitable annotation strategies depending on table characteristics. Experiments\nare conducted on the Tough Tables and BiodivTab datasets from the SemTab\nchallenge, which contain the aforementioned challenges. Our method outperforms\nexisting approaches across various metrics. Furthermore, by leveraging\nLevenshtein distance to reduce redundant annotations, we achieve a 70%\nreduction in time costs and a 60% reduction in LLM token usage, providing an\nefficient and cost-effective solution for STA."}
{"id": "2508.12903", "pdf": "https://arxiv.org/pdf/2508.12903.pdf", "abs": "https://arxiv.org/abs/2508.12903", "title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models", "authors": ["Jinyi Han", "Xinyi Wang", "Haiquan Zhao", "Tingyun li", "Zishang Jiang", "Sihang Jiang", "Jiaqing Liang", "Xin Lin", "Weikang Zhou", "Zeye Sun", "Fei Yu", "Yanghua Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub."}
{"id": "2508.12981", "pdf": "https://arxiv.org/pdf/2508.12981.pdf", "abs": "https://arxiv.org/abs/2508.12981", "title": "Analyzing Information Sharing and Coordination in Multi-Agent Planning", "authors": ["Tianyue Ou", "Saujas Vaduguru", "Daniel Fried"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-agent systems (MASs) have pushed the boundaries of large language model\n(LLM) agents in domains such as web research and software engineering. However,\nlong-horizon, multi-constraint planning tasks involve conditioning on detailed\ninformation and satisfying complex interdependent constraints, which can pose a\nchallenge for these systems. In this study, we construct an LLM-based MAS for a\ntravel planning task which is representative of these challenges. We evaluate\nthe impact of a notebook to facilitate information sharing, and evaluate an\norchestrator agent to improve coordination in free form conversation between\nagents. We find that the notebook reduces errors due to hallucinated details by\n18%, while an orchestrator directs the MAS to focus on and further reduce\nerrors by up to 13.5% within focused sub-areas. Combining both mechanisms\nachieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute\nimprovement over the single-agent baseline's 7.5% pass rate. These results\nhighlight the potential of structured information sharing and reflective\norchestration as key components in MASs for long horizon planning with LLMs."}
{"id": "2508.13024", "pdf": "https://arxiv.org/pdf/2508.13024.pdf", "abs": "https://arxiv.org/abs/2508.13024", "title": "WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents", "authors": ["Ralph Peeters", "Aaron Steiner", "Luca Schwarz", "Julian Yuya Caspary", "Christian Bizer"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-based web agents have the potential to automate long-running web tasks,\nsuch as finding offers for specific products in multiple online shops and\nsubsequently ordering the cheapest products that meet the users needs. This\npaper introduces WebMall, a multi-shop online shopping benchmark for evaluating\nthe effectiveness and efficiency of web agents for comparison-shopping. WebMall\nconsists of four simulated online shops populated with authentic product offers\nsourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These\ntasks include basic tasks such as finding specific products in multiple shops,\nperforming price comparisons, adding items to the shopping cart, and completing\ncheckout. Advanced tasks involve searching for products based on vague\nrequirements, identifying suitable substitutes, and finding compatible\nproducts. Compared to existing e-commerce benchmarks, such as WebShop or\nShoppingBench, WebMall introduces comparison-shopping tasks across multiple\nshops. Furthermore, the product offers are more heterogeneous, as they\noriginate from hundreds of distinct real-world shops. The tasks in WebMall\nrequire longer interaction trajectories than those in WebShop, while remaining\nrepresentative of real-world shopping behaviors. We evaluate eight baseline\nagents on WebMall, varying in observation modality, memory utilization, and\nunderlying large language model (GPT 4.1 and Claude Sonnet 4). The\nbest-performing configurations achieve completion rates of 75% and 53%, and F1\nscores of 87% and 63%, on the basic and advanced task sets, respectively.\nWebMall is publicly released to facilitate research on web agents and to\npromote advancements in navigation, reasoning, and efficiency within e-commerce\nscenarios."}
{"id": "2508.13028", "pdf": "https://arxiv.org/pdf/2508.13028.pdf", "abs": "https://arxiv.org/abs/2508.13028", "title": "Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis", "authors": ["Zhu Li", "Yuqing Zhang", "Xiyuan Gao", "Devraj Raghuvanshi", "Nagendra Kumar", "Shekhar Nayak", "Matt Coler"], "categories": ["cs.CL"], "comment": "Speech Synthesis Workshop 2025", "summary": "Sarcastic speech synthesis, which involves generating speech that effectively\nconveys sarcasm, is essential for enhancing natural interactions in\napplications such as entertainment and human-computer interaction. However,\nsynthesizing sarcastic speech remains a challenge due to the nuanced prosody\nthat characterizes sarcasm, as well as the limited availability of annotated\nsarcastic speech data. To address these challenges, this study introduces a\nnovel approach that integrates feedback loss from a bi-modal sarcasm detection\nmodel into the TTS training process, enhancing the model's ability to capture\nand convey sarcasm. In addition, by leveraging transfer learning, a speech\nsynthesis model pre-trained on read speech undergoes a two-stage fine-tuning\nprocess. First, it is fine-tuned on a diverse dataset encompassing various\nspeech styles, including sarcastic speech. In the second stage, the model is\nfurther refined using a dataset focused specifically on sarcastic speech,\nenhancing its ability to generate sarcasm-aware speech. Objective and\nsubjective evaluations demonstrate that our proposed methods improve the\nquality, naturalness, and sarcasm-awareness of synthesized speech."}
{"id": "2508.13037", "pdf": "https://arxiv.org/pdf/2508.13037.pdf", "abs": "https://arxiv.org/abs/2508.13037", "title": "Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction", "authors": ["Xinhe Li", "Jiajun Liu", "Peng Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCAI2025", "summary": "Recent studies have demonstrated that Large Language Models (LLMs) have\nstrong mathematical reasoning abilities but rely on hundreds of billions of\nparameters. To tackle the challenge of poor reasoning in Small Language Models\n(SLMs), existing methods typically leverage LLMs to generate massive amounts of\ndata for cramming training. In psychology, they are akin to System 1 thinking,\nwhich resolves reasoning problems rapidly based on experience and intuition.\nHowever, human learning also requires System 2 thinking, where knowledge is\nfirst acquired and then reinforced through practice. Inspired by such two\ndistinct modes of thinking, we propose a novel method based on the multi-LoRA\nInteraction for mathematical reasoning Distillation (LoRID). First, we input\nthe question and reasoning of each sample into an LLM to create\nknowledge-enhanced datasets. Subsequently, we train a LoRA block on the student\nmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts\nfor problem-solving. Then, to imitate System 2 thinking, we train the Knowledge\nGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs only\nknowledge after receiving problems, while the latter uses that knowledge to\nperform reasoning. Finally, to address the randomness in the generation of IR\nand DR, we evaluate whether their outputs are consistent, and the inference\nprocess needs to be iterated if not. This step can enhance the mathematical\nreasoning ability of SLMs through mutual feedback. Experimental results show\nthat LoRID achieves state-of-the-art performance, especially on the GSM8K\ndataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,\n12.3%, and 1.8% accuracy across the five base models, respectively."}
{"id": "2508.13044", "pdf": "https://arxiv.org/pdf/2508.13044.pdf", "abs": "https://arxiv.org/abs/2508.13044", "title": "BÃ¼yÃ¼k Dil Modelleri iÃ§in TR-MMLU BenchmarkÄ±: Performans DeÄerlendirmesi, Zorluklar ve Ä°yileÅtirme FÄ±rsatlarÄ±", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih GÃ¼mÃ¼Å", "Banu Diri", "SavaÅ YÄ±ldÄ±rÄ±m", "Ãner AytaÅ"], "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "comment": "10 pages, in Turkish language, 5 figures. Presented at the 2025 33rd\n  Signal Processing and Communications Applications Conference (SIU), 25--28\n  June 2025, Sile, Istanbul, T\\\"urkiye", "summary": "Language models have made significant advancements in understanding and\ngenerating human language, achieving remarkable success in various\napplications. However, evaluating these models remains a challenge,\nparticularly for resource-limited languages like Turkish. To address this\nissue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive\nevaluation framework designed to assess the linguistic and conceptual\ncapabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a\nmeticulously curated dataset comprising 6,200 multiple-choice questions across\n62 sections within the Turkish education system. This benchmark provides a\nstandard framework for Turkish NLP research, enabling detailed analyses of\nLLMs' capabilities in processing Turkish text. In this study, we evaluated\nstate-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model\ndesign. TR-MMLU sets a new standard for advancing Turkish NLP research and\ninspiring future innovations."}
{"id": "2508.13058", "pdf": "https://arxiv.org/pdf/2508.13058.pdf", "abs": "https://arxiv.org/abs/2508.13058", "title": "DoÄal Dil Ä°Ålemede Tokenizasyon StandartlarÄ± ve ÃlÃ§Ã¼mÃ¼: TÃ¼rkÃ§e Ãzerinden BÃ¼yÃ¼k Dil Modellerinin KarÅÄ±laÅtÄ±rmalÄ± Analizi", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih GÃ¼mÃ¼Å", "Sercan KarakaÅ", "Banu Diri", "SavaÅ YÄ±ldÄ±rÄ±m"], "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "comment": "in Turkish language, Presented at the 2025 33rd Signal Processing and\n  Communications Applications Conference (SIU), 25--28 June 2025, \\c{S}ile,\n  Istanbul, T\\\"urkiye", "summary": "Tokenization is a fundamental preprocessing step in Natural Language\nProcessing (NLP), significantly impacting the capability of large language\nmodels (LLMs) to capture linguistic and semantic nuances. This study introduces\na novel evaluation framework addressing tokenization challenges specific to\nmorphologically-rich and low-resource languages such as Turkish. Utilizing the\nTurkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from\nthe Turkish education system, we assessed tokenizers based on vocabulary size,\ntoken count, processing time, language-specific token percentages (\\%TR), and\ntoken purity (\\%Pure). These newly proposed metrics measure how effectively\ntokenizers preserve linguistic structures. Our analysis reveals that\nlanguage-specific token percentages exhibit a stronger correlation with\ndownstream performance (e.g., MMLU scores) than token purity. Furthermore,\nincreasing model parameters alone does not necessarily enhance linguistic\nperformance, underscoring the importance of tailored, language-specific\ntokenization methods. The proposed framework establishes robust and practical\ntokenization standards for morphologically complex languages."}
{"id": "2508.13060", "pdf": "https://arxiv.org/pdf/2508.13060.pdf", "abs": "https://arxiv.org/abs/2508.13060", "title": "Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database", "authors": ["John Alderete", "Macarious Kin Fung Hui", "Aanchan Mohan"], "categories": ["cs.CL"], "comment": "5 pages, 6 figures, 1 table, Interspeech 2025 (Rotterdam)", "summary": "The Simon Fraser University Speech Error Database (SFUSED) is a public data\ncollection developed for linguistic and psycholinguistic research. Here we\ndemonstrate how its design and annotations can be used to test and evaluate\nspeech recognition models. The database comprises systematically annotated\nspeech errors from spontaneous English speech, with each error tagged for\nintended and actual error productions. The annotation schema incorporates\nmultiple classificatory dimensions that are of some value to model assessment,\nincluding linguistic hierarchical level, contextual sensitivity, degraded\nwords, word corrections, and both word-level and syllable-level error\npositioning. To assess the value of these classificatory variables, we\nevaluated the transcription accuracy of WhisperX across 5,300 documented word\nand phonological errors. This analysis demonstrates the atabase's effectiveness\nas a diagnostic tool for ASR system performance."}
{"id": "2508.13070", "pdf": "https://arxiv.org/pdf/2508.13070.pdf", "abs": "https://arxiv.org/abs/2508.13070", "title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning", "authors": ["Long Ma", "Fangwei Zhong", "Yizhou Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern causal language models, followed by rapid developments in discrete\ndiffusion models, can now produce a wide variety of interesting and useful\ncontent. However, these families of models are predominantly trained to output\ntokens with a fixed (left-to-right) or random order, which may deviate from the\nlogical order in which tokens are generated originally. In this paper, we\nobserve that current causal and diffusion models encounter difficulties in\nproblems that require adaptive token generation orders to solve tractably,\nwhich we characterize with the $\\mathcal{V}$-information framework. Motivated\nby this, we propose Reinforced Context Order Recovery (ReCOR), a\nreinforcement-learning-based framework to extract adaptive, data-dependent\ntoken generation orders from text data without annotations. Self-supervised by\ntoken prediction statistics, ReCOR estimates the hardness of predicting every\nunfilled token and adaptively selects the next token during both training and\ninference. Experiments on challenging reasoning and planning datasets\ndemonstrate the superior performance of ReCOR compared with baselines,\nsometimes outperforming oracle models supervised with the ground-truth order."}
{"id": "2508.13079", "pdf": "https://arxiv.org/pdf/2508.13079.pdf", "abs": "https://arxiv.org/abs/2508.13079", "title": "DocHPLT: A Massively Multilingual Document-Level Translation Dataset", "authors": ["DayyÃ¡n O'Brien", "Bhavitvya Malik", "Ona de Gibert", "Pinzhen Chen", "Barry Haddow", "JÃ¶rg Tiedemann"], "categories": ["cs.CL"], "comment": null, "summary": "Existing document-level machine translation resources are only available for\na handful of languages, mostly high-resourced ones. To facilitate the training\nand evaluation of document-level translation and, more broadly, long-context\nmodeling for global communities, we create DocHPLT, the largest publicly\navailable document-level translation dataset to date. It contains 124 million\naligned document pairs across 50 languages paired with English, comprising 4.26\nbillion sentences, with further possibility to provide 2500 bonus pairs not\ninvolving English. Unlike previous reconstruction-based approaches that piece\ntogether documents from sentence-level data, we modify an existing web\nextraction pipeline to preserve complete document integrity from the source,\nretaining all content including unaligned portions. After our preliminary\nexperiments identify the optimal training context strategy for document-level\ntranslation, we demonstrate that LLMs fine-tuned on DocHPLT substantially\noutperform off-the-shelf instruction-tuned baselines, with particularly\ndramatic improvements for under-resourced languages. We open-source the dataset\nunder a permissive license, providing essential infrastructure for advancing\nmultilingual document-level translation."}
{"id": "2508.13107", "pdf": "https://arxiv.org/pdf/2508.13107.pdf", "abs": "https://arxiv.org/abs/2508.13107", "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research", "authors": ["Figarri Keisha", "Prince Singh", "Pallavi", "Dion Fernandes", "Aravindh Manivannan", "Ilham Wicaksono", "Faisal Ahmad"], "categories": ["cs.CL", "cs.IR", "F.2.2, H.3.3, I.2.7"], "comment": "submitted to NLLP 2025 Workshop", "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding\nlarge language model outputs in cited sources, a capability that is especially\ncritical in the legal domain. We present an end-to-end RAG pipeline that\nrevisits and extends the LegalBenchRAG baseline with three targeted\nenhancements: (i) a context-aware query translator that disentangles document\nreferences from natural-language questions and adapts retrieval depth and\nresponse style based on expertise and specificity, (ii) open-source retrieval\nstrategies using SBERT and GTE embeddings that achieve substantial performance\ngains (improving Recall@K by 30-95\\% and Precision@K by $\\sim$2.5$\\times$ for\n$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and\ngeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to\nassess semantic alignment and faithfulness across models and prompt designs.\nOur results show that carefully designed open-source pipelines can rival or\noutperform proprietary approaches in retrieval quality, while a custom\nlegal-grounded prompt consistently produces more faithful and contextually\nrelevant answers than baseline prompting. Taken together, these contributions\ndemonstrate the potential of task-aware, component-level tuning to deliver\nlegally grounded, reproducible, and cost-effective RAG systems for legal\nresearch assistance."}
{"id": "2508.13118", "pdf": "https://arxiv.org/pdf/2508.13118.pdf", "abs": "https://arxiv.org/abs/2508.13118", "title": "AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation", "authors": ["Zefang Liu", "Arman Anwar"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Incident response (IR) requires fast, coordinated, and well-informed\ndecision-making to contain and mitigate cyber threats. While large language\nmodels (LLMs) have shown promise as autonomous agents in simulated IR settings,\ntheir reasoning is often limited by a lack of access to external knowledge. In\nthis work, we present AutoBnB-RAG, an extension of the AutoBnB framework that\nincorporates retrieval-augmented generation (RAG) into multi-agent incident\nresponse simulations. Built on the Backdoors & Breaches (B&B) tabletop game\nenvironment, AutoBnB-RAG enables agents to issue retrieval queries and\nincorporate external evidence during collaborative investigations. We introduce\ntwo retrieval settings: one grounded in curated technical documentation\n(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We\nevaluate performance across eight team structures, including newly introduced\nargumentative configurations designed to promote critical reasoning. To\nvalidate practical utility, we also simulate real-world cyber incidents based\non public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct\ncomplex multi-stage attacks. Our results show that retrieval augmentation\nimproves decision quality and success rates across diverse organizational\nmodels. This work demonstrates the value of integrating retrieval mechanisms\ninto LLM-based multi-agent systems for cybersecurity decision-making."}
{"id": "2508.13124", "pdf": "https://arxiv.org/pdf/2508.13124.pdf", "abs": "https://arxiv.org/abs/2508.13124", "title": "Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries", "authors": ["Kawin Mayilvaghanan", "Siddhant Gupta", "Ayush Kumar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abstractive summarization is a core application in contact centers, where\nLarge Language Models (LLMs) generate millions of summaries of call transcripts\ndaily. Despite their apparent quality, it remains unclear whether LLMs\nsystematically under- or over-attend to specific aspects of the transcript,\npotentially introducing biases in the generated summary. While prior work has\nexamined social and positional biases, the specific forms of bias pertinent to\ncontact center operations - which we term Operational Bias - have remained\nunexplored. To address this gap, we introduce BlindSpot, a framework built upon\na taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)\nfor the identification and quantification of these biases. BlindSpot leverages\nan LLM as a zero-shot classifier to derive categorical distributions for each\nbias dimension in a pair of transcript and its summary. The bias is then\nquantified using two metrics: Fidelity Gap (the JS Divergence between\ndistributions) and Coverage (the percentage of source labels omitted). Using\nBlindSpot, we conducted an empirical study with 2500 real call transcripts and\ntheir summaries generated by 20 LLMs of varying scales and families (e.g., GPT,\nLlama, Claude). Our analysis reveals that biases are systemic and present\nacross all evaluated models, regardless of size or family."}
{"id": "2508.13130", "pdf": "https://arxiv.org/pdf/2508.13130.pdf", "abs": "https://arxiv.org/abs/2508.13130", "title": "MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation", "authors": ["Kareem Elozeiri", "Mervat Abassy", "Preslav Nakov", "Yuxia Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Commonsense validation evaluates whether a sentence aligns with everyday\nhuman understanding, a critical capability for developing robust natural\nlanguage understanding systems. While substantial progress has been made in\nEnglish, the task remains underexplored in Arabic, particularly given its rich\nlinguistic diversity. Existing Arabic resources have primarily focused on\nModern Standard Arabic (MSA), leaving regional dialects underrepresented\ndespite their prevalence in spoken contexts. To bridge this gap, we present two\nkey contributions: (i) we introduce MuDRiC, an extended Arabic commonsense\ndataset incorporating multiple dialects, and (ii) a novel method adapting Graph\nConvolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances\nsemantic relationship modeling for improved commonsense validation. Our\nexperimental results demonstrate that this approach achieves superior\nperformance in Arabic commonsense validation. Our work enhances Arabic natural\nlanguage understanding by providing both a foundational dataset and a novel\nmethod for handling its complex variations. To the best of our knowledge, we\nrelease the first Arabic multi-dialect commonsense reasoning dataset."}
{"id": "2508.13131", "pdf": "https://arxiv.org/pdf/2508.13131.pdf", "abs": "https://arxiv.org/abs/2508.13131", "title": "Improving Detection of Watermarked Language Models", "authors": ["Dara Bahri", "John Wieting"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Watermarking has recently emerged as an effective strategy for detecting the\ngenerations of large language models (LLMs). The strength of a watermark\ntypically depends strongly on the entropy afforded by the language model and\nthe set of input prompts. However, entropy can be quite limited in practice,\nespecially for models that are post-trained, for example via instruction tuning\nor reinforcement learning from human feedback (RLHF), which makes detection\nbased on watermarking alone challenging. In this work, we investigate whether\ndetection can be improved by combining watermark detectors with non-watermark\nones. We explore a number of hybrid schemes that combine the two, observing\nperformance gains over either class of detector under a wide range of\nexperimental conditions."}
{"id": "2508.13141", "pdf": "https://arxiv.org/pdf/2508.13141.pdf", "abs": "https://arxiv.org/abs/2508.13141", "title": "OptimalThinkingBench: Evaluating Over and Underthinking in LLMs", "authors": ["Pranjal Aggarwal", "Seungone Kim", "Jack Lanchantin", "Sean Welleck", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "categories": ["cs.CL", "cs.LG"], "comment": "26 pages, 6 tables, 10 figures", "summary": "Thinking LLMs solve complex tasks at the expense of increased compute and\noverthinking on simpler problems, while non-thinking LLMs are faster and\ncheaper but underthink on harder reasoning problems. This has led to the\ndevelopment of separate thinking and non-thinking LLM variants, leaving the\nonus of selecting the optimal model for each query on the end user. In this\nwork, we introduce OptimalThinkingBench, a unified benchmark that jointly\nevaluates overthinking and underthinking in LLMs and also encourages the\ndevelopment of optimally-thinking models that balance performance and\nefficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,\nfeaturing simple queries in 72 domains, and UnderthinkingBench, containing 11\nchallenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we\nperform extensive evaluation of 33 different thinking and non-thinking models\nand show that no model is able to optimally think on our benchmark. Thinking\nmodels often overthink for hundreds of tokens on the simplest user queries\nwithout improving performance. In contrast, large non-thinking models\nunderthink, often falling short of much smaller thinking models. We further\nexplore several methods to encourage optimal thinking, but find that these\napproaches often improve on one sub-benchmark at the expense of the other,\nhighlighting the need for better unified and optimal models in the future."}
{"id": "2508.13144", "pdf": "https://arxiv.org/pdf/2508.13144.pdf", "abs": "https://arxiv.org/abs/2508.13144", "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation", "authors": ["David Heineman", "Valentin Hofmann", "Ian Magnusson", "Yuling Gu", "Noah A. Smith", "Hannaneh Hajishirzi", "Kyle Lo", "Jesse Dodge"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Developing large language models is expensive and involves making decisions\nwith small experiments, typically by evaluating on large, multi-task evaluation\nsuites. In this work, we analyze specific properties which make a benchmark\nmore reliable for such decisions, and interventions to design higher-quality\nevaluation benchmarks. We introduce two key metrics that show differences in\ncurrent benchmarks: signal, a benchmark's ability to separate better models\nfrom worse models, and noise, a benchmark's sensitivity to random variability\nbetween training steps. We demonstrate that benchmarks with a better\nsignal-to-noise ratio are more reliable when making decisions at small scale,\nand those with less noise have lower scaling law prediction error. These\nresults suggest that improving signal or noise will lead to more useful\nbenchmarks, so we introduce three interventions designed to directly affect\nsignal or noise. For example, we propose that switching to a metric that has\nbetter signal and noise (e.g., perplexity rather than accuracy) leads to better\nreliability and improved scaling law error. We also find that filtering noisy\nsubtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable\nmulti-task evaluations. We also find that averaging the output of a model's\nintermediate checkpoints to reduce noise leads to consistent improvements. We\nconclude by recommending that those creating new benchmarks, or selecting which\nexisting benchmarks to use, aim for high signal and low noise. We use 30\nbenchmarks for these experiments, and 375 open-weight language models from 60M\nto 32B parameters, resulting in a new, publicly available dataset of 900K\nevaluation benchmark results, totaling 200M instances."}
{"id": "2508.13152", "pdf": "https://arxiv.org/pdf/2508.13152.pdf", "abs": "https://arxiv.org/abs/2508.13152", "title": "RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns", "authors": ["Xin Chen", "Junchao Wu", "Shu Yang", "Runzhe Zhan", "Zeyu Wu", "Ziyang Luo", "Di Wang", "Min Yang", "Lidia S. Chao", "Derek F. Wong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to TACL 2025. This version is a pre-MIT Press publication\n  version", "summary": "Detecting content generated by large language models (LLMs) is crucial for\npreventing misuse and building trustworthy AI systems. Although existing\ndetection methods perform well, their robustness in out-of-distribution (OOD)\nscenarios is still lacking. In this paper, we hypothesize that, compared to\nfeatures used by existing detection methods, the internal representations of\nLLMs contain more comprehensive and raw features that can more effectively\ncapture and distinguish the statistical pattern differences between\nLLM-generated texts (LGT) and human-written texts (HWT). We validated this\nhypothesis across different LLMs and observed significant differences in neural\nactivation patterns when processing these two types of texts. Based on this, we\npropose RepreGuard, an efficient statistics-based detection method.\nSpecifically, we first employ a surrogate model to collect representation of\nLGT and HWT, and extract the distinct activation feature that can better\nidentify LGT. We can classify the text by calculating the projection score of\nthe text representations along this feature direction and comparing with a\nprecomputed threshold. Experimental results show that RepreGuard outperforms\nall baselines with average 94.92% AUROC on both in-distribution (ID) and OOD\nscenarios, while also demonstrating robust resilience to various text sizes and\nmainstream attacks. Data and code are publicly available at:\nhttps://github.com/NLP2CT/RepreGuard"}
{"id": "2508.11661", "pdf": "https://arxiv.org/pdf/2508.11661.pdf", "abs": "https://arxiv.org/abs/2508.11661", "title": "Sparse Attention across Multiple-context KV Cache", "authors": ["Ziyi Cao", "Qingyi Si", "Jingbin Zhang", "Bingquan Liu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models face significant cost challenges in long-sequence\ninference. To address this, reusing historical Key-Value (KV) Cache for\nimproved inference efficiency has become a mainstream approach. Recent advances\nfurther enhance throughput by sparse attention mechanisms to select the most\nrelevant KV Cache, thereby reducing sequence length. However, such techniques\nare limited to single-context scenarios, where historical KV Cache is computed\nsequentially with causal-attention dependencies. In retrieval-augmented\ngeneration (RAG) scenarios, where retrieved documents as context are unknown\nbeforehand, each document's KV Cache is computed and stored independently\n(termed multiple-context KV Cache), lacking cross-attention between contexts.\nThis renders existing methods ineffective. Although prior work partially\nrecomputes multiple-context KV Cache to mitigate accuracy loss from missing\ncross-attention, it requires retaining all KV Cache throughout, failing to\nreduce memory overhead. This paper presents SamKV, the first exploration of\nattention sparsification for multiple-context KV Cache. Specifically, SamKV\ntakes into account the complementary information of other contexts when\nsparsifying one context, and then locally recomputes the sparsified\ninformation. Experiments demonstrate that our method compresses sequence length\nto 15% without accuracy degradation compared with full-recompuation baselines,\nsignificantly boosting throughput in multi-context RAG scenarios."}
{"id": "2508.11667", "pdf": "https://arxiv.org/pdf/2508.11667.pdf", "abs": "https://arxiv.org/abs/2508.11667", "title": "Assessing Representation Stability for Transformer Models", "authors": ["Bryan E. Tuck", "Rakesh M. Verma"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 pages, 19 figures, 8 tables. Code available at\n  https://github.com/ReDASers/representation-stability", "summary": "Adversarial text attacks remain a persistent threat to transformer models,\nyet existing defenses are typically attack-specific or require costly model\nretraining. We introduce Representation Stability (RS), a model-agnostic\ndetection framework that identifies adversarial examples by measuring how\nembedding representations change when important words are masked. RS first\nranks words using importance heuristics, then measures embedding sensitivity to\nmasking top-k critical words, and processes the resulting patterns with a\nBiLSTM detector. Experiments show that adversarially perturbed words exhibit\ndisproportionately high masking sensitivity compared to naturally important\nwords. Across three datasets, three attack types, and two victim models, RS\nachieves over 88% detection accuracy and demonstrates competitive performance\ncompared to existing state-of-the-art methods, often at lower computational\ncost. Using Normalized Discounted Cumulative Gain (NDCG) to measure\nperturbation identification quality, we reveal that gradient-based ranking\noutperforms attention and random selection approaches, with identification\nquality correlating with detection performance for word-level attacks. RS also\ngeneralizes well to unseen datasets, attacks, and models without retraining,\nproviding a practical solution for adversarial text detection."}
{"id": "2508.11710", "pdf": "https://arxiv.org/pdf/2508.11710.pdf", "abs": "https://arxiv.org/abs/2508.11710", "title": "Code Vulnerability Detection Across Different Programming Languages with AI Models", "authors": ["Hael Abdulhakim Ali Humran", "Ferdi Sonmez"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Security vulnerabilities present in a code that has been written in diverse\nprogramming languages are among the most critical yet complicated aspects of\nsource code to detect. Static analysis tools based on rule-based patterns\nusually do not work well at detecting the context-dependent bugs and lead to\nhigh false positive rates. Recent developments in artificial intelligence,\nspecifically the use of transformer-based models like CodeBERT and CodeLlama,\nprovide light to this problem, as they show potential in finding such flaws\nbetter. This paper presents the implementations of these models on various\ndatasets of code vulnerability, showing how off-the-shelf models can\nsuccessfully produce predictive capacity in models through dynamic fine-tuning\nof the models on vulnerable and safe code fragments. The methodology comprises\nthe gathering of the dataset, normalization of the language, fine-tuning of the\nmodel, and incorporation of ensemble learning and explainable AI. Experiments\nshow that a well-trained CodeBERT can be as good as or even better than some\nexisting static analyzers in terms of accuracy greater than 97%. Further study\nhas indicated that although language models can achieve close-to-perfect\nrecall, the precision can decrease. A solution to this is given by hybrid\nmodels and validation procedures, which will reduce false positives. According\nto the results, the AI-based solutions generalize to different programming\nlanguages and classes of vulnerability. Nevertheless, robustness,\ninterpretability, and deployment readiness are still being developed. The\nresults illustrate the probabilities that AI will enhance the trustworthiness\nin the usability and scalability of machine-learning-based detectors of\nvulnerabilities."}
{"id": "2508.11737", "pdf": "https://arxiv.org/pdf/2508.11737.pdf", "abs": "https://arxiv.org/abs/2508.11737", "title": "Ovis2.5 Technical Report", "authors": ["Shiyin Lu", "Yang Li", "Yu Xia", "Yuwei Hu", "Shanshan Zhao", "Yanqing Ma", "Zhichao Wei", "Yinglun Li", "Lunhao Duan", "Jianshan Zhao", "Yuxuan Han", "Haijun Li", "Wanying Chen", "Junke Tang", "Chengkun Hou", "Zhixing Du", "Tianli Zhou", "Wenjie Zhang", "Huping Ding", "Jiahe Li", "Wen Li", "Gui Hu", "Yiliang Gu", "Siran Yang", "Jiamang Wang", "Hailong Sun", "Yibo Wang", "Hui Sun", "Jinlong Huang", "Yuping He", "Shengze Shi", "Weihong Zhang", "Guodong Zheng", "Junpeng Jiang", "Sensen Gao", "Yi-Feng Wu", "Sijia Chen", "Yuhui Chen", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We present Ovis2.5, a successor to Ovis2 designed for native-resolution\nvisual perception and strong multimodal reasoning. Ovis2.5 integrates a\nnative-resolution vision transformer that processes images at their native,\nvariable resolutions, avoiding the degradation from fixed-resolution tiling and\npreserving both fine detail and global layout -- crucial for visually dense\ncontent like complex charts. To strengthen reasoning, we train the model to\nmove beyond linear chain-of-thought and perform reflection -- including\nself-checking and revision. This advanced capability is exposed as an optional\n\"thinking mode\" at inference time, allowing users to trade latency for enhanced\naccuracy on difficult inputs. The model is trained via a comprehensive\nfive-phase curriculum that progressively builds its skills. The process begins\nwith foundational visual and multimodal pretraining, advances through\nlarge-scale instruction tuning, and culminates in alignment and reasoning\nenhancement using DPO and GRPO. To scale these upgrades efficiently, we employ\nmultimodal data packing and hybrid parallelism, yielding a significant\nend-to-end speedup. We release two open-source models: Ovis2.5-9B and\nOvis2.5-2B. The latter continues the \"small model, big performance\" philosophy\nof Ovis2, making it ideal for resource-constrained, on-device scenarios. On the\nOpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a\nsubstantial improvement over its predecessor, Ovis2-8B, and achieving\nstate-of-the-art results among open-source MLLMs in the sub-40B parameter\nrange; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate\nscores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong\ncapabilities on grounding and video tasks, and achieves open-source SOTA at its\nscale for complex chart analysis."}
{"id": "2508.11759", "pdf": "https://arxiv.org/pdf/2508.11759.pdf", "abs": "https://arxiv.org/abs/2508.11759", "title": "Using Natural Language for Human-Robot Collaboration in the Real World", "authors": ["Peter Lindes", "Kaoutar Skiker"], "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "34 pages, 11 figures, 5 tables. Submitted for publication (2026) in\n  W.F. Lawless, Ranjeev Mittu, Shannon P. McGrarry, & Marco Brambilla (Eds.),\n  Generative AI Risks and Benefits within Human-Machine Teams, Elsevier,\n  Chapter 6", "summary": "We have a vision of a day when autonomous robots can collaborate with humans\nas assistants in performing complex tasks in the physical world. This vision\nincludes that the robots will have the ability to communicate with their human\ncollaborators using language that is natural to the humans. Traditional\nInteractive Task Learning (ITL) systems have some of this ability, but the\nlanguage they can understand is very limited. The advent of large language\nmodels (LLMs) provides an opportunity to greatly improve the language\nunderstanding of robots, yet integrating the language abilities of LLMs with\nrobots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that\nwork closely with humans, and discuss how they could be much better\ncollaborators with robust language abilities. We then explore how an AI system\nwith a cognitive agent that controls a physical robot at its core, interacts\nwith both a human and an LLM, and accumulates situational knowledge through its\nexperiences, can be a possible approach to reach that vision. We focus on three\nspecific challenges of having the robot understand natural language, and\npresent a simple proof-of-concept experiment using ChatGPT for each. Finally,\nwe discuss what it will take to turn these simple experiments into an\noperational system where LLM-assisted language understanding is a part of an\nintegrated robotic assistant that uses language to collaborate with humans."}
{"id": "2508.11801", "pdf": "https://arxiv.org/pdf/2508.11801.pdf", "abs": "https://arxiv.org/abs/2508.11801", "title": "VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models", "authors": ["Ming Cheng", "Tong Wu", "Jiazhen Hu", "Jiaying Gong", "Hoda Eldardiry"], "categories": ["cs.CV", "cs.CL"], "comment": "5 pages, 2 figures, 5 tables, accepted in CIKM 2025", "summary": "Attribute Value Extraction (AVE) is important for structuring product\ninformation in e-commerce. However, existing AVE datasets are primarily limited\nto text-to-text or image-to-text settings, lacking support for product videos,\ndiverse attribute coverage, and public availability. To address these gaps, we\nintroduce VideoAVE, the first publicly available video-to-text e-commerce AVE\ndataset across 14 different domains and covering 172 unique attributes. To\nensure data quality, we propose a post-hoc CLIP-based Mixture of Experts\nfiltering system (CLIP-MoE) to remove the mismatched video-product pairs,\nresulting in a refined dataset of 224k training data and 25k evaluation data.\nIn order to evaluate the usability of the dataset, we further establish a\ncomprehensive benchmark by evaluating several state-of-the-art video vision\nlanguage models (VLMs) under both attribute-conditioned value prediction and\nopen attribute-value pair extraction tasks. Our results analysis reveals that\nvideo-to-text AVE remains a challenging problem, particularly in open settings,\nand there is still room for developing more advanced VLMs capable of leveraging\neffective temporal information. The dataset and benchmark code for VideoAVE are\navailable at: https://github.com/gjiaying/VideoAVE"}
{"id": "2508.11808", "pdf": "https://arxiv.org/pdf/2508.11808.pdf", "abs": "https://arxiv.org/abs/2508.11808", "title": "Labels or Input? Rethinking Augmentation in Multimodal Hate Detection", "authors": ["Sahajpreet Singh", "Rongxin Ouyang", "Subhayan Mukerjee", "Kokil Jaidka"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.MM", "I.2.7; I.2.10"], "comment": "13 pages, 2 figures, 7 tables", "summary": "The modern web is saturated with multimodal content, intensifying the\nchallenge of detecting hateful memes, where harmful intent is often conveyed\nthrough subtle interactions between text and image under the guise of humor or\nsatire. While recent advances in Vision-Language Models (VLMs) show promise,\nthese models lack support for fine-grained supervision and remain susceptible\nto implicit hate speech. In this paper, we present a dual-pronged approach to\nimprove multimodal hate detection. First, we propose a prompt optimization\nframework that systematically varies prompt structure, supervision granularity,\nand training modality. We show that prompt design and label scaling both\ninfluence performance, with structured prompts improving robustness even in\nsmall models, and InternVL2 achieving the best F1-scores across binary and\nscaled settings. Second, we introduce a multimodal data augmentation pipeline\nthat generates 2,479 counterfactually neutral memes by isolating and rewriting\nthe hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,\nsuccessfully reduces spurious correlations and improves classifier\ngeneralization. Our approaches inspire new directions for building synthetic\ndata to train robust and fair vision-language models. Our findings demonstrate\nthat prompt structure and data composition are as critical as model size, and\nthat targeted augmentation can support more trustworthy and context-sensitive\nhate detection."}
{"id": "2508.11860", "pdf": "https://arxiv.org/pdf/2508.11860.pdf", "abs": "https://arxiv.org/abs/2508.11860", "title": "LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework", "authors": ["Frazier N. Baker", "Daniel Adu-Ampratwum", "Reza Averly", "Botao Yu", "Huan Sun", "Xia Ning"], "categories": ["cs.AI", "cs.CL"], "comment": "24 pages, 5 figures", "summary": "Large language model (LLM) agent evaluators leverage specialized tools to\nground the rational decision-making of LLMs, making them well-suited to aid in\nscientific discoveries, such as constrained retrosynthesis planning.\nConstrained retrosynthesis planning is an essential, yet challenging, process\nwithin chemistry for identifying synthetic routes from commercially available\nstarting materials to desired target molecules, subject to practical\nconstraints. Here, we present LARC, the first LLM-based Agentic framework for\nRetrosynthesis planning under Constraints. LARC incorporates agentic constraint\nevaluation, through an Agent-as-a-Judge, directly into the retrosynthesis\nplanning process, using agentic feedback grounded in tool-based reasoning to\nguide and constrain route generation. We rigorously evaluate LARC on a\ncarefully curated set of 48 constrained retrosynthesis planning tasks across 3\nconstraint types. LARC achieves a 72.9% success rate on these tasks, vastly\noutperforming LLM baselines and approaching human expert-level success in\nsubstantially less time. The LARC framework is extensible, and serves as a\nfirst step towards an effective agentic tool or a co-scientist to human experts\nfor constrained retrosynthesis."}
{"id": "2508.11886", "pdf": "https://arxiv.org/pdf/2508.11886.pdf", "abs": "https://arxiv.org/abs/2508.11886", "title": "EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models", "authors": ["Wenhui Zhu", "Xiwen Chen", "Zhipeng Wang", "Shao Tang", "Sayan Ghosh", "Xuanzhao Dong", "Rajat Koner", "Yalin Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "eess.IV"], "comment": null, "summary": "Instructed Visual Segmentation (IVS) tasks require segmenting objects in\nimages or videos based on natural language instructions. While recent\nmultimodal large language models (MLLMs) have achieved strong performance on\nIVS, their inference cost remains a major bottleneck, particularly in video. We\nempirically analyze visual token sampling in MLLMs and observe a strong\ncorrelation between subset token coverage and segmentation performance. This\nmotivates our design of a simple and effective token pruning method that\nselects a compact yet spatially representative subset of tokens to accelerate\ninference. In this paper, we introduce a novel visual token pruning method for\nIVS, called EVTP-IV, which builds upon the k-center by integrating spatial\ninformation to ensure better coverage. We further provide an\ninformation-theoretic analysis to support our design. Experiments on standard\nIVS benchmarks show that our method achieves up to 5X speed-up on video tasks\nand 3.5X on image tasks, while maintaining comparable accuracy using only 20%\nof the tokens. Our method also consistently outperforms state-of-the-art\npruning baselines under varying pruning ratios."}
{"id": "2508.11925", "pdf": "https://arxiv.org/pdf/2508.11925.pdf", "abs": "https://arxiv.org/abs/2508.11925", "title": "Optimizing Token Choice for Code Watermarking: A RL Approach", "authors": ["Zhimeng Guo", "Huaisheng Zhu", "Siyuan Xu", "Hangfan Zhang", "Teng Xiao", "Minhao Cheng"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "18 pages, 3 figures", "summary": "The need for detecting LLM-generated code necessitates watermarking systems\ncapable of operating within its highly structured and syntactically constrained\nenvironment. To address this, we introduce CodeTracer, an innovative adaptive\ncode watermarking framework underpinned by a novel reinforcement learning\ntraining paradigm. At its core, CodeTracer features a policy-driven approach\nthat utilizes a parameterized model to intelligently bias token choices during\nnext-token prediction. This strategy ensures that embedded watermarks maintain\ncode functionality while exhibiting subtle yet statistically detectable\ndeviations from typical token distributions. To facilitate policy learning, we\ndevise a comprehensive reward system that seamlessly integrates execution\nfeedback with watermark embedding signals, balancing process-level and\noutcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization\nto enable gradient-based optimization of discrete watermarking decisions.\nExtensive comparative evaluations demonstrate CodeTracer's significant\nsuperiority over state-of-the-art baselines in both watermark detectability and\nthe preservation of generated code's functionality."}
{"id": "2508.11944", "pdf": "https://arxiv.org/pdf/2508.11944.pdf", "abs": "https://arxiv.org/abs/2508.11944", "title": "CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs", "authors": ["Hongtao Liu", "Zhicheng Du", "Zihe Wang", "Weiran Shen"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Game-playing ability serves as an indicator for evaluating the strategic\nreasoning capability of large language models (LLMs). While most existing\nstudies rely on utility performance metrics, which are not robust enough due to\nvariations in opponent behavior and game structure. To address this limitation,\nwe propose \\textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation\nframework inspired by the cognitive hierarchy models from behavioral economics.\nWe hypothesize that agents have bounded rationality -- different agents behave\nat varying reasoning depths/levels. We evaluate LLMs' strategic reasoning\nthrough a three-phase systematic framework, utilizing behavioral data from six\nstate-of-the-art LLMs across fifteen carefully selected normal-form games.\nExperiments show that LLMs exhibit consistent strategic reasoning levels across\ndiverse opponents, confirming the framework's robustness and generalization\ncapability. We also analyze the effects of two key mechanisms (Chat Mechanism\nand Memory Mechanism) on strategic reasoning performance. Results indicate that\nthe Chat Mechanism significantly degrades strategic reasoning, whereas the\nMemory Mechanism enhances it. These insights position CHBench as a promising\ntool for evaluating LLM capabilities, with significant potential for future\nresearch and practical applications."}
{"id": "2508.12072", "pdf": "https://arxiv.org/pdf/2508.12072.pdf", "abs": "https://arxiv.org/abs/2508.12072", "title": "Mitigating Jailbreaks with Intent-Aware LLMs", "authors": ["Wei Jie Yeo", "Ranjan Satapathy", "Erik Cambria"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Despite extensive safety-tuning, large language models (LLMs) remain\nvulnerable to jailbreak attacks via adversarially crafted instructions,\nreflecting a persistent trade-off between safety and task performance. In this\nwork, we propose Intent-FT, a simple and lightweight fine-tuning approach that\nexplicitly trains LLMs to infer the underlying intent of an instruction before\nresponding. By fine-tuning on a targeted set of adversarial instructions,\nIntent-FT enables LLMs to generalize intent deduction to unseen attacks,\nthereby substantially improving their robustness. We comprehensively evaluate\nboth parametric and non-parametric attacks across open-source and proprietary\nmodels, considering harmfulness from attacks, utility, over-refusal, and impact\nagainst white-box threats. Empirically, Intent-FT consistently mitigates all\nevaluated attack categories, with no single attack exceeding a 50\\% success\nrate -- whereas existing defenses remain only partially effective. Importantly,\nour method preserves the model's general capabilities and reduces excessive\nrefusals on benign instructions containing superficially harmful keywords.\nFurthermore, models trained with Intent-FT accurately identify hidden harmful\nintent in adversarial attacks, and these learned intentions can be effectively\ntransferred to enhance vanilla model defenses."}
{"id": "2508.12081", "pdf": "https://arxiv.org/pdf/2508.12081.pdf", "abs": "https://arxiv.org/abs/2508.12081", "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models", "authors": ["Haidong Xu", "Guangwei Xu", "Zhedong Zheng", "Xiatian Zhu", "Wei Ji", "Xiangtai Li", "Ruijie Guo", "Meishan Zhang", "Min zhang", "Hao Fei"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "20 pages,13 figures", "summary": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input."}
{"id": "2508.12104", "pdf": "https://arxiv.org/pdf/2508.12104.pdf", "abs": "https://arxiv.org/abs/2508.12104", "title": "Generative Medical Event Models Improve with Scale", "authors": ["Shane Waxler", "Paul Blazek", "Davis White", "Daniel Sneider", "Kevin Chung", "Mani Nagarathnam", "Patrick Williams", "Hank Voeller", "Karen Wong", "Matthew Swanhorst", "Sheng Zhang", "Naoto Usuyama", "Cliff Wong", "Tristan Naumann", "Hoifung Poon", "Andrew Loza", "Daniella Meeker", "Seth Hain", "Rahul Shah"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Realizing personalized medicine at scale calls for methods that distill\ninsights from longitudinal patient journeys, which can be viewed as a sequence\nof medical events. Foundation models pretrained on large-scale medical event\ndata represent a promising direction for scaling real-world evidence generation\nand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with\nmedical events from de-identified longitudinal health records for 16.3 billion\nencounters over 300 million unique patient records from 310 health systems, we\nintroduce the Cosmos Medical Event Transformer ( CoMET) models, a family of\ndecoder-only transformer models pretrained on 118 million patients representing\n115 billion discrete medical events (151 billion tokens). We present the\nlargest scaling-law study for medical event data, establishing a methodology\nfor pretraining and revealing power-law scaling relationships for compute,\ntokens, and model size. Based on this, we pretrained a series of\ncompute-optimal models with up to 1 billion parameters. Conditioned on a\npatient's real-world history, CoMET autoregressively generates the next medical\nevent, simulating patient health timelines. We studied 78 real-world tasks,\nincluding diagnosis prediction, disease prognosis, and healthcare operations.\nRemarkably for a foundation model with generic pretraining and simulation-based\ninference, CoMET generally outperformed or matched task-specific supervised\nmodels on these tasks, without requiring task-specific fine-tuning or few-shot\nexamples. CoMET's predictive power consistently improves as the model and\npretraining scale. Our results show that CoMET, a generative medical event\nfoundation model, can effectively capture complex clinical dynamics, providing\nan extensible and generalizable framework to support clinical decision-making,\nstreamline healthcare operations, and improve patient outcomes."}
{"id": "2508.12116", "pdf": "https://arxiv.org/pdf/2508.12116.pdf", "abs": "https://arxiv.org/abs/2508.12116", "title": "DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections", "authors": ["Haebin Shin", "Lei Ji", "Xiao Liu", "Zhiwei Yu", "Qi Chen", "Yeyun Gong"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As numerous instruction-tuning datasets continue to emerge during the\npost-training stage, dynamically balancing and optimizing their mixtures has\nbecome a critical challenge. To address this, we propose DynamixSFT, a dynamic\nand automated method for instruction-tuning dataset mixture optimization. We\nformulate the problem as a multi-armed bandit setup and introduce a\nPrior-scaled Boltzmann Exploration that softly anchors the updated sampling\ndistribution to the original dataset proportions, thereby preserving the\ninherent diversity and coverage of the collection. Sampling probabilities are\nupdated using a lightweight 1-Step Look-ahead Reward, reflecting how much the\ndataset contributes to improving the model's performance at its current state.\nWhen applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning\ndatasets, DynamixSFT achieves up to a 2.2% performance improvement across 10\nbenchmarks. Furthermore, we provide a comprehensive analysis and visualizations\nto offer deeper insights into the adaptive dynamics of our method."}
{"id": "2508.12398", "pdf": "https://arxiv.org/pdf/2508.12398.pdf", "abs": "https://arxiv.org/abs/2508.12398", "title": "Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position", "authors": ["Zhixin Xie", "Xurui Song", "Jun Luo"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have recently emerged as a\ncompetitive non-autoregressive paradigm due to their unique training and\ninference approach. However, there is currently a lack of safety study on this\nnovel architecture. In this paper, we present the first analysis of dLLMs'\nsafety performance and propose a novel safety alignment method tailored to\ntheir unique generation characteristics. Specifically, we identify a critical\nasymmetry between the defender and attacker in terms of security. For the\ndefender, we reveal that the middle tokens of the response, rather than the\ninitial ones, are more critical to the overall safety of dLLM outputs; this\nseems to suggest that aligning middle tokens can be more beneficial to the\ndefender. The attacker, on the contrary, may have limited power to manipulate\nmiddle tokens, as we find dLLMs have a strong tendency towards a sequential\ngeneration order in practice, forcing the attack to meet this distribution and\ndiverting it from influencing the critical middle tokens. Building on this\nasymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method\nthat directly aligns the model's middle generation with safe refusals\nexploiting reinforcement learning. We implement MOSA and compare its security\nperformance against eight attack methods on two benchmarks. We also test the\nutility of MOSA-aligned dLLM on coding, math, and general reasoning. The\nresults strongly prove the superiority of MOSA."}
{"id": "2508.12425", "pdf": "https://arxiv.org/pdf/2508.12425.pdf", "abs": "https://arxiv.org/abs/2508.12425", "title": "Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning", "authors": ["Phuong Minh Nguyen", "Tien Huu Dang", "Naoya Inoue"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved\napproach to standard CoT, for logical reasoning in large language models\n(LLMs). The key idea is to integrate lightweight symbolic representations into\nfew-shot prompts, structuring the inference steps with a consistent strategy to\nmake reasoning patterns more explicit within a non-iterative reasoning process.\nBy incorporating these symbolic structures, our method preserves the\ngeneralizability of standard prompting techniques while enhancing the\ntransparency, interpretability, and analyzability of LLM logical reasoning.\nExtensive experiments on four well-known logical reasoning benchmarks --\nProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse\nreasoning scenarios -- demonstrate the effectiveness of the proposed approach,\nparticularly in complex reasoning tasks that require navigating multiple\nconstraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'\nreasoning capabilities across various model sizes and significantly outperforms\nconventional CoT on three out of four datasets, ProofWriter, ProntoQA, and\nLogicalDeduction."}
{"id": "2508.12430", "pdf": "https://arxiv.org/pdf/2508.12430.pdf", "abs": "https://arxiv.org/abs/2508.12430", "title": "Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations", "authors": ["Yahsin Yeh", "Yilun Wu", "Bokai Ruan", "Honghan Shuai"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Natural language explanations in visual question answering (VQA-NLE) aim to\nmake black-box models more transparent by elucidating their decision-making\nprocesses. However, we find that existing VQA-NLE systems can produce\ninconsistent explanations and reach conclusions without genuinely understanding\nthe underlying context, exposing weaknesses in either their inference pipeline\nor explanation-generation mechanism. To highlight these vulnerabilities, we not\nonly leverage an existing adversarial strategy to perturb questions but also\npropose a novel strategy that minimally alters images to induce contradictory\nor spurious outputs. We further introduce a mitigation method that leverages\nexternal knowledge to alleviate these inconsistencies, thereby bolstering model\nrobustness. Extensive evaluations on two standard benchmarks and two widely\nused VQA-NLE models underscore the effectiveness of our attacks and the\npotential of knowledge-based defenses, ultimately revealing pressing security\nand reliability concerns in current VQA-NLE systems."}
{"id": "2508.12574", "pdf": "https://arxiv.org/pdf/2508.12574.pdf", "abs": "https://arxiv.org/abs/2508.12574", "title": "Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network", "authors": ["Bin Ma", "Yifei Zhang", "Yongjin Xian", "Qi Li", "Linna Zhou", "Gongxun Miao"], "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "With the development of social media networks, rumor detection models have\nattracted more and more attention. Whereas, these models primarily focus on\nclassifying contexts as rumors or not, lacking the capability to locate and\nmark specific rumor content. To address this limitation, this paper proposes a\nnovel rumor detection model named Insight Rumors to locate and mark rumor\ncontent within textual data. Specifically, we propose the Bidirectional Mamba2\nNetwork with Dot-Product Attention (Att_BiMamba2), a network that constructs a\nbidirectional Mamba2 model and applies dot-product attention to weight and\ncombine the outputs from both directions, thereby enhancing the representation\nof high-dimensional rumor features. Simultaneously, a Rumor Locating and\nMarking module is designed to locate and mark rumors. The module constructs a\nskip-connection network to project high-dimensional rumor features onto\nlow-dimensional label features. Moreover, Conditional Random Fields (CRF) is\nemployed to impose strong constraints on the output label features, ensuring\naccurate rumor content location. Additionally, a labeled dataset for rumor\nlocating and marking is constructed, with the effectiveness of the proposed\nmodel is evaluated through comprehensive experiments. Extensive experiments\nindicate that the proposed scheme not only detects rumors accurately but also\nlocates and marks them in context precisely, outperforming state-of-the-art\nschemes that can only discriminate rumors roughly."}
{"id": "2508.12611", "pdf": "https://arxiv.org/pdf/2508.12611.pdf", "abs": "https://arxiv.org/abs/2508.12611", "title": "An LLM + ASP Workflow for Joint Entity-Relation Extraction", "authors": ["Trang Tran", "Trung Hoang Le", "Huiping Cao", "Tran Cao Son"], "categories": ["cs.AI", "cs.CL", "I.2.7; F.4.1"], "comment": "13 pages, 1 figure, Accepted as Technical Communication, 41st\n  International Conference on Logic Programming", "summary": "Joint entity-relation extraction (JERE) identifies both entities and their\nrelationships simultaneously. Traditional machine-learning based approaches to\nperforming this task require a large corpus of annotated data and lack the\nability to easily incorporate domain specific information in the construction\nof the model. Therefore, creating a model for JERE is often labor intensive,\ntime consuming, and elaboration intolerant. In this paper, we propose\nharnessing the capabilities of generative pretrained large language models\n(LLMs) and the knowledge representation and reasoning capabilities of Answer\nSet Programming (ASP) to perform JERE. We present a generic workflow for JERE\nusing LLMs and ASP. The workflow is generic in the sense that it can be applied\nfor JERE in any domain. It takes advantage of LLM's capability in natural\nlanguage understanding in that it works directly with unannotated text. It\nexploits the elaboration tolerant feature of ASP in that no modification of its\ncore program is required when additional domain specific knowledge, in the form\nof type specifications, is found and needs to be used. We demonstrate the\nusefulness of the proposed workflow through experiments with limited training\ndata on three well-known benchmarks for JERE. The results of our experiments\nshow that the LLM + ASP workflow is better than state-of-the-art JERE systems\nin several categories with only 10\\% of training data. It is able to achieve a\n2.5 times (35\\% over 15\\%) improvement in the Relation Extraction task for the\nSciERC corpus, one of the most difficult benchmarks."}
{"id": "2508.12680", "pdf": "https://arxiv.org/pdf/2508.12680.pdf", "abs": "https://arxiv.org/abs/2508.12680", "title": "Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation", "authors": ["Yuheng Zha", "Kun Zhou", "Yujia Wu", "Yushu Wang", "Jie Feng", "Zhi Xu", "Shibo Hao", "Zhengzhong Liu", "Eric P. Xing", "Zhiting Hu"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Despite their success, current training pipelines for reasoning VLMs focus on\na limited range of tasks, such as mathematical and logical reasoning. As a\nresult, these models face difficulties in generalizing their reasoning\ncapabilities to a wide range of domains, primarily due to the scarcity of\nreadily available and verifiable reward data beyond these narrowly defined\nareas. Moreover, integrating data from multiple domains is challenging, as the\ncompatibility between domain-specific datasets remains uncertain. To address\nthese limitations, we build a comprehensive RL-ready visual reasoning dataset\nfrom 46 data sources across 8 dimensions, covering a wide range of tasks such\nas infographic, mathematical, spatial, cross-image, graphic user interface,\nmedical, common sense and general science. We propose an influence function\nbased data selection and difficulty based filtering strategy to identify\nhigh-quality training samples from this dataset. Subsequently, we train the\nVLM, referred to as Vision-G1, using multi-round RL with a data curriculum to\niteratively improve its visual reasoning capabilities. Our model achieves\nstate-of-the-art performance across various visual reasoning benchmarks,\noutperforming similar-sized VLMs and even proprietary models like GPT-4o and\nGemini-1.5 Flash. The model, code and dataset are publicly available at\nhttps://github.com/yuh-zha/Vision-G1."}
{"id": "2508.12790", "pdf": "https://arxiv.org/pdf/2508.12790.pdf", "abs": "https://arxiv.org/abs/2508.12790", "title": "Reinforcement Learning with Rubric Anchors", "authors": ["Zenan Huang", "Yihong Zhuang", "Guoshan Lu", "Zeyu Qin", "Haokai Xu", "Tianyu Zhao", "Ru Peng", "Jiaqi Hu", "Zhanming Shen", "Xiaomeng Hu", "Xijun Gu", "Peiyi Tu", "Jiaxin Liu", "Wenyu Chen", "Yuzhuo Fu", "Zhiting Fan", "Yanmei Gu", "Yuanyuan Wang", "Zhengkai Yang", "Jianguo Li", "Junbo Zhao"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "technical report", "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing Large Language Models (LLMs), exemplified by\nthe success of OpenAI's o-series. In RLVR, rewards are derived from verifiable\nsignals-such as passing unit tests in code generation or matching correct\nanswers in mathematical reasoning. While effective, this requirement largely\nconfines RLVR to domains with automatically checkable outcomes. To overcome\nthis, we extend the RLVR paradigm to open-ended tasks by integrating\nrubric-based rewards, where carefully designed rubrics serve as structured,\nmodel-interpretable criteria for automatic scoring of subjective outputs. We\nconstruct, to our knowledge, the largest rubric reward system to date, with\nover 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.\nImplementing rubric-based RL is challenging; we tackle these issues with a\nclear framework and present an open-sourced Qwen-30B-A3B model with notable\ngains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended\nbenchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by\n+2.4%, while preserving general and reasoning abilities. 2) Our method provides\nfine-grained stylistic control, using rubrics as anchors to mitigate the\n\"AI-like\" tone and produce more human-like, expressive responses. We share key\nlessons in rubric construction, data selection, and training, and discuss\nlimitations and future releases."}
{"id": "2508.12792", "pdf": "https://arxiv.org/pdf/2508.12792.pdf", "abs": "https://arxiv.org/abs/2508.12792", "title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap", "authors": ["Felipe Maia Polo", "Xinhe Wang", "Mikhail Yurochkin", "Gongjun Xu", "Moulinath Banerjee", "Yuekai Sun"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large language models are increasingly used as judges (LLM-as-a-judge) to\nevaluate model outputs at scale, but their assessments often diverge\nsystematically from human judgments. We present Bridge, a unified statistical\nframework that explicitly bridges human and LLM evaluations under both absolute\nscoring and pairwise comparison paradigms. Bridge posits a latent human\npreference score for each prompt-response pair and models LLM deviations as\nlinear transformations of covariates that capture sources of discrepancies.\nThis offers a simple and principled framework for refining LLM ratings and\ncharacterizing systematic discrepancies between humans and LLMs. We provide an\nefficient fitting algorithm with asymptotic guarantees for statistical\ninference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot\nArena), Bridge achieves higher agreement with human ratings (accuracy,\ncalibration, and KL divergence) and exposes systematic human-LLM gaps."}
{"id": "2508.12801", "pdf": "https://arxiv.org/pdf/2508.12801.pdf", "abs": "https://arxiv.org/abs/2508.12801", "title": "Maximum Score Routing For Mixture-of-Experts", "authors": ["Bowen Dong", "Yilong Fan", "Yutao Sun", "Zhenyu Li", "Tengyu Pan", "Xun Zhou", "Jianyong Wang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Routing networks in sparsely activated mixture-of-experts (MoE) dynamically\nallocate input tokens to top-k experts through differentiable sparse\ntransformations, enabling scalable model capacity while preserving\ncomputational efficiency. Traditional MoE networks impose an expert capacity\nconstraint to ensure GPU-friendly computation. However, this leads to token\ndropping when capacity is saturated and results in low hardware efficiency due\nto padding in underutilized experts. Removing the capacity constraint, in turn,\ncompromises load balancing and computational efficiency. To address these\nissues, we propose Maximum Score Routing ($\\mathbf{MaxScore}$), a novel MoE\nrouting paradigm that models routing as a minimum-cost maximum-flow problem and\nintegrates a SoftTopk operator. MaxScore resolves the fundamental limitations\nof iterative rerouting and optimal transport formulations, achieving lower\ntraining losses and higher evaluation scores at equivalent FLOPs compared to\nboth constrained and unconstrained baselines. Implementation details and\nexperimental configurations can be obtained from\n$\\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$."}
{"id": "2508.12815", "pdf": "https://arxiv.org/pdf/2508.12815.pdf", "abs": "https://arxiv.org/abs/2508.12815", "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs", "authors": ["Jayneel Parekh", "Pegah Khayatan", "Mustafa Shukor", "Arnaud Dapogny", "Alasdair Newson", "Matthieu Cord"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines."}
{"id": "2508.12854", "pdf": "https://arxiv.org/pdf/2508.12854.pdf", "abs": "https://arxiv.org/abs/2508.12854", "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model", "authors": ["Ronghao Lin", "Shuai Shen", "Weipeng Hu", "Qiaolin He", "Aolin Xiong", "Li Huang", "Haifeng Hu", "Yap-peng Tan"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MM"], "comment": "Accepted at ACM MM 2025 Grand Challenge", "summary": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG."}
{"id": "2508.12905", "pdf": "https://arxiv.org/pdf/2508.12905.pdf", "abs": "https://arxiv.org/abs/2508.12905", "title": "TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML", "authors": ["Ismail Lamaakal", "Chaymae Yahyati", "Khalid El Makkaoui", "Ibrahim Ouahbi", "Yassine Maleh"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce TCUQ, a single pass, label free uncertainty monitor for\nstreaming TinyML that converts short horizon temporal consistency captured via\nlightweight signals on posteriors and features into a calibrated risk score\nwith an O(W ) ring buffer and O(1) per step updates. A streaming conformal\nlayer turns this score into a budgeted accept/abstain rule, yielding calibrated\nbehavior without online labels or extra forward passes. On microcontrollers,\nTCUQ fits comfortably on kilobyte scale devices and reduces footprint and\nlatency versus early exit and deep ensembles (typically about 50 to 60% smaller\nand about 30 to 45% faster), while methods of similar accuracy often run out of\nmemory. Under corrupted in distribution streams, TCUQ improves accuracy drop\ndetection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high\nseverities; for failure detection it attains up to 0.92 AUROC. These results\nshow that temporal consistency, coupled with streaming conformal calibration,\nprovides a practical and resource efficient foundation for on device monitoring\nin TinyML."}
{"id": "2508.12907", "pdf": "https://arxiv.org/pdf/2508.12907.pdf", "abs": "https://arxiv.org/abs/2508.12907", "title": "SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML", "authors": ["Ismail Lamaakal", "Chaymae Yahyati", "Khalid El Makkaoui", "Ibrahim Ouahbi", "Yassine Maleh"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce \\textbf{SNAP-UQ}, a single-pass, label-free uncertainty method\nfor TinyML that estimates risk from \\emph{depth-wise next-activation\nprediction}: tiny int8 heads forecast the statistics of the next layer from a\ncompressed view of the previous one, and a lightweight monotone mapper turns\nthe resulting surprisal into an actionable score. The design requires no\ntemporal buffers, auxiliary exits, or repeated forward passes, and adds only a\nfew tens of kilobytes to MCU deployments. Across vision and audio backbones,\nSNAP-UQ consistently reduces flash and latency relative to early-exit and deep\nensembles (typically $\\sim$40--60\\% smaller and $\\sim$25--35\\% faster), with\ncompeting methods of similar accuracy often exceeding memory limits. In\ncorrupted streams it improves accuracy-drop detection by several AUPRC points\nand maintains strong failure detection (AUROC $\\approx$0.9) in a single pass.\nGrounding uncertainty in layer-to-layer dynamics yields a practical,\nresource-efficient basis for on-device monitoring in TinyML."}
{"id": "2508.13021", "pdf": "https://arxiv.org/pdf/2508.13021.pdf", "abs": "https://arxiv.org/abs/2508.13021", "title": "PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models", "authors": ["Pengcheng Huang", "Shuhao Liu", "Zhenghao Liu", "Yukun Yan", "Shuo Wang", "Zulong Chen", "Tong Xiao"], "categories": ["cs.AI", "cs.CL"], "comment": "17 pages,13 figures", "summary": "Recent advances in masked diffusion models (MDMs) have established them as\npowerful non-autoregressive alternatives for sequence generation. Nevertheless,\nour preliminary experiments reveal that the generation quality of MDMs is still\nhighly sensitive to the choice of decoding strategy. In particular, widely\nadopted uncertainty-based samplers suffer from two key limitations: a lack of\nglobal trajectory control and a pronounced bias toward trivial tokens in the\nearly stages of decoding. These shortcomings restrict the full potential of\nMDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling\n(PC-Sampler), a novel decoding strategy that unifies global trajectory planning\nwith content-aware informativeness maximization. PC-Sampler incorporates a\nposition-aware weighting mechanism to regulate the decoding path and a\ncalibrated confidence score to suppress the premature selection of trivial\ntokens. Extensive experiments on three advanced MDMs across seven challenging\nbenchmarks-including logical reasoning and planning tasks-demonstrate that\nPC-Sampler consistently outperforms existing MDM decoding strategies by more\nthan 10% on average, significantly narrowing the performance gap with\nstate-of-the-art autoregressive models. All codes are available at\nhttps://github.com/NEUIR/PC-Sampler."}
{"id": "2508.13142", "pdf": "https://arxiv.org/pdf/2508.13142.pdf", "abs": "https://arxiv.org/abs/2508.13142", "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study", "authors": ["Zhongang Cai", "Yubo Wang", "Qingping Sun", "Ruisi Wang", "Chenyang Gu", "Wanqi Yin", "Zhiqian Lin", "Zhitao Yang", "Chen Wei", "Xuanke Shi", "Kewang Deng", "Xiaoyang Han", "Zukai Chen", "Jiaqi Li", "Xiangyu Fan", "Hanming Deng", "Lewei Lu", "Bo Li", "Ziwei Liu", "Quan Wang", "Dahua Lin", "Lei Yang"], "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "cs.RO"], "comment": null, "summary": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models."}
{"id": "2211.09949", "pdf": "https://arxiv.org/pdf/2211.09949.pdf", "abs": "https://arxiv.org/abs/2211.09949", "title": "Is Smaller Always Faster? Tradeoffs in Compressing Self-Supervised Speech Transformers", "authors": ["Tzu-Quan Lin", "Tsung-Huan Yang", "Chun-Yao Chang", "Kuang-Ming Chen", "Tzu-hsun Feng", "Hung-yi Lee", "Hao Tang"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at ASRU 2025. Code is available at\n  https://github.com/nervjack2/Speech-SSL-Compression", "summary": "Transformer-based self-supervised models have achieved remarkable success in\nspeech processing, but their large size and high inference cost present\nsignificant challenges for real-world deployment. While numerous compression\ntechniques have been proposed, inconsistent evaluation metrics make it\ndifficult to compare their practical effectiveness. In this work, we conduct a\ncomprehensive study of four common compression methods, including weight\npruning, head pruning, low-rank approximation, and knowledge distillation on\nself-supervised speech Transformers. We evaluate each method under three key\nmetrics: parameter count, multiply-accumulate operations, and real-time factor.\nResults show that each method offers distinct advantages. In addition, we\ncontextualize recent compression techniques, comparing DistilHuBERT, FitHuBERT,\nLightHuBERT, ARMHuBERT, and STaRHuBERT under the same framework, offering\npractical guidance on compression for deployment."}
{"id": "2310.10679", "pdf": "https://arxiv.org/pdf/2310.10679.pdf", "abs": "https://arxiv.org/abs/2310.10679", "title": "Large language models can replicate cross-cultural differences in personality", "authors": ["PaweÅ Niszczota", "Mateusz Janczak", "MichaÅ Misiak"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4.2; J.4"], "comment": "27 pages: 12 pages of manuscript + 15 pages of supplementary\n  materials; in V3 added information that this is the Author Accepted\n  Manuscript version; in V4 license changed to CC-BY", "summary": "We use a large-scale experiment (N=8000) to determine whether GPT-4 can\nreplicate cross-cultural differences in the Big Five, measured using the\nTen-Item Personality Inventory. We used the US and South Korea as the cultural\npair, given that prior research suggests substantial personality differences\nbetween people from these two countries. We manipulated the target of the\nsimulation (US vs. Korean), the language of the inventory (English vs. Korean),\nand the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4\nreplicated the cross-cultural differences for each factor. However, mean\nratings had an upward bias and exhibited lower variation than in the human\nsamples, as well as lower structural validity. We provide preliminary evidence\nthat LLMs can aid cross-cultural researchers and practitioners."}
{"id": "2405.11430", "pdf": "https://arxiv.org/pdf/2405.11430.pdf", "abs": "https://arxiv.org/abs/2405.11430", "title": "MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation", "authors": ["Jianbo Dai", "Jianqiao Lu", "Yunlong Feng", "Guangtao Zeng", "Rongju Ruan", "Ming Cheng", "Dong Huang", "Haochen Tan", "Zhijiang Guo"], "categories": ["cs.CL"], "comment": "43 pages, dataset and code are available at\n  https://github.com/SparksofAGI/MHPP, leaderboard can be found at\n  https://sparksofagi.github.io/MHPP/", "summary": "Recent advancements in large language models (LLMs) have greatly improved\ncode generation, specifically at the function level. For instance, GPT-4o has\nachieved a 91.0\\% pass rate on HumanEval. However, this draws into question the\nadequacy of existing benchmarks in thoroughly assessing function-level code\ngeneration capabilities. Our study analyzed two common benchmarks, HumanEval\nand MBPP, and found that these might not thoroughly evaluate LLMs' code\ngeneration capacities due to limitations in quality, difficulty, and\ngranularity. To resolve this, we introduce the Mostly Hard Python Problems\n(MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on\nthe combination of natural language and code reasoning, MHPP gauges LLMs'\nabilities to comprehend specifications and restrictions, engage in multi-step\nreasoning, and apply coding knowledge effectively. Initial evaluations of 26\nLLMs using MHPP showed many high-performing models on HumanEval failed to\nachieve similar success on MHPP. Moreover, MHPP highlighted various previously\nundiscovered limitations within various LLMs, leading us to believe that it\ncould pave the way for a better understanding of LLMs' capabilities and\nlimitations. MHPP, evaluation pipeline, and leaderboard can be found in\nhttps://github.com/SparksofAGI/MHPP."}
{"id": "2406.05328", "pdf": "https://arxiv.org/pdf/2406.05328.pdf", "abs": "https://arxiv.org/abs/2406.05328", "title": "FacLens: Transferable Probe for Foreseeing Non-Factuality in Fact-Seeking Question Answering of Large Language Models", "authors": ["Yanling Wang", "Haoyang Li", "Hao Zou", "Jing Zhang", "Xinlei He", "Qi Li", "Ke Xu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite advancements in large language models (LLMs), non-factual responses\nstill persist in fact-seeking question answering. Unlike extensive studies on\npost-hoc detection of these responses, this work studies non-factuality\nprediction (NFP), predicting whether an LLM will generate a non-factual\nresponse prior to the response generation. Previous NFP methods have shown\nLLMs' awareness of their knowledge, but they face challenges in terms of\nefficiency and transferability. In this work, we propose a lightweight model\nnamed Factuality Lens (FacLens), which effectively probes hidden\nrepresentations of fact-seeking questions for the NFP task. Moreover, we\ndiscover that hidden question representations sourced from different LLMs\nexhibit similar NFP patterns, enabling the transferability of FacLens across\ndifferent LLMs to reduce development costs. Extensive experiments highlight\nFacLens's superiority in both effectiveness and efficiency."}
{"id": "2409.09866", "pdf": "https://arxiv.org/pdf/2409.09866.pdf", "abs": "https://arxiv.org/abs/2409.09866", "title": "S2Cap: A Benchmark and a Baseline for Singing Style Captioning", "authors": ["Hyunjong Ok", "Jaeho Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "CIKM 2025 Resource Paper", "summary": "Singing voices contain much richer information than common voices, including\nvaried vocal and acoustic properties. However, current open-source audio-text\ndatasets for singing voices capture only a narrow range of attributes and lack\nacoustic features, leading to limited utility towards downstream tasks, such as\nstyle captioning. To fill this gap, we formally define the singing style\ncaptioning task and present S2Cap, a dataset of singing voices with detailed\ndescriptions covering diverse vocal, acoustic, and demographic characteristics.\nUsing this dataset, we develop an efficient and straightforward baseline\nalgorithm for singing style captioning. The dataset is available at\nhttps://zenodo.org/records/15673764."}
{"id": "2409.11041", "pdf": "https://arxiv.org/pdf/2409.11041.pdf", "abs": "https://arxiv.org/abs/2409.11041", "title": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "comment": "Accepted to ITL4HRI workshop at RO-MAN 2025 conference", "summary": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops)."}
{"id": "2410.05362", "pdf": "https://arxiv.org/pdf/2410.05362.pdf", "abs": "https://arxiv.org/abs/2410.05362", "title": "LLMs Are In-Context Bandit Reinforcement Learners", "authors": ["Giovanni Monea", "Antoine Bosselut", "KiantÃ© Brantley", "Yoav Artzi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published at COLM 2025", "summary": "Large Language Models (LLMs) excel at in-context learning (ICL), a supervised\nlearning technique that relies on adding annotated examples to the model\ncontext. We investigate a contextual bandit version of in-context reinforcement\nlearning (ICRL), where models learn in-context, online, from external reward,\ninstead of supervised data. We show that LLMs effectively demonstrate such\nlearning, and provide a detailed study of the phenomena, experimenting with\nchallenging classification tasks and models of sizes from 500M to 70B\nparameters. This includes identifying and addressing the instability of the\nprocess, demonstrating learning with both semantic and abstract labels, and\nshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, while\nalso underscoring fundamental limitations in their implicit reasoning about\nerrors."}
{"id": "2410.07745", "pdf": "https://arxiv.org/pdf/2410.07745.pdf", "abs": "https://arxiv.org/abs/2410.07745", "title": "StepTool: Enhancing Multi-Step Tool Usage in LLMs via Step-Grained Reinforcement Learning", "authors": ["Yuanqing Yu", "Zhefan Wang", "Weizhi Ma", "Shuai Wang", "Chuhan Wu", "Zhiqiang Guo", "Min Zhang"], "categories": ["cs.CL"], "comment": "Accepted by CIKM'25", "summary": "Despite their powerful text generation capabilities, large language models\n(LLMs) still struggle to effectively utilize external tools to solve complex\ntasks, a challenge known as tool learning. Existing methods primarily rely on\nsupervised fine-tuning, treating tool learning as a text generation problem\nwhile overlooking the decision-making complexities inherent in multi-step\ncontexts. In this work, we propose modeling tool learning as a dynamic\ndecision-making process and introduce StepTool, a novel step-grained\nreinforcement learning framework that enhances LLMs' capabilities in multi-step\ntool use. StepTool comprises two key components: Step-grained Reward Shaping,\nwhich assigns rewards to each tool interaction based on its invocation success\nand contribution to task completion; and Step-grained Optimization, which\napplies policy gradient methods to optimize the model across multiple decision\nsteps. Extensive experiments across diverse benchmarks show that StepTool\nconsistently outperforms both SFT-based and RL-based baselines in terms of task\nPass Rate and Recall of relevant tools. Furthermore, our analysis suggests that\nStepTool helps models discover new tool-use strategies rather than merely\nre-weighting prior knowledge. These results highlight the importance of\nfine-grained decision modeling in tool learning and establish StepTool as a\ngeneral and robust solution for enhancing multi-step tool use in LLMs. Code and\ndata are available at https://github.com/yuyq18/StepTool."}
{"id": "2411.01077", "pdf": "https://arxiv.org/pdf/2411.01077.pdf", "abs": "https://arxiv.org/abs/2411.01077", "title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection", "authors": ["Zhipeng Wei", "Yuqi Liu", "N. Benjamin Erichson"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted output, posing a potential threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This alters the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the unsafe prediction rate, bypassing existing\nsafeguards."}
{"id": "2411.02083", "pdf": "https://arxiv.org/pdf/2411.02083.pdf", "abs": "https://arxiv.org/abs/2411.02083", "title": "Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models", "authors": ["Jonas Zausinger", "Lars Pennig", "Anamarija Kozina", "Sean Sdahl", "Julian Sikora", "Adrian Dendorfer", "Timofey Kuznetsov", "Mohamad Hagog", "Nina Wiedemann", "Kacper Chlodny", "Vincent Limbach", "Anna Ketteler", "Thorben Prein", "Vishwa Mohan Singh", "Michael Morris Danziger", "Jannis Born"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "comment": "ICML 2025", "summary": "While language models have exceptional capabilities at text generation, they\nlack a natural inductive bias for emitting numbers and thus struggle in tasks\ninvolving quantitative reasoning, especially arithmetic. One fundamental\nlimitation is the nature of the cross-entropy (CE) loss, which assumes a\nnominal scale and thus cannot convey proximity between generated number tokens.\nIn response, we here present a regression-like loss that operates purely on\ntoken level. Our proposed Number Token Loss (NTL) comes in two flavors and\nminimizes either the $L_p$ norm or the Wasserstein distance between the\nnumerical values of the real and predicted number tokens. NTL can easily be\nadded to any language model and extend the CE objective during training without\nruntime overhead. We evaluate the proposed scheme on various mathematical\ndatasets and find that it consistently improves performance in math-related\ntasks. In a direct comparison on a regression task, we find that NTL can match\nthe performance of a regression head, despite operating on token level.\nFinally, we scale NTL up to 3B parameter models and observe improved\nperformance, demonstrating its potential for seamless integration into LLMs. We\nhope to inspire LLM developers to improve their pretraining objectives and\ndistribute NTL as a minimalistic and lightweight PyPI package $ntloss$:\nhttps://github.com/ai4sd/number-token-loss. Development code for full paper\nreproduction is available separately."}
{"id": "2411.16252", "pdf": "https://arxiv.org/pdf/2411.16252.pdf", "abs": "https://arxiv.org/abs/2411.16252", "title": "NormXLogit: The Head-on-Top Never Lies", "authors": ["Sina Abbasi", "Mohammad Reza Modarres", "Mohammad Taher Pilehvar"], "categories": ["cs.CL"], "comment": "Added comparisons on computational efficiency, included experiments\n  on a new dataset with an additional evaluation metric for classification\n  tasks, expanded explanations and discussions in the experiments, and\n  presented a worked example for alignment metrics computation", "summary": "With new large language models (LLMs) emerging frequently, it is important to\nconsider the potential value of model-agnostic approaches that can provide\ninterpretability across a variety of architectures. While recent advances in\nLLM interpretability show promise, many rely on complex, model-specific methods\nwith high computational costs. To address these limitations, we propose\nNormXLogit, a novel technique for assessing the significance of individual\ninput tokens. This method operates based on the input and output\nrepresentations associated with each token. First, we demonstrate that during\nthe pre-training of LLMs, the norms of word embeddings effectively capture\ntoken importance. Second, we reveal a significant relationship between a\ntoken's importance and the extent to which its representation can resemble the\nmodel's final prediction. Extensive analyses reveal that our approach\noutperforms existing gradient-based methods in terms of faithfulness and offers\ncompetitive performance in layer-wise explanations compared to leading\narchitecture-specific techniques."}
{"id": "2412.16976", "pdf": "https://arxiv.org/pdf/2412.16976.pdf", "abs": "https://arxiv.org/abs/2412.16976", "title": "On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity Recognition in Health Corpora", "authors": ["Tzu-Chieh Chen", "Wen-Yang Lin"], "categories": ["cs.CL", "cs.AI", "I.2.7; J.3"], "comment": "13 pages; a short version named \"Beyond GPT-NER: ChatGPT as Ensemble\n  Arbitrator for Discontinuous Named Entity Recognition in Health Corpora\" has\n  been accpeted for presentation at MedInfo2025", "summary": "Named Entity Recognition has traditionally been a key task in natural\nlanguage processing, aiming to identify and extract important terms from\nunstructured text data. However, a notable challenge for contemporary\ndeep-learning NER models has been identifying discontinuous entities, which are\noften fragmented within the text. To date, methods to address Discontinuous\nNamed Entity Recognition have not been explored using ensemble learning to the\nbest of our knowledge. Furthermore, the rise of large language models, such as\nChatGPT in recent years, has shown significant effectiveness across many NLP\ntasks. Most existing approaches, however, have primarily utilized ChatGPT as a\nproblem-solving tool rather than exploring its potential as an integrative\nelement within ensemble learning algorithms. In this study, we investigated the\nintegration of ChatGPT as an arbitrator within an ensemble method, aiming to\nenhance performance on DNER tasks. Our method combines five state-of-the-art\nNER models with ChatGPT using custom prompt engineering to assess the\nrobustness and generalization capabilities of the ensemble algorithm. We\nconducted experiments on three benchmark medical datasets, comparing our method\nagainst the five SOTA models, individual applications of GPT-3.5 and GPT-4, and\na voting ensemble method. The results indicate that our proposed fusion of\nChatGPT with the ensemble learning algorithm outperforms the SOTA results in\nthe CADEC, ShARe13, and ShARe14 datasets, showcasing its potential to enhance\nNLP applications in the healthcare domain."}
{"id": "2501.14528", "pdf": "https://arxiv.org/pdf/2501.14528.pdf", "abs": "https://arxiv.org/abs/2501.14528", "title": "Idiom Detection in Sorani Kurdish Texts", "authors": ["Skala Kamaran Omer", "Hossein Hassani"], "categories": ["cs.CL"], "comment": "22 pages, 8 figures, 7 tables", "summary": "Idiom detection using Natural Language Processing (NLP) is the computerized\nprocess of recognizing figurative expressions within a text that convey\nmeanings beyond the literal interpretation of the words. While idiom detection\nhas seen significant progress across various languages, the Kurdish language\nfaces a considerable research gap in this area despite the importance of idioms\nin tasks like machine translation and sentiment analysis. This study addresses\nidiom detection in Sorani Kurdish by approaching it as a text classification\ntask using deep learning techniques. To tackle this, we developed a dataset\ncontaining 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse\ncontexts. Using this dataset, we developed and evaluated three deep learning\nmodels: KuBERT-based transformer sequence classification, a Recurrent\nConvolutional Neural Network (RCNN), and a BiLSTM model with an attention\nmechanism. The evaluations revealed that the transformer model, the fine-tuned\nBERT, consistently outperformed the others, achieving nearly 99% accuracy while\nthe RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the\neffectiveness of Transformer-based architectures in low-resource languages like\nKurdish. This research provides a dataset, three optimized models, and insights\ninto idiom detection, laying a foundation for advancing Kurdish NLP."}
{"id": "2501.17771", "pdf": "https://arxiv.org/pdf/2501.17771.pdf", "abs": "https://arxiv.org/abs/2501.17771", "title": "2SSP: A Two-Stage Framework for Structured Pruning of LLMs", "authors": ["Fabrizio Sandri", "Elia Cunegatti", "Giovanni Iacca"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in Transactions on Machine Learning Research (TMLR)", "summary": "We propose a novel Two-Stage framework for Structured Pruning (\\textsc{2SSP})\nfor pruning Large Language Models (LLMs), which combines two different\nstrategies of pruning, namely Width and Depth Pruning. The first stage (Width\nPruning) removes entire neurons, hence their corresponding rows and columns,\naiming to preserve the connectivity among the pruned structures in the\nintermediate state of the Feed-Forward Networks in each Transformer block. This\nis done based on an importance score measuring the impact of each neuron on the\noutput magnitude. The second stage (Depth Pruning), instead, removes entire\nAttention submodules. This is done by applying an iterative process that\nremoves the Attention with the minimum impact on a given metric of interest (in\nour case, perplexity). We also propose a novel mechanism to balance the\nsparsity rate of the two stages w.r.t. to the desired global sparsity. We test\n\\textsc{2SSP} on four LLM families and three sparsity rates (25\\%, 37.5\\%, and\n50\\%), measuring the resulting perplexity over three language modeling datasets\nas well as the performance over six downstream tasks. Our method consistently\noutperforms five state-of-the-art competitors over three language modeling and\nsix downstream tasks, with an up to two-order-of-magnitude gain in terms of\npruning time. The code is available at https://github.com/FabrizioSandri/2SSP."}
{"id": "2501.19258", "pdf": "https://arxiv.org/pdf/2501.19258.pdf", "abs": "https://arxiv.org/abs/2501.19258", "title": "VisualSpeech: Enhancing Prosody Modeling in TTS Using Video", "authors": ["Shumin Que", "Anton Ragni"], "categories": ["cs.CL"], "comment": null, "summary": "Text-to-Speech (TTS) synthesis faces the inherent challenge of producing\nmultiple speech outputs with varying prosody given a single text input. While\nprevious research has addressed this by predicting prosodic information from\nboth text and speech, additional contextual information, such as video, remains\nunder-utilized despite being available in many applications. This paper\ninvestigates the potential of integrating visual context to enhance prosody\nprediction. We propose a novel model, VisualSpeech, which incorporates visual\nand textual information for improving prosody generation in TTS. Empirical\nresults indicate that incorporating visual features improves prosodic modeling,\nenhancing the expressiveness of the synthesized speech. Audio samples are\navailable at https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/."}
{"id": "2502.08266", "pdf": "https://arxiv.org/pdf/2502.08266.pdf", "abs": "https://arxiv.org/abs/2502.08266", "title": "Dealing with Annotator Disagreement in Hate Speech Classification", "authors": ["Somaiyeh Dehghan", "Mehmet Umut Sen", "Berrin Yanikoglu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; I.2.7"], "comment": "20 pages, 3 Tables", "summary": "Hate speech detection is a crucial task, especially on social media, where\nharmful content can spread quickly. Implementing machine learning models to\nautomatically identify and address hate speech is essential for mitigating its\nimpact and preventing its proliferation. The first step in developing an\neffective hate speech detection model is to acquire a high-quality dataset for\ntraining. Labeled data is essential for most natural language processing tasks,\nbut categorizing hate speech is difficult due to the diverse and often\nsubjective nature of hate speech, which can lead to varying interpretations and\ndisagreements among annotators. This paper examines strategies for addressing\nannotator disagreement, an issue that has been largely overlooked. In\nparticular, we evaluate various automatic approaches for aggregating multiple\nannotations, in the context of hate speech classification in Turkish tweets.\nOur work highlights the importance of the problem and provides state-of-the-art\nbenchmark results for the detection and understanding of hate speech in online\ndiscourse."}
{"id": "2502.13959", "pdf": "https://arxiv.org/pdf/2502.13959.pdf", "abs": "https://arxiv.org/abs/2502.13959", "title": "LIDDIA: Language-based Intelligent Drug Discovery Agent", "authors": ["Reza Averly", "Frazier N. Baker", "Ian A. Watson", "Xia Ning"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Drug discovery is a long, expensive, and complex process, relying heavily on\nhuman medicinal chemists, who can spend years searching the vast space of\npotential therapies. Recent advances in artificial intelligence for chemistry\nhave sought to expedite individual drug discovery tasks; however, there remains\na critical need for an intelligent agent that can navigate the drug discovery\nprocess. Towards this end, we introduce LIDDIA, an autonomous agent capable of\nintelligently navigating the drug discovery process in silico. By leveraging\nthe reasoning capabilities of large language models, LIDDIA serves as a\nlow-cost and highly-adaptable tool for autonomous drug discovery. We\ncomprehensively examine LIDDIA , demonstrating that (1) it can generate\nmolecules meeting key pharmaceutical criteria on over 70% of 30 clinically\nrelevant targets, (2) it intelligently balances exploration and exploitation in\nthe chemical space, and (3) it identifies one promising novel candidate on\nAR/NR3C4, a critical target for both prostate and breast cancers. Code and\ndataset are available at https://github.com/ninglab/LIDDiA"}
{"id": "2503.04721", "pdf": "https://arxiv.org/pdf/2503.04721.pdf", "abs": "https://arxiv.org/abs/2503.04721", "title": "Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities", "authors": ["Guan-Ting Lin", "Jiachen Lian", "Tingle Li", "Qirui Wang", "Gopala Anumanchipalli", "Alexander H. Liu", "Hung-yi Lee"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted by ASRU 2025", "summary": "Spoken dialogue modeling poses challenges beyond text-based language\nmodeling, requiring real-time interaction, turn-taking, and backchanneling.\nWhile most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing\none turn at a time - emerging full-duplex SDMs can listen and speak\nsimultaneously, enabling more natural conversations. However, current\nevaluations remain limited, focusing mainly on turn-based metrics or coarse\ncorpus-level analyses. To address this, we introduce Full-Duplex-Bench, a\nbenchmark that systematically evaluates key interactive behaviors: pause\nhandling, backchanneling, turn-taking, and interruption management. Our\nframework uses automatic metrics for consistent, reproducible assessment and\nprovides a fair, fast evaluation setup. By releasing our benchmark and code, we\naim to advance spoken dialogue modeling and foster the development of more\nnatural and engaging SDMs."}
{"id": "2503.07303", "pdf": "https://arxiv.org/pdf/2503.07303.pdf", "abs": "https://arxiv.org/abs/2503.07303", "title": "An Information-Theoretic Approach to Identifying Formulaic Clusters in Textual Data", "authors": ["Gideon Yoffe", "Yair Segev", "Barak Sober"], "categories": ["cs.CL"], "comment": null, "summary": "Texts, whether literary or historical, exhibit structural and stylistic\npatterns shaped by their purpose, authorship, and cultural context. Formulaic\ntexts, characterized by repetition and constrained expression, tend to have\nlower variability in self-information compared to more dynamic compositions.\nIdentifying such patterns in historical documents, particularly multi-author\ntexts like the Hebrew Bible provides insights into their origins, purpose, and\ntransmission.\n  This study aims to identify formulaic clusters -- sections exhibiting\nsystematic repetition and structural constraints -- by analyzing recurring\nphrases, syntactic structures, and stylistic markers. However, distinguishing\nformulaic from non-formulaic elements in an unsupervised manner presents a\ncomputational challenge, especially in high-dimensional textual spaces where\npatterns must be inferred without predefined labels.\n  To address this, we develop an information-theoretic algorithm leveraging\nweighted self-information distributions to detect structured patterns in text,\nunlike covariance-based methods, which become unstable in small-sample,\nhigh-dimensional settings, our approach directly models variations in\nself-information to identify formulaicity. By extending classical discrete\nself-information measures with a continuous formulation based on differential\nself-information, our method remains applicable across different types of\ntextual representations, including neural embeddings under Gaussian priors.\n  Applied to hypothesized authorial divisions in the Hebrew Bible, our approach\nsuccessfully isolates stylistic layers, providing a quantitative framework for\ntextual stratification. This method enhances our ability to analyze\ncompositional patterns, offering deeper insights into the literary and cultural\nevolution of texts shaped by complex authorship and editorial processes."}
{"id": "2503.11280", "pdf": "https://arxiv.org/pdf/2503.11280.pdf", "abs": "https://arxiv.org/abs/2503.11280", "title": "High-Dimensional Interlingual Representations of Large Language Models", "authors": ["Bryan Wilie", "Samuel Cahyawijaya", "Junxian He", "Pascale Fung"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning."}
{"id": "2503.15904", "pdf": "https://arxiv.org/pdf/2503.15904.pdf", "abs": "https://arxiv.org/abs/2503.15904", "title": "More Women, Same Stereotypes: Unpacking the Gender Bias Paradox in Large Language Models", "authors": ["Evan Chen", "Run-Jun Zhan", "Yan-Bai Lin", "Hung-Hsuan Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet concerns persist regarding their tendency to reflect or amplify social\nbiases. This study introduces a novel evaluation framework to uncover gender\nbiases in LLMs: using free-form storytelling to surface biases embedded within\nthe models. A systematic analysis of ten prominent LLMs shows a consistent\npattern of overrepresenting female characters across occupations, likely due to\nsupervised fine-tuning (SFT) and reinforcement learning from human feedback\n(RLHF). Paradoxically, despite this overrepresentation, the occupational gender\ndistributions produced by these LLMs align more closely with human stereotypes\nthan with real-world labor data. This highlights the challenge and importance\nof implementing balanced mitigation measures to promote fairness and prevent\nthe establishment of potentially new biases. We release the prompts and\nLLM-generated stories at GitHub."}
{"id": "2503.17287", "pdf": "https://arxiv.org/pdf/2503.17287.pdf", "abs": "https://arxiv.org/abs/2503.17287", "title": "FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models", "authors": ["Mingyang Song", "Mao Zheng", "Zheng Li", "Wenjie Yang", "Xuan Luo", "Yue Pan", "Feng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Improving training efficiency continues to be one of the primary challenges\nin large-scale Reinforcement Learning (RL). In this paper, we investigate how\ncontext length and the complexity of training data influence the RL scaling\ntraining process of R1-distilled reasoning models, e.g.,\nDeepSeek-R1-Distill-Qwen-1.5B. Our experimental results reveal that: (1) simply\ncontrolling the context length and curating the training data based on the\ninput prompt length can effectively improve the training efficiency of RL\nscaling, achieving better performance with more concise CoT; (2) properly\nscaling the context length helps mitigate entropy collapse; and (3) carefully\nchoosing the context length facilitates achieving efficient LLM training and\nreasoning. Inspired by these insights, we propose FastCuRL, a curriculum RL\nframework with stage-wise context scaling to achieve efficient LLM training and\nreasoning. Extensive experimental results demonstrate that FastCuRL-1.5B-V3\nsignificantly outperforms state-of-the-art reasoning models on five\ncompetition-level benchmarks and achieves 49.6% accuracy on AIME 2024.\nFurthermore, FastCuRL-1.5B-Preview surpasses DeepScaleR-1.5B-Preview on five\nbenchmarks while only using a single node with 8 GPUs and a total of 50% of\ntraining steps."}
{"id": "2503.17811", "pdf": "https://arxiv.org/pdf/2503.17811.pdf", "abs": "https://arxiv.org/abs/2503.17811", "title": "Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models", "authors": ["Wenqi Pei", "Hailing Xu", "Hengyuan Zhao", "Shizheng Hou", "Han Chen", "Zining Zhang", "Pingyi Luo", "Bingsheng He"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "DL4C @ ICLR 2025", "summary": "Natural Language to SQL (NL2SQL) has seen significant advancements with large\nlanguage models (LLMs). However, these models often depend on closed-source\nsystems and high computational resources, posing challenges in data privacy and\ndeployment. In contrast, small language models (SLMs) struggle with NL2SQL\ntasks, exhibiting poor performance and incompatibility with existing\nframeworks. To address these issues, we introduce Feather-SQL, a new\nlightweight framework tailored for SLMs. Feather-SQL improves SQL executability\nand accuracy through 1) schema pruning and linking, 2) multi-path and\nmulti-candidate generation. Additionally, we introduce the 1+1 Model\nCollaboration Paradigm, which pairs a strong general-purpose chat model with a\nfine-tuned SQL specialist, combining strong analytical reasoning with\nhigh-precision SQL generation. Experimental results on BIRD demonstrate that\nFeather-SQL improves NL2SQL performance on SLMs, with around 10% boost for\nmodels without fine-tuning. The proposed paradigm raises the accuracy ceiling\nof SLMs to 54.76%, highlighting its effectiveness."}
{"id": "2503.23512", "pdf": "https://arxiv.org/pdf/2503.23512.pdf", "abs": "https://arxiv.org/abs/2503.23512", "title": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives", "authors": ["Qiang Yi", "Yangfan He", "Jianhui Wang", "Xinyuan Song", "Shiyao Qian", "Xinhang Yuan", "Li Sun", "Yi Xin", "Jingqun Tang", "Keqin Li", "Kuan Lu", "Menghao Huo", "Jiaqi Chen", "Tianyu Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach, incorporating TF-IDF and cosine similarity to\nidentify related episodes and enhance the overall story structure. Results from\ntesting multiple LLM-generated stories demonstrate that SCORE significantly\nimproves the consistency and stability of narrative coherence compared to\nbaseline GPT models, providing a more robust method for evaluating and refining\nAI-generated narratives."}
{"id": "2503.24115", "pdf": "https://arxiv.org/pdf/2503.24115.pdf", "abs": "https://arxiv.org/abs/2503.24115", "title": "TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection", "authors": ["Zhiming Ma", "Peidong Wang", "Minhua Huang", "Jingpeng Wang", "Kai Wu", "Xiangzhao Lv", "Yachun Pang", "Yin Yang", "Wenjie Tang", "Yuchen Kang"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud."}
{"id": "2504.03454", "pdf": "https://arxiv.org/pdf/2504.03454.pdf", "abs": "https://arxiv.org/abs/2504.03454", "title": "SpectR: Dynamically Composing LM Experts with Spectral Routing", "authors": ["William Fleshman", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Training large, general-purpose language models poses significant challenges.\nThe growing availability of specialized expert models, fine-tuned from\npretrained models for specific tasks or domains, offers a promising\nalternative. Leveraging the potential of these existing expert models in\nreal-world applications requires effective methods to select or merge the\nmodels best suited for a given task. This paper introduces SPECTR, an approach\nfor dynamically composing expert models at each time step during inference.\nNotably, our method requires no additional training and enables flexible,\ntoken- and layer-wise model combinations. Our experimental results demonstrate\nthat SPECTR improves routing accuracy over alternative training-free methods,\nincreasing task performance across expert domains."}
{"id": "2504.04823", "pdf": "https://arxiv.org/pdf/2504.04823.pdf", "abs": "https://arxiv.org/abs/2504.04823", "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models", "authors": ["Ruikang Liu", "Yuxuan Sun", "Manyi Zhang", "Haoli Bai", "Xianzhi Yu", "Tiezheng Yu", "Chun Yuan", "Lu Hou"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025", "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."}
{"id": "2504.05410", "pdf": "https://arxiv.org/pdf/2504.05410.pdf", "abs": "https://arxiv.org/abs/2504.05410", "title": "Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling", "authors": ["Benjamin Lipkin", "Benjamin LeBrun", "Jacob Hoover Vigly", "JoÃ£o Loula", "David R. MacIver", "Li Du", "Jason Eisner", "Ryan Cotterell", "Vikash Mansinghka", "Timothy J. O'Donnell", "Alexander K. Lew", "Tim Vieira"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "COLM 2025", "summary": "The dominant approach to generating from language models subject to some\nconstraint is locally constrained decoding (LCD), incrementally sampling tokens\nat each time step such that the constraint is never violated. Typically, this\nis achieved through token masking: looping over the vocabulary and excluding\nnon-conforming tokens. There are two important problems with this approach. (i)\nEvaluating the constraint on every token can be prohibitively expensive -- LM\nvocabularies often exceed $100,000$ tokens. (ii) LCD can distort the global\ndistribution over strings, sampling tokens based only on local information,\neven if they lead down dead-end paths. This work introduces a new algorithm\nthat addresses both these problems. First, to avoid evaluating a constraint on\nthe full vocabulary at each step of generation, we propose an adaptive\nrejection sampling algorithm that typically requires orders of magnitude fewer\nconstraint evaluations. Second, we show how this algorithm can be extended to\nproduce low-variance, unbiased estimates of importance weights at a very small\nadditional cost -- estimates that can be soundly used within previously\nproposed sequential Monte Carlo algorithms to correct for the myopic behavior\nof local constraint enforcement. Through extensive empirical evaluation in\ntext-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON\ndomains, we show that our approach is superior to state-of-the-art baselines,\nsupporting a broader class of constraints and improving both runtime and\nperformance. Additional theoretical and empirical analyses show that our\nmethod's runtime efficiency is driven by its dynamic use of computation,\nscaling with the divergence between the unconstrained and constrained LM, and\nas a consequence, runtime improvements are greater for better models."}
{"id": "2504.15219", "pdf": "https://arxiv.org/pdf/2504.15219.pdf", "abs": "https://arxiv.org/abs/2504.15219", "title": "EvalAgent: Discovering Implicit Evaluation Criteria from the Web", "authors": ["Manya Wadhwa", "Zayne Sprague", "Chaitanya Malaviya", "Philippe Laban", "Junyi Jessy Li", "Greg Durrett"], "categories": ["cs.CL"], "comment": "Published at COLM 2025", "summary": "Evaluation of language model outputs on structured writing tasks is typically\nconducted with a number of desirable criteria presented to human evaluators or\nlarge language models (LLMs). For instance, on a prompt like \"Help me draft an\nacademic talk on coffee intake vs research productivity\", a model response may\nbe evaluated for criteria like accuracy and coherence. However, high-quality\nresponses should do more than just satisfy basic task requirements. An\neffective response to this query should include quintessential features of an\nacademic talk, such as a compelling opening, clear research questions, and a\ntakeaway. To help identify these implicit criteria, we introduce EvalAgent, a\nnovel framework designed to automatically uncover nuanced and task-specific\ncriteria. EvalAgent first mines expert-authored online guidance. It then uses\nthis evidence to propose diverse, long-tail evaluation criteria that are\ngrounded in reliable external sources. Our experiments demonstrate that the\ngrounded criteria produced by EvalAgent are often implicit (not directly stated\nin the user's prompt), yet specific (high degree of lexical precision).\nFurther, EvalAgent criteria are often not satisfied by initial responses but\nthey are actionable, such that responses can be refined to satisfy them.\nFinally, we show that combining LLM-generated and EvalAgent criteria uncovers\nmore human-valued criteria than using LLMs alone."}
{"id": "2505.01479", "pdf": "https://arxiv.org/pdf/2505.01479.pdf", "abs": "https://arxiv.org/abs/2505.01479", "title": "Deliberate Planning in Language Models with Symbolic Representation", "authors": ["Siheng Xiong", "Zhangding Liu", "Jieyu Zhou", "Yusen Su"], "categories": ["cs.CL"], "comment": null, "summary": "Planning remains a core challenge for language models (LMs), particularly in\ndomains that require coherent multi-step action sequences grounded in external\nconstraints. We introduce SymPlanner, a novel framework that equips LMs with\nstructured planning capabilities by interfacing them with a symbolic\nenvironment that serves as an explicit world model. Rather than relying purely\non natural language reasoning, SymPlanner grounds the planning process in a\nsymbolic state space, where a policy model proposes actions and a symbolic\nenvironment deterministically executes and verifies their effects. To enhance\nexploration and improve robustness, we introduce Iterative Correction (IC),\nwhich refines previously proposed actions by leveraging feedback from the\nsymbolic environment to eliminate invalid decisions and guide the model toward\nvalid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained\ncomparison of candidate plans by evaluating them jointly. We evaluate\nSymPlanner on PlanBench, demonstrating that it produces more coherent, diverse,\nand verifiable plans than pure natural language baselines."}
{"id": "2505.06987", "pdf": "https://arxiv.org/pdf/2505.06987.pdf", "abs": "https://arxiv.org/abs/2505.06987", "title": "Convert Language Model into a Value-based Strategic Planner", "authors": ["Xiaoyu Wang", "Yue Zhao", "Qingqing Gu", "Zhonglin Jiang", "Xiaokai Chen", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 6 figures, ACL 2025 Industry Track", "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines."}
{"id": "2505.14425", "pdf": "https://arxiv.org/pdf/2505.14425.pdf", "abs": "https://arxiv.org/abs/2505.14425", "title": "From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "comment": "17 pages", "summary": "Instruction-tuned large language models (LLMs) have shown strong performance\non a variety of tasks; however, generalizing from synthetic to human-authored\ninstructions in grounded environments remains a challenge for them. In this\nwork, we study generalization challenges in spatial grounding tasks where\nmodels interpret and translate instructions for building object arrangements on\na $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate\ntheir performance on a benchmark dataset containing both synthetic and\nhuman-written instructions. Our results reveal that while models generalize\nwell on simple tasks, their performance degrades significantly on more complex\ntasks. We present a detailed error analysis of the gaps in instruction\ngeneralization."}
{"id": "2505.19743", "pdf": "https://arxiv.org/pdf/2505.19743.pdf", "abs": "https://arxiv.org/abs/2505.19743", "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models", "authors": ["Yang Zhang", "Yu Yu", "Bo Tang", "Yu Zhu", "Chuxiong Sun", "Wenqiang Wei", "Jie Hu", "Zipeng Xie", "Zhiyu Li", "Feiyu Xiong", "Edward Chung"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs. The source code and\nimplementation details are publicly available at\nhttps://github.com/IAAR-Shanghai/MARA, and the trained models are released at\nhttps://huggingface.co/IAAR-Shanghai/MARA_AGENTS."}
{"id": "2505.20841", "pdf": "https://arxiv.org/pdf/2505.20841.pdf", "abs": "https://arxiv.org/abs/2505.20841", "title": "Concealment of Intent: A Game-Theoretic Analysis", "authors": ["Xinbo Wu", "Abhishek Umrawal", "Lav R. Varshney"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) grow more capable, concerns about their safe\ndeployment have also grown. Although alignment mechanisms have been introduced\nto deter misuse, they remain vulnerable to carefully designed adversarial\nprompts. In this work, we present a scalable attack strategy: intent-hiding\nadversarial prompting, which conceals malicious intent through the composition\nof skills. We develop a game-theoretic framework to model the interaction\nbetween such attacks and defense systems that apply both prompt and response\nfiltering. Our analysis identifies equilibrium points and reveals structural\nadvantages for the attacker. To counter these threats, we propose and analyze a\ndefense mechanism tailored to intent-hiding attacks. Empirically, we validate\nthe attack's effectiveness on multiple real-world LLMs across a range of\nmalicious behaviors, demonstrating clear advantages over existing adversarial\nprompting techniques."}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657.pdf", "abs": "https://arxiv.org/abs/2505.21657", "title": "Explaining Large Language Models with gSMILE", "authors": ["Zeinab Dehghani", "Mohammed Naveed Akram", "Koorosh Aslansefat", "Adil Khan", "Yiannis Papadopoulos"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) such as GPT, LLaMA, and Claude achieve\nremarkable performance in text generation but remain opaque in their\ndecision-making processes, limiting trust and accountability in high-stakes\napplications. We present gSMILE (generative SMILE), a model-agnostic,\nperturbation-based framework for token-level interpretability in LLMs.\nExtending the SMILE methodology, gSMILE uses controlled prompt perturbations,\nWasserstein distance metrics, and weighted linear surrogates to identify input\ntokens with the most significant impact on the output. This process enables the\ngeneration of intuitive heatmaps that visually highlight influential tokens and\nreasoning paths. We evaluate gSMILE across leading LLMs (OpenAI's\ngpt-3.5-turbo-instruct, Meta's LLaMA 3.1 Instruct Turbo, and Anthropic's Claude\n2.1) using attribution fidelity, attribution consistency, attribution\nstability, attribution faithfulness, and attribution accuracy as metrics.\nResults show that gSMILE delivers reliable human-aligned attributions, with\nClaude 2.1 excelling in attention fidelity and GPT-3.5 achieving the highest\noutput consistency. These findings demonstrate gSMILE's ability to balance\nmodel performance and interpretability, enabling more transparent and\ntrustworthy AI systems."}
{"id": "2505.23548", "pdf": "https://arxiv.org/pdf/2505.23548.pdf", "abs": "https://arxiv.org/abs/2505.23548", "title": "Translation in the Wild", "authors": ["Yuri Balashov"], "categories": ["cs.CL"], "comment": "4 figures", "summary": "Large Language Models (LLMs) excel in translation among other things,\ndemonstrating competitive performance for many language pairs in zero- and\nfew-shot settings. But unlike dedicated neural machine translation models, LLMs\nare not trained on any translation-related objective. What explains their\nremarkable translation abilities? Are these abilities grounded in \"incidental\nbilingualism\" (Briakou et al. 2023) in training data? Does instruction tuning\ncontribute to it? Are LLMs capable of aligning and leveraging semantically\nidentical or similar monolingual contents from different corners of the\ninternet that are unlikely to fit in a single context window? I offer some\nreflections on this topic, informed by recent studies and growing user\nexperience. My working hypothesis is that LLMs' translation abilities originate\nin two different types of pre-training data that may be internalized by the\nmodels in different ways. I discuss the prospects for testing the \"duality\"\nhypothesis empirically and its implications for reconceptualizing translation,\nhuman and machine, in the age of deep learning."}
{"id": "2507.03865", "pdf": "https://arxiv.org/pdf/2507.03865.pdf", "abs": "https://arxiv.org/abs/2507.03865", "title": "OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference", "authors": ["Seungjun Shin", "Jaehoon Oh", "Dokwan Oh"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025 (final version)", "summary": "Attention mechanisms are central to the success of large language models\n(LLMs), enabling them to capture intricate token dependencies and implicitly\nassign importance to each token. Recent studies have revealed the sink token,\nwhich receives disproportionately high attention despite their limited semantic\nrole. In this paper, we first expand the relationship between the sink token\nand other tokens, moving beyond attention to explore their similarity in hidden\nstates, considering the layer depth. We observe that as the layers get deeper,\nthe cosine similarity between the normalized hidden states of the sink token\nand those of other tokens increases, and that the normalized hidden states of\nthe sink token exhibit negligible changes. These imply that other tokens\nconsistently are directed toward the sink token throughout the layers. Next, we\npropose a dynamic token selection method, called OrthoRank, using these\nfindings to select important tokens. Specifically, in a certain layer, we\ndefine token importance by the speed at which the token moves toward the sink\ntoken. This is converted into orthogonality with the sink token, meaning that\ntokens that are more orthogonal to the sink token are assigned greater\nimportance. Finally, through extensive experiments, we demonstrated that our\nmethod results in lower perplexity and higher zero-shot accuracy compared to\nlayer pruning methods at the same sparsity ratio with comparable throughput,\nwhile also achieving superior performance on LongBench."}
{"id": "2507.05346", "pdf": "https://arxiv.org/pdf/2507.05346.pdf", "abs": "https://arxiv.org/abs/2507.05346", "title": "LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks", "authors": ["William Fleshman", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The proliferation of fine-tuned language model experts for specific tasks and\ndomains signals the need for efficient selection and combination methods. We\npropose LoRA-Augmented Generation (LAG) for leveraging large libraries of\nknowledge and task-specific LoRA adapters. LAG requires no additional training\nor access to data, and efficiently filters, retrieves, and applies experts on a\nper-token and layer basis. We evaluate LAG on various knowledge-intensive\ntasks, achieving superior performance over existing data-free methods. We\nexplore scenarios where additional data is available, demonstrating LAG's\ncompatibility with alternative solutions such as retrieval-augmented generation\n(RAG)."}
{"id": "2507.14913", "pdf": "https://arxiv.org/pdf/2507.14913.pdf", "abs": "https://arxiv.org/abs/2507.14913", "title": "PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation", "authors": ["Eliya Habba", "Noam Dahan", "Gili Lior", "Gabriel Stanovsky"], "categories": ["cs.CL"], "comment": "Eliya Habba and Noam Dahan contributed equally to this work", "summary": "Evaluating LLMs with a single prompt has proven unreliable, with small\nchanges leading to significant performance differences. However, generating the\nprompt variations needed for a more robust multi-prompt evaluation is\nchallenging, limiting its adoption in practice. To address this, we introduce\nPromptSuite, a framework that enables the automatic generation of various\nprompts. PromptSuite is flexible - working out of the box on a wide range of\ntasks and benchmarks. It follows a modular prompt design, allowing controlled\nperturbations to each component, and is extensible, supporting the addition of\nnew components and perturbation types. Through a series of case studies, we\nshow that PromptSuite provides meaningful variations to support strong\nevaluation practices. It is available through both a Python API:\nhttps://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:\nhttps://promptsuite.streamlit.app/"}
{"id": "2508.00819", "pdf": "https://arxiv.org/pdf/2508.00819.pdf", "abs": "https://arxiv.org/abs/2508.00819", "title": "Beyond Fixed: Training-Free Variable-Length Denoising for Diffusion Large Language Models", "authors": ["Jinsong Li", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Jiaqi Wang", "Dahua Lin"], "categories": ["cs.CL"], "comment": "Code is available at https://github.com/Li-Jinsong/DAEDAL", "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation."}
{"id": "2508.04349", "pdf": "https://arxiv.org/pdf/2508.04349.pdf", "abs": "https://arxiv.org/abs/2508.04349", "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy", "authors": ["Hongze Tan", "Jianfei Pan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models."}
{"id": "2508.05028", "pdf": "https://arxiv.org/pdf/2508.05028.pdf", "abs": "https://arxiv.org/abs/2508.05028", "title": "Evaluation of Finetuned LLMs in AMR Parsing", "authors": ["Shu Han Ho"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages, 32 figures", "summary": "AMR (Abstract Meaning Representation) is a semantic formalism that encodes\nsentence meaning as rooted, directed, acyclic graphs, where nodes represent\nconcepts and edges denote semantic relations. Finetuning decoder only Large\nLanguage Models (LLMs) represent a promising novel straightfoward direction for\nAMR parsing. This paper presents a comprehensive evaluation of finetuning four\ndistinct LLM architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA\nDistilled using the LDC2020T02 Gold AMR3.0 test set. Our results have shown\nthat straightfoward finetuning of decoder only LLMs can achieve comparable\nperformance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2\ndemonstrates competitive performance against SOTA AMR parsers given a\nstraightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full\nLDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching\nGraphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a\nconsistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5\nexcels in structural validity."}
{"id": "2508.06309", "pdf": "https://arxiv.org/pdf/2508.06309.pdf", "abs": "https://arxiv.org/abs/2508.06309", "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC", "authors": ["Ruichong Zhang"], "categories": ["cs.CL", "math.PR"], "comment": "The code is available at the same directory as the TeX source. Run\n  `main_mdir.py` for details", "summary": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible."}
{"id": "2508.07534", "pdf": "https://arxiv.org/pdf/2508.07534.pdf", "abs": "https://arxiv.org/abs/2508.07534", "title": "From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Daixuan Cheng", "Fei Bai", "Beichen Zhang", "Yinqian Min", "Yanzipeng Gao", "Wayne Xin Zhao", "Ji-Rong Wen"], "categories": ["cs.CL"], "comment": "27pages,25figures. arXiv admin note: text overlap with\n  arXiv:2508.02260", "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based\nfeedback to guide LLMs in generating and refining complex reasoning chains -- a\nprocess critically dependent on effective exploration strategies. While prior\nwork has demonstrated RLVR's empirical success, the fundamental mechanisms\ngoverning LLMs' exploration behaviors remain underexplored. This technical\nreport presents a systematic investigation of exploration capacities in RLVR,\ncovering four main aspects: (1) exploration space shaping, where we develop\nquantitative metrics to characterize LLMs' capability boundaries; (2)\nentropy-performance exchange, analyzed across training stages, individual\ninstances, and token-level patterns; and (3) RL performance optimization,\nexamining methods to effectively translate exploration gains into measurable\nimprovements. By unifying previously identified insights with new empirical\nevidence, this work aims to provide a foundational framework for advancing RLVR\nsystems."}
{"id": "2508.08276", "pdf": "https://arxiv.org/pdf/2508.08276.pdf", "abs": "https://arxiv.org/abs/2508.08276", "title": "Evaluating Contrast Localizer for Identifying Causal Units in Social & Mathematical Tasks in Language Models", "authors": ["Yassine Jamaa", "Badr AlKhamissi", "Satrajit Ghosh", "Martin Schrimpf"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at the Interplay of Model Behavior and Model Internals\n  Workshop co-located with COLM 2025", "summary": "This work adapts a neuroscientific contrast localizer to pinpoint causally\nrelevant units for Theory of Mind (ToM) and mathematical reasoning tasks in\nlarge language models (LLMs) and vision-language models (VLMs). Across 11 LLMs\nand 5 VLMs ranging in size from 3B to 90B parameters, we localize top-activated\nunits using contrastive stimulus sets and assess their causal role via targeted\nablations. We compare the effect of lesioning functionally selected units\nagainst low-activation and randomly selected units on downstream accuracy\nacross established ToM and mathematical benchmarks. Contrary to expectations,\nlow-activation units sometimes produced larger performance drops than the\nhighly activated ones, and units derived from the mathematical localizer often\nimpaired ToM performance more than those from the ToM localizer. These findings\ncall into question the causal relevance of contrast-based localizers and\nhighlight the need for broader stimulus sets and more accurately capture\ntask-specific units."}
{"id": "2508.09809", "pdf": "https://arxiv.org/pdf/2508.09809.pdf", "abs": "https://arxiv.org/abs/2508.09809", "title": "A Comprehensive Review of Datasets for Clinical Mental Health AI Systems", "authors": ["Aishik Mandal", "Prottay Kumar Adhikary", "Hiba Arnaout", "Iryna Gurevych", "Tanmoy Chakraborty"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 3 figures", "summary": "Mental health disorders are rising worldwide. However, the availability of\ntrained clinicians has not scaled proportionally, leaving many people without\nadequate or timely support. To bridge this gap, recent studies have shown the\npromise of Artificial Intelligence (AI) to assist mental health diagnosis,\nmonitoring, and intervention. However, the development of efficient, reliable,\nand ethical AI to assist clinicians is heavily dependent on high-quality\nclinical training datasets. Despite growing interest in data curation for\ntraining clinical AI assistants, existing datasets largely remain scattered,\nunder-documented, and often inaccessible, hindering the reproducibility,\ncomparability, and generalizability of AI models developed for clinical mental\nhealth care. In this paper, we present the first comprehensive survey of\nclinical mental health datasets relevant to the training and development of\nAI-powered clinical assistants. We categorize these datasets by mental\ndisorders (e.g., depression, schizophrenia), data modalities (e.g., text,\nspeech, physiological signals), task types (e.g., diagnosis prediction, symptom\nseverity estimation, intervention generation), accessibility (public,\nrestricted or private), and sociocultural context (e.g., language and cultural\nbackground). Along with these, we also investigate synthetic clinical mental\nhealth datasets. Our survey identifies critical gaps such as a lack of\nlongitudinal data, limited cultural and linguistic representation, inconsistent\ncollection and annotation standards, and a lack of modalities in synthetic\ndata. We conclude by outlining key challenges in curating and standardizing\nfuture datasets and provide actionable recommendations to facilitate the\ndevelopment of more robust, generalizable, and equitable mental health AI\nsystems."}
{"id": "2508.09958", "pdf": "https://arxiv.org/pdf/2508.09958.pdf", "abs": "https://arxiv.org/abs/2508.09958", "title": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks", "authors": ["Baran Atalar", "Eddie Zhang", "Carlee Joe-Wong"], "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to AAAI 2026", "summary": "With the increasing popularity of large language models (LLMs) for a variety\nof tasks, there has been a growing interest in strategies that can predict\nwhich out of a set of LLMs will yield a successful answer at low cost. This\nproblem promises to become more and more relevant as providers like Microsoft\nallow users to easily create custom LLM \"assistants\" specialized to particular\ntypes of queries. However, some tasks (i.e., queries) may be too specialized\nand difficult for a single LLM to handle alone. These applications often\nbenefit from breaking down the task into smaller subtasks, each of which can\nthen be executed by a LLM expected to perform well on that specific subtask.\nFor example, in extracting a diagnosis from medical records, one can first\nselect an LLM to summarize the record, select another to validate the summary,\nand then select another, possibly different, LLM to extract the diagnosis from\nthe summarized record. Unlike existing LLM selection or routing algorithms,\nthis setting requires that we select a sequence of LLMs, with the output of\neach LLM feeding into the next and potentially influencing its success. Thus,\nunlike single LLM selection, the quality of each subtask's output directly\naffects the inputs, and hence the cost and success rate, of downstream LLMs,\ncreating complex performance dependencies that must be learned and accounted\nfor during selection. We propose a neural contextual bandit-based algorithm\nthat trains neural networks that model LLM success on each subtask in an online\nmanner, thus learning to guide the LLM selections for the different subtasks,\neven in the absence of historical LLM performance data. Experiments on\ntelecommunications question answering and medical diagnosis prediction datasets\nillustrate the effectiveness of our proposed approach compared to other LLM\nselection algorithms."}
{"id": "2508.10027", "pdf": "https://arxiv.org/pdf/2508.10027.pdf", "abs": "https://arxiv.org/abs/2508.10027", "title": "LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data", "authors": ["Ali Zolnour", "Hossein Azadmaleki", "Yasaman Haghbin", "Fatemeh Taherinezhad", "Mohamad Javad Momeni Nezhad", "Sina Rashidi", "Masoud Khani", "AmirSajjad Taleban", "Samin Mahdizadeh Sani", "Maryam Dadkhah", "James M. Noble", "Suzanne Bakken", "Yadollah Yaghoobzadeh", "Abdol-Hossein Vahabie", "Masoud Rouhizadeh", "Maryam Zolnoori"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Alzheimer's disease and related dementias (ADRD) affect approximately five\nmillion older adults in the U.S., yet over half remain undiagnosed.\nSpeech-based natural language processing (NLP) offers a promising, scalable\napproach to detect early cognitive decline through linguistic markers.\n  To develop and evaluate a screening pipeline that (i) fuses transformer\nembeddings with handcrafted linguistic features, (ii) tests data augmentation\nusing synthetic speech generated by large language models (LLMs), and (iii)\nbenchmarks unimodal and multimodal LLM classifiers for ADRD detection.\n  Transcripts from the DementiaBank \"cookie-theft\" task (n = 237) were used.\nTen transformer models were evaluated under three fine-tuning strategies. A\nfusion model combined embeddings from the top-performing transformer with 110\nlexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,\nMinistral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic\nspeech, which was used to augment training data. Three multimodal models\n(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in\nzero-shot and fine-tuned settings.\n  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or\ntransformer-only baselines. Augmenting training data with 2x MedAlpaca-7B\nsynthetic speech increased F1 to 85.7. Fine-tuning significantly improved\nunimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current\nmultimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =\n66.0). Performance gains aligned with the distributional similarity between\nsynthetic and real speech.\n  Integrating transformer embeddings with linguistic features enhances ADRD\ndetection from speech. Clinically tuned LLMs effectively support both\nclassification and data augmentation, while further advancement is needed in\nmultimodal modeling."}
{"id": "2508.10180", "pdf": "https://arxiv.org/pdf/2508.10180.pdf", "abs": "https://arxiv.org/abs/2508.10180", "title": "Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs", "authors": ["Wenlong Deng", "Jiaming Zhang", "Qi Zeng", "Christos Thrampoulidis", "Boying Gong", "Xiaoxiao Li"], "categories": ["cs.CL"], "comment": null, "summary": "Quantifying the influence of individual training samples is essential for\nenhancing the transparency and accountability of large language models (LLMs)\nand vision-language models (VLMs). However, existing data valuation methods\noften rely on Hessian information or model retraining, making them\ncomputationally prohibitive for billion-parameter models. In this work, we\nintroduce For-Value, a forward-only data valuation framework that enables\nscalable and efficient influence estimation for both LLMs and VLMs. By\nleveraging the rich representations of modern foundation models, For-Value\ncomputes influence scores using a simple closed-form expression based solely on\na single forward pass, thereby eliminating the need for costly gradient\ncomputations. Our theoretical analysis demonstrates that For-Value accurately\nestimates per-sample influence by capturing alignment in hidden representations\nand prediction errors between training and validation samples. Extensive\nexperiments show that For-Value matches or outperforms gradient-based baselines\nin identifying impactful fine-tuning examples and effectively detecting\nmislabeled data."}
{"id": "2508.10795", "pdf": "https://arxiv.org/pdf/2508.10795.pdf", "abs": "https://arxiv.org/abs/2508.10795", "title": "Beyond \"Not Novel Enough\": Enriching Scholarly Critique with LLM-Assisted Feedback", "authors": ["Osama Mohammed Afzal", "Preslav Nakov", "Tom Hope", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": null, "summary": "Novelty assessment is a central yet understudied aspect of peer review,\nparticularly in high volume fields like NLP where reviewer capacity is\nincreasingly strained. We present a structured approach for automated novelty\nevaluation that models expert reviewer behavior through three stages: content\nextraction from submissions, retrieval and synthesis of related work, and\nstructured comparison for evidence based assessment. Our method is informed by\na large scale analysis of human written novelty reviews and captures key\npatterns such as independent claim verification and contextual reasoning.\nEvaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty\nassessments, the approach achieves 86.5% alignment with human reasoning and\n75.3% agreement on novelty conclusions - substantially outperforming existing\nLLM based baselines. The method produces detailed, literature aware analyses\nand improves consistency over ad hoc reviewer judgments. These results\nhighlight the potential for structured LLM assisted approaches to support more\nrigorous and transparent peer review without displacing human expertise. Data\nand code are made available."}
{"id": "2508.10995", "pdf": "https://arxiv.org/pdf/2508.10995.pdf", "abs": "https://arxiv.org/abs/2508.10995", "title": "Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling", "authors": ["Tejomay Kishor Padole", "Suyash P Awate", "Pushpak Bhattacharyya"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted as a main conference submission in the European Conference\n  on Artificial Intelligence (ECAI 2025)", "summary": "Masked diffusion language models (MDMs) have recently gained traction as a\nviable generative framework for natural language. This can be attributed to its\nscalability and ease of training compared to other diffusion model paradigms\nfor discrete data, establishing itself as the state-of-the-art\nnon-autoregressive generator for discrete data. Diffusion models, in general,\nhave shown excellent ability to improve the generation quality by leveraging\ninference-time scaling either by increasing the number of denoising steps or by\nusing external verifiers on top of the outputs of each step to guide the\ngeneration. In this work, we propose a verifier-based inference-time scaling\nmethod that aids in finding a better candidate generation during the denoising\nprocess of the MDM. Our experiments demonstrate the application of MDMs for\nstandard text-style transfer tasks and establish MDMs as a better alternative\nto autoregressive language models. Additionally, we show that a simple\nsoft-value-based verifier setup for MDMs using off-the-shelf pre-trained\nembedding models leads to significant gains in generation quality even when\nused on top of typical classifier-free guidance setups in the existing\nliterature."}
{"id": "2508.11343", "pdf": "https://arxiv.org/pdf/2508.11343.pdf", "abs": "https://arxiv.org/abs/2508.11343", "title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis", "authors": ["Haitong Luo", "Weiyao Zhang", "Suhang Wang", "Wenji Zou", "Chungang Lin", "Xuying Meng", "Yujun Zhang"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge."}
{"id": "1909.08191", "pdf": "https://arxiv.org/pdf/1909.08191.pdf", "abs": "https://arxiv.org/abs/1909.08191", "title": "Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space", "authors": ["Hung Nghiep Tran", "Atsuhiro Takasu"], "categories": ["cs.AI", "cs.CL", "cs.DL"], "comment": "TPDL 2019; remove details from the appendix for official dataset\n  publication later", "summary": "The trends of open science have enabled several open scholarly datasets which\ninclude millions of papers and authors. Managing, exploring, and utilizing such\nlarge and complicated datasets effectively are challenging. In recent years,\nthe knowledge graph has emerged as a universal data format for representing\nknowledge about heterogeneous entities and their relationships. The knowledge\ngraph can be modeled by knowledge graph embedding methods, which represent\nentities and relations as embedding vectors in semantic space, then model the\ninteractions between these embedding vectors. However, the semantic structures\nin the knowledge graph embedding space are not well-studied, thus knowledge\ngraph embedding methods are usually only used for knowledge graph completion\nbut not data representation and analysis. In this paper, we propose to analyze\nthese semantic structures based on the well-studied word embedding space and\nuse them to support data exploration. We also define the semantic queries,\nwhich are algebraic operations between the embedding vectors in the knowledge\ngraph embedding space, to solve queries such as similarity and analogy between\nthe entities on the original datasets. We then design a general framework for\ndata exploration by semantic queries and discuss the solution to some\ntraditional scholarly data exploration tasks. We also propose some new\ninteresting tasks that can be solved based on the uncanny semantic structures\nof the embedding space."}
{"id": "2403.19103", "pdf": "https://arxiv.org/pdf/2403.19103.pdf", "abs": "https://arxiv.org/abs/2403.19103", "title": "Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation", "authors": ["Yutong He", "Alexander Robey", "Naoki Murata", "Yiding Jiang", "Joshua Nathaniel Williams", "George J. Pappas", "Hamed Hassani", "Yuki Mitsufuji", "Ruslan Salakhutdinov", "J. Zico Kolter"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Prompt engineering is an effective but labor-intensive way to control\ntext-to-image (T2I) generative models. Its time-intensive nature and complexity\nhave spurred the development of algorithms for automated prompt generation.\nHowever, these methods often struggle with transferability across T2I models,\nrequire white-box access to the underlying model, or produce non-intuitive\nprompts. In this work, we introduce PRISM, an algorithm that automatically\nproduces human-interpretable and transferable prompts that can effectively\ngenerate desired concepts given only black-box access to T2I models. Inspired\nby large language model (LLM) jailbreaking, PRISM leverages the in-context\nlearning ability of LLMs to iteratively refine the candidate prompt\ndistribution built upon the reference images. Our experiments demonstrate the\nversatility and effectiveness of PRISM in generating accurate prompts for\nobjects, styles, and images across multiple T2I models, including Stable\nDiffusion, DALL-E, and Midjourney."}
{"id": "2406.05881", "pdf": "https://arxiv.org/pdf/2406.05881.pdf", "abs": "https://arxiv.org/abs/2406.05881", "title": "LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning", "authors": ["Utsav Singh", "Pramit Bhattacharyya", "Vinay P. Namboodiri"], "categories": ["cs.LG", "cs.CL", "cs.RO"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable abilities in logical\nreasoning, in-context learning, and code generation. However, translating\nnatural language instructions into effective robotic control policies remains a\nsignificant challenge, especially for tasks requiring long-horizon planning and\noperating under sparse reward conditions. Hierarchical Reinforcement Learning\n(HRL) provides a natural framework to address this challenge in robotics;\nhowever, it typically suffers from non-stationarity caused by the changing\nbehavior of the lower-level policy during training, destabilizing higher-level\npolicy learning. We introduce LGR2, a novel HRL framework that leverages LLMs\nto generate language-guided reward functions for the higher-level policy. By\ndecoupling high-level reward generation from low-level policy changes, LGR2\nfundamentally mitigates the non-stationarity problem in off-policy HRL,\nenabling stable and efficient learning. To further enhance sample efficiency in\nsparse environments, we integrate goal-conditioned hindsight experience\nrelabeling. Extensive experiments across simulated and real-world robotic\nnavigation and manipulation tasks demonstrate LGR2 outperforms both\nhierarchical and non-hierarchical baselines, achieving over 55% success rates\non challenging tasks and robust transfer to real robots, without additional\nfine-tuning."}
{"id": "2406.08391", "pdf": "https://arxiv.org/pdf/2406.08391.pdf", "abs": "https://arxiv.org/abs/2406.08391", "title": "Large Language Models Must Be Taught to Know What They Don't Know", "authors": ["Sanyam Kapoor", "Nate Gruver", "Manley Roberts", "Katherine Collins", "Arka Pal", "Umang Bhatt", "Adrian Weller", "Samuel Dooley", "Micah Goldblum", "Andrew Gordon Wilson"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "NeurIPS 2024 Camera Ready", "summary": "When using large language models (LLMs) in high-stakes applications, we need\nto know when we can trust their predictions. Some works argue that prompting\nhigh-performance LLMs is sufficient to produce calibrated uncertainties, while\nothers introduce sampling methods that can be prohibitively expensive. In this\nwork, we first argue that prompting on its own is insufficient to achieve good\ncalibration and then show that fine-tuning on a small dataset of correct and\nincorrect answers can create an uncertainty estimate with good generalization\nand small computational overhead. We show that a thousand graded examples are\nsufficient to outperform baseline methods and that training through the\nfeatures of a model is necessary for good performance and tractable for large\nopen-source models when using LoRA. We also investigate the mechanisms that\nenable reliable LLM uncertainty estimation, finding that many models can be\nused as general-purpose uncertainty estimators, applicable not just to their\nown uncertainties but also the uncertainty of other models. Lastly, we show\nthat uncertainty estimates inform human use of LLMs in human-AI collaborative\nsettings through a user study."}
{"id": "2408.13442", "pdf": "https://arxiv.org/pdf/2408.13442.pdf", "abs": "https://arxiv.org/abs/2408.13442", "title": "A Law of Next-Token Prediction in Large Language Models", "authors": ["Hangfeng He", "Weijie J. Su"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "Accepted at Physical Review Research", "summary": "Large language models (LLMs) have been widely employed across various\napplication domains, yet their black-box nature poses significant challenges to\nunderstanding how these models process input data internally to make\npredictions. In this paper, we introduce a precise and quantitative law that\ngoverns the learning of contextualized token embeddings through intermediate\nlayers in pre-trained LLMs for next-token prediction. Our findings reveal that\neach layer contributes equally to enhancing prediction accuracy, from the\nlowest to the highest layer -- a universal phenomenon observed across a diverse\narray of open-source LLMs, irrespective of their architectures or pre-training\ndata. We demonstrate that this law offers new perspectives and actionable\ninsights to inform and guide practices in LLM development and applications,\nincluding model scaling, pre-training tasks, and interpretation."}
{"id": "2504.01911", "pdf": "https://arxiv.org/pdf/2504.01911.pdf", "abs": "https://arxiv.org/abs/2504.01911", "title": "Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning", "authors": ["Yinggan Xu", "Hana Kimlee", "Yijia Xiao", "Di Luo"], "categories": ["cs.AI", "cs.CL", "cs.HC", "physics.comp-ph"], "comment": "ICML 2025 Workshop on MAS", "summary": "Large Language Models (LLMs) are playing an increasingly important role in\nphysics research by assisting with symbolic manipulation, numerical\ncomputation, and scientific reasoning. However, ensuring the reliability,\ntransparency, and interpretability of their outputs remains a major challenge.\nIn this work, we introduce a novel multi-agent LLM physicist framework that\nfosters collaboration between AI and human scientists through three key\nmodules: a reasoning module, an interpretation module, and an AI-scientist\ninteraction module. Recognizing that effective physics reasoning demands\nlogical rigor, quantitative accuracy, and alignment with established\ntheoretical models, we propose an interpretation module that employs a team of\nspecialized LLM agents-including summarizers, model builders, visualization\ntools, and testers-to systematically structure LLM outputs into transparent,\nphysically grounded science models. A case study demonstrates that our approach\nsignificantly improves interpretability, enables systematic validation, and\nenhances human-AI collaboration in physics problem-solving and discovery. Our\nwork bridges free-form LLM reasoning with interpretable, executable models for\nscientific analysis, enabling more transparent and verifiable AI-augmented\nresearch."}
{"id": "2504.15466", "pdf": "https://arxiv.org/pdf/2504.15466.pdf", "abs": "https://arxiv.org/abs/2504.15466", "title": "Learning Adaptive Parallel Reasoning with Language Models", "authors": ["Jiayi Pan", "Xiuyu Li", "Long Lian", "Charlie Snell", "Yifei Zhou", "Adam Yala", "Trevor Darrell", "Kurt Keutzer", "Alane Suhr"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at COLM 2025. Code, model, and data are available at\n  https://github.com/Parallel-Reasoning/APR. The first three authors\n  contributed equally to this work", "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation."}
{"id": "2505.13757", "pdf": "https://arxiv.org/pdf/2505.13757.pdf", "abs": "https://arxiv.org/abs/2505.13757", "title": "CoRank: LLM-Based Compact Reranking with Document Features for Scientific Retrieval", "authors": ["Runchu Tian", "Xueqiang Xu", "Bowen Jin", "SeongKu Kang", "Jiawei Han"], "categories": ["cs.IR", "cs.CL"], "comment": "12 pages, 5 figures", "summary": "Scientific retrieval is essential for advancing scientific knowledge\ndiscovery. Within this process, document reranking plays a critical role in\nrefining first-stage retrieval results. However, standard LLM listwise\nreranking faces challenges in the scientific domain. First-stage retrieval is\noften suboptimal in the scientific domain, so relevant documents are ranked\nlower. Meanwhile, conventional listwise reranking places the full text of\ncandidates into the context window, limiting the number of candidates that can\nbe considered. As a result, many relevant documents are excluded before\nreranking, constraining overall retrieval performance. To address these\nchallenges, we explore semantic-feature-based compact document representations\n(e.g., categories, sections, and keywords) and propose CoRank, a training-free,\nmodel-agnostic reranking framework for scientific retrieval. It presents a\nthree-stage solution: (i) offline extraction of document features, (ii)\ncoarse-grained reranking using these compact representations, and (iii)\nfine-grained reranking on full texts of the top candidates from (ii). This\nintegrated process addresses suboptimal first-stage retrieval: Compact\nrepresentations allow more documents to fit within the context window,\nimproving candidate set coverage, while the final fine-grained ranking ensures\na more accurate ordering. Experiments on 5 academic retrieval datasets show\nthat CoRank significantly improves reranking performance across different LLM\nbackbones (average nDCG@10 from 50.6 to 55.5). Overall, these results\nunderscore the synergistic interaction between information extraction and\ninformation retrieval, demonstrating how structured semantic features can\nenhance reranking in the scientific domain."}
{"id": "2505.20152", "pdf": "https://arxiv.org/pdf/2505.20152.pdf", "abs": "https://arxiv.org/abs/2505.20152", "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models", "authors": ["Kai Sun", "Yushi Bai", "Zhen Yang", "Jiajie Zhang", "Ji Qi", "Lei Hou", "Juanzi Li"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our hard negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further conduct ablation studies to analyze three key factors:\nhard negative types, the efficiency of image-based negatives, and training\nconfigurations. These analyses yield important insights into optimizing hard\nnegative strategies for geometric reasoning tasks."}
{"id": "2505.22146", "pdf": "https://arxiv.org/pdf/2505.22146.pdf", "abs": "https://arxiv.org/abs/2505.22146", "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "authors": ["Guangfu Hao", "Haojie Wen", "Liangxuan Guo", "Yang Chen", "Yanchao Bi", "Shan Yu"], "categories": ["cs.CV", "cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Flexible tool selection reflects a complex cognitive ability that\ndistinguishes humans from other species, yet computational models that capture\nthis ability remain underdeveloped. We developed a framework using\nlow-dimensional attribute representations to bridge visual tool perception and\nlinguistic task understanding. We constructed a comprehensive dataset (ToolNet)\ncontaining 115 common tools labeled with 13 carefully designed attributes\nspanning physical, functional, and psychological properties, paired with\nnatural language scenarios describing tool usage. Visual encoders (ResNet or\nViT) extract attributes from tool images while fine-tuned language models\n(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our\napproach achieves 74% accuracy in tool selection tasks-significantly\noutperforming direct tool matching (20%) and smaller multimodal models\n(21%-58%), while approaching performance of much larger models like GPT-4o\n(73%) with substantially fewer parameters. Human evaluation studies validate\nour framework's alignment with human decision-making patterns, and\ngeneralization experiments demonstrate effective performance on novel tool\ncategories. Ablation studies revealed that manipulation-related attributes\n(graspability, elongation, hand-relatedness) consistently prove most critical\nacross modalities. This work provides a parameter-efficient, interpretable\nsolution that mimics human-like tool cognition, advancing both cognitive\nscience understanding and practical applications in tool selection tasks."}
{"id": "2506.00845", "pdf": "https://arxiv.org/pdf/2506.00845.pdf", "abs": "https://arxiv.org/abs/2506.00845", "title": "Generalizable LLM Learning of Graph Synthetic Data with Post-training Alignment", "authors": ["Yizhuo Zhang", "Heng Wang", "Shangbin Feng", "Zhaoxuan Tan", "Xinyun Liu", "Yulia Tsvetkov"], "categories": ["cs.LG", "cs.CL", "I.2.7"], "comment": "8 pages, 1 figures, 2 tables. Experimental code and results are\n  publicly available at\n  https://anonymous.4open.science/r/Graph_RL-BF08/readme.md", "summary": "Previous research has sought to enhance the graph reasoning capabilities of\nLLMs by supervised fine-tuning on synthetic graph data. While these led to\nspecialized LLMs better at solving graph algorithm problems, we don't need LLMs\nfor shortest path: we need generalization from synthetic graph data to\nreal-world tasks with implicit graph structures. In this work, we propose to\nunlock generalizable learning of graph with post-training alignment with\nsynthetic data. We first design solution-based and process-based rewards for\nsynthetic graph problems: instead of rigid memorizing response patterns in\ndirect fine-tuning, we posit that post-training alignment would help LLMs grasp\nthe essentials underlying graph reasoning and alleviate overfitting on\nsynthetic data. We employ post-training alignment algorithms such as GRPO and\nDPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on synthetic graph\ndata. We then compare them against existing settings on both in-domain\nsynthetic tasks and out-of-domain real-world tasks with implicit graph\nstructures such as multi-hop QA, structured planning, and more. Extensive\nexperiments demonstrate that our post-training alignment recipe leads to\nstatistically significant improvement on 5 datasets, with an average gain of\n12.9% over baseline settings. Further analysis reveals that process-based\nrewards consistently outperform solution-based rewards on synthetic data but\nnot on real-world tasks, and compositionality and explainable intermediate\nsteps remains a critical challenge even after post-training alignment."}
{"id": "2506.02720", "pdf": "https://arxiv.org/pdf/2506.02720.pdf", "abs": "https://arxiv.org/abs/2506.02720", "title": "LocalGPT: Benchmarking and Advancing Large Language Models for Local Life Services in Meituan", "authors": ["Xiaochong Lan", "Jie Feng", "Jiahuan Lei", "Xinlei Shi", "Yong Li"], "categories": ["cs.AI", "cs.CL"], "comment": "KDD 2025", "summary": "Large language models (LLMs) have exhibited remarkable capabilities and\nachieved significant breakthroughs across various domains, leading to their\nwidespread adoption in recent years. Building on this progress, we investigate\ntheir potential in the realm of local life services. In this study, we\nestablish a comprehensive benchmark and systematically evaluate the performance\nof diverse LLMs across a wide range of tasks relevant to local life services.\nTo further enhance their effectiveness, we explore two key approaches: model\nfine-tuning and agent-based workflows. Our findings reveal that even a\nrelatively compact 7B model can attain performance levels comparable to a much\nlarger 72B model, effectively balancing inference cost and model capability.\nThis optimization greatly enhances the feasibility and efficiency of deploying\nLLMs in real-world online services, making them more practical and accessible\nfor local life applications."}
{"id": "2506.17208", "pdf": "https://arxiv.org/pdf/2506.17208.pdf", "abs": "https://arxiv.org/abs/2506.17208", "title": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems", "authors": ["Matias Martinez", "Xavier Franch"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid progress in Automated Program Repair (APR) has been driven by\nadvances in AI, particularly large language models (LLMs) and agent-based\nsystems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair\nsystems using real issues and pull requests mined from 12 popular open-source\nPython repositories. Its public leaderboards -- SWE-Bench Lite and SWE-Bench\nVerified -- have become central platforms for tracking progress and comparing\nsolutions. However, because the submission process does not require detailed\ndocumentation, the architectural design and origin of many solutions remain\nunclear. In this paper, we present the first comprehensive study of all\nsubmissions to the SWE-Bench Lite (79 entries) and Verified (99 entries)\nleaderboards, analyzing 80 unique approaches across dimensions such as\nsubmitter type, product availability, LLM usage, and system architecture. Our\nfindings reveal the dominance of proprietary LLMs (especially Claude 3.5), the\npresence of both agentic and non-agentic designs, and a contributor base\nspanning from individual developers to large tech companies."}
{"id": "2506.18843", "pdf": "https://arxiv.org/pdf/2506.18843.pdf", "abs": "https://arxiv.org/abs/2506.18843", "title": "USAD: Universal Speech and Audio Representation via Distillation", "authors": ["Heng-Jui Chang", "Saurabhchand Bhati", "James Glass", "Alexander H. Liu"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to ASRU 2025", "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks."}
{"id": "2507.00449", "pdf": "https://arxiv.org/pdf/2507.00449.pdf", "abs": "https://arxiv.org/abs/2507.00449", "title": "Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention", "authors": ["Zhihao Zhan", "Jianan Zhao", "Zhaocheng Zhu", "Jian Tang"], "categories": ["cs.LG", "cs.CL", "I.2.7"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models, 18\n  pages, 9 figures", "summary": "Efficient long-context modeling remains a critical challenge for natural\nlanguage processing (NLP), as the time complexity of the predominant\nTransformer architecture scales quadratically with the sequence length. While\nstate-space models (SSMs) offer alternative sub-quadratic solutions, they\nstruggle to capture long-range dependencies effectively. In this work, we focus\non analyzing and improving the long-context modeling capabilities of SSMs. We\nshow that the widely used synthetic task, associative recall, which requires a\nmodel to recall a value associated with a single key without context,\ninsufficiently represents the complexities of real-world long-context modeling.\nTo address this limitation, we extend the associative recall to a novel\nsynthetic task, \\emph{joint recall}, which requires a model to recall the value\nassociated with a key given in a specified context. Theoretically, we prove\nthat SSMs do not have the expressiveness to solve multi-query joint recall in\nsub-quadratic time complexity. To resolve this issue, we propose a solution\nbased on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which\nhas the expressiveness to solve multi-query joint recall with sub-quadratic\ncomputation. To bridge the gap between theoretical analysis and real-world\napplications, we propose locality-sensitive Hashing Attention with sparse Key\nSelection (HAX), which instantiates the theoretical solution and is further\ntailored to natural language domains. Extensive experiments on both synthetic\nand real-world long-context benchmarks show that HAX consistently outperforms\nSSM baselines and SSMs integrated with context-independent sparse attention\n(CISA)."}
{"id": "2507.12341", "pdf": "https://arxiv.org/pdf/2507.12341.pdf", "abs": "https://arxiv.org/abs/2507.12341", "title": "Nonlinear Concept Erasure: a Density Matching Approach", "authors": ["Antoine Saillenfest", "Pirmin Lemberger"], "categories": ["cs.LG", "cs.CL"], "comment": "17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)", "summary": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness."}
{"id": "2507.19196", "pdf": "https://arxiv.org/pdf/2507.19196.pdf", "abs": "https://arxiv.org/abs/2507.19196", "title": "Towards Multimodal Social Conversations with Robots: Using Vision-Language Models", "authors": ["Ruben Janssens", "Tony Belpaeme"], "categories": ["cs.RO", "cs.CL", "cs.HC"], "comment": "Accepted at the workshop \"Human - Foundation Models Interaction: A\n  Focus On Multimodal Information\" (FoMo-HRI) at IEEE RO-MAN 2025 (Camera-ready\n  version)", "summary": "Large language models have given social robots the ability to autonomously\nengage in open-domain conversations. However, they are still missing a\nfundamental social skill: making use of the multiple modalities that carry\nsocial interactions. While previous work has focused on task-oriented\ninteractions that require referencing the environment or specific phenomena in\nsocial interactions such as dialogue breakdowns, we outline the overall needs\nof a multimodal system for social conversations with robots. We then argue that\nvision-language models are able to process this wide range of visual\ninformation in a sufficiently general manner for autonomous social robots. We\ndescribe how to adapt them to this setting, which technical challenges remain,\nand briefly discuss evaluation practices."}
{"id": "2508.00554", "pdf": "https://arxiv.org/pdf/2508.00554.pdf", "abs": "https://arxiv.org/abs/2508.00554", "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism", "authors": ["Li Zhao", "Rui Sun", "Zuoyou Jiang", "Bo Yang", "Yuxiao Bai", "Mengting Chen", "Xinyang Wang", "Jing Li", "Zuo Bai"], "categories": ["q-fin.TR", "cs.CL", "q-fin.CP"], "comment": null, "summary": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multi-agent systems and traditional quantitative investment methods\nacross diverse evaluation metrics. ContestTrade is open-sourced on GitHub at\nhttps://github.com/FinStep-AI/ContestTrade."}
{"id": "2508.05571", "pdf": "https://arxiv.org/pdf/2508.05571.pdf", "abs": "https://arxiv.org/abs/2508.05571", "title": "iFairy: the First 2-bit Complex LLM with All Parameters in $\\{\\pm1, \\pm i\\}$", "authors": ["Feiyu Wang", "Guoan Wang", "Yihao Zhang", "Shengfan Wang", "Weitao Li", "Bokai Huang", "Shimao Chen", "Zihan Jiang", "Rui Xu", "Tong Yang"], "categories": ["cs.LG", "cs.CL"], "comment": "15 pages, 9 figures", "summary": "Quantization-Aware Training (QAT) integrates quantization into the training\nloop, enabling LLMs to learn robust low-bit representations, and is widely\nrecognized as one of the most promising research directions. All current QAT\nresearch focuses on minimizing quantization error on full-precision models,\nwhere the full-precision accuracy acts as an upper bound (accuracy ceiling). No\nexisting method has even attempted to surpass this ceiling. To break this\nceiling, we propose a new paradigm: raising the ceiling (full-precision model),\nand then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$,\nthe first 2-bit quantization framework for complex-valued LLMs. Specifically,\nour method leverages the representational advantages of the complex domain to\nboost full-precision accuracy. We map weights to the fourth roots of unity\n$\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically\noptimal 2-bit representation. Importantly, each quantized weight has either a\nzero real or imaginary part, enabling multiplication-free inference using only\nadditions and element swaps. Experimental results show that Fairy$\\pm i$\noutperforms the ceiling of existing 2-bit quantization approaches in terms of\nboth PPL and downstream tasks, while maintaining strict storage and compute\nefficiency. This work opens a new direction for building highly accurate and\npractical LLMs under extremely low-bit constraints."}
{"id": "2508.05668", "pdf": "https://arxiv.org/pdf/2508.05668.pdf", "abs": "https://arxiv.org/abs/2508.05668", "title": "A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges", "authors": ["Yunjia Xi", "Jianghao Lin", "Yongzhao Xiao", "Zheli Zhou", "Rong Shan", "Te Gao", "Jiachen Zhu", "Weiwen Liu", "Yong Yu", "Weinan Zhang"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has significantly revolutionized\nweb search. The emergence of LLM-based Search Agents marks a pivotal shift\ntowards deeper, dynamic, autonomous information seeking. These agents can\ncomprehend user intentions and environmental context and execute multi-turn\nretrieval with dynamic planning, extending search capabilities far beyond the\nweb. Leading examples like OpenAI's Deep Research highlight their potential for\ndeep information mining and real-world applications. This survey provides the\nfirst systematic analysis of search agents. We comprehensively analyze and\ncategorize existing works from the perspectives of architecture, optimization,\napplication, and evaluation, ultimately identifying critical open challenges\nand outlining promising future research directions in this rapidly evolving\nfield. Our repository is available on\nhttps://github.com/YunjiaXi/Awesome-Search-Agent-Papers."}
{"id": "2508.08508", "pdf": "https://arxiv.org/pdf/2508.08508.pdf", "abs": "https://arxiv.org/abs/2508.08508", "title": "Re:Verse -- Can Your VLM Read a Manga?", "authors": ["Aaditya Baranwal", "Madhav Kataria", "Naitik Agrawal", "Yogesh S Rawat", "Shruti Vyas"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted (oral) at ICCV (AISTORY Workshop) 2025", "summary": "Current Vision Language Models (VLMs) demonstrate a critical gap between\nsurface-level recognition and deep narrative reasoning when processing\nsequential visual storytelling. Through a comprehensive investigation of manga\nnarrative understanding, we reveal that while recent large multimodal models\nexcel at individual panel interpretation, they systematically fail at temporal\ncausality and cross-panel cohesion, core requirements for coherent story\ncomprehension. We introduce a novel evaluation framework that combines\nfine-grained multimodal annotation, cross-modal embedding analysis, and\nretrieval-augmented assessment to systematically characterize these\nlimitations.\n  Our methodology includes (i) a rigorous annotation protocol linking visual\nelements to narrative structure through aligned light novel text, (ii)\ncomprehensive evaluation across multiple reasoning paradigms, including direct\ninference and retrieval-augmented generation, and (iii) cross-modal similarity\nanalysis revealing fundamental misalignments in current VLMs' joint\nrepresentations. Applying this framework to Re:Zero manga across 11 chapters\nwith 308 annotated panels, we conduct the first systematic study of long-form\nnarrative understanding in VLMs through three core evaluation axes: generative\nstorytelling, contextual dialogue grounding, and temporal reasoning. Our\nfindings demonstrate that current models lack genuine story-level intelligence,\nstruggling particularly with non-linear narratives, character consistency, and\ncausal inference across extended sequences. This work establishes both the\nfoundation and practical methodology for evaluating narrative intelligence,\nwhile providing actionable insights into the capability of deep sequential\nunderstanding of Discrete Visual Narratives beyond basic recognition in\nMultimodal Models.\n  Project Page: https://re-verse.vercel.app"}
{"id": "2508.10824", "pdf": "https://arxiv.org/pdf/2508.10824.pdf", "abs": "https://arxiv.org/abs/2508.10824", "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Enhanced Model Architectures", "authors": ["Parsa Omidi", "Xingshuai Huang", "Axel Laborieux", "Bahareh Nikpour", "Tianyu Shi", "Armaghan Eshaghi"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures."}
{"id": "2508.11328", "pdf": "https://arxiv.org/pdf/2508.11328.pdf", "abs": "https://arxiv.org/abs/2508.11328", "title": "Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning", "authors": ["Haitong Luo", "Suhang Wang", "Weiyao Zhang", "Ruiqi Meng", "Xuying Meng", "Yujun Zhang"], "categories": ["cs.LG", "cs.CL"], "comment": "Under Review", "summary": "Graph ``pre-training and prompt-tuning'' aligns downstream tasks with\npre-trained objectives to enable efficient knowledge transfer under limited\nsupervision. However, existing methods rely on homophily-based low-frequency\nknowledge, failing to handle diverse spectral distributions in real-world\ngraphs with varying homophily. Our theoretical analysis reveals a spectral\nspecificity principle: optimal knowledge transfer requires alignment between\npre-trained spectral filters and the intrinsic spectrum of downstream graphs.\nUnder limited supervision, large spectral gaps between pre-training and\ndownstream tasks impede effective adaptation. To bridge this gap, we propose\nthe HS-GPPT model, a novel framework that ensures spectral alignment throughout\nboth pre-training and prompt-tuning. We utilize a hybrid spectral filter\nbackbone and local-global contrastive learning to acquire abundant spectral\nknowledge. Then we design prompt graphs to align the spectral distribution with\npretexts, facilitating spectral knowledge transfer across homophily and\nheterophily. Extensive experiments validate the effectiveness under both\ntransductive and inductive learning settings. Our code is available at\nhttps://anonymous.4open.science/r/HS-GPPT-62D2/."}
