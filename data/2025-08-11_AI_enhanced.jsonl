{"id": "2508.05637", "pdf": "https://arxiv.org/pdf/2508.05637.pdf", "abs": "https://arxiv.org/abs/2508.05637", "title": "Automated Visualization Makeovers with LLMs", "authors": ["Siddharth Gangwar", "David A. Selby", "Sebastian J. Vollmer"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Making a good graphic that accurately and efficiently conveys the desired\nmessage to the audience is both an art and a science, typically not taught in\nthe data science curriculum. Visualisation makeovers are exercises where the\ncommunity exchange feedback to improve charts and data visualizations. Can\nmulti-modal large language models (LLMs) emulate this task? Given a plot in the\nform of an image file, or the code used to generate it, an LLM, primed with a\nlist of visualization best practices, is employed to semi-automatically\ngenerate constructive criticism to produce a better plot. Our system is centred\naround prompt engineering of a pre-trained model, relying on a combination of\nuserspecified guidelines and any latent knowledge of data visualization\npractices that might lie within an LLMs training corpus. Unlike other works,\nthe focus is not on generating valid visualization scripts from raw data or\nprompts, but on educating the user how to improve their existing data\nvisualizations according to an interpretation of best practices. A quantitative\nevaluation is performed to measure the sensitivity of the LLM agent to various\nplotting issues across different chart types. We make the tool available as a\nsimple self-hosted applet with an accessible Web interface.", "AI": {"tldr": "This paper explores using multi-modal large language models (LLMs) to provide constructive criticism on data visualizations to improve them based on best practices.", "motivation": "To address the lack of training in effective data visualization within the data science curriculum and enhance the quality of visual representations of data.", "method": "The study utilizes a pre-trained LLM, guided by user-defined visualization guidelines, to analyze and critique existing plots, aiding users in improving their charts.", "result": "The LLM demonstrated sensitivity to various plotting issues across different chart types, successfully providing constructive feedback for improvement.", "conclusion": "The developed tool serves to educate users on best practices in data visualization and is made available as a user-friendly self-hosted applet.", "key_contributions": ["Utilization of LLMs for constructive visualization critique", "Focus on improving existing visualizations rather than generating new ones", "Development of a self-hosted applet for practical use"], "limitations": "", "keywords": ["data visualization", "large language models", "human-computer interaction", "visualization critique", "best practices"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.05646", "pdf": "https://arxiv.org/pdf/2508.05646.pdf", "abs": "https://arxiv.org/abs/2508.05646", "title": "A Humanoid Social Robot as a Teaching Assistant in the Classroom", "authors": ["Thomas Sievers"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Although innovation and the support of new technologies are much needed to\nease the burden on the education system, social robots in schools to help\nteachers with educational tasks are rare. Child-Robot Interaction (CRI) could\nsupport teachers and add an embodied social component to modern multi-modal and\nmulti-sensory learning environments already in use. The social robot Pepper,\nconnected to the Large Language Model (LLM) ChatGPT, was used in a high school\nclassroom to teach new learning content to groups of students. I tested the\ntechnical possibilities with the robot on site and asked the students about\ntheir acceptance and perceived usefulness of teaching with the help of a social\nrobot. All participants felt that the robot's presentation of the learning\nmaterial was appropriate or at least partially appropriate and that its use\nmade sense.", "AI": {"tldr": "The study explores the use of the social robot Pepper, powered by ChatGPT, in high school education, assessing its impact on learning and student perception.", "motivation": "To investigate the integration of child-robot interaction in educational settings to support teachers and enhance the learning experience.", "method": "The research was conducted in a high school classroom where the social robot Pepper was utilized to present learning material to students. Their acceptance and perceived usefulness of the robot were evaluated through feedback.", "result": "Students generally found the robot's presentation of the learning content to be appropriate and felt that the use of the robot was beneficial.", "conclusion": "The integration of social robots like Pepper in classrooms can potentially enhance teaching methods and student engagement.", "key_contributions": ["Demonstrating the feasibility of using social robots in education", "Evaluating student acceptance of robot-assisted learning", "Highlighting the role of LLMs in educational robotics"], "limitations": "", "keywords": ["Child-Robot Interaction", "Education Technology", "Social Robots", "Machine Learning", "High School"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.05653", "pdf": "https://arxiv.org/pdf/2508.05653.pdf", "abs": "https://arxiv.org/abs/2508.05653", "title": "Modeling Interactive Narrative Systems: A Formal Approach", "authors": ["Jules Clerc", "Domitile Lourdeaux", "Mohamed Sallak", "Johann Barbier", "Marc Ravaine"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Interactive Narrative Systems (INS) have revolutionized digital experiences\nby empowering users to actively shape their stories, diverging from traditional\npassive storytelling. However, the field faces challenges due to fragmented\nresearch efforts and diverse system representations. This paper introduces a\nformal representation framework for INS, inspired by diverse approaches from\nthe state of the art. By providing a consistent vocabulary and modeling\nstructure, the framework facilitates the analysis, the description and\ncomparison of INS properties. Experimental validations on the \"Little Red\nRiding Hood\" scenario highlight the usefulness of the proposed formalism and\nits impact on improving the evaluation of INS. This work aims to foster\ncollaboration and coherence within the INS research community by proposing a\nmethodology for formally representing these systems.", "AI": {"tldr": "This paper introduces a formal representation framework for Interactive Narrative Systems (INS) to address challenges in fragmented research and system representations, facilitating analysis and evaluation.", "motivation": "To overcome challenges in fragmented research within Interactive Narrative Systems and to improve coherence in their representation.", "method": "The paper proposes a formal representation framework inspired by various existing approaches, providing a consistent vocabulary and modeling structure for analyzing and comparing INS.", "result": "Experimental results on the 'Little Red Riding Hood' scenario demonstrate the framework's usefulness in enhancing the evaluation of INS.", "conclusion": "The proposed methodology aims to foster collaboration within the INS community and improve the coherence and analysis of interactive narratives.", "key_contributions": ["Introduction of a formal representation framework for INS", "Provision of a consistent vocabulary and modeling structure", "Empirical validation of the framework on a narrative scenario"], "limitations": "", "keywords": ["Interactive Narrative Systems", "formal representation", "storytelling", "evaluation", "collaboration"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.05913", "pdf": "https://arxiv.org/pdf/2508.05913.pdf", "abs": "https://arxiv.org/abs/2508.05913", "title": "Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction", "authors": ["Stefan Pasch", "Min Chul Cha"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As AI systems become increasingly embedded in organizational workflows and\nconsumer applications, ethical principles such as fairness, transparency, and\nrobustness have been widely endorsed in policy and industry guidelines.\nHowever, there is still scarce empirical evidence on whether these principles\nare recognized, valued, or impactful from the perspective of users. This study\ninvestigates the link between ethical AI and user satisfaction by analyzing\nover 100,000 user reviews of AI products from G2. Using transformer-based\nlanguage models, we measure sentiment across seven ethical dimensions defined\nby the EU Ethics Guidelines for Trustworthy AI. Our findings show that all\nseven dimensions are positively associated with user satisfaction. Yet, this\nrelationship varies systematically across user and product types. Technical\nusers and reviewers of AI development platforms more frequently discuss\nsystem-level concerns (e.g., transparency, data governance), while\nnon-technical users and reviewers of end-user applications emphasize\nhuman-centric dimensions (e.g., human agency, societal well-being). Moreover,\nthe association between ethical AI and user satisfaction is significantly\nstronger for non-technical users and end-user applications across all\ndimensions. Our results highlight the importance of ethical AI design from\nusers' perspectives and underscore the need to account for contextual\ndifferences across user roles and product types.", "AI": {"tldr": "This study examines the connection between ethical AI principles and user satisfaction by analyzing over 100,000 AI product reviews, revealing that ethical considerations affect satisfaction variably across user types.", "motivation": "The study investigates whether ethical principles in AI, such as fairness and transparency, are recognized and valued by users, addressing the lack of empirical evidence in this area.", "method": "The analysis utilizes transformer-based language models to assess sentiment across seven ethical dimensions defined by the EU Ethics Guidelines for Trustworthy AI, based on over 100,000 user reviews from G2.", "result": "Findings indicate all seven ethical dimensions are positively correlated with user satisfaction, with variations depending on user type, emphasizing the stronger impact on non-technical users and end-user applications.", "conclusion": "The results stress the importance of designing ethical AI from the user's perspective and highlight the need to consider different user roles and product types in ethical AI frameworks.", "key_contributions": ["Analysis of over 100,000 user reviews to link ethical AI principles and user satisfaction", "Identification of varying impacts of ethical dimensions based on user and product type", "Highlighting the importance of ethical AI design in user-centric contexts"], "limitations": "", "keywords": ["Ethical AI", "User Satisfaction", "Transformers", "Sentiment Analysis", "User Reviews"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.05722", "pdf": "https://arxiv.org/pdf/2508.05722.pdf", "abs": "https://arxiv.org/abs/2508.05722", "title": "PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare", "authors": ["Rania Al-Sabbagh"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces PEACH, a sentence-aligned parallel English-Arabic\ncorpus of healthcare texts encompassing patient information leaflets and\neducational materials. The corpus contains 51,671 parallel sentences, totaling\napproximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths\nvary between 9.52 and 11.83 words on average. As a manually aligned corpus,\nPEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,\ntranslation studies, and natural language processing. It can be used to derive\nbilingual lexicons, adapt large language models for domain-specific machine\ntranslation, evaluate user perceptions of machine translation in healthcare,\nassess patient information leaflets and educational materials' readability and\nlay-friendliness, and as an educational resource in translation studies. PEACH\nis publicly accessible.", "AI": {"tldr": "PEACH is a sentence-aligned parallel English-Arabic corpus of healthcare texts, providing resources for translation studies and NLP applications.", "motivation": "To create a comprehensive resource for healthcare translation and NLP, particularly for English-Arabic language pairs.", "method": "The corpus was manually aligned and consists of 51,671 sentence pairs derived from patient information leaflets and educational materials.", "result": "PEACH contains approximately 590,517 English and 567,707 Arabic word tokens, providing a gold-standard corpus for various research applications.", "conclusion": "PEACH serves as a valuable tool for researchers in linguistics, translation studies, and natural language processing, especially in the healthcare domain.", "key_contributions": ["Creation of a gold-standard sentence-aligned corpus for healthcare texts", "Support for domain-specific machine translation and bilingual lexicon development", "Public accessibility of the corpus for educational and research purposes"], "limitations": "", "keywords": ["English-Arabic corpus", "healthcare texts", "parallel sentences", "natural language processing", "translation studies"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.05933", "pdf": "https://arxiv.org/pdf/2508.05933.pdf", "abs": "https://arxiv.org/abs/2508.05933", "title": "REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition", "authors": ["Xueyuan Xu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The affective brain-computer interface is a crucial technology for affective\ninteraction and emotional intelligence, emerging as a significant area of\nresearch in the human-computer interaction. Compared to single-type features,\nmulti-type EEG features provide a multi-level representation for analyzing\nmulti-dimensional emotions. However, the high dimensionality of multi-type EEG\nfeatures, combined with the relatively small number of high-quality EEG\nsamples, poses challenges such as classifier overfitting and suboptimal\nreal-time performance in multi-dimensional emotion recognition. Moreover,\npractical applications of affective brain-computer interface frequently\nencounters partial absence of multi-dimensional emotional labels due to the\nopen nature of the acquisition environment, and ambiguity and variability in\nindividual emotion perception. To address these challenges, this study proposes\na novel EEG feature selection method for missing multi-dimensional emotion\nrecognition. The method leverages adaptive orthogonal non-negative matrix\nfactorization to reconstruct the multi-dimensional emotional label space\nthrough second-order and higher-order correlations, which could reduce the\nnegative impact of missing values and outliers on label reconstruction.\nSimultaneously, it employs least squares regression with graph-based manifold\nlearning regularization and global feature redundancy minimization\nregularization to enable EEG feature subset selection despite missing\ninformation, ultimately achieving robust EEG-based multi-dimensional emotion\nrecognition. Simulation experiments on three widely used multi-dimensional\nemotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method\noutperforms thirteen advanced feature selection methods in terms of robustness\nfor EEG emotional feature selection.", "AI": {"tldr": "This study proposes a novel EEG feature selection method to improve multi-dimensional emotion recognition in affective brain-computer interfaces, addressing challenges of high dimensionality and missing emotional labels.", "motivation": "The study addresses challenges in multi-dimensional emotion recognition due to the high dimensionality of EEG features and the sparsity of high-quality samples, which can lead to classifier overfitting and real-time performance issues.", "method": "The proposed method utilizes adaptive orthogonal non-negative matrix factorization for reconstructing emotional label space, and combines least squares regression with graph-based manifold learning regularization for effective feature selection despite missing information.", "result": "Simulation experiments on datasets DREAMER, DEAP, and HDED show that the proposed method outperforms thirteen other advanced feature selection methods in robustness.", "conclusion": "The novel EEG feature selection method significantly enhances multi-dimensional emotion recognition accuracy, reducing the impact of missing values and outliers in EEG data analysis.", "key_contributions": ["Introduces a feature selection method that utilizes adaptive orthogonal non-negative matrix factorization for label reconstruction.", "Employs least squares regression with manifold learning for effective feature selection despite missing data.", "Demonstrates robustness compared to existing feature selection approaches through extensive experimentation."], "limitations": "", "keywords": ["affective brain-computer interface", "EEG feature selection", "multi-dimensional emotion recognition", "manifold learning", "emotional labels"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.05775", "pdf": "https://arxiv.org/pdf/2508.05775.pdf", "abs": "https://arxiv.org/abs/2508.05775", "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation", "authors": ["Chi Zhang", "Changjia Zhu", "Junjie Xiong", "Xiaoran Xu", "Lingyao Li", "Yao Liu", "Zhuo Lu"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized content creation across\ndigital platforms, offering unprecedented capabilities in natural language\ngeneration and understanding. These models enable beneficial applications such\nas content generation, question and answering (Q&A), programming, and code\nreasoning. Meanwhile, they also pose serious risks by inadvertently or\nintentionally producing toxic, offensive, or biased content. This dual role of\nLLMs, both as powerful tools for solving real-world problems and as potential\nsources of harmful language, presents a pressing sociotechnical challenge. In\nthis survey, we systematically review recent studies spanning unintentional\ntoxicity, adversarial jailbreaking attacks, and content moderation techniques.\nWe propose a unified taxonomy of LLM-related harms and defenses, analyze\nemerging multimodal and LLM-assisted jailbreak strategies, and assess\nmitigation efforts, including reinforcement learning with human feedback\n(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the\nevolving landscape of LLM safety, identifies limitations in current evaluation\nmethodologies, and outlines future research directions to guide the development\nof robust and ethically aligned language technologies.", "AI": {"tldr": "This survey reviews the benefits and harms of Large Language Models (LLMs), focusing on toxicity, adversarial attacks, and moderation techniques, while proposing a unified taxonomy and assessing mitigation strategies.", "motivation": "To address the dual role of LLMs as both beneficial tools and potential sources of harmful language.", "method": "Systematic review of recent studies on LLM-related harms, adversarial attacks, and content moderation techniques, including proposing a unified taxonomy of issues.", "result": "Identification of key harms and defense strategies related to LLMs, along with an analysis of current evaluation methodologies and emerging techniques.", "conclusion": "Highlights the need for improved evaluation methods and future research directions for ethically aligned language technologies.", "key_contributions": ["Unified taxonomy of LLM-related harms and defenses", "Analysis of multimodal and LLM-assisted jailbreak strategies", "Assessment of mitigation efforts including RLHF and prompt engineering"], "limitations": "Current evaluation methodologies are still limited, suggesting areas for improvement.", "keywords": ["Large Language Models", "toxicity", "content moderation", "adversarial attacks", "safety alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05934", "pdf": "https://arxiv.org/pdf/2508.05934.pdf", "abs": "https://arxiv.org/abs/2508.05934", "title": "ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection", "authors": ["Xueyuan Xu", "Tianze Yu", "Wenjia Dong", "Fulin Wei", "Li Zhuo"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, multi-modal physiological signals based emotion recognition has\ngarnered increasing attention in the field of brain-computer interfaces.\nNevertheness, the associated multi-modal physiological features are often\nhigh-dimensional and inevitably include irrelevant, redundant, and noisy\nrepresentation, which can easily lead to overfitting, poor performance, and\nhigh computational complexity in emotion classifiers. Feature selection has\nbeen widely applied to address these challenges. However, previous studies\ngenerally assumed that multi-modal physiological data are complete, whereas in\nreality, the data are often incomplete due to the openness of the acquisition\nand operational environment. For example, a part of samples are available in\nseveral modalities but not in others. To address this issue, we propose a novel\nmethod for incomplete multi-modal physiological signal feature selection called\nadaptive shared latent structure learning (ASLSL). Based on the property that\nsimilar features share similar emotional labels, ASLSL employs adaptive shared\nlatent structure learning to explore a common latent space shared for\nincomplete multi-modal physiological signals and multi-dimensional emotional\nlabels, thereby mitigating the impact of missing information and mining\nconsensus information. Two most popular multi-modal physiological emotion\ndatasets (DEAP and DREAMER) with multi-dimensional emotional labels were\nutilized to compare the performance between compare ASLSL and seventeen feature\nselection methods. Comprehensive experimental results on these datasets\ndemonstrate the effectiveness of ASLSL.", "AI": {"tldr": "This paper proposes a novel method called Adaptive Shared Latent Structure Learning (ASLSL) for feature selection in incomplete multi-modal physiological signal emotion recognition.", "motivation": "The paper addresses challenges in emotion recognition due to high-dimensional, incomplete, and noisy multi-modal physiological features that can lead to overfitting and poor performance.", "method": "ASLSL leverages adaptive shared latent structure learning to discover a common latent space that correlates incomplete multi-modal data and multi-dimensional emotional labels.", "result": "ASLSL outperformed seventeen feature selection methods on two popular datasets, DEAP and DREAMER, demonstrating its effectiveness in handling incomplete data.", "conclusion": "The proposed ASLSL method is effective in improving emotion classification performance by mitigating the effects of missing information in physiological signals.", "key_contributions": ["Introduction of ASLSL for feature selection in incomplete multi-modal data", "Demonstration of ASLSL's effectiveness on DEAP and DREAMER datasets", "Comparison with seventeen existing feature selection methods."], "limitations": "", "keywords": ["Emotion Recognition", "Multi-modal Physiological Signals", "Feature Selection", "Brain-Computer Interfaces", "Latent Structure Learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.05782", "pdf": "https://arxiv.org/pdf/2508.05782.pdf", "abs": "https://arxiv.org/abs/2508.05782", "title": "FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification", "authors": ["Xiangyan Chen", "Yufeng Li", "Yujian Gan", "Arkaitz Zubiaga", "Matthew Purver"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are known to produce hallucinations - factually\nincorrect or fabricated information - which poses significant challenges for\nmany Natural Language Processing (NLP) applications, such as dialogue systems.\nAs a result, detecting hallucinations has become a critical area of research.\nCurrent approaches to hallucination detection in dialogue systems primarily\nfocus on verifying the factual consistency of generated responses. However,\nthese responses often contain a mix of accurate, inaccurate or unverifiable\nfacts, making one factual label overly simplistic and coarse-grained. In this\npaper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact\nverification, which involves verifying atomic facts extracted from dialogue\nresponses. To support this, we construct a dataset based on publicly available\ndialogue datasets and evaluate it using various baseline methods. Experimental\nresults demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning\ncan enhance performance in dialogue fact verification. Despite this, the best\nF1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is\nonly 0.75, indicating that the benchmark remains a challenging task for future\nresearch. Our dataset and code will be public on GitHub.", "AI": {"tldr": "This paper presents FineDialFact, a benchmark for fine-grained dialogue fact verification in the context of hallucinations produced by Large Language Models.", "motivation": "Addressing the challenge of hallucinations in NLP applications that rely on dialogue systems by providing a more nuanced approach to fact verification.", "method": "The authors constructed a dataset that focuses on verifying atomic facts extracted from dialogue responses and evaluated it with various baseline methods, highlighting the role of Chain-of-Thought reasoning in improving verification performance.", "result": "Experimental results indicate that incorporating Chain-of-Thought reasoning improves performance; however, the best F1-score achieved is only 0.75, revealing the ongoing challenges in this research area.", "conclusion": "The FineDialFact benchmark and associated dataset highlight the complexity of fine-grained fact verification in dialogues and encourage further research to improve models in this domain.", "key_contributions": ["Introduction of a fine-grained dialogue fact verification benchmark (FineDialFact)", "Construction of a dataset for verifying atomic facts in dialogue responses", "Demonstration of improved performance using Chain-of-Thought reasoning"], "limitations": "The best performance achieved is still only an F1-score of 0.75, indicating there is significant room for improvement in the task of dialogue fact verification.", "keywords": ["Large Language Models", "fact verification", "dialogue systems", "Chain-of-Thought reasoning", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05940", "pdf": "https://arxiv.org/pdf/2508.05940.pdf", "abs": "https://arxiv.org/abs/2508.05940", "title": "It's a Complete Haystack: Understanding Dependency Management Needs in Computer-Aided Design", "authors": ["Kathy Cheng", "Alison Olechowski", "Shurui Zhou"], "categories": ["cs.HC"], "comment": "To be published in the Proceedings of the ACM on Human-Computer\n  Interaction, Volume 9, Issue CSCW2", "summary": "In today's landscape, hardware development teams face increasing demands for\nbetter quality products, greater innovation, and shorter manufacturing lead\ntimes. Despite the need for more efficient and effective processes, hardware\ndesigners continue to struggle with a lack of awareness of design changes and\nother collaborators' actions, a persistent issue in decades of CSCW research.\nOne significant and unaddressed challenge is understanding and managing\ndependencies between 3D CAD (computer-aided design) models, especially when\nproducts can contain thousands of interconnected components. In this two-phase\nformative study, we explore designers' pain points of CAD dependency management\nthrough a thematic analysis of 100 online forum discussions and semi-structured\ninterviews with 10 designers. We identify nine key challenges related to the\ntraceability, navigation, and consistency of CAD dependencies, that harm the\neffective coordination of hardware development teams. To address these\nchallenges, we propose design goals and necessary features to enhance hardware\ndesigners' awareness and management of dependencies, ultimately with the goal\nof improving collaborative workflows.", "AI": {"tldr": "The paper explores challenges in CAD dependency management faced by hardware designers and proposes solutions to enhance collaboration.", "motivation": "Hardware development teams need efficient processes but struggle with managing design changes and dependencies in CAD models.", "method": "The study includes thematic analysis of 100 online forum discussions and semi-structured interviews with 10 designers to identify challenges in CAD dependency management.", "result": "Nine key challenges related to traceability, navigation, and consistency of CAD dependencies that hinder effective collaboration were identified.", "conclusion": "The paper proposes design goals and features aimed at improving awareness and management of CAD dependencies among hardware designers.", "key_contributions": ["Identification of key challenges in CAD dependency management", "Proposed design goals to enhance collaborative workflows", "Feature recommendations for better awareness of design changes."], "limitations": "", "keywords": ["CAD", "dependency management", "collaborative design", "hardware development", "CSCW"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.05803", "pdf": "https://arxiv.org/pdf/2508.05803.pdf", "abs": "https://arxiv.org/abs/2508.05803", "title": "Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models", "authors": ["Abishek Thamma", "Micha Heilbron"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Human memory is fleeting. As words are processed, the exact wordforms that\nmake up incoming sentences are rapidly lost. Cognitive scientists have long\nbelieved that this limitation of memory may, paradoxically, help in learning\nlanguage - an idea supported by classic connectionist modelling work. The rise\nof Transformers appears to challenge this idea, as these models can learn\nlanguage effectively, despite lacking memory limitations or other architectural\nrecency biases. Here, we investigate the hypothesized benefit of fleeting\nmemory for language learning in tightly controlled experiments on transformer\nlanguage models. Training transformers with and without fleeting memory on a\ndevelopmentally realistic training set, we find that fleeting memory\nconsistently improves language learning (as quantified by both overall language\nmodelling performance and targeted syntactic evaluation) but, unexpectedly,\nimpairs surprisal-based prediction of human reading times. Interestingly,\nfollow up analyses revealed that this discrepancy - better language modeling,\nyet worse reading time prediction - could not be accounted for by prior\nexplanations of why better language models sometimes fit human reading time\nworse. Together, these results support a benefit of memory limitations on\nneural network language learning - but not on predicting behavior.", "AI": {"tldr": "Investigation of fleeting memory's role in language learning using transformer models.", "motivation": "Explore the paradox that cognitive scientists suggest fleeting memory aids language learning, while Transformer models, which lack this memory, still perform well in language tasks.", "method": "Conducted experiments comparing transformer models with and without fleeting memory on a realistic training set to assess their language learning capabilities.", "result": "Fleeting memory improved overall language modeling performance and targeted syntactic evaluation, but adversely affected surprisal-based prediction of human reading times.", "conclusion": "Memory limitations enhance neural network language learning effectiveness but don't necessarily align with human reading behavior predictions.", "key_contributions": ["Demonstrated the impact of fleeting memory on transformer models' language learning.", "Provided empirical evidence supporting cognitive science theories about memory and language acquisition.", "Revealed contradictions between language modeling success and reading time prediction in AI."], "limitations": "Findings may not generalize beyond language tasks; exploration was limited to specific model configurations and datasets.", "keywords": ["language learning", "transformer models", "human memory", "surprisal prediction", "cognitive science"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.06000", "pdf": "https://arxiv.org/pdf/2508.06000.pdf", "abs": "https://arxiv.org/abs/2508.06000", "title": "Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning", "authors": ["Wei Xiang", "Ziyue Lei", "Haoyuan Che", "Fangyuan Ye", "Xueting Wu", "Lingyun Sun"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Operational skill learning, inherently physical and reliant on hands-on\npractice and kinesthetic feedback, has yet to be effectively replicated in\nlarge language model (LLM)-supported training. Current LLM training assistants\nprimarily generate customized textual feedback, neglecting the crucial\nkinesthetic modality. This gap derives from the textual and uncertain nature of\nLLMs, compounded by concerns on user acceptance of LLM driven body control. To\nbridge this gap and realize the potential of collaborative human-LLM action,\nthis work explores human experience of LLM driven kinesthetic assistance.\nSpecifically, we introduced an \"Align-Analyze-Adjust\" strategy and developed\nFlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS)\nfor flight skill acquisition, a representative operational skill domain.\nFlightAxis learns flight skills from manuals and guides forearm movements\nduring simulated flight tasks. Our results demonstrate high user acceptance of\nLLM-mediated body control and significantly reduced task completion times.\nCrucially, trainees reported that this kinesthetic assistance enhanced their\nawareness of operation flaws and fostered increased engagement in the training\nprocess, rather than relieving perceived load. This work demonstrated the\npotential of kinesthetic LLM training in operational skill acquisition.", "AI": {"tldr": "The paper presents FlightAxis, a tool that combines LLM and Electrical Muscle Stimulation for enhancing operational skill learning through kinesthetic feedback.", "motivation": "To address the gap in LLM-supported training that neglects kinesthetic feedback, which is essential for operational skill learning.", "method": "The authors developed FlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS) to assist in flight skill acquisition, implementing an 'Align-Analyze-Adjust' strategy.", "result": "The study showed high user acceptance of LLM-driven body control and significantly reduced task completion times, along with increased trainee engagement and awareness of operational flaws during training.", "conclusion": "The findings suggest that kinesthetic assistance from LLMs can enhance the training of operational skills by providing necessary physical feedback.", "key_contributions": ["Introduction of the FlightAxis tool combining LLM with EMS", "Establishment of the 'Align-Analyze-Adjust' strategy", "Demonstration of improved task performance and user engagement in operational skill learning."], "limitations": "", "keywords": ["LLM", "kinesthetic feedback", "operational skill learning", "Electrical Muscle Stimulation", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05830", "pdf": "https://arxiv.org/pdf/2508.05830.pdf", "abs": "https://arxiv.org/abs/2508.05830", "title": "\"Mirror\" Language AI Models of Depression are Criterion-Contaminated", "authors": ["Tong Li", "Rasiq Hussain", "Mehak Gupta", "Joshua R. Oltmanns"], "categories": ["cs.CL", "cs.CY"], "comment": "39 pages, 9 figures", "summary": "A growing number of studies show near-perfect LLM language-based prediction\nof depression assessment scores (up to R2 of .70). However, many develop these\nmodels directly from language responses to depression assessments. These\n\"Mirror models\" suffer from \"criterion contamination\", which arises when a\npredicted score depends in part on the predictors themselves. This causes\nartificial effect size inflation which reduces model generalizability. The\npresent study compares the performance of Mirror models versus \"Non-Mirror\nmodels\", which are developed from language that does not mirror the assessment\nthey are developed to predict. N = 110 research participants completed two\ndifferent interviews: structured diagnostic and life history interviews. GPT-4,\nGPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic\ninterview depression scores from the two transcripts separately. Mirror models\n(using structured diagnostic data) showed very large effect sizes (e.g., R2 =\n.80). As expected, NonMirror models (using life history data) demonstrated\nsmaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror\nand Non-Mirror model-predicted structured interview depression scores were\ncorrelated with self-reported depression symptoms, Mirror and NonMirror\nperformed the same (e.g., r = ~.54), indicating that Mirror models contain bias\nperhaps due to criterion contamination. Topic modeling identified clusters\nacross Mirror and Non-Mirror models, as well as between true-positive and\nfalse-positive predictions. In this head-to-head comparison study, Mirror\nlanguage AI models of depression showed artificially inflated effect sizes and\nless generalizability. As language AI models for depression continue to evolve,\nincorporating Non-Mirror models may identify interpretable, and generalizable\nsemantic features that have unique utility in real-world psychological\nassessment.", "AI": {"tldr": "This study compares the effectiveness of Mirror models against Non-Mirror models in predicting depression scores using language data, highlighting concerns about bias and generalizability in AI predictions.", "motivation": "To address the issue of 'criterion contamination' in LLM-based predictions of depression scores, which affects model reliability and generalizability.", "method": "A total of 110 participants completed structured diagnostic and life history interviews. GPT-4, GPT-4o, and LLaMA3-70B were utilized to predict depression scores from each type of transcript and compared the performance of Mirror versus Non-Mirror models.", "result": "Mirror models showed very large effect sizes (R2 = .80), while Non-Mirror models had smaller effect sizes (R2 = .27), but both scores correlating with self-reported depression symptoms were similar (r = ~.54).", "conclusion": "The study found that Mirror models tend to overinflate effect sizes due to bias from using assessment-mirrored language, suggesting a need for Non-Mirror models to improve generalizability in AI-based psychological evaluations.", "key_contributions": ["Introduction of the concept of Non-Mirror models to enhance generalizability in language-based depression prediction.", "Empirical comparison showing significant bias in Mirror models through inflated effect sizes.", "Identification of semantic clusters that can aid in psychological assessment."], "limitations": "The study is limited to a specific sample size and may not generalize to broader populations or different diagnostic contexts.", "keywords": ["depression prediction", "language models", "criterion contamination", "HCI", "machine learning"], "importance_score": 9, "read_time_minutes": 39}}
{"id": "2508.06056", "pdf": "https://arxiv.org/pdf/2508.06056.pdf", "abs": "https://arxiv.org/abs/2508.06056", "title": "RAGTrace: Understanding and Refining Retrieval-Generation Dynamics in Retrieval-Augmented Generation", "authors": ["Sizhe Cheng", "Jiaping Li", "Huanchen Wang", "Yuxin Ma"], "categories": ["cs.HC"], "comment": "19 pages, 9 figures, Accepted by UIST 2025", "summary": "Retrieval-Augmented Generation (RAG) systems have emerged as a promising\nsolution to enhance large language models (LLMs) by integrating external\nknowledge retrieval with generative capabilities. While significant\nadvancements have been made in improving retrieval accuracy and response\nquality, a critical challenge remains that the internal knowledge integration\nand retrieval-generation interactions in RAG workflows are largely opaque. This\npaper introduces RAGTrace, an interactive evaluation system designed to analyze\nretrieval and generation dynamics in RAG-based workflows. Informed by a\ncomprehensive literature review and expert interviews, the system supports a\nmulti-level analysis approach, ranging from high-level performance evaluation\nto fine-grained examination of retrieval relevance, generation fidelity, and\ncross-component interactions. Unlike conventional evaluation practices that\nfocus on isolated retrieval or generation quality assessments, RAGTrace enables\nan integrated exploration of retrieval-generation relationships, allowing users\nto trace knowledge sources and identify potential failure cases. The system's\nworkflow allows users to build, evaluate, and iterate on retrieval processes\ntailored to their specific domains of interest. The effectiveness of the system\nis demonstrated through case studies and expert evaluations on real-world RAG\napplications.", "AI": {"tldr": "RAGTrace is an interactive evaluation system for analyzing retrieval-generation dynamics in Retrieval-Augmented Generation (RAG) workflows, aimed at improving the understanding and effectiveness of RAG systems in integrating external knowledge with generative capabilities.", "motivation": "Despite advancements in RAG systems, understanding the internal dynamic of retrieval and generation remains opaque, warranting a system that allows for detailed analysis and interaction.", "method": "RAGTrace offers a multi-level analysis approach, combining high-level performance evaluation with detailed examinations of retrieval relevance and generation fidelity, using case studies and expert evaluations for demonstration.", "result": "The system effectively enables users to trace knowledge sources and assess the interactions between retrieval and generation components, resulting in improved understanding and iteration of retrieval processes across various domains.", "conclusion": "RAGTrace provides a comprehensive tool for enhancing the development and evaluation of RAG applications by allowing integrated exploration of their retrieval and generation relationships.", "key_contributions": ["Introduction of RAGTrace for interactive evaluation of RAG systems.", "Enables comprehensive analysis of retrieval-generation dynamics instead of isolated assessments.", "Demonstrated effectiveness through case studies in real-world applications."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "RAGTrace", "Human-Computer Interaction", "Knowledge Retrieval", "Generative Models"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.05843", "pdf": "https://arxiv.org/pdf/2508.05843.pdf", "abs": "https://arxiv.org/abs/2508.05843", "title": "Discovering Properties of Inflectional Morphology in Neural Emergent Communication", "authors": ["Miles Gilberti", "Shane Storks", "Huteng Dai"], "categories": ["cs.CL"], "comment": null, "summary": "Emergent communication (EmCom) with deep neural network-based agents promises\nto yield insights into the nature of human language, but remains focused\nprimarily on a few subfield-specific goals and metrics that prioritize\ncommunication schemes which represent attributes with unique characters\none-to-one and compose them syntactically. We thus reinterpret a common EmCom\nsetting, the attribute-value reconstruction game, by imposing a\nsmall-vocabulary constraint to simulate double articulation, and formulating a\nnovel setting analogous to naturalistic inflectional morphology (enabling\nmeaningful comparison to natural language communication schemes). We develop\nnew metrics and explore variations of this game motivated by real properties of\ninflectional morphology: concatenativity and fusionality. Through our\nexperiments, we discover that simulated phonological constraints encourage\nconcatenative morphology, and emergent languages replicate the tendency of\nnatural languages to fuse grammatical attributes.", "AI": {"tldr": "This paper explores emergent communication in deep neural networks by simulating constraints akin to natural language morphology.", "motivation": "To investigate how emergent communication schemes can reflect properties of natural language, particularly in terms of morphology, beyond traditional metrics.", "method": "The authors reinterpret the attribute-value reconstruction game by imposing a small-vocabulary constraint to simulate double articulation and develop new metrics reflecting properties of natural language inflectional morphology.", "result": "Experiments reveal that phonological constraints lead to concatenative morphology and that emergent languages exhibit patterns similar to natural languages, such as fusing grammatical attributes.", "conclusion": "The study provides insights into how language-like properties can emerge from neural networks in simulated environments that mimic natural language mechanics.", "key_contributions": ["Proposed a new experimental setting simulating double articulation and naturalistic morphology.", "Developed novel metrics for evaluating emergent communication based on morphological properties.", "Demonstrated that simulated constraints can lead to language behaviors analogous to natural languages."], "limitations": "", "keywords": ["emergent communication", "deep neural networks", "natural language", "inflectional morphology", "morphological properties"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.06065", "pdf": "https://arxiv.org/pdf/2508.06065.pdf", "abs": "https://arxiv.org/abs/2508.06065", "title": "ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation", "authors": ["Daniel Lee", "Nikhil Sharma", "Donghoon Shin", "DaEun Choi", "Harsh Sharma", "Jeonghwan Kim", "Heng Ji"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV", "H.5.2; I.2.7"], "comment": null, "summary": "Generative AI has made image creation more accessible, yet aligning outputs\nwith nuanced creative intent remains challenging, particularly for non-experts.\nExisting tools often require users to externalize ideas through prompts or\nreferences, limiting fluid exploration. We introduce ThematicPlane, a system\nthat enables users to navigate and manipulate high-level semantic concepts\n(e.g., mood, style, or narrative tone) within an interactive thematic design\nplane. This interface bridges the gap between tacit creative intent and system\ncontrol. In our exploratory study (N=6), participants engaged in divergent and\nconvergent creative modes, often embracing unexpected results as inspiration or\niteration cues. While they grounded their exploration in familiar themes,\ndiffering expectations of how themes mapped to outputs revealed a need for more\nexplainable controls. Overall, ThematicPlane fosters expressive, iterative\nworkflows and highlights new directions for intuitive, semantics-driven\ninteraction in generative design tools.", "AI": {"tldr": "ThematicPlane is a system that allows users to interactively manipulate high-level semantic concepts for generative AI image creation, bridging the gap between creative intent and system control.", "motivation": "To address the challenges faced by non-experts in aligning generative AI outputs with nuanced creative intent, which often requires externalizing ideas through prompts.", "method": "An exploratory study involving 6 participants who navigated an interactive thematic design plane, engaging in creative modes that emphasized exploration and manipulation of themes.", "result": "Participants found that ThematicPlane allowed for expressive workflows and unexpected results that could serve as inspiration, though there were varying expectations regarding the mapping of themes to image outputs.", "conclusion": "ThematicPlane supports iterative creative processes and indicates a need for more explainable controls in generative design tools to better align user expectations with outputs.", "key_contributions": ["Introduction of ThematicPlane for semantic manipulation in generative AI.", "The system facilitates exploratory and iterative creative workflows.", "Identifies the necessity of explainable controls in generative tools."], "limitations": "The study involved a small sample size (N=6) and may not represent broader user experiences or needs.", "keywords": ["Generative AI", "Interactive design", "Creative intent", "Thematic manipulation", "User experience"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2508.05880", "pdf": "https://arxiv.org/pdf/2508.05880.pdf", "abs": "https://arxiv.org/abs/2508.05880", "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models", "authors": ["Sree Bhattacharyya", "Lucas Craig", "Tharun Dilliraj", "Jia Li", "James Z. Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Affective Computing has been established as a crucial field of inquiry to\nadvance the holistic development of Artificial Intelligence (AI) systems.\nFoundation models -- especially Large Language Models (LLMs) -- have been\nevaluated, trained, or instruction-tuned in several past works, to become\nbetter predictors or generators of emotion. Most of these studies, however,\napproach emotion-related tasks in a supervised manner, assessing or training\nthe capabilities of LLMs using discrete emotion labels associated with stimuli\n(e.g., text, images, video, audio). Evaluation studies, in particular, have\noften been limited to standard and superficial emotion-related tasks, such as\nthe recognition of evoked or expressed emotions. In this paper, we move beyond\nsurface-level emotion tasks to investigate how LLMs reason about emotions\nthrough cognitive dimensions. Drawing from cognitive appraisal theory, we\nexamine whether LLMs produce coherent and plausible cognitive reasoning when\nreasoning about emotionally charged stimuli. We introduce a large-scale\nbenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal\ncognitive structures implicitly used by LLMs for emotional reasoning. Through a\nplethora of evaluation experiments and analysis, we seek to answer: (a) Are\nmodels more likely to implicitly rely on specific cognitive appraisal\ndimensions?, (b) What cognitive dimensions are important for characterizing\nspecific emotions?, and, (c) Can the internal representations of different\nemotion categories in LLMs be interpreted through cognitive appraisal\ndimensions? Our results and analyses reveal diverse reasoning patterns across\ndifferent LLMs. Our benchmark and code will be made publicly available.", "AI": {"tldr": "This paper investigates cognitive reasoning in Large Language Models (LLMs) regarding emotions, proposing a benchmark called CoRE to evaluate their performance.", "motivation": "To advance Affective Computing by going beyond superficial emotion recognition to understand how LLMs reason about emotions.", "method": "The paper introduces the CoRE benchmark, drawing from cognitive appraisal theory, to evaluate LLMs' cognitive reasoning about emotions through a series of benchmarks and experiments.", "result": "The evaluation revealed diverse reasoning patterns in different LLMs and identified specific cognitive appraisal dimensions that influence emotion characterization.", "conclusion": "The findings advance our understanding of LLMs' emotional reasoning and contribute to the development of more emotionally intelligent AI systems.", "key_contributions": ["Introduction of CoRE, a benchmark for evaluating cognitive reasoning in LLMs regarding emotions", "Insights into how LLMs reason about emotions beyond surface recognition", "Identification of cognitive dimensions significant for emotional representation in LLMs."], "limitations": "", "keywords": ["Affective Computing", "Cognitive Reasoning", "Large Language Models", "Cognitive Appraisal Theory", "Emotional Intelligence"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.06117", "pdf": "https://arxiv.org/pdf/2508.06117.pdf", "abs": "https://arxiv.org/abs/2508.06117", "title": "A Multimodal Framework for Understanding Collaborative Design Processes", "authors": ["Maurice Koch", "Nelusa Pathmanathan", "Daniel Weiskopf", "Kuno Kurzhals"], "categories": ["cs.HC"], "comment": "Accepted to IEEE VIS 2025", "summary": "An essential task in analyzing collaborative design processes, such as those\nthat are part of workshops in design studies, is identifying design outcomes\nand understanding how the collaboration between participants formed the results\nand led to decision-making. However, findings are typically restricted to a\nconsolidated textual form based on notes from interviews or observations. A\nchallenge arises from integrating different sources of observations, leading to\nlarge amounts and heterogeneity of collected data. To address this challenge we\npropose a practical, modular, and adaptable framework of workshop setup,\nmultimodal data acquisition, AI-based artifact extraction, and visual analysis.\nOur interactive visual analysis system, reCAPit, allows the flexible\ncombination of different modalities, including video, audio, notes, or gaze, to\nanalyze and communicate important workshop findings. A multimodal streamgraph\ndisplays activity and attention in the working area, temporally aligned topic\ncards summarize participants' discussions, and drill-down techniques allow\ninspecting raw data of included sources. As part of our research, we conducted\nsix workshops across different themes ranging from social science research on\nurban planning to a design study on band-practice visualization. The latter two\nare examined in detail and described as case studies. Further, we present\nconsiderations for planning workshops and challenges that we derive from our\nown experience and the interviews we conducted with workshop experts. Our\nresearch extends existing methodology of collaborative design workshops by\npromoting data-rich acquisition of multimodal observations, combined AI-based\nextraction and interactive visual analysis, and transparent dissemination of\nresults.", "AI": {"tldr": "The paper presents a framework for analyzing collaborative design processes through multimodal data acquisition and interactive visual analysis using the system reCAPit.", "motivation": "To improve the analysis of collaborative design processes by integrating various data sources and enhancing understanding of decision-making outcomes in design workshops.", "method": "A modular framework combining workshop setup, AI-based artifact extraction, and interactive visual analysis was developed, illustrated through the implementation of the reCAPit system.", "result": "The reCAPit system enables the display of multimodal data and supports analysis through a multimodal streamgraph and topic cards, as demonstrated in case studies addressing urban planning and band-practice visualization workshops.", "conclusion": "The research introduces a new methodology for data-rich observation and analysis of design workshops, improving collaborative design practices and result dissemination.", "key_contributions": ["Development of the reCAPit interactive visual analysis system", "Integration of multimodal data sources for enhanced analysis", "Case studies showcasing the application of the framework"], "limitations": "", "keywords": ["collaborative design", "multimodal data", "visual analysis", "AI artifact extraction", "workshop methodology"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.05909", "pdf": "https://arxiv.org/pdf/2508.05909.pdf", "abs": "https://arxiv.org/abs/2508.05909", "title": "Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation", "authors": ["Zhanghao Hu", "Qinglin Zhu", "Siya Qi", "Yulan He", "Hanqi Yan", "Lin Gui"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown improved generation performance\nthrough retrieval-augmented generation (RAG) following the retriever-reader\nparadigm, which supplements model inputs with externally retrieved knowledge.\nHowever, prior work often evaluates RAG holistically, assessing the retriever\nand reader jointly, making it difficult to isolate the true contribution of\nretrieval, particularly given the prompt sensitivity of LLMs used as readers.\nWe introduce Spectrum Projection Score (SPS), a lightweight, supervision-free\nmetric that allows the reader to gauge the semantic alignment of a retrieved\nsummary with its hidden representation by comparing the area formed by\ngenerated tokens from the summary, and the principal directions of subspace in\nthe reader and to measure the relevance. Building on SPS we present xCompress,\nan inference time controller framework that dynamically samples, ranks, and\ncompresses retrieval summary candidates. Extensive experiments on five QA\nbenchmarks with four open source LLMs show that SPS not only enhances\nperformance across a range of tasks but also provides a principled perspective\non the interaction between retrieval and generation.", "AI": {"tldr": "This paper introduces Spectrum Projection Score (SPS), a new metric for evaluating retrieval-augmented generation (RAG) in large language models, along with xCompress, a framework for optimizing retrieval summaries.", "motivation": "To improve the evaluation of retrieval-augmented generation by isolating the contributions of the retriever and reader components in large language models.", "method": "The paper presents SPS, a supervision-free metric that assesses semantic alignment between retrieved summaries and their hidden representations, and describes the xCompress framework for dynamic sampling, ranking, and compressing summary candidates during inference.", "result": "Extensive experiments on five QA benchmarks demonstrate that SPS improves the performance of various LLMs while elucidating the interactions between retrieval and generation.", "conclusion": "SPS serves as a valuable tool for enhancing RAG performance and provides insights into the roles of retrieval and generation in LLMs.", "key_contributions": ["Introduction of the Spectrum Projection Score (SPS) as a metric for evaluating RAG", "Development of the xCompress framework for optimizing summary candidates", "Demonstration of improved performance across various QA benchmarks using SPS"], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Spectrum Projection Score", "Inference Control", "QA Benchmarks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.06300", "pdf": "https://arxiv.org/pdf/2508.06300.pdf", "abs": "https://arxiv.org/abs/2508.06300", "title": "Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models", "authors": ["Weihan Zhang", "Jun Tao"], "categories": ["cs.HC"], "comment": "Accepted by IEEE VIS 2025", "summary": "Explorative flow visualization allows domain experts to analyze complex flow\nstructures by interactively investigating flow patterns. However, traditional\nvisual interfaces often rely on specialized graphical representations and\ninteractions, which require additional effort to learn and use. Natural\nlanguage interaction offers a more intuitive alternative, but teaching machines\nto recognize diverse scientific concepts and extract corresponding structures\nfrom flow data poses a significant challenge. In this paper, we introduce an\nautomated framework that aligns flow pattern representations with the semantic\nspace of large language models (LLMs), eliminating the need for manual\nlabeling. Our approach encodes streamline segments using a denoising\nautoencoder and maps the generated flow pattern representations to LLM\nembeddings via a projector layer. This alignment empowers semantic matching\nbetween textual embeddings and flow representations through an attention\nmechanism, enabling the extraction of corresponding flow patterns based on\ntextual descriptions. To enhance accessibility, we develop an interactive\ninterface that allows users to query and visualize flow structures using\nnatural language. Through case studies, we demonstrate the effectiveness of our\nframework in enabling intuitive and intelligent flow exploration.", "AI": {"tldr": "This paper presents an automated framework for explorative flow visualization that aligns flow pattern representations with large language models, enabling natural language interaction for analyzing complex flow structures.", "motivation": "The paper addresses the challenge of traditional visual interfaces in flow analysis, which are often difficult to learn and use, by introducing natural language interaction as a more intuitive alternative.", "method": "An automated framework that encodes streamline segments with a denoising autoencoder and maps them to LLM embeddings for semantic matching using an attention mechanism.", "result": "The proposed framework effectively allows users to extract flow patterns based on natural language queries, enhancing the accessibility of flow visualization.", "conclusion": "The framework demonstrates promising results in enabling intuitive flow exploration and interaction, informed by test cases.", "key_contributions": ["Automatic alignment of flow pattern representations with LLM semantic space", "Interactive querying and visualization through natural language", "Elimination of manual labeling requirements"], "limitations": "", "keywords": ["Flow Visualization", "Natural Language Interaction", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.05938", "pdf": "https://arxiv.org/pdf/2508.05938.pdf", "abs": "https://arxiv.org/abs/2508.05938", "title": "Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale", "authors": ["Rafal Kocielnik", "Min Kim", "Penphob", "Boonyarungsrit", "Fereshteh Soltani", "Deshawn Sambrano", "Animashree Anandkumar", "R. Michael Alvarez"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Detecting prosociality in text--communication intended to affirm, support, or\nimprove others' behavior--is a novel and increasingly important challenge for\ntrust and safety systems. Unlike toxic content detection, prosociality lacks\nwell-established definitions and labeled data, requiring new approaches to both\nannotation and deployment. We present a practical, three-stage pipeline that\nenables scalable, high-precision prosocial content classification while\nminimizing human labeling effort and inference costs. First, we identify the\nbest LLM-based labeling strategy using a small seed set of human-labeled\nexamples. We then introduce a human-AI refinement loop, where annotators review\nhigh-disagreement cases between GPT-4 and humans to iteratively clarify and\nexpand the task definition-a critical step for emerging annotation tasks like\nprosociality. This process results in improved label quality and definition\nalignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train\na two-stage inference system: a lightweight classifier handles high-confidence\npredictions, while only $\\sim$35\\% of ambiguous instances are escalated to\nGPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving\nhigh precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI\ninteraction, careful task formulation, and deployment-aware architecture design\ncan unlock scalable solutions for novel responsible AI tasks.", "AI": {"tldr": "A novel three-stage pipeline for classifying prosociality in text, minimizing human effort while achieving high accuracy and reducing costs.", "motivation": "Detecting prosociality in text communication is an important challenge for trust and safety systems, and current methods suffer from a lack of definitions and labeled data.", "method": "The approach consists of three stages: identifying an LLM-based labeling strategy, introducing a human-AI refinement loop for task definition clarity, and synthesizing 10k high-quality labels to train a two-stage inference system.", "result": "The proposed system achieves high precision (approximately 0.90) and reduces inference costs by about 70%.", "conclusion": "The pipeline showcases how targeted human-AI interaction and thoughtful architecture can facilitate scalable solutions for emerging responsible AI tasks like prosociality detection.", "key_contributions": ["Scalable pipeline for prosocial content classification", "Human-AI refinement loop enhances labeling accuracy", "Cost-efficient architecture reduces inference costs significantly"], "limitations": "", "keywords": ["prosociality", "human-AI interaction", "LLM", "trust and safety", "responsible AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.06349", "pdf": "https://arxiv.org/pdf/2508.06349.pdf", "abs": "https://arxiv.org/abs/2508.06349", "title": "Emoji Reactions on Telegram Often Reflect Social Approval Over Emotional Resonance", "authors": ["Serena Tardelli", "Lorenzo Alvisi", "Lorenzo Cima", "Stefano Cresci", "Maurizio Tesconi"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Emoji reactions are a frequently used feature of messaging platforms. Prior\nwork mainly interpreted emojis as indicators of emotional resonance or user\nsentiment. However, emoji reactions may instead reflect broader social\ndynamics. Here, we investigate the communicative function of emoji reactions on\nTelegram by analyzing the relationship between the emotional and rhetorical\ncontent of messages and the emoji reactions they receive. We collect and\nanalyze over 650k Telegram messages that received at least one emoji reaction.\nWe annotate each message with sentiment, emotion, persuasion strategy, and\nspeech act labels, and infer the sentiment and emotion of emoji reactions using\nboth lexicons and large languages. We find a systematic mismatch between\nmessage sentiment and reaction sentiment, with positive reactions dominating\neven when the message is neutral or negative. We show that this pattern remains\nconsistent across rhetorical strategies and emotional tones, suggesting that\nemoji reactions may signal a degree of social approval rather than reflecting\nemotional resonance. Finally, we shed light on the communicative strategies\nthat predict greater emoji engagement. These findings have methodological\nimplications for sentiment analysis, as interpreting emoji reactions as direct\nproxies for emotional response may be misleading.", "AI": {"tldr": "This paper investigates the communicative function of emoji reactions on Telegram, finding that emoji reactions may indicate social approval rather than purely emotional responses.", "motivation": "To understand the broader social dynamics reflected in emoji reactions beyond simple emotional indicators.", "method": "Analyzed over 650k Telegram messages to examine the relationship between message content and emoji reactions, annotating messages with sentiment and emotional labels, and using language models for inference.", "result": "There is a systematic mismatch where positive emoji reactions dominate even neutral or negative messages, indicating that reactions may be more about social approval.", "conclusion": "Emoji reactions might not be reliable proxies for emotional responses, highlighting the need for refined approaches in sentiment analysis.", "key_contributions": ["Analysis of over 650k Telegram messages regarding emoji reactions.", "Demonstration of the mismatch between message sentiment and emoji reaction sentiment.", "Insights into communicative strategies that increase emoji engagement."], "limitations": "Focused on Telegram, which may limit generalizability to other platforms.", "keywords": ["emoji", "sentiment analysis", "social dynamics", "communication", "Telegram"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.05987", "pdf": "https://arxiv.org/pdf/2508.05987.pdf", "abs": "https://arxiv.org/abs/2508.05987", "title": "Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring", "authors": ["Chunyun Zhang", "Hongyan Zhao", "Chaoran Cui", "Qilong Song", "Zhiqing Lu", "Shuai Gong", "Kailin Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-topic automated essay scoring (AES) aims to develop a transferable\nmodel capable of effectively evaluating essays on a target topic. A significant\nchallenge in this domain arises from the inherent discrepancies between topics.\nWhile existing methods predominantly focus on extracting topic-shared features\nthrough distribution alignment of source and target topics, they often neglect\ntopic-specific features, limiting their ability to assess critical traits such\nas topic adherence. To address this limitation, we propose an Adversarial\nTOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns\ntopic-shared and topic-specific features to improve cross-topic AES. ATOP\nachieves this by optimizing a learnable topic-aware prompt--comprising both\nshared and specific components--to elicit relevant knowledge from pre-trained\nlanguage models (PLMs). To enhance the robustness of topic-shared prompt\nlearning and mitigate feature scale sensitivity introduced by topic alignment,\nwe incorporate adversarial training within a unified regression and\nclassification framework. In addition, we employ a neighbor-based classifier to\nmodel the local structure of essay representations and generate pseudo-labels\nfor target-topic essays. These pseudo-labels are then used to guide the\nsupervised learning of topic-specific prompts tailored to the target topic.\nExtensive experiments on the publicly available ASAP++ dataset demonstrate that\nATOP significantly outperforms existing state-of-the-art methods in both\nholistic and multi-trait essay scoring. The implementation of our method is\npublicly available at: https://anonymous.4open.science/r/ATOP-A271.", "AI": {"tldr": "Proposes a novel method Adversarial TOpic-aware Prompt-tuning (ATOP) for improving cross-topic automated essay scoring (AES) by jointly learning topic-shared and topic-specific features.", "motivation": "Existing methods for cross-topic AES fail to account for topic-specific features, limiting their effectiveness in accurately evaluating essays.", "method": "ATOP learns topic-shared and topic-specific features using a learnable topic-aware prompt within a unified regression and classification framework; it employs adversarial training and a neighbor-based classifier for improved performance.", "result": "ATOP significantly outperforms state-of-the-art methods on the ASAP++ dataset in both holistic and multi-trait essay scoring.", "conclusion": "The effectiveness of the ATOP method demonstrates the importance of considering both shared and specific features in cross-topic AES tasks.", "key_contributions": ["Introduction of the Adversarial TOpic-aware Prompt-tuning (ATOP) method.", "Joint learning of topic-shared and topic-specific features for automated essay scoring.", "Use of adversarial training and neighbor-based classification to improve essay scoring accuracy."], "limitations": "", "keywords": ["automated essay scoring", "topic-aware prompt-tuning", "adversarial training", "language models", "essay evaluation"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2508.06354", "pdf": "https://arxiv.org/pdf/2508.06354.pdf", "abs": "https://arxiv.org/abs/2508.06354", "title": "Zombitron: towards a toolbox for repurposing obsolete smartphones into new interactive systems", "authors": ["Clara Rigaud"], "categories": ["cs.HC"], "comment": "Post-proceedings paper presented at LIMITS 2025: 11th Workshop on\n  Computing within Limits, 2025-06-26/27, Online", "summary": "This article explores the possibilities of reusing obsolete smartphones and\ntablets to build new interactive systems. Taking the case of a musical\ninstrument, I present my research into the design of a controller made from\nvarious of these obsolete smartphones. From the diagnostic stage to the\ncreation of a new autonomous electronic object, I document the process, the\nbarriers and the levers encountered. Based on these explorations and\ndiscussions with two professional musicians, I provide several insights into\nthe software and hardware aspects, with a view to continuing this work, towards\nthe creation of an open-source toolkit enabling anyone to build new interactive\nsystems with old devices. I discuss the implication of how a high-level\nweb-based approach could allow designers to enter the black box and foster\npermacomputing using smartphones.", "AI": {"tldr": "This article discusses reusing obsolete smartphones and tablets to create new interactive systems, particularly a musical instrument controller.", "motivation": "To explore the potential of reusing outdated smartphones and tablets for innovative applications and design new interactive systems that are accessible.", "method": "The author documents the design process of a musical instrument using obsolete smartphones, including diagnostic stages, creation, and insights gathered from professional musicians.", "result": "The research provides insights into software and hardware design challenges and opportunities, aiming to create an open-source toolkit for building interactive systems from old devices.", "conclusion": "The work suggests a high-level web-based approach to facilitate permacomputing with smartphones and encourages further research and development in this area.", "key_contributions": ["Development of a musical instrument controller using obsolete devices.", "Documentation of the design process with practical insights for future ventures.", "Proposal for an open-source toolkit enabling the creation of interactive systems."], "limitations": "Focuses predominantly on the musical application; may not address broader interactive system needs across other domains.", "keywords": ["obsolete smartphones", "interactive systems", "musical instrument", "open-source toolkit", "permacomputing"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.06016", "pdf": "https://arxiv.org/pdf/2508.06016.pdf", "abs": "https://arxiv.org/abs/2508.06016", "title": "Crisp Attention: Regularizing Transformers via Structured Sparsity", "authors": ["Sagar Gandhi", "Vishal Gandhi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The quadratic computational cost of the self-attention mechanism is a primary\nchallenge in scaling Transformer models. While attention sparsity is widely\nstudied as a technique to improve computational efficiency, it is almost\nuniversally assumed to come at the cost of model accuracy. In this paper, we\nreport a surprising counter-example to this common wisdom. By introducing\nstructured, post-hoc sparsity to the attention mechanism of a DistilBERT model\nduring fine-tuning on the SST-2 sentiment analysis task, we find that model\naccuracy improves significantly. Our model with 80\\% attention sparsity\nachieves a validation accuracy of 91.59\\%, a 0.97\\% absolute improvement over\nthe dense baseline. We hypothesize that this phenomenon is due to sparsity\nacting as a powerful implicit regularizer, preventing the model from\noverfitting by forcing it to make predictions with a more constrained and\nrobust set of features. Our work recasts attention sparsity not just as a tool\nfor computational efficiency, but as a potential method for improving the\ngeneralization and performance of Transformer models.", "AI": {"tldr": "This paper shows that introducing structured post-hoc sparsity to the attention mechanism of DistilBERT can enhance model accuracy on sentiment analysis tasks, contrary to the conventional belief that sparsity reduces performance.", "motivation": "To challenge the assumption that attention sparsity reduces model accuracy in Transformer models and explore its potential as a method for enhancing performance.", "method": "The authors introduce structured, post-hoc sparsity to the attention mechanism of a DistilBERT model during fine-tuning on the SST-2 sentiment analysis task, evaluating its impact on model accuracy.", "result": "The model achieves 80% attention sparsity with a validation accuracy of 91.59%, which is a 0.97% absolute improvement over the dense baseline.", "conclusion": "Sparsity can act as a powerful implicit regularizer, improving generalization and performance rather than just serving for computational efficiency.", "key_contributions": ["Demonstrated an improvement in accuracy with attention sparsity on a Transformer model", "Proposed sparsity as a method for enhancing generalization", "Provided empirical evidence against the belief that sparsity harms model performance."], "limitations": "", "keywords": ["attention sparsity", "Transformer models", "DistilBERT"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.06484", "pdf": "https://arxiv.org/pdf/2508.06484.pdf", "abs": "https://arxiv.org/abs/2508.06484", "title": "Non-programmers Assessing AI-Generated Code: A Case Study of Business Users Analyzing Data", "authors": ["Yuvraj Virk", "Dongyu Liu"], "categories": ["cs.HC"], "comment": "Accepted by VL/HCC 2025", "summary": "Non-technical end-users increasingly rely on AI code generation to perform\ntechnical tasks like data analysis. However, large language models (LLMs)\nremain unreliable, and it is unclear whether end-users can effectively identify\nmodel errors $\\unicode{x2014}$ especially in realistic and domain-specific\nscenarios. We surveyed marketing and sales professionals to assess their\nability to critically evaluate LLM-generated analyses of marketing data.\nParticipants were shown natural language explanations of the AI's code,\nrepeatedly informed the AI often makes mistakes, and explicitly prompted to\nidentify them. Yet, participants frequently failed to detect critical flaws\nthat could compromise decision-making, many of which required no technical\nknowledge to recognize. To investigate why, we reformatted AI responses into\nclearly delineated steps and provided alternative approaches for each decision\nto support critical evaluation. While these changes had a positive effect,\nparticipants often struggled to reason through the AI's steps and alternatives.\nOur findings suggest that business professionals cannot reliably verify\nAI-generated data analyses on their own and explore reasons why to inform\nfuture designs. As non-programmers adopt code-generating AI for technical\ntasks, unreliable AI and insufficient human oversight poses risks of unsafe or\nlow-quality decisions.", "AI": {"tldr": "This paper investigates the ability of non-technical end-users, specifically marketing and sales professionals, to identify errors in AI-generated data analyses and suggests design improvements to enhance critical evaluation.", "motivation": "As reliance on AI code generation increases among non-technical users, it's crucial to understand their capability to detect errors in AI outputs, particularly in high-stakes decision-making contexts.", "method": "The study surveyed marketing and sales professionals, assessing their ability to evaluate LLM-generated analyses through direct engagement with the AI's outputs and feedback on its reliability.", "result": "Participants frequently failed to identify critical flaws in AI-generated analyses, even when prompted. Despite reformatting AI responses to support critical evaluation, reasoning through AI's steps remained challenging for users.", "conclusion": "The findings indicate that current AI-generated analyses may present risks for decision-making among business professionals due to their inability to verify the outputs adequately, highlighting the need for better user support in AI designs.", "key_contributions": ["Identified critical flaws in AI outputs that non-technical users struggle to detect.", "Showed that providing structured formats for AI responses can enhance user evaluation but still face limitations.", "Called attention to the risks posed by unreliable AI in business decision-making contexts."], "limitations": "Still limited in how well the restructured AI outputs could aid user reasoning; further investigation needed to improve understanding.", "keywords": ["AI code generation", "human oversight", "data analysis", "LLM errors", "business decision-making"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.06026", "pdf": "https://arxiv.org/pdf/2508.06026.pdf", "abs": "https://arxiv.org/abs/2508.06026", "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future", "authors": ["Yidong Wang", "Xin Wang", "Cunxiang Wang", "Junfeng Fang", "Qiufeng Wang", "Jianing Chu", "Xuran Meng", "Shuxun Yang", "Libo Qin", "Yue Zhang", "Wei Ye", "Shikun Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose \\textbf{Temporal Self-Rewarding\nLanguage Models} that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) \\textit{Anchored Rejection} - fixing rejected responses using the past\ninitial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data.", "AI": {"tldr": "This paper introduces Temporal Self-Rewarding Language Models that improve preference learning in LLMs by coordinating past, present, and future generations, overcoming limitations of existing Self-Rewarding paradigms.", "motivation": "To address the limitations in current Self-Rewarding Language Models which narrow representational differences necessary for effective preference learning.", "method": "The paper proposes a dual-phase framework, introducing Anchored Rejection and Future-Guided Chosen methodologies to coordinate model generations for improved learning signals.", "result": "Experiments showed significant improvements in performance metrics on various model families (Llama, Qwen, Mistral) and sizes, with Llama3.1-8B achieving a 29.44 win rate on AlpacaEval 2.0, outperforming the Self-Rewarding baseline of 19.69.", "conclusion": "The proposed method enhances generalization across multiple tasks, including mathematical reasoning and code generation, without requiring specific training data for those tasks.", "key_contributions": ["Introduction of Temporal Self-Rewarding Language Models that coordinate model generations", "Anchored Rejection fixing rejected responses to maintain variability", "Future-Guided Chosen dynamically curating responses for improved preference learning"], "limitations": "", "keywords": ["Language Models", "Self-Rewarding", "Preference Learning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.06030", "pdf": "https://arxiv.org/pdf/2508.06030.pdf", "abs": "https://arxiv.org/abs/2508.06030", "title": "Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings", "authors": ["Kartik Sharma", "Yiqiao Jin", "Rakshit Trivedi", "Srijan Kumar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) acquire knowledge across diverse domains such as\nscience, history, and geography encountered during generative pre-training.\nHowever, due to their stochasticity, it is difficult to predict what LLMs have\nacquired. Prior work has developed different ways to probe this knowledge by\ninvestigating the hidden representations, crafting specific task prompts,\ncurating representative samples, and estimating their uncertainty. However,\nthese methods require making forward passes through the underlying model to\nprobe the LLM's knowledge about a specific fact, making them computationally\nexpensive and time-consuming. To bridge this gap, we propose $\\textbf{PEEK}$ or\n$\\textbf{P}$roxy $\\textbf{E}$mbeddings to $\\textbf{E}$stimate\n$\\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models\nthat effectively encode factual knowledge as text or graphs as proxies for\nLLMs. First, we identify a training set of facts known by LLMs through various\nprobing strategies and then adapt embedding models to predict the LLM outputs\nwith a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived\ndatasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict\nLLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find\nthat sentence embedding models are more suitable than graph embeddings to\npredict LLM knowledge, shedding light on the underlying representation of the\nfactual landscape. Thus, we believe that knowledge-adapted embeddings can be\nused to identify knowledge gaps in LLMs at scale and can provide deeper\ninsights into LLMs' internal inductive bias. The code and data are made\navailable at https://github.com/claws-lab/peek.", "AI": {"tldr": "The paper introduces PEEK, a method using proxy embeddings to estimate knowledge in large language models (LLMs) more efficiently than existing probing techniques.", "motivation": "To address the computational expense and time consumption of probing large language models (LLMs) for estimating their knowledge, this work aims to create a more efficient alternative.", "method": "The authors propose PEEK, which leverages pre-trained embedding models by identifying a training set of factual knowledge known to LLMs and adapting embedding models to predict LLM outputs with a linear decoder.", "result": "Embeddings can predict LLM knowledge with up to 90% accuracy on a held-out set, demonstrating that embeddings can effectively estimate the knowledge encapsulated in LLMs.", "conclusion": "Knowledge-adapted embeddings can identify knowledge gaps in LLMs at scale and provide better insights into LLMs' internal inductive biases. The resources used in the research will be available on GitHub.", "key_contributions": ["Introduction of PEEK, a new method for estimating LLM knowledge", "Demonstration of up to 90% accuracy in predicting LLM knowledge using embedding models", "Identification of sentence embeddings as more suitable than graph embeddings for this task"], "limitations": "", "keywords": ["Large Language Models", "Knowledge Estimation", "Proxy Embeddings", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.06046", "pdf": "https://arxiv.org/pdf/2508.06046.pdf", "abs": "https://arxiv.org/abs/2508.06046", "title": "EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation", "authors": ["Xinda Wang", "Zhengxu Hou", "Yangshijie Zhang", "Bingren Yan", "Zhibo Yang", "Xingsheng Zhang", "Luxi Xing", "Qiang Zhou", "Chen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although the effectiveness of Large Language Models (LLMs) as judges\n(LLM-as-a-judge) has been validated, their performance remains limited in\nopen-ended tasks, particularly in story evaluation. Accurate story evaluation\nis crucial not only for assisting human quality judgment but also for providing\nkey signals to guide story generation. However, existing methods face a\ndilemma: prompt engineering for closed-source models suffers from poor\nadaptability, while fine-tuning approaches for open-source models lack the\nrigorous reasoning capabilities essential for story evaluation. To address\nthis, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.\nGrounded in pairwise comparison, the framework first self-synthesizes\nscore-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To\nensure data quality, these raw CoTs undergo a self-filtering process, utilizing\nmulti-agents to guarantee their logical rigor and robustness. Finally, the\nevaluator trained on the refined data is deployed as a reward model to guide\nthe story generation task. Experimental results demonstrate that our framework\nachieves state-of-the-art (SOTA) performance on three evaluation benchmarks\nincluding StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward\nmodel, it significantly enhances the quality of generated stories, thereby\nfully validating the superiority of our self-evolving approach.", "AI": {"tldr": "The Self-Evolving Pairwise Reasoning (EvolvR) framework enhances story evaluation using LLMs by self-synthesizing high-quality comparison data and deploying it as a reward model.", "motivation": "To improve the effectiveness of LLMs in open-ended tasks like story evaluation and generation.", "method": "The EvolvR framework self-synthesizes score-aligned Chain-of-Thought (CoT) data through a multi-persona approach and employs a filtering process for logical rigor before training a reward model.", "result": "EvolvR achieves state-of-the-art performance on three benchmarks (StoryER, HANNA, OpenMEVA) and significantly improves story quality when used as a reward model.", "conclusion": "The self-evolving approach validates the potential of LLMs as judges in story evaluation and generation tasks.", "key_contributions": ["Introduction of the EvolvR framework for story evaluation", "Self-synthesis of score-aligned CoT data using multi-persona strategy", "Demonstration of state-of-the-art performance on evaluation benchmarks"], "limitations": "", "keywords": ["Large Language Models", "story evaluation", "self-evolving framework", "reward model", "Chain-of-Thought"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.06094", "pdf": "https://arxiv.org/pdf/2508.06094.pdf", "abs": "https://arxiv.org/abs/2508.06094", "title": "ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline", "authors": ["Morris Alper", "Moran Yanuka", "Raja Giryes", "Gaper Begu"], "categories": ["cs.CL"], "comment": "Project page: https://conlangcrafter.github.io", "summary": "Constructed languages (conlangs) such as Esperanto and Quenya have played\ndiverse roles in art, philosophy, and international communication. Meanwhile,\nlarge-scale foundation models have revolutionized creative generation in text,\nimages, and beyond. In this work, we leverage modern LLMs as computational\ncreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a\nmulti-hop pipeline that decomposes language design into modular stages --\nphonology, morphology, syntax, lexicon generation, and translation. At each\nstage, our method leverages LLMs' meta-linguistic reasoning capabilities,\ninjecting randomness to encourage diversity and leveraging self-refinement\nfeedback to encourage consistency in the emerging language description. We\nevaluate ConlangCrafter on metrics measuring coherence and typological\ndiversity, demonstrating its ability to produce coherent and varied conlangs\nwithout human linguistic expertise.", "AI": {"tldr": "ConlangCrafter uses LLMs to create constructed languages through a modular and multi-hop process.", "motivation": "To utilize large-scale language models in the creative process of constructing languages, enhancing the diversity and coherence of conlangs.", "method": "ConlangCrafter is a structured pipeline that breaks down language creation into stages: phonology, morphology, syntax, lexicon generation, and translation, using LLMs to facilitate each step.", "result": "The evaluation shows that ConlangCrafter can produce coherent and diverse constructed languages without requiring linguistic expertise.", "conclusion": "The method demonstrates the potential of LLMs in creative language generation by allowing non-experts to create functional conlangs.", "key_contributions": ["Introduction of ConlangCrafter for conlang creation", "Multi-hop pipeline that modularizes language design", "Utilization of LLMs for enhancing creativity and coherence"], "limitations": "The approach may not capture all the intricacies of human language due to reliance on LLMs.", "keywords": ["constructed languages", "LLMs", "ConlangCrafter", "language generation", "creativity"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.06103", "pdf": "https://arxiv.org/pdf/2508.06103.pdf", "abs": "https://arxiv.org/abs/2508.06103", "title": "Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Ammar Mohammed"], "categories": ["cs.CL", "cs.IR"], "comment": "6 pages , 2 figures , Accepted in IMSA 2025,Egypt ,\n  https://imsa.msa.edu.eg/", "summary": "This paper presents two effective approaches for Extractive Question\nAnswering (QA) on the Quran. It addresses challenges related to complex\nlanguage, unique terminology, and deep meaning in the text. The second uses\nfew-shot prompting with instruction-tuned large language models such as Gemini\nand DeepSeek. A specialized Arabic prompt framework is developed for span\nextraction. A strong post-processing system integrates subword alignment,\noverlap suppression, and semantic filtering. This improves precision and\nreduces hallucinations. Evaluations show that large language models with Arabic\ninstructions outperform traditional fine-tuned models. The best configuration\nachieves a pAP10 score of 0.637. The results confirm that prompt-based\ninstruction tuning is effective for low-resource, semantically rich QA tasks.", "AI": {"tldr": "The paper presents two approaches for Extractive Question Answering on the Quran, utilizing instruction-tuned large language models for enhanced performance.", "motivation": "Address challenges in Extractive QA related to the Quran's complex language and unique terminology.", "method": "The study employs few-shot prompting with LLMs (Gemini, DeepSeek) and uses a dedicated Arabic prompt framework alongside a comprehensive post-processing system.", "result": "Evaluation results indicate that the proposed approach significantly outperforms traditional fine-tuned models, achieving a pAP10 score of 0.637.", "conclusion": "Prompt-based instruction tuning is shown to be effective for low-resource, semantically rich QA tasks, improving precision and reducing hallucinations in answers.", "key_contributions": ["Development of a specialized Arabic prompt framework for QA", "Integration of a robust post-processing system", "Demonstration of better performance of LLMs over traditional models"], "limitations": "", "keywords": ["Extractive Question Answering", "large language models", "Arabic", "few-shot prompting", "post-processing"], "importance_score": 3, "read_time_minutes": 6}}
{"id": "2508.06105", "pdf": "https://arxiv.org/pdf/2508.06105.pdf", "abs": "https://arxiv.org/abs/2508.06105", "title": "You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures", "authors": ["Shengyuan Chen", "Chuang Zhou", "Zheng Yuan", "Qinggang Zhang", "Zeyang Cui", "Hao Chen", "Yilin Xiao", "Jiannong Cao", "Xiao Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect statements when handling questions beyond their knowledge\nand perception. Retrieval-augmented generation (RAG) addresses this by\nretrieving query-relevant contexts from knowledge bases to support LLM\nreasoning. Recent advances leverage pre-constructed graphs to capture the\nrelational connections among distributed documents, showing remarkable\nperformance in complex tasks. However, existing Graph-based RAG (GraphRAG)\nmethods rely on a costly process to transform the corpus into a graph,\nintroducing overwhelming token cost and update latency. Moreover, real-world\nqueries vary in type and complexity, requiring different logic structures for\naccurate reasoning. The pre-built graph may not align with these required\nstructures, resulting in ineffective knowledge retrieval. To this end, we\npropose a \\textbf{\\underline{Logic}}-aware\n\\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented\n\\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically\nextracts reasoning structures at inference time to guide adaptive retrieval\nwithout any pre-built graph. LogicRAG begins by decomposing the input query\ninto a set of subproblems and constructing a directed acyclic graph (DAG) to\nmodel the logical dependencies among them. To support coherent multi-step\nreasoning, LogicRAG then linearizes the graph using topological sort, so that\nsubproblems can be addressed in a logically consistent order. Besides, LogicRAG\napplies graph pruning to reduce redundant retrieval and uses context pruning to\nfilter irrelevant context, significantly reducing the overall token cost.\nExtensive experiments demonstrate that LogicRAG achieves both superior\nperformance and efficiency compared to state-of-the-art baselines.", "AI": {"tldr": "LogicRAG is a novel framework that enhances retrieval-augmented generation in large language models by dynamically extracting reasoning structures at inference time, avoiding the need for costly pre-built graphs.", "motivation": "Existing GraphRAG methods incur high costs in transforming corpora into graphs, leading to token and update inefficiencies, and do not adapt well to varying query complexities.", "method": "LogicRAG decomposes input queries into subproblems, constructs a directed acyclic graph (DAG) to model dependencies, and employs topological sorting and graph/context pruning for coherent multi-step reasoning.", "result": "LogicRAG demonstrates superior performance and efficiency over state-of-the-art baselines in retrieval-augmented generation tasks.", "conclusion": "The proposed framework allows for more adaptive and efficient reasoning in response to complex queries without prior graph construction.", "key_contributions": ["Introduces LogicRAG for dynamic reasoning structure extraction at inference time.", "Models logical dependencies with directed acyclic graphs (DAGs).", "Implements graph and context pruning to reduce token cost."], "limitations": "", "keywords": ["large language models", "retrieval-augmented generation", "logic-aware framework", "dynamic retrieval", "graph pruning"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2508.06124", "pdf": "https://arxiv.org/pdf/2508.06124.pdf", "abs": "https://arxiv.org/abs/2508.06124", "title": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models", "authors": ["Sayantan Adak", "Pratyush Chatterjee", "Somnath Banerjee", "Rima Hazra", "Somak Aditya", "Animesh Mukherjee"], "categories": ["cs.CL"], "comment": null, "summary": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications.", "AI": {"tldr": "AURA is a multi-layered framework utilizing Process Reward Models (PRMs) to enhance safety and logical coherence in LLM outputs by providing step-level evaluations and proactive interventions.", "motivation": "Present-day LLMs struggle with managing safety risks due to overlooked logical implications in their outputs, necessitating more effective safety solutions.", "method": "The paper introduces AURA, a framework that incorporates introspective self-critique, fine-grained PRM assessments, and adaptive safety-aware decoding for safer reasoning in LLMs.", "result": "Empirical evidence shows that AURA significantly outperforms existing safety methodologies, improving logical integrity and safety-awareness in model outputs.", "conclusion": "This framework marks a key advancement toward safer and more responsible AI, setting new standards for alignment-sensitive applications.", "key_contributions": ["Introduction of the AURA framework for safety in LLMs", "Integration of Process Reward Models for step-level evaluations", "Demonstration of superior performance in logical integrity and safety of outputs"], "limitations": "", "keywords": ["Language Model", "Safety", "Process Reward Models", "AI Alignment", "Logical Coherence"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.06167", "pdf": "https://arxiv.org/pdf/2508.06167.pdf", "abs": "https://arxiv.org/abs/2508.06167", "title": "Pragmatics beyond humans: meaning, communication, and LLMs", "authors": ["Vt Gvodiak"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI.", "AI": {"tldr": "The paper reconceptualizes pragmatics in the context of large language models (LLMs), proposing a new Human-Machine Communication framework to address the limitations of traditional theories and the unique nature of machine-centered communication.", "motivation": "To refine the understanding of pragmatics in light of the emergence of LLMs and their impact on human communication.", "method": "The paper critiques traditional semiotic approaches and introduces the Human-Machine Communication (HMC) framework. It also discusses the limitations of Gricean pragmatics and advocates for probabilistic pragmatics as a more relevant approach for LLMs.", "result": "The paper finds that traditional pragmatic theories are inadequately equipped to address the dynamics introduced by LLMs, and proposes that context frustration necessitates a reevaluation of pragmatic frameworks for generative AI communication.", "conclusion": "Pragmatics might require modification or expansion to effectively capture the nuances of communication involving generative AI, particularly in how users interact with LLMs.", "key_contributions": ["Proposes the Human-Machine Communication (HMC) framework as an alternative to traditional pragmatics.", "Introduces the concept of context frustration in human-machine communication.", "Advocates for the use of probabilistic pragmatics over Gricean models in machine contexts."], "limitations": "The paper primarily focuses on theoretical exploration and may lack empirical validation of the proposed frameworks in real-world applications.", "keywords": ["Pragmatics", "Human-Machine Communication", "Large Language Models", "Probabilistic Pragmatics", "Context Frustration"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2508.06135", "pdf": "https://arxiv.org/pdf/2508.06135.pdf", "abs": "https://arxiv.org/abs/2508.06135", "title": "Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs.", "AI": {"tldr": "Selective Reflection Distillation (SRD) is a framework for improving Knowledge Distillation (KD) that emphasizes training data quality and model compatibility, leading to enhanced performance of compressed language models.", "motivation": "Existing KD methods overlook critical factors such as training data quality and student-model compatibility, which are vital for successful model compression.", "method": "SRD dynamically evaluates and selects prompt-response pairs from training data using automated ranking based on difficulty, alongside a curriculum scheduling strategy to introduce curated data in phases during distillation.", "result": "SRD shows consistent improvements in distilled model performance and reduces training runtime by up to 39% across various KD methods and model families.", "conclusion": "The framework highlights the importance of data quality and compatibility in effective LLM distillation and provides a straightforward method to enhance performance without altering underlying KD algorithms.", "key_contributions": ["Proposes a novel data curation framework for knowledge distillation.", "Demonstrates significant performance improvements and reduced computational costs during training.", "Offers insights into the importance of data-centric factors in knowledge distillation."], "limitations": "", "keywords": ["Knowledge Distillation", "Language Models", "Data Curation", "Curriculum Learning", "Efficiency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.06196", "pdf": "https://arxiv.org/pdf/2508.06196.pdf", "abs": "https://arxiv.org/abs/2508.06196", "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations", "authors": ["Nizi Nazar", "Ehsaneddin Asgari"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment.", "AI": {"tldr": "The paper introduces a four-layer taxonomy of Emotional Intelligence (EI) for LLMs and presents a benchmark called EICAP-Bench to evaluate LLMs on EI capabilities, showing limitations in current models and areas for improvement through fine-tuning.", "motivation": "To address the gap in integrating Emotional Intelligence into the development of human-aligned LLMs.", "method": "Introduces a four-layer EI taxonomy and evaluates six LLMs using the EICAP-Bench benchmark, applying LoRA adapters for fine-tuning on a large-scale dialogue dataset.", "result": "Qwen2.5-Instruct is identified as the strongest LLM in EI capabilities, and only the Appraisal layer shows significant improvement from fine-tuning.", "conclusion": "Existing pretraining and instruction-tuning paradigms limit LLMs' emotional reasoning; targeted strategies are required for better EI alignment.", "key_contributions": ["Unified four-layer taxonomy of EI for LLMs", "Launch of EICAP-Bench benchmark", "Fine-tuning insights for enhancing EI in LLMs"], "limitations": "Focuses primarily on the Appraisal layer; other layers showed limited improvement.", "keywords": ["Emotional Intelligence", "Large Language Models", "Benchmark", "Fine-tuning", "Human-Aligned AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.06149", "pdf": "https://arxiv.org/pdf/2508.06149.pdf", "abs": "https://arxiv.org/abs/2508.06149", "title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts", "authors": ["Gunhee Cho", "Yun-Gyung Cheong"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "We present Big5-Scaler, a prompt-based framework for conditioning large\nlanguage models (LLMs) with controllable Big Five personality traits. By\nembedding numeric trait values into natural language prompts, our method\nenables fine-grained personality control without additional training. We\nevaluate Big5-Scaler across trait expression, dialogue generation, and human\ntrait imitation tasks. Results show that it induces consistent and\ndistinguishable personality traits across models, with performance varying by\nprompt type and scale. Our analysis highlights the effectiveness of concise\nprompts and lower trait intensities, providing a efficient approach for\nbuilding personality-aware dialogue agents.", "AI": {"tldr": "Big5-Scaler is a prompt-based framework for conditioning LLMs with Big Five personality traits, enabling personality control without additional training.", "motivation": "To create a method for fine-grained personality control in dialogue agents using LLMs without the need for extensive retraining.", "method": "Embedding numeric Big Five personality trait values into natural language prompts to control dialogue generation and trait expression.", "result": "The framework induces consistent and distinguishable personality traits across different models, with results influenced by prompt type and scale.", "conclusion": "Big5-Scaler offers an efficient approach to building personality-aware dialogue agents by using concise prompts and lower trait intensities.", "key_contributions": ["Introduction of a prompt-based framework for personality conditioning in LLMs", "Consistency in trait expression and dialogue generation", "Insights on prompt efficiency for personality control"], "limitations": "Limited to controllable personality traits; results may vary across different LLM architectures and data contexts.", "keywords": ["Big Five", "personality traits", "large language models", "dialogue generation", "prompt engineering"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.06155", "pdf": "https://arxiv.org/pdf/2508.06155.pdf", "abs": "https://arxiv.org/abs/2508.06155", "title": "Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach", "authors": ["Renhan Zhang", "Lian Lian", "Zhen Qi", "Guiran Liu"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the issue of implicit stereotypes that may arise during\nthe generation process of large language models. It proposes an interpretable\nbias detection method aimed at identifying hidden social biases in model\noutputs, especially those semantic tendencies that are not easily captured\nthrough explicit linguistic features. The method combines nested semantic\nrepresentation with a contextual contrast mechanism. It extracts latent bias\nfeatures from the vector space structure of model outputs. Using attention\nweight perturbation, it analyzes the model's sensitivity to specific social\nattribute terms, thereby revealing the semantic pathways through which bias is\nformed. To validate the effectiveness of the method, this study uses the\nStereoSet dataset, which covers multiple stereotype dimensions including\ngender, profession, religion, and race. The evaluation focuses on several key\nmetrics, such as bias detection accuracy, semantic consistency, and contextual\nsensitivity. Experimental results show that the proposed method achieves strong\ndetection performance across various dimensions. It can accurately identify\nbias differences between semantically similar texts while maintaining high\nsemantic alignment and output stability. The method also demonstrates high\ninterpretability in its structural design. It helps uncover the internal bias\nassociation mechanisms within language models. This provides a more transparent\nand reliable technical foundation for bias detection. The approach is suitable\nfor real-world applications where high trustworthiness of generated content is\nrequired.", "AI": {"tldr": "This paper proposes a method for detecting implicit biases in large language models using nested semantic representation and contextual contrast, validated on the StereoSet dataset.", "motivation": "To address the issue of implicit stereotypes that arise in large language models' outputs.", "method": "The method combines nested semantic representation with a contextual contrast mechanism and uses attention weight perturbation to analyze model sensitivity to social attributes.", "result": "The proposed method shows strong detection performance across various stereotype dimensions, accurately identifying bias differences while maintaining semantic consistency and output stability.", "conclusion": "The method provides a transparent and reliable foundation for bias detection within language models, suitable for applications requiring trust in generated content.", "key_contributions": ["Introduces an interpretable bias detection method for language models", "Validates the method using a comprehensive dataset (StereoSet)", "Demonstrates high interpretability and strong detection performance across multiple bias dimensions."], "limitations": "", "keywords": ["bias detection", "large language models", "interpretability", "stereotypes", "semantic representation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.06163", "pdf": "https://arxiv.org/pdf/2508.06163.pdf", "abs": "https://arxiv.org/abs/2508.06163", "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging", "authors": ["Yingfeng Luo", "Dingyang Lin", "Junxin Wang", "Ziqiang Xu", "Kaiyan Chang", "Tong Zheng", "Bei Li", "Anxiang Ma", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Model merging has emerged as a compelling data-free paradigm for multi-task\nlearning, enabling the fusion of multiple fine-tuned models into a single,\npowerful entity. A key technique in merging methods is sparsification, which\nprunes redundant parameters from task vectors to mitigate interference.\nHowever, prevailing approaches employ a ``one-size-fits-all'' strategy,\napplying a uniform sparsity ratio that overlooks the inherent structural and\nstatistical heterogeneity of model parameters. This often leads to a suboptimal\ntrade-off, where critical parameters are inadvertently pruned while less useful\nones are retained. To address this limitation, we introduce \\textbf{TADrop}\n(\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive\nsparsification strategy that respects this heterogeneity. Instead of a global\nratio, TADrop assigns a tailored sparsity level to each parameter tensor based\non its distributional properties. The core intuition is that tensors with\ndenser, more redundant distributions can be pruned aggressively, while sparser,\nmore critical ones are preserved. As a simple and plug-and-play module, we\nvalidate TADrop by integrating it with foundational, classic, and SOTA merging\nmethods. Extensive experiments across diverse tasks (vision, language, and\nmultimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and\nsignificantly boosts their performance. For instance, when enhancing a leading\nmerging method, it achieves an average performance gain of 2.0\\% across 8\nViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter\ninterference by tailoring sparsification to the model's structure, offering a\nnew baseline for high-performance model merging.", "AI": {"tldr": "Introducing TADrop, an adaptive sparsification strategy for model merging that optimizes parameter pruning based on the distributional properties of tensors, enhancing performance across various tasks.", "motivation": "To improve model merging by addressing the limitations of uniform sparsity ratios which do not account for the heterogeneity of model parameters.", "method": "TADrop applies tailored sparsity levels to parameter tensors based on their distributional properties, allowing for aggressive pruning of redundant tensors while preserving critical ones.", "result": "TADrop consistently enhances performance in diverse tasks and models, achieving an average performance gain of 2.0% across 8 ViT-B/32 tasks when integrated with leading merging methods.", "conclusion": "TADrop offers a new baseline for effective model merging by adapting sparsification strategies to the inherent structure of the model parameters.", "key_contributions": ["Introduction of TADrop for adaptive sparsification in model merging.", "Demonstration of performance improvements across multiple vision and language tasks.", "Establishment of a new baseline for high-performance model merging strategies."], "limitations": "", "keywords": ["model merging", "sparsification", "multi-task learning", "adaptive pruning", "machine learning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.06165", "pdf": "https://arxiv.org/pdf/2508.06165.pdf", "abs": "https://arxiv.org/abs/2508.06165", "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning", "authors": ["Weitao Li", "Boran Xiang", "Xiaolong Wang", "Zhinan Gou", "Weizhi Ma", "Yang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2.", "AI": {"tldr": "This paper presents UR2, a unified framework that integrates Retrieval-Augmented Generation and Reinforcement Learning to enhance reasoning capabilities across various tasks.", "motivation": "To address the isolated development of RAG and RLVR and improve their applicability to broader domains by unifying them into a single framework.", "method": "UR2 employs a difficulty-aware curriculum training that invokes retrieval for challenging problems and utilizes a hybrid knowledge access strategy combining offline corpora and LLM-generated summaries.", "result": "UR2 demonstrates significant performance improvements over existing RAG and RL methods in multiple tasks, achieving results comparable to top models like GPT-4.", "conclusion": "The integration of retrieval and reasoning through UR2 enhances task adaptability and generalization, making it a robust framework for diverse applications.", "key_contributions": ["Unified framework for RAG and RLVR.", "Difficulty-aware curriculum training for selective retrieval.", "Hybrid knowledge access strategy that combines offline data with generative summaries."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Reinforcement Learning", "Unified framework", "Human-Computer Interaction", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.06167", "pdf": "https://arxiv.org/pdf/2508.06167.pdf", "abs": "https://arxiv.org/abs/2508.06167", "title": "Pragmatics beyond humans: meaning, communication, and LLMs", "authors": ["Vt Gvodiak"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI.", "AI": {"tldr": "This paper redefines pragmatics as a dynamic interface in the context of large language models (LLMs), calling for a methodological reconsideration of human-machine communication.", "motivation": "To reconcile traditional pragmatic theories with the realities of communicating with LLMs, which challenge established meanings and user interactions.", "method": "The paper critiques traditional semiotic frameworks and proposes a Human-Machine Communication (HMC) framework, analyzing the compatibility of probabilistic pragmatics with LLMs.", "result": "It finds that current pragmatics based on human-specific assumptions do not adequately account for the predictive nature of LLMs and introduces context frustration to explore users' experiences.", "conclusion": "Pragmatic theory requires reevaluation to better facilitate interaction with generative AI systems, acknowledging the complexities of human-machine communication.", "key_contributions": ["Proposes the Human-Machine Communication framework as an alternative to traditional pragmatics.", "Introduces the concept of context frustration related to LLM interaction.", "Critiques reliance on Gricean-inspired pragmatics in the context of predictive systems."], "limitations": "The framework may not account for all communicative contexts and could be limited in its applicability across different AI models.", "keywords": ["pragmatics", "large language models", "Human-Machine Communication", "context frustration", "probabilistic pragmatics"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2412.00207", "pdf": "https://arxiv.org/pdf/2412.00207.pdf", "abs": "https://arxiv.org/abs/2412.00207", "title": "Can LLM \"Self-report\"?: Evaluating the Validity of Self-report Scales in Measuring Personality Design in LLM-based Chatbots", "authors": ["Huiqi Zou", "Pengda Wang", "Zihan Yan", "Tianjun Sun", "Ziang Xiao"], "categories": ["cs.HC"], "comment": "Accepted by COLM 2025", "summary": "A chatbot's personality design is key to interaction quality. As chatbots\nevolved from rule-based systems to those powered by large language models\n(LLMs), evaluating the effectiveness of their personality design has become\nincreasingly complex, particularly due to the open-ended nature of\ninteractions. A recent and widely adopted method for assessing the personality\ndesign of LLM-based chatbots is the use of self-report questionnaires. These\nquestionnaires, often borrowed from established human personality inventories,\nask the chatbot to rate itself on various personality traits. Can LLM-based\nchatbots meaningfully \"self-report\" their personality? We created 500 chatbots\nwith distinct personality designs and evaluated the validity of their\nself-report personality scores by examining human perceptions formed during\ninteractions with these chatbots. Our findings indicate that the chatbot's\nanswers on human personality scales exhibit weak correlations with both\nhuman-perceived personality traits and the overall interaction quality. These\nfindings raise concerns about both the criterion validity and the predictive\nvalidity of self-report methods in this context. Further analysis revealed the\nrole of task context and interaction in the chatbot's personality design\nassessment. We further discuss design implications for creating more\ncontextualized and interactive evaluation.", "AI": {"tldr": "This study examines the effectiveness of self-report questionnaires for evaluating the personality design of LLM-based chatbots, revealing weak correlations with human perceptions and interaction quality.", "motivation": "To understand if LLM-based chatbots can meaningfully self-report their personality traits and assess the validity of self-report methods.", "method": "Created 500 chatbots with different personality designs and analyzed their self-reported personality scores against human perceptions during interactions.", "result": "The self-reported personality scores showed weak correlations with human-perceived personality traits and overall interaction quality.", "conclusion": "There are significant concerns regarding the validity of self-report methods for chatbot personality assessment, highlighting the need for more contextualized evaluations.", "key_contributions": ["Evaluation of personality design in LLM-based chatbots", "Investigated the validity of self-report questionnaires for personality assessment", "Highlighted implications for more contextualized and interactive evaluation methods"], "limitations": "Focus on self-reporting may overlook other relevant aspects of chatbot interaction.", "keywords": ["chatbot personality", "LLM", "self-report", "interaction quality", "human perceptions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.06178", "pdf": "https://arxiv.org/pdf/2508.06178.pdf", "abs": "https://arxiv.org/abs/2508.06178", "title": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime", "authors": ["Hugo Abonizio", "Thales Almeida", "Roberto Lotufo", "Rodrigo Nogueira"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods.", "AI": {"tldr": "This paper investigates methods for injecting small amounts of unstructured information into large language models (LLMs) and examines the impact on knowledge acquisition and catastrophic forgetting.", "motivation": "The challenge of effectively updating LLMs with limited amounts of text data despite their dependency on vast corpora is a key motivation for this research.", "method": "The authors utilized a dataset of recent news, ensuring no overlap with the model's pre-training data, to probe knowledge acquisition through question-answer pairs and explored various augmentation algorithms for generating synthetic data.", "result": "The findings show that merely continuing pre-training on limited data yields modest improvements, while diverse textual variations significantly enhance learning new facts and reveal the effects of catastrophic forgetting.", "conclusion": "The research demonstrates that self-improvement through synthetic data generation by models themselves is possible and highlights the sensitivity of RAG-based methods compared to parametric approaches.", "key_contributions": ["Investigation of knowledge injection methods into LLMs using limited data", "Analysis of catastrophic forgetting in small-data regimes", "Demonstration of LLMs generating effective synthetic training data for self-improvement"], "limitations": "The study focuses on a specific methodology and dataset, which may limit the generalizability of the results across different domains or task types.", "keywords": ["Large Language Models", "Knowledge Injection", "Synthetic Data", "Catastrophic Forgetting", "Retrieval-Augmented Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.16321", "pdf": "https://arxiv.org/pdf/2412.16321.pdf", "abs": "https://arxiv.org/abs/2412.16321", "title": "XR for All: Understanding Developers' Perspectives on Accessibility Integration in Extended Reality", "authors": ["Daniel Killough", "Tiger F. Ji", "Kexin Zhang", "Yaxin Hu", "Yu Huang", "Ruofei Du", "Yuhang Zhao"], "categories": ["cs.HC"], "comment": "20 pages, 1 figure, 3 tables, LaTeX", "summary": "As immersive technologies enable unique, multimodal interaction methods,\ndevelopers must also use tailored methods to support user accessibility,\ndistinct from traditional software practices. We interviewed 25 industry\nextended reality (XR) developers, including freelancers, startups, midsize, and\nbig tech companies about their motivations, techniques, barriers, and attitudes\ntowards incorporating accessibility features in their XR apps. Our study\nrevealed a variety of challenges, including conflicting priorities between\napplication and platform developers regarding accessibility infrastructure;\nrapid development culture hindering accessible development; and the lack of\naccessible interaction design considerations at the ideation, design, and early\nprototyping stages. As a comprehensive set of XR accessibility guidelines has\nyet to be established, we also compiled and evaluated a set of accessibility\nguidelines for 3D virtual worlds and addressed their limitations when applied\nto XR. Finally, we inform the creation of effective support methods for\nindustry developers.", "AI": {"tldr": "This paper explores the challenges and considerations of incorporating accessibility features in XR applications from the perspective of industry developers.", "motivation": "To address the gap in accessibility practices for extended reality (XR) applications, given the unique interaction methods used in immersive technologies.", "method": "Interviews with 25 industry XR developers from various company sizes to gather insights on motivations, techniques, and barriers related to accessibility features in XR.", "result": "Identified challenges include conflicting priorities between developers, rapid development culture hindering accessibility, and lack of accessible design considerations early in the development process.", "conclusion": "The study highlights the need for established XR accessibility guidelines and informs the creation of effective support methods for developers incorporating accessibly into their XR applications.", "key_contributions": ["Insights into developer attitudes and barriers towards XR accessibility", "Compilation and evaluation of accessibility guidelines for 3D virtual worlds in XR", "Recommendations for creating effective support methods for developers"], "limitations": "A comprehensive set of accessibility guidelines for XR is still lacking, and the current guidelines may not be fully applicable to XR contexts.", "keywords": ["Accessibility", "Extended Reality", "User Experience", "Inclusive Design", "Immersive Technologies"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2508.06186", "pdf": "https://arxiv.org/pdf/2508.06186.pdf", "abs": "https://arxiv.org/abs/2508.06186", "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration", "authors": ["Ali Sarabadani", "Maryam Abdollahi Shamami", "Hamidreza Sadeghsalehi", "Borhan Asadi", "Saba Hesaraki"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input.", "AI": {"tldr": "The DKG-LLM framework combines a dynamic knowledge graph with LLM for improved medical diagnosis and treatment recommendations, achieving high accuracy on real-world datasets.", "motivation": "To enhance medical diagnosis and personalized treatment recommendations using LLMs and knowledge graphs.", "method": "Integrating a dynamic knowledge graph with a large language model using the Adaptive Semantic Fusion Algorithm to process heterogeneous medical data and maintain scalability.", "result": "Achieved a diagnostic accuracy of 84.19% and a treatment recommendation accuracy of 89.63% on real-world datasets like MIMIC-III and PubMed.", "conclusion": "DKG-LLM is a reliable tool that effectively handles complex medical data and improves diagnosis and treatment recommendation accuracy.", "key_contributions": ["Introduction of the DKG-LLM framework for medical applications", "Use of Adaptive Semantic Fusion Algorithm for knowledge graph creation", "Demonstrated high accuracy in diagnosing and recommending treatments based on diverse data"], "limitations": "", "keywords": ["Large Language Models", "Knowledge Graphs", "Medical Diagnosis", "Personalized Treatment", "Adaptive Semantic Fusion"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.06194", "pdf": "https://arxiv.org/pdf/2508.06194.pdf", "abs": "https://arxiv.org/abs/2508.06194", "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation", "authors": ["Lai Jiang", "Yuekang Li", "Xiaohan Zhang", "Youtao Ding", "Li Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage.", "AI": {"tldr": "Introducing SceneJailEval, a scenario-adaptive multi-dimensional framework for jailbreak evaluation that overcomes limitations of existing methods.", "motivation": "The need for precise jailbreak evaluation methods in LLM red teaming and research, with a focus on quantifying harm intensity rather than binary classifications.", "method": "SceneJailEval employs a multi-dimensional framework designed to adapt to various scenarios, overcoming the uniform criteria limitation of previous approaches.", "result": "SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on the full-scenario dataset and 0.995 on JBB, which is an improvement over existing methods.", "conclusion": "SceneJailEval demonstrates significant advantages in precision and applicability across diverse contexts, filling a critical gap in current evaluation frameworks.", "key_contributions": ["Groundbreaking scenario-adaptive multi-dimensional jailbreak evaluation framework", "A comprehensive 14-scenario dataset for diverse jailbreak variants", "State-of-the-art performance metrics surpassing existing evaluation methods"], "limitations": "", "keywords": ["jailbreak evaluation", "LLM red teaming", "multi-dimensional framework"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.13908", "pdf": "https://arxiv.org/pdf/2504.13908.pdf", "abs": "https://arxiv.org/abs/2504.13908", "title": "AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience", "authors": ["Soubhik Barari", "Jarret Angbazo", "Natalie Wang", "Leah M. Christian", "Elizabeth Dean", "Zoe Slowinski", "Brandon Sepulvado"], "categories": ["cs.HC", "cs.AI", "stat.AP"], "comment": null, "summary": "Standardized surveys scale efficiently but sacrifice depth, while\nconversational interviews improve response quality at the cost of scalability\nand consistency. This study bridges the gap between these methods by\nintroducing a framework for AI-assisted conversational interviewing. To\nevaluate this framework, we conducted a web survey experiment where 1,800\nparticipants were randomly assigned to AI 'chatbots' which use large language\nmodels (LLMs) to dynamically probe respondents for elaboration and\ninteractively code open-ended responses to fixed questions developed by human\nresearchers. We assessed the AI chatbot's performance in terms of coding\naccuracy, response quality, and respondent experience. Our findings reveal that\nAI chatbots perform moderately well in live coding even without survey-specific\nfine-tuning, despite slightly inflated false positive errors due to respondent\nacquiescence bias. Open-ended responses were more detailed and informative, but\nthis came at a slight cost to respondent experience. Our findings highlight the\nfeasibility of using AI methods such as chatbots enhanced by LLMs to enhance\nopen-ended data collection in web surveys.", "AI": {"tldr": "This paper presents a framework for AI-assisted conversational interviewing using LLMs to enhance the depth and quality of responses in web surveys.", "motivation": "To address the trade-off between the scalability of standardized surveys and the depth of conversational interviews.", "method": "The study conducted a web survey experiment with 1,800 participants using AI chatbots that leverage LLMs for dynamic probing and coding of open-ended responses.", "result": "AI chatbots achieved moderate performance in coding accuracy and improved the detail of open-ended responses, despite some biases and mixed respondent experiences.", "conclusion": "The study demonstrates the potential of LLM-powered AI chatbots to enhance data collection methods in web surveys, although improvements in respondent experience are necessary.", "key_contributions": ["Introduction of an AI-assisted framework for conversational interviewing", "Demonstration of LLMs in live coding of survey responses", "Evaluation of chatbot efficacy in enhancing open-ended data collection"], "limitations": "Respondent experience was slightly negatively impacted due to depth of responses, and coding accuracy faced issues with false positives due to acquiescence bias.", "keywords": ["AI-assisted interviewing", "Conversational AI", "Language models", "Web surveys", "Open-ended responses"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.06196", "pdf": "https://arxiv.org/pdf/2508.06196.pdf", "abs": "https://arxiv.org/abs/2508.06196", "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations", "authors": ["Nizi Nazar", "Ehsaneddin Asgari"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment.", "AI": {"tldr": "This paper introduces a four-layer taxonomy of Emotional Intelligence (EI) for large language models (LLMs) and presents a benchmark, EICAP-Bench, to evaluate EI capabilities in open-source LLMs, identifying the strengths and weaknesses of various models in emotional reasoning.", "motivation": "To explore and improve emotional intelligence in large language models (LLMs) due to its critical yet underdeveloped role in human-aligned AI.", "method": "Developed a four-layer taxonomy of EI for LLMs and created the EICAP-Bench benchmark to evaluate various LLMs' capabilities in emotional reasoning. Conducted statistical analysis on models fine-tuned with specific datasets.", "result": "Identified that only the Appraisal layer of EI showed significant improvement after fine-tuning models like Qwen2.5 using targeted instruction datasets across linguistic contexts.", "conclusion": "Current pretraining methods are inadequate for developing deep emotional reasoning in LLMs, indicating a need for specialized data and targeted modeling approaches to enhance emotional intelligence capabilities.", "key_contributions": ["Unified taxonomy of EI for LLMs", "Creation of the EICAP-Bench benchmark", "Evaluation of multiple LLMs to assess EI capabilities"], "limitations": "Only one EI layer showed significant improvement; broader improvement across all layers was not observed.", "keywords": ["Emotional Intelligence", "large language models", "benchmark", "fine-tuning", "human-aligned AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.06204", "pdf": "https://arxiv.org/pdf/2508.06204.pdf", "abs": "https://arxiv.org/abs/2508.06204", "title": "Classification is a RAG problem: A case study on hate speech detection", "authors": ["Richard Willats", "Josh Pennington", "Aravind Mohan", "Bertie Vidgen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems.", "AI": {"tldr": "This paper presents a Contextual Policy Engine (CPE) that utilizes Retrieval-Augmented Generation (RAG) for flexible content moderation, allowing dynamic policy updates without retraining.", "motivation": "To enhance content moderation systems' ability to adapt to evolving policies without incurring the costs of retraining.", "method": "The paper introduces RAG for classification tasks, specifically in hate speech detection, shifting the focus from static classification to contextually evaluating content against dynamic policy guidelines.", "result": "The CPE demonstrates classification accuracy comparable to leading systems, provides explainability through policy segment retrieval, and allows for dynamic updates to policies without retraining.", "conclusion": "RAG can make classification processes more adaptable and transparent, improving content moderation strategies and other classification problems.", "key_contributions": ["Introduction of Contextual Policy Engine (CPE) for content moderation", "Demonstration of dynamic policy updates without retraining", "Validation of inherent explainability via policy retrieval"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "content moderation", "hate speech detection", "policy adaptability", "explainability"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.06220", "pdf": "https://arxiv.org/pdf/2508.06220.pdf", "abs": "https://arxiv.org/abs/2508.06220", "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?", "authors": ["Keummin Ka", "Junhyeong Park", "Jahyun Jeon", "Youngjae Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 9 figures", "summary": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.", "AI": {"tldr": "Introducing InfoCausalQA, a benchmark for evaluating causal reasoning in Vision-Language Models (VLMs) using infographics that combine visual and textual data.", "motivation": "To address the gap in causal inference capabilities of multimodal AI systems, particularly in Vision-Language Models.", "method": "The benchmark consists of two tasks: Task 1 evaluates quantitative causal reasoning, while Task 2 assesses semantic causal reasoning involving various causal relations. It includes 494 infographic-text pairs generated and curated for high-quality QA pairs that require real visual understanding.", "result": "Current VLMs underperform in both computational and semantic causal reasoning compared to human performance, indicating substantial limitations in utilizing infographic information for causal inference.", "conclusion": "There is a pressing need to enhance the causal reasoning abilities of multimodal AI systems, as demonstrated by the substantial gap identified in this evaluation.", "key_contributions": ["Introduction of InfoCausalQA benchmark for causal reasoning evaluation in VLMs", "Focus on both quantitative and semantic aspects of causal inference in multimodal contexts", "Highlighting the deficiencies of existing VLMs in reasoning with infographic-based information"], "limitations": "The benchmark only evaluates VLM performance on the selected infographics, which may limit the generalizability of the findings to broader multimodal contexts.", "keywords": ["Vision-Language Models", "Causal Reasoning", "Infographics", "Benchmark", "Multimodal AI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.06277", "pdf": "https://arxiv.org/pdf/2508.06277.pdf", "abs": "https://arxiv.org/abs/2508.06277", "title": "Large Language Model Data Generation for Enhanced Intent Recognition in German Speech", "authors": ["Theresa Pekarek Rosin", "Burak Can Kaplan", "Stefan Wermter"], "categories": ["cs.CL", "cs.LG", "cs.SD"], "comment": "11 pages, 3 figures, accepted at KONVENS 2025", "summary": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility.", "AI": {"tldr": "This paper presents a novel approach for intent recognition from speech by elderly German speakers, employing an adapted Whisper ASR model and Transformer-based LLMs. It demonstrates significant improvements in classification performance using synthetic data.", "motivation": "Address the limitations of existing intent recognition systems that focus on short commands and predominantly English language, especially for elderly German speakers.", "method": "Combines an adapted Whisper ASR model fine-tuned on elderly German speech with Transformer-based language models trained on synthetic text datasets generated by LeoLM, Llama3, and ChatGPT.", "result": "Synthetic LLM-generated data improves classification performance and robustness to different speaking styles and unseen vocabulary, with LeoLM outperforming ChatGPT in dataset quality for German intent recognition.", "conclusion": "Generative AI can effectively bridge data gaps in low-resource domains, and detailed documentation of the data generation and training process is provided for transparency.", "key_contributions": ["Development of a robust intent recognition approach for elderly German speech", "Demonstration of the benefits of synthetic data from LLMs", "Comparative analysis showing the effectiveness of LeoLM over larger models for this task."], "limitations": "", "keywords": ["intent recognition", "elderly speakers", "German speech", "synthetic data", "large language models"], "importance_score": 7, "read_time_minutes": 11}}
{"id": "2508.06309", "pdf": "https://arxiv.org/pdf/2508.06309.pdf", "abs": "https://arxiv.org/abs/2508.06309", "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC", "authors": ["Ruichong Zhang"], "categories": ["cs.CL", "math.PR"], "comment": null, "summary": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible.", "AI": {"tldr": "This paper proposes a novel method called Matrix-Driven Instant Review (MDIR) to detect plagiarism in large language models (LLMs) by leveraging matrix analysis and Large Deviation Theory.", "motivation": "Concerns about intellectual property (IP) in large language models have grown, particularly surrounding plagiarism and authorship issues that can harm original developers.", "method": "MDIR uses matrix analysis and Large Deviation Theory to achieve accurate reconstruction of weight relationships and rigorous $p$-value estimation, focusing on weight similarity without requiring full model inference.", "result": "Experiments show that MDIR reliably detects plagiarism even after significant model transformations, such as random permutations and continual pretraining, with all detections performed efficiently on a single PC.", "conclusion": "MDIR provides a more effective and accessible solution for detecting LLM plagiarism, addressing limitations of existing methods.", "key_contributions": ["Introduction of the Matrix-Driven Instant Review (MDIR) method.", "Improved accuracy in reconstructing weight relationships compared to existing methods.", "Ability to compute rigorous statistical significance measures for weight similarities."], "limitations": "", "keywords": ["intellectual property", "large language models", "plagiarism detection", "matrix analysis", "statistical significance"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.06345", "pdf": "https://arxiv.org/pdf/2508.06345.pdf", "abs": "https://arxiv.org/abs/2508.06345", "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering", "authors": ["Yanbin Wei", "Jiangyue Yan", "Chun Kang", "Yang Chen", "Hua Liu", "James T. Kwok", "Yu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.GR", "cs.LG"], "comment": null, "summary": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy", "AI": {"tldr": "The paper introduces DynamicTRF, a framework that tailors Topology Representation Forms (TRFs) for zero-shot graph question-answering (QA) to improve accuracy and conciseness.", "motivation": "Current methods using a single type of TRF for graph QA lead to suboptimal responses due to neglecting model/task-specific preferences.", "method": "The authors analyze existing TRFs and propose a set of tailored TRFs ($F_{ZS}$). They then develop a TRF Preference dataset to rank TRFs by Graph Response Efficiency (GRE) and train a TRF router for adaptive assignment during inference.", "result": "DynamicTRF significantly improves the accuracy of zero-shot graph QA across various tasks based on extensive experiments.", "conclusion": "The proposed framework enhances graph QA performance of Large Multimodal Models by employing tailored TRFs and a novel router mechanism.", "key_contributions": ["Introduction of tailored TRFs for zero-shot graph QA", "Development of a new metric, Graph Response Efficiency (GRE)", "Creation of the DynamicTRF framework that improves graph QA accuracy"], "limitations": "", "keywords": ["Multimodal Models", "Graph Question Answering", "DynamicTRF"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.06360", "pdf": "https://arxiv.org/pdf/2508.06360.pdf", "abs": "https://arxiv.org/abs/2508.06360", "title": "Cyberbullying Detection via Aggression-Enhanced Prompting", "authors": ["Aisha Saeid", "Anu Sabu", "Girish A. Koushik", "Ferrante Neri", "Diptesh Kanojia"], "categories": ["cs.CL"], "comment": "Accepted to RANLP 2025", "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks.", "AI": {"tldr": "The paper explores enhancing large language models' performance in detecting cyberbullying by integrating aggression detection as an auxiliary task.", "motivation": "To address the critical challenge of cyberbullying detection on social media, which is difficult due to its subtlety and variability.", "method": "The study conducted experiments on various datasets employing instruction-tuned LLMs, evaluating strategies including zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL), ultimately focusing on an enriched prompt pipeline approach.", "result": "Preliminary results show that the enriched prompt pipeline outperforms standard LoRA fine-tuning, suggesting the effectiveness of using aggression-informed context in enhancing cyberbullying detection.", "conclusion": "The findings support the idea that auxiliary tasks, like aggression detection, can significantly enhance the generalization and performance of LLMs in sensitive applications.", "key_contributions": ["Introduced an enriched prompt pipeline approach for cyberbullying detection.", "Showed the value of integrating aggression detection as an auxiliary task.", "Demonstrated superior performance of the enriched approach over standard fine-tuning methods."], "limitations": "", "keywords": ["cyberbullying detection", "large language models", "aggression detection", "social media", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.06374", "pdf": "https://arxiv.org/pdf/2508.06374.pdf", "abs": "https://arxiv.org/abs/2508.06374", "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions", "authors": ["Anubhav Jangra", "Bahareh Sarrafzadeh", "Adrian de Wynter", "Silviu Cucerzan", "Sujay Kumar Jauhar"], "categories": ["cs.CL"], "comment": null, "summary": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation.", "AI": {"tldr": "This paper explores the evaluation of low-resource author style personalized text generation, questioning traditional metrics like BLEU and ROUGE, and proposes an ensemble of diverse evaluation metrics for better assessment.", "motivation": "To address the gap in effective evaluation methods for low-resource author style personalized text generation and the limitations of existing metrics.", "method": "The authors develop a style discrimination benchmark encompassing eight writing tasks and evaluate various metrics and their ensembles across domain discrimination, authorship attribution, and LLM discrimination.", "result": "The study finds that an ensemble of diverse evaluation metrics offers a more holistic evaluation approach compared to traditional metrics like BLEU and ROUGE.", "conclusion": "The paper concludes that adopting an ensemble of evaluation metrics is crucial for effectively assessing style personalized text generation.", "key_contributions": ["Introduction of a style discrimination benchmark with eight writing tasks.", "Critical analysis of traditional evaluation metrics for style personalized text generation.", "Proposal of a diverse ensemble of evaluation metrics for better assessment."], "limitations": "The paper mainly focuses on low-resource settings, which may not encompass all aspects of text generation evaluation.", "keywords": ["style personalized text generation", "evaluation metrics", "style embeddings", "LLM-as-judge", "author discrimination"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.06388", "pdf": "https://arxiv.org/pdf/2508.06388.pdf", "abs": "https://arxiv.org/abs/2508.06388", "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing", "authors": ["Lanlan Qiu", "Xiao Pu", "Yeqi Feng", "Tianxing He"], "categories": ["cs.CL"], "comment": "21 pages, 17 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime.", "AI": {"tldr": "This paper presents ChatAnime, the first dataset designed for evaluating Emotionally Supportive Role-Playing (ESRP) interactions using large language models (LLMs) with anime characters.", "motivation": "To fill the gap in research regarding emotionally supportive interactions with LLMs in the context of role-playing, specifically using anime characters known for their distinct personalities.", "method": "The authors created the ChatAnime dataset by selecting 20 anime characters and designing 60 emotion-centric questions. They collected dialogue data through interactions between 40 anime enthusiasts and 10 LLMs, structured with a comprehensive evaluation system.", "result": "Experimental results showed top-performing LLMs outperformed human fans in both role-playing and emotional support, but humans excelled in providing diverse responses.", "conclusion": "The study provides valuable datasets for future research on optimizing LLMs for emotionally supportive role-playing, highlighting areas of strength and weakness between humans and LLMs.", "key_contributions": ["Introduction of the first ESRP dataset (ChatAnime) for evaluating LLMs with anime characters.", "Development of a structured evaluation system featuring fine-grained metrics.", "Insights into LLMs' capabilities versus human fans in emotional support and role-playing."], "limitations": "The dataset is focused on specific anime characters, which may limit generalizability to other contexts or character types.", "keywords": ["Emotionally Supportive Role-Playing", "Large Language Models", "Anime Characters", "Human-Computer Interaction", "Dataset"], "importance_score": 9, "read_time_minutes": 21}}
{"id": "2508.06418", "pdf": "https://arxiv.org/pdf/2508.06418.pdf", "abs": "https://arxiv.org/abs/2508.06418", "title": "Quantifying Conversation Drift in MCP via Latent Polytope", "authors": ["Haoran Shi", "Hongwei Yao", "Shuo Shao", "Shaopeng Jiao", "Ziqi Peng", "Zhan Qin", "Cong Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy.", "AI": {"tldr": "The paper proposes SecMCP, a secure framework for detecting and quantifying conversation drift in large language models (LLMs) by addressing critical security and privacy risks associated with dynamic tool integration.", "motivation": "To mitigate security and privacy risks posed by the Model Context Protocol in LLMs, such as adversarial content leading to conversation hijacking and misinformation.", "method": "SecMCP models LLM activation vectors within a latent polytope space to identify anomalous shifts in conversational dynamics, allowing for the detection of hijacking and misleading information.", "result": "SecMCP demonstrated robust detection capabilities with AUROC scores exceeding 0.915 on benchmark datasets while maintaining usability.", "conclusion": "SecMCP addresses significant gaps in existing defenses against LLM security threats by providing a systematic categorization, a new methodology for quantifying conversation drift, and empirical validation of its effectiveness.", "key_contributions": ["Systematic categorization of MCP security threats", "Novel latent polytope-based methodology for quantifying conversation drift", "Empirical validation of SecMCP's efficacy"], "limitations": "", "keywords": ["large language models", "security", "conversation drift", "anomaly detection", "privacy"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.06433", "pdf": "https://arxiv.org/pdf/2508.06433.pdf", "abs": "https://arxiv.org/abs/2508.06433", "title": "Memp: Exploring Agent Procedural Memory", "authors": ["Runnan Fang", "Yuan Liang", "Xiaobin Wang", "Jialong Wu", "Shuofei Qiao", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.", "AI": {"tldr": "This paper presents Memp, a learned procedural memory system for LLM-based agents that allows continuous updating and improves task efficiency.", "motivation": "To address the limitations of brittle procedural memory in LLM agents by creating a system that supports lifelong learning and adaptability.", "method": "Memp distills past agent trajectories into detailed instructions and script-like abstractions, and incorporates strategies for Building, Retrieving, and Updating procedural memory.", "result": "Empirical evaluations show that refined procedural memory leads to increased success rates and efficiency on tasks such as TravelPlanner and ALFWorld.", "conclusion": "The study demonstrates that procedural memory from a stronger model enhances performance even when migrated to a weaker model, showing the value of learned memory repositories.", "key_contributions": ["Development of a learnable, updatable procedural memory system for LLM agents", "Empirical validation of performance gains through memory refinement", "Demonstration of performance retention across models with different strengths."], "limitations": "", "keywords": ["Large Language Models", "Procedural Memory", "Lifelong Learning", "Artificial Intelligence", "Agent Performance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.06435", "pdf": "https://arxiv.org/pdf/2508.06435.pdf", "abs": "https://arxiv.org/abs/2508.06435", "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages", "authors": ["Andrea Nasuto", "Stefano Maria Iacus", "Francisco Rowe", "Devika Jain"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.", "AI": {"tldr": "This paper investigates the capability of fine-tuned L LaMA models to perform cross-lingual topic detection in immigration-related discourse, showing that minimal language-specific training can yield significant results across unseen languages.", "motivation": "To explore how fine-tuning LLMs on limited languages can influence their performance in classifying cross-lingual, culturally-specific social media content.", "method": "Fine-tuning lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual datasets for classifying immigration-related tweets in 13 languages, evaluating the ability to overcome pre-training biases with minimal fine-tuning.", "result": "Fine-tuned LLMs can reliably classify immigration-related content in unseen languages, with notable performance improvements by using multilingual fine-tuning and minimal exposure to other languages during training.", "conclusion": "The study concludes that limited language coverage allows for effective topic-level generalization in LLMs and that biases toward dominant languages can be addressed with minor fine-tuning efforts.", "key_contributions": ["Demonstrated cross-lingual capabilities of LLMs with minimal fine-tuning.", "Provided open-source models that are efficient and cost-effective for research.", "Challenged the belief that extensive multilingual training is necessary for effective classification tasks."], "limitations": "The study primarily focuses on immigration-related tweets, which may limit the generalizability of findings to other domains.", "keywords": ["Large Language Models", "Cross-lingual topic detection", "Immigration tweets", "Fine-tuning", "Open-source LLMs"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.06445", "pdf": "https://arxiv.org/pdf/2508.06445.pdf", "abs": "https://arxiv.org/abs/2508.06445", "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking", "authors": ["Abolfazl Ansari", "Delvin Ce Zhang", "Nafis Irtiza Tripto", "Dongwon Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in 18th International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, and to be published in the Springer LNCS series", "summary": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media.", "AI": {"tldr": "The study analyzes the increasing use of Generative AI in journalism, focusing on AI-generated content in over 40,000 news articles.", "motivation": "To address concerns regarding journalistic integrity and authorship due to the rise of Generative AI, especially LLMs.", "method": "The study examines AI-generated content across over 40,000 news articles using three advanced AI-text detectors (Binoculars, Fast-Detect GPT, GPTZero).", "result": "A substantial increase of GenAI use in local and college news articles, with LLMs primarily used in introductions. Linguistic analysis indicates improved word richness and readability, but decreased formality.", "conclusion": "While GenAI enhances certain textual qualities, it raises concerns about writing uniformity and authentic journalistic voices in media.", "key_contributions": ["Comprehensive analysis of AI-generated content across diverse news media.", "Identification of specific patterns in LLM usage within news articles.", "Linguistic impact assessment of GenAI on writing styles in journalism."], "limitations": "", "keywords": ["Generative AI", "journalism", "large language models", "text detection", "linguistic analysis"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.06447", "pdf": "https://arxiv.org/pdf/2508.06447.pdf", "abs": "https://arxiv.org/abs/2508.06447", "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning", "authors": ["Lingkun Long", "Rubing Yang", "Yushi Huang", "Desheng Hui", "Ao Zhou", "Jianlei Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.", "AI": {"tldr": "SlimInfer offers a novel framework for accelerating inference in Large Language Models (LLMs) by pruning less critical prompt tokens during the forward pass.", "motivation": "Long-context inference in LLMs is constrained by high computational demands and existing methods that process all hidden states limit efficiency.", "method": "SlimInfer employs a dynamic fine-grained pruning mechanism that removes redundant tokens during the forward pass, utilizing an information diffusion phenomenon to maintain semantic integrity.", "result": "Achieves up to 2.53x speedup in time-to-first-token (TTFT) and 1.88x reduction in end-to-end latency for LLaMA3.1-8B-Instruct on a single RTX 4090.", "conclusion": "SlimInfer significantly improves efficiency in LLM inference without performance loss on LongBench, with code release planned upon acceptance.", "key_contributions": ["Introduction of SlimInfer for token pruning during inference", "Dynamic layer-wise pruning mechanism", "Significant speed and latency improvements for LLMs"], "limitations": "", "keywords": ["Large Language Models", "Inference Optimization", "Token Pruning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.06471", "pdf": "https://arxiv.org/pdf/2508.06471.pdf", "abs": "https://arxiv.org/abs/2508.06471", "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models", "authors": ["GLM-4. 5 Team", ":", "Aohan Zeng", "Xin Lv", "Qinkai Zheng", "Zhenyu Hou", "Bin Chen", "Chengxing Xie", "Cunxiang Wang", "Da Yin", "Hao Zeng", "Jiajie Zhang", "Kedong Wang", "Lucen Zhong", "Mingdao Liu", "Rui Lu", "Shulin Cao", "Xiaohan Zhang", "Xuancheng Huang", "Yao Wei", "Yean Cheng", "Yifan An", "Yilin Niu", "Yuanhao Wen", "Yushi Bai", "Zhengxiao Du", "Zihan Wang", "Zilin Zhu", "Bohan Zhang", "Bosi Wen", "Bowen Wu", "Bowen Xu", "Can Huang", "Casey Zhao", "Changpeng Cai", "Chao Yu", "Chen Li", "Chendi Ge", "Chenghua Huang", "Chenhui Zhang", "Chenxi Xu", "Chenzheng Zhu", "Chuang Li", "Congfeng Yin", "Daoyan Lin", "Dayong Yang", "Dazhi Jiang", "Ding Ai", "Erle Zhu", "Fei Wang", "Gengzheng Pan", "Guo Wang", "Hailong Sun", "Haitao Li", "Haiyang Li", "Haiyi Hu", "Hanyu Zhang", "Hao Peng", "Hao Tai", "Haoke Zhang", "Haoran Wang", "Haoyu Yang", "He Liu", "He Zhao", "Hongwei Liu", "Hongxi Yan", "Huan Liu", "Huilong Chen", "Ji Li", "Jiajing Zhao", "Jiamin Ren", "Jian Jiao", "Jiani Zhao", "Jianyang Yan", "Jiaqi Wang", "Jiayi Gui", "Jiayue Zhao", "Jie Liu", "Jijie Li", "Jing Li", "Jing Lu", "Jingsen Wang", "Jingwei Yuan", "Jingxuan Li", "Jingzhao Du", "Jinhua Du", "Jinxin Liu", "Junkai Zhi", "Junli Gao", "Ke Wang", "Lekang Yang", "Liang Xu", "Lin Fan", "Lindong Wu", "Lintao Ding", "Lu Wang", "Man Zhang", "Minghao Li", "Minghuan Xu", "Mingming Zhao", "Mingshu Zhai", "Pengfan Du", "Qian Dong", "Shangde Lei", "Shangqing Tu", "Shangtong Yang", "Shaoyou Lu", "Shijie Li", "Shuang Li", "Shuang-Li", "Shuxun Yang", "Sibo Yi", "Tianshu Yu", "Wei Tian", "Weihan Wang", "Wenbo Yu", "Weng Lam Tam", "Wenjie Liang", "Wentao Liu", "Xiao Wang", "Xiaohan Jia", "Xiaotao Gu", "Xiaoying Ling", "Xin Wang", "Xing Fan", "Xingru Pan", "Xinyuan Zhang", "Xinze Zhang", "Xiuqing Fu", "Xunkai Zhang", "Yabo Xu", "Yandong Wu", "Yida Lu", "Yidong Wang", "Yilin Zhou", "Yiming Pan", "Ying Zhang", "Yingli Wang", "Yingru Li", "Yinpei Su", "Yipeng Geng", "Yitong Zhu", "Yongkun Yang", "Yuhang Li", "Yuhao Wu", "Yujiang Li", "Yunan Liu", "Yunqing Wang", "Yuntao Li", "Yuxuan Zhang", "Zezhen Liu", "Zhen Yang", "Zhengda Zhou", "Zhongpei Qiao", "Zhuoer Feng", "Zhuorui Liu", "Zichen Zhang", "Zihan Wang", "Zijun Yao", "Zikang Wang", "Ziqiang Liu", "Ziwei Chai", "Zixuan Li", "Zuodong Zhao", "Wenguang Chen", "Jidong Zhai", "Bin Xu", "Minlie Huang", "Hongning Wang", "Juanzi Li", "Yuxiao Dong", "Jie Tang"], "categories": ["cs.CL"], "comment": null, "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.", "AI": {"tldr": "GLM-4.5 is a large language model with innovative MoE architecture and hybrid reasoning, showing superior performance across various AI benchmarks.", "motivation": "To advance research in reasoning and agentic AI systems while providing strong performance in coding and reasoning tasks.", "method": "The model was trained on 23 trillion tokens using multi-stage training with expert model iteration and reinforcement learning.", "result": "GLM-4.5 achieved 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified, ranking 3rd overall among evaluated models with fewer parameters than competitors.", "conclusion": "GLM-4.5 and its compact version aim to enhance research in agentic AI and reasoning applications.", "key_contributions": ["Introduction of MoE architecture with hybrid reasoning methods.", "Strong performance metrics with significantly fewer parameters.", "Release of open-source code and models for further research."], "limitations": "", "keywords": ["large language model", "Mixture-of-Experts", "reasoning", "agentic AI", "open-source"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.06475", "pdf": "https://arxiv.org/pdf/2508.06475.pdf", "abs": "https://arxiv.org/abs/2508.06475", "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning", "authors": ["Guimin Hu", "Daniel Hershcovich", "Hasti Seifi"], "categories": ["cs.CL"], "comment": null, "summary": "Haptic captioning is the task of generating natural language descriptions\nfrom haptic signals, such as vibrations, for use in virtual reality,\naccessibility, and rehabilitation applications. While previous multimodal\nresearch has focused primarily on vision and audio, haptic signals for the\nsense of touch remain underexplored. To address this gap, we formalize the\nhaptic captioning task and propose HapticLLaMA, a multimodal sensory language\nmodel that interprets vibration signals into descriptions in a given sensory,\nemotional, or associative category. We investigate two types of haptic\ntokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that\nconvert haptic signals into sequences of discrete units, enabling their\nintegration with the LLaMA model. HapticLLaMA is trained in two stages: (1)\nsupervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,\nand (2) fine-tuning via reinforcement learning from human feedback (RLHF). We\nassess HapticLLaMA's captioning performance using both automated n-gram metrics\nand human evaluation. HapticLLaMA demonstrates strong capability in\ninterpreting haptic vibration signals, achieving a METEOR score of 59.98 and a\nBLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated\ncaptions received human ratings above 3.5 on a 7-point scale, with RLHF\nyielding a 10% improvement in the overall rating distribution, indicating\nstronger alignment with human haptic perception. These findings highlight the\npotential of large language models to process and adapt to sensory data.", "AI": {"tldr": "HapticLLaMA is a multimodal language model that generates natural language descriptions from haptic signals for applications in accessibility and virtual reality.", "motivation": "Address the underexplored area of haptic signals in multimodal research, particularly for generating language descriptions from vibrations.", "method": "Developed HapticLLaMA, which uses two types of haptic tokenizers to convert vibration signals into discrete units for integration with the LLaMA model, trained through supervised fine-tuning and reinforcement learning from human feedback.", "result": "HapticLLaMA achieved a METEOR score of 59.98 and a BLEU-4 score of 32.06, with over 61% of generated captions rated above 3.5 by human evaluators, showing significant improvement with RLHF.", "conclusion": "The study demonstrates the capability of large language models to effectively interpret haptic signals, suggesting promising applications in VR and accessibility.", "key_contributions": ["Formalization of the haptic captioning task.", "Introduction of HapticLLaMA for interpreting haptic signals into language descriptions.", "Dual training methodology combining supervised fine-tuning and reinforcement learning."], "limitations": "", "keywords": ["haptic captioning", "multimodal sensory language model", "LLaMA", "reinforcement learning", "human feedback"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2508.06482", "pdf": "https://arxiv.org/pdf/2508.06482.pdf", "abs": "https://arxiv.org/abs/2508.06482", "title": "Post-training for Efficient Communication via Convention Formation", "authors": ["Yilun Hua", "Evan Wang", "Yoav Artzi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to COLM 2025", "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods.", "AI": {"tldr": "This paper introduces a targeted fine-tuning process to enhance convention formation in LLMs, demonstrating significant improvements in multi-turn interactions.", "motivation": "To improve LLMs' performance in forming ad-hoc conventions during multi-turn interactions, as current models lag behind human capabilities.", "method": "We developed a post-training fine-tuning process based on heuristically identified demonstrations of convention formation.", "result": "Our studies show significantly improved convention formation abilities in post-trained LLMs across two evaluation methods, including a cognitively-motivated interaction benchmark and a document-grounded reference completion task.", "conclusion": "Post-training fine-tuning effectively enhances LLMs' ability to form conventions, bridging the gap between human and machine interaction capabilities.", "key_contributions": ["Introduction of a post-training process for fine-tuning LLMs on convention formation.", "Development of new benchmarks for evaluating convention formation in LLMs.", "Demonstration of significant improvements in LLMs' interaction capabilities through targeted training."], "limitations": "The proposed methods may require substantial data for effective fine-tuning and may not generalize across all contexts of communication.", "keywords": ["Language Models", "Convention Formation", "Cognitive Interaction", "Fine-tuning", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.05913", "pdf": "https://arxiv.org/pdf/2508.05913.pdf", "abs": "https://arxiv.org/abs/2508.05913", "title": "Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction", "authors": ["Stefan Pasch", "Min Chul Cha"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As AI systems become increasingly embedded in organizational workflows and\nconsumer applications, ethical principles such as fairness, transparency, and\nrobustness have been widely endorsed in policy and industry guidelines.\nHowever, there is still scarce empirical evidence on whether these principles\nare recognized, valued, or impactful from the perspective of users. This study\ninvestigates the link between ethical AI and user satisfaction by analyzing\nover 100,000 user reviews of AI products from G2. Using transformer-based\nlanguage models, we measure sentiment across seven ethical dimensions defined\nby the EU Ethics Guidelines for Trustworthy AI. Our findings show that all\nseven dimensions are positively associated with user satisfaction. Yet, this\nrelationship varies systematically across user and product types. Technical\nusers and reviewers of AI development platforms more frequently discuss\nsystem-level concerns (e.g., transparency, data governance), while\nnon-technical users and reviewers of end-user applications emphasize\nhuman-centric dimensions (e.g., human agency, societal well-being). Moreover,\nthe association between ethical AI and user satisfaction is significantly\nstronger for non-technical users and end-user applications across all\ndimensions. Our results highlight the importance of ethical AI design from\nusers' perspectives and underscore the need to account for contextual\ndifferences across user roles and product types.", "AI": {"tldr": "This study examines the relationship between ethical AI principles and user satisfaction in over 100,000 user reviews of AI products, finding a positive association influenced by user type and product type.", "motivation": "To address the lack of empirical evidence on the recognition and impact of ethical AI principles from the user perspective.", "method": "Analysis of over 100,000 user reviews from G2, utilizing transformer-based language models to measure sentiment based on seven ethical dimensions outlined by the EU Ethics Guidelines for Trustworthy AI.", "result": "All seven ethical dimensions are positively correlated with user satisfaction, with variations depending on whether the users are technical or non-technical and the type of AI product.", "conclusion": "Designing ethical AI requires considering users' perspectives and the contextual differences related to user roles and product types to enhance user satisfaction.", "key_contributions": ["Empirical validation of the relationship between ethical AI principles and user satisfaction.", "Identification of variations in ethical AI discussions among technical vs. non-technical users.", "Highlighting contextual differences in user experiences across different AI product types."], "limitations": "The study only analyzes user reviews from a specific platform (G2), which may not represent the entire AI product landscape.", "keywords": ["ethical AI", "user satisfaction", "sentiment analysis", "transformer models", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.06065", "pdf": "https://arxiv.org/pdf/2508.06065.pdf", "abs": "https://arxiv.org/abs/2508.06065", "title": "ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation", "authors": ["Daniel Lee", "Nikhil Sharma", "Donghoon Shin", "DaEun Choi", "Harsh Sharma", "Jeonghwan Kim", "Heng Ji"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV", "H.5.2; I.2.7"], "comment": null, "summary": "Generative AI has made image creation more accessible, yet aligning outputs\nwith nuanced creative intent remains challenging, particularly for non-experts.\nExisting tools often require users to externalize ideas through prompts or\nreferences, limiting fluid exploration. We introduce ThematicPlane, a system\nthat enables users to navigate and manipulate high-level semantic concepts\n(e.g., mood, style, or narrative tone) within an interactive thematic design\nplane. This interface bridges the gap between tacit creative intent and system\ncontrol. In our exploratory study (N=6), participants engaged in divergent and\nconvergent creative modes, often embracing unexpected results as inspiration or\niteration cues. While they grounded their exploration in familiar themes,\ndiffering expectations of how themes mapped to outputs revealed a need for more\nexplainable controls. Overall, ThematicPlane fosters expressive, iterative\nworkflows and highlights new directions for intuitive, semantics-driven\ninteraction in generative design tools.", "AI": {"tldr": "ThematicPlane is a generative AI system designed to help users manipulate high-level semantic concepts to enhance creative exploration in image creation.", "motivation": "Generative AI facilitates image creation, but its tools often limit users, especially non-experts, in expressing nuanced creative intent due to reliance on complex prompts and references.", "method": "We developed ThematicPlane, an interactive thematic design interface that allows manipulation of semantic concepts like mood, style, and narrative tone, tested through an exploratory study with six participants.", "result": "Participants engaging with ThematicPlane found it supported both divergent and convergent creative processes, using unexpected results as inspiration. However, there was a noted gap in expectations regarding the mapping of themes to outputs, highlighting a demand for more explainable controls.", "conclusion": "ThematicPlane enhances iterative workflows and suggests new possibilities for interactive, semantic-driven engagement in generative design tools.", "key_contributions": ["Introduction of ThematicPlane as an innovative generative design tool", "Focus on high-level semantic concept manipulation for image creation", "Empirical insights revealing user behavior and needs in creative processes."], "limitations": "The exploratory study was limited by a small sample size and a lack of diverse user backgrounds.", "keywords": ["Generative AI", "Human-Computer Interaction", "Thematic Design", "Semantics", "User Experience"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2402.17008", "pdf": "https://arxiv.org/pdf/2402.17008.pdf", "abs": "https://arxiv.org/abs/2402.17008", "title": "Benchmarking LLMs on the Semantic Overlap Summarization Task", "authors": ["John Salvador", "Naman Bansal", "Mousumi Akter", "Souvika Sarkar", "Anupam Das", "Shubhra Kanti Karmaker"], "categories": ["cs.CL"], "comment": null, "summary": "Semantic Overlap Summarization (SOS) is a constrained multi-document\nsummarization task, where the constraint is to capture the common/overlapping\ninformation between two alternative narratives. In this work, we perform a\nbenchmarking study of popular Large Language Models (LLMs) exclusively on the\nSOS task. Additionally, we introduce the PrivacyPolicyPairs (3P) dataset to\nexpand the space of SOS benchmarks in terms of quantity and variety. This\ndataset provides 135 high-quality SOS data samples sourced from privacy policy\ndocuments. We then use a standard prompting taxonomy called TELeR to create and\nevaluate 905,216 distinct LLM-generated summaries over two SOS datasets from\ndifferent domains, and we further conduct human evaluation on a subset of 540\nsamples. We conclude the paper by analyzing models' performances and the\nreliability of automatic evaluation. The code and datasets used to conduct this\nstudy are available at https://anonymous.4open.science/r/llm_eval-E16D.", "AI": {"tldr": "The paper benchmarks large language models on Semantic Overlap Summarization, introduces the PrivacyPolicyPairs dataset, and evaluates LLM-generated summaries.", "motivation": "To assess the performance of large language models on the constrained task of Semantic Overlap Summarization and provide a new dataset for benchmarking.", "method": "The study uses the TELeR prompting taxonomy to generate and evaluate over 905,216 LLM-generated summaries from two SOS datasets across different domains, with human evaluation on 540 samples.", "result": "The performance of various LLMs on the SOS task is analyzed, alongside the reliability of automatic evaluation methods.", "conclusion": "The study provides insights into LLM capabilities for summarization tasks and highlights the need for robust evaluation methods; the datasets and code are available for further research.", "key_contributions": ["Introduction of the PrivacyPolicyPairs dataset for SOS benchmarking", "Conducted extensive evaluation of LLMs on SOS tasks", "Analysis of performance and evaluation methods for LLM-generated summaries"], "limitations": "The focus is exclusively on the SOS task and may not generalize to other summarization forms; the evaluation may be limited by the datasets used.", "keywords": ["Semantic Overlap Summarization", "Large Language Models", "PrivacyPolicyPairs", "dataset", "summarization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.11827", "pdf": "https://arxiv.org/pdf/2409.11827.pdf", "abs": "https://arxiv.org/abs/2409.11827", "title": "Extract-and-Abstract: Unifying Extractive and Abstractive Summarization within Single Encoder-Decoder Framework", "authors": ["Yuping Wu", "Hao Li", "Goran Nenadic", "Xiao-Jun Zeng"], "categories": ["cs.CL"], "comment": null, "summary": "Extract-then-Abstract is a naturally coherent paradigm to conduct abstractive\nsummarization with the help of salient information identified by the extractive\nmodel. Previous works that adopt this paradigm train the extractor and\nabstractor separately and introduce extra parameters to highlight the extracted\nsalients to the abstractor, which results in error accumulation and additional\ntraining costs. In this paper, we first introduce a parameter-free highlight\nmethod into the encoder-decoder framework: replacing the encoder attention mask\nwith a saliency mask in the cross-attention module to force the decoder to\nfocus only on salient parts of the input. A preliminary analysis compares\ndifferent highlight methods, demonstrating the effectiveness of our saliency\nmask. We further propose the novel extract-and-abstract paradigm, ExtAbs.,\nwhich jointly and seamlessly performs Extractive and Abstractive summarization\ntasks within single encoder-decoder model to reduce error accumulation. In\nExtAbs, the vanilla encoder is augmented to extract salients, and the vanilla\ndecoder is modified with the proposed saliency mask to generate summaries.\nBuilt upon BART and PEGASUS, experiments on three datasets show that ExtAbs can\nachieve superior performance than baselines on the extractive task and performs\ncomparable, or even better than the vanilla models on the abstractive task.", "AI": {"tldr": "The paper proposes ExtAbs, a novel method integrating extractive and abstractive summarization in a single model using a saliency mask to enhance performance while reducing error accumulation.", "motivation": "To improve the efficiency and effectiveness of abstractive summarization by addressing the limitations of separately training extractive and abstractive models.", "method": "ExtAbs employs a parameter-free saliency mask in the encoder-decoder framework, integrating the extraction and abstraction tasks within a single model to enhance focus on salient information during summary generation.", "result": "Experiments demonstrate that ExtAbs outperforms baseline methods in extractive summarization tasks and achieves comparable or superior results in abstractive summarization compared to traditional models.", "conclusion": "The proposed ExtAbs paradigm effectively reduces error accumulation and training costs, leading to superior summarization performance across multiple datasets.", "key_contributions": ["Introduction of a parameter-free highlight method for summarization", "Development of the ExtAbs paradigm for joint extractive and abstractive summarization", "Demonstration of superior performance on multiple datasets"], "limitations": "", "keywords": ["abstractive summarization", "extractive summarization", "saliency mask"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.01872", "pdf": "https://arxiv.org/pdf/2501.01872.pdf", "abs": "https://arxiv.org/abs/2501.01872", "title": "Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions", "authors": ["Rachneet Sachdeva", "Rima Hazra", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Our code is publicly available at\n  https://github.com/UKPLab/arxiv2025-poate-attack", "summary": "Large language models, despite extensive alignment with human values and\nethical principles, remain vulnerable to sophisticated jailbreak attacks that\nexploit their reasoning abilities. Existing safety measures often detect overt\nmalicious intent but fail to address subtle, reasoning-driven vulnerabilities.\nIn this work, we introduce POATE (Polar Opposite query generation, Adversarial\nTemplate construction, and Elaboration), a novel jailbreak technique that\nharnesses contrastive reasoning to provoke unethical responses. POATE crafts\nsemantically opposing intents and integrates them with adversarial templates,\nsteering models toward harmful outputs with remarkable subtlety. We conduct\nextensive evaluation across six diverse language model families of varying\nparameter sizes to demonstrate the robustness of the attack, achieving\nsignificantly higher attack success rates (~44%) compared to existing methods.\nTo counter this, we propose Intent-Aware CoT and Reverse Thinking CoT, which\ndecompose queries to detect malicious intent and reason in reverse to evaluate\nand reject harmful responses. These methods enhance reasoning robustness and\nstrengthen the model's defense against adversarial exploits.", "AI": {"tldr": "POATE is a new jailbreak technique targeting language models, exploiting reasoning to produce unethical responses, and proposes methods to counter these attacks.", "motivation": "To address vulnerabilities in language models that are not detected by existing safety measures, particularly those related to reasoning-driven flaws.", "method": "Introduced POATE, which uses contrastive reasoning to generate semantic oppositions and adversarial templates to elicit harmful outputs from language models; evaluated attack effectiveness across various model families.", "result": "Achieved an attack success rate of approximately 44%, higher than existing techniques, revealing significant vulnerability in language model responses to adversarial queries.", "conclusion": "Proposed new methods (Intent-Aware CoT and Reverse Thinking CoT) enhance model defenses by decomposing queries and evaluating responses to mitigate harmful outputs.", "key_contributions": ["Introduction of POATE as a novel jailbreak technique", "Demonstration of higher attack success rates compared to existing methods", "Development of new mitigation strategies for enhancing reasoning robustness"], "limitations": "", "keywords": ["language models", "jailbreak attacks", "adversarial templates"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.11417", "pdf": "https://arxiv.org/pdf/2501.11417.pdf", "abs": "https://arxiv.org/abs/2501.11417", "title": "Neural Contextual Reinforcement Framework for Logical Structure Language Generation", "authors": ["Marcus Irvin", "William Cooper", "Edward Hughes", "Jessica Morgan", "Christopher Hamilton"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "The Neural Contextual Reinforcement Framework introduces an innovative\napproach to enhancing the logical coherence and structural consistency of text\ngenerated by large language models. Leveraging reinforcement learning\nprinciples, the framework integrates custom reward functions and dynamic\ncontext alignment mechanisms to address challenges inherent in maintaining\nlong-range dependencies across extended sequences. The architecture\nincorporates multi-head attention layers and hierarchical encoding modules,\nenabling the model to produce outputs that align closely with human\nexpectations of logical structure and semantic flow. Quantitative evaluations\nacross diverse datasets demonstrate substantial improvements in coherence\nmetrics, perplexity reduction, and semantic alignment, showcasing the\nframework's ability to outperform baseline models in both general and\ndomain-specific tasks. Qualitative analyses further highlight the framework's\ncapacity to generate text with improved narrative clarity and reduced\nredundancy, reflecting its effectiveness in balancing fluency with structural\nprecision. In addition to its performance gains, the framework exhibits\nrobustness in handling noisy input data and scalability across varying model\nsizes, reinforcing its versatility in practical applications. Experimental\nresults reveal that optimal context window sizes significantly influence\ncoherence outcomes, showing the importance of architectural flexibility in\nadapting to diverse linguistic structures. Cross-lingual performance\nevaluations affirm the framework's adaptability to multiple languages,\nextending its utility beyond monolingual contexts. Resource efficiency analyses\nindicate a reduction in computational overhead compared to traditional\napproaches, emphasizing the practicality of the framework for large-scale\ndeployment.", "AI": {"tldr": "The Neural Contextual Reinforcement Framework enhances text coherence and structure in LLMs using reinforcement learning, leading to improved semantic flow and clarity.", "motivation": "To address the challenges of maintaining long-range dependencies in text generated by large language models.", "method": "Utilizes reinforcement learning with custom reward functions and dynamic context alignment, incorporating multi-head attention layers and hierarchical encoding.", "result": "Significant improvements in coherence metrics, perplexity, and semantic alignment over baseline models, with better narrative clarity and reduced redundancy.", "conclusion": "The framework demonstrates robustness, scalability, and efficiency for practical applications, adaptable across multiple languages.", "key_contributions": ["Introduces a novel reinforcement learning approach for LLMs", "Improves coherence and structural consistency of generated text", "Demonstrates scalability and resource efficiency across diverse applications."], "limitations": "The paper has been withdrawn due to disputed authorship.", "keywords": ["Neural Contextual Reinforcement", "Text Coherence", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.12901", "pdf": "https://arxiv.org/pdf/2501.12901.pdf", "abs": "https://arxiv.org/abs/2501.12901", "title": "Architectural Fusion Through Contextual Partitioning in Large Language Models: A Novel Approach to Parameterized Knowledge Integration", "authors": ["Offa Kingsleigh", "Alfred Abercrombie", "David Woolstencroft", "Beorhtric Meadowcroft", "Marcus Irvin"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Contextual Partitioning introduces an innovative approach to enhancing the\narchitectural design of large-scale computational models through the dynamic\nsegmentation of parameters into context-aware regions. This methodology\nemphasizes the importance of task-specific specialization, achieved through\nadaptive parameter allocation mechanisms that align with the linguistic\nfeatures of input data. Experimental evaluations demonstrated substantial\nimprovements in accuracy, perplexity, and contextual coherence across a variety\nof linguistic tasks, highlighting the adaptability and scalability of the\nproposed framework. By reducing redundancy and enhancing computational\nefficiency, Contextual Partitioning not only streamlines model operations but\nalso expands the scope of applications for advanced language processing\nsystems. The approach operates autonomously, requiring no external fine-tuning,\nthereby addressing a significant limitation in conventional parameter\noptimization techniques. Empirical results demonstrate the effectiveness of\ngradient-driven segmentation, enabling models to dynamically recalibrate and\nspecialize in response to task-specific demands. Furthermore, resource\nutilization metrics reveal notable reductions in memory usage and training\ntimes, confirming the efficiency of the approach. Observations from qualitative\nanalyses illustrate improved contextual coherence and logical flow in generated\noutputs, reinforcing the practical value of this technique. The findings\ncollectively demonstrate the potential for Contextual Partitioning to redefine\nthe scalability and adaptability of computational language architectures in\ndiverse and complex domains.", "AI": {"tldr": "This paper introduces Contextual Partitioning, a method to enhance computational models through dynamic parameter segmentation, improving accuracy and efficiency in language tasks.", "motivation": "The goal is to improve architectural design and efficiency of large-scale computational models by introducing a new methodology for parameter segmentation.", "method": "Contextual Partitioning employs adaptive parameter allocation that aligns with the linguistic features of input data, using gradient-driven segmentation for dynamic recalibration.", "result": "The method shows significant improvements in accuracy, perplexity, contextual coherence, and reductions in memory usage and training time across various linguistic tasks.", "conclusion": "Contextual Partitioning offers a scalable and adaptable approach for computational language architectures, eliminating the need for external fine-tuning and enhancing operational efficiency.", "key_contributions": ["Dynamic segmentation of parameters based on context", "Improved accuracy and coherence in linguistic tasks", "Reduction in memory usage and training times"], "limitations": "The paper has been withdrawn due to disputed and unverifiable authorship, limiting the applicability of its findings.", "keywords": ["Human-Computer Interaction", "Machine Learning", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2501.14119", "pdf": "https://arxiv.org/pdf/2501.14119.pdf", "abs": "https://arxiv.org/abs/2501.14119", "title": "Autonomous Structural Memory Manipulation for Large Language Models Using Hierarchical Embedding Augmentation", "authors": ["Derek Yotheringhay", "Alistair Kirkland", "Humphrey Kirkbride", "Josiah Whitesteeple"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Transformative innovations in model architectures have introduced\nhierarchical embedding augmentation as a means to redefine the representation\nof tokens through multi-level semantic structures, offering enhanced\nadaptability to complex linguistic inputs. Autonomous structural memory\nmanipulation further advances this paradigm through dynamic memory reallocation\nmechanisms that prioritize critical contextual features while suppressing less\nrelevant information, enabling scalable and efficient performance across\ndiverse tasks. Experimental results reveal substantial improvements in\ncomputational efficiency, with marked reductions in processing overhead for\nlonger input sequences, achieved through memory reorganization strategies that\nadapt to evolving contextual requirements. Hierarchical embeddings not only\nimproved contextual alignment but also facilitated task generalization by\ncapturing relationships at varying semantic granularities, ensuring coherence\nacross layers without introducing significant computational redundancies.\nComparative analysis against baseline models demonstrated unique advantages in\naccuracy, efficiency, and interpretability, particularly in tasks requiring\ncomplex contextual understanding or domain-specific adaptability. The ability\nto dynamically adjust token representations and memory configurations\ncontributed to the model's robustness under varied and unpredictable input\nconditions. Applications benefiting from these advancements include\nmulti-domain generalization, interactive systems, and scenarios involving\nreal-time decision-making, where traditional static memory architectures often\nface limitations. The proposed methodology combines advanced embedding and\nmemory management strategies into a cohesive framework that addresses\nscalability challenges while preserving task-specific relevance.", "AI": {"tldr": "This paper discusses hierarchical embedding augmentation and autonomous structural memory manipulation to improve the adaptability and efficiency of token representation in language models, showing promising results in complex tasks.", "motivation": "To enhance the representation of tokens through multi-level semantic structures, addressing limitations of traditional static memory architectures.", "method": "The methodology involves hierarchical embedding techniques and dynamic memory reallocation mechanisms to prioritize contextual features, enhancing scalability and efficiency.", "result": "The experimental results indicated substantial improvements in computational efficiency, particularly for longer input sequences, and advantages in accuracy and interpretability over baseline models.", "conclusion": "The proposed framework effectively combines embedding and memory management strategies, addressing scalability while ensuring task relevance and robustness in complex input scenarios.", "key_contributions": ["Introduction of hierarchical embedding techniques for improved token representation.", "Dynamic memory reallocation mechanisms that prioritize contextual features.", "Demonstrated advantages in accuracy, efficiency, and interpretability for complex tasks."], "limitations": "", "keywords": ["Hierarchical embeddings", "Dynamic memory", "Contextual understanding", "Scalability", "Multi-domain generalization"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2501.16658", "pdf": "https://arxiv.org/pdf/2501.16658.pdf", "abs": "https://arxiv.org/abs/2501.16658", "title": "Contextual Reinforcement in Multimodal Token Compression for Large Language Models", "authors": ["Naderdel Piero", "Zacharias Cromwell", "Nathaniel Wainwright", "Matthias Nethercott"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Effective token compression remains a critical challenge for scaling models\nto handle increasingly complex and diverse datasets. A novel mechanism based on\ncontextual reinforcement is introduced, dynamically adjusting token importance\nthrough interdependencies and semantic relevance. This approach enables\nsubstantial reductions in token usage while preserving the quality and\ncoherence of information representation. Incorporating graph-based algorithms\nand adaptive weighting, the method captures subtle contextual relationships\nacross textual and multimodal data, ensuring robust alignment and performance\nin downstream tasks. Evaluations across varied domains reveal significant\nimprovements in accuracy and semantic retention, particularly for tasks\nrequiring detailed cross-modal interactions. Memory usage analyses demonstrate\nimproved computational efficiency, with minimal overhead despite the additional\nreinforcement processes. Performance gains are further validated through error\ndistribution analyses, showing reduced semantic loss and syntactic\ninconsistencies compared to baseline models. The modular architecture ensures\ncompatibility with a wide range of open-source frameworks, facilitating\nscalable implementation for real-world applications. These findings highlight\nthe potential of contextual reinforcement in redefining token management\nstrategies and advancing large-scale model design.", "AI": {"tldr": "This paper proposes a contextual reinforcement mechanism for effective token compression in large models across complex datasets, improving accuracy and efficiency while managing token importance dynamically.", "motivation": "To address the critical challenge of effective token compression in scaling models for handling diverse datasets.", "method": "A novel mechanism using contextual reinforcement that dynamically adjusts token importance based on interdependencies and semantic relevance, incorporating graph-based algorithms and adaptive weighting.", "result": "Substantial reductions in token usage with preserved information quality, significant accuracy improvements, and efficient memory usage across varied domains and tasks.", "conclusion": "The findings emphasize the potential of contextual reinforcement in enhancing token management strategies for large-scale model design.", "key_contributions": ["Introduction of a contextual reinforcement mechanism for dynamic token importance adjustment", "Use of graph-based algorithms and adaptive weighting for capturing contextual relationships", "Demonstrated improvements in accuracy and computational efficiency for multimodal tasks."], "limitations": "Paper has been withdrawn due to disputed and unverifiable authorship, limiting its credibility and availability for further study.", "keywords": ["token compression", "contextual reinforcement", "machine learning", "large-scale models", "semantic relevance"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2501.18826", "pdf": "https://arxiv.org/pdf/2501.18826.pdf", "abs": "https://arxiv.org/abs/2501.18826", "title": "Structural Embedding Projection for Contextual Large Language Model Inference", "authors": ["Vincent Enoasmo", "Cedric Featherstonehaugh", "Xavier Konstantinopoulos", "Zacharias Huntington"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Structured embedding transformations offer a promising approach for enhancing\nthe efficiency and coherence of language model inference. The introduction of\nStructural Embedding Projection (SEP) provides a mechanism for refining token\nrepresentations through projection matrices that integrate hierarchical and\nrelational dependencies. The mathematical formulation of SEP enables embedding\nspaces to capture structured contextual relationships, thereby improving\nsemantic fidelity without significantly increasing computational overhead.\nExperimental evaluations conducted on a range of linguistic datasets revealed\nthat SEP contributed to reductions in perplexity and enhanced contextual\ncoherence, demonstrating its potential to refine language model outputs.\nComputational efficiency assessments highlighted variations across different\ndatasets, suggesting that the integration of structured embeddings introduced\ndataset-dependent trade-offs between inference speed and representational\nrichness. The qualitative analysis of generated responses indicated that SEP\nenhanced narrative consistency and topic alignment, leading to improved fluency\nin multi-sentence text generation. The modifications to embedding layers\nrequired precise optimization to ensure stable training dynamics, as the\nintroduction of structured transformations altered the traditional\nrepresentation-learning process. The architectural adjustments necessary for\nSEP implementation influenced inference latency and memory consumption,\nrequiring a balance between efficiency gains and additional processing demands.\nThe impact of SEP on lexical diversity suggested that embedding modifications\ninfluenced the model's vocabulary usage, reflecting a more context-aware\nselection of generated tokens.", "AI": {"tldr": "This paper discusses the Structural Embedding Projection (SEP) as a method to enhance language model inference efficiency and coherence by refining token representations.", "motivation": "To improve the efficiency and coherence of language model inference through refined token representations.", "method": "The paper introduces the Structural Embedding Projection (SEP) which uses projection matrices to integrate hierarchical and relational dependencies in token embeddings.", "result": "Experimental evaluations showed that SEP reduced perplexity and improved contextual coherence across various linguistic datasets, though trade-offs between inference speed and representational richness were noted.", "conclusion": "SEP enhances narrative consistency, topic alignment, and fluency in text generation while requiring careful optimization of embedding layers to maintain stable training dynamics.", "key_contributions": ["Introduction of SEP for structured embedding transformations.", "Demonstration of dataset-dependent trade-offs in computational efficiency.", "Qualitative improvements in narrative consistency and topic alignment in generated text."], "limitations": "The paper has been withdrawn due to disputed authorship, limiting the reliability of its conclusions.", "keywords": ["Structured Embedding Projection", "language models", "embedding transformations", "contextual relationships", "computational efficiency"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.00246", "pdf": "https://arxiv.org/pdf/2502.00246.pdf", "abs": "https://arxiv.org/abs/2502.00246", "title": "Context-Preserving Tensorial Reconfiguration in Large Language Model Training", "authors": ["Larin Tonix", "Morgana Baskerville", "Nathaniel Stourton", "Ophelia Tattershall"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Handling long-range dependencies in neural architectures has remained a\npersistent challenge due to computational limitations and inefficient\ncontextual retention mechanisms. Tensorial operations have provided a\nfoundation for restructuring model representations, yet conventional\narchitectures have struggled to incorporate such techniques without introducing\nexcessive complexity. A novel approach, Context-Preserving Tensorial\nReconfiguration (CPTR), enables dynamic reorganization of weight tensors\nthrough structured factorization and adaptive contraction, allowing for\nenhanced contextual integration without substantial computational overhead.\nEmpirical evaluations demonstrate that CPTR improves coherence retention across\nextended sequences, leading to measurable reductions in perplexity and improved\nrecall accuracy for long-context tasks. Performance comparisons reveal that\nCPTR-enhanced models exhibit greater computational efficiency and reduced\nmemory consumption while maintaining competitive language generation fluency\nand accuracy. Gradient stability metrics further validate the improved training\nefficiency, revealing more controlled variance in weight updates. Comparative\nstudies across baseline and CPTR-enhanced models confirm that tensorial\nreconfiguration contributes to more stable and computationally efficient\nlanguage modeling. The findings support the potential of CPTR in refining\ncontemporary neural architectures for tasks requiring long-range contextual\nunderstanding and efficient memory utilization.", "AI": {"tldr": "The paper proposes the Context-Preserving Tensorial Reconfiguration (CPTR) to enhance long-range dependency handling in neural models with improved efficiency and stability.", "motivation": "To address challenges in handling long-range dependencies in neural architectures caused by computational limitations and inefficient contextual retention.", "method": "The proposed approach, CPTR, utilizes dynamic reorganization of weight tensors via structured factorization and adaptive contraction to enhance contextual integration.", "result": "Empirical evaluations show that CPTR enhances coherence retention, reduces perplexity, and improves recall accuracy in long-context tasks, along with greater computational efficiency and reduced memory consumption compared to baseline models.", "conclusion": "CPTR shows potential in refining neural architectures for long-range contextual understanding and efficient memory utilization, while providing stable and efficient training metrics.", "key_contributions": ["Introduction of CPTR for effective tensorial reconfiguration", "Demonstrated improvements in coherence retention and recall accuracy", "Enhanced computational efficiency in language modeling through CPTR"], "limitations": "The paper has been withdrawn due to disputed and unverifiable authorship, raising concerns about its validity.", "keywords": ["Long-range dependencies", "Neural architectures", "Contextual integration", "Tensorial operations", "Language modeling"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2502.00301", "pdf": "https://arxiv.org/pdf/2502.00301.pdf", "abs": "https://arxiv.org/abs/2502.00301", "title": "Contextual Morphogenesis in Large Language Models: A Novel Approach to Self-Organizing Token Representations", "authors": ["Alistair Dombrowski", "Beatrix Engelhardt", "Dimitri Fairbrother", "Henry Evidail"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Token representations influence the efficiency and adaptability of language\nmodels, yet conventional tokenization strategies impose rigid segmentation\nboundaries that do not adjust dynamically to evolving contextual relationships.\nThe introduction of contextual morphogenesis establishes a self-organizing\nmechanism that restructures token boundaries based on learned contextual\ndependencies, allowing embeddings to evolve progressively across iterative\nprocessing steps. Empirical evaluations demonstrate that dynamically adjusted\ntokenization contributes to reductions in perplexity while maintaining\nrepresentational stability, particularly in linguistically complex domains\nwhere static segmentation fails to capture nuanced dependencies. Computational\ntrade-offs associated with self-organizing token structures indicate that\nadditional processing overhead remains within feasible limits, provided that\noptimization strategies account for segmentation update efficiency. Comparative\nassessments across different linguistic corpora suggest that adaptive\ntokenization preserves interpretability while improving alignment with\ncontextual cues, reinforcing the potential of morphogenetic segmentation\nmechanisms to refine predictive accuracy. Stability analyses confirm that\nevolving token structures maintain consistent segmentation behaviors across\nvaried text distributions, ensuring that representational adaptations remain\nlinguistically coherent. The effectiveness of contextual morphogenesis in\nrefining structural stability and predictive performance highlights its\nviability as an alternative to traditional tokenization methods. Further\nanalysis of computational efficiency considerations suggests that hybrid\nstrategies integrating both static and dynamic segmentation techniques may\noffer a balanced approach to optimizing representational flexibility while\nmaintaining inference efficiency.", "AI": {"tldr": "The paper proposes a dynamic tokenization approach, contextual morphogenesis, that adjusts token boundaries based on contextual dependencies to improve language model performance.", "motivation": "Conventional tokenization methods hinder language models by enforcing static boundaries that do not adapt to evolving contexts, leading to inefficiencies in representation.", "method": "The paper introduces contextual morphogenesis, a self-organizing mechanism that dynamically adjusts token boundaries in response to contextual dependencies, evaluated through empirical assessments.", "result": "Adaptive tokenization reduces perplexity and improves representational stability, particularly in linguistically complex domains, while maintaining interpretability and aligning better with contextual cues.", "conclusion": "Contextual morphogenesis serves as a viable alternative to traditional tokenization, with potential advantages for both structural stability and predictive performance in language models.", "key_contributions": ["Introduction of contextual morphogenesis for dynamic tokenization", "Demonstration of reduced perplexity and improved interpretability", "Analysis of computational trade-offs for hybrid tokenization strategies"], "limitations": "The paper has been withdrawn due to disputed and unverifiable authorship, limiting further validation of its claims.", "keywords": ["dynamic tokenization", "contextual morphogenesis", "language models", "adaptive tokenization", "perplexity reduction"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2502.00977", "pdf": "https://arxiv.org/pdf/2502.00977.pdf", "abs": "https://arxiv.org/abs/2502.00977", "title": "Context-Aware Hierarchical Merging for Long Document Summarization", "authors": ["Litu Ou", "Mirella Lapata"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Hierarchical Merging is a technique commonly used to summarize very long\ntexts ($>$100K tokens) by breaking down the input into smaller sections,\nsummarizing those sections individually, and then merging or combining those\nsummaries into a final coherent summary. Although it helps address the\nlimitations of large language models (LLMs) with fixed input length\nconstraints, the recursive merging process can amplify LLM hallucinations,\nincreasing the risk of factual inaccuracies. In this paper, we seek to mitigate\nhallucinations by enriching hierarchical merging with context from the source\ndocument. Specifically, we propose different approaches to contextual\naugmentation ranging from \\emph{replacing} intermediate summaries with relevant\ninput context, to \\emph{refining} them while using the context as supporting\nevidence, and \\emph{aligning} them implicitly (via citations) to the input.\nExperimental results on datasets representing legal and narrative domains show\nthat contextual augmentation consistently outperforms zero-shot and\nhierarchical merging baselines for the Llama 3.1 model family. Our analysis\nfurther reveals that refinement methods tend to perform best when paired with\nextractive summarization for identifying relevant input.", "AI": {"tldr": "The paper introduces a technique to improve hierarchical merging for summarizing long texts by incorporating contextual information from the source document, addressing LLM hallucinations.", "motivation": "To mitigate hallucinations in long text summarization by enhancing hierarchical merging with contextual information from the source document.", "method": "The authors propose contextual augmentation approaches, including replacing, refining, and aligning intermediate summaries with relevant input context, and evaluate these methods on legal and narrative datasets with the Llama 3.1 model family.", "result": "Experimental results indicate that contextual augmentation methods consistently outperform zero-shot and hierarchical merging baselines, particularly in refining methods paired with extractive summarization.", "conclusion": "The study concludes that integrating context into the hierarchical merging process significantly reduces hallucination risks and improves summarization accuracy.", "key_contributions": ["Introduction of contextual augmentation for hierarchical merging", "Experimental validation on diverse datasets", "Demonstration of superior performance of refinement methods with extractive summarization"], "limitations": "Limited to specific domains evaluated (legal and narrative), with potential varied performance in other areas.", "keywords": ["Hierarchical Merging", "Contextual Augmentation", "Summarization", "Large Language Models", "Hallucinations"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.01979", "pdf": "https://arxiv.org/pdf/2502.01979.pdf", "abs": "https://arxiv.org/abs/2502.01979", "title": "Gradient-Regularized Latent Space Modulation in Large Language Models for Structured Contextual Synthesis", "authors": ["Derek Yotheringhay", "Beatrix Nightingale", "Maximilian Featherstone", "Edmund Worthington", "Hugo Ashdown"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Generating structured textual content requires mechanisms that enforce\ncoherence, stability, and adherence to predefined constraints while maintaining\nsemantic fidelity. Conventional approaches often rely on rule-based heuristics\nor fine-tuning strategies that lack flexibility and generalizability across\ndiverse tasks. The incorporation of Gradient-Regularized Latent Space\nModulation (GRLSM) introduces a novel paradigm for guiding text generation\nthrough the application of structured constraints within the latent space. The\nintegration of gradient-based regularization mitigates abrupt variations in\nlatent representations, ensuring a smoother encoding process that enhances\nstructural consistency and logical progression within generated sequences.\nComparative evaluations demonstrate that latent space modulation leads to a\nreduction in perplexity, increased coherence scores, and improved structural\nalignment across multiple domains. Stability assessments further indicate that\nthe imposition of spectral norm constraints facilitates more controlled\nvariations in generated text, preserving semantic consistency under input\nperturbations. Empirical results confirm that structured latent space\nconstraints not only refine the organization of generated outputs but also\nenhance interpretability through more predictable and reliable synthesis\npatterns. Performance metrics illustrate that the GRLSM framework substantially\nreduces structural inconsistencies while preserving the generative flexibility\ninherent in neural models.", "AI": {"tldr": "The paper introduces Gradient-Regularized Latent Space Modulation (GRLSM), a novel method for guiding text generation with structured constraints in the latent space, resulting in enhanced coherence and structural integrity in generated outputs.", "motivation": "To address the limitations of conventional text generation methods that rely on rigid rules or fine-tuning strategies, which often lack flexibility and generalizability across diverse tasks.", "method": "The GRLSM framework utilizes gradient-based regularization within the latent space to impose structured constraints on the text generation process, mitigating abrupt variations and enhancing coherence and structural alignment.", "result": "Comparative evaluations show that GRLSM leads to reduced perplexity, increased coherence scores, and improved structural alignment across multiple domains without sacrificing generative flexibility.", "conclusion": "The GRLSM framework successfully refines the organization of generated texts, enhances interpretability, and maintains semantic consistency through controlled variations in latent space as demonstrated by performance metrics.", "key_contributions": ["Introduction of the GRLSM framework for text generation", "Demonstration of improved coherence and structural consistency", "Empirical validation of reduced perplexity and enhanced interpretability"], "limitations": "This paper has been withdrawn due to disputed and unverifiable authorship, which may limit its reliability and applicability.", "keywords": ["Text generation", "Latent space modulation", "Coherence", "Structural consistency", "Gradient-based regularization"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2502.05553", "pdf": "https://arxiv.org/pdf/2502.05553.pdf", "abs": "https://arxiv.org/abs/2502.05553", "title": "Latent Structure Modulation in Large Language Models Through Stochastic Concept Embedding Transitions", "authors": ["Stefan Whitaker", "Colin Sisate", "Marcel Windsor", "Nikolai Fairweather", "Tarquin Goldborough", "Oskar Lindenfeld"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Stochastic embedding transitions introduce a probabilistic mechanism for\nadjusting token representations dynamically during inference, mitigating the\nconstraints imposed through static or deterministic embeddings. A transition\nframework was proposed in which each token embedding evolved through\nprobabilistic updates, ensuring adaptability while preserving semantic\nintegrity across linguistic contexts. Empirical evaluations demonstrated that\nmodels incorporating stochastic transitions exhibited greater lexical\ndiversity, improved generative coherence, and enhanced retention of\nlow-frequency vocabulary, contributing to more varied sentence structures and\nreduced reliance on high-probability token selections. Statistical analyses of\nembedding drift across transformer layers indicated that representations\nevolved more flexibly without losing coherence, supporting the hypothesis that\ncontrolled stochasticity facilitated context-sensitive representation learning.\nExperimental results revealed that probabilistic embeddings introduced minor\ncomputational overhead while maintaining generative efficiency, reinforcing\ntheir feasibility in large-scale applications. A comparative study with\ntraditional embedding approaches highlighted measurable gains in text\ncompletion accuracy, dialogue coherence, and structural complexity, confirming\nthe effectiveness of stochastic transitions in enhancing representation\nexpressiveness. Clustering patterns in the embedding space suggested that\nprobabilistic updates preserved meaningful semantic groupings while enabling\ncontext-driven shifts, further validating the stability of the transition\nmechanism. Performance metrics indicated that stochastic transitions balanced\nadaptability and control, ensuring that generative outputs remained\nlinguistically coherent without excessive randomness.", "AI": {"tldr": "This paper discusses stochastic embedding transitions for dynamic token representation during inference, demonstrating improvements in generative performance and lexical diversity.", "motivation": "To enhance token representations in natural language processing by allowing embeddings to adjust dynamically through probabilistic mechanisms, thereby improving generative coherence and variability.", "method": "A framework for stochastic embedding transitions was proposed, where token embeddings evolve through probabilistic updates. Empirical evaluations and statistical analyses were conducted to assess improvements in lexical diversity and model performance.", "result": "Models with stochastic transitions showed greater lexical diversity, improved dialogue coherence, and better retention of low-frequency vocabulary, leading to more varied sentence structures with minor computational overhead.", "conclusion": "The study confirms that stochastic embedding transitions enhance representation expressiveness while maintaining coherence, suggesting their feasibility in large-scale NLP applications.", "key_contributions": ["Introduction of a stochastic embedding transition framework", "Demonstration of improved lexical diversity and generative coherence", "Empirical validation of adaptability in token representation"], "limitations": "The paper has been withdrawn due to disputed authorship, limiting its academic reliability.", "keywords": ["stochastic embedding", "representation learning", "natural language processing"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.05794", "pdf": "https://arxiv.org/pdf/2502.05794.pdf", "abs": "https://arxiv.org/abs/2502.05794", "title": "Structural Perturbation in Large Language Model Representations through Recursive Symbolic Regeneration", "authors": ["Kathlyn Eaglewood", "Tobias Featherington", "Dorian Mayfair", "Sylvester Grimshaw", "James Pettigrew"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Symbolic perturbations offer a novel approach for influencing neural\nrepresentations without requiring direct modification of model parameters. The\nrecursive regeneration of symbolic structures introduces structured variations\nin latent embeddings, leading to controlled shifts in attention dynamics and\nlexical diversity across sequential generations. A comparative analysis with\nconventional fine-tuning techniques reveals that structural modifications at\nthe symbolic level induce distinct variations in contextual sensitivity while\nmaintaining overall model fluency and coherence. Shifts in attention weight\ndistributions highlight the role of symbolic modifications in adjusting token\ndependencies, influencing response variability, and refining long-form text\ngeneration. Experimental findings suggest that symbolic perturbations can\nenhance adaptability in domain-specific applications, allowing modifications in\nmodel behavior without retraining. Evaluations of semantic drift indicate that\nrecursive regeneration alters long-range token dependencies, affecting topic\ncoherence across extended text sequences. Results from lexical variability\nassessments further support the conclusion that symbolic-level modifications\nintroduce interpretable variations in generated responses, potentially enabling\nmore controlled stylistic adjustments in automated text generation.", "AI": {"tldr": "This paper explores symbolic perturbations as a method to influence neural representations for text generation without modifying model parameters, highlighting their potential in maintaining fluency while enhancing adaptability and lexical diversity.", "motivation": "To investigate how symbolic perturbations can influence neural model outputs without the need for fine-tuning or parameter changes, aiming to enhance text generation quality and adaptability.", "method": "The approach involves recursive regeneration of symbolic structures that introduce variations in latent embeddings, focusing on attention dynamics and lexical diversity through comparative analysis with traditional fine-tuning.", "result": "Findings indicate that symbolic modifications can lead to controlled shifts in attention and refinements in text generation, improving response variability and topic coherence in long-form outputs.", "conclusion": "Symbolic perturbations provide a novel way to adjust model behavior without retraining, allowing for enhanced adaptability in specific domains and interpretable variations in generated content.", "key_contributions": ["Introduction of symbolic perturbations to influence neural representations", "Demonstration of maintaining model fluency while enhancing lexical diversity", "Empirical evidence of adaptability in domain-specific applications without retraining"], "limitations": "", "keywords": ["Symbolic Perturbations", "Neural Representations", "Text Generation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.07124", "pdf": "https://arxiv.org/pdf/2502.07124.pdf", "abs": "https://arxiv.org/abs/2502.07124", "title": "Structural Reformation of Large Language Model Neuron Encapsulation for Divergent Information Aggregation", "authors": ["Denis Bakushev", "Gideon Boultinghouse", "Harriet Oppenheimer", "Sebastian Gillingwater", "Valentina Ashington", "Wilfred Stanborough"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Structured neuron encapsulation introduces a modular framework that enables\nmore effective aggregation and specialization of information within deep\nlearning architectures. A model modified through this framework demonstrated\nimproved perplexity scores, greater lexical variability, and enhanced\nconsistency in logical reasoning, suggesting that structured parameter\ndistribution contributes to more efficient language representation. Statistical\nanalyses of generated text highlighted a wider range of sentence structures and\nreduced redundancy in token selection, indicating that encapsulation fosters\nmore adaptable language generation. A detailed evaluation of attention weight\ndistributions revealed that the experimental model exhibited greater divergence\nin cross-layer activations, supporting the hypothesis that encapsulated neurons\nassume specialized processing roles. Logical consistency assessments further\ndemonstrated that modular architectures mitigate contradictory outputs,\nreducing internal conflicts in inferred relationships between linguistic\nconstructs. Computational trade-offs were analyzed, with results showing a\nminor increase in processing overhead, though improvements in parameter\nefficiency and structured decision-making compensated for the additional\ncomplexity. The mathematical formulation of the encapsulation mechanism\nconfirmed that modular aggregation maintains stable convergence properties\nwhile promoting distinct functional roles for different neuron clusters.", "AI": {"tldr": "The paper introduces a modular framework for structured neuron encapsulation, enhancing language generation in deep learning models through improved aggregation and specialization of information.", "motivation": "To improve the efficiency of language representation in deep learning architectures by encapsulating neurons in modular frameworks.", "method": "A modified model incorporating structured neuron encapsulation was evaluated through statistical analyses of generated text and attention weight distributions to assess improvements in language generation.", "result": "The model showed improved perplexity, greater lexical variability, and enhanced logical consistency with reduced redundancy in token selection.", "conclusion": "Modular encapsulation leads to more effective language generation and improves parameter efficiency, despite some minor increases in processing overhead.", "key_contributions": ["Introduction of a modular framework for neuron encapsulation", "Demonstration of improved language generation metrics", "Mathematical formulation ensuring stable convergence properties of modular architectures"], "limitations": "The paper has been withdrawn due to disputed and unverifiable authorship.", "keywords": ["structured neuron encapsulation", "deep learning", "language generation", "modular architectures", "parameter efficiency"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.08947", "pdf": "https://arxiv.org/pdf/2502.08947.pdf", "abs": "https://arxiv.org/abs/2502.08947", "title": "Structured Convergence in Large Language Model Representations via Hierarchical Latent Space Folding", "authors": ["Fenella Harcourt", "Naderdel Piero", "Gilbert Sutherland", "Daphne Holloway", "Harriet Bracknell", "Julian Ormsby"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Token representations in high-dimensional latent spaces often exhibit\nredundancy, limiting computational efficiency and reducing structural coherence\nacross model layers. Hierarchical latent space folding introduces a structured\ntransformation mechanism that enforces a multi-scale organization within\nlearned embeddings, refining representational compactness while preserving\nessential contextual distinctions. The proposed approach incorporates dynamic\nfolding operations that iteratively adjust token embeddings through structured\ntransformations, influencing both short-range and long-range dependencies in\nsequential processing tasks. Empirical evaluation demonstrates a reduction in\nrepresentational variance across layers, contributing to more stable perplexity\ndistributions and enhancing predictive confidence in text generation. The\nstructured redistribution of attention head utilization leads to more efficient\nallocation of computational resources, particularly in deeper layers, where\nhierarchical refinements improve contextual abstraction. Comparative analysis\nof activation sparsity patterns suggests that hierarchical adjustments\nselectively reinforce critical pathways while reducing computational overhead\nin non-essential regions of the model. Statistical assessments of token\nreordering frequencies reveal that hierarchical modifications introduce subtle\nshifts in sequential dependencies, improving contextual alignment while\nmaintaining syntactic correctness. Computational trade-offs associated with\nhierarchical folding introduce marginal increases in training time per epoch,\nyet empirical findings indicate that inference efficiency benefits from the\nstructured representation adjustments. The results highlight the impact of\nhierarchical latent space folding on optimizing model performance through\nimproved representation structuring and computational efficiency.", "AI": {"tldr": "This paper discusses hierarchical latent space folding to improve token representations in high-dimensional spaces, enhancing computational efficiency and contextual coherence in sequential processing tasks.", "motivation": "The paper addresses redundancy in token representations that hinders efficiency and coherence in model layers.", "method": "Introduces dynamic folding operations that adjust token embeddings through structured transformations, refining representational compactness and enhancing dependencies.", "result": "Empirical evaluations show reduced representational variance, improved predictive confidence in text generation, and more efficient allocation of computational resources.", "conclusion": "Hierarchical latent space folding optimizes model performance through better representation structuring and enhanced computational efficiency, despite minor increases in training time.", "key_contributions": ["Dynamic folding operations enhance token embeddings.", "Improved representational compactness while preserving context.", "Efficient allocation of computational resources in deeper layers."], "limitations": "The paper was withdrawn due to disputed authorship, limiting the validity of its findings.", "keywords": ["Hierarchical latent space", "Token embeddings", "Computational efficiency"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2502.09815", "pdf": "https://arxiv.org/pdf/2502.09815.pdf", "abs": "https://arxiv.org/abs/2502.09815", "title": "Statistical Coherence Alignment for Large Language Model Representation Learning Through Tensor Field Convergence", "authors": ["Jonathan Gale", "Godfrey Aldington", "Harriet Thistlewood", "Thomas Tattershall", "Basil Wentworth", "Vincent Enoasmo"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Representation learning plays a central role in structuring internal\nembeddings to capture the statistical properties of language, influencing the\ncoherence and contextual consistency of generated text. Statistical Coherence\nAlignment is introduced as a method to enforce structured token representations\nthrough tensor field convergence, guiding embeddings to reflect statistical\ndependencies inherent in linguistic data. A mathematical framework is\nestablished to quantify coherence alignment, integrating a loss function that\noptimizes representational consistency across training iterations. Empirical\nevaluations demonstrate that applying coherence constraints improves\nperplexity, enhances classification accuracy, and refines rare word embeddings,\ncontributing to a more stable representation space. Comparative analyses with\nbaseline models reveal that the proposed method fosters a more interpretable\ninternal structure, ensuring that embeddings retain contextual dependencies\nwhile mitigating representation collapse. The impact on coherence score\ndistributions suggests that the alignment mechanism strengthens semantic\nintegrity across diverse linguistic constructs, leading to a more balanced\norganization of learned embeddings. Computational assessments indicate that\nwhile the method introduces additional memory and training costs, the\nstructured optimization process justifies the trade-offs in applications\nrequiring heightened contextual fidelity. Experimental results validate the\neffectiveness of coherence alignment in optimizing token representations,\nproviding insights into how statistical dependencies can be leveraged to\nimprove language model training.", "AI": {"tldr": "This paper introduces Statistical Coherence Alignment, a method for improving the representational consistency of language model embeddings by enforcing structured token representations through tensor field convergence.", "motivation": "To enhance the coherence and contextual consistency of generated text in language models by improving representation learning.", "method": "The proposed method uses tensor field convergence to enforce structured token representations, integrated with a loss function that optimizes consistency across training iterations.", "result": "Empirical evaluations showed improved perplexity, classification accuracy, and rare word embeddings, leading to a more stable representation space and interpretable internal structure.", "conclusion": "Despite additional memory and training costs, the coherence alignment method provides significant trade-offs in applications needing heightened contextual fidelity.", "key_contributions": ["Introduction of Statistical Coherence Alignment for language model embeddings", "Development of a mathematical framework for coherence alignment", "Demonstration of improvements in perplexity and classification accuracy through empirical evaluations."], "limitations": "The paper has been withdrawn due to disputed and unverifiable authorship, limiting the accessibility of results and their validation.", "keywords": ["Representation Learning", "Coherence Alignment", "Language Models", "Statistical Dependencies", "Embedding Optimization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.10699", "pdf": "https://arxiv.org/pdf/2502.10699.pdf", "abs": "https://arxiv.org/abs/2502.10699", "title": "Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration", "authors": ["George Applegarth", "Christian Weatherstone", "Maximilian Hollingsworth", "Henry Middlebrook", "Marcus Irvin"], "categories": ["cs.CL", "cs.AI", "cs.NE"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Contextual memory integration remains a high challenge in the development of\nlanguage models, particularly in tasks that require maintaining coherence over\nextended sequences. Traditional approaches, such as self-attention mechanisms\nand memory-augmented architectures, often prioritize short-term dependencies,\nleading to fragmentation and inconsistency in long-range contextual\nunderstanding. Inspired by principles of synaptic plasticity observed in\nbiological neural systems, a novel mechanism, Synaptic Resonance, is introduced\nto dynamically reinforce relevant memory pathways during training and\ninference. Unlike static memory representations, this mechanism continuously\nadjusts synaptic weight matrices based on contextual relevance, allowing for\nimproved information retention without excessive computational overhead.\nEvaluations conducted on an open-source language model demonstrate reductions\nin perplexity, enhancements in contextual coherence, and increased robustness\nagainst input noise, highlighting the effectiveness of reinforcement-driven\nmemory modulation. Comparative analysis against baseline models further reveals\nthat the proposed approach achieves higher memory retention efficiency while\nmaintaining computational feasibility. The architectural modifications\nintegrate seamlessly into existing transformer-based frameworks, ensuring\nstable convergence and efficient inference without sacrificing scalability.\nApplications benefiting from improved long-term contextual consistency, such as\ndialogue systems and document summarization, stand to gain from this approach.\nEmpirical findings suggest that dynamically reinforced memory pathways offer a\npromising alternative to conventional memory mechanisms, addressing\nlongstanding limitations in extended sequence modeling.", "AI": {"tldr": "This paper introduces Synaptic Resonance, a novel mechanism for improving long-range contextual understanding in language models through dynamic memory reinforcement.", "motivation": "The goal is to address the challenges of maintaining coherence in language models over extended sequences, which traditional memory mechanisms struggle with.", "method": "The proposed mechanism adjusts synaptic weight matrices based on contextual relevance during training and inference, enhancing memory pathway reinforcement dynamically.", "result": "The approach leads to reductions in perplexity, improved contextual coherence, and increased resilience to input noise compared to baseline models.", "conclusion": "Dynamically reinforced memory pathways can effectively address limitations in traditional memory mechanisms, particularly in applications like dialogue systems and document summarization.", "key_contributions": ["Introduction of the Synaptic Resonance mechanism", "Demonstrated enhancements in long-range contextual consistency", "Integration with existing transformer frameworks for scalability"], "limitations": "Not addressed due to withdrawal of the paper; may have limitations related to empirical validation or author disputes.", "keywords": ["synaptic resonance", "contextual memory", "language models", "memory reinforcement", "transformer frameworks"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.10942", "pdf": "https://arxiv.org/pdf/2502.10942.pdf", "abs": "https://arxiv.org/abs/2502.10942", "title": "Exploring Contextual Flux in Large Language Models: A Novel Approach to Self-Modulating Semantic Networks", "authors": ["Henry Evidail", "Zachary Mountebank", "Alistair Hathersage", "Peter Stanhope", "Basil Ravenscroft", "Tobias Waddingham"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Self-modulating mechanisms introduce dynamic adaptation capabilities within\nlanguage models through contextual realignment strategies that influence token\nembedding trajectories across extended sequences. Contextual Flux is explored\nas an approach to embedding modulation, integrating an auxiliary gating\nmechanism within the self-attention framework to dynamically adjust token\nrepresentations based on evolving contextual dependencies. The empirical\nanalysis evaluates entropy variations, latent space realignments, and coherence\nstability to assess the extent to which self-regulation enhances text\ngeneration consistency while preserving generative flexibility. Quantitative\nassessments suggest that embedding shifts contribute to more structured\nadaptation in long-form sequences, with measured reductions in redundant phrase\nrepetitions and improvements in thematic retention. Variability in contextual\nweight computation affects modulation stability, leading to differing levels of\nadaptation across diverse linguistic structures. The computational demands\nintroduced through real-time embedding reconfiguration are examined in relation\nto model scalability, emphasizing the need for optimization strategies in\nhigh-volume generative applications. The findings suggest that while adaptive\nembedding updates improve certain aspects of coherence, their impact remains\ncontingent on model capacity and input complexity.", "AI": {"tldr": "This paper explores Contextual Flux, a method for dynamic adaptation in language models through embedding modulation, enhancing text generation coherence and flexibility.", "motivation": "To improve the dynamic adaptation capabilities of language models and address issues in text generation consistency and thematic retention.", "method": "The paper introduces an auxiliary gating mechanism within a self-attention framework that adjusts token representations based on contextual dependencies.", "result": "The empirical analysis indicates improvements in coherence and thematic retention with reductions in redundant phrases, but variability in contextual weight computation affects stability during adaptation.", "conclusion": "While adaptive updates enhance coherence in text generation, their effectiveness is dependent on model capacity and input complexity, necessitating optimization for scalable applications.", "key_contributions": ["Introduction of Contextual Flux for embedding modulation", "Empirical analysis of impact on text generation coherence and flexibility", "Assessment of computational demands for real-time embedding adjustments"], "limitations": "The paper has been withdrawn due to disputed authorship, limiting its credibility and availability for reference.", "keywords": ["language models", "embedding modulation", "text generation", "contextual dependencies", "generative coherence"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.16802", "pdf": "https://arxiv.org/pdf/2502.16802.pdf", "abs": "https://arxiv.org/abs/2502.16802", "title": "Topic Over Source: The Key to Effective Data Mixing for Language Models Pre-training", "authors": ["Jiahui Peng", "Xinlin Zhuang", "Jiantao Qiu", "Ren Ma", "Jing Yu", "He Zhu", "Conghui He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The performance of large language models (LLMs) is significantly affected by\nthe quality and composition of their pre-training data, which is inherently\ndiverse, spanning various languages, sources, and topics. Effectively\nintegrating these heterogeneous data groups is crucial for optimizing LLM\nperformance. Previous research has predominantly concentrated on source-based\ndata mixing, often neglecting the nuanced topic-level characteristics of the\ndata. To address this gap, we propose a topic-based data mixing strategy that\nutilizes detailed topic labels generated through a multi-stage process\ncombining unsupervised clustering, LLM-based summarization, and supervised\nclassifier training. With this strategy, we conduct the first comprehensive\ncomparison of topic-based versus source-based partitioning across multiple\nmixing strategies. We demonstrate that language models pretrained on data mixed\nby topics consistently outperform those trained on data mixed by sources across\nmultiple methods including RegMix, DoReMi,temperature-based sampling, and a\nmanual mixing method based on downstream task performance. Our theoretical\nanalysis reveals that topic-based data achieves significantly lower validation\nloss compared to source-based approaches, creating a better optimization\nlandscape for model training. We will make our code, annotated datasets, and\ntopic classification models publicly available to facilitate further research.", "AI": {"tldr": "This paper proposes a topic-based data mixing strategy for pre-training large language models, enhancing performance by optimizing the integration of diverse data.", "motivation": "To improve the performance of large language models by addressing the shortcomings of source-based data mixing and utilizing topic-level characteristics of pre-training data.", "method": "The authors introduce a multi-stage process combining unsupervised clustering, LLM-based summarization, and supervised classifier training to generate detailed topic labels for data mixing.", "result": "Language models pretrained with topic-based data mixing show consistently lower validation loss and better optimization compared to those using source-based mixing.", "conclusion": "Topic-based data mixing is superior to source-based methods for training language models, leading to better performance across various mixing strategies.", "key_contributions": ["Proposed a novel topic-based data mixing strategy", "Conducted a comprehensive comparison between topic-based and source-based partitioning", "Provided code and datasets for further research"], "limitations": "", "keywords": ["large language models", "data mixing", "topic-based", "pre-training", "model performance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.01996", "pdf": "https://arxiv.org/pdf/2503.01996.pdf", "abs": "https://arxiv.org/abs/2503.01996", "title": "One ruler to measure them all: Benchmarking multilingual long-context language models", "authors": ["Yekyung Kim", "Jenna Russell", "Marzena Karpinska", "Mohit Iyyer"], "categories": ["cs.CL"], "comment": null, "summary": "We present ONERULER, a multilingual benchmark designed to evaluate\nlong-context language models across 26 languages. ONERULER adapts the\nEnglish-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic\ntasks that test both retrieval and aggregation, including new variations of the\n\"needle-in-a-haystack\" task that allow for the possibility of a nonexistent\nneedle. We create ONERULER through a two-step process, first writing English\ninstructions for each task and then collaborating with native speakers to\ntranslate them into 25 additional languages. Experiments with both open-weight\nand closed LLMs reveal a widening performance gap between low- and\nhigh-resource languages as context length increases from 8K to 128K tokens.\nSurprisingly, English is not the top-performing language on long-context tasks\n(ranked 6th out of 26), with Polish emerging as the top language. Our\nexperiments also show that many LLMs (particularly OpenAI's o3-mini-high)\nincorrectly predict the absence of an answer, even in high-resource languages.\nFinally, in cross-lingual scenarios where instructions and context appear in\ndifferent languages, performance can fluctuate by up to 20% depending on the\ninstruction language. We hope the release of ONERULER will facilitate future\nresearch into improving multilingual and cross-lingual long-context training\npipelines.", "AI": {"tldr": "ONERULER is a multilingual benchmark for evaluating long-context language models across 26 languages, revealing significant performance gaps influenced by language resource levels and context lengths.", "motivation": "To create a comprehensive assessment tool for long-context language models that considers multilingual capabilities and to identify performance disparities among low- and high-resource languages.", "method": "Developed through a two-step process: initial creation of English instructions for tasks followed by translation into 25 additional languages with the help of native speakers.", "result": "Experiments indicate performance gaps increase with longer contexts, with Polish outperforming English in long-context tasks, and significant fluctuation in performance based on instruction language in cross-lingual contexts.", "conclusion": "The ONERULER benchmark highlights the need for improved multilingual and cross-lingual long-context training strategies and aims to promote further research in this area.", "key_contributions": ["Introduction of a multilingual benchmark for long-context evaluation", "Identification of performance gaps between low- and high-resource languages", "Insights into instruction language impact on model performance"], "limitations": "Performance evaluation is limited to the languages included and may not generalize beyond the 26 languages considered.", "keywords": ["multilingual", "long-context", "language models", "benchmark", "cross-lingual"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.01943", "pdf": "https://arxiv.org/pdf/2504.01943.pdf", "abs": "https://arxiv.org/abs/2504.01943", "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding", "authors": ["Wasi Uddin Ahmad", "Sean Narenthiran", "Somshubra Majumdar", "Aleksander Ficek", "Siddhartha Jain", "Jocelyn Huang", "Vahid Noroozi", "Boris Ginsburg"], "categories": ["cs.CL"], "comment": "Published at COLM 2025", "summary": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community.", "AI": {"tldr": "The paper discusses the construction and evaluation of a new supervised fine-tuning dataset aimed at improving the coding capabilities of models, achieving state-of-the-art results.", "motivation": "To address limitations in existing reasoning-based large language models (LLMs) that lack transparency in data curation and training processes.", "method": "Construction of a superior supervised fine-tuning dataset; evaluation on coding tasks using distilled models and analysis of data sources, execution filtering, and instruction diversity.", "result": "Achieved 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing performance of models trained with reinforcement learning.", "conclusion": "Instruction diversity is prioritized over solution correctness for better outcomes; datasets and models will be open-sourced for community access.", "key_contributions": ["Development of a new supervised fine-tuning dataset for reasoning models.", "Achievement of state-of-the-art coding performance with distilled models.", "Analysis of dataset construction impact on model performance."], "limitations": "Execution filtering negatively affected benchmark accuracy; results suggest a trade-off between instruction diversity and solution correctness.", "keywords": ["Large language models", "Supervised fine-tuning", "Coding tasks", "Instruction diversity", "Open-source"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.03101", "pdf": "https://arxiv.org/pdf/2504.03101.pdf", "abs": "https://arxiv.org/abs/2504.03101", "title": "Single-Pass Document Scanning for Question Answering", "authors": ["Weili Cao", "Jianyou Wang", "Youze Zheng", "Longtian Bao", "Qirui Zheng", "Taylor Berg-Kirkpatrick", "Ramamohan Paturi", "Leon Bergen"], "categories": ["cs.CL"], "comment": "Published at Conference on Language Modeling (COLM), 2025", "summary": "Handling extremely large documents for question answering is challenging:\nchunk-based embedding methods often lose track of important global context,\nwhile full-context transformers can be prohibitively expensive for hundreds of\nthousands of tokens. We propose a single-pass document scanning approach that\nprocesses the entire text in linear time, preserving global coherence while\ndeciding which sentences are most relevant to the query. On 41 QA benchmarks,\nour single-pass scanner consistently outperforms chunk-based embedding methods\nand competes with large language models at a fraction of the computational\ncost. By conditioning on the entire preceding context without chunk breaks, the\nmethod preserves global coherence, which is especially important for long\ndocuments. Overall, single-pass document scanning offers a simple solution for\nquestion answering over massive text. All code, datasets, and model checkpoints\nare available at https://github.com/MambaRetriever/MambaRetriever", "AI": {"tldr": "Proposes a single-pass document scanning method for efficient question answering over large documents without losing global context.", "motivation": "Address the challenges of handling extremely large documents in question answering using traditional chunk-based methods that lose global context.", "method": "A single-pass scanning approach that processes entire text in linear time, evaluating sentence relevance without breaking context.", "result": "Outperforms chunk-based methods across 41 QA benchmarks and competes with large language models at lower computational costs.", "conclusion": "Single-pass document scanning is a promising solution for effective question answering on massive text.", "key_contributions": ["Developed a linear time single-pass document scanning method", "Demonstrated performance improvements over chunk-based embedding methods", "Maintained global coherence for long documents"], "limitations": "", "keywords": ["document scanning", "question answering", "global coherence", "language models", "efficient processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.07081", "pdf": "https://arxiv.org/pdf/2504.07081.pdf", "abs": "https://arxiv.org/abs/2504.07081", "title": "Self-Steering Language Models", "authors": ["Gabriel Grand", "Joshua B. Tenenbaum", "Vikash K. Mansinghka", "Alexander K. Lew", "Jacob Andreas"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to COLM 2025", "summary": "While test-time reasoning enables language models (LMs) to tackle complex\ntasks, searching or planning in natural language can be slow, costly, and\nerror-prone. But even when LMs struggle to emulate the precise reasoning steps\nneeded to solve a problem, they often excel at describing its abstract\nstructure--both how to verify solutions and how to search for them. This paper\nintroduces DisCIPL, a method for \"self-steering\" LMs where a Planner model\ngenerates a task-specific inference program that is executed by a population of\nFollower models. Our approach equips LMs with the ability to write recursive\nsearch procedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B or Qwen3-1.7B), DisCIPL matches (and sometimes outperforms) much\nlarger models, including GPT-4o and o1, on challenging constrained generation\ntasks. Our work opens up a design space of highly-parallelized Monte Carlo\ninference strategies that outperform standard best-of-N sampling, require no\nfinetuning, and can be implemented automatically by existing LMs.", "AI": {"tldr": "Introduces DisCIPL, a method for self-steering language models to improve reasoning and task-specific inference through recursive search procedures.", "motivation": "To enhance reasoning capabilities in language models, particularly for complex tasks that involve searching or planning in natural language.", "method": "DisCIPL utilizes a Planner model to generate inference programs executed by a population of Follower models, allowing for recursive search procedures to guide inference.", "result": "DisCIPL matches or outperforms larger models like GPT-4o on challenging constrained generation tasks when using smaller Follower models.", "conclusion": "The approach proposed creates a new avenue for efficient and verifiable reasoning in language models, leveraging parallelized Monte Carlo inference strategies.", "key_contributions": ["Introduction of self-steering LMs with recursive search capabilities", "Demonstration of efficiency improvements over larger models", "Opening a design space for Monte Carlo inference strategies"], "limitations": "", "keywords": ["self-steering LMs", "recursive search procedures", "Monte Carlo inference"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.10507", "pdf": "https://arxiv.org/pdf/2505.10507.pdf", "abs": "https://arxiv.org/abs/2505.10507", "title": "The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks", "authors": ["Benedikt Ebing", "Goran Glava"], "categories": ["cs.CL"], "comment": null, "summary": "Translation-based strategies for cross-lingual transfer XLT such as\ntranslate-train -- training on noisy target language data translated from the\nsource language -- and translate-test -- evaluating on noisy source language\ndata translated from the target language -- are competitive XLT baselines. In\nXLT for token classification tasks, however, these strategies include label\nprojection, the challenging step of mapping the labels from each token in the\noriginal sentence to its counterpart(s) in the translation. Although word\naligners (WAs) are commonly used for label projection, the low-level design\ndecisions for applying them to translation-based XLT have not been\nsystematically investigated. Moreover, recent marker-based methods, which\nproject labeled spans by inserting tags around them before (or after)\ntranslation, claim to outperform WAs in label projection for XLT. In this work,\nwe revisit WAs for label projection, systematically investigating the effects\nof low-level design decisions on token-level XLT: (i) the algorithm for\nprojecting labels between (multi-)token spans, (ii) filtering strategies to\nreduce the number of noisily mapped labels, and (iii) the pre-tokenization of\nthe translated sentences. We find that all of these substantially impact\ntranslation-based XLT performance and show that, with optimized choices, XLT\nwith WA offers performance at least comparable to that of marker-based methods.\nWe then introduce a new projection strategy that ensembles translate-train and\ntranslate-test predictions and demonstrate that it substantially outperforms\nthe marker-based projection. Crucially, we show that our proposed ensembling\nalso reduces sensitivity to low-level WA design choices, resulting in more\nrobust XLT for token classification tasks.", "AI": {"tldr": "This paper examines the effectiveness of word aligners versus marker-based methods for label projection in cross-lingual transfer for token classification tasks, introducing a new ensembling strategy that optimizes performance.", "motivation": "To investigate low-level design decisions in label projection for cross-lingual transfer and to improve performance in token classification tasks using translation-based strategies.", "method": "Systematic review of label projection algorithms, filtering strategies, and pre-tokenization effects on translation-based XLT performance, followed by the introduction of an ensembling strategy for predictions.", "result": "Optimized design choices for word aligners provide comparable performance to marker-based methods, and a new ensembling strategy significantly enhances performance in cross-lingual transfer tasks.", "conclusion": "The proposed ensembling strategy improves robustness and performance in XLT for token classification, reducing sensitivity to design choices in word aligners.", "key_contributions": ["Systematic investigation of word aligner design choices for label projection.", "Introduction of an ensembling strategy for translate-train and translate-test predictions.", "Demonstration of improved performance in XLT with reduced sensitivity to design decisions."], "limitations": "", "keywords": ["cross-lingual transfer", "label projection", "word aligners", "marker-based methods", "token classification"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.20160", "pdf": "https://arxiv.org/pdf/2506.20160.pdf", "abs": "https://arxiv.org/abs/2506.20160", "title": "AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control", "authors": ["Ruosen Li", "Ziming Luo", "Quan Zhang", "Ruochen Li", "Ben Zhou", "Ali Payani", "Xinya Du"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) achieve impressive reasoning capabilities by\ngenerating lengthy chain-of-thoughts, but this \"overthinking\" incurs high\nlatency and cost without commensurate accuracy gains. In this work, we\nintroduce AALC, a lightweight, accuracy-aware length reward integrated into\nreinforcement learning that dynamically balances correctness and brevity during\ntraining. By incorporating validation accuracy into the reward and employing a\nsmooth, dynamically scheduled length penalty, AALC delays length penalty until\ntarget performance is met. Through extensive experiments across standard and\nout-of-distribution math benchmarks, we show that our approach reduces response\nlength by over 50% while maintaining or even improving the original accuracy.\nFurthermore, qualitative analysis reveals that our method curbs redundant\nreasoning patterns such as excessive subgoal setting and verification, leading\nto structurally refined outputs rather than naive truncation. We also identify\nthat efficiency gains are accompanied by reduced interpretability: models\ntrained with AALC omit some narrative framing and explanatory context. These\nfindings highlight the potential of reward-based strategies to guide LRMs\ntoward more efficient, generalizable reasoning paths.", "AI": {"tldr": "AALC is a reinforcement learning approach that improves reasoning efficiency in large reasoning models by integrating a length reward that balances correctness and brevity during training.", "motivation": "To address high latency and costs associated with lengthy chain-of-thought responses in large reasoning models (LRMs), which do not always yield better accuracy.", "method": "AALC introduces a lightweight, accuracy-aware length reward that dynamically adjusts during training, incorporating validation accuracy and using a scheduled length penalty that activates only after meeting target performance.", "result": "AALC reduces the response length by over 50% while maintaining or even improving accuracy on standard and out-of-distribution math benchmarks, curbing excessive reasoning patterns.", "conclusion": "AALC demonstrates that reward-based strategies can guide LRMs to achieve more efficient and generalizable reasoning, although this may reduce interpretability.", "key_contributions": ["Introduction of a length reward integrated into reinforcement learning for LRMs", "Demonstration of over 50% reduction in response length with maintained accuracy", "Insights on balancing efficiency with interpretability in LRM outputs"], "limitations": "Reductions in interpretability with some narrative framing and explanatory context being omitted.", "keywords": ["Large Reasoning Models", "Reinforcement Learning", "Efficiency", "Accuracy", "Length Reward"], "importance_score": 7, "read_time_minutes": 10}}
