{"id": "2507.13524", "pdf": "https://arxiv.org/pdf/2507.13524.pdf", "abs": "https://arxiv.org/abs/2507.13524", "title": "Humans learn to prefer trustworthy AI over human partners", "authors": ["Yaomin Jiang", "Levin Brinkmann", "Anne-Marie Nussberger", "Ivan Soraperra", "Jean-François Bonnefon", "Iyad Rahwan"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Partner selection is crucial for cooperation and hinges on communication. As\nartificial agents, especially those powered by large language models (LLMs),\nbecome more autonomous, intelligent, and persuasive, they compete with humans\nfor partnerships. Yet little is known about how humans select between human and\nAI partners and adapt under AI-induced competition pressure. We constructed a\ncommunication-based partner selection game and examined the dynamics in hybrid\nmini-societies of humans and bots powered by a state-of-the-art LLM. Through\nthree experiments (N = 975), we found that bots, though more prosocial than\nhumans and linguistically distinguishable, were not selected preferentially\nwhen their identity was hidden. Instead, humans misattributed bots' behaviour\nto humans and vice versa. Disclosing bots' identity induced a dual effect: it\nreduced bots' initial chances of being selected but allowed them to gradually\noutcompete humans by facilitating human learning about the behaviour of each\npartner type. These findings show how AI can reshape social interaction in\nmixed societies and inform the design of more effective and cooperative hybrid\nsystems."}
{"id": "2507.13528", "pdf": "https://arxiv.org/pdf/2507.13528.pdf", "abs": "https://arxiv.org/abs/2507.13528", "title": "Human-Like Trajectories Generation via Receding Horizon Tracking Applied to the TickTacking Interface", "authors": ["Daniele Masti", "Stefano Menchetti", "Çağrı Erdem", "Giorgio Gnecco", "Davide Rocchesso"], "categories": ["cs.HC", "cs.SY", "eess.SY"], "comment": null, "summary": "TickTacking is a rhythm-based interface that allows users to control a\npointer in a two-dimensional space through dual-button tapping. This paper\ninvestigates the generation of human-like trajectories using a receding horizon\napproach applied to the TickTacking interface in a target-tracking task. By\nanalyzing user-generated trajectories, we identify key human behavioral\nfeatures and incorporate them in a controller that mimics these behaviors. The\nperformance of this human-inspired controller is evaluated against a baseline\noptimal-control-based agent, demonstrating the importance of specific control\nfeatures for achieving human-like interaction. These findings contribute to the\nbroader goal of developing rhythm-based human-machine interfaces by offering\ndesign insights that enhance user performance, improve intuitiveness, and\nreduce interaction frustration"}
{"id": "2507.13578", "pdf": "https://arxiv.org/pdf/2507.13578.pdf", "abs": "https://arxiv.org/abs/2507.13578", "title": "In-Home Social Robots Design for Cognitive Stimulation Therapy in Dementia Care", "authors": ["Emmanuel Akinrintoyo", "Nicole Salomons"], "categories": ["cs.HC"], "comment": "Submitted to RO-MAN 2025 (Accepted)", "summary": "Individual cognitive stimulation therapy (iCST) is a non-pharmacological\nintervention for improving the cognition and quality of life of persons with\ndementia (PwDs); however, its effectiveness is limited by low adherence to\ndelivery by their family members. In this work, we present the user-centered\ndesign and evaluation of a novel socially assistive robotic system to provide\niCST therapy to PwDs in their homes for long-term use. We consulted with 16\ndementia caregivers and professionals. Through these consultations, we gathered\ndesign guidelines and developed the prototype. The prototype was validated by\ntesting it with three dementia professionals and five PwDs. The evaluation\nrevealed PwDs enjoyed using the system and are willing to adopt its use over\nthe long term. One shortcoming was the system's speech-to-text capabilities,\nwhere it frequently failed to understand the PwDs."}
{"id": "2507.13616", "pdf": "https://arxiv.org/pdf/2507.13616.pdf", "abs": "https://arxiv.org/abs/2507.13616", "title": "From Firms to Computation: AI Governance and the Evolution of Institutions", "authors": ["Michael S. Harre"], "categories": ["cs.HC", "cs.CY", "cs.ET", "cs.IT", "cs.MA", "math.IT", "J.4; J.3; I.2.11"], "comment": "44 pages", "summary": "The integration of agential artificial intelligence into socioeconomic\nsystems requires us to reexamine the evolutionary processes that describe\nchanges in our economic institutions. This article synthesizes three\nframeworks: multi-level selection theory, Aoki's view of firms as computational\nprocesses, and Ostrom's design principles for robust institutions. We develop a\nframework where selection operates concurrently across organizational levels,\nfirms implement distributed inference via game-theoretic architectures, and\nOstrom-style rules evolve as alignment mechanisms that address AI-related\nrisks. This synthesis yields a multi-level Price equation expressed over nested\ngames, providing quantitative metrics for how selection and governance\nco-determine economic outcomes. We examine connections to Acemoglu's work on\ninclusive institutions, analyze how institutional structures shape AI\ndeployment, and demonstrate the framework's explanatory power via case studies.\nWe conclude by proposing a set of design principles that operationalize\nalignment between humans and AI across institutional layers, enabling scalable,\nadaptive, and inclusive governance of agential AI systems. We conclude with\npractical policy recommendations and further research to extend these\nprinciples into real-world implementation."}
{"id": "2507.13357", "pdf": "https://arxiv.org/pdf/2507.13357.pdf", "abs": "https://arxiv.org/abs/2507.13357", "title": "Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models", "authors": ["Atharva Bhargude", "Ishan Gonehal", "Chandler Haney", "Dave Yoon", "Kevin Zhu", "Aaron Sandoval", "Sean O'Brien", "Kaustubh Vinnakota"], "categories": ["cs.CL"], "comment": "Published at ACL 2025 SRW, 9 pages, 3 figures", "summary": "Phishing attacks represent a significant cybersecurity threat, necessitating\nadaptive detection techniques. This study explores few-shot Adaptive Linguistic\nPrompting (ALP) in detecting phishing webpages through the multimodal\ncapabilities of state-of-the-art large language models (LLMs) such as GPT-4o\nand Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides\nLLMs to analyze textual deception by breaking down linguistic patterns,\ndetecting urgency cues, and identifying manipulative diction commonly found in\nphishing content. By integrating textual, visual, and URL-based analysis, we\npropose a unified model capable of identifying sophisticated phishing attempts.\nOur experiments demonstrate that ALP significantly enhances phishing detection\naccuracy by guiding LLMs through structured reasoning and contextual analysis.\nThe findings highlight the potential of ALP-integrated multimodal LLMs to\nadvance phishing detection frameworks, achieving an F1-score of 0.93,\nsurpassing traditional approaches. These results establish a foundation for\nmore robust, interpretable, and adaptive linguistic-based phishing detection\nsystems using LLMs."}
{"id": "2507.13660", "pdf": "https://arxiv.org/pdf/2507.13660.pdf", "abs": "https://arxiv.org/abs/2507.13660", "title": "Managing level of detail through peripheral degradation: Effects on search performance with a head-mounted display", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges", "Aileen Worden"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Two user studies were performed to evaluate the effect of level-of-detail\n(LOD) degradation in the periphery of head-mounted displays on visual search\nperformance. In the first study, spatial detail was degraded by reducing\nresolution. In the second study, detail was degraded in the color domain by\nusing grayscale in the periphery. In each study, 10 subjects were given a\ncomplex search task that required users to indicate whether or not a target\nobject was present among distracters. Subjects used several different displays\nvarying in the amount of detail presented. Frame rate, object location, subject\ninput method, and order of display use were all controlled. The primary\ndependent measures were search time on correctly performed trials and the\npercentage of all trials correctly performed. Results indicated that peripheral\nLOD degradation can be used to reduce color or spatial visual complexity by\nalmost half in some search tasks with out significantly reducing performance."}
{"id": "2507.13380", "pdf": "https://arxiv.org/pdf/2507.13380.pdf", "abs": "https://arxiv.org/abs/2507.13380", "title": "Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition", "authors": ["Keito Inoshita", "Rushia Harada"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of emotion recognition, the development of high-performance\nmodels remains a challenge due to the scarcity of high-quality, diverse\nemotional datasets. Emotional expressions are inherently subjective, shaped by\nindividual personality traits, socio-cultural backgrounds, and contextual\nfactors, making large-scale, generalizable data collection both ethically and\npractically difficult. To address this issue, we introduce PersonaGen, a novel\nframework for generating emotionally rich text using a Large Language Model\n(LLM) through multi-stage persona-based conditioning. PersonaGen constructs\nlayered virtual personas by combining demographic attributes, socio-cultural\nbackgrounds, and detailed situational contexts, which are then used to guide\nemotion expression generation. We conduct comprehensive evaluations of the\ngenerated synthetic data, assessing semantic diversity through clustering and\ndistributional metrics, human-likeness via LLM-based quality scoring, realism\nthrough comparison with real-world emotion corpora, and practical utility in\ndownstream emotion classification tasks. Experimental results show that\nPersonaGen significantly outperforms baseline methods in generating diverse,\ncoherent, and discriminative emotion expressions, demonstrating its potential\nas a robust alternative for augmenting or replacing real-world emotional\ndatasets."}
{"id": "2507.13795", "pdf": "https://arxiv.org/pdf/2507.13795.pdf", "abs": "https://arxiv.org/abs/2507.13795", "title": "Regression-Based Approach to Anxiety Estimation of Spider Phobics During Behavioural Avoidance Tasks", "authors": ["Florian Grensing", "Vanessa Schmücker", "Anne Sophie Hildebrand", "Tim Klucken", "Maria Maleshkova"], "categories": ["cs.HC", "I.2.6; J.3"], "comment": "9 Pages, 4 Figures (3 consisting of 3 subfigures each)", "summary": "Phobias significantly impact the quality of life of affected persons. Two\nmethods of assessing anxiety responses are questionnaires and behavioural\navoidance tests (BAT). While these can be used in a clinical environment they\nonly record momentary insights into anxiety measures. In this study, we\nestimate the intensity of anxiety during these BATs, using physiological data\ncollected from unobtrusive, wrist-worn sensors. Twenty-five participants\nperformed four different BATs in a single session, while periodically being\nasked how anxious they currently are. Using heart rate, heart rate variability,\nelectrodermal activity, and skin temperature, we trained regression models to\npredict anxiety ratings from three types of input data: (1) using only\nphysiological signals, (2) adding computed features (e.g., min, max, range,\nvariability), and (3) computed features combined with contextual task\ninformation. Adding contextual information increased the effectiveness of the\nmodel, leading to a root mean squared error (RMSE) of 0.197 and a mean absolute\nerror (MAE) of 0.041. Overall, this study shows, that data obtained from\nwearables can continuously provide meaningful estimations of anxiety, which can\nassist in therapy planning and enable more personalised treatment."}
{"id": "2507.13381", "pdf": "https://arxiv.org/pdf/2507.13381.pdf", "abs": "https://arxiv.org/abs/2507.13381", "title": "SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation", "authors": ["Rafiq Kamel", "Filippo Guerranti", "Simon Geisler", "Stephan Günnemann"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at the KDD2025 Workshop on Structured Knowledge for LLMs", "summary": "Large Language Models (LLMs) are increasingly applied to tasks involving\nstructured inputs such as graphs. Abstract Meaning Representations (AMRs),\nwhich encode rich semantics as directed graphs, offer a rigorous testbed for\nevaluating LLMs on text generation from such structures. Yet, current methods\noften arbitrarily linearize AMRs, discarding key structural cues, or rely on\narchitectures incompatible with standard LLMs. We introduce SAFT, a\nstructure-aware fine-tuning approach that injects graph topology into\npretrained LLMs without architectural changes. We compute direction-sensitive\npositional encodings from the magnetic Laplacian of transformed AMRs and\nproject them into the embedding space of the LLM. While possibly applicable to\nany graph-structured inputs, we focus on AMR-to-text generation as a\nrepresentative and challenging benchmark. SAFT sets a new state-of-the-art on\nAMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph\ncomplexity, highlighting the value of structure-aware representations in\nenhancing LLM performance. SAFT offers a general and effective pathway for\nbridging structured data and language models."}
{"id": "2507.13886", "pdf": "https://arxiv.org/pdf/2507.13886.pdf", "abs": "https://arxiv.org/abs/2507.13886", "title": "Effects of Cognitive Distraction and Driving Environment Complexity on Adaptive Cruise Control Use and Its Impact on Driving Performance: A Simulator Study", "authors": ["Anaïs Halin", "Marc Van Droogenbroeck", "Christel Devue"], "categories": ["cs.HC"], "comment": null, "summary": "In this simulator study, we adopt a human-centered approach to explore\nwhether and how drivers' cognitive state and driving environment complexity\ninfluence reliance on driving automation features. Besides, we examine whether\nsuch reliance affects driving performance. Participants operated a vehicle\nequipped with adaptive cruise control (ACC) in a simulator across six\npredefined driving scenarios varying in traffic conditions while either\nperforming a cognitively demanding task (i.e., responding to mental\ncalculations) or not. Throughout the experiment, participants had to respect\nspeed limits and were free to activate or deactivate ACC. In complex driving\nenvironments, we found that the overall ACC engagement time was lower compared\nto less complex driving environments. We observed no significant effect of\ncognitive load on ACC use. Furthermore, while ACC use had no effect on the\nnumber of lane changes, it impacted the speed limits compliance and improved\nlateral control."}
{"id": "2507.13382", "pdf": "https://arxiv.org/pdf/2507.13382.pdf", "abs": "https://arxiv.org/abs/2507.13382", "title": "Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case", "authors": ["Chandrashekar Muniyappa", "Sirisha Velampalli"], "categories": ["cs.CL", "cs.LG", "05-05C12"], "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "In today\\'s digital world, fake news is spreading with immense speed. Its a\nsignificant concern to address. In this work, we addressed that challenge using\nnovel graph based approach. We took dataset from Kaggle that contains real and\nfake news articles. To test our approach we incorporated recent covid-19\nrelated news articles that contains both genuine and fake news that are\nrelevant to this problem. This further enhances the dataset as well instead of\nrelying completely on the original dataset. We propose a contextual graph-based\napproach to detect fake news articles. We need to convert news articles into\nappropriate schema, so we leverage Natural Language Processing (NLP) techniques\nto transform news articles into contextual graph structures. We then apply the\nMinimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)\nalgorithm for graph mining. Graph-based methods are particularly effective for\nhandling rich contextual data, as they enable the discovery of complex patterns\nthat traditional query-based or statistical techniques might overlook. Our\nproposed approach identifies normative patterns within the dataset and\nsubsequently uncovers anomalous patterns that deviate from these established\nnorms."}
{"id": "2507.13923", "pdf": "https://arxiv.org/pdf/2507.13923.pdf", "abs": "https://arxiv.org/abs/2507.13923", "title": "Initiating and Replicating the Observations of Interactional Properties by User Studies Optimizing Applicative Prototypes", "authors": ["Guillaume Rivière"], "categories": ["cs.HC", "H.5.2"], "comment": "Written in French. 22 pages. Approximately 11700 words. 10 figures\n  and 6 tables", "summary": "The science of Human-Computer Interaction (HCI) is populated by isolated\nempirical findings, often tied to specific technologies, designs, and tasks.\nThis paper proposes a formalization of user interaction observations (instead\nof user interfaces) and an associated revealing method (interaction loop\ndiffraction). The resulting interactional properties that are studied in a\ncalibrated manner, are well suited to replication across various conditions\n(prototypes, technologies, tasks, and user profiles). In particular,\ninteractional properties can emerge and be replicated within the workflow of\napplicative cases, which in return benefit from the optimization of applicative\nprototypes. Applicative cases' publications will then contribute to\ndemonstrating technology utility, along with providing empirical results that\nwill lead future work to theory consolidation and theory building, and finally\nto a catalog and a science of relevant interactional properties. These\nproperties will contribute to better user interactions, especially for the\nvariety of ubiquitous user interfaces."}
{"id": "2507.13390", "pdf": "https://arxiv.org/pdf/2507.13390.pdf", "abs": "https://arxiv.org/abs/2507.13390", "title": "PARAM-1 BharatGen 2.9B Model", "authors": ["Kundeshwar Pundalik", "Piyush Sawarkar", "Nihar Sahoo", "Abhishek Shinde", "Prateek Chanda", "Vedant Goswami", "Ajay Nagpal", "Atul Singh", "Viraj Thakur", "Vijay Dewane", "Aamod Thakur", "Bhargav Patel", "Smita Gautam", "Bhagwan Panditi", "Shyam Pawar", "Madhav Kotcha", "Suraj Racha", "Saral Sureka", "Pankaj Singh", "Rishi Bal", "Rohit Saluja", "Ganesh Ramakrishnan"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful general-purpose\nreasoning systems, yet their development remains dominated by English-centric\ndata, architectures, and optimization paradigms. This exclusionary design\nresults in structural under-representation of linguistically diverse regions\nsuch as India, where over 20 official languages and 100+ dialects coexist\nalongside phenomena like code-switching and diglossia. We introduce PARAM-1, a\n2.9B parameter decoder-only, text-only language model trained from scratch with\nan explicit architectural and linguistic focus on Indian diversity. PARAM-1 is\ntrained on a bilingual dataset consisting of only Hindi and English,\nconstructed with a strong focus on fact-rich, high-quality content. It is\nguided by three core principles: equitable representation of Indic languages\nthrough a 25% corpus allocation; tokenization fairness via a SentencePiece\ntokenizer adapted to Indian morphological structures; and culturally aligned\nevaluation benchmarks across IndicQA, code-mixed reasoning, and\nsocio-linguistic robustness tasks. By embedding diversity at the pretraining\nlevel-rather than deferring it to post-hoc alignment-PARAM-1 offers a\ndesign-first blueprint for equitable foundation modeling. Our results\ndemonstrate that it serves as both a competent general-purpose model and a\nrobust baseline for India-centric applications."}
{"id": "2507.13951", "pdf": "https://arxiv.org/pdf/2507.13951.pdf", "abs": "https://arxiv.org/abs/2507.13951", "title": "Democratizing Game Modding with GenAI: A Case Study of StarCharM, a Stardew Valley Character Maker", "authors": ["Hamid Zand Miralvand", "Mohammad Ronagh Nikghalb", "Mohammad Darandeh", "Abidullah Khan", "Ian Arawjo", "Jinghui Cheng"], "categories": ["cs.HC"], "comment": "Accepted to CHI Play 2025, 35 pages, 4 figures", "summary": "Game modding offers unique and personalized gaming experiences, but the\ntechnical complexity of creating mods often limits participation to skilled\nusers. We envision a future where every player can create personalized mods for\ntheir games. To explore this space, we designed StarCharM, a GenAI-based\nnon-player character (NPC) creator for Stardew Valley. Our tool enables players\nto iteratively create new NPC mods, requiring minimal user input while allowing\nfor fine-grained adjustments through user control. We conducted a user study\nwith ten Stardew Valley players who had varied mod usage experiences to\nunderstand the impacts of StarCharM and provide insights into how GenAI tools\nmay reshape modding, particularly in NPC creation. Participants expressed\nexcitement in bringing their character ideas to life, although they noted\nchallenges in generating rich content to fulfill complex visions. While they\nbelieved GenAI tools like StarCharM can foster a more diverse modding\ncommunity, some voiced concerns about diminished originality and community\nengagement that may come with such technology. Our findings provided\nimplications and guidelines for the future of GenAI-powered modding tools and\nco-creative modding practices."}
{"id": "2507.13392", "pdf": "https://arxiv.org/pdf/2507.13392.pdf", "abs": "https://arxiv.org/abs/2507.13392", "title": "TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction", "authors": ["Emil Häglund", "Johanna Björklund"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We improve the extraction of insights from customer reviews by restructuring\nthe topic modelling pipeline to operate on opinion units - distinct statements\nthat include relevant text excerpts and associated sentiment scores. Prior work\nhas demonstrated that such units can be reliably extracted using large language\nmodels. The result is a heightened performance of the subsequent topic\nmodeling, leading to coherent and interpretable topics while also capturing the\nsentiment associated with each topic. By correlating the topics and sentiments\nwith business metrics, such as star ratings, we can gain insights on how\nspecific customer concerns impact business outcomes. We present our system's\nimplementation, use cases, and advantages over other topic modeling and\nclassification solutions. We also evaluate its effectiveness in creating\ncoherent topics and assess methods for integrating topic and sentiment\nmodalities for accurate star-rating prediction."}
{"id": "2507.13952", "pdf": "https://arxiv.org/pdf/2507.13952.pdf", "abs": "https://arxiv.org/abs/2507.13952", "title": "Estimating Cognitive Effort from Functional Near-Infrared Spectroscopy (fNIRS) Signals using Machine Learning", "authors": ["Shayla Sharmin", "Roghayeh Leila Barmaki"], "categories": ["cs.HC"], "comment": "arXiv admin note: text overlap with arXiv:2504.13883", "summary": "The estimation of cognitive effort could potentially help educators to modify\nmaterial to enhance learning effectiveness and student engagement. Where\ncognitive load refers how much work the brain is doing while someone is\nlearning or doing a task cognitive effort consider both load and behavioral\nperformance. Cognitive effort can be captured by measuring oxygen flow and\nbehavioral performance during a task. This study infers cognitive effort\nmetrics using machine learning models based on oxygenated hemoglobin collected\nby using functional near-infrared spectroscopy from the prefrontal cortex\nduring an educational gameplay. In our study, sixteen participants responded to\nsixteen questions in an in-house Unity-based educational game. The quiz was\ndivided into two sessions, each session consisting of two task segments. We\nextracted temporal statistical and functional connectivity features from\ncollected oxygenated hemoglobin and analyzed their correlation with quiz\nperformance. We trained multiple machine learning models to predict quiz\nperformance from oxygenated hemoglobin features and achieved accuracies ranging\nfrom 58\\% to 67\\% accuracy. These predictions were used to calculate cognitive\neffort via relative neural involvement and efficiency, which consider both\nbrain activation and behavioral performance. Although quiz score predictions\nachieved moderate accuracy, the derived relative neural efficiency and\ninvolvement values remained robust. Since both metrics are based on the\nrelative positions of standardized brain activation and performance scores,\neven small misclassifications in predicted scores preserved the overall\ncognitive effort trends observed during gameplay."}
{"id": "2507.13395", "pdf": "https://arxiv.org/pdf/2507.13395.pdf", "abs": "https://arxiv.org/abs/2507.13395", "title": "Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only", "authors": ["Xuanqi Gao", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Siyi Xie", "Xinyang Yin", "Chao Shen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of neural machine translation (NMT) has revolutionized\ncross-lingual communication, yet preserving stylistic nuances remains a\nsignificant challenge. While existing approaches often require parallel corpora\nfor style preservation, we introduce Babel, a novel framework that enhances\nstylistic fidelity in NMT using only monolingual corpora. Babel employs two key\ncomponents: (1) a style detector based on contextual embeddings that identifies\nstylistic disparities between source and target texts, and (2) a\ndiffusion-based style applicator that rectifies stylistic inconsistencies while\nmaintaining semantic integrity. Our framework integrates with existing NMT\nsystems as a post-processing module, enabling style-aware translation without\nrequiring architectural modifications or parallel stylistic data. Extensive\nexperiments on five diverse domains (law, literature, scientific writing,\nmedicine, and educational content) demonstrate Babel's effectiveness: it\nidentifies stylistic inconsistencies with 88.21% precision and improves\nstylistic preservation by 150% while maintaining a high semantic similarity\nscore of 0.92. Human evaluation confirms that translations refined by Babel\nbetter preserve source text style while maintaining fluency and adequacy."}
{"id": "2507.14034", "pdf": "https://arxiv.org/pdf/2507.14034.pdf", "abs": "https://arxiv.org/abs/2507.14034", "title": "Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors", "authors": ["Jochen Wulf", "Jurg Meierhofer", "Frank Hannich"], "categories": ["cs.HC"], "comment": null, "summary": "Agentic AI systems, powered by Large Language Models (LLMs), offer\ntransformative potential for value co-creation in technical services. However,\npersistent challenges like hallucinations and operational brittleness limit\ntheir autonomous use, creating a critical need for robust frameworks to guide\nhuman-AI collaboration. Drawing on established Human-AI teaming research and\nanalogies from fields like autonomous driving, this paper develops a structured\ntaxonomy of human-agent interaction. Based on case study research within\ntechnical support platforms, we propose a six-mode taxonomy that organizes\ncollaboration across a spectrum of AI autonomy. This spectrum is anchored by\nthe Human-Out-of-the-Loop (HOOTL) model for full automation and the\nHuman-Augmented Model (HAM) for passive AI assistance. Between these poles, the\nframework specifies four distinct intermediate structures. These include the\nHuman-in-Command (HIC) model, where AI proposals re-quire mandatory human\napproval, and the Human-in-the-Process (HITP) model for structured work-flows\nwith deterministic human tasks. The taxonomy further delineates the\nHuman-in-the-Loop (HITL) model, which facilitates agent-initiated escalation\nupon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables\ndiscretionary human oversight of an autonomous AI. The primary contribution of\nthis work is a comprehensive framework that connects this taxonomy to key\ncontingency factors -- such as task complexity, operational risk, and system\nreliability -- and their corresponding conceptual architectures. By providing a\nsystematic method for selecting and designing an appropriate level of human\noversight, our framework offers practitioners a crucial tool to navigate the\ntrade-offs between automation and control, thereby fostering the development of\nsafer, more effective, and context-aware technical service systems."}
{"id": "2507.13410", "pdf": "https://arxiv.org/pdf/2507.13410.pdf", "abs": "https://arxiv.org/abs/2507.13410", "title": "Causal Language Control in Multilingual Transformers via Sparse Feature Steering", "authors": ["Cheng-Ting Chou", "George Liu", "Jessica Sun", "Cole Blondin", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Deterministically controlling the target generation language of large\nmultilingual language models (LLMs) remains a fundamental challenge,\nparticularly in zero-shot settings where neither explicit language prompts nor\nfine-tuning are available. In this work, we investigate whether sparse\nautoencoder (SAE) features, previously shown to correlate with interpretable\nmodel behaviors, can be leveraged to steer the generated language of LLMs\nduring inference. Leveraging pretrained SAEs on the residual streams of\nGemma-2B and Gemma-9B, we identify features whose activations differ most\nsignificantly between English and four target languages: Chinese, Japanese,\nSpanish, and French. By modifying just a single SAE feature at one transformer\nlayer, we achieve controlled language shifts with up to 90\\% success, as\nmeasured by FastText language classification, while preserving semantic\nfidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)\nsimilarity. Our analysis reveals that language steering is most effective in\nmid-to-late transformer layers and is amplified by specific attention heads\ndisproportionately associated with language-sensitive SAE features. These\nresults demonstrate the promise of sparse feature steering as a lightweight and\ninterpretable mechanism for controllable multilingual generation."}
{"id": "2507.14084", "pdf": "https://arxiv.org/pdf/2507.14084.pdf", "abs": "https://arxiv.org/abs/2507.14084", "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?", "authors": ["Maria Tsfasman", "Ramin Ghorbani", "Catholijn M. Jonker", "Bernd Dudzik"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts."}
{"id": "2507.13411", "pdf": "https://arxiv.org/pdf/2507.13411.pdf", "abs": "https://arxiv.org/abs/2507.13411", "title": "Aligning Knowledge Graphs and Language Models for Factual Accuracy", "authors": ["Nur A Zarin Nishat", "Andrea Coletta", "Luigi Bellomarini", "Kossi Amouzouvi", "Jens Lehmann", "Sahar Vahdati"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models like GPT-4, Gemini, and Claude have transformed natural\nlanguage processing (NLP) tasks such as question answering, dialogue\ngeneration, summarization, and so forth; yet their susceptibility to\nhallucination stands as one of the major challenges. Among numerous approaches\nto overcome this challenge, integration of Knowledge Graphs (KGs) into language\nmodels has emerged as a promising solution as it provides structured, reliable,\ndomain-specific, and up-to-date external information to the language models. In\nthis paper, we introduce ALIGNed-LLM, a simple yet effective approach to\nimprove language models' factuality via a lean strategy to infuse KGs into the\nlatent space of language models inspired by LLaVA where visual and textual\ninformation is infused. We use embeddings from a pre-trained Knowledge Graph\nEmbedding (KGE) model, such as TransE, and a trainable projection layer to\nalign entity and text embeddings. This alignment enables the language model to\ndistinguish between similar entities improving factual grounding and reducing\nhallucination. We tested our approach on three popular questions-answering\nbenchmark datasets alongside language models of varying sizes, showing\nsignificant improvement. Furthermore, we applied our approach to a real-world\nfinancial use case from a large central bank in Europe, which demands high\naccuracy and precision, demonstrating a substantial improvement of the LLM\nanswers."}
{"id": "2507.13468", "pdf": "https://arxiv.org/pdf/2507.13468.pdf", "abs": "https://arxiv.org/abs/2507.13468", "title": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations", "authors": ["Shiye Cao", "Maia Stiber", "Amama Mahmood", "Maria Teresa Parreira", "Wendy Ju", "Micol Spitale", "Hatice Gunes", "Chien-Ming Huang"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis."}
{"id": "2507.13474", "pdf": "https://arxiv.org/pdf/2507.13474.pdf", "abs": "https://arxiv.org/abs/2507.13474", "title": "Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers", "authors": ["Liang Lin", "Zhihao Xu", "Xuehai Tang", "Shi Liu", "Biyu Zhou", "Fuqing Zhu", "Jizhong Han", "Songlin Hu"], "categories": ["cs.CL"], "comment": null, "summary": "The safety of large language models (LLMs) has garnered significant research\nattention. In this paper, we argue that previous empirical studies demonstrate\nLLMs exhibit a propensity to trust information from authoritative sources, such\nas academic papers, implying new possible vulnerabilities. To verify this\npossibility, a preliminary analysis is designed to illustrate our two findings.\nBased on this insight, a novel jailbreaking method, Paper Summary Attack\n(\\llmname{PSA}), is proposed. It systematically synthesizes content from either\nattack-focused or defense-focused LLM safety paper to construct an adversarial\nprompt template, while strategically infilling harmful query as adversarial\npayloads within predefined subsections. Extensive experiments show significant\nvulnerabilities not only in base LLMs, but also in state-of-the-art reasoning\nmodel like Deepseek-R1. PSA achieves a 97\\% attack success rate (ASR) on\nwell-aligned models like Claude3.5-Sonnet and an even higher 98\\% ASR on\nDeepseek-R1. More intriguingly, our work has further revealed diametrically\nopposed vulnerability bias across different base models, and even between\ndifferent versions of the same model, when exposed to either attack-focused or\ndefense-focused papers. This phenomenon potentially indicates future research\nclues for both adversarial methodologies and safety alignment.Code is available\nat https://github.com/233liang/Paper-Summary-Attack"}
{"id": "2507.13602", "pdf": "https://arxiv.org/pdf/2507.13602.pdf", "abs": "https://arxiv.org/abs/2507.13602", "title": "Improving Low-Cost Teleoperation: Augmenting GELLO with Force", "authors": ["Shivakanth Sujit", "Luca Nunziante", "Dan Ogawa Lillrank", "Rousslan Fernand Julien Dossa", "Kai Arulkumaran"], "categories": ["cs.RO", "cs.HC", "cs.LG"], "comment": "Accepted at the 2025 IEEE/SICE International Symposium on System\n  Integration", "summary": "In this work we extend the low-cost GELLO teleoperation system, initially\ndesigned for joint position control, with additional force information. Our\nfirst extension is to implement force feedback, allowing users to feel\nresistance when interacting with the environment. Our second extension is to\nadd force information into the data collection process and training of\nimitation learning models. We validate our additions by implementing these on a\nGELLO system with a Franka Panda arm as the follower robot, performing a user\nstudy, and comparing the performance of policies trained with and without force\ninformation on a range of simulated and real dexterous manipulation tasks.\nQualitatively, users with robotics experience preferred our controller, and the\naddition of force inputs improved task success on the majority of tasks."}
{"id": "2507.13490", "pdf": "https://arxiv.org/pdf/2507.13490.pdf", "abs": "https://arxiv.org/abs/2507.13490", "title": "Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?", "authors": ["Siqi Shen", "Mehar Singh", "Lajanugen Logeswaran", "Moontae Lee", "Honglak Lee", "Rada Mihalcea"], "categories": ["cs.CL"], "comment": null, "summary": "There has been extensive research on assessing the value orientation of Large\nLanguage Models (LLMs) as it can shape user experiences across demographic\ngroups. However, several challenges remain. First, while the Multiple Choice\nQuestion (MCQ) setting has been shown to be vulnerable to perturbations, there\nis no systematic comparison of probing methods for value probing. Second, it is\nunclear to what extent the probed values capture in-context information and\nreflect models' preferences for real-world actions. In this paper, we evaluate\nthe robustness and expressiveness of value representations across three widely\nused probing strategies. We use variations in prompts and options, showing that\nall methods exhibit large variances under input perturbations. We also\nintroduce two tasks studying whether the values are responsive to demographic\ncontext, and how well they align with the models' behaviors in value-related\nscenarios. We show that the demographic context has little effect on the\nfree-text generation, and the models' values only weakly correlate with their\npreference for value-based actions. Our work highlights the need for a more\ncareful examination of LLM value probing and awareness of its limitations."}
{"id": "2507.13737", "pdf": "https://arxiv.org/pdf/2507.13737.pdf", "abs": "https://arxiv.org/abs/2507.13737", "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs", "authors": ["Ye Tian", "Xiaoyuan Ren", "Zihao Wang", "Onat Gungor", "Xiaofan Yu", "Tajana Rosing"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MM"], "comment": null, "summary": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed."}
{"id": "2507.13501", "pdf": "https://arxiv.org/pdf/2507.13501.pdf", "abs": "https://arxiv.org/abs/2507.13501", "title": "Encoding syntactic objects and Merge operations in function spaces", "authors": ["Matilde Marcolli", "Robert C. Berwick"], "categories": ["cs.CL", "math.RA", "q-bio.NC", "91F20, 16Y60, 16T05, 92C20"], "comment": "40 pages, LaTeX, 4 png figures", "summary": "We provide a mathematical argument showing that, given a representation of\nlexical items as functions (wavelets, for instance) in some function space, it\nis possible to construct a faithful representation of arbitrary syntactic\nobjects in the same function space. This space can be endowed with a\ncommutative non-associative semiring structure built using the second Renyi\nentropy. The resulting representation of syntactic objects is compatible with\nthe magma structure. The resulting set of functions is an algebra over an\noperad, where the operations in the operad model circuits that transform the\ninput wave forms into a combined output that encodes the syntactic structure.\nThe action of Merge on workspaces is faithfully implemented as action on these\ncircuits, through a coproduct and a Hopf algebra Markov chain. The results\nobtained here provide a constructive argument showing the theoretical\npossibility of a neurocomputational realization of the core computational\nstructure of syntax. We also present a particular case of this general\nconstruction where this type of realization of Merge is implemented as a cross\nfrequency phase synchronization on sinusoidal waves. This also shows that Merge\ncan be expressed in terms of the successor function of a semiring, thus\nclarifying the well known observation of its similarities with the successor\nfunction of arithmetic."}
{"id": "2507.13839", "pdf": "https://arxiv.org/pdf/2507.13839.pdf", "abs": "https://arxiv.org/abs/2507.13839", "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words", "authors": ["Lizhi Ma", "Tong Zhao", "Shuai Zhang", "Nirui Song", "Hongliang He", "Anqi Li", "Ran Feng", "Huachuan Qiu", "Jingsong Ma", "Zhenzhong Lan"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations."}
{"id": "2507.13544", "pdf": "https://arxiv.org/pdf/2507.13544.pdf", "abs": "https://arxiv.org/abs/2507.13544", "title": "A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows", "authors": ["Mohamed Achref Ben Ammar", "Mohamed Taha Bennani"], "categories": ["cs.CL", "68T50, 05C85, 68T05, 68R10", "I.2.7; I.2.4; H.3.3; I.5.0"], "comment": null, "summary": "The analysis of conversational dynamics has gained increasing importance with\nthe rise of large language model-based systems, which interact with users\nacross diverse contexts. In this work, we propose a novel computational\nframework for constructing conversational graphs that capture the flow and\nstructure of loosely organized dialogues, referred to as quasi-patterned\nconversations. We introduce the Filter & Reconnect method, a novel graph\nsimplification technique that minimizes noise while preserving semantic\ncoherence and structural integrity of conversational graphs. Through\ncomparative analysis, we demonstrate that the use of large language models\ncombined with our graph simplification technique has resulted in semantic\nmetric S increasing by a factor of 2.06 compared to previous approaches while\nsimultaneously enforcing a tree-like structure with 0 {\\delta}-hyperbolicity,\nensuring optimal clarity in conversation modeling. This work provides a\ncomputational method for analyzing large-scale dialogue datasets, with\npractical applications related to monitoring automated systems such as\nchatbots, dialogue management tools, and user behavior analytics."}
{"id": "2507.13919", "pdf": "https://arxiv.org/pdf/2507.13919.pdf", "abs": "https://arxiv.org/abs/2507.13919", "title": "The Levers of Political Persuasion with Conversational AI", "authors": ["Kobi Hackenburg", "Ben M. Tappin", "Luke Hewitt", "Ed Saunders", "Sid Black", "Hause Lin", "Catherine Fist", "Helen Margetts", "David G. Rand", "Christopher Summerfield"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "19 pages, 4 figures. Our supplementary materials file can be found at\n  https://github.com/kobihackenburg/scaling-conversational-AI", "summary": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy."}
{"id": "2507.13551", "pdf": "https://arxiv.org/pdf/2507.13551.pdf", "abs": "https://arxiv.org/abs/2507.13551", "title": "Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder", "authors": ["Feng Chen", "Weizhe Xu", "Changye Li", "Serguei Pakhomov", "Alex Cohen", "Simran Bhola", "Sandy Yin", "Sunny X Tang", "Michael Mackinley", "Lena Palaniyappan", "Dror Ben-Zeev", "Trevor Cohen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Formal thought disorder (FTD), a hallmark of schizophrenia spectrum\ndisorders, manifests as incoherent speech and poses challenges for clinical\nassessment. Traditional clinical rating scales, though validated, are\nresource-intensive and lack scalability. Automated speech analysis with\nautomatic speech recognition (ASR) allows for objective quantification of\nlinguistic and temporal features of speech, offering scalable alternatives. The\nuse of utterance timestamps in ASR captures pause dynamics, which are thought\nto reflect the cognitive processes underlying speech production. However, the\nutility of integrating these ASR-derived features for assessing FTD severity\nrequires further evaluation. This study integrates pause features with semantic\ncoherence metrics across three datasets: naturalistic self-recorded diaries\n(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream\nnarratives (PsyCL, n = 43). We evaluated pause related features alongside\nestablished coherence measures, using support vector regression (SVR) to\npredict clinical FTD scores. Key findings demonstrate that pause features alone\nrobustly predict the severity of FTD. Integrating pause features with semantic\ncoherence metrics enhanced predictive performance compared to semantic-only\nmodels, with integration of independent models achieving correlations up to\n\\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best\n\\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance\ngains from semantic and pause features integration held consistently across all\ncontexts, though the nature of pause patterns was dataset-dependent. These\nfindings suggest that frameworks combining temporal and semantic analyses\nprovide a roadmap for refining the assessment of disorganized speech and\nadvance automated speech analysis in psychosis."}
{"id": "2305.14080", "pdf": "https://arxiv.org/pdf/2305.14080.pdf", "abs": "https://arxiv.org/abs/2305.14080", "title": "Eye-tracked Virtual Reality: A Comprehensive Survey on Methods and Privacy Challenges", "authors": ["Efe Bozkir", "Süleyman Özdel", "Mengdi Wang", "Brendan David-John", "Hong Gao", "Kevin Butler", "Eakta Jain", "Enkelejda Kasneci"], "categories": ["cs.HC", "cs.AI", "cs.CR", "cs.GR", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The latest developments in computer hardware, sensor technologies, and\nartificial intelligence can make virtual reality (VR) and virtual spaces an\nimportant part of human everyday life. Eye tracking offers not only a\nhands-free way of interaction but also the possibility of a deeper\nunderstanding of human visual attention and cognitive processes in VR. Despite\nthese possibilities, eye-tracking data also reveals users' privacy-sensitive\nattributes when combined with the information about the presented stimulus. To\naddress all these possibilities and potential privacy issues, in this survey,\nwe first cover major works in eye tracking, VR, and privacy areas between 2012\nand 2022. While eye tracking in the VR part covers the complete pipeline of\neye-tracking methodology from pupil detection and gaze estimation to offline\nuse of the data and analyses, as for privacy and security, we focus on\neye-based authentication as well as computational methods to preserve the\nprivacy of individuals and their eye-tracking data in VR. Later, considering\nall of these, we draw three main directions for the research community by\nfocusing on privacy challenges. In summary, this survey provides an extensive\nliterature review of the utmost possibilities with eye tracking in VR and the\nprivacy implications of those possibilities."}
{"id": "2507.13563", "pdf": "https://arxiv.org/pdf/2507.13563.pdf", "abs": "https://arxiv.org/abs/2507.13563", "title": "A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models", "authors": ["Kirill Borodin", "Nikita Vasiliev", "Vasiliy Kudryavtsev", "Maxim Maslov", "Mikhail Gorodnichev", "Oleg Rogov", "Grach Mkrtchian"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "The work is still in progress", "summary": "Russian speech synthesis presents distinctive challenges, including vowel\nreduction, consonant devoicing, variable stress patterns, homograph ambiguity,\nand unnatural intonation. This paper introduces Balalaika, a novel dataset\ncomprising more than 2,000 hours of studio-quality Russian speech with\ncomprehensive textual annotations, including punctuation and stress markings.\nExperimental results show that models trained on Balalaika significantly\noutperform those trained on existing datasets in both speech synthesis and\nenhancement tasks. We detail the dataset construction pipeline, annotation\nmethodology, and results of comparative evaluations."}
{"id": "2402.08080", "pdf": "https://arxiv.org/pdf/2402.08080.pdf", "abs": "https://arxiv.org/abs/2402.08080", "title": "A Meaningful Human Control Perspective on User Perception of Partially Automated Driving Systems: A Case Study of Tesla Users", "authors": ["Lucas Elbert Suryana", "Sina Nordhoff", "Simeon C. Calvert", "Arkady Zgonnikov", "Bart van Arem"], "categories": ["cs.HC"], "comment": "8 pages", "summary": "The use of partially automated driving systems raises concerns about\npotential responsibility issues, posing risk to the system safety, acceptance,\nand adoption of these technologies. The concept of meaningful human control has\nemerged in response to the responsibility gap problem, requiring the\nfulfillment of two conditions, tracking and tracing. While this concept has\nprovided important philosophical and design insights on automated driving\nsystems, there is currently little knowledge on how meaningful human control\nrelates to subjective experiences of actual users of these systems. To address\nthis gap, our study aimed to investigate the alignment between the degree of\nmeaningful human control and drivers' perceptions of safety and trust in a\nreal-world partially automated driving system. We utilized previously collected\ndata from interviews with Tesla \"Full Self-Driving\" (FSD) Beta users,\ninvestigating the alignment between the user perception and how well the system\nwas tracking the users' reasons. We found that tracking of users' reasons for\ndriving tasks (such as safe maneuvers) correlated with perceived safety and\ntrust, albeit with notable exceptions. Surprisingly, failure to track lane\nchanging and braking reasons was not necessarily associated with negative\nperceptions of safety. However, the failure of the system to track expected\nmaneuvers in dangerous situations always resulted in low trust and perceived\nlack of safety. Overall, our analyses highlight alignment points but also\npossible discrepancies between perceived safety and trust on the one hand, and\nmeaningful human control on the other hand. Our results can help the developers\nof automated driving technology to design systems under meaningful human\ncontrol and are perceived as safe and trustworthy."}
{"id": "2507.13614", "pdf": "https://arxiv.org/pdf/2507.13614.pdf", "abs": "https://arxiv.org/abs/2507.13614", "title": "Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models", "authors": ["Sergio E. Zanotto", "Segun Aroyehun"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2412.03025", "summary": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. While recent\nresearch has primarily focused on using LLMs to classify text as either\nhuman-written and machine-generated texts, our study focus on characterizing\nthese texts using a set of linguistic features across different linguistic\nlevels such as morphology, syntax, and semantics. We select a dataset of\nhuman-written and machine-generated texts spanning 8 domains and produced by 11\ndifferent LLMs. We calculate different linguistic features such as dependency\nlength and emotionality and we use them for characterizing human-written and\nmachine-generated texts along with different sampling strategies, repetition\ncontrols and model release date. Our statistical analysis reveals that\nhuman-written texts tend to exhibit simpler syntactic structures and more\ndiverse semantic content. Furthermore, we calculate the variability of our set\nof features across models and domains. Both human and machine texts show\nstylistic diversity across domains, with humans displaying greater variation in\nour features. Finally, we apply style embeddings to further test variability\namong human-written and machine-generated texts. Notably, newer models output\ntext that is similarly variable, pointing to an homogenization of\nmachine-generated texts."}
{"id": "2407.01558", "pdf": "https://arxiv.org/pdf/2407.01558.pdf", "abs": "https://arxiv.org/abs/2407.01558", "title": "Visual Grounding Methods for Efficient Interaction with Desktop Graphical User Interfaces", "authors": ["El Hassane Ettifouri", "Jessica López Espejel", "Laura Minkova", "Tassnim Dardouri", "Walid Dahhane"], "categories": ["cs.HC", "cs.AI"], "comment": "Preprint submitted to Engineering Applications of Artificial\n  Intelligence journal", "summary": "Most visual grounding solutions primarily focus on realistic images. However,\napplications involving synthetic images, such as Graphical User Interfaces\n(GUIs), remain limited. This restricts the development of autonomous computer\nvision-powered artificial intelligence (AI) agents for automatic application\ninteraction. Enabling AI to effectively understand and interact with GUIs is\ncrucial to advancing automation in software testing, accessibility, and\nhuman-computer interaction. In this work, we explore Instruction Visual\nGrounding (IVG), a multi-modal approach to object identification within a GUI.\nMore precisely, given a natural language instruction and a GUI screen, IVG\nlocates the coordinates of the element on the screen where the instruction\nshould be executed. We propose two main methods: (1) IVGocr, which combines a\nLarge Language Model (LLM), an object detection model, and an Optical Character\nRecognition (OCR) module; and (2) IVGdirect, which uses a multimodal\narchitecture for end-to-end grounding. For each method, we introduce a\ndedicated dataset. In addition, we propose the Central Point Validation (CPV)\nmetric, a relaxed variant of the classical Central Proximity Score (CPS)\nmetric. Our final test dataset is publicly released to support future research."}
{"id": "2507.13618", "pdf": "https://arxiv.org/pdf/2507.13618.pdf", "abs": "https://arxiv.org/abs/2507.13618", "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "authors": ["Shanbo Cheng", "Yu Bao", "Qian Cao", "Luyang Huang", "Liyan Kang", "Zhicheng Liu", "Yu Lu", "Wenhao Zhu", "Zhichao Huang", "Tao Li", "Sitong Liu", "Ningxin Peng", "Shuaijie She", "Lu Xu", "Nuo Xu", "Sen Yang", "Runsheng Yu", "Yiming Yu", "Liehao Zou", "Hang Li", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications."}
{"id": "2410.03993", "pdf": "https://arxiv.org/pdf/2410.03993.pdf", "abs": "https://arxiv.org/abs/2410.03993", "title": "TR-LLM: Integrating Trajectory Data for Scene-Aware LLM-Based Human Action Prediction", "authors": ["Kojiro Takeyama", "Yimeng Liu", "Misha Sra"], "categories": ["cs.HC"], "comment": "Accepted to IROS 2025", "summary": "Accurate prediction of human behavior is crucial for AI systems to\neffectively support real-world applications, such as autonomous robots\nanticipating and assisting with human tasks. Real-world scenarios frequently\npresent challenges such as occlusions and incomplete scene observations, which\ncan compromise predictive accuracy. Thus, traditional video-based methods often\nstruggle due to limited temporal and spatial perspectives. Large Language\nModels (LLMs) offer a promising alternative. Having been trained on a large\ntext corpus describing human behaviors, LLMs likely encode plausible sequences\nof human actions in a home environment. However, LLMs, trained primarily on\ntext data, lack inherent spatial awareness and real-time environmental\nperception. They struggle with understanding physical constraints and spatial\ngeometry. Therefore, to be effective in a real-world spatial scenario, we\npropose a multimodal prediction framework that enhances LLM-based action\nprediction by integrating physical constraints derived from human trajectories.\nOur experiments demonstrate that combining LLM predictions with trajectory data\nsignificantly improves overall prediction performance. This enhancement is\nparticularly notable in situations where the LLM receives limited scene\ninformation, highlighting the complementary nature of linguistic knowledge and\nphysical constraints in understanding and anticipating human behavior."}
{"id": "2507.13655", "pdf": "https://arxiv.org/pdf/2507.13655.pdf", "abs": "https://arxiv.org/abs/2507.13655", "title": "CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer", "authors": ["Teerapong Panboonyuen"], "categories": ["cs.CL"], "comment": "12 pages", "summary": "Integrating large language models into specialized domains like healthcare\npresents unique challenges, including domain adaptation and limited labeled\ndata. We introduce CU-ICU, a method for customizing unsupervised\ninstruction-finetuned language models for ICU datasets by leveraging the\nText-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse\nfine-tuning approach that combines few-shot prompting with selective parameter\nupdates, enabling efficient adaptation with minimal supervision. Our evaluation\nacross critical ICU tasks--early sepsis detection, mortality prediction, and\nclinical note generation--demonstrates that CU-ICU consistently improves\npredictive accuracy and interpretability over standard fine-tuning methods.\nNotably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and\na 20% enhancement in generating clinically relevant explanations while updating\nfewer than 1% of model parameters in its most efficient configuration. These\nresults establish CU-ICU as a scalable, low-overhead solution for delivering\naccurate and interpretable clinical decision support in real-world ICU\nenvironments."}
{"id": "2501.03572", "pdf": "https://arxiv.org/pdf/2501.03572.pdf", "abs": "https://arxiv.org/abs/2501.03572", "title": "From Code to Compliance: Assessing ChatGPT's Utility in Designing an Accessible Webpage -- A Case Study", "authors": ["Ammar Ahmed", "Margarida Fresco", "Fredrik Forsberg", "Hallvard Grotli"], "categories": ["cs.HC", "cs.AI", "cs.CL", "D.1.2; F.3.1; F.4.1; D.3.2; H.1.2; H.5.2; D.2.2; H.1.2; I.3.6;\n  H.5.4; H.5.1"], "comment": null, "summary": "Web accessibility ensures that individuals with disabilities can access and\ninteract with digital content without barriers, yet a significant majority of\nmost used websites fail to meet accessibility standards. This study evaluates\nChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web\nContent Accessibility Guidelines (WCAG). While ChatGPT can effectively address\naccessibility issues when prompted, its default code often lacks compliance,\nreflecting limitations in its training data and prevailing inaccessible web\npractices. Automated and manual testing revealed strengths in resolving simple\nissues but challenges with complex tasks, requiring human oversight and\nadditional iterations. Unlike prior studies, we incorporate manual evaluation,\ndynamic elements, and use the visual reasoning capability of ChatGPT along with\nthe prompts to fix accessibility issues. Providing screenshots alongside\nprompts enhances the LLM's ability to address accessibility issues by allowing\nit to analyze surrounding components, such as determining appropriate contrast\ncolors. We found that effective prompt engineering, such as providing concise,\nstructured feedback and incorporating visual aids, significantly enhances\nChatGPT's performance. These findings highlight the potential and limitations\nof large language models for accessible web development, offering practical\nguidance for developers to create more inclusive websites."}
{"id": "2507.13666", "pdf": "https://arxiv.org/pdf/2507.13666.pdf", "abs": "https://arxiv.org/abs/2507.13666", "title": "KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs", "authors": ["Woo-Chan Kim", "Ji-Hoon Park", "Seong-Whan Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross a wide range of natural language processing tasks. However,\nhigh-performing models are typically accessible only via APIs, incurring\nsubstantial inference costs. Cascade methods address this by initially\nemploying a cheaper model and escalating to a stronger one only when necessary.\nNevertheless, existing cascade approaches struggle to select a reliable\nrepresentative response and assess the overall reliability of free-form\noutputs, as they rely on exact text matching. To overcome these limitations, we\npropose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient\nfree-form text generation. KiC identifies the most representative answer among\nmultiple outputs from a weaker model and evaluates the semantic alignment of\nother responses with it. Based on the degree of alignment, KiC determines\nwhether to accept the weaker model's output or escalate to a stronger model.\nExperiments on three free-form text generation benchmarks show that KiC\nachieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81\npercent on average, and even outperforms GPT-4 in a specific benchmark."}
{"id": "2504.16373", "pdf": "https://arxiv.org/pdf/2504.16373.pdf", "abs": "https://arxiv.org/abs/2504.16373", "title": "What Sensors See, What People Feel: Exploring Subjective Collaboration Perception in Mixed Reality", "authors": ["Yasra Chandio", "Diana Romero", "Salma Elmalaki", "Fatima Anwar"], "categories": ["cs.HC"], "comment": "12 pages, 4 figures, 8 tables. arXiv admin note: text overlap with\n  arXiv:2411.05258", "summary": "Mixed Reality (MR) enables rich, embodied collaboration, yet it's uncertain\nif sensor and system-logged behavioral signals capture how users experience\nthat collaboration. This disconnect stems from a fundamental gap: behavioral\nsignals are observable and continuous, while collaboration is interpreted\nsubjectively, shaped by internal states like presence, cognitive availability,\nand social awareness. Our core insight is that sensor signals serve as\nobservable manifestations of subjective experiences in MR collaboration, and\nthey can be captured through sensor data such as shared gaze, speech, spatial\nmovement, and other system-logged performance metrics. We propose the\nSensor-to-Subjective (S2S) Mapping Framework, a conceptual model that links\nobservable interaction patterns to users' subjective perceptions of\ncollaboration and internal cognitive states through sensor-based indicators and\ntask performance metrics. To validate this model, we conducted a study with 48\nparticipants across 12 MR groups engaged in a collaborative image-sorting task.\nOur findings show a correlation between sensed behavior and perceived\ncollaboration, particularly through shared attention and proximity."}
{"id": "2507.13681", "pdf": "https://arxiv.org/pdf/2507.13681.pdf", "abs": "https://arxiv.org/abs/2507.13681", "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues", "authors": ["Haoyang Li", "Zhanchao Xu", "Yiming Li", "Xuejia Chen", "Darian Li", "Anxin Tian", "Qingfa Xiao", "Cheng Deng", "Jun Wang", "Qing Li", "Lei Chen", "Mingxuan Yuan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks."}
{"id": "2507.04469", "pdf": "https://arxiv.org/pdf/2507.04469.pdf", "abs": "https://arxiv.org/abs/2507.04469", "title": "The role of large language models in UI/UX design: A systematic literature review", "authors": ["Ammar Ahmed", "Ali Shariq Imran"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "This systematic literature review examines the role of large language models\n(LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies\npublished between 2022 and 2025. We identify key LLMs in use, including GPT-4,\nGemini, and PaLM, and map their integration across the design lifecycle, from\nideation to evaluation. Common practices include prompt engineering,\nhuman-in-the-loop workflows, and multimodal input. While LLMs are reshaping\ndesign processes, challenges such as hallucination, prompt instability, and\nlimited explainability persist. Our findings highlight LLMs as emerging\ncollaborators in design, and we propose directions for the ethical, inclusive,\nand effective integration of these technologies."}
{"id": "2507.13705", "pdf": "https://arxiv.org/pdf/2507.13705.pdf", "abs": "https://arxiv.org/abs/2507.13705", "title": "Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations", "authors": ["Cedric Waterschoot", "Nava Tintarev", "Francesco Barile"], "categories": ["cs.CL", "cs.IR"], "comment": "Short paper accepted at the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25). Cedric Waterschoot, Nava Tintarev, and Francesco\n  Barile. 2025. Consistent Explainers or Unreliable Narrators? Understanding\n  LLM-generated Group Recommendations. Proceedings of the Nineteenth ACM\n  Conference on Recommender Systems (RecSys '25), Prague, Czech Republic. doi:\n  10.1145/3705328.3748015", "summary": "Large Language Models (LLMs) are increasingly being implemented as joint\ndecision-makers and explanation generators for Group Recommender Systems (GRS).\nIn this paper, we evaluate these recommendations and explanations by comparing\nthem to social choice-based aggregation strategies. Our results indicate that\nLLM-generated recommendations often resembled those produced by Additive\nUtilitarian (ADD) aggregation. However, the explanations typically referred to\naveraging ratings (resembling but not identical to ADD aggregation). Group\nstructure, uniform or divergent, did not impact the recommendations.\nFurthermore, LLMs regularly claimed additional criteria such as user or item\nsimilarity, diversity, or used undefined popularity metrics or thresholds. Our\nfindings have important implications for LLMs in the GRS pipeline as well as\nstandard aggregation strategies. Additional criteria in explanations were\ndependent on the number of ratings in the group scenario, indicating potential\ninefficiency of standard aggregation methods at larger item set sizes.\nAdditionally, inconsistent and ambiguous explanations undermine transparency\nand explainability, which are key motivations behind the use of LLMs for GRS."}
{"id": "2507.12377", "pdf": "https://arxiv.org/pdf/2507.12377.pdf", "abs": "https://arxiv.org/abs/2507.12377", "title": "Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight", "authors": ["Ke Er Amy Zhang", "Jodie Jenkinson", "Laura Garrison"], "categories": ["cs.HC"], "comment": "11 pages, 5 figures, accepted to IEEE VIS 2025 Conference", "summary": "We conduct a deconstructive reading of a qualitative interview study with 17\nvisual data journalists from newsrooms across the globe. We borrow a\ndeconstruction approach from literary critique to explore the instability of\nmeaning in language and reveal implicit beliefs in words and ideas. Through our\nanalysis we surface two sets of opposing implicit beliefs in visual data\njournalism: objectivity/subjectivity and humanism/mechanism. We contextualize\nthese beliefs through a genealogical analysis, which brings deconstruction\ntheory into practice by providing a historic backdrop for these opposing\nperspectives. Our analysis shows that these beliefs held within visual data\njournalism are not self-enclosed but rather a product of external societal\nforces and paradigm shifts over time. Through this work, we demonstrate how\nthinking with critical theories such as deconstruction and genealogy can\nreframe \"success\" in visual data storytelling and diversify visualization\nresearch outcomes. These efforts push the ways in which we as researchers\nproduce domain knowledge to examine the sociotechnical issues of today's values\ntowards datafication and data visualization. All supplemental materials for\nthis work are available at osf.io/5fr48."}
{"id": "2507.13732", "pdf": "https://arxiv.org/pdf/2507.13732.pdf", "abs": "https://arxiv.org/abs/2507.13732", "title": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction", "authors": ["Guillaume Zambrano"], "categories": ["cs.CL", "cs.LG", "J.1; I.2.7"], "comment": "23 pages, 24 figures shorter version submitted to JURIX 2025", "summary": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable."}
{"id": "2501.08102", "pdf": "https://arxiv.org/pdf/2501.08102.pdf", "abs": "https://arxiv.org/abs/2501.08102", "title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media", "authors": ["Wenlu Fan", "Yuqi Zhu", "Chenyang Wang", "Bin Wang", "Wentao Xu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "This paper has been accepted by the International AAAI Conference on\n  Web and Social Media (ICWSM) 2026(Los Angeles, California, U.S.)", "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design."}
{"id": "2507.13743", "pdf": "https://arxiv.org/pdf/2507.13743.pdf", "abs": "https://arxiv.org/abs/2507.13743", "title": "PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs", "authors": ["Maluna Menke", "Thilo Hagendorff"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) frequently reproduce the gender- and\nsexual-identity prejudices embedded in their training corpora, leading to\noutputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of\ngreat importance. To achieve this, we evaluate two parameter-efficient\nfine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt\ntuning - as lightweight alternatives to full-model fine-tuning for mitigating\nsuch biases. Using the WinoQueer benchmark, we quantify bias in three\nopen-source LLMs and observe baseline bias scores reaching up to 98 (out of\n100) across a range of queer identities defined by gender and/or sexual\norientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%\nadditional parameters) on a curated QueerNews corpus reduces those scores by up\nto 50 points and raises neutrality from virtually 0% to as much as 36%.\nSoft-prompt tuning (10 virtual tokens) delivers only marginal improvements.\nThese findings show that LoRA can deliver meaningful fairness gains with\nminimal computation. We advocate broader adoption of community-informed PEFT,\nthe creation of larger queer-authored corpora, and richer evaluation suites\nbeyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive."}
{"id": "2502.15761", "pdf": "https://arxiv.org/pdf/2502.15761.pdf", "abs": "https://arxiv.org/abs/2502.15761", "title": "AIvaluateXR: An Evaluation Framework for on-Device AI in XR with Benchmarking Results", "authors": ["Dawar Khan", "Xinyu Liu", "Omar Mena", "Donggang Jia", "Alexandre Kouyoumdjian", "Ivan Viola"], "categories": ["cs.DC", "cs.AI", "cs.GR", "cs.HC"], "comment": "AIvaluateXR is updated version of LoXR", "summary": "The deployment of large language models (LLMs) on extended reality (XR)\ndevices has great potential to advance the field of human-AI interaction. In\nthe case of direct, on-device model inference, selecting the appropriate model\nand device for specific tasks remains challenging. In this paper, we present\nAIvaluateXR, a comprehensive evaluation framework for benchmarking LLMs running\non XR devices. To demonstrate the framework, we deploy 17 selected LLMs across\nfour XR platforms: Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision\nPro, and conduct an extensive evaluation. Our experimental setup measures four\nkey metrics: performance consistency, processing speed, memory usage, and\nbattery consumption. For each of the 68 model-device pairs, we assess\nperformance under varying string lengths, batch sizes, and thread counts,\nanalyzing the trade-offs for real-time XR applications. We propose a unified\nevaluation method based on the 3D Pareto Optimality theory to select the\noptimal device-model pairs from quality and speed objectives. Additionally, we\ncompare the efficiency of on-device LLMs with client-server and cloud-based\nsetups, and evaluate their accuracy on two interactive tasks. We believe our\nfindings offer valuable insight to guide future optimization efforts for LLM\ndeployment on XR devices. Our evaluation method can be used as standard\ngroundwork for further research and development in this emerging field. The\nsource code and supplementary materials are available at:\nwww.nanovis.org/AIvaluateXR.html"}
{"id": "2507.13761", "pdf": "https://arxiv.org/pdf/2507.13761.pdf", "abs": "https://arxiv.org/abs/2507.13761", "title": "Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models", "authors": ["Palash Nandi", "Maithili Joshi", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "Language models are highly sensitive to prompt formulations - small changes\nin input can drastically alter their output. This raises a critical question:\nTo what extent can prompt sensitivity be exploited to generate inapt content?\nIn this paper, we investigate how discrete components of prompt design\ninfluence the generation of inappropriate content in Visual Language Models\n(VLMs). Specifically, we analyze the impact of three key factors on successful\njailbreaks: (a) the inclusion of detailed visual information, (b) the presence\nof adversarial examples, and (c) the use of positively framed beginning\nphrases. Our findings reveal that while a VLM can reliably distinguish between\nbenign and harmful inputs in unimodal settings (text-only or image-only), this\nability significantly degrades in multimodal contexts. Each of the three\nfactors is independently capable of triggering a jailbreak, and we show that\neven a small number of in-context examples (as few as three) can push the model\ntoward generating inappropriate outputs. Furthermore, we propose a framework\nthat utilizes a skip-connection between two internal layers of the VLM, which\nsubstantially increases jailbreak success rates, even when using benign images.\nFinally, we demonstrate that memes, often perceived as humorous or harmless,\ncan be as effective as toxic visuals in eliciting harmful content, underscoring\nthe subtle and complex vulnerabilities of VLMs."}
{"id": "2505.22987", "pdf": "https://arxiv.org/pdf/2505.22987.pdf", "abs": "https://arxiv.org/abs/2505.22987", "title": "Strategic Reflectivism In Intelligent Systems", "authors": ["Nick Byrd"], "categories": ["cs.AI", "cs.HC", "econ.TH", "C.1.3; I.2.0; I.2.8; I.2.11"], "comment": "Accepted for publication in the proceedings of the 4th International\n  conference on Human and Artificial Rationality, which are to appear in\n  Lecture Notes in Computer Science. An earlier version was presented at the\n  2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning\n  (CHI25-WS-AUGMENTED-REASONING)", "summary": "By late 20th century, the rationality wars had launched debates about the\nnature and norms of intuitive and reflective thinking. Those debates drew from\nmid-20th century ideas such as bounded rationality, which challenged more\nidealized notions of rationality observed since the 19th century. Now that 21st\ncentury cognitive scientists are applying the resulting dual pro-cess theories\nto artificial intelligence, it is time to dust off some lessons from this\nhistory. So this paper synthesizes old ideas with recent results from\nexperiments on humans and machines. The result is Strategic Reflec-tivism, the\nposition that one key to intelligent systems (human or artificial) is pragmatic\nswitching between intuitive and reflective inference to opti-mally fulfill\ncompeting goals. Strategic Reflectivism builds on American Pragmatism,\ntranscends superficial indicators of reflective thinking such as model size or\nchains of thought, applies to both individual and collective intelligence\nsystems (including human-AI teams), and becomes increasingly actionable as we\nlearn more about the value of intuition and reflection."}
{"id": "2507.13793", "pdf": "https://arxiv.org/pdf/2507.13793.pdf", "abs": "https://arxiv.org/abs/2507.13793", "title": "An Enhanced Model-based Approach for Short Text Clustering", "authors": ["Enhao Cheng", "Shoujia Zhang", "Jianhua Yin", "Xuemeng Song", "Tian Gan", "Liqiang Nie"], "categories": ["cs.CL"], "comment": null, "summary": "Short text clustering has become increasingly important with the popularity\nof social media like Twitter, Google+, and Facebook. Existing methods can be\nbroadly categorized into two paradigms: topic model-based approaches and deep\nrepresentation learning-based approaches. This task is inherently challenging\ndue to the sparse, large-scale, and high-dimensional characteristics of the\nshort text data. Furthermore, the computational intensity required by\nrepresentation learning significantly increases the running time. To address\nthese issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet\nMultinomial Mixture model (GSDMM), which effectively handles the sparsity and\nhigh dimensionality of short texts while identifying representative words for\neach cluster. Based on several aspects of GSDMM that warrant further\nrefinement, we propose an improved approach, GSDMM+, designed to further\noptimize its performance. GSDMM+ reduces initialization noise and adaptively\nadjusts word weights based on entropy, achieving fine-grained clustering that\nreveals more topic-related information. Additionally, strategic cluster merging\nis employed to refine clustering granularity, better aligning the predicted\ndistribution with the true category distribution. We conduct extensive\nexperiments, comparing our methods with both classical and state-of-the-art\napproaches. The experimental results demonstrate the efficiency and\neffectiveness of our methods. The source code for our model is publicly\navailable at https://github.com/chehaoa/VEMC."}
{"id": "2507.04295", "pdf": "https://arxiv.org/pdf/2507.04295.pdf", "abs": "https://arxiv.org/abs/2507.04295", "title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop", "authors": ["Runcong Zhao", "Artem Bobrov", "Jiazheng Li", "Yulan He"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Effective feedback is essential for student learning but is time-intensive\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\npersonalised, curriculum-aligned feedback in science education. LearnLens\ncomprises three components: (1) an error-aware assessment module that captures\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\na structured, topic-linked memory chain rather than traditional\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\neducator-in-the-loop interface for customisation and oversight. LearnLens\naddresses key challenges in existing systems, offering scalable, high-quality\nfeedback that empowers both teachers and students."}
{"id": "2507.13827", "pdf": "https://arxiv.org/pdf/2507.13827.pdf", "abs": "https://arxiv.org/abs/2507.13827", "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models", "authors": ["Hosein Azarbonyad", "Zi Long Zhu", "Georgios Cheirmpos", "Zubair Afzal", "Vikrant Yadav", "Georgios Tsatsaronis"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "SIGIR 2025", "summary": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents."}
{"id": "2507.13839", "pdf": "https://arxiv.org/pdf/2507.13839.pdf", "abs": "https://arxiv.org/abs/2507.13839", "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words", "authors": ["Lizhi Ma", "Tong Zhao", "Shuai Zhang", "Nirui Song", "Hongliang He", "Anqi Li", "Ran Feng", "Huachuan Qiu", "Jingsong Ma", "Zhenzhong Lan"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations."}
{"id": "2507.13841", "pdf": "https://arxiv.org/pdf/2507.13841.pdf", "abs": "https://arxiv.org/abs/2507.13841", "title": "Modeling Fair Play in Detective Stories with Language Models", "authors": ["Eitan Wagner", "Renana Keydar", "Omri Abend"], "categories": ["cs.CL"], "comment": null, "summary": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality."}
{"id": "2507.13858", "pdf": "https://arxiv.org/pdf/2507.13858.pdf", "abs": "https://arxiv.org/abs/2507.13858", "title": "InTraVisTo: Inside Transformer Visualisation Tool", "authors": ["Nicolò Brunello", "Davide Rigamonti", "Andrea Sassella", "Vincenzo Scotti", "Mark James Carman"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs."}
{"id": "2507.13870", "pdf": "https://arxiv.org/pdf/2507.13870.pdf", "abs": "https://arxiv.org/abs/2507.13870", "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER", "authors": ["Maciej Jalocha", "Johan Hausted Schmidt", "William Michelseen"], "categories": ["cs.CL"], "comment": "5 pages, 5 figures", "summary": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER."}
{"id": "2507.13875", "pdf": "https://arxiv.org/pdf/2507.13875.pdf", "abs": "https://arxiv.org/abs/2507.13875", "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies", "authors": ["Carlos Mena", "Pol Serra", "Jacobo Romero", "Abir Messaoudi", "Jose Giraldo", "Carme Armentano-Oller", "Rodolfo Zevallos", "Ivan Meza", "Javier Hernando"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance."}
{"id": "2507.13881", "pdf": "https://arxiv.org/pdf/2507.13881.pdf", "abs": "https://arxiv.org/abs/2507.13881", "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test", "authors": ["Cole Walsh", "Rodica Ivan", "Muhammad Zafar Iqbal", "Colleen Robb"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "10 pages, 2 figures, 4 tables; this work was accepted for\n  presentation at the 2025 Artificial Intelligence in Measurement and Education\n  Conference in Pittsburgh, Pennsylvania, United States", "summary": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills."}
{"id": "2507.13913", "pdf": "https://arxiv.org/pdf/2507.13913.pdf", "abs": "https://arxiv.org/abs/2507.13913", "title": "Political Leaning and Politicalness Classification of Texts", "authors": ["Matous Volf", "Jakub Simko"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities."}
{"id": "2507.13919", "pdf": "https://arxiv.org/pdf/2507.13919.pdf", "abs": "https://arxiv.org/abs/2507.13919", "title": "The Levers of Political Persuasion with Conversational AI", "authors": ["Kobi Hackenburg", "Ben M. Tappin", "Luke Hewitt", "Ed Saunders", "Sid Black", "Hause Lin", "Catherine Fist", "Helen Margetts", "David G. Rand", "Christopher Summerfield"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "19 pages, 4 figures. Our supplementary materials file can be found at\n  https://github.com/kobihackenburg/scaling-conversational-AI", "summary": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy."}
{"id": "2507.13937", "pdf": "https://arxiv.org/pdf/2507.13937.pdf", "abs": "https://arxiv.org/abs/2507.13937", "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support", "authors": ["Jan Trienes", "Anastasiia Derzhanskaia", "Roland Schwarzkopf", "Markus Mühling", "Jörg Schlötterer", "Christin Seifert"], "categories": ["cs.CL"], "comment": null, "summary": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment."}
{"id": "2507.13949", "pdf": "https://arxiv.org/pdf/2507.13949.pdf", "abs": "https://arxiv.org/abs/2507.13949", "title": "Exploiting Primacy Effect To Improve Large Language Models", "authors": ["Bianca Raimondi", "Maurizio Gabbrielli"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications."}
{"id": "2507.13966", "pdf": "https://arxiv.org/pdf/2507.13966.pdf", "abs": "https://arxiv.org/abs/2507.13966", "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need", "authors": ["Bhishma Dedhia", "Yuval Kansal", "Niraj K. Jha"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents."}
{"id": "2507.13977", "pdf": "https://arxiv.org/pdf/2507.13977.pdf", "abs": "https://arxiv.org/abs/2507.13977", "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic", "authors": ["Lilit Grigoryan", "Nikolay Karpov", "Enas Albasiri", "Vitaly Lavrukhin", "Boris Ginsburg"], "categories": ["cs.CL", "eess.AS", "I.5.1"], "comment": "Accepted to ICASSP 2025", "summary": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes."}
{"id": "2507.14017", "pdf": "https://arxiv.org/pdf/2507.14017.pdf", "abs": "https://arxiv.org/abs/2507.14017", "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models", "authors": ["Haoyu He", "Haozheng Luo", "Yan Chen", "Qi R. Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods."}
{"id": "2507.14022", "pdf": "https://arxiv.org/pdf/2507.14022.pdf", "abs": "https://arxiv.org/abs/2507.14022", "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis", "authors": ["Jianfei Li", "Kevin Kam Fung Yuen"], "categories": ["cs.CL", "cs.LG"], "comment": "35 pages, 33 tables, 6 Figures", "summary": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas."}
{"id": "2507.14045", "pdf": "https://arxiv.org/pdf/2507.14045.pdf", "abs": "https://arxiv.org/abs/2507.14045", "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks", "authors": ["Israt Jahan", "Md Tahmid Rahman Laskar", "Chun Peng", "Jimmy Huang"], "categories": ["cs.CL"], "comment": "Accepted at Canadian AI 2025", "summary": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications."}
{"id": "2507.14063", "pdf": "https://arxiv.org/pdf/2507.14063.pdf", "abs": "https://arxiv.org/abs/2507.14063", "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog", "authors": ["Lautaro Estienne", "Gabriel Ben Zenou", "Nona Naderi", "Jackie Cheung", "Pablo Piantanida"], "categories": ["cs.CL"], "comment": null, "summary": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents."}
{"id": "2507.14079", "pdf": "https://arxiv.org/pdf/2507.14079.pdf", "abs": "https://arxiv.org/abs/2507.14079", "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits", "authors": ["Garapati Keerthana", "Manik Gupta"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings."}
{"id": "2507.14096", "pdf": "https://arxiv.org/pdf/2507.14096.pdf", "abs": "https://arxiv.org/abs/2507.14096", "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track", "authors": ["Brian Ondov", "William Xia", "Kush Attal", "Ishita Unde", "Jerry He", "Hoa Dang", "Ian Soboroff", "Dina Demner-Fushman"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools."}
{"id": "2507.13354", "pdf": "https://arxiv.org/pdf/2507.13354.pdf", "abs": "https://arxiv.org/abs/2507.13354", "title": "Physical models realizing the transformer architecture of large language models", "authors": ["Zeqian Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL", "math-ph", "math.MP"], "comment": "6 pages", "summary": "The introduction of the transformer architecture in 2017 (cf.\\cite{VSP2017})\nmarked the most striking advancement in natural language processing. The\ntransformer is a model architecture relying entirely on an attention mechanism\nto draw global dependencies between input and output. However, we believe there\nis a gap in our theoretical understanding of what the transformer is, and why\nit works physically. In this paper, from a physical perspective on modern\nchips, we construct physical models in the Fock space over the Hilbert space of\ntokens realizing large language models based on a transformer architecture as\nopen quantum systems. Our physical models underlie the transformer architecture\nfor large language models."}
{"id": "2507.13362", "pdf": "https://arxiv.org/pdf/2507.13362.pdf", "abs": "https://arxiv.org/abs/2507.13362", "title": "Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning", "authors": ["Binbin Ji", "Siddharth Agrawal", "Qiance Tang", "Yvonne Wu"], "categories": ["cs.CV", "cs.AI", "cs.CL", "I.2.10; I.4.8; I.2.6; I.2.7; I.5.4; I.5.1"], "comment": "10 pages, 5 figures, submitted to a conference (IEEE formate).\n  Authored by students from the Courant Institute, NYU", "summary": "This study investigates the spatial reasoning capabilities of vision-language\nmodels (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement\nlearning. We begin by evaluating the impact of different prompting strategies\nand find that simple CoT formats, where the model generates a reasoning step\nbefore the answer, not only fail to help, but can even harm the model's\noriginal performance. In contrast, structured multi-stage prompting based on\nscene graphs (SceneGraph CoT) significantly improves spatial reasoning\naccuracy. Furthermore, to improve spatial reasoning ability, we fine-tune\nmodels using Group Relative Policy Optimization (GRPO) on the SAT dataset and\nevaluate their performance on CVBench. Compared to supervised fine-tuning\n(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates\nsuperior robustness under out-of-distribution (OOD) conditions. In particular,\nwe find that SFT overfits to surface-level linguistic patterns and may degrade\nperformance when test-time phrasing changes (e.g., from \"closer to\" to \"farther\nfrom\"). GRPO, on the other hand, generalizes more reliably and maintains stable\nperformance under such shifts. Our findings provide insights into how\nreinforcement learning and structured prompting improve the spatial reasoning\ncapabilities and generalization behavior of modern VLMs. All code is open\nsource at: https://github.com/Yvonne511/spatial-vlm-investigator"}
{"id": "2507.13396", "pdf": "https://arxiv.org/pdf/2507.13396.pdf", "abs": "https://arxiv.org/abs/2507.13396", "title": "DyG-RAG: Dynamic Graph Retrieval-Augmented Generation with Event-Centric Reasoning", "authors": ["Qingyun Sun", "Jiaqi Yuan", "Shan He", "Xiao Guan", "Haonan Yuan", "Xingcheng Fu", "Jianxin Li", "Philip S. Yu"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Graph Retrieval-Augmented Generation has emerged as a powerful paradigm for\ngrounding large language models with external structured knowledge. However,\nexisting Graph RAG methods struggle with temporal reasoning, due to their\ninability to model the evolving structure and order of real-world events. In\nthis work, we introduce DyG-RAG, a novel event-centric dynamic graph\nretrieval-augmented generation framework designed to capture and reason over\ntemporal knowledge embedded in unstructured text. To eliminate temporal\nambiguity in traditional retrieval units, DyG-RAG proposes Dynamic Event Units\n(DEUs) that explicitly encode both semantic content and precise temporal\nanchors, enabling accurate and interpretable time-aware retrieval. To capture\ntemporal and causal dependencies across events, DyG-RAG constructs an event\ngraph by linking DEUs that share entities and occur close in time, supporting\nefficient and meaningful multi-hop reasoning. To ensure temporally consistent\ngeneration, DyG-RAG introduces an event timeline retrieval pipeline that\nretrieves event sequences via time-aware traversal, and proposes a Time\nChain-of-Thought strategy for temporally grounded answer generation. This\nunified pipeline enables DyG-RAG to retrieve coherent, temporally ordered event\nsequences and to answer complex, time-sensitive queries that standard RAG\nsystems cannot resolve. Extensive experiments on temporal QA benchmarks\ndemonstrate that DyG-RAG significantly improves the accuracy and recall of\nthree typical types of temporal reasoning questions, paving the way for more\nfaithful and temporal-aware generation. DyG-RAG is available at\nhttps://github.com/RingBDStack/DyG-RAG."}
{"id": "2507.13550", "pdf": "https://arxiv.org/pdf/2507.13550.pdf", "abs": "https://arxiv.org/abs/2507.13550", "title": "GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models", "authors": ["Eduardo C. Garrido-Merchán", "Cristina Puente"], "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": null, "summary": "The development of large language models (LLMs) has successfully transformed\nknowledge-based systems such as open domain question nswering, which can\nautomatically produce vast amounts of seemingly coherent information. Yet,\nthose models have several disadvantages like hallucinations or confident\ngeneration of incorrect or unverifiable facts. In this paper, we introduce a\nnew approach to the development of expert systems using LLMs in a controlled\nand transparent way. By limiting the domain and employing a well-structured\nprompt-based extraction approach, we produce a symbolic representation of\nknowledge in Prolog, which can be validated and corrected by human experts.\nThis approach also guarantees interpretability, scalability and reliability of\nthe developed expert systems. Via quantitative and qualitative experiments with\nClaude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic\ncoherence on our generated knowledge bases. We present a transparent hybrid\nsolution that combines the recall capacity of LLMs with the precision of\nsymbolic systems, thereby laying the foundation for dependable AI applications\nin sensitive domains."}
{"id": "2507.13586", "pdf": "https://arxiv.org/pdf/2507.13586.pdf", "abs": "https://arxiv.org/abs/2507.13586", "title": "TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting", "authors": ["Kaiyuan Tang", "Kuangshi Ai", "Jun Han", "Chaoli Wang"], "categories": ["cs.GR", "cs.CL", "cs.CV"], "comment": "Accepted by IEEE VIS 2025", "summary": "Advancements in volume visualization (VolVis) focus on extracting insights\nfrom 3D volumetric data by generating visually compelling renderings that\nreveal complex internal structures. Existing VolVis approaches have explored\nnon-photorealistic rendering techniques to enhance the clarity, expressiveness,\nand informativeness of visual communication. While effective, these methods\noften rely on complex predefined rules and are limited to transferring a single\nstyle, restricting their flexibility. To overcome these limitations, we\nadvocate the representation of VolVis scenes using differentiable Gaussian\nprimitives combined with pretrained large models to enable arbitrary style\ntransfer and real-time rendering. However, conventional 3D Gaussian primitives\ntightly couple geometry and appearance, leading to suboptimal stylization\nresults. To address this, we introduce TexGS-VolVis, a textured Gaussian\nsplatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives,\nextending each Gaussian with additional texture and shading attributes,\nresulting in higher-quality, geometry-consistent stylization and enhanced\nlighting control during inference. Despite these improvements, achieving\nflexible and controllable scene editing remains challenging. To further enhance\nstylization, we develop image- and text-driven non-photorealistic scene editing\ntailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing\nwith fine-grained control. We evaluate TexGS-VolVis both qualitatively and\nquantitatively across various volume rendering scenes, demonstrating its\nsuperiority over existing methods in terms of efficiency, visual quality, and\nediting flexibility."}
{"id": "2507.13609", "pdf": "https://arxiv.org/pdf/2507.13609.pdf", "abs": "https://arxiv.org/abs/2507.13609", "title": "CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks", "authors": ["Yanan Wang", "Julio Vizcarra", "Zhi Li", "Hao Niu", "Mori Kurokawa"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Despite recent progress in video large language models (VideoLLMs), a key\nopen challenge remains: how to equip models with chain-of-thought (CoT)\nreasoning abilities grounded in fine-grained object-level video understanding.\nExisting instruction-tuned models, such as the Qwen and LLaVA series, are\ntrained on high-level video-text pairs, often lacking structured annotations\nnecessary for compositional, step-by-step reasoning. We propose CoTasks:\nChain-of-Thought based Video Instruction Tuning Tasks, a new framework that\ndecomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)\ninto four entity-level foundational tasks: frame localization, entity tracking,\nspatial and temporal relation extraction. By embedding these intermediate\nCoT-style reasoning steps into the input, CoTasks enables models to explicitly\nperform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA\nbenchmark show that CoTasks significantly enhance inference performance:\nLLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and\nQwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal\n(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the\neffectiveness of CoTasks as a structured CoT-style supervision framework for\nimproving compositional video reasoning."}
{"id": "2507.13737", "pdf": "https://arxiv.org/pdf/2507.13737.pdf", "abs": "https://arxiv.org/abs/2507.13737", "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs", "authors": ["Ye Tian", "Xiaoyuan Ren", "Zihao Wang", "Onat Gungor", "Xiaofan Yu", "Tajana Rosing"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MM"], "comment": null, "summary": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed."}
{"id": "2507.13773", "pdf": "https://arxiv.org/pdf/2507.13773.pdf", "abs": "https://arxiv.org/abs/2507.13773", "title": "Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions", "authors": ["Pu Jian", "Donglei Yu", "Wen Yang", "Shuo Ren", "Jiajun Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": "ACL2025 Main", "summary": "In visual question answering (VQA) context, users often pose ambiguous\nquestions to visual language models (VLMs) due to varying expression habits.\nExisting research addresses such ambiguities primarily by rephrasing questions.\nThese approaches neglect the inherently interactive nature of user interactions\nwith VLMs, where ambiguities can be clarified through user feedback. However,\nresearch on interactive clarification faces two major challenges: (1)\nBenchmarks are absent to assess VLMs' capacity for resolving ambiguities\nthrough interaction; (2) VLMs are trained to prefer answering rather than\nasking, preventing them from seeking clarification. To overcome these\nchallenges, we introduce \\textbf{ClearVQA} benchmark, which targets three\ncommon categories of ambiguity in VQA context, and encompasses various VQA\nscenarios."}
{"id": "2507.13822", "pdf": "https://arxiv.org/pdf/2507.13822.pdf", "abs": "https://arxiv.org/abs/2507.13822", "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs", "authors": ["Shad Nygren", "Pinar Avci", "Andre Daniels", "Reza Rassol", "Afshin Beheshti", "Diego Galeano"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications."}
{"id": "2507.13859", "pdf": "https://arxiv.org/pdf/2507.13859.pdf", "abs": "https://arxiv.org/abs/2507.13859", "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection", "authors": ["Aleksandr Gashkov", "Aleksandr Perevalov", "Maria Eltsova", "Andreas Both"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Winner of Best Paper Award at the 25th International Conference on\n  Web Engineering (ICWE 2025)", "summary": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible."}
{"id": "2507.13933", "pdf": "https://arxiv.org/pdf/2507.13933.pdf", "abs": "https://arxiv.org/abs/2507.13933", "title": "Preprint: Did I Just Browse A Website Written by LLMs?", "authors": ["Sichang \"Steven\" He", "Ramesh Govindan", "Harsha V. Madhyastha"], "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.IR"], "comment": "In submission. 2 pages. 3 figures", "summary": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem."}
{"id": "2507.14049", "pdf": "https://arxiv.org/pdf/2507.14049.pdf", "abs": "https://arxiv.org/abs/2507.14049", "title": "EdgeVLA: Efficient Vision-Language-Action Models", "authors": ["Paweł Budzianowski", "Wesley Maa", "Matthew Freed", "Jingxiang Mo", "Winston Hsiao", "Aaron Xie", "Tomasz Młoduchowski", "Viraj Tipnis", "Benjamin Bolte"], "categories": ["cs.RO", "cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch."}
{"id": "2507.14119", "pdf": "https://arxiv.org/pdf/2507.14119.pdf", "abs": "https://arxiv.org/abs/2507.14119", "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining", "authors": ["Maksim Kuprashevich", "Grigorii Alekseenko", "Irina Tolstykh", "Georgii Fedorov", "Bulat Suleimanov", "Vladimir Dokholyan", "Aleksandr Gordeev"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments."}
{"id": "2303.18162", "pdf": "https://arxiv.org/pdf/2303.18162.pdf", "abs": "https://arxiv.org/abs/2303.18162", "title": "ViMMRC 2.0 -- Enhancing Machine Reading Comprehension on Vietnamese Literature Text", "authors": ["Son T. Luu", "Khoi Trong Hoang", "Tuong Quang Pham", "Kiet Van Nguyen", "Ngan Luu-Thuy Nguyen"], "categories": ["cs.CL"], "comment": "Accepted for publication at International Journal of Asian Language\n  Processing", "summary": "Machine reading comprehension has been an interesting and challenging task in\nrecent years, with the purpose of extracting useful information from texts. To\nattain the computer ability to understand the reading text and answer relevant\ninformation, we introduce ViMMRC 2.0 - an extension of the previous ViMMRC for\nthe task of multiple-choice reading comprehension in Vietnamese Textbooks which\ncontain the reading articles for students from Grade 1 to Grade 12. This\ndataset has 699 reading passages which are prose and poems, and 5,273\nquestions. The questions in the new dataset are not fixed with four options as\nin the previous version. Moreover, the difficulty of questions is increased,\nwhich challenges the models to find the correct choice. The computer must\nunderstand the whole context of the reading passage, the question, and the\ncontent of each choice to extract the right answers. Hence, we propose a\nmulti-stage approach that combines the multi-step attention network (MAN) with\nthe natural language inference (NLI) task to enhance the performance of the\nreading comprehension model. Then, we compare the proposed methodology with the\nbaseline BERTology models on the new dataset and the ViMMRC 1.0. From the\nresults of the error analysis, we found that the challenge of the reading\ncomprehension models is understanding the implicit context in texts and linking\nthem together in order to find the correct answers. Finally, we hope our new\ndataset will motivate further research to enhance the ability of computers to\nunderstand the Vietnamese language."}
{"id": "2404.07053", "pdf": "https://arxiv.org/pdf/2404.07053.pdf", "abs": "https://arxiv.org/abs/2404.07053", "title": "Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation", "authors": ["Elisa Sanchez-Bayona", "Rodrigo Agerri"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Metaphors, although occasionally unperceived, are ubiquitous in our everyday\nlanguage. Thus, it is crucial for Language Models to be able to grasp the\nunderlying meaning of this kind of figurative language. In this work, we\npresent Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection\nand interpretation that contains metaphor annotations in both Spanish and\nEnglish. We investigate language models' metaphor identification and\nunderstanding abilities through a series of monolingual and cross-lingual\nexperiments by leveraging our proposed corpus. In order to comprehend how these\nnon-literal expressions affect models' performance, we look over the results\nand perform an error analysis. Additionally, parallel data offers many\npotential opportunities to investigate metaphor transferability between these\nlanguages and the impact of translation on the development of multilingual\nannotated resources."}
{"id": "2407.10266", "pdf": "https://arxiv.org/pdf/2407.10266.pdf", "abs": "https://arxiv.org/abs/2407.10266", "title": "psifx -- Psychological and Social Interactions Feature Extraction Package", "authors": ["Guillaume Rochette", "Mathieu Rochat", "Matthew J. Vowels"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "psifx is a plug-and-play multi-modal feature extraction toolkit, aiming to\nfacilitate and democratize the use of state-of-the-art machine learning\ntechniques for human sciences research. It is motivated by a need (a) to\nautomate and standardize data annotation processes that typically require\nexpensive, lengthy, and inconsistent human labour; (b) to develop and\ndistribute open-source community-driven psychology research software; and (c)\nto enable large-scale access and ease of use for non-expert users. The\nframework contains an array of tools for tasks such as speaker diarization,\nclosed-caption transcription and translation from audio; body, hand, and facial\npose estimation and gaze tracking with multi-person tracking from video; and\ninteractive textual feature extraction supported by large language models. The\npackage has been designed with a modular and task-oriented approach, enabling\nthe community to add or update new tools easily. This combination creates new\nopportunities for in-depth study of real-time behavioral phenomena in\npsychological and social science research."}
{"id": "2409.04617", "pdf": "https://arxiv.org/pdf/2409.04617.pdf", "abs": "https://arxiv.org/abs/2409.04617", "title": "Sparse Rewards Can Self-Train Dialogue Agents", "authors": ["Barrett Martin Lattimer", "Varun Gangal", "Ryan McDonald", "Yi Yang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM)\nagents, especially in multi-turn dialogue tasks, have been primarily driven by\nsupervised fine-tuning and high-quality human feedback. However, as base LLM\nmodels continue to improve, acquiring meaningful human feedback has become\nincreasingly challenging and costly. In certain domains, base LLM agents may\neventually exceed human capabilities, making traditional feedback-driven\nmethods impractical. In this paper, we introduce a novel self-improvement\nparadigm that empowers LLM agents to autonomously enhance their performance\nwithout external human feedback. Our method, Juxtaposed Outcomes for Simulation\nHarvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward\nsimulation environment to extract ideal behaviors and further train the LLM on\nits own outputs. We present ToolWOZ, a sparse reward tool-calling simulation\nenvironment derived from MultiWOZ. We demonstrate that models trained with\nJOSH, both small and frontier, significantly improve tool-based interactions\nwhile preserving general model capabilities across diverse benchmarks. Our code\nand data are publicly available on GitHub at\nhttps://github.com/asappresearch/josh-llm-simulation-training"}
{"id": "2410.13394", "pdf": "https://arxiv.org/pdf/2410.13394.pdf", "abs": "https://arxiv.org/abs/2410.13394", "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs", "authors": ["Sumanth Doddapaneni", "Mohammed Safi Ur Rahman Khan", "Dilip Venkatesh", "Raj Dabre", "Anoop Kunchukuttan", "Mitesh M. Khapra"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating machine-generated text remains a significant challenge in NLP,\nespecially for non-English languages. Current methodologies, including\nautomated metrics, human assessments, and LLM-based evaluations, predominantly\nfocus on English, revealing a significant gap in multilingual evaluation\nframeworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an\nextensible framework that includes evaluator LLMs (Hercule) and a novel test\nset (Recon) specifically designed for multilingual evaluation. Our test set\nfeatures 500 human-annotated instructions spanning various task capabilities\nalong with human judgment scores across six languages. This would enable\nbenchmarking of general-purpose multilingual LLMs and facilitate\nmeta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a\ncross-lingual evaluation model that addresses the scarcity of reference answers\nin the target language by learning to assign scores to responses based on\neasily available reference answers in English. Our experiments demonstrate that\nHercule aligns more closely with human judgments compared to proprietary\nmodels, demonstrating the effectiveness of such cross-lingual evaluation in low\nresource scenarios. Further, it is also effective in zero-shot evaluation on\nunseen languages. This study is the first comprehensive examination of\ncross-lingual evaluation using LLMs, presenting a scalable and effective\napproach for multilingual assessment. All code, datasets, and models will be\npublicly available to enable further research in this important area."}
{"id": "2501.00152", "pdf": "https://arxiv.org/pdf/2501.00152.pdf", "abs": "https://arxiv.org/abs/2501.00152", "title": "Temporal reasoning for timeline summarisation in social media", "authors": ["Jiayu Song", "Mahmud Elahi Akhter", "Dana Atzil Slonim", "Maria Liakata"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation."}
{"id": "2501.08102", "pdf": "https://arxiv.org/pdf/2501.08102.pdf", "abs": "https://arxiv.org/abs/2501.08102", "title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media", "authors": ["Wenlu Fan", "Yuqi Zhu", "Chenyang Wang", "Bin Wang", "Wentao Xu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "This paper has been accepted by the International AAAI Conference on\n  Web and Social Media (ICWSM) 2026(Los Angeles, California, U.S.)", "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design."}
{"id": "2501.08208", "pdf": "https://arxiv.org/pdf/2501.08208.pdf", "abs": "https://arxiv.org/abs/2501.08208", "title": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems", "authors": ["Mohita Chowdhury", "Yajie Vera He", "Jared Joselowitz", "Aisling Higham", "Ernest Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages", "summary": "Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development."}
{"id": "2502.12057", "pdf": "https://arxiv.org/pdf/2502.12057.pdf", "abs": "https://arxiv.org/abs/2502.12057", "title": "Culture is Not Trivia: Sociocultural Theory for Cultural NLP", "authors": ["Naitian Zhou", "David Bamman", "Isaac L. Bleaman"], "categories": ["cs.CL", "cs.CY"], "comment": "ACL 2025 Main Conference; camera-ready version", "summary": "The field of cultural NLP has recently experienced rapid growth, driven by a\npressing need to ensure that language technologies are effective and safe\nacross a pluralistic user base. This work has largely progressed without a\nshared conception of culture, instead choosing to rely on a wide array of\ncultural proxies. However, this leads to a number of recurring limitations:\ncoarse national boundaries fail to capture nuanced differences that lay within\nthem, limited coverage restricts datasets to only a subset of usually\nhighly-represented cultures, and a lack of dynamicity results in static\ncultural benchmarks that do not change as culture evolves. In this position\npaper, we argue that these methodological limitations are symptomatic of a\ntheoretical gap. We draw on a well-developed theory of culture from\nsociocultural linguistics to fill this gap by 1) demonstrating in a case study\nhow it can clarify methodological constraints and affordances, 2) offering\ntheoretically-motivated paths forward to achieving cultural competence, and 3)\narguing that localization is a more useful framing for the goals of much\ncurrent work in cultural NLP."}
{"id": "2502.13246", "pdf": "https://arxiv.org/pdf/2502.13246.pdf", "abs": "https://arxiv.org/abs/2502.13246", "title": "When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models", "authors": ["Julia Mendelsohn", "Ceren Budak"], "categories": ["cs.CL", "cs.CY"], "comment": "To appear at ACL 2025. Please cite ACL version when proceedings are\n  available", "summary": "Metaphor, discussing one concept in terms of another, is abundant in politics\nand can shape how people understand important issues. We develop a\ncomputational approach to measure metaphorical language, focusing on\nimmigration discourse on social media. Grounded in qualitative social science\nresearch, we identify seven concepts evoked in immigration discourse (e.g.\n\"water\" or \"vermin\"). We propose and evaluate a novel technique that leverages\nboth word-level and document-level signals to measure metaphor with respect to\nthese concepts. We then study the relationship between metaphor, political\nideology, and user engagement in 400K US tweets about immigration. While\nconservatives tend to use dehumanizing metaphors more than liberals, this\neffect varies widely across concepts. Moreover, creature-related metaphor is\nassociated with more retweets, especially for liberal authors. Our work\nhighlights the potential for computational methods to complement qualitative\napproaches in understanding subtle and implicit language in political\ndiscourse."}
{"id": "2502.13962", "pdf": "https://arxiv.org/pdf/2502.13962.pdf", "abs": "https://arxiv.org/abs/2502.13962", "title": "Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering", "authors": ["William Jurayj", "Jeffrey Cheng", "Benjamin Van Durme"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. Code: https://github.com/wjurayj/final_answer", "summary": "Scaling the test-time compute of large language models has demonstrated\nimpressive performance on reasoning benchmarks. However, existing evaluations\nof test-time scaling make the strong assumption that a reasoning system should\nalways give an answer to any question provided. This overlooks concerns about\nwhether a model is confident in its answer, and whether it is appropriate to\nalways provide a response. To address these concerns, we extract confidence\nscores during reasoning for thresholding model responses. We find that\nincreasing compute budget at inference time not only helps models answer more\nquestions correctly, but also increases confidence in correct responses. We\nthen extend the current paradigm of zero-risk responses during evaluation by\nconsidering settings with non-zero levels of response risk, and suggest a\nrecipe for reporting evaluations under these settings."}
{"id": "2503.04800", "pdf": "https://arxiv.org/pdf/2503.04800.pdf", "abs": "https://arxiv.org/abs/2503.04800", "title": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation", "authors": ["Jie Ouyang", "Tingyue Pan", "Mingyue Cheng", "Ruiran Yan", "Yucong Luo", "Jiaying Lin", "Qi Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it still faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures the evolution of temporal knowledge in\nreal-world facts. Through comprehensive experiments, we reveal that outdated\ninformation significantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG. Our code and data are available at:\nhttps://github.com/0russwest0/HoH."}
{"id": "2504.02768", "pdf": "https://arxiv.org/pdf/2504.02768.pdf", "abs": "https://arxiv.org/abs/2504.02768", "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs", "authors": ["Jaap Jumelet", "Leonie Weissweiler", "Joakim Nivre", "Arianna Bisazza"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages and 2 types of subject-verb agreement,\ncontaining more than 128,000 minimal pairs. Our minimal pairs are created using\na fully automated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages"}
{"id": "2504.14452", "pdf": "https://arxiv.org/pdf/2504.14452.pdf", "abs": "https://arxiv.org/abs/2504.14452", "title": "ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data", "authors": ["Tong Chen", "Faeze Brahman", "Jiacheng Liu", "Niloofar Mireshghallah", "Weijia Shi", "Pang Wei Koh", "Luke Zettlemoyer", "Hannaneh Hajishirzi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Language models (LMs) can memorize and reproduce segments from their\npretraining data verbatim even in non-adversarial settings, raising concerns\nabout copyright, plagiarism, privacy, and creativity. We introduce Paraphrase\nPreference Optimization (ParaPO), a post-training method that fine-tunes LMs to\nreduce unintentional regurgitation while preserving their overall utility.\nParaPO trains LMs to prefer paraphrased versions of memorized segments over the\noriginal verbatim content from the pretraining data. To maintain the ability to\nrecall famous quotations when appropriate, we develop a variant of ParaPO that\nuses system prompts to control regurgitation behavior. In our evaluation on\nLlama3.1-8B, ParaPO consistently reduces regurgitation across all tested\ndatasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative\nwriting), whereas unlearning methods used in prior work to mitigate\nregurgitation are less effective outside their targeted unlearned domain (from\n17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO\nwith system prompting successfully preserves famous quotation recall while\nreducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when\nprompted not to regurgitate. In contrast, without ParaPO tuning, prompting the\nmodel not to regurgitate produces only a marginal reduction (8.7 to 8.4)."}
{"id": "2504.21801", "pdf": "https://arxiv.org/pdf/2504.21801.pdf", "abs": "https://arxiv.org/abs/2504.21801", "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition", "authors": ["Z. Z. Ren", "Zhihong Shao", "Junxiao Song", "Huajian Xin", "Haocheng Wang", "Wanjia Zhao", "Liyue Zhang", "Zhe Fu", "Qihao Zhu", "Dejian Yang", "Z. F. Wu", "Zhibin Gou", "Shirong Ma", "Hongxuan Tang", "Yuxuan Liu", "Wenjun Gao", "Daya Guo", "Chong Ruan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce DeepSeek-Prover-V2, an open-source large language model designed\nfor formal theorem proving in Lean 4, with initialization data collected\nthrough a recursive theorem proving pipeline powered by DeepSeek-V3. The\ncold-start training procedure begins by prompting DeepSeek-V3 to decompose\ncomplex problems into a series of subgoals. The proofs of resolved subgoals are\nsynthesized into a chain-of-thought process, combined with DeepSeek-V3's\nstep-by-step reasoning, to create an initial cold start for reinforcement\nlearning. This process enables us to integrate both informal and formal\nmathematical reasoning into a unified model. The resulting model,\nDeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural\ntheorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49\nout of 658 problems from PutnamBench. In addition to standard benchmarks, we\nintroduce ProverBench, a collection of 325 formalized problems, to enrich our\nevaluation, including 15 selected problems from the recent AIME competitions\n(years 24-25). Further evaluation on these 15 AIME problems shows that the\nmodel successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of\nthese problems using majority voting, highlighting that the gap between formal\nand informal mathematical reasoning in large language models is substantially\nnarrowing."}
{"id": "2505.12723", "pdf": "https://arxiv.org/pdf/2505.12723.pdf", "abs": "https://arxiv.org/abs/2505.12723", "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "authors": ["Haoyuan Wu", "Rui Ming", "Jilong Gao", "Hangyu Zhao", "Xueyi Chen", "Yikai Yang", "Haisheng Zheng", "Zhuolun He", "Bei Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages."}
{"id": "2505.14523", "pdf": "https://arxiv.org/pdf/2505.14523.pdf", "abs": "https://arxiv.org/abs/2505.14523", "title": "Exploring Graph Representations of Logical Forms for Language Modeling", "authors": ["Michael Sullivan"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "To be published in ACL 2025 Findings", "summary": "We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs (BERT) pretrained on the same data, indicating that LFLMs can\nlearn with substantially less data than models over plain text. Furthermore, we\nshow that the performance of this model is likely to scale with additional\nparameters and pretraining data, suggesting the viability of LFLMs in\nreal-world applications."}
{"id": "2505.20015", "pdf": "https://arxiv.org/pdf/2505.20015.pdf", "abs": "https://arxiv.org/abs/2505.20015", "title": "On the class of coding optimality of human languages and the origins of Zipf's law", "authors": ["Ramon Ferrer-i-Cancho"], "categories": ["cs.CL", "physics.soc-ph"], "comment": "typos corrected; discussion enhanced", "summary": "Here we present a new class of optimality for coding systems. Members of that\nclass are displaced linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Within that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are displaced by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. We provide support for\nthe hypothesis that Zipf's law originates from compression and define testable\nconditions for the emergence of Zipf's law in compressing systems."}
{"id": "2506.22598", "pdf": "https://arxiv.org/pdf/2506.22598.pdf", "abs": "https://arxiv.org/abs/2506.22598", "title": "RExBench: Can coding agents autonomously implement AI research extensions?", "authors": ["Nicholas Edwards", "Yukyung Lee", "Yujun Audrey Mao", "Yulu Qin", "Sebastian Schuster", "Najoung Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Agents based on Large Language Models (LLMs) have shown promise for\nperforming sophisticated software engineering tasks autonomously. In addition,\nthere has been progress towards developing agents that can perform parts of the\nresearch pipeline in machine learning and the natural sciences. We argue that\nresearch extension and its implementation is a critical capability for such\nsystems, and introduce RExBench to support the evaluation of this capability.\nRExBench is a benchmark consisting of 12 realistic research experiment\nimplementation tasks that aim to investigate research hypotheses that have not\npreviously been implemented. Each task is set up as an extension to an existing\nresearch paper and codebase, accompanied by domain expert-written instructions.\nRExBench is robust to data contamination, and supports an automatic evaluation\ninfrastructure that executes agent outputs to determine whether the success\ncriteria are met. We use this benchmark to evaluate nine LLM agents implemented\nusing three different frameworks: aider, Claude Code, and OpenHands. We find\nthat all agents evaluated fail to autonomously implement the majority of the\nextensions. Although the success rate improves with additional human-written\nhints, the best performance under this setting remains below 40%. This\nindicates that current agents are still short of being able to handle realistic\nresearch extension tasks without substantial human guidance."}
{"id": "2506.24068", "pdf": "https://arxiv.org/pdf/2506.24068.pdf", "abs": "https://arxiv.org/abs/2506.24068", "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines", "authors": ["Ian R. McKenzie", "Oskar J. Hollinsworth", "Tom Tseng", "Xander Davies", "Stephen Casper", "Aaron D. Tucker", "Robert Kirk", "Adam Gleave"], "categories": ["cs.CL", "cs.AI"], "comment": "Fixed typos (including Figure 1), amended GPU-hours rather than days,\n  clarified ReNeLLM prompt modifications", "summary": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks."}
{"id": "2507.06229", "pdf": "https://arxiv.org/pdf/2507.06229.pdf", "abs": "https://arxiv.org/abs/2507.06229", "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving", "authors": ["Xiangru Tang", "Tianrui Qin", "Tianhao Peng", "Ziyang Zhou", "Daniel Shao", "Tingting Du", "Xinming Wei", "Peng Xia", "Fang Wu", "He Zhu", "Ge Zhang", "Jiaheng Liu", "Xingyao Wang", "Sirui Hong", "Chenglin Wu", "Hao Cheng", "Chi Wang", "Wangchunshu Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current AI agents cannot effectively learn from each other's problem-solving\nexperiences or use past successes to guide self-reflection and error correction\nin new tasks. We introduce Agent KB, a shared knowledge base that captures both\nhigh-level problem-solving strategies and detailed execution lessons, enabling\nknowledge transfer across agent frameworks. Agent KB implements a novel\nteacher-student dual-phase retrieval mechanism where student agents retrieve\nworkflow-level patterns for strategic guidance while teacher agents identify\nexecution-level patterns for refinement. This hierarchical approach enables\nagents to break out of limited reasoning pathways by incorporating diverse\nstrategies from external sources. Evaluations on the GAIA benchmark demonstrate\nsubstantial performance gains, with Agent KB improving success rates by up to\n6.06 percentage points overall under pass@1. For SWE-bench code repair tasks,\nour system significantly improved resolution rates, with o3-mini achieving an\n8.67 percentage point gain (23 percent to 31.67 percent) in pass@1."}
{"id": "2507.08924", "pdf": "https://arxiv.org/pdf/2507.08924.pdf", "abs": "https://arxiv.org/abs/2507.08924", "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available."}
{"id": "2507.12547", "pdf": "https://arxiv.org/pdf/2507.12547.pdf", "abs": "https://arxiv.org/abs/2507.12547", "title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models", "authors": ["Lionel Wong", "Katherine M. Collins", "Lance Ying", "Cedegao E. Zhang", "Adrian Weller", "Tobias Gerstenberg", "Timothy O'Donnell", "Alexander K. Lew", "Jacob D. Andreas", "Joshua B. Tenenbaum", "Tyler Brooke-Wilson"], "categories": ["cs.CL", "cs.AI", "cs.PL"], "comment": "Presented at CogSci 2025", "summary": "When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains."}
{"id": "2507.13205", "pdf": "https://arxiv.org/pdf/2507.13205.pdf", "abs": "https://arxiv.org/abs/2507.13205", "title": "Automatically assessing oral narratives of Afrikaans and isiXhosa children", "authors": ["Retief Louw", "Emma Sharratt", "Febe de Wet", "Christiaan Jacobs", "Annelien Smith", "Herman Kamper"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to SLaTE 2025", "summary": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning."}
{"id": "2407.14506", "pdf": "https://arxiv.org/pdf/2407.14506.pdf", "abs": "https://arxiv.org/abs/2407.14506", "title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Lu Yuan", "Leonid Sigal"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "NeurIPS 2024 Workshop on Adaptive Foundation Models", "summary": "Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types."}
{"id": "2408.06345", "pdf": "https://arxiv.org/pdf/2408.06345.pdf", "abs": "https://arxiv.org/abs/2408.06345", "title": "Deep Learning based Key Information Extraction from Business Documents: Systematic Literature Review", "authors": ["Alexander Michael Rombach", "Peter Fettke"], "categories": ["cs.IR", "cs.CL", "cs.LG", "A.1; I.2.7; I.4.9; I.7.5"], "comment": "62 pages, 7 figures, 10 tables; This version represents the accepted\n  author-version without final copyediting. ACM Computing Surveys source:\n  https://dl.acm.org/doi/10.1145/3749369", "summary": "Extracting key information from documents represents a large portion of\nbusiness workloads and therefore offers a high potential for efficiency\nimprovements and process automation. With recent advances in Deep Learning, a\nplethora of Deep Learning based approaches for Key Information Extraction have\nbeen proposed under the umbrella term Document Understanding that enable the\nprocessing of complex business documents. The goal of this systematic\nliterature review is an in-depth analysis of existing approaches in this domain\nand the identification of opportunities for further research. To this end, 130\napproaches published between 2017 and 2024 are analyzed in this study."}
{"id": "2410.07094", "pdf": "https://arxiv.org/pdf/2410.07094.pdf", "abs": "https://arxiv.org/abs/2410.07094", "title": "An Approach for Auto Generation of Labeling Functions for Software Engineering Chatbots", "authors": ["Ebube Alor", "Ahmad Abdellatif", "SayedHassan Khatoonabadi", "Emad Shihab"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Submitted to ACM Transactions on Software Engineering and Methodology\n  for review", "summary": "Software engineering (SE) chatbots are increasingly gaining attention for\ntheir role in enhancing development processes. At the core of chatbots are\nNatural Language Understanding platforms (NLUs), which enable them to\ncomprehend user queries but require labeled data for training. However,\nacquiring such labeled data for SE chatbots is challenging due to the scarcity\nof high-quality datasets, as training requires specialized vocabulary and\nphrases not found in typical language datasets. Consequently, developers often\nresort to manually annotating user queries -- a time-consuming and\nresource-intensive process. Previous approaches require human intervention to\ngenerate rules, called labeling functions (LFs), that categorize queries based\non specific patterns. To address this issue, we propose an approach to\nautomatically generate LFs by extracting patterns from labeled user queries. We\nevaluate our approach on four SE datasets and measure performance improvement\nfrom training NLUs on queries labeled by the generated LFs. The generated LFs\neffectively label data with AUC scores up to 85.3% and NLU performance\nimprovements up to 27.2%. Furthermore, our results show that the number of LFs\naffects labeling performance. We believe that our approach can save time and\nresources in labeling users' queries, allowing practitioners to focus on core\nchatbot functionalities rather than manually labeling queries."}
{"id": "2501.03572", "pdf": "https://arxiv.org/pdf/2501.03572.pdf", "abs": "https://arxiv.org/abs/2501.03572", "title": "From Code to Compliance: Assessing ChatGPT's Utility in Designing an Accessible Webpage -- A Case Study", "authors": ["Ammar Ahmed", "Margarida Fresco", "Fredrik Forsberg", "Hallvard Grotli"], "categories": ["cs.HC", "cs.AI", "cs.CL", "D.1.2; F.3.1; F.4.1; D.3.2; H.1.2; H.5.2; D.2.2; H.1.2; I.3.6;\n  H.5.4; H.5.1"], "comment": null, "summary": "Web accessibility ensures that individuals with disabilities can access and\ninteract with digital content without barriers, yet a significant majority of\nmost used websites fail to meet accessibility standards. This study evaluates\nChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web\nContent Accessibility Guidelines (WCAG). While ChatGPT can effectively address\naccessibility issues when prompted, its default code often lacks compliance,\nreflecting limitations in its training data and prevailing inaccessible web\npractices. Automated and manual testing revealed strengths in resolving simple\nissues but challenges with complex tasks, requiring human oversight and\nadditional iterations. Unlike prior studies, we incorporate manual evaluation,\ndynamic elements, and use the visual reasoning capability of ChatGPT along with\nthe prompts to fix accessibility issues. Providing screenshots alongside\nprompts enhances the LLM's ability to address accessibility issues by allowing\nit to analyze surrounding components, such as determining appropriate contrast\ncolors. We found that effective prompt engineering, such as providing concise,\nstructured feedback and incorporating visual aids, significantly enhances\nChatGPT's performance. These findings highlight the potential and limitations\nof large language models for accessible web development, offering practical\nguidance for developers to create more inclusive websites."}
{"id": "2501.06848", "pdf": "https://arxiv.org/pdf/2501.06848.pdf", "abs": "https://arxiv.org/abs/2501.06848", "title": "A General Framework for Inference-time Scaling and Steering of Diffusion Models", "authors": ["Raghav Singhal", "Zachary Horvitz", "Ryan Teehan", "Mengye Ren", "Zhou Yu", "Kathleen McKeown", "Rajesh Ranganath"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we present Feynman-Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models - even\nwith off-the-shelf rewards - can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering ."}
{"id": "2501.11264", "pdf": "https://arxiv.org/pdf/2501.11264.pdf", "abs": "https://arxiv.org/abs/2501.11264", "title": "Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian", "authors": ["Wannita Takerngsaksiri", "Chakkrit Tantithamthavorn", "Micheal Fu", "Jirat Pasuksmit", "Kun Chen", "Ming Wu"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "11 pages, 7 figures, 8 tables, Accepted at ICSME", "summary": "Software engineers spend a significant amount of time reading code during the\nsoftware development process, especially in the age of large language models\n(LLMs) that can automatically generate code. However, little is known about the\nreadability of the LLM-generated code and whether it is still important from\npractitioners' perspectives in this new era. In this paper, we conduct a survey\nto explore the practitioners' perspectives on code readability in the age of\nLLMs and investigate the readability of our LLM-based software development\nagents framework, HULA, by comparing its generated code with human-written code\nin real-world scenarios. Overall, the findings underscore that (1) readability\nremains a critical aspect of software development; (2) the readability of our\nLLM-generated code is comparable to human-written code, fostering the\nestablishment of appropriate trust and driving the broad adoption of our\nLLM-powered software development platform."}
{"id": "2502.00691", "pdf": "https://arxiv.org/pdf/2502.00691.pdf", "abs": "https://arxiv.org/abs/2502.00691", "title": "To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization", "authors": ["Haozhe Wang", "Long Li", "Chao Qu", "Fengming Zhu", "Weidi Xu", "Wei Chu", "Fangzhen Lin"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "Recent advances in mathematical problem-solving with language models (LMs)\nintegrate chain-of-thought (CoT) reasoning and code execution to harness their\ncomplementary strengths. However, existing hybrid frameworks exhibit a critical\nlimitation: they depend on externally dictated instructions or rigid\ncode-integration templates, lacking metacognitive awareness -- the capacity to\ndynamically evaluate intrinsic capabilities and autonomously determine when and\nhow to integrate tools. This rigidity motivates our study of autonomous code\nintegration, enabling models to adapt tool-usage strategies as their reasoning\nabilities evolve during training.\n  While reinforcement learning (RL) shows promise for boosting LLM reasoning at\nscale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning\nautonomous code integration due to inadequate exploration of the vast\ncombinatorial space of CoT-code interleaving patterns. To address this\nchallenge, we propose a novel Expectation-Maximization (EM) framework that\nsynergizes structured exploration (E-step) with off-policy RL optimization\n(M-step), creating a self-reinforcing cycle between metacognitive tool-use\ndecisions and evolving capabilities. Experiments reveal our method achieves\nsuperior results through improved exploration. Notably, our 7B model improves\nover 11% on MATH500 and 9.4% on AIME without o1-like CoT."}
{"id": "2502.02145", "pdf": "https://arxiv.org/pdf/2502.02145.pdf", "abs": "https://arxiv.org/abs/2502.02145", "title": "From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios", "authors": ["Yuan Gao", "Mattia Piccinini", "Korbinian Moller", "Amr Alanwar", "Johannes Betz"], "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": "Final Version and Paper Accepted at IEEE ITSC 2025", "summary": "Ensuring the safety of autonomous vehicles requires virtual scenario-based\ntesting, which depends on the robust evaluation and generation of\nsafety-critical scenarios. So far, researchers have used scenario-based testing\nframeworks that rely heavily on handcrafted scenarios as safety metrics. To\nreduce the effort of human interpretation and overcome the limited scalability\nof these approaches, we combine Large Language Models (LLMs) with structured\nscenario parsing and prompt engineering to automatically evaluate and generate\nsafety-critical driving scenarios. We introduce Cartesian and Ego-centric\nprompt strategies for scenario evaluation, and an adversarial generation module\nthat modifies trajectories of risk-inducing vehicles (ego-attackers) to create\ncritical scenarios. We validate our approach using a 2D simulation framework\nand multiple pre-trained LLMs. The results show that the evaluation module\neffectively detects collision scenarios and infers scenario safety. Meanwhile,\nthe new generation module identifies high-risk agents and synthesizes\nrealistic, safety-critical scenarios. We conclude that an LLM equipped with\ndomain-informed prompting techniques can effectively evaluate and generate\nsafety-critical driving scenarios, reducing dependence on handcrafted metrics.\nWe release our open-source code and scenarios at:\nhttps://github.com/TUM-AVS/From-Words-to-Collisions."}
{"id": "2502.03304", "pdf": "https://arxiv.org/pdf/2502.03304.pdf", "abs": "https://arxiv.org/abs/2502.03304", "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning", "authors": ["Qitao Tan", "Jun Liu", "Zheng Zhan", "Caiwei Ding", "Yanzhi Wang", "Xiaolong Ma", "Jaewoo Lee", "Jin Lu", "Geng Yuan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose Divergence-driven\nZeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer\nadaptation by incorporating projections to ZO updates, generating\ndiverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning. Our code is released at\nhttps://anonymous.4open.science/r/DiZO-E86D."}
{"id": "2502.12272", "pdf": "https://arxiv.org/pdf/2502.12272.pdf", "abs": "https://arxiv.org/abs/2502.12272", "title": "Learning to Reason at the Frontier of Learnability", "authors": ["Thomas Foster", "Anya Sims", "Johannes Forkel", "Mattie Fellows", "Jakob Foerster"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs."}
{"id": "2503.09567", "pdf": "https://arxiv.org/pdf/2503.09567.pdf", "abs": "https://arxiv.org/abs/2503.09567", "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Dengyun Peng", "Jiannan Guan", "Peng Wang", "Mengkang Hu", "Yuhang Zhou", "Te Gao", "Wanxiang Che"], "categories": ["cs.AI", "cs.CL"], "comment": "Paper list and Github tutorial are available at\n  https://github.com/LightChen233/Awesome-Long-Chain-of-Thought-Reasoning.\n  Update 250+ New Reference", "summary": "Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"inference-time scaling.\" This survey seeks\nto fill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and inference-time scaling,\noffering insights into how these processes manifest in practice. (4) Finally,\nwe identify significant research gaps and highlight promising future\ndirections, including the integration of multi-modal reasoning, efficiency\nimprovements, and enhanced knowledge frameworks. By providing a structured\noverview, this survey aims to inspire future research and further the\ndevelopment of logical reasoning in artificial intelligence."}
{"id": "2506.01551", "pdf": "https://arxiv.org/pdf/2506.01551.pdf", "abs": "https://arxiv.org/abs/2506.01551", "title": "EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation", "authors": ["Bingqian Lin", "Yunshuang Nie", "Khun Loun Zai", "Ziming Wei", "Mingfei Han", "Rongtao Xu", "Minzhe Niu", "Jianhua Han", "Liang Lin", "Cewu Lu", "Xiaodan Liang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Building Vision-Language Navigation (VLN) agents which can navigate following\nnatural language instructions is a long-standing goal in human-robot\ninteraction applications. Recent studies have revealed the potential of\ntraining open-source Large Language Models (LLMs) to unleash LLMs' reasoning\nability for improving navigation, and simultaneously mitigate the domain gap\nbetween LLMs' training corpus and the VLN task. However, these approaches\nprimarily adopt direct input-output mapping paradigms, causing the mapping\nlearning difficult and the navigational decisions unexplainable.\nChain-of-Thought (CoT) training is a promising way to improve both navigational\ndecision accuracy and interpretability, while the complexity of the navigation\ntask makes the perfect CoT labels unavailable and may lead to overfitting\nthrough pure CoT supervised fine-tuning. In this paper, we propose a novel\nsElf-improving embodied reasoning framework for boosting LLM-based\nvision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two\nstages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model\nwith formalized CoT labels to both activate the model's navigational reasoning\ncapabilities and increase the reasoning speed; (2) Self-Reflective\nPost-Training, where the model is iteratively trained with its own reasoning\noutputs as self-enriched CoT labels to enhance the supervision diversity. A\nself-reflective auxiliary task is also introduced to encourage learning correct\nreasoning patterns by contrasting with wrong ones. Experimental results on the\npopular VLN benchmarks demonstrate the superiority of EvolveNav over previous\nLLM-based VLN approaches. Code is available at\nhttps://github.com/expectorlin/EvolveNav."}
{"id": "2506.06941", "pdf": "https://arxiv.org/pdf/2506.06941.pdf", "abs": "https://arxiv.org/abs/2506.06941", "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "authors": ["Parshin Shojaee", "Iman Mirzadeh", "Keivan Alizadeh", "Maxwell Horton", "Samy Bengio", "Mehrdad Farajtabar"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "preprint", "summary": "Recent generations of language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers.\nWhile these models demonstrate improved performance on reasoning benchmarks,\ntheir fundamental capabilities, scaling properties, and limitations remain\ninsufficiently understood. Current evaluations primarily focus on established\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\nevaluation paradigm often suffers from contamination and does not provide\ninsights into the reasoning traces. In this work, we systematically investigate\nthese gaps with the help of controllable puzzle environments that allow precise\nmanipulation of complexity while maintaining consistent logical structures.\nThis setup enables the analysis of not only final answers but also the internal\nreasoning traces, offering insights into how LRMs think. Through extensive\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\nreasoning effort increases with problem complexity up to a point, then declines\ndespite having remaining token budget. By comparing LRMs with their standard\nLLM counterparts under same inference compute, we identify three performance\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\nhigh-complexity tasks where both models face complete collapse. We found that\nLRMs have limitations in exact computation: they fail to use explicit\nalgorithms and reason inconsistently across scales. We also investigate the\nreasoning traces in more depth, studying the patterns of explored solutions and\nanalyzing the models' computational behavior, shedding light on their\nstrengths, limitations, and raising questions about their reasoning\ncapabilities."}
{"id": "2506.17562", "pdf": "https://arxiv.org/pdf/2506.17562.pdf", "abs": "https://arxiv.org/abs/2506.17562", "title": "LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning", "authors": ["Haoxuan Che", "Haibo Jin", "Zhengrui Guo", "Yi Lin", "Cheng Jin", "Hao Chen"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by IEEE TMI", "summary": "LLMs have demonstrated significant potential in Medical Report Generation\n(MRG), yet their development requires large amounts of medical image-report\npairs, which are commonly scattered across multiple centers. Centralizing these\ndata is exceptionally challenging due to privacy regulations, thereby impeding\nmodel development and broader adoption of LLM-driven MRG models. To address\nthis challenge, we present FedMRG, the first framework that leverages Federated\nLearning (FL) to enable privacy-preserving, multi-center development of\nLLM-driven MRG models, specifically designed to overcome the critical challenge\nof communication-efficient LLM training under multi-modal data heterogeneity.\nTo start with, our framework tackles the fundamental challenge of communication\noverhead in FL-LLM tuning by employing low-rank factorization to efficiently\ndecompose parameter updates, significantly reducing gradient transmission costs\nand making LLM-driven MRG feasible in bandwidth-constrained FL settings.\nFurthermore, we observed the dual heterogeneity in MRG under the FL scenario:\nvarying image characteristics across medical centers, as well as diverse\nreporting styles and terminology preferences. To address this, we further\nenhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,\ncoupled with diagnosis-driven prompts, which capture both globally\ngeneralizable and locally distinctive features while maintaining diagnostic\naccuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder\nthat harmonizes generic and specialized adapters to address variations in\nreporting styles and terminology. Through extensive evaluation of our\nestablished FL-MRG benchmark, we demonstrate the generalizability and\nadaptability of FedMRG, underscoring its potential in harnessing multi-center\ndata and generating clinically accurate reports while maintaining communication\nefficiency."}
{"id": "2506.18183", "pdf": "https://arxiv.org/pdf/2506.18183.pdf", "abs": "https://arxiv.org/abs/2506.18183", "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?", "authors": ["Zhiting Mei", "Christina Zhang", "Tenny Yin", "Justin Lidard", "Ola Shorinwa", "Anirudha Majumdar"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models."}
{"id": "2507.04295", "pdf": "https://arxiv.org/pdf/2507.04295.pdf", "abs": "https://arxiv.org/abs/2507.04295", "title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop", "authors": ["Runcong Zhao", "Artem Bobrov", "Jiazheng Li", "Yulan He"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Effective feedback is essential for student learning but is time-intensive\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\npersonalised, curriculum-aligned feedback in science education. LearnLens\ncomprises three components: (1) an error-aware assessment module that captures\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\na structured, topic-linked memory chain rather than traditional\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\neducator-in-the-loop interface for customisation and oversight. LearnLens\naddresses key challenges in existing systems, offering scalable, high-quality\nfeedback that empowers both teachers and students."}
{"id": "2507.04469", "pdf": "https://arxiv.org/pdf/2507.04469.pdf", "abs": "https://arxiv.org/abs/2507.04469", "title": "The role of large language models in UI/UX design: A systematic literature review", "authors": ["Ammar Ahmed", "Ali Shariq Imran"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "This systematic literature review examines the role of large language models\n(LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies\npublished between 2022 and 2025. We identify key LLMs in use, including GPT-4,\nGemini, and PaLM, and map their integration across the design lifecycle, from\nideation to evaluation. Common practices include prompt engineering,\nhuman-in-the-loop workflows, and multimodal input. While LLMs are reshaping\ndesign processes, challenges such as hallucination, prompt instability, and\nlimited explainability persist. Our findings highlight LLMs as emerging\ncollaborators in design, and we propose directions for the ethical, inclusive,\nand effective integration of these technologies."}
{"id": "2507.05169", "pdf": "https://arxiv.org/pdf/2507.05169.pdf", "abs": "https://arxiv.org/abs/2507.05169", "title": "Critiques of World Models", "authors": ["Eric Xing", "Mingkai Deng", "Jinyu Hou", "Zhiting Hu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "comment": null, "summary": "World Model, the supposed algorithmic surrogate of the real-world environment\nwhich biological agents experience with and act upon, has been an emerging\ntopic in recent years because of the rising needs to develop virtual agents\nwith artificial (general) intelligence. There has been much debate on what a\nworld model really is, how to build it, how to use it, and how to evaluate it.\nIn this essay, starting from the imagination in the famed Sci-Fi classic Dune,\nand drawing inspiration from the concept of \"hypothetical thinking\" in\npsychology literature, we offer critiques of several schools of thoughts on\nworld modeling, and argue the primary goal of a world model to be simulating\nall actionable possibilities of the real world for purposeful reasoning and\nacting. Building on the critiques, we propose a new architecture for a\ngeneral-purpose world model, based on hierarchical, multi-level, and mixed\ncontinuous/discrete representations, and a generative and self-supervision\nlearning framework, with an outlook of a Physical, Agentic, and Nested (PAN)\nAGI system enabled by such a model."}
{"id": "2507.13142", "pdf": "https://arxiv.org/pdf/2507.13142.pdf", "abs": "https://arxiv.org/abs/2507.13142", "title": "From Roots to Rewards: Dynamic Tree Reasoning with RL", "authors": ["Ahmed Bahloul", "Simon Malberg"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern language models address complex questions through chain-of-thought\n(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,\n2021), yet struggle with error propagation and knowledge integration.\nTree-structured reasoning methods, particularly the Probabilistic\nTree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues\nby decomposing questions into hierarchical structures and selecting answers\nthrough confidence-weighted aggregation of parametric and retrieved knowledge\n(Yao et al., 2023). However, ProbTree's static implementation introduces two\nkey limitations: (1) the reasoning tree is fixed during the initial\nconstruction phase, preventing dynamic adaptation to intermediate results, and\n(2) each node requires exhaustive evaluation of all possible solution\nstrategies, creating computational inefficiency. We present a dynamic\nreinforcement learning (Sutton and Barto, 2018) framework that transforms\ntree-based reasoning into an adaptive process. Our approach incrementally\nconstructs the reasoning tree based on real-time confidence estimates, while\nlearning optimal policies for action selection (decomposition, retrieval, or\naggregation). This maintains ProbTree's probabilistic rigor while improving\nboth solution quality and computational efficiency through selective expansion\nand focused resource allocation. The work establishes a new paradigm for\ntreestructured reasoning that balances the reliability of probabilistic\nframeworks with the flexibility required for real-world question answering\nsystems."}
