<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 21]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.CL](#cs.CL) [Total: 36]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [quant-ph](#quant-ph) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 7]
- [cs.SE](#cs.SE) [Total: 3]
- [math.OC](#math.OC) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 11]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [eess.SY](#eess.SY) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Quality of explanation of xAI from the prespective of Italian end-users: Italian version of System Causability Scale (SCS)](https://arxiv.org/abs/2504.16193)
*Carmine Attanasio,Alireza Mortezapour*

Main category: cs.HC

TL;DR: The study validates the Italian version of the System Causability Scale (I-SCS) for measuring the quality of explanations in explainable AI (xAI).


<details>
  <summary>Details</summary>
Motivation: The study aims to ensure quality explanations of AI algorithms beyond computer science, addressing a need in the research community.

Method: The English version of the I-SCS was translated to Italian using the forward-backward translation method, followed by content validity index calculation and cognitive interviews with end users.

Result: Following validation, the Italian version of the I-SCS was refined to 9 questions after removing one due to low content validity, and was comprehended well by Italian end users.

Conclusion: The validated Italian version of the I-SCS can be utilized in future research and by xAI developers in Italian cultural contexts.

Abstract: Background and aim: Considering the scope of the application of artificial
intelligence beyond the field of computer science, one of the concerns of
researchers is to provide quality explanations about the functioning of
algorithms based on artificial intelligence and the data extracted from it. The
purpose of the present study is to validate the Italian version of system
causability scale (I-SCS) to measure the quality of explanations provided in a
xAI.
  Method: For this purpose, the English version, initially provided in 2020 in
coordination with the main developer, was utilized. The forward-backward
translation method was applied to ensure accuracy. Finally, these nine steps
were completed by calculating the content validity index/ratio and conducting
cognitive interviews with representative end users.
  Results: The original version of the questionnaire consisted of 10 questions.
However, based on the obtained indexes (CVR below 0.49), one question (Question
8) was entirely removed. After completing the aforementioned steps, the Italian
version contained 9 questions. The representative sample of Italian end users
fully comprehended the meaning and content of the questions in the Italian
version.
  Conclusion: The Italian version obtained in this study can be used in future
research studies as well as in the field by xAI developers. This tool can be
used to measure the quality of explanations provided for an xAI system in
Italian culture.

</details>


### [2] [Subthreshold Jitter in VR Can Induce Visual Discomfort](https://arxiv.org/abs/2504.16295)
*Samuel J. Levulis,Kevin W. Rio,James Wilmott,Charlie S. Burlingham,Phillip Guan*

Main category: cs.HC

TL;DR: The study investigates how subthreshold visual-vestibular conflicts can induce discomfort in virtual reality, emphasizing the importance of frequent surveys in detecting symptoms.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing VR comfort studies that focus on extreme visual-vestibular conflicts and their generalizability to naturalistic VR experiences.

Method: A psychophysical study was conducted to determine perceptual thresholds for sinusoidal noise in render pose. Subthreshold jitter was introduced during gameplay in a VR environment, and discomfort was measured using traditional and frequent surveys.

Result: Participants experienced visual discomfort during VR sessions with subthreshold jitter, but traditional pre- and post-test measures did not show significant differences, while time-resolved surveys revealed significant discomfort changes.

Conclusion: The findings suggest that lightweight, frequent surveys are crucial for accurately measuring visual discomfort in naturalistic VR usage scenarios.

Abstract: Visual-vestibular conflicts (VVCs) are a primary contributor to visually
induced motion sickness (VIMS) in head-mounted displays (HMDs). However,
virtual reality (VR) comfort studies often rely on exposing seated or standing
users to experiences with high intensity visual motion (such as roller
coasters). These drastic VVCs tend to induce pronounced VIMS symptoms that can
be reliably detected across individuals using common survey measures. The
conclusions from studies using these extreme motion-based conflicts may not
accurately generalize to naturalistic use cases in VR where efforts are made to
minimize, rather than maximize, VIMS symptoms. In this work, we show that a
subthreshold visual-vestibular conflict can induce measurable discomfort during
naturalistic, long duration use. We first present a psychophysical study,
conducted outside of an HMD, to rigorously identify the perceptual thresholds
for sinusoidal noise in render pose (i.e., jitter) resulting in erroneous 3D
motion of rendered content. We next introduce subthreshold levels of jitter to
a Meta Quest 3 VR HMD and demonstrate that this can induce visual discomfort in
participants playing the commercially-available game Cubism across a
three-session, repeated-measures study. Importantly, we did not identify
statistically significant comfort differences between control and jitter
conditions with traditional pre- and post-test comparison of Simulator Sickness
Questionnaire (SSQ) scores. Significant differences were only identified using
the Motion Illness Symptoms Classification (MISC) survey administered every 10
minutes across each 90 minute session. This highlights the benefits of
incorporating time-resolved data points and suggests that lightweight, more
frequent surveys may be important tools for measuring visual discomfort in more
ecologically-valid scenarios.

</details>


### [3] [Media Content Atlas: A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs](https://arxiv.org/abs/2504.16323)
*Merve Cerit,Eric Zelikman,Mu-Jung Cho,Thomas N. Robinson,Byron Reeves,Nilam Ram,Nick Haber*

Main category: cs.HC

TL;DR: The Media Content Atlas (MCA) is a new tool for analyzing large-scale screen data using multimodal large language models (MLLMs), enabling advanced content analysis and visualization.


<details>
  <summary>Details</summary>
Motivation: The ongoing evolution of digital media necessitates flexible and scalable tools to study complex media experiences effectively.

Method: The MCA pipeline employs MLLMs for moment-by-moment content analysis, clustering, topic modeling, image retrieval, and visualization, evaluated on 1.12 million smartphone screenshots over a month from 112 adults.

Result: Expert evaluators found MCA's clustering highly relevant (96% rated) and descriptions largely accurate (83% rated), demonstrating its capability for open-ended exploration and hypothesis generation.

Conclusion: MCA enhances both inductive and deductive research methods, offering significant potential for advancing media and HCI research.

Abstract: As digital media use continues to evolve and influence various aspects of
life, developing flexible and scalable tools to study complex media experiences
is essential. This study introduces the Media Content Atlas (MCA), a novel
pipeline designed to help researchers investigate large-scale screen data
beyond traditional screen-use metrics. Leveraging multimodal large language
models (MLLMs), MCA enables moment-by-moment content analysis, content-based
clustering, topic modeling, image retrieval, and interactive visualizations.
Evaluated on 1.12 million smartphone screenshots continuously captured during
screen use from 112 adults over an entire month, MCA facilitates open-ended
exploration and hypothesis generation as well as hypothesis-driven
investigations at an unprecedented scale. Expert evaluators underscored its
usability and potential for research and intervention design, with clustering
results rated 96% relevant and descriptions 83% accurate. By bridging
methodological possibilities with domain-specific needs, MCA accelerates both
inductive and deductive inquiry, presenting new opportunities for media and HCI
research.

</details>


### [4] [What Sensors See, What People Feel: Exploring Subjective Collaboration Perception in Mixed Reality](https://arxiv.org/abs/2504.16373)
*Yasra Chandio,Diana Romero,Salma Elmalaki,Fatima Anwar*

Main category: cs.HC

TL;DR: The paper introduces the Sensor-to-Subjective (S2S) Mapping Framework linking observable behaviors in Mixed Reality (MR) to users' subjective experiences in collaboration, validated through a study with 48 participants.


<details>
  <summary>Details</summary>
Motivation: To address the disconnect between observable behavioral signals in Mixed Reality and the subjective experiences of users during collaboration, which are influenced by internal states.

Method: The study introduced the Sensor-to-Subjective Mapping Framework, capturing behavioral signals through sensor data like shared gaze and speech, and tested the model with 48 participants in collaborative MR tasks.

Result: The study found a correlation between sensed behaviors, such as shared attention and proximity, and users' perceived collaboration experiences.

Conclusion: The S2S Mapping Framework effectively links quantifiable sensor data to qualitative subjective experiences in MR collaboration, enhancing our understanding of user interactions.

Abstract: Mixed Reality (MR) enables rich, embodied collaboration, yet it's uncertain
if sensor and system-logged behavioral signals capture how users experience
that collaboration. This disconnect stems from a fundamental gap: behavioral
signals are observable and continuous, while collaboration is interpreted
subjectively, shaped by internal states like presence, cognitive availability,
and social awareness. Our core insight is that sensor signals serve as
observable manifestations of subjective experiences in MR collaboration, and
they can be captured through sensor data such as shared gaze, speech, spatial
movement, and other system-logged performance metrics. We propose the
Sensor-to-Subjective (S2S) Mapping Framework, a conceptual model that links
observable interaction patterns to users' subjective perceptions of
collaboration and internal cognitive states through sensor-based indicators and
task performance metrics. To validate this model, we conducted a study with 48
participants across 12 MR groups engaged in a collaborative image-sorting task.
Our findings show a correlation between sensed behavior and perceived
collaboration, particularly through shared attention and proximity.

</details>


### [5] [Cyberoception: Finding a Painlessly-Measurable New Sense in the Cyberworld Towards Emotion-Awareness in Computing](https://arxiv.org/abs/2504.16378)
*Tadashi Okoshi,Zexiong Gao,Tan Yi Zhen,Takumi Karasawa,Takeshi Miki,Wataru Sasaki,Rajesh K. Balan*

Main category: cs.HC

TL;DR: The study introduces the concept of "cyberoception," a new form of interoception that can be measured using smartphone sensors, aimed at enhancing emotion recognition in human-computer interactions.


<details>
  <summary>Details</summary>
Motivation: Accurate emotion recognition in affective computing requires a deep understanding of individual differences in emotional abilities, which existing interoception measurement methods fail to achieve in real-world settings.

Method: A hybrid 10-day experiment combining in-lab and in-the-wild approaches to explore the relationship between a new type of cyberoception and users' emotional states, measured through smartphone behavior.

Result: The 'Turn On' cyberoception type, reflecting users' perception of how often they turn on their smartphones, was found to significantly correlate with participants' emotional valence.

Conclusion: Cyberoception has the potential to enhance emotion recognition and user experience in applications by leveraging everyday smartphone usage, serving as a foundational concept for user-friendly and emotion-aware technologies.

Abstract: In Affective computing, recognizing users' emotions accurately is the basis
of affective human-computer interaction. Understanding users' interoception
contributes to a better understanding of individually different emotional
abilities, which is essential for achieving inter-individually accurate emotion
estimation. However, existing interoception measurement methods, such as the
heart rate discrimination task, have several limitations, including their
dependence on a well-controlled laboratory environment and precision apparatus,
making monitoring users' interoception challenging. This study aims to
determine other forms of data that can explain users' interoceptive or similar
states in their real-world lives and propose a novel hypothetical concept
"cyberoception," a new sense (1) which has properties similar to interoception
in terms of the correlation with other emotion-related abilities, and (2) which
can be measured only by the sensors embedded inside commodity smartphone
devices in users' daily lives. Results from a 10-day-long in-lab/in-the-wild
hybrid experiment reveal a specific cyberoception type "Turn On" (users'
subjective sensory perception about the frequency of turning-on behavior on
their smartphones), significantly related to participants' emotional valence.
We anticipate that cyberoception to serve as a fundamental building block for
developing more "emotion-aware", user-friendly applications and services.

</details>


### [6] [FeedQUAC: Quick Unobtrusive AI-Generated Commentary](https://arxiv.org/abs/2504.16416)
*Tao Long,Kendra Wannamaker,Jo Vermeulen,George Fitzmaurice,Justin Matejka*

Main category: cs.HC

TL;DR: The paper presents FeedQUAC, an AI design companion that provides ambient feedback in real-time to enhance the creative workflows of designers.


<details>
  <summary>Details</summary>
Motivation: Gathering constant feedback during the design process can be labor-intensive and disruptive, creating the need for a more seamless solution.

Method: A design probe study with eight participants was conducted to explore the effectiveness of FeedQUAC, which delivers real-time AI-generated commentary from various personas.

Result: Participants reported benefits including convenience, playfulness, a boost in confidence, and inspiration, while suggesting additional features such as chat interaction and context curation.

Conclusion: AI feedback can significantly enhance creative workflows, revealing its strengths, limitations, and how it can be integrated into existing design processes, highlighting the importance of ambient interaction in future creativity support systems.

Abstract: Design thrives on feedback. However, gathering constant feedback throughout
the design process can be labor-intensive and disruptive. We explore how AI can
bridge this gap by providing effortless, ambient feedback. We introduce
FeedQUAC, a design companion that delivers real-time AI-generated commentary
from a variety of perspectives through different personas. A design probe study
with eight participants highlights how designers can leverage quick yet ambient
AI feedback to enhance their creative workflows. Participants highlight
benefits such as convenience, playfulness, confidence boost, and inspiration
from this lightweight feedback agent, while suggesting additional features,
like chat interaction and context curation. We discuss the role of AI feedback,
its strengths and limitations, and how to integrate it into existing design
workflows while balancing user involvement. Our findings also suggest that
ambient interaction is a valuable consideration for both the design and
evaluation of future creativity support systems.

</details>


### [7] [Advancing Radar Hand Gesture Recognition: A Hybrid Spectrum Synthetic Framework Merging Simulation with Neural Networks](https://arxiv.org/abs/2504.16423)
*Jiaqi Tang,Xinbo Xu,Yinsong Xu,Qingchao Chen*

Main category: cs.HC

TL;DR: A novel hybrid framework for synthetic radar data generation improves hand gesture recognition performance by addressing simulation challenges and data scarcity.


<details>
  <summary>Details</summary>
Motivation: Limited radar datasets hinder hand gesture recognition performance, and existing synthetic data methods fail to accurately simulate radar wave interactions and converge effectively.

Method: The proposed framework combines a cylinder mesh-based hand reflection model with RadarWeightNet, a small-scale neural network that assigns weights to simulated radar signals, addressing both geometric complexities and simulation-to-real gaps.

Result: The hybrid framework achieved up to 63% SSIM in synthetic performance and a 30% improvement in classification performance in few-shot learning scenarios, demonstrating its effectiveness under data scarcity conditions.

Conclusion: The framework successfully balances physical accuracy and machine learning adaptability, enhancing hand gesture recognition capabilities in situations with limited radar data.

Abstract: Millimeter wave (mmWave) radar sensors play a vital role in hand gesture
recognition (HGR) by detecting subtle motions while preserving user privacy.
However, the limited scale of radar datasets hinders the performance. Existing
synthetic data generation methods fall short in two key areas. On the one hand,
modeling-based approaches fail to accurately simulate the wave propagation and
reflection at the hand-gesture level, facing unique complexities such as
diffraction and occlusion. On the other hand, generative model-based methods
are hard to converge while radar data is limited, lacking interpretability, and
sometimes fail to produce kinematically plausible results. To overcome these
limitations, we propose a novel hybrid spectrum synthetic framework leveraging
visual hand gesture data. It combines a cylinder mesh-based hand reflection
model with a small-scale neural network called RadarWeightNet, which focuses on
assigning weights to simulated signals. Our framework addresses two key
challenges: achieving accurate simulation of complex hand geometry and bridging
the simulation-to-real gap in a data-driven manner while preserving
interpretability, which balances physical accuracy with machine learning
adaptability. We tested our framework under extreme scenarios where radar data
is scarce. The results demonstrate the effectiveness of our hybrid framework,
achieving up to 63% SSIM in synthetic performance and up to 30% improvement in
classification performance in few-shot learning.

</details>


### [8] [Insect-Computer Hybrid Speaker: Speaker using Chirp of the Cicada Controlled by Electrical Muscle Stimulation](https://arxiv.org/abs/2504.16459)
*Yuga Tsukuda,Naoto Nishida,Jun Lu,Yoichi Ochiai*

Main category: cs.HC

TL;DR: This paper presents an Insect-Computer Hybrid Speaker that uses cicadas as speakers triggered through Electrical Muscle Stimulation (EMS) for music creation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the use of insects for interaction with third parties, an area that has been under-researched despite advancements in controlling insects and obtaining feedback.

Method: The method involves using Electrical Muscle Stimulation to trigger cicadas as speakers, investigating suitable waveforms, voltage ranges, and maximum chirp pitches.

Result: The study identifies effective waveform characteristics, voltage ranges, and the maximum pitch cicadas can produce for successful sound generation.

Conclusion: The findings can enhance the interface between biological organisms and technology, facilitating innovative music creation methods.

Abstract: We propose "Insect-Computer Hybrid Speaker", which enables us to make musics
made from combinations of computer and insects. Lots of studies have proposed
methods and interfaces for controlling insects and obtaining feedback. However,
there have been less research on the use of insects for interaction with third
parties. In this paper, we propose a method in which cicadas are used as
speakers triggered by using Electrical Muscle Stimulation (EMS). We explored
and investigated the suitable waveform of chirp to be controlled, the
appropriate voltage range, and the maximum pitch at which cicadas can chirp.

</details>


### [9] [Helping Blind People Grasp: Enhancing a Tactile Bracelet with an Automated Hand Navigation System](https://arxiv.org/abs/2504.16502)
*Marcin Furtak,Florian Pätzold,Tim Kietzmann,Silke M. Kärcher,Peter König*

Main category: cs.HC

TL;DR: A tactile bracelet has been developed to assist visually impaired individuals in grasping objects through guided vibrations.


<details>
  <summary>Details</summary>
Motivation: To improve grasping abilities and quality of life for visually impaired individuals by providing assistive technology.

Method: The study involved a fully automated tactile bracelet system that detects and tracks target objects, guiding the user's hand with vibration commands through various tasks simulating real-life scenarios.

Result: The system successfully guided participants in tasks to grasp objects, track specific items among distractors, and navigate obstacles, even in less structured environments, demonstrating its practical utility.

Conclusion: The bracelet enhances the ability of visually impaired users to autonomously interact with their environment, indicating a significant improvement in their quality of life.

Abstract: Grasping constitutes a critical challenge for visually impaired people. To
address this problem, we developed a tactile bracelet that assists in grasping
by guiding the user's hand to a target object using vibration commands. Here we
demonstrate the fully automated system around the bracelet, which can
confidently detect and track target and distractor objects and reliably guide
the user's hand. We validate our approach in three tasks that resemble complex,
everyday use cases. In a grasping task, the participants grasp varying target
objects on a table, guided via the automated hand navigation system. In the
multiple objects task, participants grasp objects from the same class,
demonstrating our system's ability to track one specific object without
targeting surrounding distractor objects. Finally, the participants grasp one
specific target object by avoiding an obstacle along the way in the depth
navigation task, showcasing the potential to utilize our system's depth
estimations to navigate even complex scenarios. Additionally, we demonstrate
that the system can aid users in the real world by testing it in a less
structured environment with a blind participant. Overall, our results
demonstrate that the system, by translating the AI-processed visual inputs into
a reduced data rate of actionable signals, enables autonomous behavior in
everyday environments, thus potentially increasing the quality of life of
visually impaired people.

</details>


### [10] [SafeSpect: Safety-First Augmented Reality Heads-up Display for Drone Inspections](https://arxiv.org/abs/2504.16533)
*Peisen Xu,Jérémie Garcia,Wei Tsang Ooi,Christophe Jouffrais*

Main category: cs.HC

TL;DR: This paper presents an adaptive augmented reality (AR) interface designed to improve drone pilots' situational awareness and reduce cognitive load during operations.


<details>
  <summary>Details</summary>
Motivation: Current tablet interfaces for drone operations create cognitive overload and hinder situational awareness, necessitating a better solution to support pilots in safety-critical tasks.

Method: The authors conducted participatory design workshops with professional pilots to identify key features, subsequently developing an adaptive AR interface that switches between task and safety views. They evaluated it via a user study comparing a 2D tablet, static AR, and adaptive AR interfaces during a building inspection task.

Result: The user study with 15 participants revealed that the AR interface facilitated better access to safety information, and the adaptive AR system significantly lowered cognitive load while improving situational awareness, maintaining task performance.

Conclusion: The findings support the development of safety-first heads-up AR interfaces for drone operations, contributing valuable design insights.

Abstract: Current tablet-based interfaces for drone operations often impose a heavy
cognitive load on pilots and reduce situational awareness by dividing attention
between the video feed and the real world. To address these challenges, we
designed a heads-up augmented reality (AR) interface that overlays in-situ
information to support drone pilots in safety-critical tasks. Through
participatory design workshops with professional pilots, we identified key
features and developed an adaptive AR interface that dynamically switches
between task and safety views to prevent information overload. We evaluated our
prototype by creating a realistic building inspection task and comparing three
interfaces: a 2D tablet, a static AR, and our adaptive AR design. A user study
with 15 participants showed that the AR interface improved access to safety
information, while the adaptive AR interface reduced cognitive load and
enhanced situational awareness without compromising task performance. We offer
design insights for developing safety-first heads-up AR interfaces.

</details>


### [11] [Exploring human-SAV interaction using large language models: The impact of psychological ownership and anthropomorphism on user experience](https://arxiv.org/abs/2504.16548)
*Lirui Guo,Michael G. Burke,Wynita M. Griggs*

Main category: cs.HC

TL;DR: The paper examines how conversational UIs powered by LLMs influence users' perceptions and adoption of shared autonomous vehicles (SAVs) through anthropomorphism and psychological ownership.


<details>
  <summary>Details</summary>
Motivation: To address the gap in research on how prompt strategies in LLM-powered SAV UIs impact users' perceptions and intentions to adopt this technology.

Method: Four SAV UIs with differing levels of anthropomorphic features and psychological ownership triggers were designed. Quantitative and qualitative measures related to psychological ownership, anthropomorphism, sentiment, and acceptance were collected after participant interactions with each UI.

Result: The anthropomorphic SAV conversational UI increased users' perception of human-like qualities and positively impacted the sentiment of responses compared to the control group.

Conclusion: Designing LLM-based conversational UIs with higher anthropomorphism and psychological ownership triggers can enhance user experience and promote the adoption of SAVs.

Abstract: There has been extensive prior work exploring how psychological factors such
as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs).
However, limited research has been conducted on how prompt strategies in large
language model (LLM)-powered SAV User Interfaces (UIs) affect users'
perceptions, experiences, and intentions to adopt such technology. In this
work, we investigate how conversational UIs powered by LLMs drive these
psychological factors and psychological ownership, the sense of possession a
user may come to feel towards an entity or object they may not legally own. We
designed four SAV UIs with varying levels of anthropomorphic characteristics
and psychological ownership triggers. Quantitative measures of psychological
ownership, anthropomorphism, quality of service, disclosure tendency, sentiment
of SAV responses, and overall acceptance were collected after participants
interacted with each SAV. Qualitative feedback was also gathered regarding the
experience of psychological ownership during the interactions. The results
indicate that an SAV conversational UI designed to be more anthropomorphic and
to induce psychological ownership improved users' perceptions of the SAV's
human-like qualities and improved the sentiment of responses compared to a
control condition. These findings provide practical guidance for designing
LLM-based conversational UIs that enhance user experience and adoption of SAVs.

</details>


### [12] [A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments](https://arxiv.org/abs/2504.16562)
*Julian Rasch,Florian Müller,Francesco Chiossi*

Main category: cs.HC

TL;DR: The paper discusses AI-driven adaptive content placement in Augmented Reality (AR) to enhance user interaction and experience.


<details>
  <summary>Details</summary>
Motivation: To address the challenges existing AR systems face in managing interactive possibilities and improve user engagement in varied environments.

Method: The paper proposes leveraging machine learning methods for dynamic content placement that responds to user movement and environmental changes.

Result: The envisioned system would optimize content distribution between AR projections and static content, aiming to reduce cognitive load on users and improve the UI layout.

Conclusion: The paper outlines a vision for innovative, intuitive, and effective AI-powered AR experiences across various industries.

Abstract: Augmented Reality (AR) is transforming the way we interact with virtual
information in the physical world. By overlaying digital content in real-world
environments, AR enables new forms of immersive and engaging experiences.
However, existing AR systems often struggle to effectively manage the many
interactive possibilities that AR presents. This vision paper speculates on
AI-driven approaches for adaptive AR content placement, dynamically adjusting
to user movement and environmental changes. By leveraging machine learning
methods, such a system would intelligently manage content distribution between
AR projections integrated into the external environment and fixed static
content, enabling seamless UI layout and potentially reducing users' cognitive
load. By exploring the possibilities of AI-driven dynamic AR content placement,
we aim to envision new opportunities for innovation and improvement in various
industries, from urban navigation and workplace productivity to immersive
learning and beyond. This paper outlines a vision for the development of more
intuitive, engaging, and effective AI-powered AR experiences.

</details>


### [13] [Bridging Data Gaps and Building Knowledge Networks in Indian Football Analytics](https://arxiv.org/abs/2504.16572)
*Sneha Nanavati,Nimmi Rangaswamy*

Main category: cs.HC

TL;DR: The paper examines the limitations and potential of football analytics adoption in India, focusing on grassroots innovation and informal networks to overcome challenges.


<details>
  <summary>Details</summary>
Motivation: The rise of football analytics globally contrasts with India, where institutional, infrastructural, and cultural challenges hinder its adoption.

Method: The study employs mixed methods, including digital ethnography, participant observation, and interviews, to explore informal analytics communities in India.

Result: Informal networks help mitigate data scarcity and support skill development, proposing HCI interventions for decentralized knowledge sharing and low-cost solutions.

Conclusion: The paper advocates for community-driven technological adaptations to enhance analytics-driven decision-making in resource-constrained environments, promoting sustainability and skill-building.

Abstract: The global rise of football analytics has rapidly transformed how clubs make
strategic decisions. However, in India, the adoption of analytics remains
constrained by institutional resistance, infrastructural limitations, and
cultural barriers -- challenges that grassroots innovation and low-cost data
solutions have the potential to overcome. Despite the growing popularity of the
Indian Super League, resource scarcity and fragmented governance continue to
hinder the widespread adoption and impact of analytics. This mixed-methods
study explores how informal, decentralised analytics communities -- comprising
amateur analysts and Twitter-based "data sleuths" -- navigate these constraints
through peer mentorship and grassroots innovation. Drawing on extensive digital
ethnography, participant observation, and interviews, the study illustrates how
these informal networks mitigate data scarcity, limited digital infrastructure,
and institutional indifference while fostering skill development and
professional growth. Building on these insights, the paper proposes HCI
interventions such as decentralised knowledge platforms to facilitate
structured, cross-border peer mentorship and low-cost data solutions --
including AI-assisted player tracking and mobile analytics dashboards -- rooted
in principles of frugal innovation. These interventions aim to bridge the data
divide, support inclusive technical engagement in sport, and enhance
analytics-driven decision-making in resource-constrained environments. This
paper contributes to HCIxB's focus on cross-border collaboration by
highlighting how community-driven technological adaptation in the Global South
can foster meaningful participation, skill-building, and long-term
sustainability through informal learning networks and scalable,
context-sensitive tools.

</details>


### [14] [PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling Assistant System](https://arxiv.org/abs/2504.16573)
*Xianghe Liu,Jiaqi Xu,Tao Sun*

Main category: cs.HC

TL;DR: PsyCounAssist is an AI-powered assistant designed to enhance psychological counseling through emotion recognition and automated reporting.


<details>
  <summary>Details</summary>
Motivation: The need for personalized and dynamic monitoring in psychological counseling practices.

Method: The system integrates multimodal emotion recognition via speech and PPG signals, and uses LLMs for automated session reporting and AI-generated follow-up support, deployed on Android tablets.

Result: Experimental evaluation shows reliable emotional classification based on PPG and demonstrates the system's potential for providing non-intrusive emotional support in real-world counseling.

Conclusion: PsyCounAssist offers a novel, ethical integration of AI in psychological counseling, enhancing the therapeutic process.

Abstract: Psychological counseling is a highly personalized and dynamic process that
requires therapists to continuously monitor emotional changes, document session
insights, and maintain therapeutic continuity. In this paper, we introduce
PsyCounAssist, a comprehensive AI-powered counseling assistant system
specifically designed to augment psychological counseling practices.
PsyCounAssist integrates multimodal emotion recognition combining speech and
photoplethysmography (PPG) signals for accurate real-time affective analysis,
automated structured session reporting using large language models (LLMs), and
personalized AI-generated follow-up support. Deployed on Android-based tablet
devices, the system demonstrates practical applicability and flexibility in
real-world counseling scenarios. Experimental evaluation confirms the
reliability of PPG-based emotional classification and highlights the system's
potential for non-intrusive, privacy-aware emotional support. PsyCounAssist
represents a novel approach to ethically and effectively integrating AI into
psychological counseling workflows.

</details>


### [15] [Algorithmic Mirror: Designing an Interactive Tool to Promote Self-Reflection for YouTube Recommendations](https://arxiv.org/abs/2504.16615)
*Yui Kondo,Kevin Dunnell,Qing Xiao,Jun Zhao,Luc Rocher*

Main category: cs.HC

TL;DR: The paper presents a novel approach called 'hypothetical inference' to enhance user understanding of AI algorithms and their impact on digital identities.


<details>
  <summary>Details</summary>
Motivation: To address the loss of control individuals have over their digital identities due to non-intuitive inferences made by big data analytics and AI systems.

Method: The research introduces 'hypothetical inference' using language models to simulate algorithmic interpretations of users' digital footprints, based on empirical studies with fourteen participants.

Result: Three key design opportunities for enhancing algorithmic literacy were identified: creating unified maps of digital footprints, simulating algorithmic inference, and incorporating temporal dimensions in visualizations.

Conclusion: The research provides a foundation for developing tools that enhance user awareness of data influence and promote autonomy in algorithm-mediated environments.

Abstract: Big Data analytics and Artificial Intelligence systems derive non-intuitive
and often unverifiable inferences about individuals' behaviors, preferences,
and private lives. Drawing on diverse, feature-rich datasets of unpredictable
value, these systems erode the intuitive connection between our actions and how
we are perceived, diminishing control over our digital identities. While
Explainable Artificial Intelligence scholars have attempted to explain the
inner workings of algorithms, their visualizations frequently overwhelm
end-users with complexity. This research introduces 'hypothetical inference', a
novel approach that uses language models to simulate how algorithms might
interpret users' digital footprints and infer personal characteristics without
requiring access to proprietary platform algorithms. Through empirical studies
with fourteen adult participants, we identified three key design opportunities
to foster critical algorithmic literacy: (1) reassembling scattered digital
footprints into a unified map, (2) simulating algorithmic inference through
LLM-generated interpretations, and (3) incorporating temporal dimensions to
visualize evolving patterns. This research lays the groundwork for tools that
can help users recognize the influence of data on platforms and develop greater
autonomy in increasingly algorithm-mediated digital environments.

</details>


### [16] [LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative Analysis](https://arxiv.org/abs/2504.16671)
*Joel Oksanen,Andrés Lucero,Perttu Hämäläinen*

Main category: cs.HC

TL;DR: The paper explores the trustworthiness of insights generated by large language models (LLMs) in qualitative design analysis, introducing LLMCode to assess their alignment with human insights.


<details>
  <summary>Details</summary>
Motivation: To address the efficiency of LLMs in qualitative analysis while considering their alignment with the context of research for design (RfD) and the interpretive processes involved.

Method: The study involved the use of LLMCode, an open-source tool that integrates Intersection over Union (IoU) and Modified Hausdorff Distance metrics to evaluate the correspondence between human and LLM-generated qualitative coding insights.

Result: Findings from two studies with 26 designers showed that while LLMs performed well in deductive coding, they struggled with the deeper interpretive aspects that human designers possess, highlighting a collaborative dynamic in refining insights.

Conclusion: The research emphasizes the need to foster appropriate reliance on LLMs by creating tools that maintain interpretive depth and encourage intuitive collaboration between designers and AI.

Abstract: The use of large language models (LLMs) in qualitative analysis offers
enhanced efficiency but raises questions about their alignment with the
contextual nature of research for design (RfD). This research examines the
trustworthiness of LLM-driven design insights, using qualitative coding as a
case study to explore the interpretive processes central to RfD. We introduce
LLMCode, an open-source tool integrating two metrics, namely Intersection over
Union (IoU) and Modified Hausdorff Distance, to assess the alignment between
human and LLM-generated insights. Across two studies involving 26 designers, we
find that while the model performs well with deductive coding, its ability to
emulate a designer's deeper interpretive lens over the data is limited,
emphasising the importance of human-AI collaboration. Our results highlight a
reciprocal dynamic where users refine LLM outputs and adapt their own
perspectives based on the model's suggestions. These findings underscore the
importance of fostering appropriate reliance on LLMs by designing tools that
preserve interpretive depth while facilitating intuitive collaboration between
designers and AI.

</details>


### [17] [Search Timelines: Visualizing Search History to Enable Cross-Session Exploratory Search](https://arxiv.org/abs/2504.16741)
*Orland Hoeber,Md Nazmul Islam,Miriam Boon,Dale Storie,Veronica Ramshaw*

Main category: cs.HC

TL;DR: The paper presents 'Search Timelines', a search interface that enhances cross-session exploratory searching in digital libraries through dynamic visualizations of search activities.


<details>
  <summary>Details</summary>
Motivation: Searchers often struggle to remember key details from their exploratory searches due to long timespans and high activity volumes, complicating both ongoing and resumed search efforts.

Method: The Search Timelines interface visualizes search activities via a dynamic timeline with two detail levels—an overview alongside search results and a detailed view in the workspace. A controlled study compared this interface with a standard digital library model.

Result: Participants using Search Timelines experienced higher engagement, usability, and perceived knowledge gain during searches and when resuming after a week, though they took longer to complete tasks.

Conclusion: Search Timelines effectively demonstrates how lightweight visualizations can improve search interfaces for exploratory search by providing a persistent view of past activities.

Abstract: Purpose: The timespan over which exploratory searching can occur, as well as
the scope and volume of the search activities undertaken, can make it difficult
for searchers to remember key details about their search activities. These
difficulties are present both in the midst of searching as well as when
resuming a search that spans multiple sessions. In this paper, we present a
search interface designed to support cross-session exploratory search in a
public digital library context. Methods: Search Timelines provides a
visualization of current and past search activities via a dynamic timeline of
the search activity (queries and saved resources). This timeline is presented
at two levels of detail. An overview timeline is provided alongside the search
results in a typical search engine results page design. A detailed timeline is
provided in the workspace, where searchers can review the history of their
search activities and their saved resources. A controlled laboratory study was
conducted to compare this approach to a baseline interface modelled after a
typical public digital library search/workspace interface. Results:
Participants who used Search Timelines reported higher levels of user
engagement, usability, and perceived knowledge gain, during an initial search
session and when resuming the search after a 7-8 day interval. This came at the
expense of the searchers taking more time to complete the search task, which we
view as positive evidence of engagement in cross-session exploratory search
processes. Conclusion: Search Timelines serves as an example of how lightweight
visualization approaches can be used to enhance typical search interface
designs to support exploratory search. The results highlight the value of
providing persistent representations of past search activities within the
search interface.

</details>


### [18] [DeBiasMe: De-biasing Human-AI Interactions with Metacognitive AIED (AI in Education) Interventions](https://arxiv.org/abs/2504.16770)
*Chaeyeon Lim*

Main category: cs.HC

TL;DR: The paper advocates for metacognitive AI literacy interventions to address human biases in AI interactions, aiming to enhance critical engagement among university students.


<details>
  <summary>Details</summary>
Motivation: There is a critical gap in understanding and mitigating human biases in interactions with generative AI, which necessitates informed engagement from students.

Method: The paper proposes three frameworks: metacognitive support with deliberate friction, bi-directional Human-AI interaction intervention, and adaptive scaffolding, exemplified through the 'DeBiasMe' project.

Result: The proposed interventions aim to improve awareness of cognitive biases and enhance user agency in AI interactions, encouraging better navigation of biases in AI use.

Conclusion: The position paper highlights the necessity of integrating cognitive adaptation to AI systems into comprehensive AI literacy frameworks, involving multiple stakeholders in the design and evaluation process.

Abstract: While generative artificial intelligence (Gen AI) increasingly transforms
academic environments, a critical gap exists in understanding and mitigating
human biases in AI interactions, such as anchoring and confirmation bias. This
position paper advocates for metacognitive AI literacy interventions to help
university students critically engage with AI and address biases across the
Human-AI interaction workflows. The paper presents the importance of
considering (1) metacognitive support with deliberate friction focusing on
human bias; (2) bi-directional Human-AI interaction intervention addressing
both input formulation and output interpretation; and (3) adaptive scaffolding
that responds to diverse user engagement patterns. These frameworks are
illustrated through ongoing work on "DeBiasMe," AIED (AI in Education)
interventions designed to enhance awareness of cognitive biases while
empowering user agency in AI interactions. The paper invites multiple
stakeholders to engage in discussions on design and evaluation methods for
scaffolding mechanisms, bias visualization, and analysis frameworks. This
position contributes to the emerging field of AI-augmented learning by
emphasizing the critical role of metacognition in helping students navigate the
complex interaction between human, statistical, and systemic biases in AI use
while highlighting how cognitive adaptation to AI systems must be explicitly
integrated into comprehensive AI literacy frameworks.

</details>


### [19] [Nurturing Language Proficiency in Spanish.speaking children Through digital competence](https://arxiv.org/abs/2504.16824)
*Rhayza Jolley Rangel*

Main category: cs.HC

TL;DR: The paper presents a digital platform designed to enhance early-age English education for Spanish-speaking children through innovative methodologies and user-centered design.


<details>
  <summary>Details</summary>
Motivation: To revolutionize early-age English education specifically tailored for Spanish-speaking children.

Method: Utilized innovative methodologies, engaging visuals, and a phonics-based approach incorporating principles of usability, accessibility, and user-centered design.

Result: A comprehensive digital platform architecture aimed at improving English education for the target demographic.

Conclusion: The design principles applied in the platform are essential for creating effective educational tools for early-age learners.

Abstract: This article explores into the intricate design and meticulous construction
of a digital platform aimed at revolutionizing early-age English education,
particularly for Spanish-speaking children. The focus of this work used an
innovative methodologies, vibrant and engaging visuals, and a comprehensive
approach to phonics. The principles of usability, accessibility, and
user-centered design are intricately woven into every facet of the platform's
architecture.

</details>


### [20] [Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models](https://arxiv.org/abs/2504.16883)
*Xuyang Zhu,Sejoon Chang,Andrew Kuik*

Main category: cs.HC

TL;DR: This study investigates how tailored warning messages about hallucinations in RAG systems impact user reasoning and actions in educational settings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address fairness and reliability concerns in Retrieval-Augmented Generation systems, particularly focusing on the emergence of hallucinations that can negatively influence user decision-making.

Method: The research involved an educational quiz setting where the effects of context-dependent warning messages on user reasoning and trust were analyzed.

Result: Preliminary findings indicate that while warnings enhance accuracy and awareness regarding hallucinations, they can also cause cognitive friction and reduce trust in the system.

Conclusion: The study contributes to the goal of developing AI systems that bolster human critical thinking and informed decision-making, rather than simply providing information.

Abstract: Retrieval-Augmented Generation (RAG) systems offer a powerful approach to
enhancing large language model (LLM) outputs by incorporating fact-checked,
contextually relevant information. However, fairness and reliability concerns
persist, as hallucinations can emerge at both the retrieval and generation
stages, affecting users' reasoning and decision-making. Our research explores
how tailored warning messages -- whose content depends on the specific context
of hallucination -- shape user reasoning and actions in an educational quiz
setting. Preliminary findings suggest that while warnings improve accuracy and
awareness of high-level hallucinations, they may also introduce cognitive
friction, leading to confusion and diminished trust in the system. By examining
these interactions, this work contributes to the broader goal of AI-augmented
reasoning: developing systems that actively support human reflection, critical
thinking, and informed decision-making rather than passive information
consumption.

</details>


### [21] [Texture: Structured Exploration of Text Datasets](https://arxiv.org/abs/2504.16898)
*Will Epperson,Arpit Mathur,Adam Perer,Dominik Moritz*

Main category: cs.HC

TL;DR: Texture is a general-purpose interactive tool for exploring text corpora that enhances text analysis through a configurable data schema and integrated visualization methods.


<details>
  <summary>Details</summary>
Motivation: There is a need for better text visualization tools that adapt to users' changing analytical goals without requiring them to switch between different tools.

Method: Texture presents a configurable data schema for text documents, allowing for various levels of attributes and integrating multiple methods for interaction and exploration into a single interface.

Result: In user studies, Texture allowed participants to represent dataset attributes effectively, iterate more quickly in their analyses, and uncover new insights compared to baseline sessions.

Conclusion: The findings support the development of scalable and interactive text exploration systems that enhance users' understanding of text data.

Abstract: Exploratory analysis of a text corpus is essential for assessing data quality
and developing meaningful hypotheses. Text analysis relies on understanding
documents through structured attributes spanning various granularities of the
documents such as words, phrases, sentences, topics, or clusters. However,
current text visualization tools typically adopt a fixed representation
tailored to specific tasks or domains, requiring users to switch tools as their
analytical goals change. To address this limitation, we present Texture, a
general-purpose interactive text exploration tool. Texture introduces a
configurable data schema for representing text documents enriched with
descriptive attributes. These attributes can appear at arbitrary levels of
granularity in the text and possibly have multiple values, including
document-level attributes, multi-valued attributes (e.g., topics), fine-grained
span-level attributes (e.g., words), and vector embeddings. The system then
combines existing interactive methods for text exploration into a single
interface that provides attribute overview visualizations, supports
cross-filtering attribute charts to explore subsets, uses embeddings for a
dataset overview and similar instance search, and contextualizes filters in the
actual documents. We evaluated Texture through a two-part user study with 10
participants from varied domains who each analyzed their own dataset in a
baseline session and then with Texture. Texture was able to represent all of
the previously derived dataset attributes, enabled participants to more quickly
iterate during their exploratory analysis, and discover new insights about
their data. Our findings contribute to the design of scalable, interactive, and
flexible exploration systems that improve users' ability to make sense of text
data.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [Representation Learning for Tabular Data: A Comprehensive Survey](https://arxiv.org/abs/2504.16109)
*Jun-Peng Jiang,Si-Yang Liu,Hao-Run Cai,Qile Zhou,Han-Jia Ye*

Main category: cs.LG

TL;DR: This survey covers the evolution and methodologies of tabular representation learning, categorizing models and discussing their strengths and nuances.


<details>
  <summary>Details</summary>
Motivation: To systematically introduce the field of tabular representation learning in machine learning, highlighting current challenges and benchmarks.

Method: The paper categorizes models into specialized, transferable, and general types, introducing a hierarchical taxonomy for specialized models and discussing ensemble methods and various extensions of tabular learning.

Result: It organizes existing methods based on their generalization capabilities and provides detailed strategies for obtaining quality representations, along with insights into new areas like multimodal learning and open-environment tabular ML.

Conclusion: The survey emphasizes the importance of adapting models to tabular data and exploring diverse approaches to improve representation learning.

Abstract: Tabular data, structured as rows and columns, is among the most prevalent
data types in machine learning classification and regression applications.
Models for learning from tabular data have continuously evolved, with Deep
Neural Networks (DNNs) recently demonstrating promising results through their
capability of representation learning. In this survey, we systematically
introduce the field of tabular representation learning, covering the
background, challenges, and benchmarks, along with the pros and cons of using
DNNs. We organize existing methods into three main categories according to
their generalization capabilities: specialized, transferable, and general
models. Specialized models focus on tasks where training and evaluation occur
within the same data distribution. We introduce a hierarchical taxonomy for
specialized models based on the key aspects of tabular data -- features,
samples, and objectives -- and delve into detailed strategies for obtaining
high-quality feature- and sample-level representations. Transferable models are
pre-trained on one or more datasets and subsequently fine-tuned on downstream
tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources,
or even cross-modalities such as vision and language. General models, also
known as tabular foundation models, extend this concept further, allowing
direct application to downstream tasks without fine-tuning. We group these
general models based on the strategies used to adapt across heterogeneous
datasets. Additionally, we explore ensemble methods, which integrate the
strengths of multiple tabular models. Finally, we discuss representative
extensions of tabular learning, including open-environment tabular machine
learning, multimodal learning with tabular data, and tabular understanding.
More information can be found in the following repository:
https://github.com/LAMDA-Tabular/Tabular-Survey.

</details>


### [23] [Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement](https://arxiv.org/abs/2504.16136)
*Chiung-Yi Tseng,Junhao Song,Ziqian Bi,Tianyang Wang,Chia Xin Liang,Ming Liu*

Main category: cs.LG

TL;DR: This paper presents an overview of Active Learning (AL) as a solution for improving machine learning performance with fewer labeled examples amidst data abundance and annotation scarcity.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical bottleneck in machine learning posed by the abundance of data and the scarcity of annotated examples, highlighting the need for effective strategies to enhance model performance.

Method: The paper discusses the fundamentals of Active Learning and its applications in various fields, focusing on research topics like uncertainty estimation, class imbalance, domain adaptation, fairness, and evaluation metrics, while also suggesting human-inspired learning methods.

Result: Active Learning techniques often outperform passive learning methods, particularly when robust evaluation measures are employed.

Conclusion: The paper serves as a resource for both researchers and practitioners, offering insights into Active Learning and identifying key challenges and future directions for advancing the field.

Abstract: In the era of data-driven intelligence, the paradox of data abundance and
annotation scarcity has emerged as a critical bottleneck in the advancement of
machine learning. This paper gives a detailed overview of Active Learning (AL),
which is a strategy in machine learning that helps models achieve better
performance using fewer labeled examples. It introduces the basic concepts of
AL and discusses how it is used in various fields such as computer vision,
natural language processing, transfer learning, and real-world applications.
The paper focuses on important research topics such as uncertainty estimation,
handling of class imbalance, domain adaptation, fairness, and the creation of
strong evaluation metrics and benchmarks. It also shows that learning methods
inspired by humans and guided by questions can improve data efficiency and help
models learn more effectively. In addition, this paper talks about current
challenges in the field, including the need to rebuild trust, ensure
reproducibility, and deal with inconsistent methodologies. It points out that
AL often gives better results than passive learning, especially when good
evaluation measures are used. This work aims to be useful for both researchers
and practitioners by providing key insights and proposing directions for future
progress in active learning.

</details>


### [24] [SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures](https://arxiv.org/abs/2504.16140)
*Max Hartman,Lav Varshney*

Main category: cs.LG

TL;DR: SparseJEPA enhances the Joint Embedding Predictive Architectures (JEPA) by integrating sparse representation learning, resulting in improved interpretability and efficiency of learned representations.


<details>
  <summary>Details</summary>
Motivation: JEPA models lack interpretability and suffer from inefficiencies due to dense embedding representations, prompting the need for an improved framework.

Method: SparseJEPA incorporates a penalty method that encourages sharing of latent space variables among features with strong semantic relationships, while maintaining predictive performance.

Result: When trained on the CIFAR-100 dataset and applied to a lightweight Vision Transformer, SparseJEPA showed improved embeddings that excel in linear-probe transfer learning for image classification and low-level tasks.

Conclusion: The introduction of sparsity improves both the quality and interpretability of learned representations, with theoretical proofs supporting the effectiveness of the grouping mechanism in reducing Multiinformation among latent variables.

Abstract: Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful
framework for learning general-purpose representations. However, these models
often lack interpretability and suffer from inefficiencies due to dense
embedding representations. We propose SparseJEPA, an extension that integrates
sparse representation learning into the JEPA framework to enhance the quality
of learned representations. SparseJEPA employs a penalty method that encourages
latent space variables to be shared among data features with strong semantic
relationships, while maintaining predictive performance. We demonstrate the
effectiveness of SparseJEPA by training on the CIFAR-100 dataset and
pre-training a lightweight Vision Transformer. The improved embeddings are
utilized in linear-probe transfer learning for both image classification and
low-level tasks, showcasing the architecture's versatility across different
transfer tasks. Furthermore, we provide a theoretical proof that demonstrates
that the grouping mechanism enhances representation quality. This was done by
displaying that grouping reduces Multiinformation among latent-variables,
including proofing the Data Processing Inequality for Multiinformation. Our
results indicate that incorporating sparsity not only refines the latent space
but also facilitates the learning of more meaningful and interpretable
representations. In further work, hope to further extend this method by finding
new ways to leverage the grouping mechanism through object-centric
representation learning.

</details>


### [25] [Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges](https://arxiv.org/abs/2504.16141)
*Yue Shi,Liangxiu Han,Xin Zhang,Tam Sobeih,Thomas Gaiser,Nguyen Huu Thuy,Dominik Behrend,Amit Kumar Srivastava,Krishnagopal Halder,Frank Ewert*

Main category: cs.LG

TL;DR: Hybrid models combining process-based models (PBMs) and deep learning (DL) outperform traditional standalone models in agricultural applications.


<details>
  <summary>Details</summary>
Motivation: To explore how combining PBMs and DL can enhance agricultural modelling by leveraging their respective strengths and addressing their limitations.

Method: A systematic review of PBMs, DL models, and hybrid PBM-DL frameworks, followed by a case study on crop dry biomass prediction comparing hybrid models against standalone PBMs and DL models.

Result: Hybrid models demonstrated superior performance compared to traditional PBMs and DL models, showing greater robustness to noisy data and better generalization in various conditions.

Conclusion: The study emphasizes the potential of hybrid modelling in agriculture by integrating domain knowledge with AI, leading to improved scalability, interpretability, and decision-making for sustainable practices.

Abstract: Process-based models (PBMs) and deep learning (DL) are two key approaches in
agricultural modelling, each offering distinct advantages and limitations. PBMs
provide mechanistic insights based on physical and biological principles,
ensuring interpretability and scientific rigour. However, they often struggle
with scalability, parameterisation, and adaptation to heterogeneous
environments. In contrast, DL models excel at capturing complex, nonlinear
patterns from large datasets but may suffer from limited interpretability, high
computational demands, and overfitting in data-scarce scenarios.
  This study presents a systematic review of PBMs, DL models, and hybrid PBM-DL
frameworks, highlighting their applications in agricultural and environmental
modelling. We classify hybrid PBM-DL approaches into DL-informed PBMs, where
neural networks refine process-based models, and PBM-informed DL, where
physical constraints guide deep learning predictions. Additionally, we conduct
a case study on crop dry biomass prediction, comparing hybrid models against
standalone PBMs and DL models under varying data quality, sample sizes, and
spatial conditions. The results demonstrate that hybrid models consistently
outperform traditional PBMs and DL models, offering greater robustness to noisy
data and improved generalisation across unseen locations.
  Finally, we discuss key challenges, including model interpretability,
scalability, and data requirements, alongside actionable recommendations for
advancing hybrid modelling in agriculture. By integrating domain knowledge with
AI-driven approaches, this study contributes to the development of scalable,
interpretable, and reproducible agricultural models that support data-driven
decision-making for sustainable agriculture.

</details>


### [26] [Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis](https://arxiv.org/abs/2504.16214)
*Xiao Zhang,Yaoyao Ding,Yang Hu,Gennady Pekhimenko*

Main category: cs.LG

TL;DR: Hexcute is a tile-based programming language designed to optimize deep learning matrix multiplication for mixed data types, achieving significant speedups over existing compilers.


<details>
  <summary>Details</summary>
Motivation: Current deep learning quantization techniques require optimizations that are not efficiently supported by existing high-level compilers, leading to the need for a new approach to matrix multiplication on GPUs.

Method: Hexcute provides a tile-based programming language that allows for fine-grained optimizations, exposing shared memory and register abstractions, and automates layout and task mapping synthesis through a type-inference-based algorithm.

Result: Hexcute shows a generalization to a wide range of deep learning operators, achieving speedups of 1.7-11.28 times compared to existing compilers for mixed-type operators, with an end-to-end speedup of up to 2.91 times.

Conclusion: Hexcute effectively balances expressiveness and programming effort, making it a valuable tool for optimizing deep learning workloads on GPUs.

Abstract: Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL
quantization techniques demand a new matrix multiplication operator with mixed
input data types, further complicating GPU optimization. Prior high-level
compilers like Triton lack the expressiveness to implement key optimizations
like fine-grained data pipelines and hardware-friendly memory layouts for these
operators, while low-level programming models, such as Hidet, Graphene, and
CUTLASS, require significant programming efforts. To balance expressiveness
with engineering effort, we propose Hexcute, a tile-based programming language
that exposes shared memory and register abstractions to enable fine-grained
optimization for these operators. Additionally, Hexcute leverages task mapping
to schedule the GPU program, and to reduce programming efforts, it automates
layout and task mapping synthesis with a novel type-inference-based algorithm.
Our evaluation shows that Hexcute generalizes to a wide range of DL operators,
achieves 1.7-11.28$\times$ speedup over existing DL compilers for mixed-type
operators, and brings up to 2.91$\times$ speedup in the end-to-end evaluation.

</details>


### [27] [Using Phonemes in cascaded S2S translation pipeline](https://arxiv.org/abs/2504.16234)
*Rene Pilz,Johannes Schneider*

Main category: cs.LG

TL;DR: The paper investigates phonemes as an alternative representation in multilingual speech-to-speech translation, finding it offers comparable quality to traditional text methods while being more resource-efficient, especially for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To explore alternatives to traditional text-based representations in multilingual simultaneous speech-to-speech translation.

Method: Trained an open-source sequence-to-sequence model on the WMT17 dataset using both standard textual and phonemic representations, evaluating performance with the BLEU metric.

Result: The phonemic approach provides comparable translation quality to the text-based method while requiring fewer resources and being more suitable for low-resource languages.

Conclusion: Phonemic representation in speech translation can be advantageous, especially for languages with limited resources.

Abstract: This paper explores the idea of using phonemes as a textual representation
within a conventional multilingual simultaneous speech-to-speech translation
pipeline, as opposed to the traditional reliance on text-based language
representations. To investigate this, we trained an open-source
sequence-to-sequence model on the WMT17 dataset in two formats: one using
standard textual representation and the other employing phonemic
representation. The performance of both approaches was assessed using the BLEU
metric. Our findings shows that the phonemic approach provides comparable
quality but offers several advantages, including lower resource requirements or
better suitability for low-resource languages.

</details>


### [28] [General Post-Processing Framework for Fairness Adjustment of Machine Learning Models](https://arxiv.org/abs/2504.16238)
*Léandre Eberhard,Nirek Sharma,Filipp Shelobolin,Aalok Ganesh Shanbhag*

Main category: cs.LG

TL;DR: This paper presents a new framework for fairness adjustments in machine learning, allowing flexible compliance with fairness metrics while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: The need for compliance with fairness constraints in machine learning across critical areas like credit underwriting and public policy is both legal and ethical, necessitating effective methods for ensuring fairness.

Method: The framework adapts in-processing techniques for post-processing fairness adjustments, allowing decoupling from model training and thus preserving average model performance while offering flexibility.

Result: The approach is compared to Adversarial Debiasing and demonstrates a comparable tradeoff between fairness and accuracy using real-world datasets.

Conclusion: The proposed framework effectively meets fairness requirements without compromising model performance, allowing for greater adaptability in various machine learning scenarios.

Abstract: As machine learning increasingly influences critical domains such as credit
underwriting, public policy, and talent acquisition, ensuring compliance with
fairness constraints is both a legal and ethical imperative. This paper
introduces a novel framework for fairness adjustments that applies to diverse
machine learning tasks, including regression and classification, and
accommodates a wide range of fairness metrics. Unlike traditional approaches
categorized as pre-processing, in-processing, or post-processing, our method
adapts in-processing techniques for use as a post-processing step. By
decoupling fairness adjustments from the model training process, our framework
preserves model performance on average while enabling greater flexibility in
model development. Key advantages include eliminating the need for custom loss
functions, enabling fairness tuning using different datasets, accommodating
proprietary models as black-box systems, and providing interpretable insights
into the fairness adjustments. We demonstrate the effectiveness of this
approach by comparing it to Adversarial Debiasing, showing that our framework
achieves a comparable fairness/accuracy tradeoff on real-world datasets.

</details>


### [29] [FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness](https://arxiv.org/abs/2504.16255)
*Tina Behzad,Mithilesh Kumar Singh,Anthony J. Ripa,Klaus Mueller*

Main category: cs.LG

TL;DR: FairPlay is a web-based application that facilitates collaborative debiasing of datasets among multiple stakeholders, allowing them to negotiate fairness without a single standard.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of differing and mutually incompatible fairness demands from various stakeholders in decision-making processes.

Method: A web-based software application named FairPlay was developed to enable users to collaboratively debias datasets through a systematic negotiation process.

Result: User studies showed that FairPlay allowed users to reach a consensus in approximately five rounds of gameplay, demonstrating its effectiveness in achieving fair outcomes.

Conclusion: FairPlay has the potential to enhance fairness in AI systems by providing a structured platform for stakeholders to negotiate and agree on fairness without a universally accepted definition.

Abstract: The issue of fairness in decision-making is a critical one, especially given
the variety of stakeholder demands for differing and mutually incompatible
versions of fairness. Adopting a strategic interaction of perspectives provides
an alternative to enforcing a singular standard of fairness. We present a
web-based software application, FairPlay, that enables multiple stakeholders to
debias datasets collaboratively. With FairPlay, users can negotiate and arrive
at a mutually acceptable outcome without a universally agreed-upon theory of
fairness. In the absence of such a tool, reaching a consensus would be highly
challenging due to the lack of a systematic negotiation process and the
inability to modify and observe changes. We have conducted user studies that
demonstrate the success of FairPlay, as users could reach a consensus within
about five rounds of gameplay, illustrating the application's potential for
enhancing fairness in AI systems.

</details>


### [30] [Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching](https://arxiv.org/abs/2504.16262)
*Junn Yong Loo,Michelle Adeline,Julia Kaiwen Lau,Fang Yu Leong,Hwa Hui Tew,Arghya Pal,Vishnu Monn Baskaran,Chee-Ming Ting,Raphaël C. -W. Phan*

Main category: cs.LG

TL;DR: VPFB is a new energy-based generative framework that bypasses implicit MCMC sampling for stable and efficient model training.


<details>
  <summary>Details</summary>
Motivation: Despite the potential of energy-based models (EBMs), the connection between potential flows and explicit EBMs remains insufficiently explored, and existing training methods can be unstable in high dimensions.

Method: The paper introduces Variational Potential Flow Bayes (VPFB), which learns an energy-parameterized potential flow through a flow-driven density homotopy that minimizes the Kullback-Leibler divergence with the data distribution, eliminating the need for implicit MCMC sampling or auxiliary networks.

Result: Experiments demonstrate that VPFB effectively addresses challenges in image generation, interpolation, out-of-distribution detection, and compositional generation, performing competitively with existing methods in sample quality and versatility.

Conclusion: VPFB enables robust and efficient generative modeling while maintaining the desirable interpretability characteristic of energy-based models.

Abstract: Energy-based models (EBMs) are a powerful class of probabilistic generative
models due to their flexibility and interpretability. However, relationships
between potential flows and explicit EBMs remain underexplored, while
contrastive divergence training via implicit Markov chain Monte Carlo (MCMC)
sampling is often unstable and expensive in high-dimensional settings. In this
paper, we propose Variational Potential Flow Bayes (VPFB), a new energy-based
generative framework that eliminates the need for implicit MCMC sampling and
does not rely on auxiliary networks or cooperative training. VPFB learns an
energy-parameterized potential flow by constructing a flow-driven density
homotopy that is matched to the data distribution through a variational loss
minimizing the Kullback-Leibler divergence between the flow-driven and marginal
homotopies. This principled formulation enables robust and efficient generative
modeling while preserving the interpretability of EBMs. Experimental results on
image generation, interpolation, out-of-distribution detection, and
compositional generation confirm the effectiveness of VPFB, showing that our
method performs competitively with existing approaches in terms of sample
quality and versatility across diverse generative modeling tasks.

</details>


### [31] [Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models](https://arxiv.org/abs/2504.16263)
*Magnus Sieverding,Nathan Steffen,Kelly Cohen*

Main category: cs.LG

TL;DR: The paper benchmarks a Gradient-Optimized Fuzzy Inference System (GF) against various machine learning models, demonstrating its superior performance in classification tasks.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of a Gradient-Optimized Fuzzy Inference System as an alternative to traditional machine learning approaches that often use derivative-free optimization methods.

Method: The study compares the GF classifier with Random Forest, XGBoost, Logistic Regression, Support Vector Machines, and Neural Networks across five diverse datasets from the UCI Machine Learning Repository.

Result: The GF model achieved competitive and often superior classification accuracy, with high precision and low training times, demonstrating consistent performance across different datasets and folds.

Conclusion: The findings suggest that gradient optimized fuzzy systems can serve as interpretable, efficient, and adaptable alternatives to complex deep learning models in supervised learning.

Abstract: This paper presents a performance benchmarking study of a Gradient-Optimized
Fuzzy Inference System (GF) classifier against several state-of-the-art machine
learning models, including Random Forest, XGBoost, Logistic Regression, Support
Vector Machines, and Neural Networks. The evaluation was conducted across five
datasets from the UCI Machine Learning Repository, each chosen for their
diversity in input types, class distributions, and classification complexity.
Unlike traditional Fuzzy Inference Systems that rely on derivative-free
optimization methods, the GF leverages gradient descent to significantly
improving training efficiency and predictive performance. Results demonstrate
that the GF model achieved competitive, and in several cases superior,
classification accuracy while maintaining high precision and exceptionally low
training times. In particular, the GF exhibited strong consistency across folds
and datasets, underscoring its robustness in handling noisy data and variable
feature sets. These findings support the potential of gradient optimized fuzzy
systems as interpretable, efficient, and adaptable alternatives to more complex
deep learning models in supervised learning tasks.

</details>


### [32] [Boosting Classifier Performance with Opposition-Based Data Transformation](https://arxiv.org/abs/2504.16268)
*Abdesslem Layeb*

Main category: cs.LG

TL;DR: This paper presents a data transformation framework utilizing Opposition-Based Learning (OBL) to enhance the performance of traditional classification algorithms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the performance of traditional classifiers by using OBL to generate synthetic opposite samples that enhance decision boundary formation.

Method: The study explores three variants of OBL—Global OBL, Class-Wise OBL, and Localized Class-Wise OBL—and integrates them with classifiers like KNN, SVM, LR, and DT across 26 diverse datasets.

Result: The results show that OBL-enhanced classifiers consistently outperform standard classifiers in accuracy and F1-score, often achieving near-perfect classification, while also improving computational efficiency, especially for SVM and LR.

Conclusion: The findings indicate that OBL is a promising lightweight data transformation strategy for enhancing classification performance in complex or sparse learning scenarios.

Abstract: In this paper, we introduce a novel data transformation framework based on
Opposition-Based Learning (OBL) to boost the performance of traditional
classification algorithms. Originally developed to accelerate convergence in
optimization tasks, OBL is leveraged here to generate synthetic opposite
samples that replace the acutely training data and improve decision boundary
formation. We explore three OBL variants; Global OBL, Class-Wise OBL, and
Localized Class-Wise OBL; and integrate them with several widely used
classifiers, including K-Nearest Neighbors (KNN), Support Vector Machines
(SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments
conducted on 26 heterogeneous and high-dimensional datasets demonstrate that
OBL-enhanced classifiers consistently outperform their standard counterparts in
terms of accuracy and F1-score, frequently achieving near-perfect or perfect
classification. Furthermore, OBL contributes to improved computational
efficiency, particularly in SVM and LR. These findings underscore the potential
of OBL as a lightweight yet powerful data transformation strategy for enhancing
classification performance, especially in complex or sparse learning
environments.

</details>


### [33] [Learning Explainable Dense Reward Shapes via Bayesian Optimization](https://arxiv.org/abs/2504.16272)
*Ryan Koo,Ian Yang,Vipul Raheja,Mingyi Hong,Kwang-Sung Jun,Dongyeop Kang*

Main category: cs.LG

TL;DR: This paper proposes a novel reward shaping function for reinforcement learning from human feedback to improve token-level credit assignment in large language models.


<details>
  <summary>Details</summary>
Motivation: Current RLHF methods rely on sparse scalar rewards that lead to suboptimal learning due to insufficient feedback for individual tokens in sequences.

Method: The authors frame reward shaping as an optimization problem and propose a reward-shaping function using explainability methods (e.g., SHAP and LIME) to derive per-token rewards, incorporating a bilevel optimization framework that combines Bayesian Optimization and policy training.

Result: The proposed method yields performance improvements on downstream tasks compared to existing baselines and accelerates the discovery of optimal policies during training.

Conclusion: Explainability methods that are feature additive can maintain the optimal policy while improving token-level reward attribution.

Abstract: Current reinforcement learning from human feedback (RLHF) pipelines for large
language model (LLM) alignment typically assign scalar rewards to sequences,
using the final token as a surrogate indicator for the quality of the entire
sequence. However, this leads to sparse feedback and suboptimal token-level
credit assignment. In this work, we frame reward shaping as an optimization
problem focused on token-level credit assignment. We propose a reward-shaping
function leveraging explainability methods such as SHAP and LIME to estimate
per-token rewards from the reward model. To learn parameters of this shaping
function, we employ a bilevel optimization framework that integrates Bayesian
Optimization and policy training to handle noise from the token reward
estimates. Our experiments show that achieving a better balance of token-level
reward attribution leads to performance improvements over baselines on
downstream tasks and finds an optimal policy faster during training.
Furthermore, we show theoretically that explainability methods that are feature
additive attribution functions maintain the optimal policy as the original
reward.

</details>


### [34] [Quantum Doubly Stochastic Transformers](https://arxiv.org/abs/2504.16275)
*Jannis Born,Filip Skogh,Kahn Rhrissorrakrai,Filippo Utro,Nico Wagner,Aleksandros Sobczyk*

Main category: cs.LG

TL;DR: The QDSFormer is a hybrid classical-quantum Transformer that improves training stability and performance by using a variational quantum circuit for doubly stochastic matrices instead of the traditional Softmax.


<details>
  <summary>Details</summary>
Motivation: Traditional Softmax normalization in Transformers leads to unstable training; doubly stochastic matrices (DSMs) improve performance, but previous methods like Sinkhorn's algorithm are inflexible.

Method: The QDSFormer replaces the Softmax in the self-attention layer with a variational quantum circuit to obtain parametric DSMs, providing a new quantum inductive bias.

Result: QDSFormer consistently outperforms standard Vision Transformers and other doubly stochastic variants on small-scale object recognition tasks, showing better information preservation and expressiveness.

Conclusion: The QDSFormer enhances training stability and mitigates performance variation, suggesting advantages of quantum-inspired methodologies in Transformers.

Abstract: At the core of the Transformer, the Softmax normalizes the attention matrix
to be right stochastic. Previous research has shown that this often
destabilizes training and that enforcing the attention matrix to be doubly
stochastic (through Sinkhorn's algorithm) consistently improves performance
across different tasks, domains and Transformer flavors. However, Sinkhorn's
algorithm is iterative, approximative, non-parametric and thus inflexible
w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been
proven that DSMs can be obtained with a parametric quantum circuit, yielding a
novel quantum inductive bias for DSMs with no known classical analogue.
Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum
doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the
self-attention layer with a variational quantum circuit. We study the
expressive power of the circuit and find that it yields more diverse DSMs that
better preserve information than classical operators. Across multiple
small-scale object recognition tasks, we find that our QDSFormer consistently
surpasses both a standard Vision Transformer and other doubly stochastic
Transformers. Beyond the established Sinkformer, this comparison includes a
novel quantum-inspired doubly stochastic Transformer (based on QR
decomposition) that can be of independent interest. The QDSFormer also shows
improved training stability and lower performance variation suggesting that it
may mitigate the notoriously unstable training of ViTs on small-scale data.

</details>


### [35] [An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon](https://arxiv.org/abs/2504.16276)
*Abhishek Jana,Moeumu Uili,James Atherton,Mark O'Brien,Joe Wood,Leandra Brickson*

Main category: cs.LG

TL;DR: The paper presents an automated pipeline for classifying bird calls of rare species, specifically targeting birds with very limited audio recordings.


<details>
  <summary>Details</summary>
Motivation: There is a need for effective bird call classification tools for rare species that are not covered by existing classifiers, which primarily focus on common birds with ample data.

Method: The authors developed a one-shot classifier using cosine similarity within the embedding space of large bird classification networks, complemented by preprocessing techniques for filtering and denoising.

Result: The system achieved 1.0 recall and 0.95 accuracy for detecting calls of the critically endangered tooth-billed pigeon, demonstrating its effectiveness in both simulated and real-world scenarios.

Conclusion: This open-source classification pipeline offers a valuable resource for conservationists working to monitor and protect endangered bird species.

Abstract: This paper presents an automated one-shot bird call classification pipeline
designed for rare species absent from large publicly available classifiers like
BirdNET and Perch. While these models excel at detecting common birds with
abundant training data, they lack options for species with only 1-3 known
recordings-a critical limitation for conservationists monitoring the last
remaining individuals of endangered birds. To address this, we leverage the
embedding space of large bird classification networks and develop a classifier
using cosine similarity, combined with filtering and denoising preprocessing
techniques, to optimize detection with minimal training data. We evaluate
various embedding spaces using clustering metrics and validate our approach in
both a simulated scenario with Xeno-Canto recordings and a real-world test on
the critically endangered tooth-billed pigeon (Didunculus strigirostris), which
has no existing classifiers and only three confirmed recordings. The final
model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon
calls, making it practical for use in the field. This open-source system
provides a practical tool for conservationists seeking to detect and monitor
rare species on the brink of extinction.

</details>


### [36] [DataS^3: Dataset Subset Selection for Specialization](https://arxiv.org/abs/2504.16277)
*Neha Hulkund,Alaa Maalouf,Levi Cai,Daniel Yang,Tsun-Hsuan Wang,Abigail O'Neil,Timm Haucke,Sandeep Mukherjee,Vikram Ramaswamy,Judy Hansen Shen,Gabriel Tseng,Mike Walmsley,Daniela Rus,Ken Goldberg,Hannah Kerner,Irene Chen,Yogesh Girdhar,Sara Beery*

Main category: cs.LG

TL;DR: The paper addresses the challenge of optimizing machine learning models for specific deployment conditions by proposing a method for dataset subset selection (DS3) that enhances performance on imbalanced and unique data distributions.


<details>
  <summary>Details</summary>
Motivation: There is a need for machine learning models to perform well on specific deployments, which often have imbalanced and unique data distributions that differ from the training data, leading to suboptimal performance.

Method: The paper introduces a formal framework for dataset subset selection (DS3) and presents DataS^3, a benchmark designed for this problem, evaluating various algorithmic methods including coresets and data curation.

Result: The study demonstrates that general-distribution methods often fail in deployment-specific tasks, while tailored deployment-specific expert subsets can achieve accuracy gains up to 51.3% compared to using all available training data.

Conclusion: The findings emphasize the importance of dataset curation tailored to specific deployments to improve performance and training efficiency in real-world machine learning applications.

Abstract: In many real-world machine learning (ML) applications (e.g. detecting broken
bones in x-ray images, detecting species in camera traps), in practice models
need to perform well on specific deployments (e.g. a specific hospital, a
specific national park) rather than the domain broadly. However, deployments
often have imbalanced, unique data distributions. Discrepancy between the
training distribution and the deployment distribution can lead to suboptimal
performance, highlighting the need to select deployment-specialized subsets
from the available training data. We formalize dataset subset selection for
specialization (DS3): given a training set drawn from a general distribution
and a (potentially unlabeled) query set drawn from the desired
deployment-specific distribution, the goal is to select a subset of the
training data that optimizes deployment performance.
  We introduce DataS^3; the first dataset and benchmark designed specifically
for the DS3 problem. DataS^3 encompasses diverse real-world application
domains, each with a set of distinct deployments to specialize in. We conduct a
comprehensive study evaluating algorithms from various families--including
coresets, data filtering, and data curation--on DataS^3, and find that
general-distribution methods consistently fail on deployment-specific tasks.
Additionally, we demonstrate the existence of manually curated
(deployment-specific) expert subsets that outperform training on all available
data with accuracy gains up to 51.3 percent. Our benchmark highlights the
critical role of tailored dataset curation in enhancing performance and
training efficiency on deployment-specific distributions, which we posit will
only become more important as global, public datasets become available across
domains and ML models are deployed in the real world.

</details>


### [37] [Affect Models Have Weak Generalizability to Atypical Speech](https://arxiv.org/abs/2504.16283)
*Jaya Narain,Amrit Romana,Vikramjit Mitra,Colin Lea,Shirley Ren*

Main category: cs.LG

TL;DR: The study investigates how speech conditions impact affect recognition models when applied to atypical speech and suggests improvements for model robustness.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand how various speech atypicalities affect the performance of models designed to recognize emotions from speech, highlighting the need for better training and evaluation datasets.

Method: The study evaluates several publicly available affect recognition models on a dataset of atypical speech and compares the results to models trained on typical speech datasets, focusing on dimensions like intelligibility, monopitch, and harshness.

Result: The presence and degree of speech atypicalities significantly influence affect model outputs; for example, atypical speech is often predicted as sad more frequently compared to typical speech.

Conclusion: Improving affect recognition for atypical speech requires broader training datasets and robust modeling techniques that accommodate differences in voice and speech.

Abstract: Speech and voice conditions can alter the acoustic properties of speech,
which could impact the performance of paralinguistic models for affect for
people with atypical speech. We evaluate publicly available models for
recognizing categorical and dimensional affect from speech on a dataset of
atypical speech, comparing results to datasets of typical speech. We
investigate three dimensions of speech atypicality: intelligibility, which is
related to pronounciation; monopitch, which is related to prosody, and
harshness, which is related to voice quality. We look at (1) distributional
trends of categorical affect predictions within the dataset, (2) distributional
comparisons of categorical affect predictions to similar datasets of typical
speech, and (3) correlation strengths between text and speech predictions for
spontaneous speech for valence and arousal. We find that the output of affect
models is significantly impacted by the presence and degree of speech
atypicalities. For instance, the percentage of speech predicted as sad is
significantly higher for all types and grades of atypical speech when compared
to similar typical speech datasets. In a preliminary investigation on improving
robustness for atypical speech, we find that fine-tuning models on
pseudo-labeled atypical speech data improves performance on atypical speech
without impacting performance on typical speech. Our results emphasize the need
for broader training and evaluation datasets for speech emotion models, and for
modeling approaches that are robust to voice and speech differences.

</details>


### [38] [Semantics at an Angle: When Cosine Similarity Works Until It Doesn't](https://arxiv.org/abs/2504.16318)
*Kisung You*

Main category: cs.LG

TL;DR: The paper reviews the strengths and limitations of cosine similarity in comparing embeddings, emphasizing its effectiveness in many scenarios but also highlighting its drawbacks and the need for alternatives.


<details>
  <summary>Details</summary>
Motivation: To explore the evolution, strengths, and limitations of cosine similarity as a metric for embedding comparison, especially as it relates to meaningful semantic information held in embedding norms.

Method: A reflective and selective examination of existing literature and practices regarding cosine similarity, with a focus on its performance in various settings and emerging alternatives.

Result: The paper identifies scenarios where cosine similarity works well and where it fails, indicating that while it is widely used, it has important limitations that need to be addressed.

Conclusion: There is a need for a better understanding of cosine similarity as a geometric and philosophical concept, and to consider emerging alternatives that could overcome its shortcomings.

Abstract: Cosine similarity has become a standard metric for comparing embeddings in
modern machine learning. Its scale-invariance and alignment with model training
objectives have contributed to its widespread adoption. However, recent studies
have revealed important limitations, particularly when embedding norms carry
meaningful semantic information. This informal article offers a reflective and
selective examination of the evolution, strengths, and limitations of cosine
similarity. We highlight why it performs well in many settings, where it tends
to break down, and how emerging alternatives are beginning to address its blind
spots. We hope to offer a mix of conceptual clarity and practical perspective,
especially for quantitative scientists who think about embeddings not just as
vectors, but as geometric and philosophical objects.

</details>


### [39] [Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks](https://arxiv.org/abs/2504.16360)
*Mao Wang,Tao Wu,Xingping Xian,Shaojie Qiao,Weina Niu,Canyixing Cui*

Main category: cs.LG

TL;DR: The paper introduces the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to enhance graph representation learning by focusing on structural pattern recognition based on node-centric subgraphs.


<details>
  <summary>Details</summary>
Motivation: Existing graph representation learning methods lack the ability to effectively analyze structural patterns, limiting their interpretability and accuracy in tasks involving graph data.

Method: GOMKCN views graphs as node-centric subgraphs and employs the Graph Optimal Matching Kernel (GOMK) to compute similarities between subgraphs and learnable filters, transforming tasks into structural pattern recognition and generating disentangled representations through optimized filters.

Result: Experiments demonstrate that GOMKCN outperforms existing methods in terms of accuracy and interpretability for graph pattern mining and prediction tasks.

Conclusion: GOMKCN advances the theoretical foundation of disentangled graph representation learning, providing a more effective approach for understanding and utilizing graph structures.

Abstract: Graphs effectively characterize relational data, driving graph representation
learning methods that uncover underlying predictive information. As
state-of-the-art approaches, Graph Neural Networks (GNNs) enable end-to-end
learning for diverse tasks. Recent disentangled graph representation learning
enhances interpretability by decoupling independent factors in graph data.
However, existing methods often implicitly and coarsely characterize graph
structures, limiting structural pattern analysis within the graph. This paper
proposes the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to
address this limitation. We view graphs as node-centric subgraphs, where each
subgraph acts as a structural factor encoding position-specific information.
This transforms graph prediction into structural pattern recognition. Inspired
by CNNs, GOMKCN introduces the Graph Optimal Matching Kernel (GOMK) as a
convolutional operator, computing similarities between subgraphs and learnable
graph filters. Mathematically, GOMK maps subgraphs and filters into a Hilbert
space, representing graphs as point sets. Disentangled representations emerge
from projecting subgraphs onto task-optimized filters, which adaptively capture
relevant structural patterns via gradient descent. Crucially, GOMK incorporates
local correspondences in similarity measurement, resolving the trade-off
between differentiability and accuracy in graph kernels. Experiments validate
that GOMKCN achieves superior accuracy and interpretability in graph pattern
mining and prediction. The framework advances the theoretical foundation for
disentangled graph representation learning.

</details>


### [40] [Natural Policy Gradient for Average Reward Non-Stationary RL](https://arxiv.org/abs/2504.16415)
*Neharika Jali,Eshika Pathak,Pranay Sharma,Guannan Qu,Gauri Joshi*

Main category: cs.LG

TL;DR: The paper introduces the Non-Stationary Natural Actor-Critic (NS-NAC), a novel model-free policy-based RL algorithm designed for non-stationary environments, providing theoretical insights and achieving a dynamic regret bound.


<details>
  <summary>Details</summary>
Motivation: To address the gaps in theoretical understanding of policy-based methods in non-stationary reinforcement learning, particularly in infinite-horizon average-reward scenarios.

Method: The authors develop the NS-NAC algorithm, which employs a policy gradient approach with change-based exploration strategies and interprets learning rates as adaptable factors. They also introduce a parameter-free version called BORL-NS-NAC that operates without prior knowledge of the variation budget.

Result: Both NS-NAC and BORL-NS-NAC achieve a dynamic regret bound of $	ilde{	ext{ O}}(|S|^{1/2}|A|^{1/2}	ext{ }    }$  $T^{5/6})$, demonstrating effective performance even in dynamic environments.

Conclusion: The study advances the understanding of policy-based approaches in non-stationary RL, offering robust theoretical guarantees for the proposed algorithms and demonstrating their adaptability to changing environments.

Abstract: We consider the problem of non-stationary reinforcement learning (RL) in the
infinite-horizon average-reward setting. We model it by a Markov Decision
Process with time-varying rewards and transition probabilities, with a
variation budget of $\Delta_T$. Existing non-stationary RL algorithms focus on
model-based and model-free value-based methods. Policy-based methods despite
their flexibility in practice are not theoretically well understood in
non-stationary RL. We propose and analyze the first model-free policy-based
algorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient
method with a restart based exploration for change and a novel interpretation
of learning rates as adapting factors. Further, we present a bandit-over-RL
based parameter-free algorithm BORL-NS-NAC that does not require prior
knowledge of the variation budget $\Delta_T$. We present a dynamic regret of
$\tilde{\mathscr O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$ for both
algorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of
the state and action spaces. The regret analysis leverages a novel adaptation
of the Lyapunov function analysis of NAC to dynamic environments and
characterizes the effects of simultaneous updates in policy, value function
estimate and changes in the environment.

</details>


### [41] [MAGIC: Near-Optimal Data Attribution for Deep Learning](https://arxiv.org/abs/2504.16430)
*Andrew Ilyas,Logan Engstrom*

Main category: cs.LG

TL;DR: This paper introduces MAGIC, a new method for predictive data attribution that effectively estimates the impact of adding or removing training data in non-convex settings.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods for predictive data attribution in large-scale non-convex settings that struggle with weak correlations to ground truth.

Method: MAGIC combines classical data attribution techniques with recent advancements in metadifferentiation to estimate the effects of training data adjustments.

Result: MAGIC provides (nearly) optimal estimates for how model predictions are affected by adding or removing specific training data points.

Conclusion: MAGIC represents a significant improvement in predictive data attribution methods, particularly in complex non-convex environments.

Abstract: The goal of predictive data attribution is to estimate how adding or removing
a given set of training datapoints will affect model predictions. In convex
settings, this goal is straightforward (i.e., via the infinitesimal jackknife).
In large-scale (non-convex) settings, however, existing methods are far less
successful -- current methods' estimates often only weakly correlate with
ground truth. In this work, we present a new data attribution method (MAGIC)
that combines classical methods and recent advances in metadifferentiation to
(nearly) optimally estimate the effect of adding or removing training data on
model predictions.

</details>


### [42] [Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion](https://arxiv.org/abs/2504.16431)
*Ruixiang Zhang,Shuangfei Zhai,Yizhe Zhang,James Thornton,Zijing Ou,Joshua Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: Target Concrete Score Matching (TCSM) is a new objective for training discrete diffusion models that enhances their flexibility and performance by integrating with existing methods and processes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the modeling and generation of discrete data using discrete diffusion processes, providing a unified framework for training these models.

Method: TCSM estimates the concrete score of the target distribution in the clean data space, enabling pre-training and post-training of discrete diffusion models with various capabilities.

Result: Experiments on language modeling tasks show that TCSM either matches or exceeds the performance of current methods while offering greater flexibility and efficiency in training.

Conclusion: TCSM represents a versatile and efficient approach for both pre-training and fine-tuning discrete diffusion models, facilitating integration with reward functions and existing models.

Abstract: Discrete diffusion is a promising framework for modeling and generating
discrete data. In this work, we present Target Concrete Score Matching (TCSM),
a novel and versatile objective for training and fine-tuning discrete diffusion
models. TCSM provides a general framework with broad applicability. It supports
pre-training discrete diffusion models directly from data samples, and many
existing discrete diffusion approaches naturally emerge as special cases of our
more general TCSM framework. Furthermore, the same TCSM objective extends to
post-training of discrete diffusion models, including fine-tuning using reward
functions or preference data, and distillation of knowledge from pre-trained
autoregressive models. These new capabilities stem from the core idea of TCSM,
estimating the concrete score of the target distribution, which resides in the
original (clean) data space. This allows seamless integration with reward
functions and pre-trained models, which inherently only operate in the clean
data space rather than the noisy intermediate spaces of diffusion processes.
Our experiments on language modeling tasks demonstrate that TCSM matches or
surpasses current methods. Additionally, TCSM is versatile, applicable to both
pre-training and post-training scenarios, offering greater flexibility and
sample efficiency.

</details>


### [43] [iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network](https://arxiv.org/abs/2504.16432)
*Ziran Liang,Rui An,Wenqi Fan,Yanghui Rao,Yuxuan Liang*

Main category: cs.LG

TL;DR: The paper presents iTFKAN, an interpretable model for time series forecasting that enhances performance and trustworthiness through model symbolization.


<details>
  <summary>Details</summary>
Motivation: Current deep forecasting methods, while effective, lack interpretability, which is crucial for trust in safety-critical applications like auto-driving and healthcare.

Method: iTFKAN employs model symbolization for interpretability and incorporates two strategies: prior knowledge injection and time-frequency synergy learning, to guide model learning in complex time series data.

Result: Experimental results show that iTFKAN achieves strong forecasting performance alongside high interpretive capabilities.

Conclusion: iTFKAN is a credible option for time series forecasting, balancing performance with the necessary interpretability for deployment in critical domains.

Abstract: As time evolves, data within specific domains exhibit predictability that
motivates time series forecasting to predict future trends from historical
data. However, current deep forecasting methods can achieve promising
performance but generally lack interpretability, hindering trustworthiness and
practical deployment in safety-critical applications such as auto-driving and
healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for
credible time series forecasting. iTFKAN enables further exploration of model
decision rationales and underlying data patterns due to its interpretability
achieved through model symbolization. Besides, iTFKAN develops two strategies,
prior knowledge injection, and time-frequency synergy learning, to effectively
guide model learning under complex intertwined time series data. Extensive
experimental results demonstrated that iTFKAN can achieve promising forecasting
performance while simultaneously possessing high interpretive capabilities.

</details>


### [44] [Private Federated Learning using Preference-Optimized Synthetic Data](https://arxiv.org/abs/2504.16438)
*Charlie Hou,Mei-Yu Wang,Yige Zhu,Daniel Lazar,Giulia Fanti*

Main category: cs.LG

TL;DR: The paper introduces POPri, an algorithm that optimizes client feedback for generating high-quality DP synthetic data in Federated Learning, and demonstrates its superiority over previous methods using the new LargeFedBench benchmark.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of DP synthetic data generation for Federated Learning, leveraging client feedback effectively as prior methods struggled with prompt engineering and iterative refinements.

Method: POPri utilizes preference optimization algorithms like Direct Preference Optimization (DPO) to refine client feedback and enhance the quality of DP synthetic data for training models.

Result: POPri significantly enhances the utility of DP synthetic data, achieving a 68% improvement in next-token prediction accuracy on LargeFedBench compared to 52% for prior methods, and 10% over traditional DP Federated Learning approaches.

Conclusion: The results underscore the effectiveness of utilizing client feedback through preference optimization, positioning POPri as a superior alternative for generating DP synthetic data in Federated Learning settings.

Abstract: In practical settings, differentially private Federated learning (DP-FL) is
the dominant method for training models from private, on-device client data.
Recent work has suggested that DP-FL may be enhanced or outperformed by methods
that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary
algorithms for generating DP synthetic data for FL applications require careful
prompt engineering based on public information and/or iterative private client
feedback. Our key insight is that the private client feedback collected by
prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be
viewed as a preference ranking. Our algorithm, Preference Optimization for
Private Client Data (POPri) harnesses client feedback using preference
optimization algorithms such as Direct Preference Optimization (DPO) to
fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,
we release LargeFedBench, a new federated text benchmark for uncontaminated LLM
evaluations on federated client data. POPri substantially improves the utility
of DP synthetic data relative to prior work on LargeFedBench datasets and an
existing benchmark from Xie et al. (2024). POPri closes the gap between
next-token prediction accuracy in the fully-private and non-private settings by
up to 68%, compared to 52% for prior synthetic data methods, and 10% for
state-of-the-art DP federated learning methods. The code and data are available
at https://github.com/meiyuw/POPri.

</details>


### [45] [Node Assigned physics-informed neural networks for thermal-hydraulic system simulation: CVH/FL module](https://arxiv.org/abs/2504.16447)
*Jeesuk Shin,Cheolwoong Kim,Sunwoong Yang,Minseo Lee,Sung Joong Kim,Joongoo Jeon*

Main category: cs.LG

TL;DR: This study introduces a novel numerical method for thermal-hydraulic system codes using node-assigned physics-informed neural networks (NA-PINN) to improve the simulation of severe accidents in nuclear power plants.


<details>
  <summary>Details</summary>
Motivation: Current thermal-hydraulic system codes like MELCOR and MAAP have limitations due to inconsistent finite difference schemes, prompting the need for an improved numerical method.

Method: The authors developed a node-assigned PINN (NA-PINN) that uses a separate neural network for each nodalization in the system code, focusing on approximating purely temporal solutions without spatial information in the inputs and outputs.

Result: In hydrodynamic module evaluations, NA-PINN achieved a maximum absolute error of 0.007, significantly better than the PINN's maximum absolute error of 1.678, indicating NA-PINN's superior accuracy.

Conclusion: This study is the first to implement a system code using PINN successfully, with plans to extend NA-PINN to a multi-physics solver in future work.

Abstract: Severe accidents (SAs) in nuclear power plants have been analyzed using
thermal-hydraulic (TH) system codes such as MELCOR and MAAP. These codes
efficiently simulate the progression of SAs, while they still have inherent
limitations due to their inconsistent finite difference schemes. The use of
empirical schemes incorporating both implicit and explicit formulations
inherently induces unidirectional coupling in multi-physics analyses. The
objective of this study is to develop a novel numerical method for TH system
codes using physics-informed neural network (PINN). They have shown strength in
solving multi-physics due to the innate feature of neural networks-automatic
differentiation. We propose a node-assigned PINN (NA-PINN) that is suitable for
the control volume approach-based system codes. NA-PINN addresses the issue of
spatial governing equation variation by assigning an individual network to each
nodalization of the system code, such that spatial information is excluded from
both the input and output domains, and each subnetwork learns to approximate a
purely temporal solution. In this phase, we evaluated the accuracy of the PINN
methods for the hydrodynamic module. In the 6 water tank simulation, PINN and
NA-PINN showed maximum absolute errors of 1.678 and 0.007, respectively. It
should be noted that only NA-PINN demonstrated acceptable accuracy. To the best
of the authors' knowledge, this is the first study to successfully implement a
system code using PINN. Our future work involves extending NA-PINN to a
multi-physics solver and developing it in a surrogate manner.

</details>


### [46] [An Effective Gram Matrix Characterizes Generalization in Deep Networks](https://arxiv.org/abs/2504.16450)
*Rubing Yang,Pratik Chaudhari*

Main category: cs.LG

TL;DR: The paper derives a differential equation that describes the evolution of the generalization gap in deep networks trained with gradient descent, linking it to dataset variability and architecture alignment.


<details>
  <summary>Details</summary>
Motivation: To understand how training affects the generalization gap in deep networks and predict test loss based on this understanding.

Method: The authors derive a differential equation governed by a contraction factor and a perturbation factor, and analyze it to compute an effective Gram matrix characterizing the generalization gap.

Result: Empirical evaluations show that the analysis can accurately predict test loss, with residuals primarily lying in the subspace of the effective Gram matrix's smallest eigenvalues, indicating a benign training process.

Conclusion: The alignment between the effective Gram matrix and the residual varies across datasets and architectures, wherein the match of data and architecture is crucial for achieving good generalization.

Abstract: We derive a differential equation that governs the evolution of the
generalization gap when a deep network is trained by gradient descent. This
differential equation is controlled by two quantities, a contraction factor
that brings together trajectories corresponding to slightly different datasets,
and a perturbation factor that accounts for them training on different
datasets. We analyze this differential equation to compute an ``effective Gram
matrix'' that characterizes the generalization gap after training in terms of
the alignment between this Gram matrix and a certain initial ``residual''.
Empirical evaluations on image classification datasets indicate that this
analysis can predict the test loss accurately. Further, at any point during
training, the residual predominantly lies in the subspace of the effective Gram
matrix with the smallest eigenvalues. This indicates that the training process
is benign, i.e., it does not lead to significant deterioration of the
generalization gap (which is zero at initialization). The alignment between the
effective Gram matrix and the residual is different for different datasets and
architectures. The match/mismatch of the data and the architecture is primarily
responsible for good/bad generalization.

</details>


### [47] [Dynamic Time-aware Continual User Representation Learning](https://arxiv.org/abs/2504.16501)
*Seungyoon Choi,Sein Kim,Hongseok Kang,Wonjoong Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: This paper presents DITTO, a novel framework for continual user representation learning that addresses the challenges of catastrophic forgetting and adapts to changing item distributions over time, outperforming existing methods in practical evaluations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations of traditional user modeling approaches that focus on single tasks and struggle with generalization and adaptability, highlighting the need for continual learning methods that incorporate the passage of time and changing item distributions.

Method: The authors propose DITTO, a dynamic time-aware continual user representation learner that reduces catastrophic forgetting and enables knowledge adaptation to current item distributions, evaluated through a new practical scenario that considers task progression over time.

Result: Extensive experiments show that DITTO outperforms state-of-the-art continual learning methods under the proposed practical evaluation scenario.

Conclusion: The study highlights the importance of realistic evaluation scenarios for continual learning and demonstrates that DITTO effectively manages the challenges of shifting item distributions and catastrophic forgetting.

Abstract: Traditional user modeling (UM) approaches have primarily focused on designing
models for a single specific task, but they face limitations in generalization
and adaptability across various tasks. Recognizing these challenges, recent
studies have shifted towards continual learning (CL)-based universal user
representation learning aiming to develop a single model capable of handling
multiple tasks. Despite advancements, existing methods are in fact evaluated
under an unrealistic scenario that does not consider the passage of time as
tasks progress, which overlooks newly emerged items that may change the item
distribution of previous tasks. In this paper, we introduce a practical
evaluation scenario on which CL-based universal user representation learning
approaches should be evaluated, which takes into account the passage of time as
tasks progress. Then, we propose a novel framework Dynamic Time-aware continual
user representation learner, named DITTO, designed to alleviate catastrophic
forgetting despite continuous shifts in item distribution, while also allowing
the knowledge acquired from previous tasks to adapt to the current shifted item
distribution. Through our extensive experiments, we demonstrate the superiority
of DITTO over state-of-the-art methods under a practical evaluation scenario.
Our source code is available at
https://github.com/seungyoon-Choi/DITTO_official.

</details>


### [48] [A Comprehensive Survey of Synthetic Tabular Data Generation](https://arxiv.org/abs/2504.16506)
*Ruxue Shi,Yili Wang,Mengnan Du,Xu Shen,Xin Wang*

Main category: cs.LG

TL;DR: This paper provides a comprehensive survey on synthetic data generation for tabular data, addressing the limitations of existing surveys and proposing a unified framework.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented insights in synthetic tabular data generation and explore recent advances in generative models, specifically LLMs and diffusion models.

Method: The authors propose a comprehensive taxonomy that organizes existing methods and provide an in-depth comparative analysis, along with a detailed pipeline for data generation and evaluation.

Result: A systematic review of synthetic tabular data generation that categorizes methods into traditional approaches, diffusion-based methods, and LLM-based models, identifies challenges, and outlines real-world applications and future research directions.

Conclusion: The survey aims to bridge diverse approaches in the field, enhance understanding of methodological interplay, and guide future research in synthetic tabular data generation.

Abstract: Tabular data remains one of the most prevalent and critical data formats
across diverse real-world applications. However, its effective use in machine
learning (ML) is often constrained by challenges such as data scarcity, privacy
concerns, and class imbalance. Synthetic data generation has emerged as a
promising solution, leveraging generative models to learn the distribution of
real datasets and produce high-fidelity, privacy-preserving samples. Various
generative paradigms have been explored, including energy-based models (EBMs),
variational autoencoders (VAEs), generative adversarial networks (GANs), large
language models (LLMs), and diffusion models. While several surveys have
investigated synthetic tabular data generation, most focus on narrow subdomains
or specific generative methods, such as GANs, diffusion models, or
privacy-preserving techniques. This limited scope often results in fragmented
insights, lacking a comprehensive synthesis that bridges diverse approaches. In
particular, recent advances driven by LLMs and diffusion-based models remain
underexplored. This gap hinders a holistic understanding of the field`s
evolution, methodological interplay, and open challenges. To address this, our
survey provides a unified and systematic review of synthetic tabular data
generation. Our contributions are threefold: (1) we propose a comprehensive
taxonomy that organizes existing methods into traditional approaches,
diffusion-based methods, and LLM-based models, and provide an in-depth
comparative analysis; (2) we detail the complete pipeline for synthetic tabular
data generation, including data synthesis, post-processing, and evaluation; (3)
we identify major challenges, explore real-world applications, and outline open
research questions and future directions to guide future work in this rapidly
evolving area.

</details>


### [49] [Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations](https://arxiv.org/abs/2504.16553)
*Mohammad Mahdi Abedi,David Pardo,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: This paper presents a hybrid optimization framework for training Physics-Informed Neural Networks (PINNs) that enhances convergence speed and stability for solving the Helmholtz equation.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the slow convergence and instability of standard gradient descent training for PINNs, especially in high-frequency wavefield simulations of the Helmholtz equation.

Method: The authors develop a method that integrates a least-squares solver into the gradient descent loss function for optimal updates of the linear output layer, applicable with or without perfectly matched layers. Practical tensor-based implementations are also provided.

Result: Numerical experiments demonstrate that the proposed LS-enhanced method achieves quicker convergence, greater accuracy, and improved stability compared to conventional PINN training, succeeding where standard GD fails.

Conclusion: The hybrid approach is efficient and scalable for large-scale wavefield simulations due to minimal computational overhead from the LS solver operating on a small normal matrix.

Abstract: Physics-Informed Neural Networks (PINNs) have shown promise in solving
partial differential equations (PDEs), including the frequency-domain Helmholtz
equation. However, standard training of PINNs using gradient descent (GD)
suffers from slow convergence and instability, particularly for high-frequency
wavefields. For scattered acoustic wavefield simulation based on Helmholtz
equation, we derive a hybrid optimization framework that accelerates training
convergence by embedding a least-squares (LS) solver directly into the GD loss
function. This formulation enables optimal updates for the linear output layer.
Our method is applicable with or without perfectly matched layers (PML), and we
provide practical tensor-based implementations for both scenarios. Numerical
experiments on benchmark velocity models demonstrate that our approach achieves
faster convergence, higher accuracy, and improved stability compared to
conventional PINN training. In particular, our results show that the
LS-enhanced method converges rapidly even in cases where standard GD-based
training fails. The LS solver operates on a small normal matrix, ensuring
minimal computational overhead and making the method scalable for large-scale
wavefield simulations.

</details>


### [50] [Unified Molecule Generation and Property Prediction](https://arxiv.org/abs/2504.16559)
*Adam Izdebski,Jan Olszewski,Pankhil Gawade,Krzysztof Koras,Serra Korkmaz,Valentin Rauscher,Jakub M. Tomczak,Ewa Szczurek*

Main category: cs.LG

TL;DR: Hyformer is a transformer-based joint model that combines data generation and property prediction, overcoming challenges in joint model training.


<details>
  <summary>Details</summary>
Motivation: To create a unified model that integrates data generation and property prediction, leveraging the benefits of both for improved capabilities in molecular tasks.

Method: Hyformer employs a transformer architecture with an alternating attention mask and a unified pre-training scheme to facilitate joint modeling.

Result: Hyformer demonstrates performance that competes with other joint models and achieves state-of-the-art results in molecular generation and property prediction tasks.

Conclusion: The use of joint modeling through Hyformer enhances molecular representation learning and shows efficacy in applications such as hit identification and antimicrobial peptide design.

Abstract: Modeling the joint distribution of the data samples and their properties
allows to construct a single model for both data generation and property
prediction, with synergistic capabilities reaching beyond purely generative or
predictive models. However, training joint models presents daunting
architectural and optimization challenges. Here, we propose Hyformer, a
transformer-based joint model that successfully blends the generative and
predictive functionalities, using an alternating attention mask together with a
unified pre-training scheme. We show that Hyformer rivals other joint models,
as well as state-of-the-art molecule generation and property prediction models.
Additionally, we show the benefits of joint modeling in downstream tasks of
molecular representation learning, hit identification and antimicrobial peptide
design.

</details>


### [51] [Hyper-Transforming Latent Diffusion Models](https://arxiv.org/abs/2504.16580)
*Ignacio Peis,Batuhan Koyuncu,Isabel Valera,Jes Frellsen*

Main category: cs.LG

TL;DR: The paper presents a new generative framework that combines Implicit Neural Representations and Transformer-based hypernetworks for efficient function generation in latent variable models.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of MLP-based hypernetworks in terms of scalability and representation capacity for generative tasks.

Method: The proposed method uses a Transformer-based decoder to generate INR parameters from latent variables, extending latent diffusion models by replacing standard decoders with a hypernetwork, which can be trained from scratch or fine-tuned selectively.

Result: The framework allows for efficient adaptation of existing generative models to INR-based representations without needing complete retraining, enhancing both representation capacity and computational efficiency.

Conclusion: This integrative approach significantly improves the performance and scalability of generative models leveraging Implicit Neural Representations.

Abstract: We introduce a novel generative framework for functions by integrating
Implicit Neural Representations (INRs) and Transformer-based hypernetworks into
latent variable models. Unlike prior approaches that rely on MLP-based
hypernetworks with scalability limitations, our method employs a
Transformer-based decoder to generate INR parameters from latent variables,
addressing both representation capacity and computational efficiency. Our
framework extends latent diffusion models (LDMs) to INR generation by replacing
standard decoders with a Transformer-based hypernetwork, which can be trained
either from scratch or via hyper-transforming-a strategy that fine-tunes only
the decoder while freezing the pre-trained latent space. This enables efficient
adaptation of existing generative models to INR-based representations without
requiring full retraining.

</details>


### [52] [Enhancing Variable Selection in Large-scale Logistic Regression: Leveraging Manual Labeling with Beneficial Noise](https://arxiv.org/abs/2504.16585)
*Xiaofei Wu,Rongmei Liang*

Main category: cs.LG

TL;DR: The paper presents a distributed penalized logistic regression (PLR) approach that leverages label noise from manual labeling to enhance variable selection in large-scale supervised learning.


<details>
  <summary>Details</summary>
Motivation: The performance of penalized logistic regression (PLR) depends on effective variable selection, and label noise from manual labeling can aid in this selection process, improving accuracy in large-scale settings.

Method: A partition-insensitive parallel algorithm based on the ADMM algorithm is proposed for PLR, allowing for effective modeling with large datasets without being affected by data distribution.

Result: The proposed PLR approach with manually-labeled noisy data outperforms traditional classification techniques in estimation and classification accuracy across various large-scale datasets.

Conclusion: The incorporation of beneficial label noise through distributed computing improves the variable selection and overall performance of PLR in large-scale supervised learning.

Abstract: In large-scale supervised learning, penalized logistic regression (PLR)
effectively addresses the overfitting problem by introducing regularization
terms yet its performance still depends on efficient variable selection
strategies. This paper theoretically demonstrates that label noise stemming
from manual labeling, which is solely related to classification difficulty,
represents a type of beneficial noise for variable selection in PLR. This
benefit is reflected in a more accurate estimation of the selected non-zero
coefficients when compared with the case where only truth labels are used.
Under large-scale settings, the sample size for PLR can become very large,
making it infeasible to store on a single machine. In such cases, distributed
computing methods are required to handle PLR model with manual labeling. This
paper presents a partition-insensitive parallel algorithm founded on the ADMM
(alternating direction method of multipliers) algorithm to address PLR by
incorporating manual labeling. The partition insensitivity of the proposed
algorithm refers to the fact that the solutions obtained by the algorithm will
not change with the distributed storage of data. In addition, the algorithm has
global convergence and a sublinear convergence rate. Experimental results
indicate that, as compared with traditional variable selection classification
techniques, the PLR with manually-labeled noisy data achieves higher estimation
and classification accuracy across multiple large-scale datasets.

</details>


### [53] [Compositional Active Learning of Synchronous Systems through Automated Alphabet Refinement](https://arxiv.org/abs/2504.16624)
*Leo Henry,Thomas Neele,Mohammad Mousavi,Matteo Sammartino*

Main category: cs.LG

TL;DR: The paper presents a new technique for compositional automata learning from behavioral observations, improving scalability and efficiency in learning component models of concurrent systems.


<details>
  <summary>Details</summary>
Motivation: To enhance the active automata learning process, particularly for concurrent systems with unknown decompositions, and to improve efficiency in learning component models.

Method: The proposed technique refines the global alphabet into component alphabets while addressing inconsistencies with global observations through a theoretical treatment of alphabet distributions. A compositional learning algorithm, CoalA, is developed and implemented using LearnLib.

Result: CoalA significantly improves learning efficiency, achieving up to five orders of magnitude reduction in membership queries across more than 630 subject systems and demonstrating better scalability in concurrency-heavy scenarios.

Conclusion: The new compositional learning approach effectively addresses the challenges of automaton learning in concurrent systems, leading to substantial improvements in both membership and equivalence query efficiency.

Abstract: Active automata learning infers automaton models of systems from behavioral
observations, a technique successfully applied to a wide range of domains.
Compositional approaches for concurrent systems have recently emerged. We take
a significant step beyond available results, including those by the authors,
and develop a general technique for compositional learning of a synchronizing
parallel system with an unknown decomposition. Our approach automatically
refines the global alphabet into component alphabets while learning the
component models. We develop a theoretical treatment of distributions of
alphabets, i.e., sets of possibly overlapping component alphabets. We
characterize counter-examples that reveal inconsistencies with global
observations, and show how to systematically update the distribution to restore
consistency. We present a compositional learning algorithm implementing these
ideas, where learning counterexamples precisely correspond to distribution
counterexamples under well-defined conditions. We provide an implementation,
called CoalA, using the state-of-the-art active learning library LearnLib. Our
experiments show that in more than 630 subject systems, CoalA delivers orders
of magnitude improvements (up to five orders) in membership queries and in
systems with significant concurrency, it also achieves better scalability in
the number of equivalence queries.

</details>


### [54] [ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data](https://arxiv.org/abs/2504.16628)
*Haoran Gu,Handing Wang,Yi Mei,Mengjie Zhang,Yaochu Jin*

Main category: cs.LG

TL;DR: The paper introduces ParetoHqD, a method for aligning large language models with human values that improves upon existing algorithms by using preference directions in objective space and focusing training on high-quality data, demonstrating superior performance in experimental tests.


<details>
  <summary>Details</summary>
Motivation: Aligning large language models with multiple human expectations and values is crucial for adequately serving various user needs, and existing algorithms face limitations due to preference representation and reward score imbalances.

Method: ParetoHqD improves preference representation by using preference directions in the objective space and employs a two-stage supervised fine-tuning process with high-quality training sets tailored to each preference direction.

Result: Experimental results showed that ParetoHqD outperformed five baseline methods on two multiobjective alignment tasks.

Conclusion: The proposed ParetoHqD effectively addresses limitations in preference representation and training in multiobjective alignment, leading to better performance in aligning language models with human values.

Abstract: Aligning large language models with multiple human expectations and values is
crucial for ensuring that they adequately serve a variety of user needs. To
this end, offline multiobjective alignment algorithms such as the
Rewards-in-Context algorithm have shown strong performance and efficiency.
However, inappropriate preference representations and training with imbalanced
reward scores limit the performance of such algorithms. In this work, we
introduce ParetoHqD that addresses the above issues by representing human
preferences as preference directions in the objective space and regarding data
near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD
follows a two-stage supervised fine-tuning process, where each stage uses an
individual Pareto high-quality training set that best matches its preference
direction. The experimental results have demonstrated the superiority of
ParetoHqD over five baselines on two multiobjective alignment tasks.

</details>


### [55] [DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization](https://arxiv.org/abs/2504.16639)
*Haoran Chen,Jiapeng Liu,Jiafan Wang,Wenjun Shi*

Main category: cs.LG

TL;DR: The paper presents a Data Augmentation Partial Least Squares Regression (DAPLSR) model that enhances classification performance in data with uneven categories by utilizing manifold optimization and synthetic sample generation.


<details>
  <summary>Details</summary>
Motivation: Traditional PLSR models underperform in situations where the data consists of uneven categories, necessitating an improved approach for better classification results.

Method: The DAPLSR model incorporates the Synthetic Minority Over-sampling Technique (SMOTE) for sample augmentation and employs the Value Difference Metric (VDM) to select resembling nearest neighbor samples. It also introduces a manifold optimization method that leverages the geometric properties of the constraint space for better numerical solutions.

Result: The DAPLSR model demonstrates superior classification performance and outstanding evaluation metrics across various datasets compared to existing methods.

Conclusion: The DAPLSR model significantly improves classification effectiveness and is a promising approach for handling uneven category data.

Abstract: Traditional Partial Least Squares Regression (PLSR) models frequently
underperform when handling data characterized by uneven categories. To address
the issue, this paper proposes a Data Augmentation Partial Least Squares
Regression (DAPLSR) model via manifold optimization. The DAPLSR model
introduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase
the number of samples and utilizes the Value Difference Metric (VDM) to select
the nearest neighbor samples that closely resemble the original samples for
generating synthetic samples. In solving the model, in order to obtain a more
accurate numerical solution for PLSR, this paper proposes a manifold
optimization method that uses the geometric properties of the constraint space
to improve model degradation and optimization. Comprehensive experiments show
that the proposed DAPLSR model achieves superior classification performance and
outstanding evaluation metrics on various datasets, significantly outperforming
existing methods.

</details>


### [56] [Representation Learning via Non-Contrastive Mutual Information](https://arxiv.org/abs/2504.16667)
*Zhaohan Daniel Guo,Bernardo Avila Pires,Khimya Khetarpal,Dale Schuurmans,Bo Dai*

Main category: cs.LG

TL;DR: The paper introduces a new self-supervised objective, Mutual Information Non-Contrastive (MINC) loss, which merges the benefits of contrastive and non-contrastive methods in representation learning.


<details>
  <summary>Details</summary>
Motivation: Labeling data is expensive and time-consuming, leading to a surplus of unlabeled data. Self-supervised learning methods can effectively utilize this unlabeled data to learn useful representations for downstream tasks.

Method: The authors transform the Spectral Contrastive Loss into a non-contrastive format, thereby avoiding pairwise comparisons, lowering variance, while maintaining the mutual information approach to prevent collapse of representations.

Result: MINC loss demonstrated consistent improvements over the baseline of Spectral Contrastive loss when applied to learning image representations on ImageNet.

Conclusion: The proposed MINC loss successfully combines strengths from both contrastive and non-contrastive methods, enabling effective representation learning from unlabeled data.

Abstract: Labeling data is often very time consuming and expensive, leaving us with a
majority of unlabeled data. Self-supervised representation learning methods
such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very
successful at learning meaningful latent representations from unlabeled image
data, resulting in much more general and transferable representations for
downstream tasks. Broadly, self-supervised methods fall into two types: 1)
Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as
BYOL. Contrastive methods are generally trying to maximize mutual information
between related data points, so they need to compare every data point to every
other data point, resulting in high variance, and thus requiring large batch
sizes to work well. Non-contrastive methods like BYOL have much lower variance
as they do not need to make pairwise comparisons, but are much trickier to
implement as they have the possibility of collapsing to a constant vector. In
this paper, we aim to develop a self-supervised objective that combines the
strength of both types. We start with a particular contrastive method called
the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we
convert it into a more general non-contrastive form; this removes the pairwise
comparisons resulting in lower variance, but keeps the mutual information
formulation of the contrastive method preventing collapse. We call our new
objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by
learning image representations on ImageNet (similar to SimCLR and BYOL) and
show that it consistently improves upon the Spectral Contrastive loss baseline.

</details>


### [57] [Efficient Data Valuation Approximation in Federated Learning: A Sampling-based Approach](https://arxiv.org/abs/2504.16668)
*Shuyue Wei,Yongxin Tong,Zimu Zhou,Tianran He,Yi Xu*

Main category: cs.LG

TL;DR: The paper presents a practical approximation algorithm for data valuation in federated learning, addressing the computational challenges of Shapley value (SV) while improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: In federated learning (FL), data providers are reluctant to share datasets unless their value can be accurately assessed. Although Shapley value is a standard metric for data valuation, its computational demands hinder practical use. Existing solutions do not achieve high accuracy and efficiency, creating a need for better methods.

Method: The authors propose a unified stratified-sampling framework for evaluating two schemes, analyze the more promising one under a FL linear regression assumption, and introduce the concept of key combinations to focus on impactful dataset combinations. They then develop the IPSS algorithm, which strategically selects high-impact dataset combinations to reduce time cost with minor approximation error.

Result: The proposed IPSS algorithm significantly reduces computational time while maintaining accuracy, outperforming several baseline methods in terms of both efficiency and effectiveness on FL benchmark datasets.

Conclusion: The proposed algorithm provides a feasible alternative for data valuation in federated learning, making the use of Shapley value practical by overcoming computational barriers and improving robustness.

Abstract: Federated learning paradigm to utilize datasets across multiple data
providers. In FL, cross-silo data providers often hesitate to share their
high-quality dataset unless their data value can be fairly assessed. Shapley
value (SV) has been advocated as the standard metric for data valuation in FL
due to its desirable properties. However, the computational overhead of SV is
prohibitive in practice, as it inherently requires training and evaluating an
FL model across an exponential number of dataset combinations. Furthermore,
existing solutions fail to achieve high accuracy and efficiency, making
practical use of SV still out of reach, because they ignore choosing suitable
computation scheme for approximation framework and overlook the property of
utility function in FL. We first propose a unified stratified-sampling
framework for two widely-used schemes. Then, we analyze and choose the more
promising scheme under the FL linear regression assumption. After that, we
identify a phenomenon termed key combinations, where only limited dataset
combinations have a high-impact on final data value. Building on these
insights, we propose a practical approximation algorithm, IPSS, which
strategically selects high-impact dataset combinations rather than evaluating
all possible combinations, thus substantially reducing time cost with minor
approximation error. Furthermore, we conduct extensive evaluations on the FL
benchmark datasets to demonstrate that our proposed algorithm outperforms a
series of representative baselines in terms of efficiency and effectiveness.

</details>


### [58] [Provable wavelet-based neural approximation](https://arxiv.org/abs/2504.16682)
*Youngmi Hur,Hyojae Lim,Mikyoung Lim*

Main category: cs.LG

TL;DR: The paper presents a wavelet-based framework to analyze neural networks' universal approximation capabilities across various activation functions.


<details>
  <summary>Details</summary>
Motivation: To investigate the conditions under which neural networks can approximate any function using different activation functions, particularly emphasizing the flexibility and error control in network design.

Method: The authors utilize wavelet frame theory in spaces of homogeneous type to derive sufficient conditions for activation functions that ensure function approximation and provide an error estimate.

Result: Sufficient conditions were established that apply to smooth and oscillatory activation functions, and a generalized approximation result for non-smooth activations was derived, controlled by the $L^2$-distance between activation functions.

Conclusion: The findings enhance the understanding of neural network design and adaptability by allowing various activation functions while ensuring effective function approximation.

Abstract: In this paper, we develop a wavelet-based theoretical framework for analyzing
the universal approximation capabilities of neural networks over a wide range
of activation functions. Leveraging wavelet frame theory on the spaces of
homogeneous type, we derive sufficient conditions on activation functions to
ensure that the associated neural network approximates any functions in the
given space, along with an error estimate. These sufficient conditions
accommodate a variety of smooth activation functions, including those that
exhibit oscillatory behavior. Furthermore, by considering the $L^2$-distance
between smooth and non-smooth activation functions, we establish a generalized
approximation result that is applicable to non-smooth activations, with the
error explicitly controlled by this distance. This provides increased
flexibility in the design of network architectures.

</details>


### [59] [MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks](https://arxiv.org/abs/2504.16683)
*Ceren Yildirim,Kamer Kaya,Sinan Yildirim,Erkay Savas*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian framework for estimating differential privacy using multiple membership inference attacks via a Markov chain Monte Carlo algorithm.


<details>
  <summary>Details</summary>
Motivation: To improve the estimation of differential privacy by considering multiple membership inference attacks instead of relying on worst-case scenarios, which may be unrealistic.

Method: A MCMC algorithm named MCMC-DP-Est is developed to provide a full posterior distribution estimate of the privacy parameter, along with a method to generate performance measurements for MIAs.

Result: The proposed framework allows for joint estimation of MIA strengths and the privacy of the training algorithm, demonstrated through numerical examples with both artificial and real data.

Conclusion: MCMC-DP-Est offers a more comprehensive and cautious approach to privacy analysis in differential privacy contexts.

Abstract: We propose a new framework for Bayesian estimation of differential privacy,
incorporating evidence from multiple membership inference attacks (MIA).
Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)
algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior
distribution of the privacy parameter (e.g., instead of just credible
intervals). Critically, the proposed method does not assume that privacy
auditing is performed with the most powerful attack on the worst-case (dataset,
challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est
jointly estimates the strengths of MIAs used and the privacy of the training
algorithm, yielding a more cautious privacy analysis. We also present an
economical way to generate measurements for the performance of an MIA that is
to be used by the MCMC method to estimate privacy. We present the use of the
methods with numerical examples with both artificial and real data.

</details>


### [60] [PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation](https://arxiv.org/abs/2504.16693)
*Wenxuan Li,Hang Zhao,Zhiyuan Yu,Yu Du,Qin Zou,Ruizhen Hu,Kai Xu*

Main category: cs.LG

TL;DR: The paper presents PIN-WM, a Physics-INformed World Model for learning non-prehensile manipulation skills in robotics through model-based reinforcement learning, achieving robust performance and effective Sim2Real transfer.


<details>
  <summary>Details</summary>
Motivation: Learning non-prehensile manipulation is difficult due to its sensitivity to friction and restitution in physical interactions, necessitating robust policy learning and generalization.

Method: The proposed PIN-WM allows for efficient identification of 3D rigid body dynamics from visual data, utilizing differentiable physics simulation and few-shot physical interaction trajectories, combined with Gaussian Splatting for observational loss. Additionally, it generates physics-aware randomizations to create diverse training scenarios.

Result: Evaluations show that PIN-WM, augmented by physics-aware digital cousins, significantly enhances learning of non-prehensile skills, achieving superior Sim2Real transfer compared to existing Real2Sim2Real methods.

Conclusion: The study confirms that PIN-WM effectively bridges the Sim2Real gap in robotic manipulation tasks, offering a novel approach to policy learning in challenging physical environments.

Abstract: While non-prehensile manipulation (e.g., controlled pushing/poking)
constitutes a foundational robotic skill, its learning remains challenging due
to the high sensitivity to complex physical interactions involving friction and
restitution. To achieve robust policy learning and generalization, we opt to
learn a world model of the 3D rigid body dynamics involved in non-prehensile
manipulations and use it for model-based reinforcement learning. We propose
PIN-WM, a Physics-INformed World Model that enables efficient end-to-end
identification of a 3D rigid body dynamical system from visual observations.
Adopting differentiable physics simulation, PIN-WM can be learned with only
few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM
is learned with observational loss induced by Gaussian Splatting without
needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM
into a group of Digital Cousins via physics-aware randomizations which perturb
physics and rendering parameters to generate diverse and meaningful variations
of the PIN-WM. Extensive evaluations on both simulation and real-world tests
demonstrate that PIN-WM, enhanced with physics-aware digital cousins,
facilitates learning robust non-prehensile manipulation skills with Sim2Real
transfer, surpassing the Real2Sim2Real state-of-the-arts.

</details>


### [61] [A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization](https://arxiv.org/abs/2504.16711)
*Shiyin Tan,Jaeeon Park,Dongyuan Li,Renhe Jiang,Manabu Okumura*

Main category: cs.LG

TL;DR: The paper presents a novel retrieval-based framework for multi-document summarization (MDS) that overcomes input length limitations by dynamically selecting queries and ranking documents based on relevance scores, leading to improved summarization outcomes.


<details>
  <summary>Details</summary>
Motivation: Current transformer-based models for MDS face limitations in input length and rely on manually crafted queries, which are impractical and lead to irrelevant content inclusion.

Method: The proposed framework integrates query selection and document ranking into a unified process, identifying salient elementary discourse units (EDUs) as latent queries and filtering out irrelevant EDUs instead of traditional truncation.

Result: The framework shows consistent improvements on multiple MDS datasets in ROUGE metrics, confirming its scalability and flexibility across different model architectures and demonstrating effective query selection and document ranking.

Conclusion: The proposed framework effectively addresses context-length constraints in MDS, making it a robust and reliable solution.

Abstract: In the field of multi-document summarization (MDS), transformer-based models
have demonstrated remarkable success, yet they suffer an input length
limitation. Current methods apply truncation after the retrieval process to fit
the context length; however, they heavily depend on manually well-crafted
queries, which are impractical to create for each document set for MDS.
Additionally, these methods retrieve information at a coarse granularity,
leading to the inclusion of irrelevant content. To address these issues, we
propose a novel retrieval-based framework that integrates query selection and
document ranking and shortening into a unified process. Our approach identifies
the most salient elementary discourse units (EDUs) from input documents and
utilizes them as latent queries. These queries guide the document ranking by
calculating relevance scores. Instead of traditional truncation, our approach
filters out irrelevant EDUs to fit the context length, ensuring that only
critical information is preserved for summarization. We evaluate our framework
on multiple MDS datasets, demonstrating consistent improvements in ROUGE
metrics while confirming its scalability and flexibility across diverse model
architectures. Additionally, we validate its effectiveness through an in-depth
analysis, emphasizing its ability to dynamically select appropriate queries and
accurately rank documents based on their relevance scores. These results
demonstrate that our framework effectively addresses context-length
constraints, establishing it as a robust and reliable solution for MDS.

</details>


### [62] [Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks](https://arxiv.org/abs/2504.16748)
*Yanan Zhao,Feng Ji,Kai Zhao,Xuhao Li,Qiyu Kang,Wenfei Liang,Yahya Alkhatib,Xingchao Jian,Wee Peng Tay*

Main category: cs.LG

TL;DR: The paper presents a novel augmentation-free Graph Contrastive Learning framework using Fractional Differential Equations, achieving state-of-the-art performance without the need for negative samples.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for effective graph representation learning methods, particularly those that do not rely on complex data augmentations or negative sampling.

Method: A framework based on graph neural diffusion models employing learnable encoders governed by Fractional Differential Equations, allowing for the generation of diverse views of graph data.

Result: The proposed method demonstrates state-of-the-art performance on various datasets, successfully capturing local and global graph information without needing negative samples.

Conclusion: The developed augmentation-free framework effectively enhances graph contrastive learning capabilities and is adaptable to different types of datasets.

Abstract: Graph Contrastive Learning (GCL) has recently made progress as an
unsupervised graph representation learning paradigm. GCL approaches can be
categorized into augmentation-based and augmentation-free methods. The former
relies on complex data augmentations, while the latter depends on encoders that
can generate distinct views of the same input. Both approaches may require
negative samples for training. In this paper, we introduce a novel
augmentation-free GCL framework based on graph neural diffusion models.
Specifically, we utilize learnable encoders governed by Fractional Differential
Equations (FDE). Each FDE is characterized by an order parameter of the
differential operator. We demonstrate that varying these parameters allows us
to produce learnable encoders that generate diverse views, capturing either
local or global information, for contrastive learning. Our model does not
require negative samples for training and is applicable to both homophilic and
heterophilic datasets. We demonstrate its effectiveness across various
datasets, achieving state-of-the-art performance.

</details>


### [63] [QAOA-PCA: Enhancing Efficiency in the Quantum Approximate Optimization Algorithm via Principal Component Analysis](https://arxiv.org/abs/2504.16755)
*Owain Parry,Phil McMinn*

Main category: cs.LG

TL;DR: QAOA-PCA is a new method that reduces the parameter space in the QAOA algorithm using PCA, leading to more efficient optimization for larger combinatorial problems.


<details>
  <summary>Details</summary>
Motivation: To address the increasing computational burden in QAOA as the number of layers and parameters grows.

Method: A reparameterization technique called QAOA-PCA employs Principal Component Analysis to reduce the dimensionality of the QAOA parameter space.

Result: Empirical evaluations on the MaxCut problem show that QAOA-PCA requires fewer iterations than standard QAOA while maintaining a competitive performance.

Conclusion: QAOA-PCA provides an efficient optimization method that balances computational efficiency and solution quality, outperforming standard QAOA in most cases when matched by parameter count.

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a promising
variational algorithm for solving combinatorial optimization problems on
near-term devices. However, as the number of layers in a QAOA circuit
increases, which is correlated with the quality of the solution, the number of
parameters to optimize grows linearly. This results in more iterations required
by the classical optimizer, which results in an increasing computational burden
as more circuit executions are needed. To mitigate this issue, we introduce
QAOA-PCA, a novel reparameterization technique that employs Principal Component
Analysis (PCA) to reduce the dimensionality of the QAOA parameter space. By
extracting principal components from optimized parameters of smaller problem
instances, QAOA-PCA facilitates efficient optimization with fewer parameters on
larger instances. Our empirical evaluation on the prominent MaxCut problem
demonstrates that QAOA-PCA consistently requires fewer iterations than standard
QAOA, achieving substantial efficiency gains. While this comes at the cost of a
slight reduction in approximation ratio compared to QAOA with the same number
of layers, QAOA-PCA almost always outperforms standard QAOA when matched by
parameter count. QAOA-PCA strikes a favorable balance between efficiency and
performance, reducing optimization overhead without significantly compromising
solution quality.

</details>


### [64] [Noise-Tolerant Coreset-Based Class Incremental Continual Learning](https://arxiv.org/abs/2504.16763)
*Edison Mucllari,Aswin Raghavan,Zachary Alan Daniels*

Main category: cs.LG

TL;DR: This paper investigates the impact of label and instance noise on continual learning methods in class-incremental learning and proposes new algorithms that enhance robustness against such noise.


<details>
  <summary>Details</summary>
Motivation: The need for continual learning algorithms that adapt to new data distributions while minimizing forgetting is vital in computer vision applications, but noise introduces challenges in the training process.

Method: The study derives a new robustness bound for continual learning methods that leverage memory constructed via Coresets to manage uncorrelated instance noise. It also develops two new continual learning algorithms aimed at creating noise-tolerant replay buffers, followed by an empirical comparison against existing memory-based approaches.

Result: The proposed algorithms show significant improvements in classification accuracy and reduction of forgetting amidst label and instance noise, outperforming traditional memory-based continual learning methods across five diverse datasets.

Conclusion: The research concludes that the new noise-tolerant methods provide a more robust approach to continual learning in the presence of noise, addressing critical limitations of existing techniques.

Abstract: Many applications of computer vision require the ability to adapt to novel
data distributions after deployment. Adaptation requires algorithms capable of
continual learning (CL). Continual learners must be plastic to adapt to novel
tasks while minimizing forgetting of previous tasks.However, CL opens up
avenues for noise to enter the training pipeline and disrupt the CL. This work
focuses on label noise and instance noise in the context of class-incremental
learning (CIL), where new classes are added to a classifier over time, and
there is no access to external data from past classes. We aim to understand the
sensitivity of CL methods that work by replaying items from a memory
constructed using the idea of Coresets. We derive a new bound for the
robustness of such a method to uncorrelated instance noise under a general
additive noise threat model, revealing several insights. Putting the theory
into practice, we create two continual learning algorithms to construct
noise-tolerant replay buffers. We empirically compare the effectiveness of
prior memory-based continual learners and the proposed algorithms under label
and uncorrelated instance noise on five diverse datasets. We show that existing
memory-based CL are not robust whereas the proposed methods exhibit significant
improvements in maximizing classification accuracy and minimizing forgetting in
the noisy CIL setting.

</details>


### [65] [Online model learning with data-assimilated reservoir computers](https://arxiv.org/abs/2504.16767)
*Andrea Nóvoa,Luca Magri*

Main category: cs.LG

TL;DR: The paper presents an online learning framework for forecasting nonlinear spatio-temporal signals using dimensionality reduction, a generalized autoregressive model, and online adaptation through data assimilation techniques.


<details>
  <summary>Details</summary>
Motivation: To develop a method for accurate forecasting of complex spatio-temporal signals, particularly in dynamic systems like fluid flows, by integrating advanced modeling and data assimilation techniques.

Method: The framework combines proper orthogonal decomposition (POD) for dimensionality reduction, a reservoir computer for forecasting dynamics, and ensemble sequential data assimilation for online adaptation.

Result: The two-fold estimation method shows improved ensemble convergence and lower reconstruction error compared to a na"ive estimation approach, while the three-fold method successfully facilitates robust online training of partially-trained reservoir computers.

Conclusion: The integration of data-driven reduced order modeling with Bayesian data assimilation creates new possibilities for scalable online model learning, enhancing the forecasting of nonlinear time series.

Abstract: We propose an online learning framework for forecasting nonlinear
spatio-temporal signals (fields). The method integrates (i) dimensionality
reduction, here, a simple proper orthogonal decomposition (POD) projection;
(ii) a generalized autoregressive model to forecast reduced dynamics, here, a
reservoir computer; (iii) online adaptation to update the reservoir computer
(the model), here, ensemble sequential data assimilation.We demonstrate the
framework on a wake past a cylinder governed by the Navier-Stokes equations,
exploring the assimilation of full flow fields (projected onto POD modes) and
sparse sensors. Three scenarios are examined: a na\"ive physical state
estimation; a two-fold estimation of physical and reservoir states; and a
three-fold estimation that also adjusts the model parameters. The two-fold
strategy significantly improves ensemble convergence and reduces reconstruction
error compared to the na\"ive approach. The three-fold approach enables robust
online training of partially-trained reservoir computers, overcoming
limitations of a priori training. By unifying data-driven reduced order
modelling with Bayesian data assimilation, this work opens new opportunities
for scalable online model learning for nonlinear time series forecasting.

</details>


### [66] [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
*Muhammad Khalifa,Rishabh Agarwal,Lajanugen Logeswaran,Jaekyeom Kim,Hao Peng,Moontae Lee,Honglak Lee,Lu Wang*

Main category: cs.LG

TL;DR: ThinkPRM is a data-efficient step-level verifier that uses minimal supervision to outperform traditional models in verification tasks.


<details>
  <summary>Details</summary>
Motivation: The need for step-level supervision in traditional process reward models (PRMs) makes their training expensive. There is a desire to improve data efficiency while maintaining high verification performance.

Method: ThinkPRM employs a long chain-of-thought (CoT) approach to verification, fine-tuned on significantly fewer process labels compared to existing discriminative PRMs.

Result: ThinkPRM achieves superior results on several benchmarks including ProcessBench, MATH-500, and AIME '24, outperforming LLM-as-a-Judge and discriminative verifiers with only 1% of the process labels from PRM800K.

Conclusion: The findings demonstrate that generative, long CoT PRMs can efficiently scale verification compute while minimizing training supervision, highlighting their potential for improved test-time performance.

Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a
key ingredient for test-time scaling. PRMs require step-level supervision,
making them expensive to train. This work aims to build data-efficient PRMs as
verbalized step-wise reward models that verify every step in the solution by
generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long
CoT verifier fine-tuned on orders of magnitude fewer process labels than those
required by discriminative PRMs. Our approach capitalizes on the inherent
reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and
discriminative verifiers -- using only 1% of the process labels in PRM800K --
across several challenging benchmarks. Specifically, ThinkPRM beats the
baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and
reward-guided search. In an out-of-domain evaluation on a subset of
GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers
trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the
same token budget, ThinkPRM scales up verification compute more effectively
compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of
ProcessBench. Our work highlights the value of generative, long CoT PRMs that
can scale test-time compute for verification while requiring minimal
supervision for training. Our code, data, and models will be released at
https://github.com/mukhal/thinkprm.

</details>


### [67] [Evaluating Autoencoders for Parametric and Invertible Multidimensional Projections](https://arxiv.org/abs/2504.16831)
*Frederik L. Dennig,Nina Geyer,Daniela Blumberg,Yannick Metz,Daniel A. Keim*

Main category: cs.LG

TL;DR: This paper evaluates three autoencoder architectures for achieving parametric and invertible projections in neural networks, highlighting the benefits of customized loss functions.


<details>
  <summary>Details</summary>
Motivation: Understanding how to simultaneously utilize parametric and invertible projections in neural networks for better data representation and generation.

Method: The study compares three autoencoder architectures trained on four datasets, focusing on learning a 2D mapping and its inverse, using t-SNE for analysis.

Result: Autoencoders with a tailored loss function generate smoother parametric and inverse projections compared to standard feed-forward neural networks, allowing customization of the smoothing effect.

Conclusion: Customized loss functions in autoencoders improve data projection quality, enabling a better trade-off between parametric and invertible projection capabilities.

Abstract: Recently, neural networks have gained attention for creating parametric and
invertible multidimensional data projections. Parametric projections allow for
embedding previously unseen data without recomputing the projection as a whole,
while invertible projections enable the generation of new data points. However,
these properties have never been explored simultaneously for arbitrary
projection methods. We evaluate three autoencoder (AE) architectures for
creating parametric and invertible projections. Based on a given projection, we
train AEs to learn a mapping into 2D space and an inverse mapping into the
original space. We perform a quantitative and qualitative comparison on four
datasets of varying dimensionality and pattern complexity using t-SNE. Our
results indicate that AEs with a customized loss function can create smoother
parametric and inverse projections than feed-forward neural networks while
giving users control over the strength of the smoothing effect.

</details>


### [68] [Improving Significant Wave Height Prediction Using Chronos Models](https://arxiv.org/abs/2504.16834)
*Yilin Zhai,Hongyuan Shi,Chao Zhan,Qing Wang,Zaijin You,Nan Wang*

Main category: cs.LG

TL;DR: Chronos, an LLM-powered model for wave height prediction, outperforms traditional methods in efficiency and forecasting accuracy in the Northwest Pacific.


<details>
  <summary>Details</summary>
Motivation: Accurate wave height prediction is crucial for maritime safety and coastal resilience, yet existing models struggle with computational efficiency and nonlinear dynamics.

Method: Chronos employs a large language model-based temporal architecture, analyzing historical wave data from three marine zones to enhance pattern recognition and forecasting efficiency.

Result: Chronos achieves a 14.3% reduction in training time, 2.5x faster inference speed, and superior performance in both short-term (1-24h) and extended-range (1-120h) forecasts, achieving a 0.575 MASE and rank 4/12 in zero-shot testing.

Conclusion: Chronos sets a new standard for wave prediction with its computational efficiency and effectiveness, providing a versatile framework applicable to complex geophysical systems.

Abstract: Accurate wave height prediction is critical for maritime safety and coastal
resilience, yet conventional physics-based models and traditional machine
learning methods face challenges in computational efficiency and nonlinear
dynamics modeling. This study introduces Chronos, the first implementation of a
large language model (LLM)-powered temporal architecture (Chronos) optimized
for wave forecasting. Through advanced temporal pattern recognition applied to
historical wave data from three strategically chosen marine zones in the
Northwest Pacific basin, our framework achieves multimodal improvements: (1)
14.3% reduction in training time with 2.5x faster inference speed compared to
PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;
(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)
sustained predictive leadership in extended-range forecasts (1-120h); and (4)
demonstrated zero-shot capability maintaining median performance (rank 4/12)
against specialized operational models. This LLM-enhanced temporal modeling
paradigm establishes a new standard in wave prediction, offering both
computationally efficient solutions and a transferable framework for complex
geophysical systems modeling.

</details>


### [69] [An Adaptive ML Framework for Power Converter Monitoring via Federated Transfer Learning](https://arxiv.org/abs/2504.16866)
*Panagiotis Kakosimos,Alireza Nemat Saberi,Luca Peretti*

Main category: cs.LG

TL;DR: The paper investigates a combined approach of transfer learning and federated learning for enhancing thermal machine learning models used in power converters.


<details>
  <summary>Details</summary>
Motivation: The need to adapt ML models to varying conditions while addressing data sharing limitations and ensuring security in machine learning applications.

Method: The framework employs a base model adapted by multiple clients using fine-tuning, Transfer Component Analysis, and Deep Domain Adaptation techniques, with the Flower framework for federated learning and Federated Averaging for aggregation.

Result: Validation indicates fine-tuning provides high accuracy for practical applications, while benchmarking offers insights into the strengths and weaknesses of the methods across different scenarios.

Conclusion: Locally hosted federated learning improves performance under data aggregation constraints, whereas cloud-based federated learning scales effectively with more clients.

Abstract: This study explores alternative framework configurations for adapting thermal
machine learning (ML) models for power converters by combining transfer
learning (TL) and federated learning (FL) in a piecewise manner. This approach
inherently addresses challenges such as varying operating conditions, data
sharing limitations, and security implications. The framework starts with a
base model that is incrementally adapted by multiple clients via adapting three
state-of-the-art domain adaptation techniques: Fine-tuning, Transfer Component
Analysis (TCA), and Deep Domain Adaptation (DDA). The Flower framework is
employed for FL, using Federated Averaging for aggregation. Validation with
field data demonstrates that fine-tuning offers a straightforward TL approach
with high accuracy, making it suitable for practical applications. Benchmarking
results reveal a comprehensive comparison of these methods, showcasing their
respective strengths and weaknesses when applied in different scenarios.
Locally hosted FL enhances performance when data aggregation is not feasible,
while cloud-based FL becomes more practical with a significant increase in the
number of clients, addressing scalability and connectivity challenges.

</details>


### [70] [Exploring How LLMs Capture and Represent Domain-Specific Knowledge](https://arxiv.org/abs/2504.16871)
*Mirian Hipolito Garcia,Camille Couturier,Daniel Madrigal Diaz,Ankur Mallick,Anastasios Kyrillidis,Robert Sim,Victor Ruhle,Saravan Rajmohan*

Main category: cs.LG

TL;DR: This paper analyzes the domain sensitivity of Large Language Models (LLMs) in distinguishing queries from different domains.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs naturally capture domain-specific nuances in natural language and how they recognize and represent different domains.

Method: Experiments were conducted to examine the hidden states generated during the prefill phase, revealing trajectories that reflect the model's domain recognition; robustness to prompt variations was also evaluated, with an approach for model selection based on domain trace matching.

Result: LLMs were found to effectively differentiate between queries from related domains, with observations that the fine-tuned model does not always outperform others in accuracy.

Conclusion: The study extends the understanding of how LLMs operate across different domains, applicable to both closed and open-ended generative tasks.

Abstract: We study whether Large Language Models (LLMs) inherently capture
domain-specific nuances in natural language. Our experiments probe the domain
sensitivity of LLMs by examining their ability to distinguish queries from
different domains using hidden states generated during the prefill phase. We
reveal latent domain-related trajectories that indicate the model's internal
recognition of query domains. We also study the robustness of these domain
representations to variations in prompt styles and sources. Our approach
leverages these representations for model selection, mapping the LLM that best
matches the domain trace of the input query (i.e., the model with the highest
performance on similar traces). Our findings show that LLMs can differentiate
queries for related domains, and that the fine-tuned model is not always the
most accurate. Unlike previous work, our interpretations apply to both closed
and open-ended generative tasks

</details>


### [71] [Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion](https://arxiv.org/abs/2504.16875)
*Julian Bedei,Murray McBain,Charles Robert Koch,Jakob Andert,David Gordon*

Main category: cs.LG

TL;DR: This paper presents a hybrid approach combining Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive Control (ML-MPC) for optimizing hydrogen-diesel dual-fuel engine control, addressing individual limitations of both methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the control of hydrogen-diesel dual-fuel engines, which involves challenges related to safety, adaptability, and modeling accuracy in existing control methods.

Method: The proposed method integrates RL with an ML-MPC framework to dynamically adjust control references while ensuring safe operations during RL's exploration phase.

Result: The approach yields a root mean square error (RMSE) of 0.44 bar in load tracking, improving on the 0.57 bar RMSE from ML-MPC alone, demonstrating effective adaptation and safety in control inputs.

Conclusion: The hybrid RL and ML-MPC method effectively ensures safe control while adapting to environmental changes, outperforming traditional methods in load tracking performance.

Abstract: Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive
Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel
dual-fuel engine control, as they can effectively control multiple-input
multiple-output systems and nonlinear processes. ML-MPC is advantageous for
providing safe and optimal controls, ensuring the engine operates within
predefined safety limits. In contrast, RL is distinguished by its adaptability
to changing conditions through its learning-based approach. However, the
practical implementation of either method alone poses challenges. RL requires
high variance in control inputs during early learning phases, which can pose
risks to the system by potentially executing unsafe actions, leading to
mechanical damage. Conversely, ML-MPC relies on an accurate system model to
generate optimal control inputs and has limited adaptability to system drifts,
such as injector aging, which naturally occur in engine applications. To
address these limitations, this study proposes a hybrid RL and ML-MPC approach
that uses an ML-MPC framework while incorporating an RL agent to dynamically
adjust the ML-MPC load tracking reference in response to changes in the
environment. At the same time, the ML-MPC ensures that actions stay safe
throughout the RL agent's exploration. To evaluate the effectiveness of this
approach, fuel pressure is deliberately varied to introduce a model-plant
mismatch between the ML-MPC and the engine test bench. The result of this
mismatch is a root mean square error (RMSE) in indicated mean effective
pressure of 0.57 bar when running the ML-MPC. The experimental results
demonstrate that RL successfully adapts to changing boundary conditions by
altering the tracking reference while ML-MPC ensures safe control inputs. The
quantitative improvement in load tracking by implementing RL is an RSME of 0.44
bar.

</details>


### [72] [I-Con: A Unifying Framework for Representation Learning](https://arxiv.org/abs/2504.16929)
*Shaden Alshammari,John Hershey,Axel Feldmann,William T. Freeman,Mark Hamilton*

Main category: cs.LG

TL;DR: The paper presents a novel information-theoretic framework that generalizes various loss functions in representation learning, leading to improved unsupervised image classification and debiasing methods.


<details>
  <summary>Details</summary>
Motivation: To unify and generalize the numerous loss functions in representation learning under a single information-theoretic equation, providing insights into their underlying geometry and facilitating the development of new loss functions.

Method: The authors introduce an integrated KL divergence framework that connects various machine learning techniques, including clustering, dimensionality reduction, and supervised learning, while presenting proofs for over 23 methods.

Result: The proposed framework leads to the development of state-of-the-art unsupervised image classifiers, achieving an 8% improvement over previous methods on the ImageNet-1K dataset, and offers new debiasing methods for contrastive representation learners.

Conclusion: This work not only advances theoretical understanding of loss functions in machine learning but also enhances practical performance in unsupervised image classification and debiasing.

Abstract: As the field of representation learning grows, there has been a proliferation
of different loss functions to solve different classes of problems. We
introduce a single information-theoretic equation that generalizes a large
collection of modern loss functions in machine learning. In particular, we
introduce a framework that shows that several broad classes of machine learning
methods are precisely minimizing an integrated KL divergence between two
conditional distributions: the supervisory and learned representations. This
viewpoint exposes a hidden information geometry underlying clustering, spectral
methods, dimensionality reduction, contrastive learning, and supervised
learning. This framework enables the development of new loss functions by
combining successful techniques from across the literature. We not only present
a wide array of proofs, connecting over 23 different approaches, but we also
leverage these theoretical results to create state-of-the-art unsupervised
image classifiers that achieve a +8% improvement over the prior
state-of-the-art on unsupervised classification on ImageNet-1K. We also
demonstrate that I-Con can be used to derive principled debiasing methods which
improve contrastive representation learners.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [73] [FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking](https://arxiv.org/abs/2504.16188)
*Jabez Magomere,Elena Kochkina,Samuel Mensah,Simerjot Kaur,Charese H. Smiley*

Main category: cs.CL

TL;DR: FinNLI is a benchmark dataset for Financial Natural Language Inference, highlighting the challenges of applying general-domain models to financial texts.


<details>
  <summary>Details</summary>
Motivation: The need for a specialized dataset for NLI in finance to address the performance gap of general-domain models in financial contexts.

Method: The dataset consists of 21,304 premise-hypothesis pairs derived from diverse financial texts, with a test set annotated by finance experts to minimize spurious correlations.

Result: Evaluation reveals significant performance degradation in NLI tasks with domain shift; Macro F1 scores for pre-trained and large language models are 74.57% and 78.62%, respectively.

Conclusion: FinNLI reveals limitations in current LLMs for financial reasoning, indicating that existing models lack generalizability and that further development is needed.

Abstract: We introduce FinNLI, a benchmark dataset for Financial Natural Language
Inference (FinNLI) across diverse financial texts like SEC Filings, Annual
Reports, and Earnings Call transcripts. Our dataset framework ensures diverse
premise-hypothesis pairs while minimizing spurious correlations. FinNLI
comprises 21,304 pairs, including a high-quality test set of 3,304 instances
annotated by finance experts. Evaluations show that domain shift significantly
degrades general-domain NLI performance. The highest Macro F1 scores for
pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and
78.62%, respectively, highlighting the dataset's difficulty. Surprisingly,
instruction-tuned financial LLMs perform poorly, suggesting limited
generalizability. FinNLI exposes weaknesses in current LLMs for financial
reasoning, indicating room for improvement.

</details>


### [74] [The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy](https://arxiv.org/abs/2504.16271)
*Frederik Bredgaard,Martin Lund Trinhammer,Elisa Bassignana*

Main category: cs.CL

TL;DR: The paper explores the use of Natural Language Processing (NLP) to automatically assess patient attachment styles from psychotherapy transcripts, aiming to enhance treatment personalization and research efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve the delivery of mental healthcare by automating the assessment of attachment style, which is currently a resource-intensive manual process using the PACS.

Method: An exploratory analysis utilizing NLP classification models to analyze psychotherapy transcripts for the automatic identification of patient attachment styles.

Result: Preliminary results indicate potential mislabeling issues, such as confusing 'preoccupied' with 'avoidant' attachment styles, which could negatively affect therapy outcomes.

Conclusion: The study paves the way for more personalized therapy approaches and further research into psychotherapy mechanisms through the efficient use of NLP.

Abstract: The delivery of mental healthcare through psychotherapy stands to benefit
immensely from developments within Natural Language Processing (NLP), in
particular through the automatic identification of patient specific qualities,
such as attachment style. Currently, the assessment of attachment style is
performed manually using the Patient Attachment Coding System (PACS; Talia et
al., 2017), which is complex, resource-consuming and requires extensive
training. To enable wide and scalable adoption of attachment informed treatment
and research, we propose the first exploratory analysis into automatically
assessing patient attachment style from psychotherapy transcripts using NLP
classification models. We further analyze the results and discuss the
implications of using automated tools for this purpose -- e.g., confusing
`preoccupied' patients with `avoidant' likely has a more negative impact on
therapy outcomes with respect to other mislabeling. Our work opens an avenue of
research enabling more personalized psychotherapy and more targeted research
into the mechanisms of psychotherapy through advancements in NLP.

</details>


### [75] [The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation](https://arxiv.org/abs/2504.16286)
*Li Weigang,Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: The study evaluates the performance of large language models (LLMs) versus traditional translation tools in Chinese-English translation, focusing on poetic intent, cultural heritage, and specialized terminology.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in preserving poetic intent, cultural heritage, and handling specialized terminology in machine translation, especially with the rise of large language models.

Method: A diverse corpus was constructed, focusing on Chinese scientific terminology, historical translation paradoxes, and literary metaphors. An evaluation system based on back-translation and the Friedman test was used to assess translation quality across six major LLMs and three traditional tools, utilizing metrics like BLEU, CHRF, TER, and semantic similarity.

Result: Key findings indicate that scientific abstracts benefit from back-translation, traditional tools outperform LLMs in distinct texts, LLMs struggle with cultural and literary retention, and a novel BLEU variant was proposed.

Conclusion: The study enhances the empirical evaluation of Chinese NLP and advances the understanding of cultural fidelity in AI-mediated translation.

Abstract: The rapid advancement of large language models (LLMs) has reshaped the
landscape of machine translation, yet challenges persist in preserving poetic
intent, cultural heritage, and handling specialized terminology in
Chinese-English translation. This study constructs a diverse corpus
encompassing Chinese scientific terminology, historical translation paradoxes,
and literary metaphors. Utilizing a back-translation and Friedman test-based
evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic
similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three
traditional translation tools. Key findings include: (1) Scientific abstracts
often benefit from back-translation, while traditional tools outperform LLMs in
linguistically distinct texts; (2) LLMs struggle with cultural and literary
retention, exemplifying the "paradox of poetic intent"; (3) Some models exhibit
"verbatim back-translation", reflecting emergent memory behavior; (4) A novel
BLEU variant using Jieba segmentation and n-gram weighting is proposed. The
study contributes to the empirical evaluation of Chinese NLP performance and
advances understanding of cultural fidelity in AI-mediated translation.

</details>


### [76] [Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives](https://arxiv.org/abs/2504.16312)
*Zhangdie Yuan,Andreas Vlachos*

Main category: cs.CL

TL;DR: This paper introduces a dataset for evaluating large language models on symmetric and antisymmetric relations, finding that these models perform poorly, and proposes a contrastive learning method for encoder retraining to improve performance.


<details>
  <summary>Details</summary>
Motivation: Capturing symmetric and antisymmetric relations is crucial for various applications, and existing models struggle with these tasks.

Method: The authors develop a novel Wikidata-derived natural language inference dataset and utilize contrastive learning with k-nearest neighbors for encoder retraining.

Result: Large language models perform at random chance levels on the proposed benchmark, while the retrained encoder achieves performance comparable to fine-tuned classification heads with enhanced few-shot learning capability and reduced catastrophic forgetting.

Conclusion: The study highlights a significant gap in relational understanding in LLMs and shows that retraining encoders can effectively improve their relational reasoning capabilities.

Abstract: Capturing symmetric (e.g., country borders another country) and antisymmetric
(e.g., parent_of) relations is crucial for a variety of applications. This
paper tackles this challenge by introducing a novel Wikidata-derived natural
language inference dataset designed to evaluate large language models (LLMs).
Our findings reveal that LLMs perform comparably to random chance on this
benchmark, highlighting a gap in relational understanding. To address this, we
explore encoder retraining via contrastive learning with k-nearest neighbors.
The retrained encoder matches the performance of fine-tuned classification
heads while offering additional benefits, including greater efficiency in
few-shot learning and improved mitigation of catastrophic forgetting.

</details>


### [77] [Transformer-Based Extraction of Statutory Definitions from the U.S. Code](https://arxiv.org/abs/2504.16353)
*Arpana Hosabettu,Harsh Shah*

Main category: cs.CL

TL;DR: This paper presents an advanced NLP system using transformer architectures to automatically extract definitions from the U.S. Code, achieving high precision and recall in identifying legal definitions and terms.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance comprehension and clarity of complex legal texts, specifically the United States Code, through the automatic extraction of legal definitions.

Method: The methodology involves a multi-stage pipeline utilizing Legal-BERT, a domain-specific transformer model, to analyze document structure, classify paragraphs, and extract defined terms and their scopes from legal texts.

Result: The extraction system demonstrated significant improvements, achieving 96.8% precision, 98.9% recall, and an F1-score of 98.2%, outperforming traditional machine learning classifiers.

Conclusion: The work improves the accessibility of legal information and sets the stage for further legal reasoning applications.

Abstract: Automatic extraction of definitions from legal texts is critical for
enhancing the comprehension and clarity of complex legal corpora such as the
United States Code (U.S.C.). We present an advanced NLP system leveraging
transformer-based architectures to automatically extract defined terms, their
definitions, and their scope from the U.S.C. We address the challenges of
automatically identifying legal definitions, extracting defined terms, and
determining their scope within this complex corpus of over 200,000 pages of
federal statutory law. Building upon previous feature-based machine learning
methods, our updated model employs domain-specific transformers (Legal-BERT)
fine-tuned specifically for statutory texts, significantly improving extraction
accuracy. Our work implements a multi-stage pipeline that combines document
structure analysis with state-of-the-art language models to process legal text
from the XML version of the U.S. Code. Each paragraph is first classified using
a fine-tuned legal domain BERT model to determine if it contains a definition.
Our system then aggregates related paragraphs into coherent definitional units
and applies a combination of attention mechanisms and rule-based patterns to
extract defined terms and their jurisdictional scope. The definition extraction
system is evaluated on multiple titles of the U.S. Code containing thousands of
definitions, demonstrating significant improvements over previous approaches.
Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score),
substantially outperforming traditional machine learning classifiers. This work
contributes to improving accessibility and understanding of legal information
while establishing a foundation for downstream legal reasoning tasks.

</details>


### [78] [Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions](https://arxiv.org/abs/2504.16358)
*Tian Bai,Huiyan Ying,Kailong Suo,Junqiu Wei,Tao Fan,Yuanfeng Song*

Main category: cs.CL

TL;DR: This paper presents the Text-to-TrajVis task and introduces the Trajectory Visualization Language (TVL), alongside a new dataset, TrajVL, for transforming natural language questions into trajectory data visualizations.


<details>
  <summary>Details</summary>
Motivation: The goal is to enable natural language interfaces for trajectory visualization systems, addressing the absence of a relevant dataset in the research community.

Method: A new visualization language (TVL) was developed, and a dataset construction method combining Large Language Models (LLMs) with human efforts was proposed, resulting in the TrajVL dataset with 18,140 (question, TVL) pairs.

Result: Evaluation of multiple LLMs on the Text-to-TrajVis task showed that while the task is feasible, it presents significant challenges.

Conclusion: The Text-to-TrajVis task is a novel and valuable area for future research, emphasizing the need for further exploration.

Abstract: This paper introduces the Text-to-TrajVis task, which aims to transform
natural language questions into trajectory data visualizations, facilitating
the development of natural language interfaces for trajectory visualization
systems. As this is a novel task, there is currently no relevant dataset
available in the community. To address this gap, we first devised a new
visualization language called Trajectory Visualization Language (TVL) to
facilitate querying trajectory data and generating visualizations. Building on
this foundation, we further proposed a dataset construction method that
integrates Large Language Models (LLMs) with human efforts to create
high-quality data. Specifically, we first generate TVLs using a comprehensive
and systematic process, and then label each TVL with corresponding natural
language questions using LLMs. This process results in the creation of the
first large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140
(question, TVL) pairs. Based on this dataset, we systematically evaluated the
performance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The
experimental results demonstrate that this task is both feasible and highly
challenging and merits further exploration within the research community.

</details>


### [79] [SplitReason: Learning To Offload Reasoning](https://arxiv.org/abs/2504.16379)
*Yash Akhauri,Anthony Fei,Chi-Chih Chang,Ahmed F. AbouElhamayed,Yueying Li,Mohamed S. Abdelfattah*

Main category: cs.CL

TL;DR: The paper proposes a method to improve reasoning efficiency in large language models by offloading challenging reasoning segments to a more capable model while a smaller model handles simpler segments.


<details>
  <summary>Details</summary>
Motivation: Long token generation during reasoning in LLMs is correlated with higher accuracy but poses efficiency challenges due to its sequential nature.

Method: The approach involves annotating difficult reasoning segments in a dataset, fine-tuning a 1.5B-parameter model using supervised and reinforcement learning to offload challenging tasks to a larger model.

Result: The method enhances AIME24 reasoning accuracy by 24% and 28.3%, while only offloading 1.35% to 5% of generated tokens.

Conclusion: The proposed SplitReason system demonstrates an effective strategy to balance accuracy and efficiency in reasoning tasks for LLMs.

Abstract: Reasoning in large language models (LLMs) tends to produce substantially
longer token generation sequences than simpler language modeling tasks. This
extended generation length reflects the multi-step, compositional nature of
reasoning and is often correlated with higher solution accuracy. From an
efficiency perspective, longer token generation exacerbates the inherently
sequential and memory-bound decoding phase of LLMs. However, not all parts of
this expensive reasoning process are equally difficult to generate. We leverage
this observation by offloading only the most challenging parts of the reasoning
process to a larger, more capable model, while performing most of the
generation with a smaller, more efficient model; furthermore, we teach the
smaller model to identify these difficult segments and independently trigger
offloading when needed. To enable this behavior, we annotate difficult segments
across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT)
dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning
fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to
offload the most challenging parts of its own reasoning process to a larger
model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while
offloading 1.35% and 5% of the generated tokens respectively. We open-source
our SplitReason model, data, code and logs.

</details>


### [80] [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
*Fahmida Liza Piya,Rahmatollah Beheshti*

Main category: cs.CL

TL;DR: The paper presents ConTextual, a novel framework for clinical text summarization that enhances decision-making by integrating context-preserving token filtering with a domain-specific knowledge graph.


<details>
  <summary>Details</summary>
Motivation: Unstructured clinical data is rich in information but is underutilized due to inadequate summarization methods that overlook critical clinical cues.

Method: ConTextual employs a Context-Preserving Token Filtering method in conjunction with a Domain-Specific Knowledge Graph to enhance contextual relevance and information extraction.

Result: Empirical evaluations demonstrate that ConTextual consistently outperforms existing summarization methods across two public benchmark datasets.

Conclusion: ConTextual favors both linguistic coherence and clinical fidelity, providing a scalable solution to improve the precision of clinical text generation.

Abstract: Unstructured clinical data can serve as a unique and rich source of
information that can meaningfully inform clinical practice. Extracting the most
pertinent context from such data is critical for exploiting its true potential
toward optimal and timely decision-making in patient care. While prior research
has explored various methods for clinical text summarization, most prior
studies either process all input tokens uniformly or rely on heuristic-based
filters, which can overlook nuanced clinical cues and fail to prioritize
information critical for decision-making. In this study, we propose Contextual,
a novel framework that integrates a Context-Preserving Token Filtering method
with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By
preserving context-specific important tokens and enriching them with structured
knowledge, ConTextual improves both linguistic coherence and clinical fidelity.
Our extensive empirical evaluations on two public benchmark datasets
demonstrate that ConTextual consistently outperforms other baselines. Our
proposed approach highlights the complementary role of token-level filtering
and structured retrieval in enhancing both linguistic and clinical integrity,
as well as offering a scalable solution for improving precision in clinical
text generation.

</details>


### [81] [Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)
*Jiahao Yuan,Xingzhe Sun,Xing Yu,Jingwen Wang,Dehui Du,Zhiqing Cui,Zixiang Di*

Main category: cs.CL

TL;DR: The paper presents the third-place winning approach, Less is More, for a low-resource structural reasoning task that utilizes minimal labeled data and enhances reasoning quality through innovative techniques.


<details>
  <summary>Details</summary>
Motivation: To tackle a low-resource structural reasoning task that requires interpretable, step-by-step rationales from LLMs with limited labeled examples.

Method: The approach employs a multi-agent framework, reverse-prompt induction, retrieval-augmented reasoning synthesis using GPT-4o, and dual-stage reward-guided filtering, fine-tuned from Meta-Llama-3-8B-Instruct with a unified LoRA+ setup.

Result: The proposed pipeline demonstrates consistent improvements in structured reasoning quality through structure validation and reward filtering across few-shot and zero-shot prompts.

Conclusion: The findings highlight the efficacy of controllable data distillation techniques in enhancing structured inference when faced with low-resource scenarios.

Abstract: The XLLM@ACL2025 Shared Task-III formulates a low-resource structural
reasoning task that challenges LLMs to generate interpretable, step-by-step
rationales with minimal labeled data. We present Less is More, the third-place
winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on
structured reasoning from only 24 labeled examples. Our approach leverages a
multi-agent framework with reverse-prompt induction, retrieval-augmented
reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to
distill high-quality supervision across three subtasks: question parsing, CoT
parsing, and step-level verification. All modules are fine-tuned from
Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure
validation with reward filtering across few-shot and zero-shot prompts, our
pipeline consistently improves structure reasoning quality. These results
underscore the value of controllable data distillation in enhancing structured
inference under low-resource constraints. Our code is available at
https://github.com/Jiahao-Yuan/Less-is-More.

</details>


### [82] [Out-of-the-Box Conditional Text Embeddings from Large Language Models](https://arxiv.org/abs/2504.16411)
*Kosuke Yamada,Peinan Zhang*

Main category: cs.CL

TL;DR: PonTE is an unsupervised method for conditional text embedding that avoids the need for extensive fine-tuning while maintaining performance comparable to supervised models.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in training models for conditional text embedding, primarily the need for extensive labeled data and the associated labor and resource costs.

Method: PonTE employs a causal large language model and a conditional prompt to create unsupervised conditional text embeddings.

Result: PonTE generates useful conditional text embeddings and achieves performance on par with supervised methods in tasks like conditional semantic text similarity and text clustering.

Conclusion: PonTE not only provides a resource-efficient alternative for generating conditional text embeddings but also enhances the interpretability of these embeddings through prompt analysis and visualization.

Abstract: Conditional text embedding is a proposed representation that captures the
shift in perspective on texts when conditioned on a specific aspect. Previous
methods have relied on extensive training data for fine-tuning models, leading
to challenges in terms of labor and resource costs. We propose PonTE, a novel
unsupervised conditional text embedding method that leverages a causal large
language model and a conditional prompt. Through experiments on conditional
semantic text similarity and text clustering, we demonstrate that PonTE can
generate useful conditional text embeddings and achieve performance comparable
to supervised methods without fine-tuning. We also show the interpretability of
text embeddings with PonTE by analyzing word generation following prompts and
embedding visualization.

</details>


### [83] [Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study](https://arxiv.org/abs/2504.16414)
*Mohammad Khodadad,Ali Shiraee Kasmaee,Mahdi Astaraki,Nicholas Sherck,Hamidreza Mahyar,Soheila Samiee*

Main category: cs.CL

TL;DR: The study introduces a benchmark to evaluate compositional reasoning in large language models (LLMs) in chemistry, revealing significant challenges and the need for document retrieval augmentation.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the compositional reasoning capabilities of large language models in the chemistry domain.

Method: A benchmark consisting of a curated dataset and an automated evaluation process was developed, integrating LLMs with named entity recognition systems to create knowledge graphs and generate multi-hop questions for testing.

Result: State-of-the-art models struggle with multi-hop compositional reasoning, and document retrieval notably affects performance, yet reasoning errors persist even with perfect retrieval.

Conclusion: The research benchmarks current LLM limitations and offers a data generation pipeline for diverse reasoning datasets, advancing the understanding of reasoning in computational linguistics.

Abstract: In this study, we introduced a new benchmark consisting of a curated dataset
and a defined evaluation process to assess the compositional reasoning
capabilities of large language models within the chemistry domain. We designed
and validated a fully automated pipeline, verified by subject matter experts,
to facilitate this task. Our approach integrates OpenAI reasoning models with
named entity recognition (NER) systems to extract chemical entities from recent
literature, which are then augmented with external knowledge bases to form a
comprehensive knowledge graph. By generating multi-hop questions across these
graphs, we assess LLM performance in both context-augmented and non-context
augmented settings. Our experiments reveal that even state-of-the-art models
face significant challenges in multi-hop compositional reasoning. The results
reflect the importance of augmenting LLMs with document retrieval, which can
have a substantial impact on improving their performance. However, even perfect
retrieval accuracy with full context does not eliminate reasoning errors,
underscoring the complexity of compositional reasoning. This work not only
benchmarks and highlights the limitations of current LLMs but also presents a
novel data generation pipeline capable of producing challenging reasoning
datasets across various domains. Overall, this research advances our
understanding of reasoning in computational linguistics.

</details>


### [84] [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)
*Hanlei Zhang,Zhuohang Li,Yeshuang Zhu,Hua Xu,Peiwu Wang,Jinchao Zhang,Jie Zhou,Haige Zhu*

Main category: cs.CL

TL;DR: The paper introduces MMLA, a benchmark for evaluating multimodal large language models (MLLMs) on their understanding of cognitive-level semantics in human conversation.


<details>
  <summary>Details</summary>
Motivation: The need for research on the capabilities of multimodal large language models to comprehend cognitive-level semantics is highlighted, as previous work in multimodal language analysis has been limited.

Method: The MMLA benchmark consists of over 61K multimodal utterances from staged and real-world scenarios, focusing on six core dimensions of multimodal semantics. Eight mainstream LLMs and MLLMs were evaluated using zero-shot inference, supervised fine-tuning, and instruction tuning methods.

Result: Experiments show that even the fine-tuned models only achieve 60% to 70% accuracy, indicating significant limitations in the ability of current MLLMs to understand complex human language.

Conclusion: MMLA is expected to provide a foundational resource for advancing research in multimodal language analysis and exploring the potential of large language models.

Abstract: Multimodal language analysis is a rapidly evolving field that leverages
multiple modalities to enhance the understanding of high-level semantics
underlying human conversational utterances. Despite its significance, little
research has investigated the capability of multimodal large language models
(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce
MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA
comprises over 61K multimodal utterances drawn from both staged and real-world
scenarios, covering six core dimensions of multimodal semantics: intent,
emotion, dialogue act, sentiment, speaking style, and communication behavior.
We evaluate eight mainstream branches of LLMs and MLLMs using three methods:
zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive
experiments reveal that even fine-tuned models achieve only about 60%~70%
accuracy, underscoring the limitations of current MLLMs in understanding
complex human language. We believe that MMLA will serve as a solid foundation
for exploring the potential of large language models in multimodal language
analysis and provide valuable resources to advance this field. The datasets and
code are open-sourced at https://github.com/thuiar/MMLA.

</details>


### [85] [EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records](https://arxiv.org/abs/2504.16448)
*Shuguang Zhao,Qiangzhong Feng,Zhiyang He,Peipei Sun,Yingying Wang,Xiaodong Tao,Xiaoliang Lu,Mei Cheng,Xinyue Wu,Yanyan Wang,Wei Liang*

Main category: cs.CL

TL;DR: EMRModel leverages LoRA-based fine-tuning and a novel prompt design to enhance structured extraction of information from medical consultation dialogues.


<details>
  <summary>Details</summary>
Motivation: The unstructured nature of medical consultation dialogues impedes their effective use in clinical diagnosis and treatment.

Method: EMRModel integrates LoRA-based fine-tuning with code-style prompts and utilizes a high-quality dataset of annotated medical consultations for structured extraction.

Result: EMRModel achieves an F1 score of 88.1%, representing a 49.5% improvement over standard pre-trained models and outperforming traditional LoRA methods.

Conclusion: The results validate EMRModel's effectiveness in extracting structured information from medical dialogues, advancing NLP in healthcare.

Abstract: Medical consultation dialogues contain critical clinical information, yet
their unstructured nature hinders effective utilization in diagnosis and
treatment. Traditional methods, relying on rule-based or shallow machine
learning techniques, struggle to capture deep and implicit semantics. Recently,
large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight
fine-tuning method, have shown promise for structured information extraction.
We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning
with code-style prompt design, aiming to efficiently convert medical
consultation dialogues into structured electronic medical records (EMRs).
Additionally, we construct a high-quality, realistically grounded dataset of
medical consultation dialogues with detailed annotations. Furthermore, we
introduce a fine-grained evaluation benchmark for medical consultation
information extraction and provide a systematic evaluation methodology,
advancing the optimization of medical natural language processing (NLP) models.
Experimental results show EMRModel achieves an F1 score of 88.1%, improving
by49.5% over standard pre-trained models. Compared to traditional LoRA
fine-tuning methods, our model shows superior performance, highlighting its
effectiveness in structured medical record extraction tasks.

</details>


### [86] [T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning](https://arxiv.org/abs/2504.16460)
*Vignesh Ethiraj,Sidhanth Menon,Divya Vijay*

Main category: cs.CL

TL;DR: T-VEC is a novel embedding model tailored for the telecommunications industry, achieving superior performance in handling telecom-specific semantics compared to standard NLP models.


<details>
  <summary>Details</summary>
Motivation: Standard NLP models struggle with the specialized vocabulary and complex concepts inherent in the telecommunications industry, leading to poor performance on related tasks.

Method: T-VEC, developed by NetoAI, is adapted from the gte-Qwen2-1.5B-instruct model using a triplet loss objective on a curated dataset, involving significant weight modifications across 338 layers, and includes a dedicated telecom-specific tokenizer.

Result: T-VEC outperforms established models with an average MTEB score of 0.825 and an exceptional performance of 0.9380 on a telecom-specific triplet evaluation benchmark, visually demonstrated through improved embedding separation.

Conclusion: This work establishes NetoAI as a leader in telecom AI innovation, offering an advanced, open-source tool for tackling industry-specific NLP challenges.

Abstract: The specialized vocabulary and complex concepts of the telecommunications
industry present significant challenges for standard Natural Language
Processing models. Generic text embeddings often fail to capture
telecom-specific semantics, hindering downstream task performance. We introduce
T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the
telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created
by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet
loss objective on a meticulously curated, large-scale dataset of
telecom-specific data. Crucially, this process involved substantial
modification of weights across 338 layers of the base model, ensuring deep
integration of domain knowledge, far exceeding superficial adaptation
techniques. We quantify this deep change via weight difference analysis. A key
contribution is the development and open-sourcing (MIT License) of the first
dedicated telecom-specific tokenizer, enhancing the handling of industry
jargon. T-VEC achieves a leading average MTEB score (0.825) compared to
established models and demonstrates vastly superior performance (0.9380 vs.
less than 0.07) on our internal telecom-specific triplet evaluation benchmark,
indicating an exceptional grasp of domain-specific nuances, visually confirmed
by improved embedding separation. This work positions NetoAI at the forefront
of telecom AI innovation, providing the community with a powerful, deeply
adapted, open-source tool.

</details>


### [87] [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](https://arxiv.org/abs/2504.16511)
*Fengze Liu,Weidong Zhou,Binbin Liu,Zhimiao Yu,Yifan Zhang,Haobin Lin,Yifeng Yu,Xiaohuan Zhou,Taifeng Wang,Yong Cao*

Main category: cs.CL

TL;DR: QuaDMix is a unified framework that optimizes data selection for training large language models by jointly considering quality and diversity, resulting in improved performance compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing data selection methods for large language model training often optimize quality and diversity separately, ignoring their trade-off, hence a unified approach is needed.

Method: QuaDMix introduces multiple quality metrics and domain classification for evaluating data points, utilizing a parameterized sampling function to balance quality and diversity; it also employs simulated experiments with LightGBM for parameter optimization.

Result: QuaDMix achieved an average performance improvement of 7.2% across various benchmarks, outperforming traditional quality and diversity strategies.

Conclusion: The results substantiate the importance of jointly optimizing data quality and diversity in training large language models.

Abstract: Quality and diversity are two critical metrics for the training data of large
language models (LLMs), positively impacting performance. Existing studies
often optimize these metrics separately, typically by first applying quality
filtering and then adjusting data proportions. However, these approaches
overlook the inherent trade-off between quality and diversity, necessitating
their joint consideration. Given a fixed training quota, it is essential to
evaluate both the quality of each data point and its complementary effect on
the overall dataset. In this paper, we introduce a unified data selection
framework called QuaDMix, which automatically optimizes the data distribution
for LLM pretraining while balancing both quality and diversity. Specifically,
we first propose multiple criteria to measure data quality and employ domain
classification to distinguish data points, thereby measuring overall diversity.
QuaDMix then employs a unified parameterized data sampling function that
determines the sampling probability of each data point based on these quality
and diversity related labels. To accelerate the search for the optimal
parameters involved in the QuaDMix framework, we conduct simulated experiments
on smaller models and use LightGBM for parameters searching, inspired by the
RegMix method. Our experiments across diverse models and datasets demonstrate
that QuaDMix achieves an average performance improvement of 7.2% across
multiple benchmarks. These results outperform the independent strategies for
quality and diversity, highlighting the necessity and ability to balance data
quality and diversity.

</details>


### [88] [Transformers for Complex Query Answering over Knowledge Hypergraphs](https://arxiv.org/abs/2504.16537)
*Hong Ting Tsang,Zihao Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: The paper introduces the Logical Knowledge Hypergraph Transformer (LKHGT) for Complex Query Answering (CQA) using newly sampled datasets to enhance representation of diverse queries in knowledge hypergraphs.


<details>
  <summary>Details</summary>
Motivation: Traditional triple knowledge graphs (KGs) are limited in representing real-world data complexities. The need for models that can handle varying arity relationships and equal entity contributions prompts the research.

Method: The authors propose LKHGT, a two-stage transformer model with a Projection Encoder for atomic projections and a Logical Encoder for complex operations, integrating Type Aware Bias (TAB) for effective token interaction.

Result: Experimental results indicate that LKHGT outperforms existing CQA methods in handling knowledge hypergraphs and shows strong generalization capabilities for out-of-distribution query types.

Conclusion: LKHGT presents a significant advancement in CQA methodologies, effectively leveraging knowledge hypergraphs to answer complex queries with greater accuracy and flexibility.

Abstract: Complex Query Answering (CQA) has been extensively studied in recent years.
In order to model data that is closer to real-world distribution, knowledge
graphs with different modalities have been introduced. Triple KGs, as the
classic KGs composed of entities and relations of arity 2, have limited
representation of real-world facts. Real-world data is more sophisticated.
While hyper-relational graphs have been introduced, there are limitations in
representing relationships of varying arity that contain entities with equal
contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and
M-FB15k-HCQA. Each dataset contains various query types that include logical
operations such as projection, negation, conjunction, and disjunction. In order
to answer knowledge hypergraph (KHG) existential first-order queries, we
propose a two-stage transformer model, the Logical Knowledge Hypergraph
Transformer (LKHGT), which consists of a Projection Encoder for atomic
projection and a Logical Encoder for complex logical operations. Both encoders
are equipped with Type Aware Bias (TAB) for capturing token interactions.
Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA
method over KHG and is able to generalize to out-of-distribution query types.

</details>


### [89] [PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression](https://arxiv.org/abs/2504.16574)
*Lizhe Chen,Binjia Zhou,Yuyao Ge,Jiayi Chen,Shiguang NI*

Main category: cs.CL

TL;DR: The paper introduces Prompt Importance Sampling (PIS), a novel framework for compressing prompts in large language models (LLMs) by analyzing attention scores to dynamically sample important tokens, achieving state-of-the-art performance and enhanced reasoning efficiency.


<details>
  <summary>Details</summary>
Motivation: The high costs associated with the performance of large language models limit their widespread adoption, creating a need for effective prompt compression methods.

Method: Prompt Importance Sampling (PIS) employs a dual-level compression mechanism: token-level compression using attention scores and reinforcement learning, and semantic-level compression through a Russian roulette sampling strategy.

Result: PIS demonstrates state-of-the-art compression performance across multiple domain benchmarks and improves reasoning efficiency by optimizing context structuring.

Conclusion: The proposed framework advances prompt engineering by providing both theoretical grounding and practical efficiency in managing contexts for large language models.

Abstract: Large language models (LLMs) have achieved remarkable progress, demonstrating
unprecedented capabilities across various natural language processing tasks.
However, the high costs associated with such exceptional performance limit the
widespread adoption of LLMs, highlighting the need for prompt compression.
Existing prompt compression methods primarily rely on heuristic truncation or
abstractive summarization techniques, which fundamentally overlook the
intrinsic mechanisms of LLMs and lack a systematic evaluation of token
importance for generation. In this work, we introduce Prompt Importance
Sampling (PIS), a novel compression framework that dynamically compresses
prompts by sampling important tokens based on the analysis of attention scores
of hidden states. PIS employs a dual-level compression mechanism: 1) at the
token level, we quantify saliency using LLM-native attention scores and
implement adaptive compression through a lightweight 9-layer reinforcement
learning (RL) network; 2) at the semantic level, we propose a Russian roulette
sampling strategy for sentence-level importance sampling. Comprehensive
evaluations across multiple domain benchmarks demonstrate that our method
achieves state-of-the-art compression performance. Notably, our framework
serendipitously enhances reasoning efficiency through optimized context
structuring. This work advances prompt engineering by offering both theoretical
grounding and practical efficiency in context management for LLMs.

</details>


### [90] [Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study](https://arxiv.org/abs/2504.16601)
*Andy Li,Wei Zhou,Rashina Hoda,Chris Bain,Peter Poon*

Main category: cs.CL

TL;DR: The study compares the translation quality of large language models (LLMs) and traditional machine translation (MT) tools for medical consultation summaries into Arabic, Chinese, and Vietnamese, finding that traditional MT tools generally excel but LLMs show potential for simpler texts.


<details>
  <summary>Details</summary>
Motivation: To assess translation quality of LLMs versus traditional MT tools for medical consultation summaries, focusing on both patient-friendly and clinician-focused texts in various languages.

Method: The study used standard automated metrics to evaluate translations by LLMs and traditional MT tools from English to Arabic, Chinese, and Vietnamese.

Result: Traditional MT tools outperformed LLMs in most cases, especially for complex texts, but LLMs performed better in translating simpler summaries into Vietnamese and Chinese. Arabic translations improved with text complexity due to the language's morphology.

Conclusion: LLMs exhibit contextual flexibility but lack consistency; existing evaluation metrics inadequately measure clinical relevance, indicating a need for domain-specific training, better evaluation methods, and human oversight in medical translation.

Abstract: This study evaluates how well large language models (LLMs) and traditional
machine translation (MT) tools translate medical consultation summaries from
English into Arabic, Chinese, and Vietnamese. It assesses both patient,
friendly and clinician, focused texts using standard automated metrics. Results
showed that traditional MT tools generally performed better, especially for
complex texts, while LLMs showed promise, particularly in Vietnamese and
Chinese, when translating simpler summaries. Arabic translations improved with
complexity due to the language's morphology. Overall, while LLMs offer
contextual flexibility, they remain inconsistent, and current evaluation
metrics fail to capture clinical relevance. The study highlights the need for
domain-specific training, improved evaluation methods, and human oversight in
medical translation.

</details>


### [91] [Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories](https://arxiv.org/abs/2504.16604)
*Mareike Lisker,Christina Gottschalk,Helena Mihaljević*

Main category: cs.CL

TL;DR: This paper evaluates the effectiveness of large language models in applying counterspeech strategies against conspiracy theories, revealing significant limitations in their output quality.


<details>
  <summary>Details</summary>
Motivation: To explore the use of large language models in generating counterspeech for conspiracy theories, addressing the absence of a suitable dataset for such applications.

Method: The study assesses GPT-4o, Llama 3, and Mistral by using structured prompts based on counterspeech strategies from psychological research.

Result: The models often produce generic, repetitive, or superficial responses, over-acknowledge fear, and frequently hallucinate facts, making their practical application problematic.

Conclusion: The findings highlight significant shortcomings in using current large language models for counterspeech against conspiracy theories, indicating a need for further refinement and research.

Abstract: Counterspeech is a key strategy against harmful online content, but scaling
expert-driven efforts is challenging. Large Language Models (LLMs) present a
potential solution, though their use in countering conspiracy theories is
under-researched. Unlike for hate speech, no datasets exist that pair
conspiracy theory comments with expert-crafted counterspeech. We address this
gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively
apply counterspeech strategies derived from psychological research provided
through structured prompts. Our results show that the models often generate
generic, repetitive, or superficial results. Additionally, they
over-acknowledge fear and frequently hallucinate facts, sources, or figures,
making their prompt-based use in practical applications problematic.

</details>


### [92] [TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval](https://arxiv.org/abs/2504.16627)
*Prasanna Devadiga,Arya Suneesh,Pawan Kumar Rajpoot,Bharatdeep Hazarika,Aditya U Baliga*

Main category: cs.CL

TL;DR: The paper presents a two-stage approach to retrieve fact-checked claims effectively in both monolingual and crosslingual contexts using a fine-tuned embedding model and an LLM-based reranker.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of retrieving previously fact-checked claims, highlighting the importance of this task in combating disinformation globally.

Method: The methodology involves a two-stage strategy: first, a reliable baseline retrieval system using a fine-tuned embedding model, followed by an LLM-based reranker to improve retrieval accuracy, especially across languages.

Result: The integrated system achieved a success@10 score of 0.938 for monolingual tests and 0.81025 for crosslingual tests, indicating high effectiveness in both settings.

Conclusion: The study concludes that LLM-based translation significantly enhances multilingual information retrieval capabilities, and the pipeline can be efficiently executed on consumer-grade GPUs.

Abstract: We address the challenge of retrieving previously fact-checked claims in
monolingual and crosslingual settings - a critical task given the global
prevalence of disinformation. Our approach follows a two-stage strategy: a
reliable baseline retrieval system using a fine-tuned embedding model and an
LLM-based reranker. Our key contribution is demonstrating how LLM-based
translation can overcome the hurdles of multilingual information retrieval.
Additionally, we focus on ensuring that the bulk of the pipeline can be
replicated on a consumer GPU. Our final integrated system achieved a success@10
score of 0.938 and 0.81025 on the monolingual and crosslingual test sets,
respectively.

</details>


### [93] [A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics](https://arxiv.org/abs/2504.16677)
*Luisa Shimabucoro,Ahmet Ustun,Marzieh Fadaee,Sebastian Ruder*

Main category: cs.CL

TL;DR: This study investigates the dynamics of cross-lingual transfer in multilingual fine-tuning of large language models across various tasks and settings.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics behind cross-lingual transfer in large language models and improve their utility in multilingual environments.

Method: Analyzed two model families with up to 35B parameters, trained on multilingual data for three generative tasks (summarization, instruction following, and mathematical reasoning) in both single-task and multi-task settings.

Result: Cross-lingual transfer dynamics were found to be influenced by the combination of post-training settings rather than isolated variables, affecting multilingual performance.

Conclusion: Conditions for effective cross-lingual transfer were identified, emphasizing the complexity of the influences at play in multilingual fine-tuning.

Abstract: In order for large language models to be useful across the globe, they are
fine-tuned to follow instructions on multilingual data. Despite the ubiquity of
such post-training, a clear understanding of the dynamics that enable
cross-lingual transfer remains elusive. This study examines cross-lingual
transfer (CLT) dynamics in realistic post-training settings. We study two model
families of up to 35B parameters in size trained on carefully controlled
mixtures of multilingual data on three generative tasks with varying levels of
complexity (summarization, instruction following, and mathematical reasoning)
in both single-task and multi-task instruction tuning settings. Overall, we
find that the dynamics of cross-lingual transfer and multilingual performance
cannot be explained by isolated variables, varying depending on the combination
of post-training settings. Finally, we identify the conditions that lead to
effective cross-lingual transfer in practice.

</details>


### [94] [HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations](https://arxiv.org/abs/2504.16754)
*Kwangseob Ahn*

Main category: cs.CL

TL;DR: This paper introduces HEMA, a dual-memory architecture for large language models that enhances coherence in long conversations beyond 300 turns.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with coherence in extended dialogues, despite performing well within shorter context windows, necessitating a solution to maintain narrative continuity in long interactions.

Method: HEMA combines two systems: Compact Memory, which keeps a global narrative summary, and Vector Memory, which stores episodic chunk embeddings queried via cosine similarity, integrated with a 6B-parameter transformer.

Result: With HEMA, factual recall accuracy improved from 41% to 87%, coherence ratings enhanced from 2.7 to 4.3 on a 5-point scale, and retrieval effectiveness metrics showed significant increases, such as P@5 >= 0.80 and R@50 >= 0.74.

Conclusion: HEMA provides a robust solution for maintaining coherence and factual recall in lengthy dialogues, supporting privacy-aware conversational AI systems capable of extended interactions without needing retraining.

Abstract: Large language models (LLMs) struggle with maintaining coherence in extended
conversations spanning hundreds of turns, despite performing well within their
context windows. This paper introduces HEMA (Hippocampus-Inspired Extended
Memory Architecture), a dual-memory system inspired by human cognitive
processes. HEMA combines Compact Memory - a continuously updated one-sentence
summary preserving global narrative coherence, and Vector Memory - an episodic
store of chunk embeddings queried via cosine similarity. When integrated with a
6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns
while keeping prompt length under 3,500 tokens. Experimental results show
substantial improvements: factual recall accuracy increases from 41% to 87%,
and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K
indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling
the area under the precision-recall curve compared to summarization-only
approaches. Ablation studies reveal two key insights: semantic forgetting
through age-weighted pruning reduces retrieval latency by 34% with minimal
recall loss, and a two-level summary hierarchy prevents cascade errors in
ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that
combining verbatim recall with semantic continuity provides a practical
solution for privacy-aware conversational AI capable of month-long dialogues
without model retraining.

</details>


### [95] [How Effective are Generative Large Language Models in Performing Requirements Classification?](https://arxiv.org/abs/2504.16768)
*Waad Alhoshan,Alessio Ferrari,Liping Zhao*

Main category: cs.CL

TL;DR: The paper investigates the performance of generative large language models in requirements classification, highlighting the importance of prompt design and model architecture while noting situation-dependent factors.


<details>
  <summary>Details</summary>
Motivation: There is a gap in exploring the effectiveness of generative LLMs for requirements classification, despite their success in natural language processing tasks.

Method: An extensive experimental study involving over 400 experiments across three datasets (PROMISE NFR, Functional-Quality, and SecReq) is conducted to assess the performance of Bloom, Gemma, and Llama in binary and multi-class requirements classification.

Result: The study finds that prompt design and LLM architecture are crucial across the board, while the impact of dataset variations is situational and dependent on task complexity.

Conclusion: The insights from this research can inform future model development and deployment strategies, emphasizing the need for optimized prompt structures and model architectures tailored to specific tasks.

Abstract: In recent years, transformer-based large language models (LLMs) have
revolutionised natural language processing (NLP), with generative models
opening new possibilities for tasks that require context-aware text generation.
Requirements engineering (RE) has also seen a surge in the experimentation of
LLMs for different tasks, including trace-link detection, regulatory
compliance, and others. Requirements classification is a common task in RE.
While non-generative LLMs like BERT have been successfully applied to this
task, there has been limited exploration of generative LLMs. This gap raises an
important question: how well can generative LLMs, which produce context-aware
outputs, perform in requirements classification? In this study, we explore the
effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing
both binary and multi-class requirements classification. We design an extensive
experimental study involving over 400 experiments across three widely used
datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes
that while factors like prompt design and LLM architecture are universally
important, others-such as dataset variations-have a more situational impact,
depending on the complexity of the classification task. This insight can guide
future model development and deployment strategies, focusing on optimising
prompt structures and aligning model architectures with task-specific needs for
improved performance.

</details>


### [96] [Evaluation Framework for AI Systems in "the Wild"](https://arxiv.org/abs/2504.16778)
*Sarah Jabbour,Trenton Chang,Anindya Das Antar,Joseph Peper,Insu Jang,Jiachen Liu,Jae-Won Chung,Shiqi He,Michael Wellman,Bryan Goodman,Elizabeth Bondi-Kelly,Kevin Samy,Rada Mihalcea,Mosharaf Chowhury,David Jurgens,Lu Wang*

Main category: cs.CL

TL;DR: This paper proposes a new framework for evaluating Generative AI systems that reflects real-world applications, focusing on dynamic assessments rather than traditional benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for GenAI do not accurately represent real-world performance, creating a disconnect between lab results and practical applications.

Method: The paper introduces a comprehensive evaluation framework emphasizing diverse and evolving inputs, alongside holistic and continuous assessment approaches.

Result: The framework provides guidelines for designing effective evaluation methods and recommendations for policymakers that prioritize societal impacts over fixed metrics.

Conclusion: By implementing this framework, GenAI models can achieve technical proficiency while ensuring ethical responsibility and positive societal impact.

Abstract: Generative AI (GenAI) models have become vital across industries, yet current
evaluation methods have not adapted to their widespread use. Traditional
evaluations often rely on benchmarks and fixed datasets, frequently failing to
reflect real-world performance, which creates a gap between lab-tested outcomes
and practical applications. This white paper proposes a comprehensive framework
for how we should evaluate real-world GenAI systems, emphasizing diverse,
evolving inputs and holistic, dynamic, and ongoing assessment approaches. The
paper offers guidance for practitioners on how to design evaluation methods
that accurately reflect real-time capabilities, and provides policymakers with
recommendations for crafting GenAI policies focused on societal impacts, rather
than fixed performance numbers or parameter sizes. We advocate for holistic
frameworks that integrate performance, fairness, and ethics and the use of
continuous, outcome-oriented methods that combine human and automated
assessments while also being transparent to foster trust among stakeholders.
Implementing these strategies ensures GenAI models are not only technically
proficient but also ethically responsible and impactful.

</details>


### [97] [MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores](https://arxiv.org/abs/2504.16786)
*Fengwei Zhou,Jiafei Song,Wenjin Jason Li,Gengjian Xue,Zhikang Zhao,Yichao Lu,Bailin Na*

Main category: cs.CL

TL;DR: The paper presents MOOSComp, a method for compressing long-context inputs in large language models, which improves performance and efficiency in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: There is a need to enhance the practical application of large language models in processing long-context inputs while minimizing inference time and resource consumption in constrained environments.

Method: MOOSComp employs a token-classification approach to compress long-context data, enhancing a BERT-based compressor by addressing the over-smoothing problem. It utilizes an inter-class cosine similarity loss during training and incorporates outlier scores during compression to preserve critical tokens, thereby improving classification accuracy and generalizability.

Result: The proposed method demonstrates superior performance across different compression ratios on long-context understanding and reasoning benchmarks, achieving a 3.3x speedup at a 4x compression ratio on mobile devices.

Conclusion: MOOSComp effectively improves the efficiency and performance of BERT-based compression in long-context processing, making it suitable for use in resource-constrained scenarios.

Abstract: Recent advances in large language models have significantly improved their
ability to process long-context input, but practical applications are
challenged by increased inference time and resource consumption, particularly
in resource-constrained environments. To address these challenges, we propose
MOOSComp, a token-classification-based long-context compression method that
enhances the performance of a BERT-based compressor by mitigating the
over-smoothing problem and incorporating outlier scores. In the training phase,
we add an inter-class cosine similarity loss term to penalize excessively
similar token representations, thereby improving the token classification
accuracy. During the compression phase, we introduce outlier scores to preserve
rare but critical tokens that are prone to be discarded in task-agnostic
compression. These scores are integrated with the classifier's output, making
the compressor more generalizable to various tasks. Superior performance is
achieved at various compression ratios on long-context understanding and
reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x
compression ratio on a resource-constrained mobile device.

</details>


### [98] [Credible plan-driven RAG method for Multi-hop Question Answering](https://arxiv.org/abs/2504.16787)
*Ningning Zhang,Chi Zhang,Zhizhong Tan,Xingxing Yang,Weiping Deng,Wenyong Wang*

Main category: cs.CL

TL;DR: The PAR RAG framework improves multi-hop question answering in Retrieval-Augmented Generation by effectively planning, acting, and reviewing to mitigate error propagation and enhance accuracy.


<details>
  <summary>Details</summary>
Motivation: Multi-hop question answering is difficult due to error propagation in reasoning paths and intermediate results in current RAG methods, leading to decreased answer accuracy.

Method: The proposed framework consists of three stages: planning, where a comprehensive and structured problem decomposition is conducted; acting, which involves executing the plan with multi-granularity verification; and reviewing to adjust intermediate results based on thorough checks.

Result: Experimental results indicate that the PAR RAG framework significantly surpasses existing methods in multi-hop QA, achieving better EM and F1 scores.

Conclusion: The PAR RAG framework provides an interpretable and incremental approach to multi-hop question answering, effectively reducing error propagation and enhancing reliability.

Abstract: Multi-hop question answering (QA) presents a considerable challenge for
Retrieval-Augmented Generation (RAG), requiring the structured decomposition of
complex queries into logical reasoning paths and the generation of dependable
intermediate results. However, deviations in reasoning paths or errors in
intermediate results, which are common in current RAG methods, may propagate
and accumulate throughout the reasoning process, diminishing the accuracy of
the answer to complex queries. To address this challenge, we propose the
Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key
stages: planning, act, and review, and aims to offer an interpretable and
incremental reasoning paradigm for accurate and reliable multi-hop question
answering by mitigating error propagation.PAR RAG initially applies a top-down
problem decomposition strategy, formulating a comprehensive plan that
integrates multiple executable steps from a holistic viewpoint. This approach
avoids the pitfalls of local optima common in traditional RAG methods, ensuring
the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a
plan execution mechanism based on multi-granularity verification. By utilizing
both coarse-grained similarity information and fine-grained relevant data, the
framework thoroughly checks and adjusts intermediate results, ensuring process
accuracy while effectively managing error propagation and amplification.
Experimental results on multi-hop QA datasets demonstrate that the PAR RAG
framework substantially outperforms existing state-of-the-art methods in key
metrics, including EM and F1 scores.

</details>


### [99] [Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention](https://arxiv.org/abs/2504.16795)
*Xiang Hu,Jiaqi Leng,Jun Zhao,Kewei Tu,Wei Wu*

Main category: cs.CL

TL;DR: HSA is a novel attention mechanism that enhances RNNs by enabling long-range random access while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: RNNs lack the ability to randomly access historical context, and existing attention mechanisms may compromise their efficiency.

Method: HSA divides inputs into chunks, selects the top-k chunks, and learns token-to-chunk relevance to enhance chunk selection precision.

Result: RAMba, combining HSA with Mamba, achieves perfect accuracy in passkey retrieval across 64 million contexts, with significant improvements on downstream tasks and a nearly constant memory footprint.

Conclusion: HSA shows significant promise for enhancing long-context modeling capabilities of RNNs.

Abstract: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is
their linear computational and space complexity enables faster training and
inference for long sequences. However, RNNs are fundamentally unable to
randomly access historical context, and simply integrating attention mechanisms
may undermine their efficiency advantages. To overcome this limitation, we
propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel
attention mechanism that enhances RNNs with long-range random access
flexibility while preserving their merits in efficiency and length
generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks
and hierarchically aggregates information. The core innovation lies in learning
token-to-chunk relevance based on fine-grained token-level information inside
each chunk. This approach enhances the precision of chunk selection across both
in-domain and out-of-domain context lengths. To make HSA efficient, we further
introduce a hardware-aligned kernel design. By combining HSA with Mamba, we
introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64
million contexts despite pre-training on only 4K-length contexts, and
significant improvements on various downstream tasks, with nearly constant
memory footprint. These results show RAMba's huge potential in long-context
modeling.

</details>


### [100] [LLM-assisted Graph-RAG Information Extraction from IFC Data](https://arxiv.org/abs/2504.16813)
*Sima Iranmanesh,Hadeel Saadany,Edlira Vakaj*

Main category: cs.CL

TL;DR: The paper presents a method using LLMs and Graph-RAG to improve retrieval of building object properties from complex IFC data.


<details>
  <summary>Details</summary>
Motivation: The complexity of IFC data representation in the construction industry necessitates improved ways to parse and retrieve building information.

Method: Utilization of LLMs in conjunction with Graph Retrieval-Augmented Generation (Graph-RAG) to access and interpret IFC data properties and relationships.

Result: The Graph-RAG parsing enhances the capabilities of generative LLMs by facilitating natural language query-response retrieval from IFC data.

Conclusion: This approach demonstrates that it is possible to simplify interactions with complex IFC data while bypassing the need for intricate processing pipelines.

Abstract: IFC data has become the general building information standard for
collaborative work in the construction industry. However, IFC data can be very
complicated because it allows for multiple ways to represent the same product
information. In this research, we utilise the capabilities of LLMs to parse the
IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to
retrieve building object properties and their relations. We will show that,
despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG
parsing enhances generative LLMs like GPT-4o with graph-based knowledge,
enabling natural language query-response retrieval without the need for a
complex pipeline.

</details>


### [101] [GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning](https://arxiv.org/abs/2504.16832)
*Luu Quy Tung,Hoang Quoc Viet,Vo Trong Thu*

Main category: cs.CL

TL;DR: The paper introduces GreenMind-Medium-14B-R1, a Vietnamese reasoning model that improves LLM tasks requiring intermediate reasoning through innovative finetuning and reward strategies.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing models regarding reasoning in Vietnamese, focusing on language mixing and factual correctness in generated outputs.

Method: The model utilizes Group Relative Policy Optimization, a high-quality synthesized Vietnamese reasoning dataset, and two reward functions to enhance reasoning processes.

Result: Experimental results show that the model outperforms previous works on the Vietnamese VLSP 2023 dataset and maintains increased linguistic consistency in responses.

Conclusion: The reasoning method proves effective over few-shot prompting techniques, demonstrating its applicability in multilingual contexts.

Abstract: Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that
require intermediate reasoning steps prior to generating a final answer. In
this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model
inspired by the finetuning strategy based on Group Relative Policy
Optimization. We also leverage a high-quality Vietnamese synthesized reasoning
dataset and design two reward functions to tackle the main limitations of this
technique: (i) language mixing, where we explicitly detect the presence of
biased language characters during the process of sampling tokens, and (ii) we
leverage Sentence Transformer-based models to ensure that the generated
reasoning content maintains factual correctness and does not distort the final
output. Experimental results on the Vietnamese dataset from the VLSP 2023
Challenge demonstrate that our model outperforms prior works and enhances
linguistic consistency in its responses. Furthermore, we extend our evaluation
to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of
our reasoning method compared to few-shot prompting techniques.

</details>


### [102] [Monte Carlo Planning with Large Language Model for Text-Based Game Agents](https://arxiv.org/abs/2504.16855)
*Zijing Shi,Meng Fang,Ling Chen*

Main category: cs.CL

TL;DR: The MC-DML algorithm combines Monte Carlo planning with enhanced language capabilities of Large Language Models for efficient performance in text-based games.


<details>
  <summary>Details</summary>
Motivation: Planning-then-learning paradigms like MCTS and RL are time-consuming and lack language understanding, necessitating a more efficient approach.

Method: The MC-DML algorithm integrates LLMs with dynamic memory mechanisms to improve exploration and action evaluation during planning.

Result: Experiments show that MC-DML significantly enhances performance in initial planning phases across various text-based games compared to existing methods.

Conclusion: MC-DML demonstrates the potential for improved efficiency in language-grounded planning within complex environments.

Abstract: Text-based games provide valuable environments for language-based autonomous
agents. However, planning-then-learning paradigms, such as those combining
Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably
time-consuming due to extensive iterations. Additionally, these algorithms
perform uncertainty-driven exploration but lack language understanding and
reasoning abilities. In this paper, we introduce the Monte Carlo planning with
Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages
the language understanding and reasoning capabilities of Large Language Models
(LLMs) alongside the exploratory advantages of tree search algorithms.
Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,
enabling them to learn from past experiences and dynamically adjust action
evaluations during planning. We conduct experiments on a series of text-based
games from the Jericho benchmark. Our results demonstrate that the MC-DML
algorithm significantly enhances performance across various games at the
initial planning phase, outperforming strong contemporary methods that require
multiple iterations. This demonstrates the effectiveness of our algorithm,
paving the way for more efficient language-grounded planning in complex
environments.

</details>


### [103] [Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification](https://arxiv.org/abs/2504.16856)
*Alexander Shvets*

Main category: cs.CL

TL;DR: The paper presents a new LLM-based data synthesis pipeline to generate a diversified dataset for sentiment analysis, addressing context and emotion category limitations in existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing sentiment analysis datasets lack contextual information and are limited in emotion categories, leading to challenges in emotion understanding and model performance.

Method: An LLM-based data synthesis pipeline using Mistral-7b to generate 100K contextual and 300K context-less training examples, focusing on story-character-centered utterances across 28 emotion classes, with extensive computational resources.

Result: The Emo Pillars models fine-tuned on the new dataset achieve state-of-the-art performance on tasks such as GoEmotions, ISEAR, and IEMOCAP, demonstrating adaptability to new domains and effective utterance diversification.

Conclusion: The proposed dataset enhances context and emotion representation in sentiment analysis, although there's a need for better handling of out-of-taxonomy labels.

Abstract: Most datasets for sentiment analysis lack context in which an opinion was
expressed, often crucial for emotion understanding, and are mainly limited by a
few emotion categories. Foundation large language models (LLMs) like GPT-4
suffer from over-predicting emotions and are too resource-intensive. We design
an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b,
for the generation of training examples for more accessible, lightweight
BERT-type encoder models. We focus on enlarging the semantic diversity of
examples and propose grounding the generation into a corpus of narratives to
produce non-repetitive story-character-centered utterances with unique contexts
over 28 emotion classes. By running 700K inferences in 450 GPU hours, we
contribute with the dataset of 100K contextual and also 300K context-less
examples to cover both scenarios. We use it for fine-tuning pre-trained
encoders, which results in several Emo Pillars models. We show that Emo Pillars
models are highly adaptive to new domains when tuned to specific tasks such as
GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on
the first three. We also validate our dataset, conducting statistical analysis
and human evaluation, and confirm the success of our measures in utterance
diversification (although less for the neutral class) and context
personalization, while pointing out the need for improved handling of
out-of-taxonomy labels within the pipeline.

</details>


### [104] [Planning with Diffusion Models for Target-Oriented Dialogue Systems](https://arxiv.org/abs/2504.16858)
*Hanwen Du,Bo Peng,Xia Ning*

Main category: cs.CL

TL;DR: DiffTOD is a novel dialogue planning framework that employs diffusion models for non-sequential target-oriented dialogue, improving flexibility and optimization over long dialogue horizons.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing dialogue planning methods, which are sequential and prone to errors, by enabling strategic, non-sequential dialogue planning.

Method: DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, utilizing a diffusion language model for estimating dialogue trajectory likelihood and introducing tailored guidance mechanisms for various target types.

Result: DiffTOD shows effective non-myopic lookahead exploration and optimizes action strategies across diverse target-oriented dialogue settings through non-sequential planning, demonstrating strong performance in complex dialogues.

Conclusion: DiffTOD offers a significant advance in target-oriented dialogue planning, providing a more flexible and effective solution compared to traditional methods.

Abstract: Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM
era, where strategic dialogue planning is crucial for directing conversations
toward specific targets. However, existing dialogue planning methods generate
dialogue plans in a step-by-step sequential manner, and may suffer from
compounding errors and myopic actions. To address these limitations, we
introduce a novel dialogue planning framework, DiffTOD, which leverages
diffusion models to enable non-sequential dialogue planning. DiffTOD formulates
dialogue planning as a trajectory generation problem with conditional guidance,
and leverages a diffusion language model to estimate the likelihood of the
dialogue trajectory. To optimize the dialogue action strategies, DiffTOD
introduces three tailored guidance mechanisms for different target types,
offering flexible guidance towards diverse TOD targets at test time. Extensive
experiments across three diverse TOD settings show that DiffTOD can effectively
perform non-myopic lookahead exploration and optimize action strategies over a
long horizon through non-sequential dialogue planning, and demonstrates strong
flexibility across complex and diverse dialogue scenarios. Our code and data
are accessible through https://anonymous.4open.science/r/DiffTOD.

</details>


### [105] [Do Large Language Models know who did what to whom?](https://arxiv.org/abs/2504.16884)
*Joseph M. Denning,Xiaohan,Guo,Bryor Snefjella,Idan A. Blank*

Main category: cs.CL

TL;DR: This study investigates whether Large Language Models (LLMs) can infer thematic roles in sentences, concluding that while they can capture thematic roles, their representations rely more on syntax than on thematic assignment.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critique that LLMs do not understand language by focusing on the specific aspect of thematic roles, which are crucial for understanding who did what in sentences.

Method: The authors conducted two experiments to evaluate the sentence representations of four LLMs, comparing their representational similarities based on syntactic and thematic aspects.

Result: The findings reveal that LLMs show representational similarity influenced more by syntactic similarity than by thematic role assignments, with minimal evidence of thematic role information across their hidden units, although some attention heads did capture thematic roles.

Conclusion: Overall, LLMs can extract thematic roles, but their influence on sentence representations is considerably weaker compared to human processing.

Abstract: Large Language Models (LLMs) are commonly criticized for not understanding
language. However, many critiques focus on cognitive abilities that, in humans,
are distinct from language processing. Here, we instead study a kind of
understanding tightly linked to language: inferring who did what to whom
(thematic roles) in a sentence. Does the central training objective of
LLMs-word prediction-result in sentence representations that capture thematic
roles? In two experiments, we characterized sentence representations in four
LLMs. In contrast to human similarity judgments, in LLMs the overall
representational similarity of sentence pairs reflected syntactic similarity
but not whether their agent and patient assignments were identical vs.
reversed. Furthermore, we found little evidence that thematic role information
was available in any subset of hidden units. However, some attention heads
robustly captured thematic roles, independently of syntax. Therefore, LLMs can
extract thematic roles but, relative to humans, this information influences
their representations more weakly.

</details>


### [106] [Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text](https://arxiv.org/abs/2504.16913)
*Shifali Agrahari,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: The paper introduces COT Fine-tuned, an innovative framework for detecting AI-generated text and identifying the specific language model responsible, utilizing a dual-task approach and Chain-of-Thought reasoning for enhanced interpretability.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated text poses risks to academic integrity, misinformation, and the ethical use of AI, necessitating robust detection methods.

Method: A dual-task approach is employed: Task A classifies text as either AI-generated or human-written, while Task B identifies the specific language model producing the text. The method integrates Chain-of-Thought reasoning for generating explanations of predictions.

Result: COT Fine-tuned demonstrates high accuracy in both tasks, effectively identifying language models and classifying human vs. AI-generated text, with significant contributions from the CoT reasoning process.

Conclusion: The study confirms that Chain-of-Thought reasoning enhances the model's effectiveness and interpretability, making it a valuable tool for text detection.

Abstract: In recent years, the detection of AI-generated text has become a critical
area of research due to concerns about academic integrity, misinformation, and
ethical AI deployment. This paper presents COT Fine-tuned, a novel framework
for detecting AI-generated text and identifying the specific language model.
responsible for generating the text. We propose a dual-task approach, where
Task A involves classifying text as AI-generated or human-written, and Task B
identifies the specific LLM behind the text. The key innovation of our method
lies in the use of Chain-of-Thought reasoning, which enables the model to
generate explanations for its predictions, enhancing transparency and
interpretability. Our experiments demonstrate that COT Fine-tuned achieves high
accuracy in both tasks, with strong performance in LLM identification and
human-AI classification. We also show that the CoT reasoning process
contributes significantly to the models effectiveness and interpretability.

</details>


### [107] [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)
*Raghav Thind,Youran Sun,Ling Liang,Haizhao Yang*

Main category: cs.CL

TL;DR: OptimAI is a framework that transforms natural language optimization problems into mathematical formats using AI agents, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of translating natural language optimization problems into mathematical formulations and the need for a suitable solver demands significant expertise, which OptimAI aims to address.

Method: The framework consists of four roles: a formulator for translations, a planner for solution strategies, a coder for operational interactions, and a code critic for outcome reflections. It includes UCB-based debug scheduling for dynamic plan adjustments.

Result: OptimAI achieves an accuracy of 88.1% on the NLP4LP dataset and 71.2% on the Optibench subset, with significant reductions in error rates: 58% and 50% respectively compared to previous best results.

Conclusion: The implementation of multi-agent collaboration within OptimAI allows the effective combination of models, enhancing productivity and accuracy in solving optimization problems from natural language descriptions.

Abstract: Optimization plays a vital role in scientific research and practical
applications, but formulating a concrete optimization problem described in
natural language into a mathematical form and selecting a suitable solver to
solve the problem requires substantial domain expertise. We introduce
\textbf{OptimAI}, a framework for solving \underline{Optim}ization problems
described in natural language by leveraging LLM-powered \underline{AI} agents,
achieving superior performance over current state-of-the-art methods. Our
framework is built upon four key roles: (1) a \emph{formulator} that translates
natural language problem descriptions into precise mathematical formulations;
(2) a \emph{planner} that constructs a high-level solution strategy prior to
execution; and (3) a \emph{coder} and a \emph{code critic} capable of
interacting with the environment and reflecting on outcomes to refine future
actions. Ablation studies confirm that all roles are essential; removing the
planner or code critic results in $5.8\times$ and $3.1\times$ drops in
productivity, respectively. Furthermore, we introduce UCB-based debug
scheduling to dynamically switch between alternative plans, yielding an
additional $3.3\times$ productivity gain. Our design emphasizes multi-agent
collaboration, allowing us to conveniently explore the synergistic effect of
combining diverse models within a unified system. Our approach attains 88.1\%
accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o
table) subset, reducing error rates by 58\% and 50\% respectively over prior
best results.

</details>


### [108] [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921)
*José Ángel González,Ian Borrego Obrador,Álvaro Romo Herrero,Areg Mikael Sarvazyan,Mara Chinea-Ríos,Angelo Basile,Marc Franco-Salvador*

Main category: cs.CL

TL;DR: IberBench is a new benchmark for evaluating large language models (LLMs) on diverse NLP tasks in multiple languages from the Iberian Peninsula and Ibero-America, addressing shortcomings of existing English-centric benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive evaluation framework for LLMs that focuses on linguistic diversity and industrial relevance, moving beyond the limitations of existing benchmarks dominated by English.

Method: IberBench integrates 101 datasets across 22 task categories and allows for continual updates and community participation, enabling evaluations on both fundamental and industry-relevant NLP tasks.

Result: Empirical evaluation of 23 LLMs showed poorer performance on industry-relevant tasks and lower scores for Galician and Basque. Some task performances were close to random, while others were above random but still below established shared task systems.

Conclusion: IberBench facilitates better evaluation of LLMs in diverse languages, providing open-source tools and a publicly accessible leaderboard to improve the assessment process.

Abstract: Large Language Models (LLMs) remain difficult to evaluate comprehensively,
particularly for languages other than English, where high-quality data is often
limited. Existing benchmarks and leaderboards are predominantly
English-centric, with only a few addressing other languages. These benchmarks
fall short in several key areas: they overlook the diversity of language
varieties, prioritize fundamental Natural Language Processing (NLP)
capabilities over tasks of industrial relevance, and are static. With these
aspects in mind, we present IberBench, a comprehensive and extensible benchmark
designed to assess LLM performance on both fundamental and industry-relevant
NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.
IberBench integrates 101 datasets from evaluation campaigns and recent
benchmarks, covering 22 task categories such as sentiment and emotion analysis,
toxicity detection, and summarization. The benchmark addresses key limitations
in current evaluation practices, such as the lack of linguistic diversity and
static evaluation setups by enabling continual updates and community-driven
model and dataset submissions moderated by a committee of experts. We evaluate
23 LLMs ranging from 100 million to 14 billion parameters and provide empirical
insights into their strengths and limitations. Our findings indicate that (i)
LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)
performance is on average lower for Galician and Basque, (iii) some tasks show
results close to random, and (iv) in other tasks LLMs perform above random but
below shared task systems. IberBench offers open-source implementations for the
entire evaluation pipeline, including dataset normalization and hosting,
incremental evaluation of LLMs, and a publicly accessible leaderboard.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [109] [Common Functional Decompositions Can Mis-attribute Differences in Outcomes Between Populations](https://arxiv.org/abs/2504.16864)
*Manuel Quintero,William T. Stephenson,Advik Shreekumar,Tamara Broderick*

Main category: stat.ME

TL;DR: The paper critiques the Kitagawa-Oaxaca-Blinder decomposition in econometrics, revealing that common nonlinear extensions can misattribute outcome differences between populations even when their covariates are identical.


<details>
  <summary>Details</summary>
Motivation: To understand the source of differential outcomes between two populations and improve upon the limitations of the KOB decomposition in capturing nonlinear relationships.

Method: The authors analyze the performance of common nonlinear functional decompositions, specifically functional ANOVA and Accumulated Local Effects, in attributing differences in outcomes.

Result: They found that these methods can erroneously attribute outcome differences to covariates even when those covariates are the same across populations, leading to misattribution.

Conclusion: The study concludes that using decompositions independent of input distribution can prevent misattribution, and conjectures that any reasonable additive decomposition influenced by covariate distribution is likely to misattribute outcomes.

Abstract: In science and social science, we often wish to explain why an outcome is
different in two populations. For instance, if a jobs program benefits members
of one city more than another, is that due to differences in program
participants (particular covariates) or the local labor markets (outcomes given
covariates)? The Kitagawa-Oaxaca-Blinder (KOB) decomposition is a standard tool
in econometrics that explains the difference in the mean outcome across two
populations. However, the KOB decomposition assumes a linear relationship
between covariates and outcomes, while the true relationship may be
meaningfully nonlinear. Modern machine learning boasts a variety of nonlinear
functional decompositions for the relationship between outcomes and covariates
in one population. It seems natural to extend the KOB decomposition using these
functional decompositions. We observe that a successful extension should not
attribute the differences to covariates -- or, respectively, to outcomes given
covariates -- if those are the same in the two populations. Unfortunately, we
demonstrate that, even in simple examples, two common decompositions --
functional ANOVA and Accumulated Local Effects -- can attribute differences to
outcomes given covariates, even when they are identical in two populations. We
provide a characterization of when functional ANOVA misattributes, as well as a
general property that any discrete decomposition must satisfy to avoid
misattribution. We show that if the decomposition is independent of its input
distribution, it does not misattribute. We further conjecture that
misattribution arises in any reasonable additive decomposition that depends on
the distribution of the covariates.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [110] [HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing](https://arxiv.org/abs/2504.16112)
*Myunghyun Rhee,Joonseop Sim,Taeyoung Ahn,Seungyong Lee,Daegun Yoon,Euiseok Kim,Kyoung Park,Youngpyo Joo,Hosik Kim*

Main category: cs.AR

TL;DR: The paper introduces a High-bandwidth Processing Unit (HPU), a co-processor designed to improve GPU efficiency during LLM inference by offloading memory-bound operations, resulting in significant performance and energy efficiency gains.


<details>
  <summary>Details</summary>
Motivation: The attention layer in Transformer-based LLMs reveals inefficiencies in current GPU architectures, particularly due to low operational intensity and high memory demands.

Method: The paper proposes and implements a High-bandwidth Processing Unit (HPU) that functions as an add-on card to GPUs, specifically designed to enhance memory utilization during large-batched LLM inference using PCIe-based FPGA cards.

Result: The novel GPU-HPU heterogeneous system shows up to 4.1x performance gains and 4.6x improvements in energy efficiency over a traditional GPU-only system, allowing for scalability without the need for additional GPUs.

Conclusion: The HPU significantly improves the efficiency of GPU systems in handling large-batched LLM inference and addresses memory limitations effectively.

Abstract: The attention layer, a core component of Transformer-based LLMs, brings out
inefficiencies in current GPU systems due to its low operational intensity and
the substantial memory requirements of KV caches. We propose a High-bandwidth
Processing Unit (HPU), a memoryintensive co-processor that enhances GPU
resource utilization during large-batched LLM inference. By offloading
memory-bound operations, the HPU allows the GPU to focus on compute-intensive
tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales
out to accommodate surging memory demands driven by large batch sizes and
extended sequence lengths. In this paper, we show the HPU prototype implemented
with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU
heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy
efficiency improvements over a GPUonly system, providing scalability without
increasing the number of GPUs.

</details>


### [111] [TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs](https://arxiv.org/abs/2504.16266)
*Ye Qiao,Zhiheng Cheng,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TL;DR: TeLLMe is a novel ternary LLM accelerator designed for low-power FPGAs that effectively supports low-bit quantized language models, enhancing edge deployment efficiency.


<details>
  <summary>Details</summary>
Motivation: The deployment of large language models on edge platforms is limited by high computational and memory demands, alongside constraints like power budgets and prefill latency.

Method: TeLLMe utilizes a table-lookup matrix engine for ternary matrix multiplication, a bandwidth-efficient attention module with a reversed reordering scheme, and an integrated normalization and quantization unit optimized for ultra-low-bit inference.

Result: TeLLMe achieves up to 9 tokens/s throughput under a 7W power budget, with prefill latencies of 0.55--1.15 s for prompts of 64--128 tokens, demonstrating a significant advancement in energy efficiency for generative AI on edge FPGAs.

Conclusion: TeLLMe establishes a new benchmark for edge FPGA applications in generative AI, leveraging low-bit quantization techniques for enhanced performance.

Abstract: Deploying large language models (LLMs) on edge platforms is challenged by
their high computational and memory demands. Although recent low-bit
quantization methods (e.g., BitNet, DeepSeek) compress weights to as little as
1.58 bits with minimal accuracy loss, edge deployment is still constrained by
limited on-chip resources, power budgets, and the often-neglected latency of
the prefill phase. We present TeLLMe, the first ternary LLM accelerator for
low-power FPGAs (e.g., AMD KV260) that fully supports both prefill and
autoregressive decoding using 1.58-bit weights and 8-bit activations. Our
contributions include: (1) a table-lookup matrix engine for ternary matmul that
merges grouped activations with online precomputation to minimize resource use;
(2) a fused, bandwidth-efficient attention module featuring a reversed
reordering scheme to accelerate prefill; and (3) a tightly integrated
normalization and quantization--dequantization unit optimized for ultra-low-bit
inference. Under a 7W power budget, TeLLMe delivers up to 9 tokens/s throughput
over 1,024-token contexts and prefill latencies of 0.55--1.15 s for 64--128
token prompts, marking a significant energy-efficiency advance and establishing
a new edge FPGA benchmark for generative AI.

</details>


### [112] [COBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference](https://arxiv.org/abs/2504.16269)
*Ye Qiao,Zhiheng Cheng,Yian Wang,Yifan Zhang,Yunzhe Deng,Sitao Huang*

Main category: cs.AR

TL;DR: COBRA is a co-optimized binary Transformer accelerator designed for edge computing that improves energy efficiency and throughput compared to existing solutions.


<details>
  <summary>Details</summary>
Motivation: The need for efficient deployment of transformer-based models on edge platforms due to their large size and computational demands.

Method: COBRA utilizes a real 1-bit binary multiplication unit and implements hardware-friendly optimizations in the attention block to improve performance.

Result: COBRA achieves a throughput of up to 3,894.7 GOPS and energy efficiency of 448.7 GOPS/Watt on edge FPGAs, demonstrating a 311x energy efficiency improvement over GPUs and a 3.5x throughput improvement over the current state-of-the-art binary accelerator.

Conclusion: COBRA effectively enables the deployment of binary transformers on edge devices with only negligible inference accuracy degradation.

Abstract: Transformer-based models have demonstrated superior performance in various
fields, including natural language processing and computer vision. However,
their enormous model size and high demands in computation, memory, and
communication limit their deployment to edge platforms for local, secure
inference. Binary transformers offer a compact, low-complexity solution for
edge deployment with reduced bandwidth needs and acceptable accuracy. However,
existing binary transformers perform inefficiently on current hardware due to
the lack of binary specific optimizations. To address this, we introduce COBRA,
an algorithm-architecture co-optimized binary Transformer accelerator for edge
computing. COBRA features a real 1-bit binary multiplication unit, enabling
matrix operations with -1, 0, and +1 values, surpassing ternary methods. With
further hardware-friendly optimizations in the attention block, COBRA achieves
up to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge
FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x
throughput improvement over the state-of-the-art binary accelerator, with only
negligible inference accuracy degradation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [113] [Introduction to Quantum Machine Learning and Quantum Architecture Search](https://arxiv.org/abs/2504.16131)
*Samuel Yen-Chi Chen,Zhiding Liang*

Main category: quant-ph

TL;DR: The paper discusses the integration of quantum computing and machine learning through quantum machine learning (QML), highlighting recent advancements and their applications.


<details>
  <summary>Details</summary>
Motivation: The integration of quantum computing and machine learning aims to improve ML algorithms through quantum principles and to enable researchers to design high-performance quantum circuit architectures more effectively.

Method: The paper reviews recent advancements in quantum machine learning and systematic approaches for designing quantum circuit architectures for QML tasks.

Result: It highlights breakthroughs in QML that can expand its application across various fields, making quantum-enhanced tools more accessible to researchers.

Conclusion: The tutorial emphasizes the transformative potential of QML and the importance of systematic approaches in harnessing quantum computing for machine learning.

Abstract: Recent advancements in quantum computing (QC) and machine learning (ML) have
fueled significant research efforts aimed at integrating these two
transformative technologies. Quantum machine learning (QML), an emerging
interdisciplinary field, leverages quantum principles to enhance the
performance of ML algorithms. Concurrently, the exploration of systematic and
automated approaches for designing high-performance quantum circuit
architectures for QML tasks has gained prominence, as these methods empower
researchers outside the quantum computing domain to effectively utilize
quantum-enhanced tools. This tutorial will provide an in-depth overview of
recent breakthroughs in both areas, highlighting their potential to expand the
application landscape of QML across diverse fields.

</details>


### [114] [Deep Neural Network Emulation of the Quantum-Classical Transition via Learned Wigner Function Dynamics](https://arxiv.org/abs/2504.16334)
*Kamran Majid*

Main category: quant-ph

TL;DR: This paper presents a novel method using deep neural networks to learn the dynamics of quantum-classical transition as Planck's constant approaches zero by predicting the time evolution of the Wigner function.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding the emergence of classical behavior from quantum mechanics as Planck's constant approaches zero.

Method: A comprehensive dataset of time-evolved Wigner functions was generated, and a deep feedforward neural network with an enhanced architecture was trained to predict the mapping from initial quantum state parameters and h-bar to the Wigner function parameters.

Result: The trained neural network achieved a final training loss of ~ 0.0390 and demonstrated a significant ability to accurately capture the mapping of the Wigner function dynamics, enabling predictions of phase-space distributions as h-bar varies.

Conclusion: This approach provides a new computational perspective on the emergence of classicality and extends previous research by focusing directly on phase-space representation.

Abstract: The emergence of classical behavior from quantum mechanics as Planck's
constant $\hbar$ approaches zero remains a fundamental challenge in physics
[1-3]. This paper introduces a novel approach employing deep neural networks to
directly learn the dynamical mapping from initial quantum state parameters (for
Gaussian wave packets of the one-dimensional harmonic oscillator) and $\hbar$
to the parameters of the time-evolved Wigner function in phase space [4-6]. A
comprehensive dataset of analytically derived time-evolved Wigner functions was
generated, and a deep feedforward neural network with an enhanced architecture
was successfully trained for this prediction task, achieving a final training
loss of ~ 0.0390. The network demonstrates a significant and previously
unrealized ability to accurately capture the underlying mapping of the Wigner
function dynamics. This allows for a direct emulation of the quantum-classical
transition by predicting the evolution of phase-space distributions as $\hbar$
is systematically varied. The implications of these findings for providing a
new computational lens on the emergence of classicality are discussed,
highlighting the potential of this direct phase-space learning approach for
studying fundamental aspects of quantum mechanics. This work presents a
significant advancement beyond previous efforts that focused on learning
observable mappings [7], offering a direct route via the phase-space
representation.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [115] [Heterogeneous networks in drug-target interaction prediction](https://arxiv.org/abs/2504.16152)
*Mohammad Molaee,Nasrollah Moghadam Charkari*

Main category: q-bio.BM

TL;DR: This survey reviews graph machine learning methods for predicting drug-target interactions to enhance drug discovery efficiency.


<details>
  <summary>Details</summary>
Motivation: To reduce the time and cost associated with drug discovery by improving drug-target interaction predictions.

Method: The survey analyzes recent graph machine learning-based methods, detailing their frameworks, contributions, datasets, and performance measurements.

Result: The review synthesizes findings from papers published between 2020 and 2024, highlighting successful applications of graph machine learning in drug-target prediction.

Conclusion: The paper outlines future challenges and areas for further research in the field of drug-target interaction prediction.

Abstract: Drug discovery requires a tremendous amount of time and cost. Computational
drug-target interaction prediction, a significant part of this process, can
reduce these requirements by narrowing the search space for wet lab
experiments. In this survey, we provide comprehensive details of graph machine
learning-based methods in predicting drug-target interaction, as they have
shown promising results in this field. These details include the overall
framework, main contribution, datasets, and their source codes. The selected
papers were mainly published from 2020 to 2024. Prior to discussing papers, we
briefly introduce the datasets commonly used with these methods and
measurements to assess their performance. Finally, future challenges and some
crucial areas that need to be explored are discussed.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [116] [A CNN-based Local-Global Self-Attention via Averaged Window Embeddings for Hierarchical ECG Analysis](https://arxiv.org/abs/2504.16097)
*Arthur Buzelin,Pedro Robles Dutenhefner,Turi Rezende,Luisa G. Porfirio,Pedro Bento,Yan Aquino,Jose Fernandes,Caio Santana,Gabriela Miana,Gisele L. Pappa,Antonio Ribeiro,Wagner Meira Jr*

Main category: eess.SP

TL;DR: The paper presents a novel Local-Global Attention ECG model (LGA-ECG) that improves ECG analysis by integrating local morphological feature extraction with global rhythm pattern modeling through deep learning techniques.


<details>
  <summary>Details</summary>
Motivation: Address the need for efficient diagnostic tools for cardiovascular diseases, specifically improving ECG analysis which is critical for accurate interpretation.

Method: The LGA-ECG model combines convolutional inductive biases with global self-attention mechanisms, extracting queries from overlapping convolutional windows for fine-grained analysis while modeling global context.

Result: Experiments on the CODE-15 dataset show that LGA-ECG outperforms existing state-of-the-art models, with ablation studies confirming the effectiveness of the local-global attention strategy.

Conclusion: The LGA-ECG model effectively captures hierarchical temporal dependencies and morphological patterns, showing promise for robust automated ECG classification in clinical settings.

Abstract: Cardiovascular diseases remain the leading cause of global mortality,
emphasizing the critical need for efficient diagnostic tools such as
electrocardiograms (ECGs). Recent advancements in deep learning, particularly
transformers, have revolutionized ECG analysis by capturing detailed waveform
features as well as global rhythm patterns. However, traditional transformers
struggle to effectively capture local morphological features that are critical
for accurate ECG interpretation. We propose a novel Local-Global Attention ECG
model (LGA-ECG) to address this limitation, integrating convolutional inductive
biases with global self-attention mechanisms. Our approach extracts queries by
averaging embeddings obtained from overlapping convolutional windows, enabling
fine-grained morphological analysis, while simultaneously modeling global
context through attention to keys and values derived from the entire sequence.
Experiments conducted on the CODE-15 dataset demonstrate that LGA-ECG
outperforms state-of-the-art models and ablation studies validate the
effectiveness of the local-global attention strategy. By capturing the
hierarchical temporal dependencies and morphological patterns in ECG signals,
this new design showcases its potential for clinical deployment with robust
automated ECG classification.

</details>


### [117] [SeizureFormer: A Transformer Model for IEA-Based Seizure Risk Forecasting](https://arxiv.org/abs/2504.16098)
*Tianning Feng,Junting Ni,Ezequiel Gleichgerrcht,Wei Jin*

Main category: eess.SP

TL;DR: SeizureFormer is a Transformer-based model designed for long-term seizure risk forecasting using clinically relevant biomarkers, achieving state-of-the-art performance in predictions.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve seizure risk forecasting in epilepsy patients using clinically relevant features that enhance prediction accuracy compared to traditional ECG-based models.

Method: The model integrates CNN-based patch embedding, multi-head self-attention, and squeeze-and-excitation blocks to capture both short-term dynamics and long-term seizure cycles, tested on data from five patients across various prediction windows.

Result: SeizureFormer achieved mean ROC AUC of 79.44% and mean PR AUC of 76.29%, outperforming statistical, machine learning, and deep learning baselines, particularly under class imbalance conditions.

Conclusion: The findings suggest that SeizureFormer can be effectively integrated into clinical practice, offering interpretable and robust tools for personalized epilepsy management.

Abstract: We present SeizureFormer, a Transformer-based model for long-term seizure
risk forecasting using interictal epileptiform activity (IEA) surrogate
biomarkers and long episode (LE) biomarkers from responsive neurostimulation
(RNS) systems. Unlike raw scalp EEG-based models, SeizureFormer leverages
structured, clinically relevant features and integrates CNN-based patch
embedding, multi-head self-attention, and squeeze-and-excitation blocks to
model both short-term dynamics and long-term seizure cycles. Tested across five
patients and multiple prediction windows (1 to 14 days), SeizureFormer achieved
state-of-the-art performance with mean ROC AUC of 79.44 percent and mean PR AUC
of 76.29 percent. Compared to statistical, machine learning, and deep learning
baselines, it demonstrates enhanced generalizability and seizure risk
forecasting performance under class imbalance. This work supports future
clinical integration of interpretable and robust seizure forecasting tools for
personalized epilepsy management.

</details>


### [118] [Towards Accurate Forecasting of Renewable Energy : Building Datasets and Benchmarking Machine Learning Models for Solar and Wind Power in France](https://arxiv.org/abs/2504.16100)
*Eloi Lindas,Yannig Goude,Philippe Ciais*

Main category: eess.SP

TL;DR: This study develops a machine learning methodology to predict solar and wind power production in France using spatially explicit weather data and site-specific information.


<details>
  <summary>Details</summary>
Motivation: Accurate predictions of non-dispatchable renewables are critical for grid stability and price forecasting, yet current approaches are limited in their spatial resolution and integration of data.

Method: The study employs machine learning models trained on a dataset combining daily power production, weather data, site capacity, and electricity prices, and explores modeling techniques including spatial averaging, PCA, and computer vision.

Result: Neural networks outperform traditional models, yielding nRMSE errors between 4% and 10% for midterm forecasts, comparable to local models at single-plant levels.

Conclusion: The proposed machine learning approaches demonstrate significant potential for enhancing regional power supply forecasting capabilities.

Abstract: Accurate prediction of non-dispatchable renewable energy sources is essential
for grid stability and price prediction. Regional power supply forecasts are
usually indirect through a bottom-up approach of plant-level forecasts,
incorporate lagged power values, and do not use the potential of spatially
resolved data. This study presents a comprehensive methodology for predicting
solar and wind power production at country scale in France using machine
learning models trained with spatially explicit weather data combined with
spatial information about production sites capacity. A dataset is built
spanning from 2012 to 2023, using daily power production data from RTE (the
national grid operator) as the target variable, with daily weather data from
ERA5, production sites capacity and location, and electricity prices as input
features. Three modeling approaches are explored to handle spatially resolved
weather data: spatial averaging over the country, dimension reduction through
principal component analysis, and a computer vision architecture to exploit
complex spatial relationships. The study benchmarks state-of-the-art machine
learning models as well as hyperparameter tuning approaches based on
cross-validation methods on daily power production data. Results indicate that
cross-validation tailored to time series is best suited to reach low error. We
found that neural networks tend to outperform traditional tree-based models,
which face challenges in extrapolation due to the increasing renewable capacity
over time. Model performance ranges from 4% to 10% in nRMSE for midterm
horizon, achieving similar error metrics to local models established at a
single-plant level, highlighting the potential of these methods for regional
power supply forecasting.

</details>


### [119] [xLSTM-ECG: Multi-label ECG Classification via Feature Fusion with xLSTM](https://arxiv.org/abs/2504.16101)
*Lei Kang,Xuanshuo Fu,Javier Vazquez-Corral,Ernest Valveny,Dimosthenis Karatzas*

Main category: eess.SP

TL;DR: The paper introduces xLSTM-ECG, a novel xLSTM network for efficient multi-label classification of ECG signals, demonstrating improved accuracy in diagnosing cardiovascular diseases.


<details>
  <summary>Details</summary>
Motivation: Cardiovascular diseases are a leading cause of mortality, necessitating the development of efficient and accurate diagnostic tools, particularly for ECG interpretation.

Method: The study employs an extended Long Short-Term Memory (xLSTM) network and utilizes Short-Time Fourier Transform (STFT) to convert ECG waveforms into the frequency domain for enhanced feature extraction and multi-label classification.

Result: Experiments on the PTB-XL dataset show that the xLSTM-ECG model achieves strong multi-label classification performance, with additional tests confirming its robustness on the Georgia 12-Lead dataset.

Conclusion: The proposed xLSTM-ECG significantly improves ECG classification accuracy, contributing to advancements in clinical diagnostics and patient care.

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality
worldwide, highlighting the critical need for efficient and accurate diagnostic
tools. Electrocardiograms (ECGs) are indispensable in diagnosing various heart
conditions; however, their manual interpretation is time-consuming and
error-prone. In this paper, we propose xLSTM-ECG, a novel approach that
leverages an extended Long Short-Term Memory (xLSTM) network for multi-label
classification of ECG signals, using the PTB-XL dataset. To the best of our
knowledge, this work represents the first design and application of xLSTM
modules specifically adapted for multi-label ECG classification. Our method
employs a Short-Time Fourier Transform (STFT) to convert time-series ECG
waveforms into the frequency domain, thereby enhancing feature extraction. The
xLSTM architecture is specifically tailored to address the complexities of
12-lead ECG recordings by capturing both local and global signal features.
Comprehensive experiments on the PTB-XL dataset reveal that our model achieves
strong multi-label classification performance, while additional tests on the
Georgia 12-Lead dataset underscore its robustness and efficiency. This approach
significantly improves ECG classification accuracy, thereby advancing clinical
diagnostics and patient care. The code will be publicly available upon
acceptance.

</details>


### [120] [A Self-supervised Learning Method for Raman Spectroscopy based on Masked Autoencoders](https://arxiv.org/abs/2504.16130)
*Pengju Ren,Ri-gui Zhou,Yaochong Li*

Main category: eess.SP

TL;DR: This paper proposes a self-supervised learning method for Raman spectroscopy called SMAE, which enhances spectral feature extraction without requiring annotated data, achieving notable performance improvements over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of supervised learning in Raman spectroscopy, specifically the reliance on costly and limited annotated spectral datasets, which hampers performance in material identification when such data is scarce.

Method: A self-supervised learning paradigm called SMAE is introduced, utilizing a Masked AutoEncoder to learn essential spectral features by reconstructing randomly masked spectral information, eliminating the need for annotations during pre-training.

Result: SMAE demonstrates over 80% clustering accuracy for 30 classes of isolated bacteria and achieves identification accuracy of 83.90% on the test set, outperforming classical unsupervised and state-of-the-art deep clustering methods, as well as showing competitive performance against a supervised ResNet.

Conclusion: The proposed SMAE effectively leverages unannotated spectral data to enhance Raman spectroscopy analysis, providing a significant advancement in handling spectral feature extraction and material identification with minimal annotated data.

Abstract: Raman spectroscopy serves as a powerful and reliable tool for analyzing the
chemical information of substances. The integration of Raman spectroscopy with
deep learning methods enables rapid qualitative and quantitative analysis of
materials. Most existing approaches adopt supervised learning methods. Although
supervised learning has achieved satisfactory accuracy in spectral analysis, it
is still constrained by costly and limited well-annotated spectral datasets for
training. When spectral annotation is challenging or the amount of annotated
data is insufficient, the performance of supervised learning in spectral
material identification declines. In order to address the challenge of feature
extraction from unannotated spectra, we propose a self-supervised learning
paradigm for Raman Spectroscopy based on a Masked AutoEncoder, termed SMAE.
SMAE does not require any spectral annotations during pre-training. By randomly
masking and then reconstructing the spectral information, the model learns
essential spectral features. The reconstructed spectra exhibit certain
denoising properties, improving the signal-to-noise ratio (SNR) by more than
twofold. Utilizing the network weights obtained from masked pre-training, SMAE
achieves clustering accuracy of over 80% for 30 classes of isolated bacteria in
a pathogenic bacterial dataset, demonstrating significant improvements compared
to classical unsupervised methods and other state-of-the-art deep clustering
methods. After fine-tuning the network with a limited amount of annotated data,
SMAE achieves an identification accuracy of 83.90% on the test set, presenting
competitive performance against the supervised ResNet (83.40%).

</details>


### [121] [A Non-Invasive Load Monitoring Method for Edge Computing Based on MobileNetV3 and Dynamic Time Regulation](https://arxiv.org/abs/2504.16142)
*Hangxu Liu,Yaojie Sun,Yu Wang*

Main category: eess.SP

TL;DR: This study presents a Dynamic Time Warping (DTW) algorithm to improve non-intrusive load monitoring (NILM) accuracy while reducing computational costs on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the high computational costs and memory requirements of existing machine learning methods in NILM, which hinder their deployment on microcontroller units (MCUs).

Method: An innovative DTW algorithm is proposed in the time-frequency domain, and performance is analyzed across six machine learning techniques in home electricity scenarios, validated through experiments on edge MCUs.

Result: The proposed method achieves 95% recognition accuracy while reducing running time by 55.55% and storage overhead by about 34.6%.

Conclusion: Future research will focus on optimizing algorithm performance and eliminating voltage transformer designs to reduce costs, aiming for more cost-effective NILM solutions in practical applications.

Abstract: In recent years, non-intrusive load monitoring (NILM) technology has
attracted much attention in the related research field by virtue of its unique
advantage of utilizing single meter data to achieve accurate decomposition of
device-level energy consumption. Cutting-edge methods based on machine learning
and deep learning have achieved remarkable results in load decomposition
accuracy by fusing time-frequency domain features. However, these methods
generally suffer from high computational costs and huge memory requirements,
which become the main obstacles for their deployment on resource-constrained
microcontroller units (MCUs). To address these challenges, this study proposes
an innovative Dynamic Time Warping (DTW) algorithm in the time-frequency domain
and systematically compares and analyzes the performance of six machine
learning techniques in home electricity scenarios. Through complete
experimental validation on edge MCUs, this scheme successfully achieves a
recognition accuracy of 95%. Meanwhile, this study deeply optimizes the
frequency domain feature extraction process, which effectively reduces the
running time by 55.55% and the storage overhead by about 34.6%. The algorithm
performance will be further optimized in future research work. Considering that
the elimination of voltage transformer design can significantly reduce the
cost, the subsequent research will focus on this direction, and is committed to
providing more cost-effective solutions for the practical application of NILM,
and providing a solid theoretical foundation and feasible technical paths for
the design of efficient NILM systems in edge computing environments.

</details>


### [122] [A Statistical Approach for Synthetic EEG Data Generation](https://arxiv.org/abs/2504.16143)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: eess.SP

TL;DR: A method combining correlation analysis and random sampling is proposed to generate high-quality synthetic EEG data for mental health research.


<details>
  <summary>Details</summary>
Motivation: EEG data is essential for diagnosing mental health conditions but is expensive and time-consuming to collect; synthetic data generation can help augment datasets for machine learning.

Method: The study employs correlation analysis to analyze dependencies among EEG frequency bands and uses random sampling to generate synthetic samples, selectively retaining those that closely match real data based on distribution analysis and classification tasks.

Result: The generated synthetic data exhibit similar statistical and structural properties to original EEG data, with retained samples showing high correlation and successful validation through a Random Forest model which performs at chance level in distinguishing synthetic from real data.

Conclusion: This method offers a scalable and privacy-preserving way to augment EEG datasets, facilitating more efficient training of models in mental health research.

Abstract: Electroencephalogram (EEG) data is crucial for diagnosing mental health
conditions but is costly and time-consuming to collect at scale. Synthetic data
generation offers a promising solution to augment datasets for machine learning
applications. However, generating high-quality synthetic EEG that preserves
emotional and mental health signals remains challenging. This study proposes a
method combining correlation analysis and random sampling to generate realistic
synthetic EEG data.
  We first analyze interdependencies between EEG frequency bands using
correlation analysis. Guided by this structure, we generate synthetic samples
via random sampling. Samples with high correlation to real data are retained
and evaluated through distribution analysis and classification tasks. A Random
Forest model trained to distinguish synthetic from real EEG performs at chance
level, indicating high fidelity.
  The generated synthetic data closely match the statistical and structural
properties of the original EEG, with similar correlation coefficients and no
significant differences in PERMANOVA tests. This method provides a scalable,
privacy-preserving approach for augmenting EEG datasets, enabling more
efficient model training in mental health research.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [123] [A Systematic Literature Review of Software Engineering Research on Jupyter Notebook](https://arxiv.org/abs/2504.16180)
*Md Saeed Siddik,Hao Li,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: The study analyzes trends and methodologies in software engineering research related to Jupyter notebooks through a systematic review of 146 publications, identifying a focus on human-computer interaction and highlighting gaps in areas such as reusability and documentation.


<details>
  <summary>Details</summary>
Motivation: With the rising use of Jupyter notebooks by researchers and developers, there is a need to enhance software engineering practices specific to this tool.

Method: The authors conducted a systematic literature review of 146 relevant publications from the DBLP Computer Science Bibliography, categorizing them by software engineering topics and analyzing publication trends.

Result: The findings show that most publications are presented in human-computer interaction venues, with significant topics including code reuse and readability; however, only 64 studies were found reusable based on their provided URLs, and many replication packages lack long-term availability.

Conclusion: There is significant underexploration of notebook-specific solutions for software engineering challenges such as testing and documentation, presenting numerous future research opportunities.

Abstract: Context: Jupyter Notebook has emerged as a versatile tool that transforms how
researchers, developers, and data scientists conduct and communicate their
work. As the adoption of Jupyter notebooks continues to rise, so does the
interest from the software engineering research community in improving the
software engineering practices for Jupyter notebooks.
  Objective: The purpose of this study is to analyze trends, gaps, and
methodologies used in software engineering research on Jupyter notebooks.
  Method: We selected 146 relevant publications from the DBLP Computer Science
Bibliography up to the end of 2024, following established systematic literature
review guidelines. We explored publication trends, categorized them based on
software engineering topics, and reported findings based on those topics.
  Results: The most popular venues for publishing software engineering research
on Jupyter notebooks are related to human-computer interaction instead of
traditional software engineering venues. Researchers have addressed a wide
range of software engineering topics on notebooks, such as code reuse,
readability, and execution environment. Although reusability is one of the
research topics for Jupyter notebooks, only 64 of the 146 studies can be reused
based on their provided URLs. Additionally, most replication packages are not
hosted on permanent repositories for long-term availability and adherence to
open science principles.
  Conclusion: Solutions specific to notebooks for software engineering issues,
including testing, refactoring, and documentation, are underexplored. Future
research opportunities exist in automatic testing frameworks, refactoring
clones between notebooks, and generating group documentation for coherent code
cells.

</details>


### [124] [Open Source Software Lifecycle Classification: Developing Wrangling Techniques for Complex Sociotechnical Systems](https://arxiv.org/abs/2504.16670)
*Wenyi Lu,Enock Kasaadah,S M Rakib Ul Karim,Matt Germonprez,Sean Goggins*

Main category: cs.SE

TL;DR: The paper addresses the need for a better understanding and classification of open source software projects amidst the growing complexity and participation in the field.


<details>
  <summary>Details</summary>
Motivation: There is a rapid evolution of open source software and a corresponding increase in project diversity and corporate involvement, necessitating an understanding of their characteristics to inform policy, economics, and design.

Method: The paper reviews existing classification attempts of open source software and conducts an empirical, mixed-methods study to classify these projects based on their lifecycle position.

Result: It illustrates the conflicting purposes for classifying open source projects and how these conflicts hinder comprehensive understanding, culminating in a proposed classification system based on lifecycle position.

Conclusion: The study offers a framework for classifying open source software projects, aiming to enhance scientific and practical insights into the nature of open source genres.

Abstract: Open source software is a rapidly evolving center for distributed work, and
understanding the characteristics of this work across its different contexts is
vital for informing policy, economics, and the design of enabling software. The
steep increase in open source projects and corporate participation have
transformed a peripheral, cottage industry component of the global technology
ecosystem into a large, infinitely complex "technology parts supplier" wired
into every corner of contemporary life. The lack of theory and tools for
breaking this complexity down into identifiable project types or strategies for
understanding them more systematically is incommensurate with current industry,
society, and developer needs. This paper reviews previous attempts to classify
open source software and other organizational ecosystems, using open source
scientific software ecosystems in contrast with those found in corporatized
open source software. It then examines the divergent and sometimes conflicting
purposes that may exist for classifying open source projects and how these
competing interests impede our progress in developing a comprehensive
understanding of how open source software projects and companies operate.
Finally, we will present an empirical, mixed-methods study demonstrating how to
classify open-source projects by their lifecycle position. This is the first
step forward, advancing our scientific and practical knowledge of open source
software through the lens of dynamic and evolving open source genres. It
concludes with examples and a proposed path forward.

</details>


### [125] [ClarifyCoder: Clarification-Aware Fine-Tuning for Programmatic Problem Solving](https://arxiv.org/abs/2504.16331)
*Jie JW Wu,Manav Chaudhary,Davit Abrahamyan,Arhaan Khaku,Anjiang Wei,Fatemeh H. Fard*

Main category: cs.SE

TL;DR: ClarifyCoder enhances large language models (LLMs) by enabling them to recognize ambiguities in programming requirements and request clarification, improving their code generation performance.


<details>
  <summary>Details</summary>
Motivation: There is a performance gap between LLMs in code generation tasks and expert software engineers, largely due to LLMs' inability to actively seek clarification on ambiguous requirements.

Method: ClarifyCoder employs synthetic data generation and instruction-tuning, consisting of a data synthesis technique that augments programming datasets with scenarios requiring clarification and a fine-tuning strategy to prioritize seeking clarification over immediate code generation.

Result: Experimental results show that ClarifyCoder significantly improves Code LLMs' communication capabilities through clarification dialogues, while maintaining their code generation proficiency.

Conclusion: The integration of clarification awareness into LLMs is essential for enhancing their performance in code generation tasks, demonstrating the importance of recognizing and querying ambiguities.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation tasks. However, a significant gap remains between their current
performance and that of expert software engineers. A key differentiator is that
human engineers actively seek clarification when faced with ambiguous
requirements, while LLMs typically generate code regardless of uncertainties in
the problem description. We present ClarifyCoder, a novel framework with
synthetic data generation and instruction-tuning that enables LLMs to identify
ambiguities and request clarification before proceeding with code generation.
While recent work has focused on LLM-based agents for iterative code
generation, we argue that the fundamental ability to recognize and query
ambiguous requirements should be intrinsic to the models themselves. Our
approach consists of two main components: (1) a data synthesis technique that
augments existing programming datasets with scenarios requiring clarification
to generate clarification-aware training data, and (2) a fine-tuning strategy
that teaches models to prioritize seeking clarification over immediate code
generation when faced with incomplete or ambiguous requirements. We further
provide an empirical analysis of integrating ClarifyCoder with standard
fine-tuning for a joint optimization of both clarify-awareness and coding
ability. Experimental results demonstrate that ClarifyCoder significantly
improves the communication capabilities of Code LLMs through meaningful
clarification dialogues while maintaining code generation capabilities.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [126] [A Geometric Approach to Problems in Optimization and Data Science](https://arxiv.org/abs/2504.16270)
*Naren Sarayu Manoj*

Main category: math.OC

TL;DR: The paper presents new results in computational and statistical machine learning by using high-dimensional geometry and probability; it is divided into two parts focusing on optimization algorithms and statistical guarantees.


<details>
  <summary>Details</summary>
Motivation: To improve the understanding and performance of problems in computational and statistical machine learning by leveraging high-dimensional geometry and probability.

Method: The authors propose new algorithms for various optimization techniques, including approximating convex polytopes and robust regression, and they also introduce a model to analyze backdoor data poisoning attacks and the robustness of graph clustering algorithms.

Result: New algorithms are developed for approximating convex polytopes, robust least squares regression, and analyzing 'helpful' misspecification in graph clustering, along with statistical guarantees for backdoor data poisoning methods.

Conclusion: The findings advance the field of machine learning by providing innovative algorithms and statistical insights that enhance robustness and optimization techniques.

Abstract: We give new results for problems in computational and statistical machine
learning using tools from high-dimensional geometry and probability.
  We break up our treatment into two parts. In Part I, we focus on
computational considerations in optimization. Specifically, we give new
algorithms for approximating convex polytopes in a stream, sparsification and
robust least squares regression, and dueling optimization.
  In Part II, we give new statistical guarantees for data science problems. In
particular, we formulate a new model in which we analyze statistical properties
of backdoor data poisoning attacks, and we study the robustness of graph
clustering algorithms to ``helpful'' misspecification.

</details>


### [127] [The Safety-Privacy Tradeoff in Linear Bandits](https://arxiv.org/abs/2504.16371)
*Arghavan Zibaie,Spencer Hutchinson,Ramtin Pedarsani,Mahnoosh Alizadeh*

Main category: math.OC

TL;DR: The paper addresses linear stochastic bandit problems with a global safety constraint, focusing on regret minimization while ensuring local differential privacy for agents' data.


<details>
  <summary>Details</summary>
Motivation: To manage the balance between privacy for individual agents and the need to meet global safety constraints in a centralized decision-making framework.

Method: The authors analyze tradeoffs between privacy levels and regret using the notion of the sharpness of the safety set, proposing a unimprovable vector of privacy levels for agents given a fixed regret budget.

Result: The proposed approach shows quantifiable tradeoffs between privacy levels and the adherence to safety constraints while minimizing regret.

Conclusion: Implementing LDP can impact safety and regret, but an optimal balance can be achieved with the right privacy levels for agents in order to satisfy global safety while minimizing regret.

Abstract: We consider a collection of linear stochastic bandit problems, each modeling
the random response of different agents to proposed interventions, coupled
together by a global safety constraint. We assume a central coordinator must
choose actions to play on each bandit with the objective of regret
minimization, while also ensuring that the expected response of all agents
satisfies the global safety constraints at each round, in spite of uncertainty
about the bandits' parameters. The agents consider their observed responses to
be private and in order to protect their sensitive information, the data
sharing with the central coordinator is performed under local differential
privacy (LDP). However, providing higher level of privacy to different agents
would have consequences in terms of safety and regret. We formalize these
tradeoffs by building on the notion of the sharpness of the safety set - a
measure of how the geometric properties of the safe set affects the growth of
regret - and propose a unilaterally unimprovable vector of privacy levels for
different agents given a maximum regret budget.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [128] [A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation for 6G: MLR, ANOVA, and Residual Distribution Analysis](https://arxiv.org/abs/2504.16688)
*Nahshon Mokua,Obiri,Kristof,Van Laerhoven*

Main category: cs.NI

TL;DR: The paper proposes a two-stage approach to model path loss in indoor LoRaWAN deployments, improving accuracy by incorporating environmental factors in signal propagation analysis.


<details>
  <summary>Details</summary>
Motivation: Path loss modeling in indoor LoRaWAN technology is challenging due to various factors such as structural obstructions and environmental conditions.

Method: A multiple linear regression model was implemented that included traditional propagation metrics and extended with environmental variables. The study used a dataset of 1,328,334 measurements and applied analysis of variance to evaluate the model's performance. Additionally, five candidate probability distributions were fitted to examine residual distributions.

Result: Incorporating environmental factors reduced unexplained variance by 42.32%. The four-component Gaussian Mixture Model outperformed single-distribution models in capturing residual heterogeneity, providing a more accurate representation of indoor signal propagation.

Conclusion: Environment-aware modeling significantly enhances the design of LoRaWAN networks, especially in the context of developing ultra-reliable communications for 6G networks.

Abstract: Modeling path loss in indoor LoRaWAN technology deployments is inherently
challenging due to structural obstructions, occupant density and activities,
and fluctuating environmental conditions. This study proposes a two-stage
approach to capture and analyze these complexities using an extensive dataset
of 1,328,334 field measurements collected over six months in a single-floor
office at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,
we implement a multiple linear regression model that includes traditional
propagation metrics (distance, structural walls) and an extension with proposed
environmental variables (relative humidity, temperature, carbon dioxide,
particulate matter, and barometric pressure). Using analysis of variance, we
demonstrate that adding these environmental factors can reduce unexplained
variance by 42.32 percent. Secondly, we examine residual distributions by
fitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,
Student's t, and Gaussian Mixture Models with one to five components. Our
results show that a four-component Gaussian Mixture Model captures the residual
heterogeneity of indoor signal propagation most accurately, significantly
outperforming single-distribution approaches. Given the push toward
ultra-reliable, context-aware communications in 6G networks, our analysis shows
that environment-aware modeling can substantially improve LoRaWAN network
design in dynamic indoor IoT deployments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [129] [MARFT: Multi-Agent Reinforcement Fine-Tuning](https://arxiv.org/abs/2504.16129)
*Junwei Liao,Muning Wen,Jun Wang,Weinan Zhang*

Main category: cs.MA

TL;DR: The paper introduces a novel Multi-Agent Reinforcement Fine-Tuning (MARFT) framework to enhance LLM-based Multi-Agent Systems (LaMAS) using foundational Reinforcement Learning (RL) techniques, addressing challenges unique to the LaMAS context.


<details>
  <summary>Details</summary>
Motivation: Limited research has investigated the application of foundational RL techniques in fine-tuning LaMAS, and MARL methodologies pose significant challenges due to the specific characteristics of LaMAS.

Method: A comprehensive study of LLM-based MARL is presented, proposing the MARFT framework that includes a universal algorithmic structure and outlines the differences between traditional MARL and MARFT for LaMAS, with practical implementation strategies.

Result: The paper details the core algorithm of MARFT and provides a complete open-source implementation, facilitating further research and adoption in real-world applications.

Conclusion: This work aims to serve as a roadmap for advancing MARFT towards resilient and adaptive solutions in agentic systems by bridging theory with practical methodologies.

Abstract: LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in
addressing complex, agentic tasks requiring multifaceted reasoning and
collaboration, from generating high-quality presentation slides to conducting
sophisticated scientific research. Meanwhile, RL has been widely recognized for
its effectiveness in enhancing agent intelligence, but limited research has
investigated the fine-tuning of LaMAS using foundational RL techniques.
Moreover, the direct application of MARL methodologies to LaMAS introduces
significant challenges, stemming from the unique characteristics and mechanisms
inherent to LaMAS. To address these challenges, this article presents a
comprehensive study of LLM-based MARL and proposes a novel paradigm termed
Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal
algorithmic framework tailored for LaMAS, outlining the conceptual foundations,
key distinctions, and practical implementation strategies. We begin by
reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage
for a parallel analysis in the multi-agent domain. In the context of LaMAS, we
elucidate critical differences between MARL and MARFT. These differences
motivate a transition toward a novel, LaMAS-oriented formulation of RFT.
Central to this work is the presentation of a robust and scalable MARFT
framework. We detail the core algorithm and provide a complete, open-source
implementation to facilitate adoption and further research. The latter sections
of the paper explore real-world application perspectives and opening challenges
in MARFT. By bridging theoretical underpinnings with practical methodologies,
this work aims to serve as a roadmap for researchers seeking to advance MARFT
toward resilient and adaptive solutions in agentic systems. Our implementation
of the proposed framework is publicly available at:
https://github.com/jwliao-ai/MARFT.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [130] [Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes](https://arxiv.org/abs/2504.16117)
*Sridevi Polavaram,Xin Zhou,Meenu Ravi,Mohammad Zarei,Anmol Srivastava*

Main category: cs.CV

TL;DR: CAIRO is an ontology-based framework for detecting and formalizing critical failures in vision systems, aimed at enhancing safety in high-stakes applications such as automated driving.


<details>
  <summary>Details</summary>
Motivation: To mitigate risks arising from vulnerabilities in vision systems due to rare or unforeseen scenarios, especially in critical sectors like surveillance and transportation.

Method: CAIRO employs an ontology-based approach to create a human-assistive discovery framework that involves human testing and evaluation of critical misdetections, adversarial attacks, and hallucinations in AI models, with a focus on automated driving systems.

Result: The analysis demonstrates scalable methods for identifying gaps between camera perception and real-world contexts, resulting in formalized test cases stored as knowledge graphs for improved accountability and reasoning.

Conclusion: CAIRO enhances the interpretability and accountability of vision systems, ultimately leading to better safety measures in the deployment of AI in critical domains.

Abstract: Vision systems are increasingly deployed in critical domains such as
surveillance, law enforcement, and transportation. However, their
vulnerabilities to rare or unforeseen scenarios pose significant safety risks.
To address these challenges, we introduce Context-Awareness and
Interpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive
discovery framework for failure cases (or CP - Critical Phenomena) detection
and formalization. CAIRO by design incentivizes human-in-the-loop for testing
and evaluation of criticality that arises from misdetections, adversarial
attacks, and hallucinations in AI black-box models. Our robust analysis of
object detection model(s) failures in automated driving systems (ADS) showcases
scalable and interpretable ways of formalizing the observed gaps between camera
perception and real-world contexts, resulting in test cases stored as explicit
knowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,
logical reasoning, and accountability.

</details>


### [131] [PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels](https://arxiv.org/abs/2504.16419)
*Qi Yang,Weichen Bi,Haiyang Shen,Yaoqi Guo,Yun Ma*

Main category: cs.CV

TL;DR: PixelWeb is a new large-scale GUI dataset with over 100,000 annotated web pages that addresses inaccuracies in existing automatic labeling of GUI elements by providing comprehensive and precise BBox annotations.


<details>
  <summary>Details</summary>
Motivation: Current GUI datasets often suffer from inaccurate annotations due to automatic labeling, negatively impacting model performance in real applications. Additionally, existing datasets primarily focus on visual BBox annotations, limiting the development of visually-related tasks.

Method: PixelWeb is created using a novel automatic annotation approach that combines visual feature extraction and DOM structure analysis, utilizing channel derivation for accurate localization of elements and layer analysis for precise BBox annotations. It also includes extensive metadata verification by multiple annotators.

Result: PixelWeb achieves a mAP95 metric performance that is 3-7 times better than existing datasets in GUI element detection tasks, demonstrating significant improvement in annotation quality and accuracy.

Conclusion: PixelWeb represents a substantial advancement in GUI dataset quality, which can enhance the performance of downstream tasks such as GUI generation and automated user interaction.

Abstract: Graphical User Interface (GUI) datasets are crucial for various downstream
tasks. However, GUI datasets often generate annotation information through
automatic labeling, which commonly results in inaccurate GUI element BBox
annotations, including missing, duplicate, or meaningless BBoxes. These issues
can degrade the performance of models trained on these datasets, limiting their
effectiveness in real-world applications. Additionally, existing GUI datasets
only provide BBox annotations visually, which restricts the development of
visually related GUI downstream tasks. To address these issues, we introduce
PixelWeb, a large-scale GUI dataset containing over 100,000 annotated web
pages. PixelWeb is constructed using a novel automatic annotation approach that
integrates visual feature extraction and Document Object Model (DOM) structure
analysis through two core modules: channel derivation and layer analysis.
Channel derivation ensures accurate localization of GUI elements in cases of
occlusion and overlapping elements by extracting BGRA four-channel bitmap
annotations. Layer analysis uses the DOM to determine the visibility and
stacking order of elements, providing precise BBox annotations. Additionally,
PixelWeb includes comprehensive metadata such as element images, contours, and
mask annotations. Manual verification by three independent annotators confirms
the high quality and accuracy of PixelWeb annotations. Experimental results on
GUI element detection tasks show that PixelWeb achieves performance on the
mAP95 metric that is 3-7 times better than existing datasets. We believe that
PixelWeb has great potential for performance improvement in downstream tasks
such as GUI generation and automated user interaction.

</details>


### [132] [Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends](https://arxiv.org/abs/2504.16134)
*Mohammad Abu Tami,Mohammed Elhenawy,Huthaifa I. Ashqar*

Main category: cs.CV

TL;DR: The paper reviews the potential of Multimodal Large Language Models (MLLMs) to enhance traffic safety by integrating diverse data for improved scene understanding and decision-making.


<details>
  <summary>Details</summary>
Motivation: Traffic safety is a significant global issue, with existing Advanced Driver-Assistance Systems (ADAS) facing challenges in complex real-world situations.

Method: The paper conducts a comprehensive analysis of MLLM-based approaches that utilize cross-modal data (visual, spatial, environmental) to enhance perception and decision-making in traffic safety.

Result: MLLMs were found to improve adversarial robustness and overall system effectiveness by leveraging key datasets and innovative methodologies in traffic safety applications.

Conclusion: MLLMs present a transformative opportunity for next-generation traffic safety systems, potentially providing scalable and context-aware solutions that can significantly enhance road safety.

Abstract: Traffic safety remains a critical global challenge, with traditional Advanced
Driver-Assistance Systems (ADAS) often struggling in dynamic real-world
scenarios due to fragmented sensor processing and susceptibility to adversarial
conditions. This paper reviews the transformative potential of Multimodal Large
Language Models (MLLMs) in addressing these limitations by integrating
cross-modal data such as visual, spatial, and environmental inputs to enable
holistic scene understanding. Through a comprehensive analysis of MLLM-based
approaches, we highlight their capabilities in enhancing perception,
decision-making, and adversarial robustness, while also examining the role of
key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research.
Furthermore, we outline future directions, including real-time edge deployment,
causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as
a cornerstone for next-generation traffic safety systems, this review
underscores their potential to revolutionize the field, offering scalable,
context-aware solutions that proactively mitigate risks and improve overall
road safety.

</details>


### [133] [SignX: The Foundation Model for Sign Recognition](https://arxiv.org/abs/2504.16315)
*Sen Fang,Chunyu Sui,Hongwei Yi,Carol Neidle,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: SignX is a foundation model framework for American Sign Language (ASL) recognition, integrating pose information for improved translation of sign language into English glosses.


<details>
  <summary>Details</summary>
Motivation: There are significant challenges in processing sign language data, particularly in translating ASL signs into English-based ID glosses, which necessitate standardized glossing conventions across datasets.

Method: The framework includes a Pose2Gloss component using an inverse diffusion model with a multi-track pose fusion layer, and a Video2Pose module based on Vision Transformer (ViT) to convert raw video into pose representation.

Result: SignX demonstrates enhanced accuracy in recognizing signs from sign language videos, achieving superior performance compared to previous methodologies.

Conclusion: SignX lays the groundwork for common pose estimation necessary for effective sign language recognition, showing promise for various human activity recognition scenarios.

Abstract: The complexity of sign language data processing brings many challenges. The
current approach to recognition of ASL signs aims to translate RGB sign
language videos through pose information into English-based ID glosses, which
serve to uniquely identify ASL signs. Note that there is no shared convention
for assigning such glosses to ASL signs, so it is essential that the same
glossing conventions are used for all of the data in the datasets that are
employed. This paper proposes SignX, a foundation model framework for sign
recognition. It is a concise yet powerful framework applicable to multiple
human activity recognition scenarios. First, we developed a Pose2Gloss
component based on an inverse diffusion model, which contains a multi-track
pose fusion layer that unifies five of the most powerful pose information
sources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens
Segmentation--into a single latent pose representation. Second, we trained a
Video2Pose module based on ViT that can directly convert raw video into signer
pose representation. Through this 2-stage training framework, we enable sign
language recognition models to be compatible with existing pose formats, laying
the foundation for the common pose estimation necessary for sign recognition.
Experimental results show that SignX can recognize signs from sign language
video, producing predicted gloss representations with greater accuracy than has
been reported in prior work.

</details>


### [134] [Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT](https://arxiv.org/abs/2504.16128)
*Stanley Mugisha,Rashid Kisitu,Florence Tushabe*

Main category: cs.CV

TL;DR: The paper proposes a hybrid knowledge distillation framework to enhance the performance of MobileNetV3 for real-time plant disease classification on resource-constrained edge devices, achieving competitive accuracy with significantly reduced computational demands.


<details>
  <summary>Details</summary>
Motivation: The challenge in agricultural IoT systems is to leverage the high accuracy of Vision Transformers while meeting the efficiency requirements of resource-constrained edge devices for real-time applications.

Method: The proposed method uses a hybrid knowledge distillation framework to transfer knowledge from a Swin Transformer teacher model to a MobileNetV3 student model, incorporating adaptive attention alignment and a dual-loss function to improve performance and address architectural mismatches.

Result: The distilled MobileNetV3 achieves 92.4% accuracy compared to 95.9% for the Swin Transformer, while reducing computational complexity by 95% and decreasing inference latency to 23ms on PC and 86ms/image on smartphones.

Conclusion: The work demonstrates that it is possible to achieve Vision Transformer-level diagnostic precision on edge devices, advancing energy-efficient crop monitoring in precision agriculture, and the code and models will be available for replication post-acceptance.

Abstract: Integrating deep learning applications into agricultural IoT systems faces a
serious challenge of balancing the high accuracy of Vision Transformers (ViTs)
with the efficiency demands of resource-constrained edge devices. Large
transformer models like the Swin Transformers excel in plant disease
classification by capturing global-local dependencies. However, their
computational complexity (34.1 GFLOPs) limits applications and renders them
impractical for real-time on-device inference. Lightweight models such as
MobileNetV3 and TinyML would be suitable for on-device inference but lack the
required spatial reasoning for fine-grained disease detection. To bridge this
gap, we propose a hybrid knowledge distillation framework that synergistically
transfers logit and attention knowledge from a Swin Transformer teacher to a
MobileNetV3 student model. Our method includes the introduction of adaptive
attention alignment to resolve cross-architecture mismatch (resolution,
channels) and a dual-loss function optimizing both class probabilities and
spatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled
MobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95%
reduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU
and 86ms/image on smartphone CPUs). Key innovations include IoT-centric
validation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching
attention maps. Comparative experiments show significant improvements over
standalone CNNs and prior distillation methods, with a 3.5% accuracy gain over
MobileNetV3 baselines. Significantly, this work advances real-time,
energy-efficient crop monitoring in precision agriculture and demonstrates how
we can attain ViT-level diagnostic precision on edge devices. Code and models
will be made available for replication after acceptance.

</details>


### [135] [Naturally Computed Scale Invariance in the Residual Stream of ResNet18](https://arxiv.org/abs/2504.16290)
*André Longon*

Main category: cs.CV

TL;DR: This study explores how ResNet18 achieves scale invariance in visual object recognition, focusing on its unique residual stream.


<details>
  <summary>Details</summary>
Motivation: Understanding how neural networks like ResNet18 achieve invariance to image-altering variables, following the limited insights gained from InceptionV1's interpretability research.

Method: The research investigates the scale invariant properties of convolutional channels in ResNet18's intermediate blocks, analyzing the residual summation of scale-equivariant representations through ablation experiments.

Result: Findings indicate that the residual stream in ResNet18 contributes to scale invariance, linking specific neural properties to the network's object recognition capabilities.

Conclusion: The study provides preliminary evidence on how the residual stream helps compute scale invariance, suggesting a significant role in robust object recognition behavior.

Abstract: An important capacity in visual object recognition is invariance to
image-altering variables which leave the identity of objects unchanged, such as
lighting, rotation, and scale. How do neural networks achieve this? Prior
mechanistic interpretability research has illuminated some invariance-building
circuitry in InceptionV1, but the results are limited and networks with
different architectures have remained largely unexplored. This work
investigates ResNet18 with a particular focus on its residual stream, an
architectural component which InceptionV1 lacks. We observe that many
convolutional channels in intermediate blocks exhibit scale invariant
properties, computed by the element-wise residual summation of scale
equivariant representations: the block input's smaller-scale copy with the
block pre-sum output's larger-scale copy. Through subsequent ablation
experiments, we attempt to causally link these neural properties with
scale-robust object recognition behavior. Our tentative findings suggest how
the residual stream computes scale invariance and its possible role in
behavior. Code is available at:
https://github.com/cest-andre/residual-stream-interp

</details>


### [136] [Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection](https://arxiv.org/abs/2504.16404)
*Md Fahimuzzman Sohan*

Main category: cs.CV

TL;DR: The study develops a deep learning model to detect cattle lameness using publicly available video data, achieving strong classification performance, particularly with a 3D CNN model.


<details>
  <summary>Details</summary>
Motivation: Cattle lameness, caused by hoof injuries and other conditions, significantly impacts their physiological activities and overall welfare, necessitating effective detection methods.

Method: The study utilized a dataset of 50 videos of 40 cattle, divided between normal and lame gait. It applied data augmentation and employed two deep learning models, ConvLSTM2D and 3D CNN, for classification.

Result: The 3D CNN model achieved a video-level classification accuracy of 90% with precision, recall, and f1-score all around 90.9%, while the ConvLSTM2D model achieved an accuracy of 85%.

Conclusion: Deep learning models, particularly the 3D CNN, are effective for detecting cattle lameness and provide a simplified processing pipeline compared to traditional methods.

Abstract: Cattle lameness is often caused by hoof injuries or interdigital dermatitis,
leads to pain and significantly impacts essential physiological activities such
as walking, feeding, and drinking. This study presents a deep learning-based
model for detecting cattle lameness, sickness, or gait abnormalities using
publicly available video data. The dataset consists of 50 unique videos from 40
individual cattle, recorded from various angles in both indoor and outdoor
environments. Half of the dataset represents naturally walking
(normal/non-lame) cattle, while the other half consists of cattle exhibiting
gait abnormalities (lame). To enhance model robustness and generalizability,
data augmentation was applied to the training data. The pre-processed videos
were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A
comparative analysis of the results demonstrates strong classification
performance. Specifically, the 3D CNN model achieved a video-level
classification accuracy of 90%, with precision, recall, and f1-score of 90.9%,
90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower
accuracy of 85%. This study highlights the effectiveness of directly applying
classification models to learn spatiotemporal features from video data,
offering an alternative to traditional multi-stage approaches that typically
involve object detection, pose estimation, and feature extraction. Besides, the
findings demonstrate that the proposed deep learning models, particularly the
3D CNN, effectively classify and detect lameness in cattle while simplifying
the processing pipeline.

</details>


### [137] [Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes](https://arxiv.org/abs/2504.16538)
*Joan Perez,Giovanni Fusco*

Main category: cs.CV

TL;DR: The paper presents SAGAI, a modular workflow for scoring streetscapes using open-access data and vision-language models, allowing for scalable urban scene analysis without the need for specialized training.


<details>
  <summary>Details</summary>
Motivation: Current methods for assessing streetscapes are either limited to basic morphometric properties or require labor-intensive qualitative analysis, necessitating a more efficient approach.

Method: SAGAI integrates data from OpenStreetMap and Google Street View, utilizing a lightweight version of the LLaVA model to produce structured spatial indicators through customizable prompts, with a pipeline that aggregates visual scores for spatial analysis.

Result: Two case studies demonstrated SAGAI's effectiveness in urban-rural scene classification (strong performance), commercial feature detection (moderate precision), and sidewalk width estimation (lower but informative results).

Conclusion: SAGAI is deployable by users without special training and can be adapted for various urban research topics by modifying its prompts.

Abstract: Streetscapes are an essential component of urban space. Their assessment is
presently either limited to morphometric properties of their mass skeleton or
requires labor-intensive qualitative evaluations of visually perceived
qualities. This paper introduces SAGAI: Streetscape Analysis with Generative
Artificial Intelligence, a modular workflow for scoring street-level urban
scenes using open-access data and vision-language models. SAGAI integrates
OpenStreetMap geometries, Google Street View imagery, and a lightweight version
of the LLaVA model to generate structured spatial indicators from images via
customizable natural language prompts. The pipeline includes an automated
mapping module that aggregates visual scores at both the point and street
levels, enabling direct cartographic interpretation. It operates without
task-specific training or proprietary software dependencies, supporting
scalable and interpretable analysis of urban environments. Two exploratory case
studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial
outputs from vision-language inference. The initial results show strong
performance for binary urban-rural scene classification, moderate precision in
commercial feature detection, and lower estimates, but still informative, of
sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a
wide range of urban research themes, such as walkability, safety, or urban
design, through prompt modification alone.

</details>


### [138] [Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections](https://arxiv.org/abs/2504.16612)
*Max Kirchner,Alexander C. Jenke,Sebastian Bodenstedt,Fiona R. Kolbinger,Oliver Saldanha,Jakob N. Kather,Martin Wagner,Stefanie Speidel*

Main category: cs.CV

TL;DR: This study explores using federated learning to train foundation models for minimally invasive surgery while addressing data-sharing limitations.


<details>
  <summary>Details</summary>
Motivation: To enable collaborative model training without data transfer, addressing the challenges of data sharing in minimally invasive surgery.

Method: The Masked Autoencoder is adapted for federated learning, enhanced with adaptive Sharpness-Aware Minimization (FedSAM) and Stochastic Weight Averaging (SWA), pretrained on Endo700k, and evaluated on Semantic Segmentation, Action Triplet Recognition, and Surgical Phase Recognition tasks.

Result: Integrating adaptive FedSAM improves pretraining and reduces reconstruction loss per patch; FL-EndoViT's performance in surgical tasks is comparable to CEN-EndoViT, with advantages in limited data scenarios and for large dataset action triplet recognition.

Conclusion: Federated learning shows promise for privacy-preserving training of surgical foundation models, highlighting the need for adapted methods like FedSAM for effective collaboration and indicating future exploration in video-based models to enhance capabilities in practical surgical settings.

Abstract: Purpose: In this study, we investigate the training of foundation models
using federated learning to address data-sharing limitations and enable
collaborative model training without data transfer for minimally invasive
surgery. Methods: Inspired by the EndoViT study, we adapt the Masked
Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware
Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is
pretrained on the Endo700k dataset collection and later fine-tuned and
evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,
and Surgical Phase Recognition. Results: Our findings demonstrate that
integrating adaptive FedSAM into the federated MAE approach improves
pretraining, leading to a reduction in reconstruction loss per patch. The
application of FL-EndoViT in surgical downstream tasks results in performance
comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over
CEN-EndoViT in surgical scene segmentation when data is limited and in action
triplet recognition when large datasets are used. Conclusion: These findings
highlight the potential of federated learning for privacy-preserving training
of surgical foundation models, offering a robust and generalizable solution for
surgical data science. Effective collaboration requires adapting federated
learning methods, such as the integration of FedSAM, which can accommodate the
inherent data heterogeneity across institutions. In future, exploring FL in
video-based models may enhance these capabilities by incorporating
spatiotemporal dynamics crucial for real-world surgical environments.

</details>


### [139] [SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting Harvest and Storage Characteristics of Sugar Beets](https://arxiv.org/abs/2504.16684)
*Gerardus Croonen,Andreas Trondl,Julia Simon,Daniel Steininger*

Main category: cs.CV

TL;DR: This paper presents a novel method for detecting and segmenting sugar beets in images to improve quality assurance in sugar production.


<details>
  <summary>Details</summary>
Motivation: The study addresses the issue of sugar loss in stored sugar beets due to microorganisms and excess vegetation, with the aim of enhancing efficiency in sugar processing through automated visual inspection.

Method: The authors developed a high-quality annotated dataset and implemented a two-stage method for detection, semantic segmentation, and mass estimation of sugar beets using monocular RGB images, conducting extensive ablation experiments across various parameters.

Result: The experiments yielded a mean Average Precision (mAP50-95) of 98.8 for sugar-beet detection and a mean Intersection over Union (mIoU) of 64.0 for the best segmentation model.

Conclusion: The findings demonstrate the effectiveness of the proposed method in accurately detecting and segmenting sugar beets, contributing to improved quality assurance in sugar production.

Abstract: While sugar beets are stored prior to processing, they lose sugar due to
factors such as microorganisms present in adherent soil and excess vegetation.
Their automated visual inspection promises to aide in quality assurance and
thereby increase efficiency throughout the processing chain of sugar
production. In this work, we present a novel high-quality annotated dataset and
two-stage method for the detection, semantic segmentation and mass estimation
of post-harvest and post-storage sugar beets in monocular RGB images. We
conduct extensive ablation experiments for the detection of sugar beets and
their fine-grained semantic segmentation regarding damages, rot, soil adhesion
and excess vegetation. For these tasks, we evaluate multiple image sizes, model
architectures and encoders, as well as the influence of environmental
conditions. Our experiments show an mAP50-95 of 98.8 for sugar-beet detection
and an mIoU of 64.0 for the best-performing segmentation model.

</details>


### [140] [Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light](https://arxiv.org/abs/2504.16922)
*Ali Hassani,Fengzhe Zhou,Aditya Kane,Jiannan Huang,Chieh-Yun Chen,Min Shi,Steven Walton,Markus Hoehnerbach,Vijay Thakkar,Michael Isaev,Qinsheng Zhang,Bing Xu,Haicheng Wu,Wen-mei Hwu,Ming-Yu Liu,Humphrey Shi*

Main category: cs.CV

TL;DR: The paper develops Generalized Neighborhood Attention (GNA) to improve speedup in sparse attention mechanisms, achieving significant speedup in end-to-end processing for generative models with no fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Sparse attention mechanisms need reliable performance improvements to overcome the inefficiencies of self-attention, critical for state-of-the-art models in computer vision bound by O(n^2) complexity.

Method: The authors introduce GNA to describe various attention patterns and create a simulator to realistically estimate speedup, implementing GNA on top of an NVIDIA Blackwell architecture FMHA kernel.

Result: GNA achieves maximum theoretical speedup in block-sparse cases, reaching 1.3 petaFLOPs/second in FP16, and offers 28% to 46% end-to-end speedup in generative models like Cosmos-7B without fine-tuning.

Conclusion: The GNA framework provides effective sparsity to attention mechanisms and will be made accessible through open-source contributions to the NATTEN project.

Abstract: Many sparse attention mechanisms such as Neighborhood Attention have
typically failed to consistently deliver speedup over the self attention
baseline. This is largely due to the level of complexity in attention
infrastructure, and the rapid evolution of AI hardware architecture. At the
same time, many state-of-the-art foundational models, particularly in computer
vision, are heavily bound by attention, and need reliable sparsity to escape
the O(n^2) complexity. In this paper, we study a class of promising sparse
attention mechanisms that focus on locality, and aim to develop a better
analytical model of their performance improvements. We first introduce
Generalized Neighborhood Attention (GNA), which can describe sliding window,
strided sliding window, and blocked attention. We then consider possible design
choices in implementing these approaches, and create a simulator that can
provide much more realistic speedup upper bounds for any given setting.
Finally, we implement GNA on top of a state-of-the-art fused multi-headed
attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in
CUTLASS. Our implementation can fully realize the maximum speedup theoretically
possible in many perfectly block-sparse cases, and achieves an effective
utilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA
configurations into off-the-shelf generative models, such as Cosmos-7B,
HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end
speedup on B200 without any fine-tuning. We will open source our simulator and
Blackwell kernels directly through the NATTEN project.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [141] [Towards a Distributed Federated Learning Aggregation Placement using Particle Swarm Intelligence](https://arxiv.org/abs/2504.16227)
*Amir Ali-Pour,Sadra Bekrani,Laya Samizadeh,Julien Gascon-Samson*

Main category: cs.DC

TL;DR: The paper introduces Flag-Swap, a Particle Swarm Optimization method for hierarchical semi-decentralized federated learning that optimizes aggregation placement with minimal reliance on systematic data exchange, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods in hierarchical semi-decentralized federated learning, which require constant monitoring of node performance and resource consumption.

Method: Flag-Swap utilizes a Particle Swarm Optimization approach to optimize aggregation placement based on processing delay rather than systematic data exchange.

Result: Simulation results indicate that Flag-Swap can achieve faster optimal placement in scenarios with many clients, with about 43% and 32% improvements in total processing time compared to random and uniform placement strategies, respectively.

Conclusion: Flag-Swap demonstrates superior performance over traditional deterministic placement strategies in hierarchical semi-decentralized federated learning environments.

Abstract: Federated learning has become a promising distributed learning concept with
extra insurance on data privacy. Extensive studies on various models of
Federated learning have been done since the coinage of its term. One of the
important derivatives of federated learning is hierarchical semi-decentralized
federated learning, which distributes the load of the aggregation task over
multiple nodes and parallelizes the aggregation workload at the breadth of each
level of the hierarchy. Various methods have also been proposed to perform
inter-cluster and intra-cluster aggregation optimally. Most of the solutions,
nonetheless, require monitoring the nodes' performance and resource consumption
at each round, which necessitates frequently exchanging systematic data. To
optimally perform distributed aggregation in SDFL with minimal reliance on
systematic data, we propose Flag-Swap, a Particle Swarm Optimization (PSO)
method that optimizes the aggregation placement according only to the
processing delay. Our simulation results show that PSO-based placement can find
the optimal placement relatively fast, even in scenarios with many clients as
candidates for aggregation. Our real-world docker-based implementation of
Flag-Swap over the recently emerged FL framework shows superior performance
compared to black-box-based deterministic placement strategies, with about 43%
minutes faster than random placement, and 32% minutes faster than uniform
placement, in terms of total processing time.

</details>


### [142] [DP2FL: Dual Prompt Personalized Federated Learning in Foundation Models](https://arxiv.org/abs/2504.16357)
*Ying Chang,Xiaohu Shi,Xiaohui Zhao,Zhaohuang Chen,Deyin Ma*

Main category: cs.DC

TL;DR: The paper introduces the Dual Prompt Personalized Federated Learning (DP2FL) framework to enhance personalized federated learning by leveraging foundation models and enabling seamless integration of new clients.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the performance of personalized federated learning on heterogeneous client data distributions while addressing issues related to limited local data and challenges in integrating new clients.

Method: The method proposed is the DP2FL framework, which incorporates dual prompts and an adaptive aggregation strategy to combine global task awareness with local data insights, allowing local models to generalize effectively.

Result: The experimental results demonstrate that DP2FL's design and strategy enhance prediction accuracy in highly heterogeneous environments and enable seamless integration of new clients without retraining.

Conclusion: DP2FL effectively addresses the challenges of personalized federated learning, showcasing the potential of foundation models in improving training outcomes and expanding the federated learning framework's adaptability.

Abstract: Personalized federated learning (PFL) has garnered significant attention for
its ability to address heterogeneous client data distributions while preserving
data privacy. However, when local client data is limited, deep learning models
often suffer from insufficient training, leading to suboptimal performance.
Foundation models, such as CLIP (Contrastive Language-Image Pretraining),
exhibit strong feature extraction capabilities and can alleviate this issue by
fine-tuning on limited local data. Despite their potential, foundation models
are rarely utilized in federated learning scenarios, and challenges related to
integrating new clients remain largely unresolved. To address these challenges,
we propose the Dual Prompt Personalized Federated Learning (DP2FL) framework,
which introduces dual prompts and an adaptive aggregation strategy. DP2FL
combines global task awareness with local data-driven insights, enabling local
models to achieve effective generalization while remaining adaptable to
specific data distributions. Moreover, DP2FL introduces a global model that
enables prediction on new data sources and seamlessly integrates newly added
clients without requiring retraining. Experimental results in highly
heterogeneous environments validate the effectiveness of DP2FL's prompt design
and aggregation strategy, underscoring the advantages of prediction on novel
data sources and demonstrating the seamless integration of new clients into the
federated learning framework.

</details>


### [143] [Simplified Swarm Learning Framework for Robust and Scalable Diagnostic Services in Cancer Histopathology](https://arxiv.org/abs/2504.16732)
*Yanjie Wu,Yuhao Ji,Saiho Lee,Juniad Akram,Ali Braytee,Ali Anaissi*

Main category: cs.DC

TL;DR: This paper presents a Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework, addressing healthcare data challenges through decentralized machine learning without blockchain, ensuring privacy and model accuracy.


<details>
  <summary>Details</summary>
Motivation: Healthcare data complexities, including privacy concerns, imbalanced datasets, and interoperability issues, require innovative machine learning solutions.

Method: The P2P-SL Framework eliminates blockchain dependencies, utilizing lightweight peer-to-peer communication for model synchronization while maintaining data privacy, applied to cancer histopathology with optimized pre-trained models.

Result: Extensive experiments show the framework effectively handles imbalanced and biased datasets, achieving performance comparable to centralized models while preserving privacy.

Conclusion: The P2P-SL Framework offers a scalable, accessible, and efficient solution for privacy-sensitive diagnostic applications, advancing machine learning in healthcare.

Abstract: The complexities of healthcare data, including privacy concerns, imbalanced
datasets, and interoperability issues, necessitate innovative machine learning
solutions. Swarm Learning (SL), a decentralized alternative to Federated
Learning, offers privacy-preserving distributed training, but its reliance on
blockchain technology hinders accessibility and scalability. This paper
introduces a \textit{Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework}
tailored for resource-constrained environments. By eliminating blockchain
dependencies and adopting lightweight peer-to-peer communication, the proposed
framework ensures robust model synchronization while maintaining data privacy.
Applied to cancer histopathology, the framework integrates optimized
pre-trained models, such as TorchXRayVision, enhanced with DenseNet decoders,
to improve diagnostic accuracy. Extensive experiments demonstrate the
framework's efficacy in handling imbalanced and biased datasets, achieving
comparable performance to centralized models while preserving privacy. This
study paves the way for democratizing advanced machine learning in healthcare,
offering a scalable, accessible, and efficient solution for privacy-sensitive
diagnostic applications.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [144] [Digital Kitchen Remodeling: Editing and Relighting Intricate Indoor Scenes from a Single Panorama](https://arxiv.org/abs/2504.16086)
*Guanzhou Ji,Azadeh O. Sawyer,Srinivasa G. Narasimhan*

Main category: cs.GR

TL;DR: A novel virtual staging application for kitchen remodeling using HDR panoramas to enhance realism and allow scene editing and relighting.


<details>
  <summary>Details</summary>
Motivation: To create a realistic virtual staging tool for kitchen remodeling that utilizes advanced HDR panoramic photography for better scene visualization.

Method: The application pipeline involves capturing HDR panoramas, generating kitchen layouts automatically, and utilizing an editable rendering pipeline for material and lighting adjustments.

Result: The introduction of a Pano-Pano HDR dataset with 141 paired indoor and outdoor panoramas and a method for low-cost photometric calibration for HDR photography.

Conclusion: The application provides a flexible and realistic platform for kitchen remodeling visualization, showcasing the potential of HDR technology in virtual staging.

Abstract: We present a novel virtual staging application for kitchen remodeling from a
single panorama. To ensure the realism of the virtual rendered scene, we
capture real-world High Dynamic Range (HDR) panoramas and recover the absolute
scene radiance for high-quality scene relighting. Our application pipeline
consists of three key components: (1) HDR photography for capturing paired
indoor and outdoor panoramas, (2) automatic kitchen layout generation with new
kitchen components, and (3) an editable rendering pipeline that flexibly edits
scene materials and relights the new virtual scene with global illumination.
Additionally, we contribute a novel Pano-Pano HDR dataset with 141 paired
indoor and outdoor panoramas and present a low-cost photometric calibration
method for panoramic HDR photography.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [145] [Physics-Informed Inference Time Scaling via Simulation-Calibrated Scientific Machine Learning](https://arxiv.org/abs/2504.16172)
*Zexi Fan,Yan Sun,Shihao Yang,Yiping Lu*

Main category: math.NA

TL;DR: SCaSML is a new physics-informed framework that enhances predictions of scientific machine learning by dynamically refining solutions to high-dimensional PDEs during inference, reducing errors by 20-50%.


<details>
  <summary>Details</summary>
Motivation: There are significant computational challenges in solving high-dimensional PDEs, and existing SciML techniques often exhibit bias and overlook important physical insights.

Method: SCaSML utilizes a physics-informed framework that refines predictions during inference by enforcing physical laws, employing Monte Carlo solvers based on new derived physical laws to quantify systematic errors.

Result: Numerical analysis shows improved convergence rates, and experiments demonstrate that SCaSML reduces prediction errors by 20-50% compared to traditional surrogate models.

Conclusion: SCaSML is the first algorithm to effectively refine approximated solutions to high-dimensional PDEs during inference, enhancing the accuracy of SciML techniques.

Abstract: High-dimensional partial differential equations (PDEs) pose significant
computational challenges across fields ranging from quantum chemistry to
economics and finance. Although scientific machine learning (SciML) techniques
offer approximate solutions, they often suffer from bias and neglect crucial
physical insights. Inspired by inference-time scaling strategies in language
models, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML),
a physics-informed framework that dynamically refines and debiases the SCiML
predictions during inference by enforcing the physical laws. SCaSML leverages
derived new physical laws that quantifies systematic errors and employs Monte
Carlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to
dynamically correct the prediction. Both numerical and theoretical analysis
confirms enhanced convergence rates via compute-optimal inference methods. Our
numerical experiments demonstrate that SCaSML reduces errors by 20-50% compared
to the base surrogate model, establishing it as the first algorithm to refine
approximated solutions to high-dimensional PDE during inference. Code of SCaSML
is available at https://github.com/Francis-Fan-create/SCaSML.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [146] [Exploring zero-shot structure-based protein fitness prediction](https://arxiv.org/abs/2504.16886)
*Arnav Sharma,Anthony Gitter*

Main category: q-bio.QM

TL;DR: The paper examines zero-shot predictions of protein sequence changes' fitness using machine learning and explores the impact of various modeling choices on accuracy, particularly focusing on disordered protein regions.


<details>
  <summary>Details</summary>
Motivation: The ability to predict protein fitness changes without additional labeled data supports practical applications in genetic variant interpretation and protein engineering.

Method: Empirical assessment of various modeling choices in structure-based prediction models and their effects on predicting fitness in disordered protein regions.

Result: The study confirms that matching protein structures to fitness assays is crucial and that predicted structures for disordered regions can mislead predictions; simple multi-modal ensembles perform well on the ProteinGym benchmark.

Conclusion: Zero-shot fitness prediction models face challenges in disordered regions, but careful modeling choices and ensemble methods can enhance predictive performance.

Abstract: The ability to make zero-shot predictions about the fitness consequences of
protein sequence changes with pre-trained machine learning models enables many
practical applications. Such models can be applied for downstream tasks like
genetic variant interpretation and protein engineering without additional
labeled data. The advent of capable protein structure prediction tools has led
to the availability of orders of magnitude more precomputed predicted
structures, giving rise to powerful structure-based fitness prediction models.
Through our experiments, we assess several modeling choices for structure-based
models and their effects on downstream fitness prediction. Zero-shot fitness
prediction models can struggle to assess the fitness landscape within
disordered regions of proteins, those that lack a fixed 3D structure. We
confirm the importance of matching protein structures to fitness assays and
find that predicted structures for disordered regions can be misleading and
affect predictive performance. Lastly, we evaluate an additional
structure-based model on the ProteinGym substitution benchmark and show that
simple multi-modal ensembles are strong baselines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [147] [Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases](https://arxiv.org/abs/2504.16273)
*Joseph Lee,Tianqi Shang,Jae Young Baik,Duy Duong-Tran,Shu Yang,Lingyao Li,Li Shen*

Main category: cs.AI

TL;DR: LLMs show promise in emergency department triage, highlighting robustness and bias concerns through intersectional analysis.


<details>
  <summary>Details</summary>
Motivation: To explore the application of LLMs in triage, which remains underexplored despite their potential in clinical decision support.

Method: A systematic investigation of LLM capabilities focusing on robustness to shifts and missing data, as well as counterfactual analysis of biases based on sex and race.

Result: LLMs demonstrate superior robustness and reveal demographic preferences, particularly sex-based differences pronounced in certain racial groups.

Conclusion: LLMs encode demographic preferences that may affect clinical outcomes, necessitating careful consideration of biases in their deployment.

Abstract: Large Language Models (LLMs) have shown promise in clinical decision support,
yet their application to triage remains underexplored. We systematically
investigate the capabilities of LLMs in emergency department triage through two
key dimensions: (1) robustness to distribution shifts and missing data, and (2)
counterfactual analysis of intersectional biases across sex and race. We assess
multiple LLM-based approaches, ranging from continued pre-training to
in-context learning, as well as machine learning approaches. Our results
indicate that LLMs exhibit superior robustness, and we investigate the key
factors contributing to the promising LLM-based approaches. Furthermore, in
this setting, we identify gaps in LLM preferences that emerge in particular
intersections of sex and race. LLMs generally exhibit sex-based differences,
but they are most pronounced in certain racial groups. These findings suggest
that LLMs encode demographic preferences that may emerge in specific clinical
contexts or particular combinations of characteristics.

</details>


### [148] [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
*Aniketh Garikaparthi,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.AI

TL;DR: The paper introduces IRIS, an open-source platform for enhancing scientific hypothesis generation using LLMs, focusing on transparency and user control.


<details>
  <summary>Details</summary>
Motivation: To explore how large language models can accelerate scientific discovery, specifically in generating novel hypotheses, while addressing deficiencies in existing approaches regarding transparency and steerability.

Method: IRIS combines a Human-in-the-loop approach with features such as adaptive test-time compute expansion using Monte Carlo Tree Search, a fine-grained feedback mechanism, and query-based literature synthesis.

Result: A user study conducted with researchers from various fields demonstrated the effectiveness of the IRIS system in improving the ideation process.

Conclusion: IRIS empowers researchers with enhanced control and insight in scientific ideation, and the code is made freely available to support further research.

Abstract: The rapid advancement in capabilities of large language models (LLMs) raises
a pivotal question: How can LLMs accelerate scientific discovery? This work
tackles the crucial first stage of research, generating novel hypotheses. While
recent work on automated hypothesis generation focuses on multi-agent
frameworks and extending test-time compute, none of the approaches effectively
incorporate transparency and steerability through a synergistic
Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:
Interactive Research Ideation System, an open-source platform designed for
researchers to leverage LLM-assisted scientific ideation. IRIS incorporates
innovative features to enhance ideation, including adaptive test-time compute
expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,
and query-based literature synthesis. Designed to empower researchers with
greater control and insight throughout the ideation process. We additionally
conduct a user study with researchers across diverse disciplines, validating
the effectiveness of our system in enhancing ideation. We open-source our code
at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System

</details>


### [149] [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)
*Ivan Moshkov,Darragh Hanley,Ivan Sorokin,Shubham Toshniwal,Christof Henkel,Benedikt Schifferer,Wei Du,Igor Gitman*

Main category: cs.AI

TL;DR: This paper discusses the winning approach to the AI Mathematical Olympiad - Progress Prize 2, which involves a new dataset and methodology for improved mathematical reasoning models.


<details>
  <summary>Details</summary>
Motivation: The aim is to build advanced mathematical reasoning models that can solve complex math problems, particularly at the olympiad level.

Method: The approach consists of creating a large dataset of math problems and solutions, integrating code execution with long reasoning models through iterative training, and developing a solution selection pipeline (GenSelect) to enhance model performance.

Result: The proposed methods resulted in 1.7M high-quality Tool-Integrated Reasoning solutions and demonstrated significant improvements on mathematical reasoning benchmarks compared to baseline methods.

Conclusion: The combination of the new dataset, integration method, and generative solution selection leads to state-of-the-art results in mathematical reasoning, with resources made available for further research.

Abstract: This paper presents our winning submission to the AI Mathematical Olympiad -
Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art
mathematical reasoning models relies on three key pillars. First, we create a
large-scale dataset comprising 540K unique high-quality math problems,
including olympiad-level problems, and their 3.2M long-reasoning solutions.
Second, we develop a novel method to integrate code execution with long
reasoning models through iterative training, generation, and quality filtering,
resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we
create a pipeline to train models to select the most promising solution from
many candidates. We show that such generative solution selection (GenSelect)
can significantly improve upon majority voting baseline. Combining these ideas,
we train a series of models that achieve state-of-the-art results on
mathematical reasoning benchmarks. To facilitate further research, we release
our code, models, and the complete OpenMathReasoning dataset under a
commercially permissive license.

</details>


### [150] [A Framework for Objective-Driven Dynamical Stochastic Fields](https://arxiv.org/abs/2504.16115)
*Yibo Jacky Zhang,Sanmi Koyejo*

Main category: cs.AI

TL;DR: This paper proposes a theoretical framework for understanding complex systems known as intelligent fields, which exhibit goal-directed behaviors.


<details>
  <summary>Details</summary>
Motivation: The complexity of dynamical and stochastic systems makes it difficult to formulate theoretical models and applications for intelligent fields.

Method: Three fundamental principles—complete configuration, locality, and purposefulness—are proposed to structure a theoretical framework for intelligent fields and explore their design within artificial intelligence contexts.

Result: The proposed framework aims to facilitate future theoretical insights and practical advancements for manipulating objective-driven dynamical stochastic systems.

Conclusion: The investigation provides a foundational understanding necessary for leveraging the capabilities of intelligent fields in real-world applications.

Abstract: Fields offer a versatile approach for describing complex systems composed of
interacting and dynamic components. In particular, some of these dynamical and
stochastic systems may exhibit goal-directed behaviors aimed at achieving
specific objectives, which we refer to as $\textit{intelligent fields}$.
However, due to their inherent complexity, it remains challenging to develop a
formal theoretical description of such systems and to effectively translate
these descriptions into practical applications. In this paper, we propose three
fundamental principles -- complete configuration, locality, and purposefulness
-- to establish a theoretical framework for understanding intelligent fields.
Moreover, we explore methodologies for designing such fields from the
perspective of artificial intelligence applications. This initial investigation
aims to lay the groundwork for future theoretical developments and practical
advances in understanding and harnessing the potential of such objective-driven
dynamical stochastic fields.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [151] [Probabilistic Emulation of the Community Radiative Transfer Model Using Machine Learning](https://arxiv.org/abs/2504.16192)
*Lucas Howard,Aneesh C. Subramanian,Gregory Thompson,Benjamin Johnson,Thomas Auligne*

Main category: physics.ao-ph

TL;DR: A machine learning-based probabilistic emulator for the Community Radiative Transfer Model (CRTM) improves computational efficiency in weather forecasting using satellite data.


<details>
  <summary>Details</summary>
Motivation: To enhance weather forecast skill by utilizing satellite observations more effectively, as existing computational methods are limited by efficiency and often leave data unused for assimilation.

Method: A neural network emulator was developed to predict brightness temperatures from the CRTM, focusing on improving the computational cost and efficiency for assimilation into forecast systems.

Result: The emulator achieves a RMSE of 0.3 K for brightness temperature predictions across all channels, with less than 0.1 K RMSE for clear sky conditions in 9 out of 10 infrared channels. The error predictions show reliability across diverse conditions.

Conclusion: The trained emulator not only replicates the relevant physics but also provides confidence in its performance with new data, making it a promising tool for more efficient weather data assimilation.

Abstract: The continuous improvement in weather forecast skill over the past several
decades is largely due to the increasing quantity of available satellite
observations and their assimilation into operational forecast systems.
Assimilating these observations requires observation operators in the form of
radiative transfer models. Significant efforts have been dedicated to enhancing
the computational efficiency of these models. Computational cost remains a
bottleneck, and a large fraction of available data goes unused for
assimilation. To address this, we used machine learning to build an efficient
neural network based probabilistic emulator of the Community Radiative Transfer
Model (CRTM), applied to the GOES Advanced Baseline Imager. The trained NN
emulator predicts brightness temperatures output by CRTM and the corresponding
error with respect to CRTM. RMSE of the predicted brightness temperature is 0.3
K averaged across all channels. For clear sky conditions, the RMSE is less than
0.1 K for 9 out of 10 infrared channels. The error predictions are generally
reliable across a wide range of conditions. Explainable AI methods demonstrate
that the trained emulator reproduces the relevant physics, increasing
confidence that the model will perform well when presented with new data.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [152] [4D Multimodal Co-attention Fusion Network with Latent Contrastive Alignment for Alzheimer's Diagnosis](https://arxiv.org/abs/2504.16798)
*Yuxiang Wei,Yanteng Zhang,Xi Xiao,Tianyang Wang,Xiao Wang,Vince D. Calhoun*

Main category: cs.MM

TL;DR: M2M-AlignNet is proposed as a novel multimodal co-attention network for early Alzheimer's diagnosis that integrates sMRI and fMRI data effectively.


<details>
  <summary>Details</summary>
Motivation: The integration of neuroimaging data with cognitive behavioral scores enhances Alzheimer's disease diagnosis, but challenges arise from the intrinsic heterogeneity across modalities that complicates feature fusion.

Method: The approach utilizes a geometry-aware multimodal co-attention framework with a multi-patch-to-multi-patch contrastive loss function for aligning and reducing discrepancies between fMRI and sMRI data, along with a latent-as-query co-attention module for discovering fusion patterns autonomously.

Result: Extensive experiments demonstrate the effectiveness of M2M-AlignNet in aligning fMRI and sMRI as biomarkers for Alzheimer's disease, confirming stronger diagnostic sensitivity.

Conclusion: The proposed method successfully bridges the heterogeneity in multimodal neuroimaging, improving early diagnosis of Alzheimer's disease through enhanced feature fusion mechanisms.

Abstract: Multimodal neuroimaging provides complementary structural and functional
insights into both human brain organization and disease-related dynamics.
Recent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's
disease (AD) through synergistic integration of neuroimaging data (e.g., sMRI,
fMRI) with behavioral cognitive scores tabular data biomarkers. However, the
intrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI
dynamics vs. 3D anatomical sMRI structure) presents critical challenges for
discriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a
geometry-aware multimodal co-attention network with latent alignment for early
AD diagnosis using sMRI and fMRI. At the core of our approach is a
multi-patch-to-multi-patch (M2M) contrastive loss function that quantifies and
reduces representational discrepancies via geometry-weighted patch
correspondence, explicitly aligning fMRI components across brain regions with
their sMRI structural substrates without one-to-one constraints. Additionally,
we propose a latent-as-query co-attention module to autonomously discover
fusion patterns, circumventing modality prioritization biases while minimizing
feature redundancy. We conduct extensive experiments to confirm the
effectiveness of our method and highlight the correspondance between fMRI and
sMRI as AD biomarkers.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [153] [Circinus: Efficient Query Planner for Compound ML Serving](https://arxiv.org/abs/2504.16397)
*Banruo Liu,Wei-Yu Lin,Minghao Fang,Yihan Jiang,Fan Lai*

Main category: cs.DB

TL;DR: Circinus is an SLO-aware query planner that improves service goodput for compound AI applications, optimizing for latency, accuracy, and cost in real-time deployments.


<details>
  <summary>Details</summary>
Motivation: The complexity of diverse SLO requirements and varying capabilities across edge and cloud infrastructure leads to significant challenges in planning for compound AI workloads, necessitating a solution that can efficiently manage this complexity.

Method: Circinus decomposes multi-query planning and multi-dimensional SLO objectives, reducing the search space by exploiting plan similarities and enhancing efficiency with a precision-aware plan profiler that strategically applies early stopping based on performance estimates.

Result: Circinus improves service goodput by 3.2-5.0 times, accelerates query planning by 4.2-5.8 times, achieving query responses in seconds and reduces deployment costs by 3.2-4.0 times compared to state-of-the-art methods.

Conclusion: Circinus significantly enhances the efficiency and effectiveness of managing compound AI workloads, making it suitable for cost-effective real-time applications.

Abstract: The rise of compound AI serving -- integrating multiple operators in a
pipeline that may span edge and cloud tiers -- enables end-user applications
such as autonomous driving, generative AI-powered meeting companions, and
immersive gaming. Achieving high service goodput -- i.e., meeting service level
objectives (SLOs) for pipeline latency, accuracy, and costs -- requires
effective planning of operator placement, configuration, and resource
allocation across infrastructure tiers. However, the diverse SLO requirements,
varying edge capabilities, and high query volumes create an enormous planning
search space, rendering current solutions fundamentally limited for real-time
serving and cost-efficient deployments.
  This paper presents Circinus, an SLO-aware query planner for large-scale
compound AI workloads. Circinus novelly decomposes multi-query planning and
multi-dimensional SLO objectives while preserving global decision quality. By
exploiting plan similarities within and across queries, it significantly
reduces search steps. It further improves per-step efficiency with a
precision-aware plan profiler that incrementally profiles and strategically
applies early stopping based on imprecise estimates of plan performance. At
scale, Circinus selects query-plan combinations to maximize global SLO goodput.
Evaluations in real-world settings show that Circinus improves service goodput
by 3.2-5.0$\times$, accelerates query planning by 4.2-5.8$\times$, achieving
query response in seconds, while reducing deployment costs by 3.2-4.0$\times$
over state of the arts even in their intended single-tier deployments.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [154] [Confidence Sequences for Generalized Linear Models via Regret Analysis](https://arxiv.org/abs/2504.16555)
*Eugenio Clerico,Hamish Flynn,Wojciech Kotłowski,Gergely Neu*

Main category: math.ST

TL;DR: The paper introduces a methodology for constructing confidence sets for parameters of statistical models through a reduction to sequential prediction in generalized linear models (GLMs).


<details>
  <summary>Details</summary>
Motivation: The goal is to develop a robust and flexible approach to constructing confidence sets that can incorporate various types of estimators and leverage existing online algorithms.

Method: The methodology consists of online-to-confidence-set conversions, which translate the problem of proving statistical claims into an algorithmic question. This includes both analytical and algorithmic conversion schemes allowing different confidence set constructions.

Result: The methodology successfully unifies existing confidence set constructions under one framework while also introducing new types of confidence sets that were not previously discussed in literature.

Conclusion: This work demonstrates that using sequential prediction techniques can provide comprehensive solutions for constructing confidence sets in GLMs, enhancing both the robustness and the applicability of statistical inference.

Abstract: We develop a methodology for constructing confidence sets for parameters of
statistical models via a reduction to sequential prediction. Our key
observation is that for any generalized linear model (GLM), one can construct
an associated game of sequential probability assignment such that achieving low
regret in the game implies a high-probability upper bound on the excess
likelihood of the true parameter of the GLM. This allows us to develop a scheme
that we call online-to-confidence-set conversions, which effectively reduces
the problem of proving the desired statistical claim to an algorithmic
question. We study two varieties of this conversion scheme: 1) analytical
conversions that only require proving the existence of algorithms with low
regret and provide confidence sets centered at the maximum-likelihood estimator
2) algorithmic conversions that actively leverage the output of the online
algorithm to construct confidence sets (and may be centered at other,
adaptively constructed point estimators). The resulting methodology recovers
all state-of-the-art confidence set constructions within a single framework,
and also provides several new types of confidence sets that were previously
unknown in the literature.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [155] [Comprehensive Evaluation of Quantitative Measurements from Automated Deep Segmentations of PSMA PET/CT Images](https://arxiv.org/abs/2504.16237)
*Obed Korshie Dzikunu,Amirhossein Toosi,Shadab Ahamed,Sara Harsini,Francois Benard,Xiaoxiao Li,Arman Rahmim*

Main category: eess.IV

TL;DR: The study evaluates deep-learning segmentation methods for PET/CT scans in prostate cancer using several quantitative metrics and introduces a new loss function, L1 weighted Dice Focal Loss (L1DFL).


<details>
  <summary>Details</summary>
Motivation: To improve quantitative measurements in the segmentation of PET/CT scans for prostate cancer beyond traditional assessments with the Dice Similarity Coefficient.

Method: Analyzed 380 PSMA targeted [18F]DCFPyL PET/CT scans, training U-Net variants with four loss functions including the proposed L1DFL, evaluating various quantitative metrics.

Result: Attention U-Net with L1DFL showed high correlation with ground truth (0.90-0.99 for SUVmax and TLA), while other models underperformed. L1DFL demonstrated superior performance for lesion count and total lesion activity.

Conclusion: L1DFL effectively reduces variability in quantifying clinical measures in prostate cancer PET/CT scans, highlighting its potential for enhanced segmentation accuracy.

Abstract: This study performs a comprehensive evaluation of quantitative measurements
as extracted from automated deep-learning-based segmentation methods, beyond
traditional Dice Similarity Coefficient assessments, focusing on six
quantitative metrics, namely SUVmax, SUVmean, total lesion activity (TLA),
tumor volume (TMTV), lesion count, and lesion spread. We analyzed 380
prostate-specific membrane antigen (PSMA) targeted [18F]DCFPyL PET/CT scans of
patients with biochemical recurrence of prostate cancer, training deep neural
networks, U-Net, Attention U-Net and SegResNet with four loss functions: Dice
Loss, Dice Cross Entropy, Dice Focal Loss, and our proposed L1 weighted Dice
Focal Loss (L1DFL). Evaluations indicated that Attention U-Net paired with
L1DFL achieved the strongest correlation with the ground truth (concordance
correlation = 0.90-0.99 for SUVmax and TLA), whereas models employing the Dice
Loss and the other two compound losses, particularly with SegResNet,
underperformed. Equivalence testing (TOST, alpha = 0.05, Delta = 20%) confirmed
high performance for SUV metrics, lesion count and TLA, with L1DFL yielding the
best performance. By contrast, tumor volume and lesion spread exhibited greater
variability. Bland-Altman, Coverage Probability, and Total Deviation Index
analyses further highlighted that our proposed L1DFL minimizes variability in
quantification of the ground truth clinical measures. The code is publicly
available at: https://github.com/ObedDzik/pca\_segment.git.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [156] [PCF-Grasp: Converting Point Completion to Geometry Feature to Enhance 6-DoF Grasp](https://arxiv.org/abs/2504.16320)
*Yaofeng Cheng,Fusheng Zha,Wei Guo,Pengfei Wang,Chao Zeng,Lining Sun,Chenguang Yang*

Main category: cs.RO

TL;DR: A novel 6-DoF grasping framework using point cloud completion improves robot grasp accuracy and success rates by integrating object shape features and a score filter for executable grasp proposals.


<details>
  <summary>Details</summary>
Motivation: Existing grasp methods based on 2.5D depth images produce incomplete point clouds, leading to low accuracy in grasping due to poor shape estimation. The aim is to enhance grasping techniques by emulating human geometric experience.

Method: The proposed framework converts point completion results into object shape features to train the 6-DoF grasp network. It includes a score filter to select executable grasp proposals based on the network's output, maintaining grasp quality across different camera viewpoints.

Result: The new framework demonstrates a 17.8% higher success rate in real-world experiments compared to the state-of-the-art methods, significantly improving the accuracy of grasp proposals.

Conclusion: The approach of utilizing complete point features alongside a selection filter enables better real-world grasping outcomes, showcasing the potential for higher robotic grasping efficiency.

Abstract: The 6-Degree of Freedom (DoF) grasp method based on point clouds has shown
significant potential in enabling robots to grasp target objects. However, most
existing methods are based on the point clouds (2.5D points) generated from
single-view depth images. These point clouds only have one surface side of the
object providing incomplete geometry information, which mislead the grasping
algorithm to judge the shape of the target object, resulting in low grasping
accuracy. Humans can accurately grasp objects from a single view by leveraging
their geometry experience to estimate object shapes. Inspired by humans, we
propose a novel 6-DoF grasping framework that converts the point completion
results as object shape features to train the 6-DoF grasp network. Here, point
completion can generate approximate complete points from the 2.5D points
similar to the human geometry experience, and converting it as shape features
is the way to utilize it to improve grasp efficiency. Furthermore, due to the
gap between the network generation and actual execution, we integrate a score
filter into our framework to select more executable grasp proposals for the
real robot. This enables our method to maintain a high grasp quality in any
camera viewpoint. Extensive experiments demonstrate that utilizing complete
point features enables the generation of significantly more accurate grasp
proposals and the inclusion of a score filter greatly enhances the credibility
of real-world robot grasping. Our method achieves a 17.8\% success rate higher
than the state-of-the-art method in real-world experiments.

</details>


### [157] [HERB: Human-augmented Efficient Reinforcement learning for Bin-packing](https://arxiv.org/abs/2504.16595)
*Gojko Perovic,Nuno Ferreira Duarte,Atabak Dehban,Gonçalo Teixeira,Egidio Falotico,José Santos-Victor*

Main category: cs.RO

TL;DR: The paper presents HERB, a human-augmented reinforcement learning framework aimed at efficiently packing irregular 3D objects using human demonstrations to improve object sequence and placement within a container.


<details>
  <summary>Details</summary>
Motivation: Efficient packing of irregular 3D objects is essential in logistics and robotics, yet traditional methods struggle due to shape variability and stability issues. Reinforcement Learning often faces inefficiencies when trained purely in simulations.

Method: The proposed framework, HERB, utilizes human demonstrations to learn optimal packing sequences while accounting for complex factors like space usage and object relationships. A placement algorithm is then trained using visual data to optimize object positioning.

Result: HERB outperforms both traditional geometric approaches and purely RL-based methods, improving packing efficiency and reducing latency. It demonstrates real-world usability through rigorous evaluations.

Conclusion: The integration of human expertise with reinforcement learning significantly enhances the performance of robotic systems in handling complex packing tasks.

Abstract: Packing objects efficiently is a fundamental problem in logistics, warehouse
automation, and robotics. While traditional packing solutions focus on
geometric optimization, packing irregular, 3D objects presents significant
challenges due to variations in shape and stability. Reinforcement
Learning~(RL) has gained popularity in robotic packing tasks, but training
purely from simulation can be inefficient and computationally expensive. In
this work, we propose HERB, a human-augmented RL framework for packing
irregular objects. We first leverage human demonstrations to learn the best
sequence of objects to pack, incorporating latent factors such as space
optimization, stability, and object relationships that are difficult to model
explicitly. Next, we train a placement algorithm that uses visual information
to determine the optimal object positioning inside a packing container. Our
approach is validated through extensive performance evaluations, analyzing both
packing efficiency and latency. Finally, we demonstrate the real-world
feasibility of our method on a robotic system. Experimental results show that
our method outperforms geometric and purely RL-based approaches by leveraging
human intuition, improving both packing robustness and adaptability. This work
highlights the potential of combining human expertise-driven RL to tackle
complex real-world packing challenges in robotic systems.

</details>


### [158] [Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator](https://arxiv.org/abs/2504.16680)
*Chenhao Li,Andreas Krause,Marco Hutter*

Main category: cs.RO

TL;DR: RWM-O is a model-based RL approach that enhances policy learning by incorporating uncertainty estimation, which improves generalization and safety in robotic control using only real-world data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of high sample complexity, safety concerns, and distributional shift in offline reinforcement learning for robotic control, particularly focusing on the limitations of existing model-based RL approaches that lack robust uncertainty estimation.

Method: The authors introduce the Offline Robotic World Model (RWM-O), which explicitly estimates epistemic uncertainty and integrates these estimates into policy optimization to penalize unreliable transitions.

Result: Experimental results demonstrate that RWM-O significantly improves generalization and safety in policy learning, allowing effective learning from real-world data without the need for a physics simulator.

Conclusion: RWM-O advances scalable and data-efficient reinforcement learning for robotics by enhancing policy robustness and stability through effective uncertainty management.

Abstract: Reinforcement Learning (RL) has demonstrated impressive capabilities in
robotic control but remains challenging due to high sample complexity, safety
concerns, and the sim-to-real gap. While offline RL eliminates the need for
risky real-world exploration by learning from pre-collected data, it suffers
from distributional shift, limiting policy generalization. Model-Based RL
(MBRL) addresses this by leveraging predictive models for synthetic rollouts,
yet existing approaches often lack robust uncertainty estimation, leading to
compounding errors in offline settings. We introduce Offline Robotic World
Model (RWM-O), a model-based approach that explicitly estimates epistemic
uncertainty to improve policy learning without reliance on a physics simulator.
By integrating these uncertainty estimates into policy optimization, our
approach penalizes unreliable transitions, reducing overfitting to model errors
and enhancing stability. Experimental results show that RWM-O improves
generalization and safety, enabling policy learning purely from real-world data
and advancing scalable, data-efficient RL for robotics.

</details>


### [159] [Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving](https://arxiv.org/abs/2504.16923)
*Jacob Levy,Jason Gibson,Bogdan Vlahov,Erica Tevere,Evangelos Theodorou,David Fridovich-Keil,Patrick Spieler*

Main category: cs.RO

TL;DR: The paper presents a framework for high-speed off-road autonomous driving that uses a Kalman filter for real-time adaptation combined with meta-learning to improve the dynamics model for diverse terrains.


<details>
  <summary>Details</summary>
Motivation: The challenges of complex terrain characteristics and modeling interactions between the vehicle and terrain hinder effective autonomous driving in off-road environments.

Method: A framework that integrates a Kalman filter-based online adaptation scheme with parameters learned through offline meta-learning to optimize the dynamics model.

Result: The proposed method demonstrates superior prediction accuracy, enhanced performance, and improved safety metrics over baseline approaches in extensive experiments, including real-world tests on an autonomous off-road vehicle.

Conclusion: The integration of meta-learning for dynamics model adaptation significantly enhances the reliability of autonomous systems in navigating varied and unfamiliar terrains.

Abstract: High-speed off-road autonomous driving presents unique challenges due to
complex, evolving terrain characteristics and the difficulty of accurately
modeling terrain-vehicle interactions. While dynamics models used in
model-based control can be learned from real-world data, they often struggle to
generalize to unseen terrain, making real-time adaptation essential. We propose
a novel framework that combines a Kalman filter-based online adaptation scheme
with meta-learned parameters to address these challenges. Offline meta-learning
optimizes the basis functions along which adaptation occurs, as well as the
adaptation parameters, while online adaptation dynamically adjusts the onboard
dynamics model in real time for model-based control. We validate our approach
through extensive experiments, including real-world testing on a full-scale
autonomous off-road vehicle, demonstrating that our method outperforms baseline
approaches in prediction accuracy, performance, and safety metrics,
particularly in safety-critical scenarios. Our results underscore the
effectiveness of meta-learned dynamics model adaptation, advancing the
development of reliable autonomous systems capable of navigating diverse and
unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [160] [Data-Assimilated Model-Based Reinforcement Learning for Partially Observed Chaotic Flows](https://arxiv.org/abs/2504.16588)
*Defne E. Ozan,Andrea Nóvoa,Luca Magri*

Main category: eess.SY

TL;DR: The paper presents a data-assimilated model-based reinforcement learning (DA-MBRL) framework to control turbulent flows in energy and transport sectors, overcoming challenges posed by chaotic dynamics and partial observability.


<details>
  <summary>Details</summary>
Motivation: To control turbulent flows in energy and transport applications, which is difficult due to chaotic dynamics and the requirement for full state information, often unavailable in experiments.

Method: The DA-MBRL framework uses a control-aware Echo State Network for predicting dynamics and integrates data assimilation with an Ensemble Kalman Filter for real-time state estimation, employing an off-policy actor-critic algorithm for learning optimal control strategies.

Result: The framework is tested on the Kuramoto-Sivashinsky equation, showing effective stabilization of spatiotemporally chaotic flow using noisy and partial measurements.

Conclusion: The proposed DA-MBRL framework successfully addresses the challenges of controlling turbulent flows with limited state information and demonstrates its efficacy through empirical testing.

Abstract: The goal of many applications in energy and transport sectors is to control
turbulent flows. However, because of chaotic dynamics and high dimensionality,
the control of turbulent flows is exceedingly difficult. Model-free
reinforcement learning (RL) methods can discover optimal control policies by
interacting with the environment, but they require full state information,
which is often unavailable in experimental settings. We propose a
data-assimilated model-based RL (DA-MBRL) framework for systems with partial
observability and noisy measurements. Our framework employs a control-aware
Echo State Network for data-driven prediction of the dynamics, and integrates
data assimilation with an Ensemble Kalman Filter for real-time state
estimation. An off-policy actor-critic algorithm is employed to learn optimal
control strategies from state estimates. The framework is tested on the
Kuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a
spatiotemporally chaotic flow from noisy and partial measurements.

</details>


### [161] [Learning Verifiable Control Policies Using Relaxed Verification](https://arxiv.org/abs/2504.16879)
*Puja Chaudhury,Alexander Estornell,Michael Everett*

Main category: eess.SY

TL;DR: The paper proposes a method for integrating formal verification into the training of learning-based control systems to ensure safety guarantees throughout runtime.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of post-training verification methods which can fail to establish safety guarantees if the trained policy does not meet specifications.

Method: The authors use differentiable reachability analysis and modify the loss function to include verification components during training.

Result: Numerical experiments on quadrotor and unicycle models demonstrate that the proposed approach results in learned control policies that meet specified reach-avoid and invariance properties.

Conclusion: Integrating verification into the training process enhances the safety and reliability of learning-based control systems.

Abstract: To provide safety guarantees for learning-based control systems, recent work
has developed formal verification methods to apply after training ends.
However, if the trained policy does not meet the specifications, or there is
conservatism in the verification algorithm, establishing these guarantees may
not be possible. Instead, this work proposes to perform verification throughout
training to ultimately aim for policies whose properties can be evaluated
throughout runtime with lightweight, relaxed verification algorithms. The
approach is to use differentiable reachability analysis and incorporate new
components into the loss function. Numerical experiments on a quadrotor model
and unicycle model highlight the ability of this approach to lead to learned
control policies that satisfy desired reach-avoid and invariance
specifications.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [162] [Breaking scaling relations with inverse catalysts: a machine learning exploration of trends in $\mathrm{CO_2}$ hydrogenation energy barriers](https://arxiv.org/abs/2504.16493)
*Luuk H. E. Kempen,Marius Juul Nielsen,Mie Andersen*

Main category: cond-mat.mtrl-sci

TL;DR: The paper presents a machine learning workflow for investigating transition states in CO2 conversion to methanol, focusing on indium oxide nanoclusters on Cu(111).


<details>
  <summary>Details</summary>
Motivation: To abate climate change and reduce reliance on fossil fuels by developing efficient catalysts for CO2 conversion, leveraging computational techniques to explore active sites effectively.

Method: The study employs a neural network-based machine learning interatomic potential to explore transition states of reaction steps at inverse catalysts, focusing on the formate intermediate over indium oxide nanoclusters supported on Cu(111).

Result: The approach achieves significant speedup compared to density functional theory, enabling the exploration of diverse active sites, and reveals structural differences in transition state geometries based on the location within the nanoclusters.

Conclusion: The findings suggest that the identified geometries break linear scaling relations, potentially explaining the superior catalytic performance of inverse catalysts noted in experiments.

Abstract: The conversion of $\mathrm{CO_2}$ into useful products such as methanol is a
key strategy for abating climate change and our dependence on fossil fuels.
Developing new catalysts for this process is costly and time-consuming and can
thus benefit from computational exploration of possible active sites. However,
this is complicated by the complexity of the materials and reaction networks.
Here, we present a workflow for exploring transition states of elementary
reaction steps at inverse catalysts, which is based on the training of a neural
network-based machine learning interatomic potential. We focus on the crucial
formate intermediate and its formation over nanoclusters of indium oxide
supported on Cu(111). The speedup compared to an approach purely based on
density functional theory allows us to probe a wide variety of active sites
found at nanoclusters of different sizes and stoichiometries. Analysis of the
obtained set of transition state geometries reveals different
structure--activity trends at the edge or interior of the nanoclusters.
Furthermore, the identified geometries allow for the breaking of linear scaling
relations, which could be a key underlying reason for the excellent catalytic
performance of inverse catalysts observed in experiments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [163] [LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval](https://arxiv.org/abs/2504.16121)
*Muhammad Rafsan Kabir,Rafeed Mohammad Sultan,Fuad Rahman,Mohammad Ruhul Amin,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.IR

TL;DR: Development of an efficient bilingual question-answering framework for regulatory documents using Retrieval Augmented Generation (RAG).


<details>
  <summary>Details</summary>
Motivation: To enhance the application of NLP techniques in legal and regulatory tasks, specifically in the context of Bangladesh Police Gazettes written in English and Bangla.

Method: An improved bilingual question-answering system utilizing Retrieval Augmented Generation (RAG) pipelines to increase accuracy in retrieving and generating responses for regulatory documents.

Result: The proposed RAG-based approach outperforms conventional RAG systems in retrieval performance and provides more precise answers, validated by evaluation on a diverse test set of Bangladesh Police Gazettes.

Conclusion: The system significantly improves access to legal information by enhancing the efficiency of searching government legal notices.

Abstract: Natural Language Processing (NLP) and computational linguistic techniques are
increasingly being applied across various domains, yet their use in legal and
regulatory tasks remains limited. To address this gap, we develop an efficient
bilingual question-answering framework for regulatory documents, specifically
the Bangladesh Police Gazettes, which contain both English and Bangla text. Our
approach employs modern Retrieval Augmented Generation (RAG) pipelines to
enhance information retrieval and response generation. In addition to
conventional RAG pipelines, we propose an advanced RAG-based approach that
improves retrieval performance, leading to more precise answers. This system
enables efficient searching for specific government legal notices, making legal
information more accessible. We evaluate both our proposed and conventional RAG
systems on a diverse test set on Bangladesh Police Gazettes, demonstrating that
our approach consistently outperforms existing methods across all evaluation
metrics.

</details>


### [164] [CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents](https://arxiv.org/abs/2504.16264)
*Francisco Valentini,Diego Kozlowski,Vincent Larivière*

Main category: cs.IR

TL;DR: This paper introduces CLIRudit, a dataset for evaluating cross-lingual information retrieval (CLIR) focusing on English queries and French documents, and benchmarks various retrieval methods on it.


<details>
  <summary>Details</summary>
Motivation: The need for effective cross-lingual academic search tools to help researchers access scholarly content in different languages, particularly in scenarios where the query language differs from the document language.

Method: The dataset is built using bilingual article metadata from the 'Erudit' platform and is evaluated with zero-shot retrieval methods including dense and sparse retrievers, machine translation, and multilingual retrievers.

Result: Large dense retrievers without cross-lingual training demonstrated zero-shot performance comparable to human translations, while sparse retrievers with document translation also achieved competitive results.

Conclusion: This research contributes to understanding cross-lingual academic retrieval and offers a framework for creating comparable datasets, promoting accessibility of scientific knowledge across languages.

Abstract: Cross-lingual information retrieval (CLIR) consists in finding relevant
documents in a language that differs from the language of the queries. This
paper presents CLIRudit, a new dataset created to evaluate cross-lingual
academic search, focusing on English queries and French documents. The dataset
is built using bilingual article metadata from \'Erudit, a Canadian publishing
platform, and is designed to represent scenarios in which researchers search
for scholarly content in languages other than English. We perform a
comprehensive benchmarking of different zero-shot first-stage retrieval methods
on the dataset, including dense and sparse retrievers, query and document
machine translation, and state-of-the-art multilingual retrievers. Our results
show that large dense retrievers, not necessarily trained for the cross-lingual
retrieval task, can achieve zero-shot performance comparable to using ground
truth human translations, without the need for machine translation. Sparse
retrievers, such as BM25 or SPLADE, combined with document translation, show
competitive results, providing an efficient alternative to large dense models.
This research advances the understanding of cross-lingual academic information
retrieval and provides a framework that others can use to build comparable
datasets across different languages and disciplines. By making the dataset and
code publicly available, we aim to facilitate further research that will help
make scientific knowledge more accessible across language barriers.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [165] [Intelligent Depression Prevention via LLM-Based Dialogue Analysis: Overcoming the Limitations of Scale-Dependent Diagnosis through Precise Emotional Pattern Recognition](https://arxiv.org/abs/2504.16504)
*Zhenguang Zhong,Zhixuan Wang*

Main category: q-bio.NC

TL;DR: The paper introduces an AI-powered depression prevention system that utilizes large language models to analyze real-time conversation cues for more accurate depression assessment, outperforming traditional questionnaires in precision and reducing misdiagnosis rates.


<details>
  <summary>Details</summary>
Motivation: Current depression screening methods rely on standardized questionnaires, which have high misdiagnosis rates due to their reliance on static symptom counts and patient recall, necessitating a more effective alternative.

Method: The system leverages large language models to monitor natural dialogue and identify linguistic features indicative of depression. It includes continuous monitoring, adaptive risk stratification, and personalized intervention strategies.

Result: The system achieves 89% precision in detecting depression-indicative features, reduces false positives by 41%, and shows 2.3x higher adherence rates to interventions compared to traditional methods. Clinical validation reveals it identifies 92% of at-risk cases missed by standard scales.

Conclusion: The research demonstrates that conversational AI represents a significant advancement in mental health assessment, moving from episodic, scale-dependent diagnosis to a continuous, nuanced approach for monitoring emotional well-being.

Abstract: Existing depression screening predominantly relies on standardized
questionnaires (e.g., PHQ-9, BDI), which suffer from high misdiagnosis rates
(18-34% in clinical studies) due to their static, symptom-counting nature and
susceptibility to patient recall bias. This paper presents an AI-powered
depression prevention system that leverages large language models (LLMs) to
analyze real-time conversational cues--including subtle emotional expressions
(e.g., micro-sentiment shifts, self-referential language patterns)--for more
accurate and dynamic mental state assessment. Our system achieves three key
innovations: (1) Continuous monitoring through natural dialogue, detecting
depression-indicative linguistic features (anhedonia markers, hopelessness
semantics) with 89% precision (vs. 72% for PHQ-9); (2) Adaptive risk
stratification that updates severity levels based on conversational context,
reducing false positives by 41% compared to scale-based thresholds; and (3)
Personalized intervention strategies tailored to users' emotional granularity,
demonstrating 2.3x higher adherence rates than generic advice. Clinical
validation with 450 participants shows the system identifies 92% of at-risk
cases missed by traditional scales, while its explainable AI interface bridges
the gap between automated analysis and clinician judgment. This work
establishes conversational AI as a paradigm shift from episodic scale-dependent
diagnosis to continuous, emotionally intelligent mental health monitoring.

</details>


### [166] [Application of an attention-based CNN-BiLSTM framework for in vivo two-photon calcium imaging of neuronal ensembles: decoding complex bilateral forelimb movements from unilateral M1](https://arxiv.org/abs/2504.16917)
*Ghazal Mirzaee,Jonathan Chang,Shahrzad Latifi*

Main category: q-bio.NC

TL;DR: A hybrid deep learning framework successfully decodes skilled forelimb movements from brain signals, demonstrating the power of AI in neuroscience.


<details>
  <summary>Details</summary>
Motivation: To decode behavior, particularly movement, from complex brain networks using advanced AI and machine learning, reflecting the growing significance of these technologies in understanding neural mechanisms and motor functions.

Method: The study employs an attention-based CNN-BiLSTM model to analyze data from in vivo two-photon calcium imaging to decode forelimb movements.

Result: The model accurately decodes complex movements of both ipsilateral and contralateral forelimbs from unilateral M1 neuronal ensembles.

Conclusion: Advanced hybrid deep learning models effectively capture the spatiotemporal dependencies of neural activity related to complex movement execution.

Abstract: Decoding behavior, such as movement, from multiscale brain networks remains a
central objective in neuroscience. Over the past decades, artificial
intelligence and machine learning have played an increasingly significant role
in elucidating the neural mechanisms underlying motor function. The advancement
of brain-monitoring technologies, capable of capturing complex neuronal signals
with high spatial and temporal resolution, necessitates the development and
application of more sophisticated machine learning models for behavioral
decoding. In this study, we employ a hybrid deep learning framework, an
attention-based CNN-BiLSTM model, to decode skilled and complex forelimb
movements using signals obtained from in vivo two-photon calcium imaging. Our
findings demonstrate that the intricate movements of both ipsilateral and
contralateral forelimbs can be accurately decoded from unilateral M1 neuronal
ensembles. These results highlight the efficacy of advanced hybrid deep
learning models in capturing the spatiotemporal dependencies of neuronal
networks activity linked to complex movement execution.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [167] [Behavior of prediction performance metrics with rare events](https://arxiv.org/abs/2504.16185)
*Emily Minus,R. Yates Coley,Susan M. Shortreed,Brian D. Williamson*

Main category: stat.ML

TL;DR: AUC may provide misleading performance measures in rare event settings, driven more by class size than event rate.


<details>
  <summary>Details</summary>
Motivation: To investigate the reliability of AUC and other performance metrics in the context of binary prediction models dealing with rare events, which are common in clinical settings.

Method: A simulation study was conducted to assess the bias and variance of AUC in relation to the number of events, event rate, and other performance metrics such as positive predictive value, accuracy, sensitivity, and specificity.

Result: The study found that poor AUC behavior is influenced by minimum class size rather than event rate; sensitivity is influenced by number of events, and specificity by non-events. Positive predictive value and accuracy also depend on event rate, even with large sample sizes.

Conclusion: AUC is deemed reliable in rare event settings as long as the total number of events is moderately large.

Abstract: Area under the receiving operator characteristic curve (AUC) is commonly
reported alongside binary prediction models. However, there are concerns that
AUC might be a misleading measure of prediction performance in the rare event
setting. This setting is common since many events of clinical importance are
rare events. We conducted a simulation study to determine when or whether AUC
is unstable in the rare event setting. Specifically, we aimed to determine
whether the bias and variance of AUC are driven by the number of events or the
event rate. We also investigated the behavior of other commonly used measures
of prediction performance, including positive predictive value, accuracy,
sensitivity, and specificity. Our results indicate that poor AUC behavior -- as
measured by empirical bias, variability of cross-validated AUC estimates, and
empirical coverage of confidence intervals -- is driven by the minimum class
size, not event rate. Performance of sensitivity is driven by the number of
events, while that of specificity is driven by the number of non-events. Other
measures, including positive predictive value and accuracy, depend on the event
rate even in large samples. AUC is reliable in the rare event setting provided
that the total number of events is moderately large.

</details>


### [168] [Covariate-dependent Graphical Model Estimation via Neural Networks with Statistical Guarantees](https://arxiv.org/abs/2504.16356)
*Jiahe Lin,Yikai Zhang,George Michailidis*

Main category: stat.ML

TL;DR: The paper presents a deep neural network approach for estimating covariate-dependent graph structures in graphical models without assuming Gaussianity.


<details>
  <summary>Details</summary>
Motivation: To address the need for flexible modeling of conditional dependencies among random variables in settings where graph structures depend on covariates.

Method: A deep neural network-based approach that allows for functional dependency on covariates and is assessed within the framework of Empirical Risk Minimization with PAC guarantees.

Result: The proposed method performs well on synthetic data and shows competitive results when benchmarked against existing approaches, with positive outcomes on real datasets from neuroscience and finance.

Conclusion: The method demonstrates its effectiveness and interpretability in various domains, showcasing its potential applications in real-world scenarios.

Abstract: Graphical models are widely used in diverse application domains to model the
conditional dependencies amongst a collection of random variables. In this
paper, we consider settings where the graph structure is covariate-dependent,
and investigate a deep neural network-based approach to estimate it. The method
allows for flexible functional dependency on the covariate, and fits the data
reasonably well in the absence of a Gaussianity assumption. Theoretical results
with PAC guarantees are established for the method, under assumptions commonly
used in an Empirical Risk Minimization framework. The performance of the
proposed method is evaluated on several synthetic data settings and benchmarked
against existing approaches. The method is further illustrated on real datasets
involving data from neuroscience and finance, respectively, and produces
interpretable results.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [169] [Cooperative Speech, Semantic Competence, and AI](https://arxiv.org/abs/2504.16092)
*Mahrad Almotahari*

Main category: cs.CY

TL;DR: The paper argues that large language models (LLMs) lack the capacity for cooperative speech due to their inability to engage in respectful communication, which raises questions about their semantic competence.


<details>
  <summary>Details</summary>
Motivation: To analyze the nature of cooperative speech and its requirements for respect and reciprocity, particularly in the context of interactions with large language models.

Method: The author examines the principles of cooperative speech and argues that LLMs do not fulfill the moral and communicative criteria necessary for assertion in conversational contexts.

Result: The paper concludes that LLMs are not cooperative interlocutors as they do not possess the requisite respect inherent in cooperative communication, leading to doubts about their semantic competence.

Conclusion: The inability of LLMs to engage in cooperative speech suggests that understanding meaning involves not just cognitive aspects but also moral considerations.

Abstract: Cooperative speech is purposive. From the speaker's perspective, one crucial
purpose is the transmission of knowledge. Cooperative speakers care about
getting things right for their conversational partners. This attitude is a kind
of respect. Cooperative speech is an ideal form of communication because
participants have respect for each other. And having respect within a
cooperative enterprise is sufficient for a particular kind of moral standing:
we ought to respect those who have respect for us. Respect demands reciprocity.
I maintain that large language models aren't owed the kind of respect that
partly constitutes a cooperative conversation. This implies that they aren't
cooperative interlocutors, otherwise we would be obliged to reciprocate the
attitude. Leveraging this conclusion, I argue that present-day LLMs are
incapable of assertion and that this raises an overlooked doubt about their
semantic competence. One upshot of this argument is that knowledge of meaning
isn't just a subject for the cognitive psychologist. It's also a subject for
the moral psychologist.

</details>


### [170] [Tinkering Against Scaling](https://arxiv.org/abs/2504.16546)
*Bolun Zhang,Yang Shen,Linzhuo Li,Yu Ji,Di Wu,Tongyu Wu,Lianghao Dai*

Main category: cs.CY

TL;DR: The paper discusses the challenges posed by large language models in AI research, emphasizing the need for a 'tinkering' approach that allows academic researchers to better engage with their tools.


<details>
  <summary>Details</summary>
Motivation: The rapid scaling in AI research has created barriers for academic researchers in computational social science and critical algorithm studies, due to the dominance and resource requirements of large language models.

Method: The proposed method is a 'tinkering' approach, which involves engaging with smaller, manageable models or components that facilitate hands-on interaction with algorithms for researchers.

Result: This approach promotes understanding and reproducibility in computational social science and fosters critical engagement with algorithms in research, addressing the black-box issue.

Conclusion: Tinkering not only enhances knowledge and engagement in computational social science and critical studies but also represents a caring practice that has wider implications for both fields.

Abstract: The ascent of scaling in artificial intelligence research has revolutionized
the field over the past decade, yet it presents significant challenges for
academic researchers, particularly in computational social science and critical
algorithm studies. The dominance of large language models, characterized by
their extensive parameters and costly training processes, creates a disparity
where only industry-affiliated researchers can access these resources. This
imbalance restricts academic researchers from fully understanding their tools,
leading to issues like reproducibility in computational social science and a
reliance on black-box metaphors in critical studies.
  To address these challenges, we propose a "tinkering" approach that is
inspired by existing works. This method involves engaging with smaller models
or components that are manageable for ordinary researchers, fostering hands-on
interaction with algorithms. We argue that tinkering is both a way of making
and knowing for computational social science and a way of knowing for critical
studies, and fundamentally, it is a way of caring that has broader implications
for both fields.

</details>


### [171] [Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design](https://arxiv.org/abs/2504.16204)
*Christian Djeffal*

Main category: cs.CY

TL;DR: The paper discusses responsible prompt engineering as a framework for integrating ethical considerations in generative AI interactions to improve societal outcomes.


<details>
  <summary>Details</summary>
Motivation: To ensure generative AI systems address societal needs while minimizing harms as their capabilities grow.

Method: The paper proposes a comprehensive framework of responsible prompt engineering comprising five components: prompt design, system selection, system configuration, performance evaluation, and prompt management, supported by empirical evidence.

Result: The analysis shows that effective prompt engineering requires balancing technical precision with ethical consciousness, enhancing societal outcomes and mitigating risks.

Conclusion: The article emphasizes the need for responsible prompt engineering in AI development, providing key research directions and practical guidelines for ethical integration.

Abstract: Responsible prompt engineering has emerged as a critical framework for
ensuring that generative artificial intelligence (AI) systems serve society's
needs while minimizing potential harms. As generative AI applications become
increasingly powerful and ubiquitous, the way we instruct and interact with
them through prompts has profound implications for fairness, accountability,
and transparency. This article examines how strategic prompt engineering can
embed ethical and legal considerations and societal values directly into AI
interactions, moving beyond mere technical optimization for functionality. This
article proposes a comprehensive framework for responsible prompt engineering
that encompasses five interconnected components: prompt design, system
selection, system configuration, performance evaluation, and prompt management.
Drawing from empirical evidence, the paper demonstrates how each component can
be leveraged to promote improved societal outcomes while mitigating potential
risks. The analysis reveals that effective prompt engineering requires a
delicate balance between technical precision and ethical consciousness,
combining the systematic rigor and focus on functionality with the nuanced
understanding of social impact. Through examination of real-world and emerging
practices, the article illustrates how responsible prompt engineering serves as
a crucial bridge between AI development and deployment, enabling organizations
to fine-tune AI outputs without modifying underlying model architectures. This
approach aligns with broader "Responsibility by Design" principles, embedding
ethical considerations directly into the implementation process rather than
treating them as post-hoc additions. The article concludes by identifying key
research directions and practical guidelines for advancing the field of
responsible prompt engineering.

</details>


### [172] [Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark](https://arxiv.org/abs/2504.16137)
*Jasper Götting,Pedro Medeiros,Jon G Sanders,Nathaniel Li,Long Phan,Karam Elabd,Lennart Justen,Dan Hendrycks,Seth Donoughe*

Main category: cs.CY

TL;DR: The Virology Capabilities Test (VCT) benchmarks large language models for troubleshooting virology protocols, revealing that models like OpenAI's o3 can outperform expert virologists.


<details>
  <summary>Details</summary>
Motivation: To assess the troubleshooting capabilities of large language models in virology laboratory protocols and raise awareness of governance issues related to their dual-use potential.

Method: VCT, a benchmark comprising $322$ multimodal questions, was developed based on contributions from PhD-level virologists to measure various types of knowledge essential for virology work.

Result: OpenAI's o3 achieved $43.8\%$ accuracy on VCT, outperforming $94\%$ of expert virologists who scored an average of $22.1\%$ in their specialized areas.

Conclusion: The superior performance of LLMs on VCT suggests a need to incorporate their capabilities into frameworks for managing dual-use technologies in life sciences.

Abstract: We present the Virology Capabilities Test (VCT), a large language model (LLM)
benchmark that measures the capability to troubleshoot complex virology
laboratory protocols. Constructed from the inputs of dozens of PhD-level expert
virologists, VCT consists of $322$ multimodal questions covering fundamental,
tacit, and visual knowledge that is essential for practical work in virology
laboratories. VCT is difficult: expert virologists with access to the internet
score an average of $22.1\%$ on questions specifically in their sub-areas of
expertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\%$
accuracy, outperforming $94\%$ of expert virologists even within their
sub-areas of specialization. The ability to provide expert-level virology
troubleshooting is inherently dual-use: it is useful for beneficial research,
but it can also be misused. Therefore, the fact that publicly available models
outperform virologists on VCT raises pressing governance considerations. We
propose that the capability of LLMs to provide expert-level troubleshooting of
dual-use virology work should be integrated into existing frameworks for
handling dual-use technologies in the life sciences.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [173] [Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security](https://arxiv.org/abs/2504.16226)
*Yazan Otoum,Arghavan Asad,Amiya Nayak*

Main category: cs.CR

TL;DR: The paper presents a dynamic attack detection and prevention framework for Edge Computing-based Next-Generation Wireless Networks (NGWN)-IoT, focusing on enhancing security against evolving cyber threats through advanced detection methods and blockchain authentication.


<details>
  <summary>Details</summary>
Motivation: The need for enhanced security in NGWN-IoT systems arises from their vulnerability to evolving cyber threats and the limitations of existing intrusion detection and prevention strategies.

Method: A dynamic attack detection and prevention approach that incorporates blockchain-based authentication using the Deoxys Authentication Algorithm (DAA), a bi-stage intrusion detection system with Improved Random Forest (IRF) and Diffusion Convolution Recurrent Neural Network (DCRNN), trust-aware service migration via Heap-Based Optimization (HBO), and on-demand virtual honeypots using the Bimodal Lattice Signature Scheme (BLISS).

Result: The proposed framework significantly outperforms existing methods in multiple performance metrics including accuracy, attack detection rate, false negative rate, precision, recall, and resource usage (memory, CPU, execution time).

Conclusion: The dynamic attack detection and prevention framework enhances the security of NGWN-enabled IoT ecosystems, making it a viable solution to contemporary cybersecurity challenges.

Abstract: Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer
enhanced bandwidth capacity for large-scale service provisioning but remain
vulnerable to evolving cyber threats. Existing intrusion detection and
prevention methods provide limited security as adversaries continually adapt
their attack strategies. We propose a dynamic attack detection and prevention
approach to address this challenge. First, blockchain-based authentication uses
the Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy
before data transmission. Next, a bi-stage intrusion detection system is
introduced: the first stage uses signature-based detection via an Improved
Random Forest (IRF) algorithm. In contrast, the second stage applies
feature-based anomaly detection using a Diffusion Convolution Recurrent Neural
Network (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level
Agreements (SLA), trust-aware service migration is performed using Heap-Based
Optimization (HBO). Additionally, on-demand virtual High-Interaction honeypots
deceive attackers and extract attack patterns, which are securely stored using
the Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based
Intrusion Detection Systems (IDS). The proposed framework is implemented in the
NS3 simulation environment and evaluated against existing methods across
multiple performance metrics, including accuracy, attack detection rate, false
negative rate, precision, recall, ROC curve, memory usage, CPU usage, and
execution time. Experimental results demonstrate that the framework
significantly outperforms existing approaches, reinforcing the security of
NGWN-enabled IoT ecosystems

</details>


### [174] [On the Consistency of GNN Explanations for Malware Detection](https://arxiv.org/abs/2504.16316)
*Hossein Shokouhinejad,Griffin Higgins,Roozbeh Razavi-Far,Hesamodin Mohammadian,Ali A. Ghorbani*

Main category: cs.CR

TL;DR: The paper presents a framework for malware detection using Control Flow Graphs and Graph Neural Networks, emphasizing model interpretability through advanced explainability techniques and a novel aggregation method.


<details>
  <summary>Details</summary>
Motivation: With the rise of malware threats, effective detection methods are needed, and CFGs with GNNs have shown promise in identifying malicious behavior.

Method: The proposed framework constructs CFGs dynamically and utilizes a hybrid approach for node feature embedding, followed by a GNN-based classifier for malware detection. It employs multiple explainability techniques and a novel aggregation method to enhance interpretability.

Result: The framework shows strong performance in accurately identifying malware samples, along with generation of reliable explanations that are evaluated using established metrics for accuracy, fidelity, and consistency.

Conclusion: The proposed framework effectively combines malware detection with interpretability, offering a robust solution to understanding and explaining detection outcomes.

Abstract: Control Flow Graphs (CFGs) are critical for analyzing program execution and
characterizing malware behavior. With the growing adoption of Graph Neural
Networks (GNNs), CFG-based representations have proven highly effective for
malware detection. This study proposes a novel framework that dynamically
constructs CFGs and embeds node features using a hybrid approach combining
rule-based encoding and autoencoder-based embedding. A GNN-based classifier is
then constructed to detect malicious behavior from the resulting graph
representations. To improve model interpretability, we apply state-of-the-art
explainability techniques, including GNNExplainer, PGExplainer, and
CaptumExplainer, the latter is utilized three attribution methods: Integrated
Gradients, Guided Backpropagation, and Saliency. In addition, we introduce a
novel aggregation method, called RankFusion, that integrates the outputs of the
top-performing explainers to enhance the explanation quality. We also evaluate
explanations using two subgraph extraction strategies, including the proposed
Greedy Edge-wise Composition (GEC) method for improved structural coherence. A
comprehensive evaluation using accuracy, fidelity, and consistency metrics
demonstrates the effectiveness of the proposed framework in terms of accurate
identification of malware samples and generating reliable and interpretable
explanations.

</details>


### [175] [Property-Preserving Hashing for $\ell_1$-Distance Predicates: Applications to Countering Adversarial Input Attacks](https://arxiv.org/abs/2504.16355)
*Hassan Asghar,Chenhan Zhang,Dali Kaafar*

Main category: cs.CR

TL;DR: The paper proposes a novel property-preserving hashing (PPH) construction for detecting similar images under adversarial attacks, particularly utilizing an $	extell_1$-distance predicate.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of perceptual hashing methods against adversarial input attacks that can evade detection by making imperceptible changes to images.

Method: The authors introduce a PPH construction specifically for an $	extell_1$-distance predicate, which checks if the $	extell_1$-distances between two images are within a specified threshold, thereby complicating adversarial attacks.

Result: The proposed scheme is efficient, with evaluation times of $0.0784$ seconds for $28 	imes 28$ grayscale images and $0.0128$ to $0.2641$ seconds for $224 	imes 224$ RGB images depending on the perturbation level.

Conclusion: The construction provides a robust solution to image similarity detection in adversarial settings while maintaining high efficiency.

Abstract: Perceptual hashing is used to detect whether an input image is similar to a
reference image with a variety of security applications. Recently, they have
been shown to succumb to adversarial input attacks which make small
imperceptible changes to the input image yet the hashing algorithm does not
detect its similarity to the original image. Property-preserving hashing (PPH)
is a recent construct in cryptography, which preserves some property
(predicate) of its inputs in the hash domain. Researchers have so far shown
constructions of PPH for Hamming distance predicates, which, for instance,
outputs 1 if two inputs are within Hamming distance $t$. A key feature of PPH
is its strong correctness guarantee, i.e., the probability that the predicate
will not be correctly evaluated in the hash domain is negligible. Motivated by
the use case of detecting similar images under adversarial setting, we propose
the first PPH construction for an $\ell_1$-distance predicate. Roughly, this
predicate checks if the two one-sided $\ell_1$-distances between two images are
within a threshold $t$. Since many adversarial attacks use $\ell_2$-distance
(related to $\ell_1$-distance) as the objective function to perturb the input
image, by appropriately choosing the threshold $t$, we can force the attacker
to add considerable noise to evade detection, and hence significantly
deteriorate the image quality. Our proposed scheme is highly efficient, and
runs in time $O(t^2)$. For grayscale images of size $28 \times 28$, we can
evaluate the predicate in $0.0784$ seconds when pixel values are perturbed by
up to $1 \%$. For larger RGB images of size $224 \times 224$, by dividing the
image into 1,000 blocks, we achieve times of $0.0128$ seconds per block for $1
\%$ change, and up to $0.2641$ seconds per block for $14\%$ change.

</details>


### [176] [From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories](https://arxiv.org/abs/2504.16449)
*Ye Tian,Yanqiu Yu,Jianguo Sun,Yanbin Wang*

Main category: cs.CR

TL;DR: This paper reviews malicious URL detection technologies, addressing gaps in existing literature by proposing a modality-based taxonomy and providing a comprehensive analysis of datasets and open-source implementations.


<details>
  <summary>Details</summary>
Motivation: The ongoing threat of malicious URLs in cybersecurity necessitates timely insights and effective detection methods, yet existing reviews have significant gaps in understanding and resources.

Method: The paper systematically analyzes detection methods from traditional blacklisting to advanced deep learning (including Transformers, GNNs, and LLMs) and introduces a modality-based taxonomy for better categorization. It also curates publicly available datasets and open-source implementations for benchmarking.

Result: The comprehensive review reveals critical gaps in existing literature, presents a novel taxonomy categorizing detection approaches by data modality, and provides a curated collection of datasets and open-source implementations for researchers.

Conclusion: The findings highlight emerging challenges in malicious URL detection and propose actionable directions for future research, while the maintained GitHub repository supports ongoing access to datasets and implementations.

Abstract: Malicious URLs persistently threaten the cybersecurity ecosystem, by either
deceiving users into divulging private data or distributing harmful payloads to
infiltrate host systems. Gaining timely insights into the current state of this
ongoing battle holds significant importance. However, existing reviews exhibit
4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures
understanding of how detection approaches exploit specific modal information
channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses;
3) No open-source implementations are collected to facilitate benchmarking; 4)
Insufficient dataset coverage.This paper presents a comprehensive review of
malicious URL detection technologies, systematically analyzing methods from
traditional blacklisting to advanced deep learning approaches (e.g.
Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel
modality-based taxonomy that categorizes existing works according to their
primary data modalities (URL, HTML, Visual, etc.). This hierarchical
classification enables both rigorous technical analysis and clear understanding
of multimodal information utilization. Furthermore, to establish a profile of
accessible datasets and address the lack of standardized benchmarking (where
current studies often lack proper baseline comparisons), we curate and analyze:
1) publicly available datasets (2016-2024), and 2) open-source implementations
from published works(2013-2025). Then, we outline essential design principles
and architectural frameworks for product-level implementations. The review
concludes by examining emerging challenges and proposing actionable directions
for future research. We maintain a GitHub repository for ongoing curating
datasets and open-source implementations:
https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.

</details>


### [177] [Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation](https://arxiv.org/abs/2504.16474)
*Meixi Zheng,Kehan Wu,Yanbo Fan,Rui Huang,Baoyuan Wu*

Main category: cs.CR

TL;DR: The paper presents a theoretical framework for improving the transferability of adversarial examples (AEs) across different models by deriving a novel transferability bound.


<details>
  <summary>Details</summary>
Motivation: The need for effective adversarial attacks on unseen target models using known surrogate models in the transfer-based black-box attack setting, as previous methods lack theoretical foundations.

Method: A novel transferability bound is derived, optimizing AEs toward flat minima over a set of surrogate models while controlling the surrogate-target model shift through adversarial model discrepancy. A new attack algorithm, DRAP, is constructed to enhance AE transferability.

Result: The results demonstrate that the proposed method effectively improves the transferability of AEs, outperforming previous heuristically designed methods, as shown through extensive experiments on NIPS2017 and CIFAR-10 datasets against various target models.

Conclusion: The theoretical guarantees and practical experiments provide a comprehensive understanding of adversarial transferability, leading to a more effective and theoretically grounded approach to crafting adversarial attacks.

Abstract: The transfer-based black-box adversarial attack setting poses the challenge
of crafting an adversarial example (AE) on known surrogate models that remain
effective against unseen target models. Due to the practical importance of this
task, numerous methods have been proposed to address this challenge. However,
most previous methods are heuristically designed and intuitively justified,
lacking a theoretical foundation. To bridge this gap, we derive a novel
transferability bound that offers provable guarantees for adversarial
transferability. Our theoretical analysis has the advantages of \textit{(i)}
deepening our understanding of previous methods by building a general attack
framework and \textit{(ii)} providing guidance for designing an effective
attack algorithm. Our theoretical results demonstrate that optimizing AEs
toward flat minima over the surrogate model set, while controlling the
surrogate-target model shift measured by the adversarial model discrepancy,
yields a comprehensive guarantee for AE transferability. The results further
lead to a general transfer-based attack framework, within which we observe that
previous methods consider only partial factors contributing to the
transferability. Algorithmically, inspired by our theoretical results, we first
elaborately construct the surrogate model set in which models exhibit diverse
adversarial vulnerabilities with respect to AEs to narrow an instantiated
adversarial model discrepancy. Then, a \textit{model-Diversity-compatible
Reverse Adversarial Perturbation} (DRAP) is generated to effectively promote
the flatness of AEs over diverse surrogate models to improve transferability.
Extensive experiments on NIPS2017 and CIFAR-10 datasets against various target
models demonstrate the effectiveness of our proposed attack.

</details>


### [178] [MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark](https://arxiv.org/abs/2504.16651)
*William Corrias,Fabio De Gaspari,Dorjan Hitaj,Luigi V. Mancini*

Main category: cs.CR

TL;DR: MAYA is a unified framework introduced for benchmarking generative password-guessing models, evaluating their effectiveness and generalization in generating complex and human-like passwords.


<details>
  <summary>Details</summary>
Motivation: The integration of generative models in password guessing highlights the need for a rigorous, standardized evaluation framework due to inconsistencies in prior research and a lack of comprehensive assessment.

Method: MAYA provides a customizable benchmarking framework that evaluates six state-of-the-art generative password-guessing models using a set of advanced testing scenarios and eight real-life password datasets, involving over 15,000 hours of computation.

Result: The evaluation shows that while generative models capture human password distribution well, their effectiveness is lower with long and complex passwords; sequential models outperform other architectures and traditional tools.

Conclusion: By releasing MAYA, the authors aim to enhance research in password generation by offering a reliable benchmarking tool for the community.

Abstract: The rapid evolution of generative models has led to their integration across
various fields, including password guessing, aiming to generate passwords that
resemble human-created ones in complexity, structure, and patterns. Despite
generative model's promise, inconsistencies in prior research and a lack of
rigorous evaluation have hindered a comprehensive understanding of their true
potential. In this paper, we introduce MAYA, a unified, customizable,
plug-and-play password benchmarking framework. MAYA provides a standardized
approach for evaluating generative password-guessing models through a rigorous
set of advanced testing scenarios and a collection of eight real-life password
datasets. Using MAYA, we comprehensively evaluate six state-of-the-art
approaches, which have been re-implemented and adapted to ensure
standardization, for a total of over 15,000 hours of computation. Our findings
indicate that these models effectively capture different aspects of human
password distribution and exhibit strong generalization capabilities. However,
their effectiveness varies significantly with long and complex passwords.
Through our evaluation, sequential models consistently outperform other
generative architectures and traditional password-guessing tools, demonstrating
unique capabilities in generating accurate and complex guesses. Moreover,
models learn and generate different password distributions, enabling a
multi-model attack that outperforms the best individual model. By releasing
MAYA, we aim to foster further research, providing the community with a new
tool to consistently and reliably benchmark password-generation techniques. Our
framework is publicly available at
https://github.com/williamcorrias/MAYA-Password-Benchmarking

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [179] [Neuro-Evolutionary Approach to Physics-Aware Symbolic Regression](https://arxiv.org/abs/2504.16503)
*Jiří Kubalík,Robert Babuška*

Main category: cs.NE

TL;DR: This paper presents a neuro-evolutionary symbolic regression method that improves model quality by combining evolutionary search with gradient-based tuning of neural networks, overcoming issues of premature convergence and computational demands.


<details>
  <summary>Details</summary>
Motivation: The goal is to enhance symbolic regression by overcoming limitations faced by traditional genetic programming and recent neural network methods, specifically addressing the issue of premature convergence to suboptimal models.

Method: The proposed method employs a neuro-evolutionary approach that combines evolutionary techniques for optimizing neural network topologies with gradient-based parameter tuning, utilizing a memory-based strategy and population perturbations for effective model training.

Result: Experimental evaluations on three real-world problems demonstrated that the new method outperformed existing neural network-based approaches in terms of model quality.

Conclusion: The neuro-evolutionary approach successfully combines the strengths of both evolutionary algorithms and neural networks, providing superior solutions for symbolic regression tasks.

Abstract: Symbolic regression is a technique that can automatically derive analytic
models from data. Traditionally, symbolic regression has been implemented
primarily through genetic programming that evolves populations of candidate
solutions sampled by genetic operators, crossover and mutation. More recently,
neural networks have been employed to learn the entire analytical model, i.e.,
its structure and coefficients, using regularized gradient-based optimization.
Although this approach tunes the model's coefficients better, it is prone to
premature convergence to suboptimal model structures. Here, we propose a
neuro-evolutionary symbolic regression method that combines the strengths of
evolutionary-based search for optimal neural network (NN) topologies with
gradient-based tuning of the network's parameters. Due to the inherent high
computational demand of evolutionary algorithms, it is not feasible to learn
the parameters of every candidate NN topology to full convergence. Thus, our
method employs a memory-based strategy and population perturbations to enhance
exploitation and reduce the risk of being trapped in suboptimal NNs. In this
way, each NN topology can be trained using only a short sequence of
backpropagation iterations. The proposed method was experimentally evaluated on
three real-world test problems and has been shown to outperform other NN-based
approaches regarding the quality of the models obtained.

</details>
