# 2025-04-24

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 21]

- [cs.LG](#cs.LG) [Total: 51]

- [cs.CL](#cs.CL) [Total: 36]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Quality of explanation of xAI from the prespective of Italian end-users: Italian version of System Causability Scale (SCS)](https://arxiv.org/abs/2504.16193)

*Carmine Attanasio, Alireza Mortezapour*

**Main category:** cs.HC

**TL;DR:** The study validates the Italian version of the system causability scale (I-SCS) to assess explanation quality in explainable AI (xAI).

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the need for quality explanations of AI algorithms, especially in contexts beyond computer science, while providing a culturally relevant measurement tool for Italian users.

**Method:** The Italian version of the I-SCS was developed using a forward-backward translation method, followed by content validity index/ratio calculation and cognitive interviews with end users.

**Result:** The original questionnaire was reduced from 10 to 9 questions after removing one question due to low content validity. The final Italian version was well understood by representative end users.

**Conclusion:** The validated Italian version of the I-SCS is suitable for use in future research and by xAI developers to assess explanation quality in the Italian context.

**Abstract:** Background and aim: Considering the scope of the application of artificial intelligence beyond the field of computer science, one of the concerns of researchers is to provide quality explanations about the functioning of algorithms based on artificial intelligence and the data extracted from it. The purpose of the present study is to validate the Italian version of system causability scale (I-SCS) to measure the quality of explanations provided in a xAI.   Method: For this purpose, the English version, initially provided in 2020 in coordination with the main developer, was utilized. The forward-backward translation method was applied to ensure accuracy. Finally, these nine steps were completed by calculating the content validity index/ratio and conducting cognitive interviews with representative end users.   Results: The original version of the questionnaire consisted of 10 questions. However, based on the obtained indexes (CVR below 0.49), one question (Question 8) was entirely removed. After completing the aforementioned steps, the Italian version contained 9 questions. The representative sample of Italian end users fully comprehended the meaning and content of the questions in the Italian version.   Conclusion: The Italian version obtained in this study can be used in future research studies as well as in the field by xAI developers. This tool can be used to measure the quality of explanations provided for an xAI system in Italian culture.

</details>


### [2] [Subthreshold Jitter in VR Can Induce Visual Discomfort](https://arxiv.org/abs/2504.16295)

*Samuel J. Levulis, Kevin W. Rio, James Wilmott, Charlie S. Burlingham, Phillip Guan*

**Main category:** cs.HC

**TL;DR:** The study investigates the effects of subthreshold visual-vestibular conflicts on comfort during long-duration virtual reality use, revealing that traditional surveys may miss crucial discomfort indicators.

<details>
  <summary>Details</summary>

**Motivation:** To challenge the reliance on extreme visual motion experiences in VR comfort studies and examine discomfort in more naturalistic settings.

**Method:** A psychophysical study identified perceptual thresholds for sinusoidal noise in render pose, followed by a pragmatic study with subthreshold jitter in a Meta Quest 3 HMD across three sessions using the game Cubism.

**Result:** While traditional SSQ scores did not reveal significant differences between control and jitter conditions, using the MISC survey every 10 minutes showed substantial visual discomfort, indicating a need for more frequent assessments.

**Conclusion:** The study underscores the importance of time-resolved data collection in measuring discomfort in VR, suggesting lightweight surveys may enhance ecological validity in research.

**Abstract:** Visual-vestibular conflicts (VVCs) are a primary contributor to visually induced motion sickness (VIMS) in head-mounted displays (HMDs). However, virtual reality (VR) comfort studies often rely on exposing seated or standing users to experiences with high intensity visual motion (such as roller coasters). These drastic VVCs tend to induce pronounced VIMS symptoms that can be reliably detected across individuals using common survey measures. The conclusions from studies using these extreme motion-based conflicts may not accurately generalize to naturalistic use cases in VR where efforts are made to minimize, rather than maximize, VIMS symptoms. In this work, we show that a subthreshold visual-vestibular conflict can induce measurable discomfort during naturalistic, long duration use. We first present a psychophysical study, conducted outside of an HMD, to rigorously identify the perceptual thresholds for sinusoidal noise in render pose (i.e., jitter) resulting in erroneous 3D motion of rendered content. We next introduce subthreshold levels of jitter to a Meta Quest 3 VR HMD and demonstrate that this can induce visual discomfort in participants playing the commercially-available game Cubism across a three-session, repeated-measures study. Importantly, we did not identify statistically significant comfort differences between control and jitter conditions with traditional pre- and post-test comparison of Simulator Sickness Questionnaire (SSQ) scores. Significant differences were only identified using the Motion Illness Symptoms Classification (MISC) survey administered every 10 minutes across each 90 minute session. This highlights the benefits of incorporating time-resolved data points and suggests that lightweight, more frequent surveys may be important tools for measuring visual discomfort in more ecologically-valid scenarios.

</details>


### [3] [Media Content Atlas: A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs](https://arxiv.org/abs/2504.16323)

*Merve Cerit, Eric Zelikman, Mu-Jung Cho, Thomas N. Robinson, Byron Reeves, Nilam Ram, Nick Haber*

**Main category:** cs.HC

**TL;DR:** The Media Content Atlas (MCA) is a new tool for analyzing large-scale screen data, enabling advanced content analysis and visualization beyond traditional metrics.

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the need for flexible and scalable tools to understand complex media experiences as digital media use evolves.

**Method:** MCA uses multimodal large language models (MLLMs) for moment-by-moment content analysis, content-based clustering, topic modeling, image retrieval, and interactive visualizations, evaluated on 1.12 million smartphone screenshots from 112 adults over a month.

**Result:** Expert evaluations revealed that clustering results were rated 96% relevant and descriptions 83% accurate, demonstrating the tool's usability and effectiveness for research.

**Conclusion:** MCA facilitates both inductive and deductive inquiry, offering new research opportunities in media and human-computer interaction (HCI) by linking methodological capabilities with specific research needs.

**Abstract:** As digital media use continues to evolve and influence various aspects of life, developing flexible and scalable tools to study complex media experiences is essential. This study introduces the Media Content Atlas (MCA), a novel pipeline designed to help researchers investigate large-scale screen data beyond traditional screen-use metrics. Leveraging multimodal large language models (MLLMs), MCA enables moment-by-moment content analysis, content-based clustering, topic modeling, image retrieval, and interactive visualizations. Evaluated on 1.12 million smartphone screenshots continuously captured during screen use from 112 adults over an entire month, MCA facilitates open-ended exploration and hypothesis generation as well as hypothesis-driven investigations at an unprecedented scale. Expert evaluators underscored its usability and potential for research and intervention design, with clustering results rated 96% relevant and descriptions 83% accurate. By bridging methodological possibilities with domain-specific needs, MCA accelerates both inductive and deductive inquiry, presenting new opportunities for media and HCI research.

</details>


### [4] [What Sensors See, What People Feel: Exploring Subjective Collaboration Perception in Mixed Reality](https://arxiv.org/abs/2504.16373)

*Yasra Chandio, Diana Romero, Salma Elmalaki, Fatima Anwar*

**Main category:** cs.HC

**TL;DR:** This paper introduces the Sensor-to-Subjective Mapping Framework (S2S) to link observable behavioral signals in Mixed Reality (MR) to users' subjective experiences during collaboration.

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the gap between observable behavioral signals in MR and the subjective experiences of collaboration, emphasizing the need to understand how these signals correlate with users' internal states.

**Method:** A study involving 48 participants across 12 MR groups was conducted, where participants performed a collaborative image-sorting task to validate the proposed S2S Mapping Framework.

**Result:** The study found a correlation between sensed behaviors, such as shared attention and proximity, and participants' perceived collaboration during the MR tasks.

**Conclusion:** The S2S Mapping Framework effectively connects observable sensor data with subjective user experiences, highlighting the importance of behavioral signals in understanding collaboration in MR environments.

**Abstract:** Mixed Reality (MR) enables rich, embodied collaboration, yet it's uncertain if sensor and system-logged behavioral signals capture how users experience that collaboration. This disconnect stems from a fundamental gap: behavioral signals are observable and continuous, while collaboration is interpreted subjectively, shaped by internal states like presence, cognitive availability, and social awareness. Our core insight is that sensor signals serve as observable manifestations of subjective experiences in MR collaboration, and they can be captured through sensor data such as shared gaze, speech, spatial movement, and other system-logged performance metrics. We propose the Sensor-to-Subjective (S2S) Mapping Framework, a conceptual model that links observable interaction patterns to users' subjective perceptions of collaboration and internal cognitive states through sensor-based indicators and task performance metrics. To validate this model, we conducted a study with 48 participants across 12 MR groups engaged in a collaborative image-sorting task. Our findings show a correlation between sensed behavior and perceived collaboration, particularly through shared attention and proximity.

</details>


### [5] [Cyberoception: Finding a Painlessly-Measurable New Sense in the Cyberworld Towards Emotion-Awareness in Computing](https://arxiv.org/abs/2504.16378)

*Tadashi Okoshi, Zexiong Gao, Tan Yi Zhen, Takumi Karasawa, Takeshi Miki, Wataru Sasaki, Rajesh K. Balan*

**Main category:** cs.HC

**TL;DR:** The study introduces 'cyberoception', a novel concept for measuring users' emotional states via smartphone sensors, overcoming limitations of existing interoception methods.

<details>
  <summary>Details</summary>

**Motivation:** Accurate emotion recognition in affective computing is hindered by the limitations of existing interoception measurement methods, which rely on controlled environments and precise equipment.

**Method:** A hybrid 10-day experiment was conducted, combining in-lab and real-world measurements, to investigate forms of data that can reflect users' interoceptive states using smartphone sensors.

**Result:** Findings indicate a specific type of cyberoception, 'Turn On', which is linked to users' emotional valence and is measurable through smartphone devices.

**Conclusion:** The concept of cyberoception may be key to creating more emotion-aware applications and improving human-computer interaction.

**Abstract:** In Affective computing, recognizing users' emotions accurately is the basis of affective human-computer interaction. Understanding users' interoception contributes to a better understanding of individually different emotional abilities, which is essential for achieving inter-individually accurate emotion estimation. However, existing interoception measurement methods, such as the heart rate discrimination task, have several limitations, including their dependence on a well-controlled laboratory environment and precision apparatus, making monitoring users' interoception challenging. This study aims to determine other forms of data that can explain users' interoceptive or similar states in their real-world lives and propose a novel hypothetical concept "cyberoception," a new sense (1) which has properties similar to interoception in terms of the correlation with other emotion-related abilities, and (2) which can be measured only by the sensors embedded inside commodity smartphone devices in users' daily lives. Results from a 10-day-long in-lab/in-the-wild hybrid experiment reveal a specific cyberoception type "Turn On" (users' subjective sensory perception about the frequency of turning-on behavior on their smartphones), significantly related to participants' emotional valence. We anticipate that cyberoception to serve as a fundamental building block for developing more "emotion-aware", user-friendly applications and services.

</details>


### [6] [FeedQUAC: Quick Unobtrusive AI-Generated Commentary](https://arxiv.org/abs/2504.16416)

*Tao Long, Kendra Wannamaker, Jo Vermeulen, George Fitzmaurice, Justin Matejka*

**Main category:** cs.HC

**TL;DR:** The paper introduces FeedQUAC, an AI tool that provides ambient feedback to enhance the design process.

<details>
  <summary>Details</summary>

**Motivation:** Design often requires constant feedback, which can be labor-intensive; thus, the paper explores how AI can simplify feedback gathering.

**Method:** The study involves a design probe with eight participants to evaluate the impacts of using FeedQUAC, an AI-enabled design companion that offers real-time commentary from various personas.

**Result:** Participants reported benefits such as convenience, a boost in confidence, increased playfulness, and inspirational insights from the ambient feedback provided by FeedQUAC, along with suggestions for improvement.

**Conclusion:** AI-generated ambient feedback can significantly enhance design workflows and should be considered in future creativity support systems, balancing user involvement with AI capabilities.

**Abstract:** Design thrives on feedback. However, gathering constant feedback throughout the design process can be labor-intensive and disruptive. We explore how AI can bridge this gap by providing effortless, ambient feedback. We introduce FeedQUAC, a design companion that delivers real-time AI-generated commentary from a variety of perspectives through different personas. A design probe study with eight participants highlights how designers can leverage quick yet ambient AI feedback to enhance their creative workflows. Participants highlight benefits such as convenience, playfulness, confidence boost, and inspiration from this lightweight feedback agent, while suggesting additional features, like chat interaction and context curation. We discuss the role of AI feedback, its strengths and limitations, and how to integrate it into existing design workflows while balancing user involvement. Our findings also suggest that ambient interaction is a valuable consideration for both the design and evaluation of future creativity support systems.

</details>


### [7] [Advancing Radar Hand Gesture Recognition: A Hybrid Spectrum Synthetic Framework Merging Simulation with Neural Networks](https://arxiv.org/abs/2504.16423)

*Jiaqi Tang, Xinbo Xu, Yinsong Xu, Qingchao Chen*

**Main category:** cs.HC

**TL;DR:** The paper presents a novel hybrid framework for generating synthetic radar data for hand gesture recognition, addressing limitations of existing methods.

<details>
  <summary>Details</summary>

**Motivation:** Limited radar datasets hinder hand gesture recognition performance, and existing synthetic data generation methods lack accuracy and interpretability in simulating radar wave propagation.

**Method:** A hybrid spectrum synthetic framework combining a cylinder mesh-based hand reflection model with RadarWeightNet, a small neural network that assigns weights to simulated radar signals.

**Result:** The framework achieved up to 63% SSIM in synthetic performance and a 30% improvement in classification performance in few-shot learning scenarios under limited radar data.

**Conclusion:** The proposed framework effectively simulates complex hand geometry and bridges the simulation-to-real gap while maintaining interpretability, enhancing the performance of hand gesture recognition systems.

**Abstract:** Millimeter wave (mmWave) radar sensors play a vital role in hand gesture recognition (HGR) by detecting subtle motions while preserving user privacy. However, the limited scale of radar datasets hinders the performance. Existing synthetic data generation methods fall short in two key areas. On the one hand, modeling-based approaches fail to accurately simulate the wave propagation and reflection at the hand-gesture level, facing unique complexities such as diffraction and occlusion. On the other hand, generative model-based methods are hard to converge while radar data is limited, lacking interpretability, and sometimes fail to produce kinematically plausible results. To overcome these limitations, we propose a novel hybrid spectrum synthetic framework leveraging visual hand gesture data. It combines a cylinder mesh-based hand reflection model with a small-scale neural network called RadarWeightNet, which focuses on assigning weights to simulated signals. Our framework addresses two key challenges: achieving accurate simulation of complex hand geometry and bridging the simulation-to-real gap in a data-driven manner while preserving interpretability, which balances physical accuracy with machine learning adaptability. We tested our framework under extreme scenarios where radar data is scarce. The results demonstrate the effectiveness of our hybrid framework, achieving up to 63% SSIM in synthetic performance and up to 30% improvement in classification performance in few-shot learning.

</details>


### [8] [Insect-Computer Hybrid Speaker: Speaker using Chirp of the Cicada Controlled by Electrical Muscle Stimulation](https://arxiv.org/abs/2504.16459)

*Yuga Tsukuda, Naoto Nishida, Jun Lu, Yoichi Ochiai*

**Main category:** cs.HC

**TL;DR:** The paper presents an Insect-Computer Hybrid Speaker using cicadas as speakers controlled by Electrical Muscle Stimulation.

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of using insects for interaction with external systems and to expand the interface possibilities beyond traditional methods of controlling insects.

**Method:** The study investigates the use of cicadas as sound sources triggered by Electrical Muscle Stimulation, focusing on suitable chirp waveforms, voltage ranges, and maximum chirping pitch.

**Result:** Determined the effective parameters for using cicadas as speakers, including optimal chirp waveform and operational voltage ranges.

**Conclusion:** The findings demonstrate that cicadas can be effectively utilized as sound sources, opening new avenues for hybrid interaction between technology and living organisms.

**Abstract:** We propose "Insect-Computer Hybrid Speaker", which enables us to make musics made from combinations of computer and insects. Lots of studies have proposed methods and interfaces for controlling insects and obtaining feedback. However, there have been less research on the use of insects for interaction with third parties. In this paper, we propose a method in which cicadas are used as speakers triggered by using Electrical Muscle Stimulation (EMS). We explored and investigated the suitable waveform of chirp to be controlled, the appropriate voltage range, and the maximum pitch at which cicadas can chirp.

</details>


### [9] [Helping Blind People Grasp: Enhancing a Tactile Bracelet with an Automated Hand Navigation System](https://arxiv.org/abs/2504.16502)

*Marcin Furtak, Florian Pätzold, Tim Kietzmann, Silke M. Kärcher, Peter König*

**Main category:** cs.HC

**TL;DR:** A tactile bracelet assists visually impaired individuals in grasping objects by guiding their hands using vibration commands, demonstrated through various tasks in both structured and real-world environments.

<details>
  <summary>Details</summary>

**Motivation:** To help visually impaired people overcome the challenges associated with grasping objects.

**Method:** Development of a tactile bracelet system that detects objects and provides vibration-based hand guidance through automated navigation in several tasks that simulate everyday scenarios.

**Result:** The system successfully enabled participants to grasp target objects while avoiding distractions and obstacles, demonstrating effective hand tracking and guidance in complex environments.

**Conclusion:** The implementation of the system shows potential to improve the quality of life for visually impaired individuals by facilitating autonomous object interaction in daily activities.

**Abstract:** Grasping constitutes a critical challenge for visually impaired people. To address this problem, we developed a tactile bracelet that assists in grasping by guiding the user's hand to a target object using vibration commands. Here we demonstrate the fully automated system around the bracelet, which can confidently detect and track target and distractor objects and reliably guide the user's hand. We validate our approach in three tasks that resemble complex, everyday use cases. In a grasping task, the participants grasp varying target objects on a table, guided via the automated hand navigation system. In the multiple objects task, participants grasp objects from the same class, demonstrating our system's ability to track one specific object without targeting surrounding distractor objects. Finally, the participants grasp one specific target object by avoiding an obstacle along the way in the depth navigation task, showcasing the potential to utilize our system's depth estimations to navigate even complex scenarios. Additionally, we demonstrate that the system can aid users in the real world by testing it in a less structured environment with a blind participant. Overall, our results demonstrate that the system, by translating the AI-processed visual inputs into a reduced data rate of actionable signals, enables autonomous behavior in everyday environments, thus potentially increasing the quality of life of visually impaired people.

</details>


### [10] [SafeSpect: Safety-First Augmented Reality Heads-up Display for Drone Inspections](https://arxiv.org/abs/2504.16533)

*Peisen Xu, Jérémie Garcia, Wei Tsang Ooi, Christophe Jouffrais*

**Main category:** cs.HC

**TL;DR:** The paper presents an adaptive augmented reality (AR) interface designed to reduce cognitive load and enhance situational awareness for drone pilots during operations.

<details>
  <summary>Details</summary>

**Motivation:** Current tablet-based drone interfaces increase cognitive load and reduce situational awareness by forcing pilots to split attention between video feeds and real-world environments.

**Method:** The authors conducted participatory design workshops with professional pilots to identify essential features and created an adaptive AR interface that adapts between task and safety views to minimize information overload. The prototype was tested through a building inspection task against standard interfaces (2D tablet and static AR), using a user study with 15 participants.

**Result:** The evaluation revealed that the AR interface enhanced access to safety information and that the adaptive AR version particularly lowered cognitive load and improved situational awareness, maintaining task performance.

**Conclusion:** The study provides valuable design insights for creating effectiveness-oriented, safety-first heads-up AR interfaces for drone operations.

**Abstract:** Current tablet-based interfaces for drone operations often impose a heavy cognitive load on pilots and reduce situational awareness by dividing attention between the video feed and the real world. To address these challenges, we designed a heads-up augmented reality (AR) interface that overlays in-situ information to support drone pilots in safety-critical tasks. Through participatory design workshops with professional pilots, we identified key features and developed an adaptive AR interface that dynamically switches between task and safety views to prevent information overload. We evaluated our prototype by creating a realistic building inspection task and comparing three interfaces: a 2D tablet, a static AR, and our adaptive AR design. A user study with 15 participants showed that the AR interface improved access to safety information, while the adaptive AR interface reduced cognitive load and enhanced situational awareness without compromising task performance. We offer design insights for developing safety-first heads-up AR interfaces.

</details>


### [11] [Exploring human-SAV interaction using large language models: The impact of psychological ownership and anthropomorphism on user experience](https://arxiv.org/abs/2504.16548)

*Lirui Guo, Michael G. Burke, Wynita M. Griggs*

**Main category:** cs.HC

**TL;DR:** This study examines the impact of LLM-powered conversational UIs on user perceptions and adoption of shared autonomous vehicles (SAVs) by manipulating anthropomorphism and psychological ownership.

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in research concerning how LLM-powered UIs affect user perceptions and intentions regarding the adoption of SAVs, particularly through psychological factors.

**Method:** Four SAV UIs were designed with differing levels of anthropomorphism and psychological ownership triggers. Quantitative measures were collected on psychological ownership, anthropomorphism, service quality, disclosure tendency, sentiment, and overall acceptance, alongside qualitative feedback.

**Result:** The anthropomorphic SAV conversational UI elicited better user perceptions and sentiment toward the SAV compared to a control condition, with improved feelings of psychological ownership noted by participants.

**Conclusion:** The findings suggest that enhancing anthropomorphism and fostering psychological ownership in LLM-based conversational UIs can significantly improve user experience and the potential adoption of SAVs.

**Abstract:** There has been extensive prior work exploring how psychological factors such as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs). However, limited research has been conducted on how prompt strategies in large language model (LLM)-powered SAV User Interfaces (UIs) affect users' perceptions, experiences, and intentions to adopt such technology. In this work, we investigate how conversational UIs powered by LLMs drive these psychological factors and psychological ownership, the sense of possession a user may come to feel towards an entity or object they may not legally own. We designed four SAV UIs with varying levels of anthropomorphic characteristics and psychological ownership triggers. Quantitative measures of psychological ownership, anthropomorphism, quality of service, disclosure tendency, sentiment of SAV responses, and overall acceptance were collected after participants interacted with each SAV. Qualitative feedback was also gathered regarding the experience of psychological ownership during the interactions. The results indicate that an SAV conversational UI designed to be more anthropomorphic and to induce psychological ownership improved users' perceptions of the SAV's human-like qualities and improved the sentiment of responses compared to a control condition. These findings provide practical guidance for designing LLM-based conversational UIs that enhance user experience and adoption of SAVs.

</details>


### [12] [A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments](https://arxiv.org/abs/2504.16562)

*Julian Rasch, Florian Müller, Francesco Chiossi*

**Main category:** cs.HC

**TL;DR:** The paper discusses AI-driven approaches for adaptive content placement in Augmented Reality (AR), aiming to improve user interaction and experience.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the challenges existing AR systems face in managing interactive possibilities, which can overwhelm users.

**Method:** The paper proposes leveraging machine learning methods for dynamic AR content placement that adapts to user movements and environmental changes.

**Result:** The proposed system would intelligently manage content distribution between AR projections and fixed content, enhancing UI layout and potentially reducing cognitive load.

**Conclusion:** The vision outlined aims to foster innovation in AR applications across various industries, leading to more intuitive and engaging user experiences.

**Abstract:** Augmented Reality (AR) is transforming the way we interact with virtual information in the physical world. By overlaying digital content in real-world environments, AR enables new forms of immersive and engaging experiences. However, existing AR systems often struggle to effectively manage the many interactive possibilities that AR presents. This vision paper speculates on AI-driven approaches for adaptive AR content placement, dynamically adjusting to user movement and environmental changes. By leveraging machine learning methods, such a system would intelligently manage content distribution between AR projections integrated into the external environment and fixed static content, enabling seamless UI layout and potentially reducing users' cognitive load. By exploring the possibilities of AI-driven dynamic AR content placement, we aim to envision new opportunities for innovation and improvement in various industries, from urban navigation and workplace productivity to immersive learning and beyond. This paper outlines a vision for the development of more intuitive, engaging, and effective AI-powered AR experiences.

</details>


### [13] [Bridging Data Gaps and Building Knowledge Networks in Indian Football Analytics](https://arxiv.org/abs/2504.16572)

*Sneha Nanavati, Nimmi Rangaswamy*

**Main category:** cs.HC

**TL;DR:** The study examines the challenges and grassroots solutions in adopting football analytics in India, emphasizing the role of informal networks in overcoming these barriers.

<details>
  <summary>Details</summary>

**Motivation:** Despite the global trend in football analytics, India faces specific challenges like institutional resistance and resource scarcity that hinder its adoption.

**Method:** The study employs a mixed-methods approach, utilizing digital ethnography, participant observation, and interviews to explore informal analytics communities in India.

**Result:** Findings reveal that informal networks effectively mitigate constraints through mentorship and innovation, fostering skill development and professional growth in analytics.

**Conclusion:** The paper suggests HCI interventions such as decentralized knowledge platforms and AI-assisted tools to enhance analytics in resource-constrained environments, thereby promoting inclusive technical engagement and decision-making.

**Abstract:** The global rise of football analytics has rapidly transformed how clubs make strategic decisions. However, in India, the adoption of analytics remains constrained by institutional resistance, infrastructural limitations, and cultural barriers -- challenges that grassroots innovation and low-cost data solutions have the potential to overcome. Despite the growing popularity of the Indian Super League, resource scarcity and fragmented governance continue to hinder the widespread adoption and impact of analytics. This mixed-methods study explores how informal, decentralised analytics communities -- comprising amateur analysts and Twitter-based "data sleuths" -- navigate these constraints through peer mentorship and grassroots innovation. Drawing on extensive digital ethnography, participant observation, and interviews, the study illustrates how these informal networks mitigate data scarcity, limited digital infrastructure, and institutional indifference while fostering skill development and professional growth. Building on these insights, the paper proposes HCI interventions such as decentralised knowledge platforms to facilitate structured, cross-border peer mentorship and low-cost data solutions -- including AI-assisted player tracking and mobile analytics dashboards -- rooted in principles of frugal innovation. These interventions aim to bridge the data divide, support inclusive technical engagement in sport, and enhance analytics-driven decision-making in resource-constrained environments. This paper contributes to HCIxB's focus on cross-border collaboration by highlighting how community-driven technological adaptation in the Global South can foster meaningful participation, skill-building, and long-term sustainability through informal learning networks and scalable, context-sensitive tools.

</details>


### [14] [PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling Assistant System](https://arxiv.org/abs/2504.16573)

*Xianghe Liu, Jiaqi Xu, Tao Sun*

**Main category:** cs.HC

**TL;DR:** PsyCounAssist is an AI-powered counseling assistant designed to enhance psychological counseling through real-time emotion recognition and automated session reporting.

<details>
  <summary>Details</summary>

**Motivation:** The need for a personalized and dynamic counseling process that allows therapists to monitor emotional changes and maintain continuity.

**Method:** PsyCounAssist employs multimodal emotion recognition using speech and photoplethysmography signals, automated reporting via large language models, and provides AI-generated personalized support, all on Android tablets.

**Result:** Experimental evaluation shows reliable emotional classification using PPG, demonstrating the system's applicability and flexibility in real-world settings.

**Conclusion:** PsyCounAssist offers a novel, ethically sound method to integrate AI into psychological counseling, enhancing both therapists' and clients' experiences.

**Abstract:** Psychological counseling is a highly personalized and dynamic process that requires therapists to continuously monitor emotional changes, document session insights, and maintain therapeutic continuity. In this paper, we introduce PsyCounAssist, a comprehensive AI-powered counseling assistant system specifically designed to augment psychological counseling practices. PsyCounAssist integrates multimodal emotion recognition combining speech and photoplethysmography (PPG) signals for accurate real-time affective analysis, automated structured session reporting using large language models (LLMs), and personalized AI-generated follow-up support. Deployed on Android-based tablet devices, the system demonstrates practical applicability and flexibility in real-world counseling scenarios. Experimental evaluation confirms the reliability of PPG-based emotional classification and highlights the system's potential for non-intrusive, privacy-aware emotional support. PsyCounAssist represents a novel approach to ethically and effectively integrating AI into psychological counseling workflows.

</details>


### [15] [Algorithmic Mirror: Designing an Interactive Tool to Promote Self-Reflection for YouTube Recommendations](https://arxiv.org/abs/2504.16615)

*Yui Kondo, Kevin Dunnell, Qing Xiao, Jun Zhao, Luc Rocher*

**Main category:** cs.HC

**TL;DR:** This research proposes 'hypothetical inference', a new method using language models to help users understand how algorithms interpret their digital footprints, promoting algorithmic literacy and user autonomy.

<details>
  <summary>Details</summary>

**Motivation:** Big Data and AI systems often produce complex and opaque inferences about individuals, diminishing their control over digital identities and complicating intuitive understanding of these processes.

**Method:** The study introduces 'hypothetical inference', leveraging language models to simulate algorithm interpretation of digital footprints without accessing proprietary algorithms. It includes empirical studies with fourteen participants to identify design opportunities.

**Result:** The research identified three design opportunities for fostering algorithmic literacy: creating a unified map of digital footprints, simulating algorithmic inferences with LLM-generated interpretations, and visualizing evolving patterns over time.

**Conclusion:** This work establishes a foundation for tools that can enhance users' understanding of data influence on digital platforms and support their autonomy in algorithm-driven environments.

**Abstract:** Big Data analytics and Artificial Intelligence systems derive non-intuitive and often unverifiable inferences about individuals' behaviors, preferences, and private lives. Drawing on diverse, feature-rich datasets of unpredictable value, these systems erode the intuitive connection between our actions and how we are perceived, diminishing control over our digital identities. While Explainable Artificial Intelligence scholars have attempted to explain the inner workings of algorithms, their visualizations frequently overwhelm end-users with complexity. This research introduces 'hypothetical inference', a novel approach that uses language models to simulate how algorithms might interpret users' digital footprints and infer personal characteristics without requiring access to proprietary platform algorithms. Through empirical studies with fourteen adult participants, we identified three key design opportunities to foster critical algorithmic literacy: (1) reassembling scattered digital footprints into a unified map, (2) simulating algorithmic inference through LLM-generated interpretations, and (3) incorporating temporal dimensions to visualize evolving patterns. This research lays the groundwork for tools that can help users recognize the influence of data on platforms and develop greater autonomy in increasingly algorithm-mediated digital environments.

</details>


### [16] [LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative Analysis](https://arxiv.org/abs/2504.16671)

*Joel Oksanen, Andrés Lucero, Perttu Hämäläinen*

**Main category:** cs.HC

**TL;DR:** This research evaluates the trustworthiness of large language models (LLMs) in qualitative design analysis, highlighting the need for human-AI collaboration.

<details>
  <summary>Details</summary>

**Motivation:** The paper investigates how LLMs can enhance qualitative analysis in design research while addressing concerns about their contextual relevance and trustworthiness.

**Method:** The study introduces LLMCode, an open-source tool that uses Intersection over Union (IoU) and Modified Hausdorff Distance metrics for assessing the alignment of human and LLM-generated insights, analyzed through two studies with 26 designers.

**Result:** Results indicate that while LLMs perform effectively in deductive coding, they struggle to replicate the nuanced interpretive insights of designers, showcasing a collaborative dynamic where designers refine LLM outputs.

**Conclusion:** The findings stress the need for tools that maintain interpretive depth and promote intuitive collaboration between human designers and LLMs.

**Abstract:** The use of large language models (LLMs) in qualitative analysis offers enhanced efficiency but raises questions about their alignment with the contextual nature of research for design (RfD). This research examines the trustworthiness of LLM-driven design insights, using qualitative coding as a case study to explore the interpretive processes central to RfD. We introduce LLMCode, an open-source tool integrating two metrics, namely Intersection over Union (IoU) and Modified Hausdorff Distance, to assess the alignment between human and LLM-generated insights. Across two studies involving 26 designers, we find that while the model performs well with deductive coding, its ability to emulate a designer's deeper interpretive lens over the data is limited, emphasising the importance of human-AI collaboration. Our results highlight a reciprocal dynamic where users refine LLM outputs and adapt their own perspectives based on the model's suggestions. These findings underscore the importance of fostering appropriate reliance on LLMs by designing tools that preserve interpretive depth while facilitating intuitive collaboration between designers and AI.

</details>


### [17] [Search Timelines: Visualizing Search History to Enable Cross-Session Exploratory Search](https://arxiv.org/abs/2504.16741)

*Orland Hoeber, Md Nazmul Islam, Miriam Boon, Dale Storie, Veronica Ramshaw*

**Main category:** cs.HC

**TL;DR:** Search Timelines enhances exploratory search in digital libraries by providing a dynamic timeline of search activities, resulting in increased user engagement but longer task completion times.

<details>
  <summary>Details</summary>

**Motivation:** Exploratory searching spans long timespans and numerous activities, making it challenging for searchers to recall key details during and between sessions.

**Method:** The study introduces Search Timelines, a visualization tool that displays current and past search activities through an overview and detailed timeline, compared to a typical public digital library interface.

**Result:** Participants using Search Timelines reported greater engagement, usability, and perceived knowledge gain, despite taking more time to complete tasks compared to the baseline interface.

**Conclusion:** Search Timelines demonstrates that lightweight visualizations can enhance search interfaces for exploratory tasks, emphasizing the importance of persistent representations of past activities.

**Abstract:** Purpose: The timespan over which exploratory searching can occur, as well as the scope and volume of the search activities undertaken, can make it difficult for searchers to remember key details about their search activities. These difficulties are present both in the midst of searching as well as when resuming a search that spans multiple sessions. In this paper, we present a search interface designed to support cross-session exploratory search in a public digital library context. Methods: Search Timelines provides a visualization of current and past search activities via a dynamic timeline of the search activity (queries and saved resources). This timeline is presented at two levels of detail. An overview timeline is provided alongside the search results in a typical search engine results page design. A detailed timeline is provided in the workspace, where searchers can review the history of their search activities and their saved resources. A controlled laboratory study was conducted to compare this approach to a baseline interface modelled after a typical public digital library search/workspace interface. Results: Participants who used Search Timelines reported higher levels of user engagement, usability, and perceived knowledge gain, during an initial search session and when resuming the search after a 7-8 day interval. This came at the expense of the searchers taking more time to complete the search task, which we view as positive evidence of engagement in cross-session exploratory search processes. Conclusion: Search Timelines serves as an example of how lightweight visualization approaches can be used to enhance typical search interface designs to support exploratory search. The results highlight the value of providing persistent representations of past search activities within the search interface.

</details>


### [18] [DeBiasMe: De-biasing Human-AI Interactions with Metacognitive AIED (AI in Education) Interventions](https://arxiv.org/abs/2504.16770)

*Chaeyeon Lim*

**Main category:** cs.HC

**TL;DR:** The paper advocates for integrating metacognitive AI literacy interventions to mitigate human biases in interactions with generative AI, focusing on enhancing university students' critical engagement with these systems.

<details>
  <summary>Details</summary>

**Motivation:** There is a critical gap in understanding and addressing human biases, such as anchoring and confirmation bias, in AI interactions within academic environments.

**Method:** The paper outlines frameworks for metacognitive support, bi-directional Human-AI interaction interventions, and adaptive scaffolding, illustrated through the 'DeBiasMe' initiative aimed at increasing awareness of cognitive biases.

**Result:** The proposed frameworks aim to empower students and enhance their agency in AI-use while promoting discussion among stakeholders on design and evaluation methods for bias-related scaffolding and visualization.

**Conclusion:** The contribution emphasizes the importance of metacognition in navigating human biases in AI, advocating for its integration into comprehensive AI literacy frameworks.

**Abstract:** While generative artificial intelligence (Gen AI) increasingly transforms academic environments, a critical gap exists in understanding and mitigating human biases in AI interactions, such as anchoring and confirmation bias. This position paper advocates for metacognitive AI literacy interventions to help university students critically engage with AI and address biases across the Human-AI interaction workflows. The paper presents the importance of considering (1) metacognitive support with deliberate friction focusing on human bias; (2) bi-directional Human-AI interaction intervention addressing both input formulation and output interpretation; and (3) adaptive scaffolding that responds to diverse user engagement patterns. These frameworks are illustrated through ongoing work on "DeBiasMe," AIED (AI in Education) interventions designed to enhance awareness of cognitive biases while empowering user agency in AI interactions. The paper invites multiple stakeholders to engage in discussions on design and evaluation methods for scaffolding mechanisms, bias visualization, and analysis frameworks. This position contributes to the emerging field of AI-augmented learning by emphasizing the critical role of metacognition in helping students navigate the complex interaction between human, statistical, and systemic biases in AI use while highlighting how cognitive adaptation to AI systems must be explicitly integrated into comprehensive AI literacy frameworks.

</details>


### [19] [Nurturing Language Proficiency in Spanish.speaking children Through digital competence](https://arxiv.org/abs/2504.16824)

*Rhayza Jolley Rangel*

**Main category:** cs.HC

**TL;DR:** The paper discusses the creation of a digital platform targeted at enhancing early English education for Spanish-speaking children.

<details>
  <summary>Details</summary>

**Motivation:** The aim is to improve English learning experiences for young Spanish-speaking learners through an engaging digital platform.

**Method:** Utilization of innovative methodologies, engaging visuals, and a comprehensive phonics approach, while incorporating usability, accessibility, and user-centered design principles.

**Result:** The platform is designed with a focus on effective learning through a visually appealing and user-centric interface.

**Conclusion:** The integration of these design principles is expected to significantly enhance the educational experience for early-age learners.

**Abstract:** This article explores into the intricate design and meticulous construction of a digital platform aimed at revolutionizing early-age English education, particularly for Spanish-speaking children. The focus of this work used an innovative methodologies, vibrant and engaging visuals, and a comprehensive approach to phonics. The principles of usability, accessibility, and user-centered design are intricately woven into every facet of the platform's architecture.

</details>


### [20] [Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models](https://arxiv.org/abs/2504.16883)

*Xuyang Zhu, Sejoon Chang, Andrew Kuik*

**Main category:** cs.HC

**TL;DR:** This research investigates the impact of tailored warning messages on user reasoning in the context of Retrieval-Augmented Generation (RAG) systems, which enhance LLM outputs but are susceptible to hallucinations.

<details>
  <summary>Details</summary>

**Motivation:** There are ongoing concerns about fairness and reliability in RAG systems due to hallucinations affecting user reasoning and decision-making; thus, understanding how to mitigate these issues is crucial.

**Method:** The study explores the effects of context-dependent warning messages on users' accuracy and awareness of hallucinations within an educational quiz framework.

**Result:** Preliminary findings indicate that while warnings enhance accuracy and awareness, they can also cause cognitive friction, leading to confusion and reduced trust in the system.

**Conclusion:** The research contributes to the development of AI systems that foster human reflection and critical thinking, rather than merely providing passive information.

**Abstract:** Retrieval-Augmented Generation (RAG) systems offer a powerful approach to enhancing large language model (LLM) outputs by incorporating fact-checked, contextually relevant information. However, fairness and reliability concerns persist, as hallucinations can emerge at both the retrieval and generation stages, affecting users' reasoning and decision-making. Our research explores how tailored warning messages -- whose content depends on the specific context of hallucination -- shape user reasoning and actions in an educational quiz setting. Preliminary findings suggest that while warnings improve accuracy and awareness of high-level hallucinations, they may also introduce cognitive friction, leading to confusion and diminished trust in the system. By examining these interactions, this work contributes to the broader goal of AI-augmented reasoning: developing systems that actively support human reflection, critical thinking, and informed decision-making rather than passive information consumption.

</details>


### [21] [Texture: Structured Exploration of Text Datasets](https://arxiv.org/abs/2504.16898)

*Will Epperson, Arpit Mathur, Adam Perer, Dominik Moritz*

**Main category:** cs.HC

**TL;DR:** Texture is a general-purpose interactive text exploration tool that enhances data analysis through a flexible and configurable schema for text representation.

<details>
  <summary>Details</summary>

**Motivation:** Current text visualization tools are limited by fixed representations and require users to switch tools for different analytical goals, hindering exploratory analysis.

**Method:** Texture introduces a configurable data schema for representing text documents with attributes at various granularities and combines multiple interactive methods for a cohesive exploration interface.

**Result:** In user studies, Texture allowed participants to effectively represent dataset attributes, iterate quickly during analysis, and uncover new insights about their data.

**Conclusion:** Texture contributes to the design of more scalable and flexible exploration systems, enhancing users' understanding of text data.

**Abstract:** Exploratory analysis of a text corpus is essential for assessing data quality and developing meaningful hypotheses. Text analysis relies on understanding documents through structured attributes spanning various granularities of the documents such as words, phrases, sentences, topics, or clusters. However, current text visualization tools typically adopt a fixed representation tailored to specific tasks or domains, requiring users to switch tools as their analytical goals change. To address this limitation, we present Texture, a general-purpose interactive text exploration tool. Texture introduces a configurable data schema for representing text documents enriched with descriptive attributes. These attributes can appear at arbitrary levels of granularity in the text and possibly have multiple values, including document-level attributes, multi-valued attributes (e.g., topics), fine-grained span-level attributes (e.g., words), and vector embeddings. The system then combines existing interactive methods for text exploration into a single interface that provides attribute overview visualizations, supports cross-filtering attribute charts to explore subsets, uses embeddings for a dataset overview and similar instance search, and contextualizes filters in the actual documents. We evaluated Texture through a two-part user study with 10 participants from varied domains who each analyzed their own dataset in a baseline session and then with Texture. Texture was able to represent all of the previously derived dataset attributes, enabled participants to more quickly iterate during their exploratory analysis, and discover new insights about their data. Our findings contribute to the design of scalable, interactive, and flexible exploration systems that improve users' ability to make sense of text data.

</details>


<div id='cs.LG'></div>

## cs.LG [[Back]](#toc)

### [22] [Representation Learning for Tabular Data: A Comprehensive Survey](https://arxiv.org/abs/2504.16109)

*Jun-Peng Jiang, Si-Yang Liu, Hao-Run Cai, Qile Zhou, Han-Jia Ye*

**Main category:** cs.LG

**TL;DR:** This survey investigates tabular representation learning, examining various models and methodologies for machine learning on tabular data, particularly emphasizing Deep Neural Networks (DNNs).

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the growing prevalence of tabular data in machine learning and the evolution of models capable of effectively learning from this data type, highlighting the need to understand their capabilities and limitations.

**Method:** The survey categorizes existing tabular representation learning methods into specialized, transferable, and general models, and introduces a hierarchical taxonomy for specialized models while discussing various strategies, including ensemble methods and their applications.

**Result:** The categorization reveals that specialized models are tailored for specific data distributions, transferable models build on pre-existing knowledge for diverse tasks, and general models enable direct application without fine-tuning, all evaluated against benchmarks and challenges.

**Conclusion:** The paper concludes that a comprehensive understanding of tabular representation learning can facilitate advancements in machine learning applications involving tabular data and encourages further exploration in related areas.

**Abstract:** Tabular data, structured as rows and columns, is among the most prevalent data types in machine learning classification and regression applications. Models for learning from tabular data have continuously evolved, with Deep Neural Networks (DNNs) recently demonstrating promising results through their capability of representation learning. In this survey, we systematically introduce the field of tabular representation learning, covering the background, challenges, and benchmarks, along with the pros and cons of using DNNs. We organize existing methods into three main categories according to their generalization capabilities: specialized, transferable, and general models. Specialized models focus on tasks where training and evaluation occur within the same data distribution. We introduce a hierarchical taxonomy for specialized models based on the key aspects of tabular data -- features, samples, and objectives -- and delve into detailed strategies for obtaining high-quality feature- and sample-level representations. Transferable models are pre-trained on one or more datasets and subsequently fine-tuned on downstream tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources, or even cross-modalities such as vision and language. General models, also known as tabular foundation models, extend this concept further, allowing direct application to downstream tasks without fine-tuning. We group these general models based on the strategies used to adapt across heterogeneous datasets. Additionally, we explore ensemble methods, which integrate the strengths of multiple tabular models. Finally, we discuss representative extensions of tabular learning, including open-environment tabular machine learning, multimodal learning with tabular data, and tabular understanding. More information can be found in the following repository: https://github.com/LAMDA-Tabular/Tabular-Survey.

</details>


### [23] [Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement](https://arxiv.org/abs/2504.16136)

*Chiung-Yi Tseng, Junhao Song, Ziqian Bi, Tianyang Wang, Chia Xin Liang, Ming Liu*

**Main category:** cs.LG

**TL;DR:** The paper discusses Active Learning (AL) as a method to improve machine learning performance with fewer labeled examples, addressing challenges and research topics in the field.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of data abundance and annotation scarcity in machine learning, which hinders model advancement.

**Method:** It provides an overview of Active Learning, exploring its concepts, applications, and key research topics such as uncertainty estimation, fairness, and evaluation metrics.

**Result:** Active Learning is shown to outperform passive learning, especially with robust evaluation measures, while also highlighting significant challenges in the field.

**Conclusion:** The paper serves as a resource for researchers and practitioners, offering insights and future directions for improving Active Learning methodologies.

**Abstract:** In the era of data-driven intelligence, the paradox of data abundance and annotation scarcity has emerged as a critical bottleneck in the advancement of machine learning. This paper gives a detailed overview of Active Learning (AL), which is a strategy in machine learning that helps models achieve better performance using fewer labeled examples. It introduces the basic concepts of AL and discusses how it is used in various fields such as computer vision, natural language processing, transfer learning, and real-world applications. The paper focuses on important research topics such as uncertainty estimation, handling of class imbalance, domain adaptation, fairness, and the creation of strong evaluation metrics and benchmarks. It also shows that learning methods inspired by humans and guided by questions can improve data efficiency and help models learn more effectively. In addition, this paper talks about current challenges in the field, including the need to rebuild trust, ensure reproducibility, and deal with inconsistent methodologies. It points out that AL often gives better results than passive learning, especially when good evaluation measures are used. This work aims to be useful for both researchers and practitioners by providing key insights and proposing directions for future progress in active learning.

</details>


### [24] [SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures](https://arxiv.org/abs/2504.16140)

*Max Hartman, Lav Varshney*

**Main category:** cs.LG

**TL;DR:** SparseJEPA enhances Joint Embedding Predictive Architectures by integrating sparse representation learning, improving interpretability and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** To address the interpretability and inefficiencies of dense embeddings in JEPA frameworks.

**Method:** SparseJEPA introduces a penalty method that promotes sharing latent space variables among semantically related data features while ensuring predictive performance.

**Result:** The framework was tested on the CIFAR-100 dataset, where it improved embedding quality and performed well in linear-probe transfer learning across various tasks.

**Conclusion:** Incorporating sparsity refines the latent space and enhances the interpretability of learned representations, with plans for further advancements in object-centric representation learning.

**Abstract:** Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful framework for learning general-purpose representations. However, these models often lack interpretability and suffer from inefficiencies due to dense embedding representations. We propose SparseJEPA, an extension that integrates sparse representation learning into the JEPA framework to enhance the quality of learned representations. SparseJEPA employs a penalty method that encourages latent space variables to be shared among data features with strong semantic relationships, while maintaining predictive performance. We demonstrate the effectiveness of SparseJEPA by training on the CIFAR-100 dataset and pre-training a lightweight Vision Transformer. The improved embeddings are utilized in linear-probe transfer learning for both image classification and low-level tasks, showcasing the architecture's versatility across different transfer tasks. Furthermore, we provide a theoretical proof that demonstrates that the grouping mechanism enhances representation quality. This was done by displaying that grouping reduces Multiinformation among latent-variables, including proofing the Data Processing Inequality for Multiinformation. Our results indicate that incorporating sparsity not only refines the latent space but also facilitates the learning of more meaningful and interpretable representations. In further work, hope to further extend this method by finding new ways to leverage the grouping mechanism through object-centric representation learning.

</details>


### [25] [Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges](https://arxiv.org/abs/2504.16141)

*Yue Shi, Liangxiu Han, Xin Zhang, Tam Sobeih, Thomas Gaiser, Nguyen Huu Thuy, Dominik Behrend, Amit Kumar Srivastava, Krishnagopal Halder, Frank Ewert*

**Main category:** cs.LG

**TL;DR:** This study systematically reviews agricultural modeling approaches, comparing process-based models (PBMs), deep learning (DL), and hybrid frameworks, demonstrating that hybrids outperform standalone models in robustness and generalization.

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the strengths and limitations of PBMs and DL in agricultural modeling and propose improvements through hybrid frameworks that combine both approaches.

**Method:** The study includes a systematic review of PBMs, DL models, and hybrid frameworks, complemented by a case study on crop dry biomass prediction, comparing the performance of hybrid models against standalone PBMs and DL models under varying data conditions.

**Result:** Hybrid models consistently outperformed traditional PBMs and DL models, showing greater robustness to noisy data and better generalization in unseen locations.

**Conclusion:** The integration of domain knowledge with AI techniques can yield scalable, interpretable, and reproducible agricultural models that enhance decision-making for sustainable agriculture.

**Abstract:** Process-based models (PBMs) and deep learning (DL) are two key approaches in agricultural modelling, each offering distinct advantages and limitations. PBMs provide mechanistic insights based on physical and biological principles, ensuring interpretability and scientific rigour. However, they often struggle with scalability, parameterisation, and adaptation to heterogeneous environments. In contrast, DL models excel at capturing complex, nonlinear patterns from large datasets but may suffer from limited interpretability, high computational demands, and overfitting in data-scarce scenarios.   This study presents a systematic review of PBMs, DL models, and hybrid PBM-DL frameworks, highlighting their applications in agricultural and environmental modelling. We classify hybrid PBM-DL approaches into DL-informed PBMs, where neural networks refine process-based models, and PBM-informed DL, where physical constraints guide deep learning predictions. Additionally, we conduct a case study on crop dry biomass prediction, comparing hybrid models against standalone PBMs and DL models under varying data quality, sample sizes, and spatial conditions. The results demonstrate that hybrid models consistently outperform traditional PBMs and DL models, offering greater robustness to noisy data and improved generalisation across unseen locations.   Finally, we discuss key challenges, including model interpretability, scalability, and data requirements, alongside actionable recommendations for advancing hybrid modelling in agriculture. By integrating domain knowledge with AI-driven approaches, this study contributes to the development of scalable, interpretable, and reproducible agricultural models that support data-driven decision-making for sustainable agriculture.

</details>


### [26] [Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis](https://arxiv.org/abs/2504.16214)

*Xiao Zhang, Yaoyao Ding, Yang Hu, Gennady Pekhimenko*

**Main category:** cs.LG

**TL;DR:** Hexcute is a tile-based programming language designed to optimize matrix multiplication for deep learning workloads on GPUs, providing significant speedups over existing compilers.

<details>
  <summary>Details</summary>

**Motivation:** Deep learning quantization techniques and mixed input data types necessitate new operators that complicate GPU optimization, demanding a balance between expressiveness and ease of programming for efficient implementation.

**Method:** Hexcute is proposed as a tile-based programming language that exposes shared memory and register abstractions, automates layout and task mapping synthesis using a type-inference-based algorithm, and leverages task mapping for GPU program scheduling.

**Result:** Hexcute generalizes to a variety of deep learning operators and achieves speedups of 1.7-11.28 times over existing deep learning compilers for mixed-type operators, alongside a 2.91 times speedup in end-to-end evaluations.

**Conclusion:** Hexcute effectively enhances GPU optimization for deep learning workloads, providing a powerful tool that balances expressiveness with reduced engineering efforts.

**Abstract:** Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL quantization techniques demand a new matrix multiplication operator with mixed input data types, further complicating GPU optimization. Prior high-level compilers like Triton lack the expressiveness to implement key optimizations like fine-grained data pipelines and hardware-friendly memory layouts for these operators, while low-level programming models, such as Hidet, Graphene, and CUTLASS, require significant programming efforts. To balance expressiveness with engineering effort, we propose Hexcute, a tile-based programming language that exposes shared memory and register abstractions to enable fine-grained optimization for these operators. Additionally, Hexcute leverages task mapping to schedule the GPU program, and to reduce programming efforts, it automates layout and task mapping synthesis with a novel type-inference-based algorithm. Our evaluation shows that Hexcute generalizes to a wide range of DL operators, achieves 1.7-11.28$\times$ speedup over existing DL compilers for mixed-type operators, and brings up to 2.91$\times$ speedup in the end-to-end evaluation.

</details>


### [27] [Using Phonemes in cascaded S2S translation pipeline](https://arxiv.org/abs/2504.16234)

*Rene Pilz, Johannes Schneider*

**Main category:** cs.LG

**TL;DR:** The paper investigates the use of phonemes in multilingual speech-to-speech translation instead of text, revealing comparable performance with resource advantages.

<details>
  <summary>Details</summary>

**Motivation:** To improve multilingual simultaneous speech-to-speech translation by exploring phonemic representation as an alternative to traditional text-based methods.

**Method:** An open-source sequence-to-sequence model was trained on the WMT17 dataset in both standard textual and phonemic formats, with performance evaluated using BLEU scores.

**Result:** The phonemic approach achieved comparable translation quality to the text-based method while demonstrating advantages like lower resource requirements and better adaptation for low-resource languages.

**Conclusion:** Phonemic representation in speech translation offers viable benefits over traditional text-based methods, particularly in resource-constrained scenarios.

**Abstract:** This paper explores the idea of using phonemes as a textual representation within a conventional multilingual simultaneous speech-to-speech translation pipeline, as opposed to the traditional reliance on text-based language representations. To investigate this, we trained an open-source sequence-to-sequence model on the WMT17 dataset in two formats: one using standard textual representation and the other employing phonemic representation. The performance of both approaches was assessed using the BLEU metric. Our findings shows that the phonemic approach provides comparable quality but offers several advantages, including lower resource requirements or better suitability for low-resource languages.

</details>


### [28] [General Post-Processing Framework for Fairness Adjustment of Machine Learning Models](https://arxiv.org/abs/2504.16238)

*Léandre Eberhard, Nirek Sharma, Filipp Shelobolin, Aalok Ganesh Shanbhag*

**Main category:** cs.LG

**TL;DR:** The paper presents a novel framework for fairness adjustments in machine learning, which preserves model performance while enabling flexibility and interpretable insights.

<details>
  <summary>Details</summary>

**Motivation:** As machine learning affects important sectors like credit underwriting and public policy, there is a pressing need to ensure compliance with fairness constraints for legal and ethical reasons.

**Method:** The proposed framework adapts in-processing techniques for post-processing to decouple fairness adjustments from model training, allowing for various fairness metrics to be applied across different machine learning tasks.

**Result:** The framework was demonstrated to achieve a comparable fairness/accuracy tradeoff when compared to Adversarial Debiasing on real-world datasets, showing its effectiveness in practical applications.

**Conclusion:** The new framework offers significant advantages over traditional methods, including flexibility, compatibility with proprietary models, and insights into fairness adjustments without compromising model performance.

**Abstract:** As machine learning increasingly influences critical domains such as credit underwriting, public policy, and talent acquisition, ensuring compliance with fairness constraints is both a legal and ethical imperative. This paper introduces a novel framework for fairness adjustments that applies to diverse machine learning tasks, including regression and classification, and accommodates a wide range of fairness metrics. Unlike traditional approaches categorized as pre-processing, in-processing, or post-processing, our method adapts in-processing techniques for use as a post-processing step. By decoupling fairness adjustments from the model training process, our framework preserves model performance on average while enabling greater flexibility in model development. Key advantages include eliminating the need for custom loss functions, enabling fairness tuning using different datasets, accommodating proprietary models as black-box systems, and providing interpretable insights into the fairness adjustments. We demonstrate the effectiveness of this approach by comparing it to Adversarial Debiasing, showing that our framework achieves a comparable fairness/accuracy tradeoff on real-world datasets.

</details>


### [29] [FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness](https://arxiv.org/abs/2504.16255)

*Tina Behzad, Mithilesh Kumar Singh, Anthony J. Ripa, Klaus Mueller*

**Main category:** cs.LG

**TL;DR:** FairPlay is a web-based tool that helps multiple stakeholders collaboratively debias datasets and negotiate fairness without a single standard.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of differing stakeholder demands for fairness in decision-making, emphasizing the need for a method that allows collaboration instead of enforcing a singular definition of fairness.

**Method:** The authors developed a web-based software application called FairPlay that facilitates negotiation among stakeholders to achieve a consensus on fairness by allowing collaborative dataset debiasing.

**Result:** User studies indicate that FairPlay enables users to reach a consensus in approximately five rounds of gameplay, effectively illustrating its utility in promoting fairness in AI systems.

**Conclusion:** The application of FairPlay shows promise in enhancing fairness in decision-making processes within AI by allowing diverse stakeholders to collaborate and negotiate their differing perspectives on fairness.

**Abstract:** The issue of fairness in decision-making is a critical one, especially given the variety of stakeholder demands for differing and mutually incompatible versions of fairness. Adopting a strategic interaction of perspectives provides an alternative to enforcing a singular standard of fairness. We present a web-based software application, FairPlay, that enables multiple stakeholders to debias datasets collaboratively. With FairPlay, users can negotiate and arrive at a mutually acceptable outcome without a universally agreed-upon theory of fairness. In the absence of such a tool, reaching a consensus would be highly challenging due to the lack of a systematic negotiation process and the inability to modify and observe changes. We have conducted user studies that demonstrate the success of FairPlay, as users could reach a consensus within about five rounds of gameplay, illustrating the application's potential for enhancing fairness in AI systems.

</details>


### [30] [Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching](https://arxiv.org/abs/2504.16262)

*Junn Yong Loo, Michelle Adeline, Julia Kaiwen Lau, Fang Yu Leong, Hwa Hui Tew, Arghya Pal, Vishnu Monn Baskaran, Chee-Ming Ting, Raphaël C. -W. Phan*

**Main category:** cs.LG

**TL;DR:** The paper introduces Variational Potential Flow Bayes (VPFB), a flexible and interpretable energy-based generative model that avoids implicit MCMC sampling, facilitating robust generative modeling.

<details>
  <summary>Details</summary>

**Motivation:** To address the instability and expense of contrastive divergence training in high-dimensional settings and to explore the under-researched relationship between potential flows and explicit energy-based models.

**Method:** VPFB learns an energy-parameterized potential flow by constructing a flow-driven density homotopy, minimizing the Kullback-Leibler divergence between the flow-driven and marginal homotopies through a variational loss.

**Result:** Experiments demonstrate that VPFB is effective in tasks such as image generation, interpolation, out-of-distribution detection, and compositional generation, competing well with existing methods in sample quality and versatility.

**Conclusion:** VPFB provides a principled and efficient framework for generative modeling that maintains the interpretability of energy-based models.

**Abstract:** Energy-based models (EBMs) are a powerful class of probabilistic generative models due to their flexibility and interpretability. However, relationships between potential flows and explicit EBMs remain underexplored, while contrastive divergence training via implicit Markov chain Monte Carlo (MCMC) sampling is often unstable and expensive in high-dimensional settings. In this paper, we propose Variational Potential Flow Bayes (VPFB), a new energy-based generative framework that eliminates the need for implicit MCMC sampling and does not rely on auxiliary networks or cooperative training. VPFB learns an energy-parameterized potential flow by constructing a flow-driven density homotopy that is matched to the data distribution through a variational loss minimizing the Kullback-Leibler divergence between the flow-driven and marginal homotopies. This principled formulation enables robust and efficient generative modeling while preserving the interpretability of EBMs. Experimental results on image generation, interpolation, out-of-distribution detection, and compositional generation confirm the effectiveness of VPFB, showing that our method performs competitively with existing approaches in terms of sample quality and versatility across diverse generative modeling tasks.

</details>


### [31] [Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models](https://arxiv.org/abs/2504.16263)

*Magnus Sieverding, Nathan Steffen, Kelly Cohen*

**Main category:** cs.LG

**TL;DR:** The paper evaluates a Gradient-Optimized Fuzzy Inference System (GF) against various machine learning models, demonstrating its superior performance and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** To benchmark a novel GF classifier's performance against established machine learning models and validate its efficiency and robustness.

**Method:** The GF classifier was evaluated on five diverse datasets from the UCI Machine Learning Repository, using gradient descent for optimization.

**Result:** The GF model achieved competitive to superior classification accuracy with high precision and low training times, showing strong consistency across different datasets.

**Conclusion:** Gradient-optimized fuzzy systems can serve as interpretable, efficient, and adaptable alternatives to complex deep learning models in supervised tasks.

**Abstract:** This paper presents a performance benchmarking study of a Gradient-Optimized Fuzzy Inference System (GF) classifier against several state-of-the-art machine learning models, including Random Forest, XGBoost, Logistic Regression, Support Vector Machines, and Neural Networks. The evaluation was conducted across five datasets from the UCI Machine Learning Repository, each chosen for their diversity in input types, class distributions, and classification complexity. Unlike traditional Fuzzy Inference Systems that rely on derivative-free optimization methods, the GF leverages gradient descent to significantly improving training efficiency and predictive performance. Results demonstrate that the GF model achieved competitive, and in several cases superior, classification accuracy while maintaining high precision and exceptionally low training times. In particular, the GF exhibited strong consistency across folds and datasets, underscoring its robustness in handling noisy data and variable feature sets. These findings support the potential of gradient optimized fuzzy systems as interpretable, efficient, and adaptable alternatives to more complex deep learning models in supervised learning tasks.

</details>


### [32] [Boosting Classifier Performance with Opposition-Based Data Transformation](https://arxiv.org/abs/2504.16268)

*Abdesslem Layeb*

**Main category:** cs.LG

**TL;DR:** The paper presents a data transformation framework using Opposition-Based Learning (OBL) to enhance traditional classification algorithms, demonstrating improved accuracy and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of traditional classification algorithms through a novel data transformation approach.

**Method:** The study explores three variants of OBL (Global OBL, Class-Wise OBL, Localized Class-Wise OBL) and integrates them with classifiers like KNN, SVM, LR, and DT across 26 datasets.

**Result:** OBL-enhanced classifiers consistently outperform standard versions in accuracy and F1-score, often achieving near-perfect classification and showing improved computational efficiency, especially in SVM and LR.

**Conclusion:** OBL is a promising lightweight data transformation strategy for enhancing classification performance in complex or sparse learning scenarios.

**Abstract:** In this paper, we introduce a novel data transformation framework based on Opposition-Based Learning (OBL) to boost the performance of traditional classification algorithms. Originally developed to accelerate convergence in optimization tasks, OBL is leveraged here to generate synthetic opposite samples that replace the acutely training data and improve decision boundary formation. We explore three OBL variants; Global OBL, Class-Wise OBL, and Localized Class-Wise OBL; and integrate them with several widely used classifiers, including K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments conducted on 26 heterogeneous and high-dimensional datasets demonstrate that OBL-enhanced classifiers consistently outperform their standard counterparts in terms of accuracy and F1-score, frequently achieving near-perfect or perfect classification. Furthermore, OBL contributes to improved computational efficiency, particularly in SVM and LR. These findings underscore the potential of OBL as a lightweight yet powerful data transformation strategy for enhancing classification performance, especially in complex or sparse learning environments.

</details>


### [33] [Learning Explainable Dense Reward Shapes via Bayesian Optimization](https://arxiv.org/abs/2504.16272)

*Ryan Koo, Ian Yang, Vipul Raheja, Mingyi Hong, Kwang-Sung Jun, Dongyeop Kang*

**Main category:** cs.LG

**TL;DR:** This paper presents a novel approach to enhance reward shaping in reinforcement learning from human feedback for language model alignment by utilizing explainability methods for per-token credit assignment.

<details>
  <summary>Details</summary>

**Motivation:** Current RLHF methods use sparse scalar rewards for sequences, leading to ineffective credit assignment for individual tokens and poor model performance.

**Method:** The authors frame the problem as an optimization task and propose a reward-shaping function based on explainability techniques (SHAP and LIME) to assign per-token rewards. A bilevel optimization framework is employed to combine Bayesian Optimization with policy training to manage noise in the reward estimates.

**Result:** The proposed method results in better token-level reward attribution, leading to significant performance improvements on downstream tasks and more efficient policy training.

**Conclusion:** The findings indicate that using explainability methods for reward shaping does not only improve performance but also maintains the optimal policy consistent with the original reward structure.

**Abstract:** Current reinforcement learning from human feedback (RLHF) pipelines for large language model (LLM) alignment typically assign scalar rewards to sequences, using the final token as a surrogate indicator for the quality of the entire sequence. However, this leads to sparse feedback and suboptimal token-level credit assignment. In this work, we frame reward shaping as an optimization problem focused on token-level credit assignment. We propose a reward-shaping function leveraging explainability methods such as SHAP and LIME to estimate per-token rewards from the reward model. To learn parameters of this shaping function, we employ a bilevel optimization framework that integrates Bayesian Optimization and policy training to handle noise from the token reward estimates. Our experiments show that achieving a better balance of token-level reward attribution leads to performance improvements over baselines on downstream tasks and finds an optimal policy faster during training. Furthermore, we show theoretically that explainability methods that are feature additive attribution functions maintain the optimal policy as the original reward.

</details>


### [34] [Quantum Doubly Stochastic Transformers](https://arxiv.org/abs/2504.16275)

*Jannis Born, Filip Skogh, Kahn Rhrissorrakrai, Filippo Utro, Nico Wagner, Aleksandros Sobczyk*

**Main category:** cs.LG

**TL;DR:** The paper introduces a hybrid classical-quantum Transformer called QDSFormer, which replaces the traditional Softmax in self-attention layers with a variational quantum circuit to improve training stability and performance.

<details>
  <summary>Details</summary>

**Motivation:** The instability in training Transformers due to the Softmax normalization led to exploring doubly stochastic matrices (DSMs) for improved stability and performance, using Sinkhorn's algorithm. The potential of parametric quantum circuits offers a new avenue for obtaining DSMs with advantages over classical methods.

**Method:** The proposed QDSFormer utilizes a variational quantum circuit to replace Softmax in the self-attention mechanism, allowing for the creation of more diverse and information-preserving doubly stochastic matrices compared to classical alternatives.

**Result:** Experimental results demonstrate that QDSFormer outperforms both standard Vision Transformers and other doubly stochastic Transformers in multiple small-scale object recognition tasks, exhibiting improved training stability and lower performance variation.

**Conclusion:** The QDSFormer presents a promising approach for alleviating the training issues of Vision Transformers, especially on smaller datasets, establishing a novel intersection of classical and quantum methodologies within the framework.

**Abstract:** At the core of the Transformer, the Softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often destabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn's algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn's algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard Vision Transformer and other doubly stochastic Transformers. Beyond the established Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. The QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data.

</details>


### [35] [An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon](https://arxiv.org/abs/2504.16276)

*Abhishek Jana, Moeumu Uili, James Atherton, Mark O'Brien, Joe Wood, Leandra Brickson*

**Main category:** cs.LG

**TL;DR:** An automated one-shot bird call classification pipeline is developed for rare bird species, achieving high accuracy and recall using minimal training data.

<details>
  <summary>Details</summary>

**Motivation:** To create a classifier that addresses the critical need for detecting rare bird species, which lack sufficient training data in existing models.

**Method:** The approach uses large bird classification networks' embedding spaces and a cosine similarity classifier, enhanced with filtering and denoising preprocessing techniques.

**Result:** The model demonstrated 1.0 recall and 0.95 accuracy in detecting calls of the critically endangered tooth-billed pigeon, validated through simulated recordings and a real-world test.

**Conclusion:** This open-source system offers a practical solution for conservationists to monitor endangered bird species effectively.

**Abstract:** This paper presents an automated one-shot bird call classification pipeline designed for rare species absent from large publicly available classifiers like BirdNET and Perch. While these models excel at detecting common birds with abundant training data, they lack options for species with only 1-3 known recordings-a critical limitation for conservationists monitoring the last remaining individuals of endangered birds. To address this, we leverage the embedding space of large bird classification networks and develop a classifier using cosine similarity, combined with filtering and denoising preprocessing techniques, to optimize detection with minimal training data. We evaluate various embedding spaces using clustering metrics and validate our approach in both a simulated scenario with Xeno-Canto recordings and a real-world test on the critically endangered tooth-billed pigeon (Didunculus strigirostris), which has no existing classifiers and only three confirmed recordings. The final model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon calls, making it practical for use in the field. This open-source system provides a practical tool for conservationists seeking to detect and monitor rare species on the brink of extinction.

</details>


### [36] [DataS^3: Dataset Subset Selection for Specialization](https://arxiv.org/abs/2504.16277)

*Neha Hulkund, Alaa Maalouf, Levi Cai, Daniel Yang, Tsun-Hsuan Wang, Abigail O'Neil, Timm Haucke, Sandeep Mukherjee, Vikram Ramaswamy, Judy Hansen Shen, Gabriel Tseng, Mike Walmsley, Daniela Rus, Ken Goldberg, Hannah Kerner, Irene Chen, Yogesh Girdhar, Sara Beery*

**Main category:** cs.LG

**TL;DR:** The paper addresses the issue of suboptimal performance in machine learning models due to discrepancies between training and deployment data distributions by formalizing dataset subset selection for specialization (DS3) and introducing a benchmark dataset, DataS^3.

<details>
  <summary>Details</summary>

**Motivation:** Machine learning applications often require models that perform well on specific deployments with unique, imbalanced data distributions, highlighting the importance of selecting deployment-specialized training data.

**Method:** The authors formalize the dataset subset selection for specialization (DS3) problem and introduce DataS^3, a benchmark dataset. They evaluate various algorithms, including coreset, data filtering, and data curation methods, on this dataset.

**Result:** The study reveals that general-distribution methods fail on deployment-specific tasks, while manually curated expert subsets significantly outperform models trained on the entire dataset, achieving accuracy gains of up to 51.3 percent.

**Conclusion:** The findings emphasize the necessity of tailored dataset curation for improving machine learning performance and efficiency, especially as more global datasets become available for real-world deployments.

**Abstract:** In many real-world machine learning (ML) applications (e.g. detecting broken bones in x-ray images, detecting species in camera traps), in practice models need to perform well on specific deployments (e.g. a specific hospital, a specific national park) rather than the domain broadly. However, deployments often have imbalanced, unique data distributions. Discrepancy between the training distribution and the deployment distribution can lead to suboptimal performance, highlighting the need to select deployment-specialized subsets from the available training data. We formalize dataset subset selection for specialization (DS3): given a training set drawn from a general distribution and a (potentially unlabeled) query set drawn from the desired deployment-specific distribution, the goal is to select a subset of the training data that optimizes deployment performance.   We introduce DataS^3; the first dataset and benchmark designed specifically for the DS3 problem. DataS^3 encompasses diverse real-world application domains, each with a set of distinct deployments to specialize in. We conduct a comprehensive study evaluating algorithms from various families--including coresets, data filtering, and data curation--on DataS^3, and find that general-distribution methods consistently fail on deployment-specific tasks. Additionally, we demonstrate the existence of manually curated (deployment-specific) expert subsets that outperform training on all available data with accuracy gains up to 51.3 percent. Our benchmark highlights the critical role of tailored dataset curation in enhancing performance and training efficiency on deployment-specific distributions, which we posit will only become more important as global, public datasets become available across domains and ML models are deployed in the real world.

</details>


### [37] [Affect Models Have Weak Generalizability to Atypical Speech](https://arxiv.org/abs/2504.16283)

*Jaya Narain, Amrit Romana, Vikramjit Mitra, Colin Lea, Shirley Ren*

**Main category:** cs.LG

**TL;DR:** Speech and voice conditions significantly affect paralinguistic models' performance for recognizing emotions in atypical speech compared to typical speech datasets.

<details>
  <summary>Details</summary>

**Motivation:** To explore how speech and voice conditions influence the acoustic properties affecting emotional recognition in atypical speech.

**Method:** Evaluation of publicly available models for recognizing categorical and dimensional affect from speech using a dataset of atypical speech, examining intelligibility, monopitch, and harshness dimensions.

**Result:** Affect model outputs are significantly affected by speech atypicalities; notably, atypical speech is predicted as sad more frequently compared to typical speech datasets. Fine-tuning models on atypical speech data shows improved performance without harming typical speech performance.

**Conclusion:** There is a critical need for broader training datasets for speech emotion recognition models and the development of robust modeling techniques that accommodate speech and voice differences.

**Abstract:** Speech and voice conditions can alter the acoustic properties of speech, which could impact the performance of paralinguistic models for affect for people with atypical speech. We evaluate publicly available models for recognizing categorical and dimensional affect from speech on a dataset of atypical speech, comparing results to datasets of typical speech. We investigate three dimensions of speech atypicality: intelligibility, which is related to pronounciation; monopitch, which is related to prosody, and harshness, which is related to voice quality. We look at (1) distributional trends of categorical affect predictions within the dataset, (2) distributional comparisons of categorical affect predictions to similar datasets of typical speech, and (3) correlation strengths between text and speech predictions for spontaneous speech for valence and arousal. We find that the output of affect models is significantly impacted by the presence and degree of speech atypicalities. For instance, the percentage of speech predicted as sad is significantly higher for all types and grades of atypical speech when compared to similar typical speech datasets. In a preliminary investigation on improving robustness for atypical speech, we find that fine-tuning models on pseudo-labeled atypical speech data improves performance on atypical speech without impacting performance on typical speech. Our results emphasize the need for broader training and evaluation datasets for speech emotion models, and for modeling approaches that are robust to voice and speech differences.

</details>


### [38] [Semantics at an Angle: When Cosine Similarity Works Until It Doesn't](https://arxiv.org/abs/2504.16318)

*Kisung You*

**Main category:** cs.LG

**TL;DR:** The paper reviews the use and limitations of cosine similarity in comparing embeddings in machine learning, exploring its strengths, breakdown scenarios, and emerging alternatives.

<details>
  <summary>Details</summary>

**Motivation:** To provide a reflective overview of cosine similarity's evolution, its strengths and limitations, and to address its applicability in contexts where embedding norms are semantically significant.

**Method:** The article offers a qualitative examination of the performance of cosine similarity and discusses both theoretical and practical insights into its use and alternatives.

**Result:** Cosine similarity is effective in many scenarios but has limitations that are exacerbated when embedding norms carry semantic weight, prompting the exploration of alternatives.

**Conclusion:** The paper aims to provide clarity on cosine similarity's role in machine learning and suggests that a deeper understanding of embeddings as geometric and philosophical entities can enhance research and application in the field.

**Abstract:** Cosine similarity has become a standard metric for comparing embeddings in modern machine learning. Its scale-invariance and alignment with model training objectives have contributed to its widespread adoption. However, recent studies have revealed important limitations, particularly when embedding norms carry meaningful semantic information. This informal article offers a reflective and selective examination of the evolution, strengths, and limitations of cosine similarity. We highlight why it performs well in many settings, where it tends to break down, and how emerging alternatives are beginning to address its blind spots. We hope to offer a mix of conceptual clarity and practical perspective, especially for quantitative scientists who think about embeddings not just as vectors, but as geometric and philosophical objects.

</details>


### [39] [Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks](https://arxiv.org/abs/2504.16360)

*Mao Wang, Tao Wu, Xingping Xian, Shaojie Qiao, Weina Niu, Canyixing Cui*

**Main category:** cs.LG

**TL;DR:** The paper presents the Graph Optimal Matching Kernel Convolutional Network (GOMKCN), a novel approach to graph representation learning that enhances interpretability and structural pattern recognition in graphs.

<details>
  <summary>Details</summary>

**Motivation:** Existing graph representation learning methods often inadequately characterize graph structures, limiting the analysis of structural patterns, necessitating a more effective approach.

**Method:** GOMKCN treats graphs as node-centric subgraphs, employing the Graph Optimal Matching Kernel (GOMK) as a convolutional operator to compute similarities between subgraphs and learnable filters, projecting them into a Hilbert space for improved pattern recognition.

**Result:** Experiments demonstrate that GOMKCN significantly improves accuracy and interpretability in graph pattern mining and prediction compared to existing methods.

**Conclusion:** The framework provides a theoretical advancement for disentangled graph representation learning, effectively addressing limitations in capturing structural patterns.

**Abstract:** Graphs effectively characterize relational data, driving graph representation learning methods that uncover underlying predictive information. As state-of-the-art approaches, Graph Neural Networks (GNNs) enable end-to-end learning for diverse tasks. Recent disentangled graph representation learning enhances interpretability by decoupling independent factors in graph data. However, existing methods often implicitly and coarsely characterize graph structures, limiting structural pattern analysis within the graph. This paper proposes the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to address this limitation. We view graphs as node-centric subgraphs, where each subgraph acts as a structural factor encoding position-specific information. This transforms graph prediction into structural pattern recognition. Inspired by CNNs, GOMKCN introduces the Graph Optimal Matching Kernel (GOMK) as a convolutional operator, computing similarities between subgraphs and learnable graph filters. Mathematically, GOMK maps subgraphs and filters into a Hilbert space, representing graphs as point sets. Disentangled representations emerge from projecting subgraphs onto task-optimized filters, which adaptively capture relevant structural patterns via gradient descent. Crucially, GOMK incorporates local correspondences in similarity measurement, resolving the trade-off between differentiability and accuracy in graph kernels. Experiments validate that GOMKCN achieves superior accuracy and interpretability in graph pattern mining and prediction. The framework advances the theoretical foundation for disentangled graph representation learning.

</details>


### [40] [Natural Policy Gradient for Average Reward Non-Stationary RL](https://arxiv.org/abs/2504.16415)

*Neharika Jali, Eshika Pathak, Pranay Sharma, Guannan Qu, Gauri Joshi*

**Main category:** cs.LG

**TL;DR:** The paper introduces a novel model-free policy-based algorithm for non-stationary reinforcement learning, addressing theoretical gaps and presenting performance guarantees.

<details>
  <summary>Details</summary>

**Motivation:** To advance the understanding and effectiveness of policy-based methods in non-stationary reinforcement learning, which are less explored compared to value-based methods.

**Method:** The study proposes the Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient method that incorporates a restart-based exploration strategy, along with a parameter-free bandit-over-RL algorithm, BORL-NS-NAC, capable of operating without prior knowledge of the variation budget.

**Result:** Both NS-NAC and BORL-NS-NAC achieve a dynamic regret of $	ilde{	ext{O}}(|S|^{1/2}|A|^{1/2}	ext{Δ}_T^{1/6}T^{5/6})$, demonstrating efficient performance in non-stationary environments.

**Conclusion:** The proposed algorithms significantly contribute to policy-based approaches in non-stationary reinforcement learning by providing a new theoretical framework and practical parameters for adaptation.

**Abstract:** We consider the problem of non-stationary reinforcement learning (RL) in the infinite-horizon average-reward setting. We model it by a Markov Decision Process with time-varying rewards and transition probabilities, with a variation budget of $\Delta_T$. Existing non-stationary RL algorithms focus on model-based and model-free value-based methods. Policy-based methods despite their flexibility in practice are not theoretically well understood in non-stationary RL. We propose and analyze the first model-free policy-based algorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient method with a restart based exploration for change and a novel interpretation of learning rates as adapting factors. Further, we present a bandit-over-RL based parameter-free algorithm BORL-NS-NAC that does not require prior knowledge of the variation budget $\Delta_T$. We present a dynamic regret of $\tilde{\mathscr O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$ for both algorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of the state and action spaces. The regret analysis leverages a novel adaptation of the Lyapunov function analysis of NAC to dynamic environments and characterizes the effects of simultaneous updates in policy, value function estimate and changes in the environment.

</details>


### [41] [MAGIC: Near-Optimal Data Attribution for Deep Learning](https://arxiv.org/abs/2504.16430)

*Andrew Ilyas, Logan Engstrom*

**Main category:** cs.LG

**TL;DR:** This paper presents MAGIC, a new data attribution method that improves the estimation of the effects of training data changes on model predictions in non-convex settings.

<details>
  <summary>Details</summary>

**Motivation:** Current methods for predictive data attribution struggle in large-scale non-convex settings, often yielding weak correlation with ground truth, highlighting the need for a more effective approach.

**Method:** MAGIC combines classical attribution methods with recent advances in metadifferentiation to estimate the impact of adding or removing training data on model predictions optimally.

**Result:** MAGIC provides significantly improved estimates of the effects of data changes compared to existing methods, achieving a much better correlation with ground truth in non-convex settings.

**Conclusion:** The proposed MAGIC method demonstrates a successful approach to predictive data attribution, addressing the limitations of current methods in non-convex scenarios.

**Abstract:** The goal of predictive data attribution is to estimate how adding or removing a given set of training datapoints will affect model predictions. In convex settings, this goal is straightforward (i.e., via the infinitesimal jackknife). In large-scale (non-convex) settings, however, existing methods are far less successful -- current methods' estimates often only weakly correlate with ground truth. In this work, we present a new data attribution method (MAGIC) that combines classical methods and recent advances in metadifferentiation to (nearly) optimally estimate the effect of adding or removing training data on model predictions.

</details>


### [42] [Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion](https://arxiv.org/abs/2504.16431)

*Ruixiang Zhang, Shuangfei Zhai, Yizhe Zhang, James Thornton, Zijing Ou, Joshua Susskind, Navdeep Jaitly*

**Main category:** cs.LG

**TL;DR:** Target Concrete Score Matching (TCSM) is a novel objective for training discrete diffusion models that enhances flexibility and performance in generating discrete data.

<details>
  <summary>Details</summary>

**Motivation:** To improve the modeling and generation of discrete data through a more versatile training objective for discrete diffusion models.

**Method:** The paper introduces TCSM, which estimates the concrete score of the target distribution directly from data samples, enabling both pre-training and post-training of discrete diffusion models.

**Result:** Experiments show that TCSM matches or exceeds the performance of existing methods on language modeling tasks and demonstrates greater flexibility and sample efficiency.

**Conclusion:** TCSM serves as a general framework for discrete diffusion modeling with broad applicability and improved training capabilities.

**Abstract:** Discrete diffusion is a promising framework for modeling and generating discrete data. In this work, we present Target Concrete Score Matching (TCSM), a novel and versatile objective for training and fine-tuning discrete diffusion models. TCSM provides a general framework with broad applicability. It supports pre-training discrete diffusion models directly from data samples, and many existing discrete diffusion approaches naturally emerge as special cases of our more general TCSM framework. Furthermore, the same TCSM objective extends to post-training of discrete diffusion models, including fine-tuning using reward functions or preference data, and distillation of knowledge from pre-trained autoregressive models. These new capabilities stem from the core idea of TCSM, estimating the concrete score of the target distribution, which resides in the original (clean) data space. This allows seamless integration with reward functions and pre-trained models, which inherently only operate in the clean data space rather than the noisy intermediate spaces of diffusion processes. Our experiments on language modeling tasks demonstrate that TCSM matches or surpasses current methods. Additionally, TCSM is versatile, applicable to both pre-training and post-training scenarios, offering greater flexibility and sample efficiency.

</details>


### [43] [iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network](https://arxiv.org/abs/2504.16432)

*Ziran Liang, Rui An, Wenqi Fan, Yanghui Rao, Yuxuan Liang*

**Main category:** cs.LG

**TL;DR:** The paper introduces iTFKAN, an interpretable model for time series forecasting that enhances both predictive performance and interpretability, addressing limitations in current deep forecasting methods.

<details>
  <summary>Details</summary>

**Motivation:** Current deep forecasting methods often lack interpretability, which impacts their trustworthiness and practical application in safety-critical fields like auto-driving and healthcare.

**Method:** The study proposes iTFKAN, which incorporates model symbolization for interpretability, and integrates prior knowledge injection and time-frequency synergy learning to guide model learning in complex time series data.

**Result:** Experimental results show that iTFKAN achieves strong forecasting performance while maintaining high interpretive capabilities.

**Conclusion:** iTFKAN effectively balances performance and interpretability, making it suitable for safety-critical applications.

**Abstract:** As time evolves, data within specific domains exhibit predictability that motivates time series forecasting to predict future trends from historical data. However, current deep forecasting methods can achieve promising performance but generally lack interpretability, hindering trustworthiness and practical deployment in safety-critical applications such as auto-driving and healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for credible time series forecasting. iTFKAN enables further exploration of model decision rationales and underlying data patterns due to its interpretability achieved through model symbolization. Besides, iTFKAN develops two strategies, prior knowledge injection, and time-frequency synergy learning, to effectively guide model learning under complex intertwined time series data. Extensive experimental results demonstrated that iTFKAN can achieve promising forecasting performance while simultaneously possessing high interpretive capabilities.

</details>


### [44] [Private Federated Learning using Preference-Optimized Synthetic Data](https://arxiv.org/abs/2504.16438)

*Charlie Hou, Mei-Yu Wang, Yige Zhu, Daniel Lazar, Giulia Fanti*

**Main category:** cs.LG

**TL;DR:** The paper introduces a novel algorithm, POPri, for generating privacy-preserving synthetic data in federated learning, enhancing the effectiveness of traditional DP-FL methods.

<details>
  <summary>Details</summary>

**Motivation:** There is a need to improve the performance of differentially private federated learning (DP-FL) by leveraging DP synthetic data, which has shown potential benefits over standard methods in recent studies.

**Method:** The POPri algorithm utilizes preference optimization based on private client feedback to tune large language models (LLMs) for creating high-quality differential privacy synthetic data.

**Result:** POPri significantly enhances the utility of DP synthetic data, achieving a 68% improvement in next-token prediction accuracy relative to fully-private settings on the LargeFedBench dataset, outperforming previous methods.

**Conclusion:** POPri demonstrates substantial advancements over existing methods in generating DP synthetic data for federated learning, positioning it as a compelling alternative to traditional DP-FL techniques.

**Abstract:** In practical settings, differentially private Federated learning (DP-FL) is the dominant method for training models from private, on-device client data. Recent work has suggested that DP-FL may be enhanced or outperformed by methods that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary algorithms for generating DP synthetic data for FL applications require careful prompt engineering based on public information and/or iterative private client feedback. Our key insight is that the private client feedback collected by prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be viewed as a preference ranking. Our algorithm, Preference Optimization for Private Client Data (POPri) harnesses client feedback using preference optimization algorithms such as Direct Preference Optimization (DPO) to fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri, we release LargeFedBench, a new federated text benchmark for uncontaminated LLM evaluations on federated client data. POPri substantially improves the utility of DP synthetic data relative to prior work on LargeFedBench datasets and an existing benchmark from Xie et al. (2024). POPri closes the gap between next-token prediction accuracy in the fully-private and non-private settings by up to 68%, compared to 52% for prior synthetic data methods, and 10% for state-of-the-art DP federated learning methods. The code and data are available at https://github.com/meiyuw/POPri.

</details>


### [45] [Node Assigned physics-informed neural networks for thermal-hydraulic system simulation: CVH/FL module](https://arxiv.org/abs/2504.16447)

*Jeesuk Shin, Cheolwoong Kim, Sunwoong Yang, Minseo Lee, Sung Joong Kim, Joongoo Jeon*

**Main category:** cs.LG

**TL;DR:** This study develops a novel numerical method for thermal-hydraulic system codes using a node-assigned physics-informed neural network (NA-PINN) to improve the accuracy of severe accident simulations in nuclear power plants.

<details>
  <summary>Details</summary>

**Motivation:** Current thermal-hydraulic system codes like MELCOR and MAAP face limitations due to inconsistent finite difference schemes and unidirectional coupling in multi-physics analyses.

**Method:** The study proposes NA-PINN, which assigns individual neural networks to each nodalization in the control volume approach, allowing for accurate temporal solution learning without spatial information in input/output domains.

**Result:** In a simulation of 6 water tanks, NA-PINN achieved a maximum absolute error of 0.007, significantly better than the 1.678 error of the traditional PINN method, demonstrating acceptable accuracy for the first time.

**Conclusion:** This research introduces a successful implementation of system code using PINN and outlines plans to extend NA-PINN into a multi-physics solver.

**Abstract:** Severe accidents (SAs) in nuclear power plants have been analyzed using thermal-hydraulic (TH) system codes such as MELCOR and MAAP. These codes efficiently simulate the progression of SAs, while they still have inherent limitations due to their inconsistent finite difference schemes. The use of empirical schemes incorporating both implicit and explicit formulations inherently induces unidirectional coupling in multi-physics analyses. The objective of this study is to develop a novel numerical method for TH system codes using physics-informed neural network (PINN). They have shown strength in solving multi-physics due to the innate feature of neural networks-automatic differentiation. We propose a node-assigned PINN (NA-PINN) that is suitable for the control volume approach-based system codes. NA-PINN addresses the issue of spatial governing equation variation by assigning an individual network to each nodalization of the system code, such that spatial information is excluded from both the input and output domains, and each subnetwork learns to approximate a purely temporal solution. In this phase, we evaluated the accuracy of the PINN methods for the hydrodynamic module. In the 6 water tank simulation, PINN and NA-PINN showed maximum absolute errors of 1.678 and 0.007, respectively. It should be noted that only NA-PINN demonstrated acceptable accuracy. To the best of the authors' knowledge, this is the first study to successfully implement a system code using PINN. Our future work involves extending NA-PINN to a multi-physics solver and developing it in a surrogate manner.

</details>


### [46] [An Effective Gram Matrix Characterizes Generalization in Deep Networks](https://arxiv.org/abs/2504.16450)

*Rubing Yang, Pratik Chaudhari*

**Main category:** cs.LG

**TL;DR:** The paper derives and analyzes a differential equation governing the generalization gap in deep networks during gradient descent training, revealing key factors affecting generalization performance.

<details>
  <summary>Details</summary>

**Motivation:** To understand the dynamics of the generalization gap in deep networks trained by gradient descent and how different datasets and architectures influence this gap.

**Method:** The authors derive a differential equation influenced by a contraction factor and a perturbation factor and compute an effective Gram matrix to characterize the generalization gap based on this equation.

**Result:** Empirical evaluations show that the analysis can accurately predict test loss, with the training process demonstrating benign characteristics that do not significantly worsen the generalization gap.

**Conclusion:** The alignment of the effective Gram matrix with the initial residual, which varies across datasets and architectures, is crucial for determining generalization quality, suggesting a match of data and architecture is key for effective training.

**Abstract:** We derive a differential equation that governs the evolution of the generalization gap when a deep network is trained by gradient descent. This differential equation is controlled by two quantities, a contraction factor that brings together trajectories corresponding to slightly different datasets, and a perturbation factor that accounts for them training on different datasets. We analyze this differential equation to compute an ``effective Gram matrix'' that characterizes the generalization gap after training in terms of the alignment between this Gram matrix and a certain initial ``residual''. Empirical evaluations on image classification datasets indicate that this analysis can predict the test loss accurately. Further, at any point during training, the residual predominantly lies in the subspace of the effective Gram matrix with the smallest eigenvalues. This indicates that the training process is benign, i.e., it does not lead to significant deterioration of the generalization gap (which is zero at initialization). The alignment between the effective Gram matrix and the residual is different for different datasets and architectures. The match/mismatch of the data and the architecture is primarily responsible for good/bad generalization.

</details>


### [47] [Dynamic Time-aware Continual User Representation Learning](https://arxiv.org/abs/2504.16501)

*Seungyoon Choi, Sein Kim, Hongseok Kang, Wonjoong Kim, Chanyoung Park*

**Main category:** cs.LG

**TL;DR:** This paper presents DITTO, a new framework for continual user representation learning that addresses the limitations of traditional user modeling by considering temporal changes in item distribution.

<details>
  <summary>Details</summary>

**Motivation:** Traditional user modeling methods struggle with generalization and adaptability across multiple tasks, prompting a need for continual learning approaches that can manage shifts in item distribution over time.

**Method:** The paper introduces DITTO, a dynamic time-aware continual user representation learner that mitigates catastrophic forgetting and adjusts knowledge from previous tasks to align with current task distributions.

**Result:** Experimental results reveal that DITTO outperforms existing state-of-the-art methods when evaluated under a more realistic scenario that accounts for changes over time.

**Conclusion:** DITTO provides a significant improvement in user representation learning by addressing temporal dynamics in task distribution, suggesting a new direction for future continual learning research.

**Abstract:** Traditional user modeling (UM) approaches have primarily focused on designing models for a single specific task, but they face limitations in generalization and adaptability across various tasks. Recognizing these challenges, recent studies have shifted towards continual learning (CL)-based universal user representation learning aiming to develop a single model capable of handling multiple tasks. Despite advancements, existing methods are in fact evaluated under an unrealistic scenario that does not consider the passage of time as tasks progress, which overlooks newly emerged items that may change the item distribution of previous tasks. In this paper, we introduce a practical evaluation scenario on which CL-based universal user representation learning approaches should be evaluated, which takes into account the passage of time as tasks progress. Then, we propose a novel framework Dynamic Time-aware continual user representation learner, named DITTO, designed to alleviate catastrophic forgetting despite continuous shifts in item distribution, while also allowing the knowledge acquired from previous tasks to adapt to the current shifted item distribution. Through our extensive experiments, we demonstrate the superiority of DITTO over state-of-the-art methods under a practical evaluation scenario. Our source code is available at https://github.com/seungyoon-Choi/DITTO_official.

</details>


### [48] [A Comprehensive Survey of Synthetic Tabular Data Generation](https://arxiv.org/abs/2504.16506)

*Ruxue Shi, Yili Wang, Mengnan Du, Xu Shen, Xin Wang*

**Main category:** cs.LG

**TL;DR:** This paper provides a comprehensive survey of synthetic tabular data generation, addressing knowledge gaps in recent advancements and proposing a unified taxonomy to organize existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The effective use of tabular data in machine learning is hindered by challenges like data scarcity, privacy concerns, and class imbalance, necessitating better insights into synthetic data generation methods.

**Method:** The paper proposes a taxonomy categorizing synthetic data generation methods into traditional approaches, diffusion-based methods, and LLM-based models, and details the pipeline for data synthesis, post-processing, and evaluation.

**Result:** The survey presents a comparative analysis of existing methods, identifies major challenges, and explores real-world applications, providing a comprehensive overview of the field.

**Conclusion:** The findings guide future research directions and address the underexplored advancements in synthetic tabular data generation.

**Abstract:** Tabular data remains one of the most prevalent and critical data formats across diverse real-world applications. However, its effective use in machine learning (ML) is often constrained by challenges such as data scarcity, privacy concerns, and class imbalance. Synthetic data generation has emerged as a promising solution, leveraging generative models to learn the distribution of real datasets and produce high-fidelity, privacy-preserving samples. Various generative paradigms have been explored, including energy-based models (EBMs), variational autoencoders (VAEs), generative adversarial networks (GANs), large language models (LLMs), and diffusion models. While several surveys have investigated synthetic tabular data generation, most focus on narrow subdomains or specific generative methods, such as GANs, diffusion models, or privacy-preserving techniques. This limited scope often results in fragmented insights, lacking a comprehensive synthesis that bridges diverse approaches. In particular, recent advances driven by LLMs and diffusion-based models remain underexplored. This gap hinders a holistic understanding of the field`s evolution, methodological interplay, and open challenges. To address this, our survey provides a unified and systematic review of synthetic tabular data generation. Our contributions are threefold: (1) we propose a comprehensive taxonomy that organizes existing methods into traditional approaches, diffusion-based methods, and LLM-based models, and provide an in-depth comparative analysis; (2) we detail the complete pipeline for synthetic tabular data generation, including data synthesis, post-processing, and evaluation; (3) we identify major challenges, explore real-world applications, and outline open research questions and future directions to guide future work in this rapidly evolving area.

</details>


### [49] [Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations](https://arxiv.org/abs/2504.16553)

*Mohammad Mahdi Abedi, David Pardo, Tariq Alkhalifah*

**Main category:** cs.LG

**TL;DR:** The paper presents a hybrid optimization framework that enhances the training of Physics-Informed Neural Networks (PINNs) by integrating a least-squares solver into the gradient descent loss function to improve convergence and stability for solving the frequency-domain Helmholtz equation.

<details>
  <summary>Details</summary>

**Motivation:** Standard training of PINNs using gradient descent suffers from slow convergence and instability for high-frequency wavefields, necessitating the development of more efficient training methods.

**Method:** A hybrid optimization framework is proposed that incorporates a least-squares solver directly into the gradient descent loss function, enabling optimal updates for the linear output layer and making the method applicable with or without perfectly matched layers (PML).

**Result:** The proposed LS-enhanced method achieves faster convergence, higher accuracy, and improved stability in numerical experiments compared to conventional PINN training, especially in scenarios where standard GD fails.

**Conclusion:** The LS solver operates on a small normal matrix, leading to minimal computational overhead, thus making the method scalable for large-scale wavefield simulations.

**Abstract:** Physics-Informed Neural Networks (PINNs) have shown promise in solving partial differential equations (PDEs), including the frequency-domain Helmholtz equation. However, standard training of PINNs using gradient descent (GD) suffers from slow convergence and instability, particularly for high-frequency wavefields. For scattered acoustic wavefield simulation based on Helmholtz equation, we derive a hybrid optimization framework that accelerates training convergence by embedding a least-squares (LS) solver directly into the GD loss function. This formulation enables optimal updates for the linear output layer. Our method is applicable with or without perfectly matched layers (PML), and we provide practical tensor-based implementations for both scenarios. Numerical experiments on benchmark velocity models demonstrate that our approach achieves faster convergence, higher accuracy, and improved stability compared to conventional PINN training. In particular, our results show that the LS-enhanced method converges rapidly even in cases where standard GD-based training fails. The LS solver operates on a small normal matrix, ensuring minimal computational overhead and making the method scalable for large-scale wavefield simulations.

</details>


### [50] [Unified Molecule Generation and Property Prediction](https://arxiv.org/abs/2504.16559)

*Adam Izdebski, Jan Olszewski, Pankhil Gawade, Krzysztof Koras, Serra Korkmaz, Valentin Rauscher, Jakub M. Tomczak, Ewa Szczurek*

**Main category:** cs.LG

**TL;DR:** Hyformer is a transformer-based joint model designed for simultaneous data generation and property prediction, overcoming architectural and optimization challenges in joint modeling.

<details>
  <summary>Details</summary>

**Motivation:** There is a need for a unified approach to model both data generation and property prediction, as current methods often operate separately, limiting their capabilities.

**Method:** Hyformer employs a transformer architecture with an alternating attention mask and a unified pre-training scheme to effectively combine generative and predictive functionalities.

**Result:** Hyformer demonstrates competitive performance against other joint models and state-of-the-art methods in molecule generation and property prediction, with positive impacts on downstream tasks such as molecular representation learning, hit identification, and antimicrobial peptide design.

**Conclusion:** The joint modeling approach with Hyformer enhances model capabilities and offers significant advantages in various molecular tasks.

**Abstract:** Modeling the joint distribution of the data samples and their properties allows to construct a single model for both data generation and property prediction, with synergistic capabilities reaching beyond purely generative or predictive models. However, training joint models presents daunting architectural and optimization challenges. Here, we propose Hyformer, a transformer-based joint model that successfully blends the generative and predictive functionalities, using an alternating attention mask together with a unified pre-training scheme. We show that Hyformer rivals other joint models, as well as state-of-the-art molecule generation and property prediction models. Additionally, we show the benefits of joint modeling in downstream tasks of molecular representation learning, hit identification and antimicrobial peptide design.

</details>


### [51] [Hyper-Transforming Latent Diffusion Models](https://arxiv.org/abs/2504.16580)

*Ignacio Peis, Batuhan Koyuncu, Isabel Valera, Jes Frellsen*

**Main category:** cs.LG

**TL;DR:** The paper presents a generative framework combining Implicit Neural Representations and Transformer-based hypernetworks to enhance latent variable models in terms of scalability and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the scalability limitations of MLP-based hypernetworks in generative models by leveraging Transformers for improved representation capacity and computational efficiency.

**Method:** The framework integrates INRs with Transformer-based hypernetworks in latent diffusion models, allowing for adaptation through either full training or the hyper-transforming strategy, which fine-tunes the decoder while keeping the latent space constant.

**Result:** The proposed method enables efficient generation of INR parameters and enhances model performance without the need for complete retraining of generative models.

**Conclusion:** The integration of Transformers significantly improves the generative capacity and efficiency of latent variable models, facilitating smoother adaptations to INR representations.

**Abstract:** We introduce a novel generative framework for functions by integrating Implicit Neural Representations (INRs) and Transformer-based hypernetworks into latent variable models. Unlike prior approaches that rely on MLP-based hypernetworks with scalability limitations, our method employs a Transformer-based decoder to generate INR parameters from latent variables, addressing both representation capacity and computational efficiency. Our framework extends latent diffusion models (LDMs) to INR generation by replacing standard decoders with a Transformer-based hypernetwork, which can be trained either from scratch or via hyper-transforming-a strategy that fine-tunes only the decoder while freezing the pre-trained latent space. This enables efficient adaptation of existing generative models to INR-based representations without requiring full retraining.

</details>


### [52] [Enhancing Variable Selection in Large-scale Logistic Regression: Leveraging Manual Labeling with Beneficial Noise](https://arxiv.org/abs/2504.16585)

*Xiaofei Wu, Rongmei Liang*

**Main category:** cs.LG

**TL;DR:** This paper investigates the benefits of label noise in penalized logistic regression (PLR) for variable selection and presents a distributed computing algorithm to handle PLR with manual labeling.

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the overfitting issue in large-scale supervised learning with PLR and explore how label noise from manual labeling aids in variable selection.

**Method:** The paper develops a partition-insensitive parallel algorithm based on the ADMM framework to implement PLR, ensuring data distribution does not affect the solution, along with demonstrating global convergence and sublinear convergence rate.

**Result:** Empirical results show that PLR using manually-labeled noisy data outperforms traditional variable selection classification methods in estimation and classification accuracy over various large-scale datasets.

**Conclusion:** The findings suggest that label noise can be beneficial for variable selection in PLR, and the proposed algorithm effectively scales PLR for large datasets while maintaining accuracy.

**Abstract:** In large-scale supervised learning, penalized logistic regression (PLR) effectively addresses the overfitting problem by introducing regularization terms yet its performance still depends on efficient variable selection strategies. This paper theoretically demonstrates that label noise stemming from manual labeling, which is solely related to classification difficulty, represents a type of beneficial noise for variable selection in PLR. This benefit is reflected in a more accurate estimation of the selected non-zero coefficients when compared with the case where only truth labels are used. Under large-scale settings, the sample size for PLR can become very large, making it infeasible to store on a single machine. In such cases, distributed computing methods are required to handle PLR model with manual labeling. This paper presents a partition-insensitive parallel algorithm founded on the ADMM (alternating direction method of multipliers) algorithm to address PLR by incorporating manual labeling. The partition insensitivity of the proposed algorithm refers to the fact that the solutions obtained by the algorithm will not change with the distributed storage of data. In addition, the algorithm has global convergence and a sublinear convergence rate. Experimental results indicate that, as compared with traditional variable selection classification techniques, the PLR with manually-labeled noisy data achieves higher estimation and classification accuracy across multiple large-scale datasets.

</details>


### [53] [Compositional Active Learning of Synchronous Systems through Automated Alphabet Refinement](https://arxiv.org/abs/2504.16624)

*Leo Henry, Thomas Neele, Mohammad Mousavi, Matteo Sammartino*

**Main category:** cs.LG

**TL;DR:** The paper develops a compositional learning technique for synchronizing parallel systems, refining global alphabets into component models and significantly improving the efficiency of active automata learning.

<details>
  <summary>Details</summary>

**Motivation:** There is a need for improved automata learning techniques for concurrent systems, particularly in terms of efficiency and scalability.

**Method:** The authors propose a compositional learning algorithm that refines the global alphabet into component alphabets, characterizes alphabet distributions, and addresses counter-examples, implemented in the CoalA system using LearnLib.

**Result:** CoalA demonstrates orders of magnitude improvements in membership queries across over 630 subject systems, and offers enhanced scalability in systems featuring significant concurrency.

**Conclusion:** The proposed technique successfully advances the field of active automata learning by providing a scalable and efficient approach to compositional learning of concurrent systems.

**Abstract:** Active automata learning infers automaton models of systems from behavioral observations, a technique successfully applied to a wide range of domains. Compositional approaches for concurrent systems have recently emerged. We take a significant step beyond available results, including those by the authors, and develop a general technique for compositional learning of a synchronizing parallel system with an unknown decomposition. Our approach automatically refines the global alphabet into component alphabets while learning the component models. We develop a theoretical treatment of distributions of alphabets, i.e., sets of possibly overlapping component alphabets. We characterize counter-examples that reveal inconsistencies with global observations, and show how to systematically update the distribution to restore consistency. We present a compositional learning algorithm implementing these ideas, where learning counterexamples precisely correspond to distribution counterexamples under well-defined conditions. We provide an implementation, called CoalA, using the state-of-the-art active learning library LearnLib. Our experiments show that in more than 630 subject systems, CoalA delivers orders of magnitude improvements (up to five orders) in membership queries and in systems with significant concurrency, it also achieves better scalability in the number of equivalence queries.

</details>


### [54] [ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data](https://arxiv.org/abs/2504.16628)

*Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, Yaochu Jin*

**Main category:** cs.LG

**TL;DR:** The paper introduces ParetoHqD, a novel approach for aligning large language models with human preferences, improving upon existing methods by better representing human values and preferences.

<details>
  <summary>Details</summary>

**Motivation:** To improve the alignment of large language models with diverse human expectations and values, addressing limitations of current offline multiobjective alignment algorithms.

**Method:** ParetoHqD represents human preferences as directions in the objective space and employs a two-stage supervised fine-tuning process, utilizing high-quality training data near the Pareto front for each preference direction.

**Result:** Experimental results show that ParetoHqD outperforms five baseline methods on two multiobjective alignment tasks.

**Conclusion:** ParetoHqD effectively enhances the alignment of language models with human preferences, showcasing improvements over existing algorithms.

**Abstract:** Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm have shown strong performance and efficiency. However, inappropriate preference representations and training with imbalanced reward scores limit the performance of such algorithms. In this work, we introduce ParetoHqD that addresses the above issues by representing human preferences as preference directions in the objective space and regarding data near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD follows a two-stage supervised fine-tuning process, where each stage uses an individual Pareto high-quality training set that best matches its preference direction. The experimental results have demonstrated the superiority of ParetoHqD over five baselines on two multiobjective alignment tasks.

</details>


### [55] [DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization](https://arxiv.org/abs/2504.16639)

*Haoran Chen, Jiapeng Liu, Jiafan Wang, Wenjun Shi*

**Main category:** cs.LG

**TL;DR:** The paper introduces a Data Augmentation Partial Least Squares Regression (DAPLSR) model to improve classification performance on data with uneven categories by using SMOTE and manifold optimization.

<details>
  <summary>Details</summary>

**Motivation:** Traditional PLSR models struggle with data containing uneven categories, leading to underperformance in classification tasks.

**Method:** The DAPLSR model incorporates the Synthetic Minority Over-sampling Technique (SMOTE) for data augmentation and employs the Value Difference Metric (VDM) to generate synthetic samples based on nearest neighbors. It also introduces a manifold optimization approach to enhance the numerical solution of the PLSR model.

**Result:** The DAPLSR model demonstrated superior classification performance and improved evaluation metrics across various datasets, significantly exceeding the performance of existing methods.

**Conclusion:** The proposed DAPLSR model effectively addresses the limitations of traditional PLSR in uneven category scenarios, advancing classification accuracy.

**Abstract:** Traditional Partial Least Squares Regression (PLSR) models frequently underperform when handling data characterized by uneven categories. To address the issue, this paper proposes a Data Augmentation Partial Least Squares Regression (DAPLSR) model via manifold optimization. The DAPLSR model introduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase the number of samples and utilizes the Value Difference Metric (VDM) to select the nearest neighbor samples that closely resemble the original samples for generating synthetic samples. In solving the model, in order to obtain a more accurate numerical solution for PLSR, this paper proposes a manifold optimization method that uses the geometric properties of the constraint space to improve model degradation and optimization. Comprehensive experiments show that the proposed DAPLSR model achieves superior classification performance and outstanding evaluation metrics on various datasets, significantly outperforming existing methods.

</details>


### [56] [Representation Learning via Non-Contrastive Mutual Information](https://arxiv.org/abs/2504.16667)

*Zhaohan Daniel Guo, Bernardo Avila Pires, Khimya Khetarpal, Dale Schuurmans, Bo Dai*

**Main category:** cs.LG

**TL;DR:** This paper introduces the Mutual Information Non-Contrastive (MINC) loss, a self-supervised objective that merges the advantages of contrastive and non-contrastive methods for learning image representations.

<details>
  <summary>Details</summary>

**Motivation:** Labeling data is time-consuming and expensive, leading to a surplus of unlabeled data. There is a need for efficient self-supervised learning techniques to extract valuable representations from this unlabeled data.

**Method:** The authors develop a new self-supervised objective by transforming the Spectral Contrastive Loss into a non-contrastive form. This approach aims to eliminate pairwise comparisons to reduce variance while retaining the mutual information aspect to prevent collapse.

**Result:** MINC consistently outperforms the Spectral Contrastive loss baseline in learning image representations on ImageNet.

**Conclusion:** The proposed MINC loss effectively combines the benefits of contrastive and non-contrastive learning, facilitating better performance in representation learning from unlabeled data.

**Abstract:** Labeling data is often very time consuming and expensive, leaving us with a majority of unlabeled data. Self-supervised representation learning methods such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very successful at learning meaningful latent representations from unlabeled image data, resulting in much more general and transferable representations for downstream tasks. Broadly, self-supervised methods fall into two types: 1) Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as BYOL. Contrastive methods are generally trying to maximize mutual information between related data points, so they need to compare every data point to every other data point, resulting in high variance, and thus requiring large batch sizes to work well. Non-contrastive methods like BYOL have much lower variance as they do not need to make pairwise comparisons, but are much trickier to implement as they have the possibility of collapsing to a constant vector. In this paper, we aim to develop a self-supervised objective that combines the strength of both types. We start with a particular contrastive method called the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we convert it into a more general non-contrastive form; this removes the pairwise comparisons resulting in lower variance, but keeps the mutual information formulation of the contrastive method preventing collapse. We call our new objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by learning image representations on ImageNet (similar to SimCLR and BYOL) and show that it consistently improves upon the Spectral Contrastive loss baseline.

</details>


### [57] [Efficient Data Valuation Approximation in Federated Learning: A Sampling-based Approach](https://arxiv.org/abs/2504.16668)

*Shuyue Wei, Yongxin Tong, Zimu Zhou, Tianran He, Yi Xu*

**Main category:** cs.LG

**TL;DR:** The paper addresses the challenge of data valuation in federated learning (FL) using Shapley value (SV), proposing a novel approximation algorithm, IPSS, that reduces computational costs while maintaining accuracy.

<details>
  <summary>Details</summary>

**Motivation:** Cross-silo data providers in federated learning are reluctant to share high-quality datasets without a fair assessment of their data value, where Shapley value is seen as the ideal measure but is computationally expensive.

**Method:** The authors develop a unified stratified-sampling framework for evaluating dataset combinations under FL, identify key combinations that significantly impact data value, and propose the IPSS algorithm to selectively evaluate these combinations for more efficient computation.

**Result:** IPSS demonstrates substantial reductions in time costs while only incurring minor approximation errors compared to exhaustive evaluations; extensive evaluations show it outperforms various established baselines in efficiency and effectiveness on FL benchmark datasets.

**Conclusion:** The proposed IPSS algorithm enables practical application of Shapley value for data valuation in federated learning by strategically selecting impactful dataset combinations, thus facilitating better collaboration among data providers.

**Abstract:** Federated learning paradigm to utilize datasets across multiple data providers. In FL, cross-silo data providers often hesitate to share their high-quality dataset unless their data value can be fairly assessed. Shapley value (SV) has been advocated as the standard metric for data valuation in FL due to its desirable properties. However, the computational overhead of SV is prohibitive in practice, as it inherently requires training and evaluating an FL model across an exponential number of dataset combinations. Furthermore, existing solutions fail to achieve high accuracy and efficiency, making practical use of SV still out of reach, because they ignore choosing suitable computation scheme for approximation framework and overlook the property of utility function in FL. We first propose a unified stratified-sampling framework for two widely-used schemes. Then, we analyze and choose the more promising scheme under the FL linear regression assumption. After that, we identify a phenomenon termed key combinations, where only limited dataset combinations have a high-impact on final data value. Building on these insights, we propose a practical approximation algorithm, IPSS, which strategically selects high-impact dataset combinations rather than evaluating all possible combinations, thus substantially reducing time cost with minor approximation error. Furthermore, we conduct extensive evaluations on the FL benchmark datasets to demonstrate that our proposed algorithm outperforms a series of representative baselines in terms of efficiency and effectiveness.

</details>


### [58] [Provable wavelet-based neural approximation](https://arxiv.org/abs/2504.16682)

*Youngmi Hur, Hyojae Lim, Mikyoung Lim*

**Main category:** cs.LG

**TL;DR:** The paper presents a wavelet-based framework for analyzing the approximation capabilities of neural networks across various activation functions, ensuring universal approximation with controlled error estimates.

<details>
  <summary>Details</summary>

**Motivation:** To analyze how different activation functions influence the universal approximation capabilities of neural networks and provide a flexible design approach for network architectures.

**Method:** Wavelet frame theory is applied to spaces of homogeneous type to derive sufficient conditions for activation functions that enable neural networks to approximate any functions in these spaces, alongside error estimates.

**Result:** The paper establishes sufficient conditions for a variety of smooth and oscillatory activation functions, including a generalized approximation result for non-smooth activations, with error controlled by the L2-distance between smooth and non-smooth functions.

**Conclusion:** The framework allows increased flexibility in neural network design and assures that networks can approximate any target function under certain activation conditions.

**Abstract:** In this paper, we develop a wavelet-based theoretical framework for analyzing the universal approximation capabilities of neural networks over a wide range of activation functions. Leveraging wavelet frame theory on the spaces of homogeneous type, we derive sufficient conditions on activation functions to ensure that the associated neural network approximates any functions in the given space, along with an error estimate. These sufficient conditions accommodate a variety of smooth activation functions, including those that exhibit oscillatory behavior. Furthermore, by considering the $L^2$-distance between smooth and non-smooth activation functions, we establish a generalized approximation result that is applicable to non-smooth activations, with the error explicitly controlled by this distance. This provides increased flexibility in the design of network architectures.

</details>


### [59] [MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks](https://arxiv.org/abs/2504.16683)

*Ceren Yildirim, Kamer Kaya, Sinan Yildirim, Erkay Savas*

**Main category:** cs.LG

**TL;DR:** The paper introduces a Bayesian framework for estimating differential privacy through a new MCMC algorithm, MCMC-DP-Est, which incorporates multiple membership inference attacks.

<details>
  <summary>Details</summary>

**Motivation:** To improve the robustness and realism of privacy analysis in differential privacy by accounting for multiple membership inference attacks without assuming worst-case scenarios.

**Method:** The proposed framework uses a Markov chain Monte Carlo algorithm (MCMC-DP-Est) to estimate the full posterior distribution of the privacy parameter, while also providing an economical method for performance measurements of the MIAs involved.

**Result:** Numerical examples illustrate the effectiveness of MCMC-DP-Est using both artificial and real datasets, demonstrating the framework's capability to yield cautious privacy estimates.

**Conclusion:** The method offers a more accurate and cautious approach to analyzing differential privacy, addressing limitations in existing methodologies that often rely on overly pessimistic worst-case assumptions.

**Abstract:** We propose a new framework for Bayesian estimation of differential privacy, incorporating evidence from multiple membership inference attacks (MIA). Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC) algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior distribution of the privacy parameter (e.g., instead of just credible intervals). Critically, the proposed method does not assume that privacy auditing is performed with the most powerful attack on the worst-case (dataset, challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est jointly estimates the strengths of MIAs used and the privacy of the training algorithm, yielding a more cautious privacy analysis. We also present an economical way to generate measurements for the performance of an MIA that is to be used by the MCMC method to estimate privacy. We present the use of the methods with numerical examples with both artificial and real data.

</details>


### [60] [PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation](https://arxiv.org/abs/2504.16693)

*Wenxuan Li, Hang Zhao, Zhiyuan Yu, Yu Du, Qin Zou, Ruizhen Hu, Kai Xu*

**Main category:** cs.LG

**TL;DR:** The paper introduces PIN-WM, a Physics-INformed World Model, that improves the learning of non-prehensile manipulation in robotics by efficiently modeling 3D rigid body dynamics from visual input and enabling robust skill transfer from simulation to real-world applications.

<details>
  <summary>Details</summary>

**Motivation:** Non-prehensile manipulation is a fundamental robotic skill that is difficult to learn due to complexities in physical interactions, and achieving robust learning and generalization remains a challenge.

**Method:** The authors propose a world model, PIN-WM, that leverages differentiable physics simulation for learning dynamics using few-shot visual observations, without requiring state estimation and utilizing Gaussian Splatting for observational loss.

**Result:** Extensive evaluations show that the PIN-WM model, with enhancements like physics-aware digital cousins, enables effective learning of manipulation skills and surpasses existing methods in Sim2Real tasks.

**Conclusion:** The approach of using a physics-informed world model significantly improves the efficiency of learning non-prehensile manipulation skills and enhances the transferability of learned skills from simulation to real world.

**Abstract:** While non-prehensile manipulation (e.g., controlled pushing/poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts.

</details>


### [61] [A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization](https://arxiv.org/abs/2504.16711)

*Shiyin Tan, Jaeeon Park, Dongyuan Li, Renhe Jiang, Manabu Okumura*

**Main category:** cs.LG

**TL;DR:** The paper presents a novel framework for multi-document summarization (MDS) that improves upon existing transformer-based models by addressing input length limitations and retrieval inefficiencies.

<details>
  <summary>Details</summary>

**Motivation:** Current MDS methods rely on manually crafted queries and truncation, which can lead to retention of irrelevant content and are impractical for different document sets.

**Method:** The proposed framework integrates query selection and document ranking processes, using salient elementary discourse units (EDUs) as latent queries to guide document ranking and relevance scoring, while filtering irrelevant EDUs instead of traditional truncation.

**Result:** The framework demonstrates consistent improvements in ROUGE metrics across multiple MDS datasets and showcases scalability and flexibility with different model architectures.

**Conclusion:** The proposed approach effectively resolves context-length constraints and enhances the summarization process, establishing itself as a robust solution for MDS.

**Abstract:** In the field of multi-document summarization (MDS), transformer-based models have demonstrated remarkable success, yet they suffer an input length limitation. Current methods apply truncation after the retrieval process to fit the context length; however, they heavily depend on manually well-crafted queries, which are impractical to create for each document set for MDS. Additionally, these methods retrieve information at a coarse granularity, leading to the inclusion of irrelevant content. To address these issues, we propose a novel retrieval-based framework that integrates query selection and document ranking and shortening into a unified process. Our approach identifies the most salient elementary discourse units (EDUs) from input documents and utilizes them as latent queries. These queries guide the document ranking by calculating relevance scores. Instead of traditional truncation, our approach filters out irrelevant EDUs to fit the context length, ensuring that only critical information is preserved for summarization. We evaluate our framework on multiple MDS datasets, demonstrating consistent improvements in ROUGE metrics while confirming its scalability and flexibility across diverse model architectures. Additionally, we validate its effectiveness through an in-depth analysis, emphasizing its ability to dynamically select appropriate queries and accurately rank documents based on their relevance scores. These results demonstrate that our framework effectively addresses context-length constraints, establishing it as a robust and reliable solution for MDS.

</details>


### [62] [Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks](https://arxiv.org/abs/2504.16748)

*Yanan Zhao, Feng Ji, Kai Zhao, Xuhao Li, Qiyu Kang, Wenfei Liang, Yahya Alkhatib, Xingchao Jian, Wee Peng Tay*

**Main category:** cs.LG

**TL;DR:** The paper presents a novel augmentation-free Graph Contrastive Learning framework using graph neural diffusion models and Fractional Differential Equations, achieving state-of-the-art performance without needing negative samples.

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in existing Graph Contrastive Learning approaches, particularly the reliance on complex augmentations or negative samples.

**Method:** The authors introduce an augmentation-free GCL framework that employs learnable encoders defined by Fractional Differential Equations (FDE) to generate diverse views of graph data.

**Result:** The proposed method effectively captures local and global information, tested across various datasets, demonstrating state-of-the-art performance in contrastive learning.

**Conclusion:** The model's ability to operate without negative samples and its applicability to both homophilic and heterophilic datasets mark significant contributions to the field of graph representation learning.

**Abstract:** Graph Contrastive Learning (GCL) has recently made progress as an unsupervised graph representation learning paradigm. GCL approaches can be categorized into augmentation-based and augmentation-free methods. The former relies on complex data augmentations, while the latter depends on encoders that can generate distinct views of the same input. Both approaches may require negative samples for training. In this paper, we introduce a novel augmentation-free GCL framework based on graph neural diffusion models. Specifically, we utilize learnable encoders governed by Fractional Differential Equations (FDE). Each FDE is characterized by an order parameter of the differential operator. We demonstrate that varying these parameters allows us to produce learnable encoders that generate diverse views, capturing either local or global information, for contrastive learning. Our model does not require negative samples for training and is applicable to both homophilic and heterophilic datasets. We demonstrate its effectiveness across various datasets, achieving state-of-the-art performance.

</details>


### [63] [QAOA-PCA: Enhancing Efficiency in the Quantum Approximate Optimization Algorithm via Principal Component Analysis](https://arxiv.org/abs/2504.16755)

*Owain Parry, Phil McMinn*

**Main category:** cs.LG

**TL;DR:** QAOA-PCA is a reparameterization technique that optimizes the Quantum Approximate Optimization Algorithm for combinatorial problems by reducing parameter space with PCA, leading to fewer iterations and greater efficiency than standard QAOA.

<details>
  <summary>Details</summary>

**Motivation:** The Quantum Approximate Optimization Algorithm struggles with increasing computational demands due to the linear growth of parameters as the circuit depth increases, necessitating more iterations from classical optimizers.

**Method:** QAOA-PCA employs Principal Component Analysis to reduce the dimensionality of the QAOA parameter space by extracting principal components from optimized parameters of smaller problem instances.

**Result:** Empirical evaluation on the MaxCut problem shows that QAOA-PCA consistently requires fewer iterations than standard QAOA, achieving significant efficiency gains while maintaining a competitive performance with reduced parameters.

**Conclusion:** QAOA-PCA effectively balances efficiency and solution quality, reducing optimization overhead without greatly compromising the approximation ratio compared to standard QAOA.

**Abstract:** The Quantum Approximate Optimization Algorithm (QAOA) is a promising variational algorithm for solving combinatorial optimization problems on near-term devices. However, as the number of layers in a QAOA circuit increases, which is correlated with the quality of the solution, the number of parameters to optimize grows linearly. This results in more iterations required by the classical optimizer, which results in an increasing computational burden as more circuit executions are needed. To mitigate this issue, we introduce QAOA-PCA, a novel reparameterization technique that employs Principal Component Analysis (PCA) to reduce the dimensionality of the QAOA parameter space. By extracting principal components from optimized parameters of smaller problem instances, QAOA-PCA facilitates efficient optimization with fewer parameters on larger instances. Our empirical evaluation on the prominent MaxCut problem demonstrates that QAOA-PCA consistently requires fewer iterations than standard QAOA, achieving substantial efficiency gains. While this comes at the cost of a slight reduction in approximation ratio compared to QAOA with the same number of layers, QAOA-PCA almost always outperforms standard QAOA when matched by parameter count. QAOA-PCA strikes a favorable balance between efficiency and performance, reducing optimization overhead without significantly compromising solution quality.

</details>


### [64] [Noise-Tolerant Coreset-Based Class Incremental Continual Learning](https://arxiv.org/abs/2504.16763)

*Edison Mucllari, Aswin Raghavan, Zachary Alan Daniels*

**Main category:** cs.LG

**TL;DR:** This paper investigates the effects of label and instance noise in continual learning (CL) for class-incremental learning (CIL) and presents new noise-tolerant algorithms that outperform existing methods in terms of classification accuracy and retention of previous tasks.

<details>
  <summary>Details</summary>

**Motivation:** The need for continual learning (CL) algorithms capable of adapting to new data while minimizing the forgetting of previous tasks is crucial for computer vision applications, especially in the presence of noise.

**Method:** The authors derive a new robustness bound for CL methods that utilize memory-based techniques, specifically focusing on replaying items from a coreset, and develop two new continual learning algorithms to create noise-tolerant replay buffers.

**Result:** The proposed continual learning algorithms demonstrate significant improvements in handling label and uncorrelated instance noise compared to existing memory-based methods across five diverse datasets, improving classification accuracy and minimizing forgetting.

**Conclusion:** The research reveals that existing memory-based continual learners lack robustness to noise, while the newly proposed methods effectively enhance performance in noisy class-incremental learning environments.

**Abstract:** Many applications of computer vision require the ability to adapt to novel data distributions after deployment. Adaptation requires algorithms capable of continual learning (CL). Continual learners must be plastic to adapt to novel tasks while minimizing forgetting of previous tasks.However, CL opens up avenues for noise to enter the training pipeline and disrupt the CL. This work focuses on label noise and instance noise in the context of class-incremental learning (CIL), where new classes are added to a classifier over time, and there is no access to external data from past classes. We aim to understand the sensitivity of CL methods that work by replaying items from a memory constructed using the idea of Coresets. We derive a new bound for the robustness of such a method to uncorrelated instance noise under a general additive noise threat model, revealing several insights. Putting the theory into practice, we create two continual learning algorithms to construct noise-tolerant replay buffers. We empirically compare the effectiveness of prior memory-based continual learners and the proposed algorithms under label and uncorrelated instance noise on five diverse datasets. We show that existing memory-based CL are not robust whereas the proposed methods exhibit significant improvements in maximizing classification accuracy and minimizing forgetting in the noisy CIL setting.

</details>


### [65] [Online model learning with data-assimilated reservoir computers](https://arxiv.org/abs/2504.16767)

*Andrea Nóvoa, Luca Magri*

**Main category:** cs.LG

**TL;DR:** The paper presents an online learning framework for forecasting nonlinear spatio-temporal signals, integrating dimensionality reduction, a generalized autoregressive model, and online adaptation techniques.

<details>
  <summary>Details</summary>

**Motivation:** To improve forecasting of nonlinear spatio-temporal signals by integrating various advanced techniques into an online learning framework.

**Method:** The framework combines proper orthogonal decomposition (POD) for dimensionality reduction, reservoir computing for forecasting, and ensemble sequential data assimilation for online model adaptation.

**Result:** The two-fold estimation method enhances ensemble convergence and reduces reconstruction error compared to a na"ive approach, while the three-fold approach allows robust online training of the reservoir computers.

**Conclusion:** The integration of data-driven reduced order modeling with Bayesian data assimilation enables scalable online model learning for nonlinear forecasting, representing significant progress in the field.

**Abstract:** We propose an online learning framework for forecasting nonlinear spatio-temporal signals (fields). The method integrates (i) dimensionality reduction, here, a simple proper orthogonal decomposition (POD) projection; (ii) a generalized autoregressive model to forecast reduced dynamics, here, a reservoir computer; (iii) online adaptation to update the reservoir computer (the model), here, ensemble sequential data assimilation.We demonstrate the framework on a wake past a cylinder governed by the Navier-Stokes equations, exploring the assimilation of full flow fields (projected onto POD modes) and sparse sensors. Three scenarios are examined: a na\"ive physical state estimation; a two-fold estimation of physical and reservoir states; and a three-fold estimation that also adjusts the model parameters. The two-fold strategy significantly improves ensemble convergence and reduces reconstruction error compared to the na\"ive approach. The three-fold approach enables robust online training of partially-trained reservoir computers, overcoming limitations of a priori training. By unifying data-driven reduced order modelling with Bayesian data assimilation, this work opens new opportunities for scalable online model learning for nonlinear time series forecasting.

</details>


### [66] [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)

*Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, Lu Wang*

**Main category:** cs.LG

**TL;DR:** ThinkPRM is a data-efficient process reward model (PRM) that uses verbalized chains of thought (CoT) to verify each step in problem-solving, outperforming traditional methods with significantly fewer training labels.

<details>
  <summary>Details</summary>

**Motivation:** To build data-efficient PRMs that can verify each step in solutions, reducing the need for expensive step-level supervision during training.

**Method:** ThinkPRM employs long chains of thought for verification, fine-tuning on a fraction (1%) of the process labels required by traditional discriminative PRMs.

**Result:** ThinkPRM outperformed LLM-as-a-Judge and discriminative verifiers across various benchmarks including ProcessBench, MATH-500, and AIME '24, showing superior performance even with minimal data.

**Conclusion:** The findings demonstrate that generative, long CoT PRMs are effective for scaling test-time computation for verification tasks while needing minimal supervision during training.

**Abstract:** Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.

</details>


### [67] [Evaluating Autoencoders for Parametric and Invertible Multidimensional Projections](https://arxiv.org/abs/2504.16831)

*Frederik L. Dennig, Nina Geyer, Daniela Blumberg, Yannick Metz, Daniel A. Keim*

**Main category:** cs.LG

**TL;DR:** The paper investigates the use of autoencoder architectures for creating parametric and invertible multidimensional data projections, demonstrating that customized loss functions can improve projection quality.

<details>
  <summary>Details</summary>

**Motivation:** The need to explore the simultaneous capabilities of parametric and invertible projections in autoencoders, which have not been previously addressed in the context of arbitrary projection methods.

**Method:** The authors evaluate three autoencoder architectures by training them to learn mappings into 2D space and their respective inverse mappings, using a customized loss function to enhance projection quality.

**Result:** Autoencoders with a customized loss function produce smoother parametric and inverse projections compared to traditional feed-forward neural networks across four datasets with diverse dimensionality and complexity.

**Conclusion:** The findings suggest that customized loss functions in autoencoders can effectively enhance the quality of multidimensional data projections, providing users with control over the smoothing effect.

**Abstract:** Recently, neural networks have gained attention for creating parametric and invertible multidimensional data projections. Parametric projections allow for embedding previously unseen data without recomputing the projection as a whole, while invertible projections enable the generation of new data points. However, these properties have never been explored simultaneously for arbitrary projection methods. We evaluate three autoencoder (AE) architectures for creating parametric and invertible projections. Based on a given projection, we train AEs to learn a mapping into 2D space and an inverse mapping into the original space. We perform a quantitative and qualitative comparison on four datasets of varying dimensionality and pattern complexity using t-SNE. Our results indicate that AEs with a customized loss function can create smoother parametric and inverse projections than feed-forward neural networks while giving users control over the strength of the smoothing effect.

</details>


### [68] [Improving Significant Wave Height Prediction Using Chronos Models](https://arxiv.org/abs/2504.16834)

*Yilin Zhai, Hongyuan Shi, Chao Zhan, Qing Wang, Zaijin You, Nan Wang*

**Main category:** cs.LG

**TL;DR:** The study presents Chronos, a large language model-based framework for improved wave height forecasting, which offers enhanced computational efficiency and predictive accuracy.

<details>
  <summary>Details</summary>

**Motivation:** Accurate wave height prediction is essential for maritime safety and resilience, yet existing methods struggle with efficiency and modeling nonlinear dynamics.

**Method:** The study introduces Chronos, which leverages advanced temporal pattern recognition on historical wave data from three selected marine zones in the Northwest Pacific basin.

**Result:** Chronos achieves a 14.3% reduction in training time and 2.5x faster inference speed, attaining a mean absolute scaled error (MASE) of 0.575 while outperforming traditional models in both short-term (1-24h) and extended-range (1-120h) forecasting.

**Conclusion:** Chronos sets a new benchmark in wave prediction by providing computationally efficient solutions and a framework applicable to other complex geophysical modeling tasks.

**Abstract:** Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling.

</details>


### [69] [An Adaptive ML Framework for Power Converter Monitoring via Federated Transfer Learning](https://arxiv.org/abs/2504.16866)

*Panagiotis Kakosimos, Alireza Nemat Saberi, Luca Peretti*

**Main category:** cs.LG

**TL;DR:** This study investigates a novel framework combining transfer learning and federated learning for adapting thermal machine learning models in power converters.

<details>
  <summary>Details</summary>

**Motivation:** The research addresses challenges like varying operating conditions, data sharing limitations, and security concerns in thermal ML applications for power converters.

**Method:** The framework uses a base model that is adapted by multiple clients through three domain adaptation techniques: Fine-tuning, Transfer Component Analysis, and Deep Domain Adaptation, utilizing the Flower framework for federated learning.

**Result:** Validation with field data shows that fine-tuning is a high-accuracy, practical approach, while benchmarking compares strengths and weaknesses of the methods in different scenarios. Locally hosted FL improves performance under certain limitations, whereas cloud-based FL is effective with more clients.

**Conclusion:** The combined use of transfer learning and federated learning creates a robust system for adapting ML models under varied conditions, enhancing scalability and addressing connectivity issues.

**Abstract:** This study explores alternative framework configurations for adapting thermal machine learning (ML) models for power converters by combining transfer learning (TL) and federated learning (FL) in a piecewise manner. This approach inherently addresses challenges such as varying operating conditions, data sharing limitations, and security implications. The framework starts with a base model that is incrementally adapted by multiple clients via adapting three state-of-the-art domain adaptation techniques: Fine-tuning, Transfer Component Analysis (TCA), and Deep Domain Adaptation (DDA). The Flower framework is employed for FL, using Federated Averaging for aggregation. Validation with field data demonstrates that fine-tuning offers a straightforward TL approach with high accuracy, making it suitable for practical applications. Benchmarking results reveal a comprehensive comparison of these methods, showcasing their respective strengths and weaknesses when applied in different scenarios. Locally hosted FL enhances performance when data aggregation is not feasible, while cloud-based FL becomes more practical with a significant increase in the number of clients, addressing scalability and connectivity challenges.

</details>


### [70] [Exploring How LLMs Capture and Represent Domain-Specific Knowledge](https://arxiv.org/abs/2504.16871)

*Mirian Hipolito Garcia, Camille Couturier, Daniel Madrigal Diaz, Ankur Mallick, Anastasios Kyrillidis, Robert Sim, Victor Ruhle, Saravan Rajmohan*

**Main category:** cs.LG

**TL;DR:** The paper investigates the domain sensitivity of Large Language Models (LLMs) and their ability to recognize domain-specific nuances in queries using hidden states.

<details>
  <summary>Details</summary>

**Motivation:** To understand whether LLMs can inherently capture and differentiate various domains in natural language, and how their internal features respond to domain-specific queries.

**Method:** Experiments were conducted to examine LLMs' ability to distinguish queries from different domains through hidden states during the prefill phase, focusing on latent domain-related trajectories and their robustness to different prompt styles and sources.

**Result:** The study revealed that LLMs can effectively differentiate between queries from related domains, showing unexpected performance patterns where fine-tuned models are not always the most accurate.

**Conclusion:** The findings suggest that LLMs possess a capacity for recognizing domain nuances applicable to both closed and open-ended generative tasks, challenging previous assumptions about model accuracy based solely on fine-tuning.

**Abstract:** We study whether Large Language Models (LLMs) inherently capture domain-specific nuances in natural language. Our experiments probe the domain sensitivity of LLMs by examining their ability to distinguish queries from different domains using hidden states generated during the prefill phase. We reveal latent domain-related trajectories that indicate the model's internal recognition of query domains. We also study the robustness of these domain representations to variations in prompt styles and sources. Our approach leverages these representations for model selection, mapping the LLM that best matches the domain trace of the input query (i.e., the model with the highest performance on similar traces). Our findings show that LLMs can differentiate queries for related domains, and that the fine-tuned model is not always the most accurate. Unlike previous work, our interpretations apply to both closed and open-ended generative tasks

</details>


### [71] [Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion](https://arxiv.org/abs/2504.16875)

*Julian Bedei, Murray McBain, Charles Robert Koch, Jakob Andert, David Gordon*

**Main category:** cs.LG

**TL;DR:** The paper proposes a hybrid approach combining Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive Control (ML-MPC) for optimizing hydrogen-diesel dual-fuel engine control, addressing safety and adaptability challenges of each method.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of pure RL, which may execute unsafe actions, and pure ML-MPC, which struggles with adaptability due to reliance on accurate system models, particularly in the presence of system drifts.

**Method:** A hybrid RL and ML-MPC approach is developed, where ML-MPC provides safe control while the RL agent dynamically adjusts the load tracking reference in response to environmental changes, tested under conditions of model-plant mismatch.

**Result:** The introduction of model-plant mismatch led to a root mean square error (RMSE) of 0.57 bar with ML-MPC alone, while the hybrid approach yielded an RMSE of 0.44 bar, demonstrating the RL's adaptability in improving load tracking.

**Conclusion:** The hybrid RL-ML-MPC approach effectively enhances load tracking and maintains safety, demonstrating significant improvements over the use of either method in isolation.

**Abstract:** Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel dual-fuel engine control, as they can effectively control multiple-input multiple-output systems and nonlinear processes. ML-MPC is advantageous for providing safe and optimal controls, ensuring the engine operates within predefined safety limits. In contrast, RL is distinguished by its adaptability to changing conditions through its learning-based approach. However, the practical implementation of either method alone poses challenges. RL requires high variance in control inputs during early learning phases, which can pose risks to the system by potentially executing unsafe actions, leading to mechanical damage. Conversely, ML-MPC relies on an accurate system model to generate optimal control inputs and has limited adaptability to system drifts, such as injector aging, which naturally occur in engine applications. To address these limitations, this study proposes a hybrid RL and ML-MPC approach that uses an ML-MPC framework while incorporating an RL agent to dynamically adjust the ML-MPC load tracking reference in response to changes in the environment. At the same time, the ML-MPC ensures that actions stay safe throughout the RL agent's exploration. To evaluate the effectiveness of this approach, fuel pressure is deliberately varied to introduce a model-plant mismatch between the ML-MPC and the engine test bench. The result of this mismatch is a root mean square error (RMSE) in indicated mean effective pressure of 0.57 bar when running the ML-MPC. The experimental results demonstrate that RL successfully adapts to changing boundary conditions by altering the tracking reference while ML-MPC ensures safe control inputs. The quantitative improvement in load tracking by implementing RL is an RSME of 0.44 bar.

</details>


### [72] [I-Con: A Unifying Framework for Representation Learning](https://arxiv.org/abs/2504.16929)

*Shaden Alshammari, John Hershey, Axel Feldmann, William T. Freeman, Mark Hamilton*

**Main category:** cs.LG

**TL;DR:** The paper presents a unified information-theoretic framework for loss functions in representation learning, connecting various methods and achieving state-of-the-art results in unsupervised image classification.

<details>
  <summary>Details</summary>

**Motivation:** With the growing complexity and variety of loss functions in representation learning, there is a need for a unified theoretical framework that encompasses and generalizes these approaches.

**Method:** The authors introduce an information-theoretic equation that shows multiple machine learning methods are minimizing an integrated KL divergence between supervisory and learned representations, leading to a comprehensive understanding of various techniques.

**Result:** The framework provides over 23 proofs connecting different loss function approaches and leads to new state-of-the-art unsupervised image classifiers with an 8% improvement on ImageNet-1K, and effective debiasing methods for contrastive representation learners.

**Conclusion:** The proposed framework not only enhances the understanding of existing methods but also facilitates the creation of novel loss functions and significantly improves performance in practical applications.

**Abstract:** As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of modern loss functions in machine learning. In particular, we introduce a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, connecting over 23 different approaches, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [73] [FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking](https://arxiv.org/abs/2504.16188)

*Jabez Magomere, Elena Kochkina, Samuel Mensah, Simerjot Kaur, Charese H. Smiley*

**Main category:** cs.CL

**TL;DR:** FinNLI is a benchmark dataset for Financial Natural Language Inference, consisting of 21,304 premise-hypothesis pairs from diverse financial texts, highlighting challenges in applying general-domain NLI models to finance-related tasks.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the need for a benchmark in Financial Natural Language Inference that captures the uniqueness of financial texts and evaluates model performance in this domain.

**Method:** The FinNLI dataset was constructed with 21,304 pairs, including a carefully annotated test set of 3,304 instances, emphasizing diversity in premise-hypothesis pairs and minimizing spurious correlations.

**Result:** Evaluations revealed that domain shift negatively impacts general-domain NLI models, with the highest Macro F1 scores being 74.57% for pre-trained models and 78.62% for large language models, indicating the dataset's complexity and challenges.

**Conclusion:** FinNLI reveals significant weaknesses in current LLMs for financial reasoning, indicating a need for improved models capable of generalizing better in the financial domain.

**Abstract:** We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the dataset's difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement.

</details>


### [74] [The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy](https://arxiv.org/abs/2504.16271)

*Frederik Bredgaard, Martin Lund Trinhammer, Elisa Bassignana*

**Main category:** cs.CL

**TL;DR:** The paper explores using Natural Language Processing (NLP) to automatically assess patient attachment styles from psychotherapy transcripts, aiming to improve mental healthcare delivery.

<details>
  <summary>Details</summary>

**Motivation:** Current manual assessment of attachment styles is complex and resource-intensive, hindering widespread adoption of attachment-informed treatments.

**Method:** The study employs NLP classification models to analyze psychotherapy transcripts for identifying patient attachment styles automatically.

**Result:** The analysis highlights potential misclassification issues, such as confusing 'preoccupied' with 'avoidant' patients, and discusses the implications for therapy outcomes.

**Conclusion:** The research paves the way for more personalized psychotherapy and targeted investigation into psychotherapy mechanisms through NLP advancements.

**Abstract:** The delivery of mental healthcare through psychotherapy stands to benefit immensely from developments within Natural Language Processing (NLP), in particular through the automatic identification of patient specific qualities, such as attachment style. Currently, the assessment of attachment style is performed manually using the Patient Attachment Coding System (PACS; Talia et al., 2017), which is complex, resource-consuming and requires extensive training. To enable wide and scalable adoption of attachment informed treatment and research, we propose the first exploratory analysis into automatically assessing patient attachment style from psychotherapy transcripts using NLP classification models. We further analyze the results and discuss the implications of using automated tools for this purpose -- e.g., confusing `preoccupied' patients with `avoidant' likely has a more negative impact on therapy outcomes with respect to other mislabeling. Our work opens an avenue of research enabling more personalized psychotherapy and more targeted research into the mechanisms of psychotherapy through advancements in NLP.

</details>


### [75] [The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation](https://arxiv.org/abs/2504.16286)

*Li Weigang, Pedro Carvalho Brom*

**Main category:** cs.CL

**TL;DR:** This study evaluates the performance of large language models versus traditional translation tools in Chinese-English translation, highlighting challenges in preserving poetic intent and cultural heritage with a novel evaluation method.

<details>
  <summary>Details</summary>

**Motivation:** The research aims to address the challenges posed by large language models in accurately translating poetic intent, cultural heritage, and specialized terminology in Chinese-English translation.

**Method:** The study constructs a diverse corpus and employs a back-translation and Friedman test-based evaluation system (BT-Fried) to assess the performance of six major LLMs and three traditional translation tools using metrics like BLEU, CHRF, TER, and semantic similarity.

**Result:** Key findings indicate that while scientific abstracts benefit from back-translation, traditional tools outperform LLMs in linguistically distinct texts, and LLMs face difficulties with cultural and literary retention, along with some showing 'verbatim back-translation'.

**Conclusion:** The study enhances the empirical evaluation of Chinese NLP performance and provides insights into the issues of cultural fidelity in AI-mediated translation.

**Abstract:** The rapid advancement of large language models (LLMs) has reshaped the landscape of machine translation, yet challenges persist in preserving poetic intent, cultural heritage, and handling specialized terminology in Chinese-English translation. This study constructs a diverse corpus encompassing Chinese scientific terminology, historical translation paradoxes, and literary metaphors. Utilizing a back-translation and Friedman test-based evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three traditional translation tools. Key findings include: (1) Scientific abstracts often benefit from back-translation, while traditional tools outperform LLMs in linguistically distinct texts; (2) LLMs struggle with cultural and literary retention, exemplifying the "paradox of poetic intent"; (3) Some models exhibit "verbatim back-translation", reflecting emergent memory behavior; (4) A novel BLEU variant using Jieba segmentation and n-gram weighting is proposed. The study contributes to the empirical evaluation of Chinese NLP performance and advances understanding of cultural fidelity in AI-mediated translation.

</details>


### [76] [Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives](https://arxiv.org/abs/2504.16312)

*Zhangdie Yuan, Andreas Vlachos*

**Main category:** cs.CL

**TL;DR:** The paper introduces a new dataset for evaluating LLMs on relational understanding, revealing their poor performance and proposing a contrastive learning method for encoder retraining that enhances efficiency and performance.

<details>
  <summary>Details</summary>

**Motivation:** Capturing symmetric and antisymmetric relations is essential for various applications, necessitating an evaluation of large language models' understanding of these relations.

**Method:** A novel Wikidata-derived natural language inference dataset was created, and the study explored encoder retraining through contrastive learning using k-nearest neighbors.

**Result:** LLMs performed at chance level on the benchmark, but the retrained encoder achieved comparable performance to fine-tuned classification heads, with improvements in few-shot learning and reduction of catastrophic forgetting.

**Conclusion:** The proposed method effectively enhances the relational understanding capabilities of LLMs, demonstrating a viable approach to improving their performance through retraining techniques.

**Abstract:** Capturing symmetric (e.g., country borders another country) and antisymmetric (e.g., parent_of) relations is crucial for a variety of applications. This paper tackles this challenge by introducing a novel Wikidata-derived natural language inference dataset designed to evaluate large language models (LLMs). Our findings reveal that LLMs perform comparably to random chance on this benchmark, highlighting a gap in relational understanding. To address this, we explore encoder retraining via contrastive learning with k-nearest neighbors. The retrained encoder matches the performance of fine-tuned classification heads while offering additional benefits, including greater efficiency in few-shot learning and improved mitigation of catastrophic forgetting.

</details>


### [77] [Transformer-Based Extraction of Statutory Definitions from the U.S. Code](https://arxiv.org/abs/2504.16353)

*Arpana Hosabettu, Harsh Shah*

**Main category:** cs.CL

**TL;DR:** A novel NLP system utilizing transformer-based architectures is developed to automatically extract definitions from the U.S. Code, achieving superior accuracy compared to traditional methods.

<details>
  <summary>Details</summary>

**Motivation:** Enhancing comprehension and clarity of complex legal texts by automatically extracting definitions and their scopes from large legal corpuses like the U.S. Code.

**Method:** The system employs a multi-stage pipeline that uses Legal-BERT, fine-tuned on statutory texts, to classify paragraphs, aggregate definitions, and extract defined terms with attention mechanisms and rule-based patterns.

**Result:** The model achieves 96.8% precision and 98.9% recall, with a 98.2% F1-score, outperforming previous machine learning classifiers on the U.S. Code dataset containing thousands of definitions.

**Conclusion:** This research significantly improves legal information accessibility and lays groundwork for future legal reasoning applications.

**Abstract:** Automatic extraction of definitions from legal texts is critical for enhancing the comprehension and clarity of complex legal corpora such as the United States Code (U.S.C.). We present an advanced NLP system leveraging transformer-based architectures to automatically extract defined terms, their definitions, and their scope from the U.S.C. We address the challenges of automatically identifying legal definitions, extracting defined terms, and determining their scope within this complex corpus of over 200,000 pages of federal statutory law. Building upon previous feature-based machine learning methods, our updated model employs domain-specific transformers (Legal-BERT) fine-tuned specifically for statutory texts, significantly improving extraction accuracy. Our work implements a multi-stage pipeline that combines document structure analysis with state-of-the-art language models to process legal text from the XML version of the U.S. Code. Each paragraph is first classified using a fine-tuned legal domain BERT model to determine if it contains a definition. Our system then aggregates related paragraphs into coherent definitional units and applies a combination of attention mechanisms and rule-based patterns to extract defined terms and their jurisdictional scope. The definition extraction system is evaluated on multiple titles of the U.S. Code containing thousands of definitions, demonstrating significant improvements over previous approaches. Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score), substantially outperforming traditional machine learning classifiers. This work contributes to improving accessibility and understanding of legal information while establishing a foundation for downstream legal reasoning tasks.

</details>


### [78] [Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions](https://arxiv.org/abs/2504.16358)

*Tian Bai, Huiyan Ying, Kailong Suo, Junqiu Wei, Tao Fan, Yuanfeng Song*

**Main category:** cs.CL

**TL;DR:** The paper presents the Text-to-TrajVis task, introducing a new visualization language and dataset to convert natural language questions into trajectory visualizations.

<details>
  <summary>Details</summary>

**Motivation:** The lack of datasets for the novel Text-to-TrajVis task necessitates the development of a solution to facilitate natural language interfaces for trajectory visualizations.

**Method:** A new visualization language, Trajectory Visualization Language (TVL), was created, and a dataset construction method was proposed that combines Large Language Models with human efforts to produce a large-scale dataset of (question, TVL) pairs.

**Result:** The resulting dataset, TrajVL, contains 18,140 (question, TVL) pairs, and performance evaluations of various LLMs show that the task is feasible yet challenging.

**Conclusion:** The study highlights the need for further exploration of the Text-to-TrajVis task within the research community.

**Abstract:** This paper introduces the Text-to-TrajVis task, which aims to transform natural language questions into trajectory data visualizations, facilitating the development of natural language interfaces for trajectory visualization systems. As this is a novel task, there is currently no relevant dataset available in the community. To address this gap, we first devised a new visualization language called Trajectory Visualization Language (TVL) to facilitate querying trajectory data and generating visualizations. Building on this foundation, we further proposed a dataset construction method that integrates Large Language Models (LLMs) with human efforts to create high-quality data. Specifically, we first generate TVLs using a comprehensive and systematic process, and then label each TVL with corresponding natural language questions using LLMs. This process results in the creation of the first large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140 (question, TVL) pairs. Based on this dataset, we systematically evaluated the performance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The experimental results demonstrate that this task is both feasible and highly challenging and merits further exploration within the research community.

</details>


### [79] [SplitReason: Learning To Offload Reasoning](https://arxiv.org/abs/2504.16379)

*Yash Akhauri, Anthony Fei, Chi-Chih Chang, Ahmed F. AbouElhamayed, Yueying Li, Mohamed S. Abdelfattah*

**Main category:** cs.CL

**TL;DR:** Optimizing reasoning in LLMs by offloading complex parts to a larger model while training a smaller model to identify these parts, leading to improved accuracy and reduced token generation.

<details>
  <summary>Details</summary>

**Motivation:** Reasoning in large language models generates longer token sequences, which is memory-intensive, yet includes segments of varying difficulty that could be optimized.

**Method:** Utilizing 18k annotated reasoning traces from the OpenR1-Math-220k dataset, we fine-tune a 1.5B-parameter model to learn when to offload challenging segments to a larger model, employing both supervised and reinforcement learning techniques.

**Result:** Achieved a 24% and 28.3% improvement in AIME24 reasoning accuracy with offloading of 1.35% and 5% of generated tokens, respectively.

**Conclusion:** The SplitReason model demonstrates significant efficiency and accuracy improvements in reasoning tasks by intelligently managing the offloading of difficult reasoning components.

**Abstract:** Reasoning in large language models (LLMs) tends to produce substantially longer token generation sequences than simpler language modeling tasks. This extended generation length reflects the multi-step, compositional nature of reasoning and is often correlated with higher solution accuracy. From an efficiency perspective, longer token generation exacerbates the inherently sequential and memory-bound decoding phase of LLMs. However, not all parts of this expensive reasoning process are equally difficult to generate. We leverage this observation by offloading only the most challenging parts of the reasoning process to a larger, more capable model, while performing most of the generation with a smaller, more efficient model; furthermore, we teach the smaller model to identify these difficult segments and independently trigger offloading when needed. To enable this behavior, we annotate difficult segments across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT) dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to offload the most challenging parts of its own reasoning process to a larger model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while offloading 1.35% and 5% of the generated tokens respectively. We open-source our SplitReason model, data, code and logs.

</details>


### [80] [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)

*Fahmida Liza Piya, Rahmatollah Beheshti*

**Main category:** cs.CL

**TL;DR:** The paper introduces ConTextual, a new framework for clinical text summarization that enhances decision-making by preserving important context and integrating a knowledge graph for improved information extraction.

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the need for effective extraction of key information from unstructured clinical data to improve decision-making in patient care, highlighting the limitations of previous summarization methods.

**Method:** ConTextual combines a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph to enhance the extraction and structuring of clinical information.

**Result:** Extensive evaluations on two benchmark datasets show that ConTextual consistently outperforms existing summarization methods in terms of linguistic coherence and clinical fidelity.

**Conclusion:** The proposed approach demonstrates the effectiveness of combining token-level filtering with structured retrieval, offering a scalable solution to enhance precision in clinical text generation.

**Abstract:** Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.

</details>


### [81] [Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)

*Jiahao Yuan, Xingzhe Sun, Xing Yu, Jingwen Wang, Dehui Du, Zhiqing Cui, Zixiang Di*

**Main category:** cs.CL

**TL;DR:** The paper presents the 'Less is More' approach, which secured third place in the XLLM@ACL2025 Shared Task-III, designed to enhance structural reasoning with minimal labeled data.

<details>
  <summary>Details</summary>

**Motivation:** To tackle low-resource structural reasoning tasks that require LLMs to generate interpretable rationales while using minimal labeled examples.

**Method:** The approach employs a multi-agent framework integrating reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering, fine-tuned from Meta-Llama-3-8B-Instruct in a unified LoRA+ setup, addressing three subtasks: question parsing, CoT parsing, and step-level verification.

**Result:** The pipeline demonstrates significant improvements in structured reasoning quality by leveraging structure validation and reward filtering in few-shot and zero-shot scenarios.

**Conclusion:** The findings highlight the effectiveness of controllable data distillation in enhancing structured inference capabilities, even with limited resources.

**Abstract:** The XLLM@ACL2025 Shared Task-III formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at https://github.com/Jiahao-Yuan/Less-is-More.

</details>


### [82] [Out-of-the-Box Conditional Text Embeddings from Large Language Models](https://arxiv.org/abs/2504.16411)

*Kosuke Yamada, Peinan Zhang*

**Main category:** cs.CL

**TL;DR:** PonTE is an unsupervised method for generating conditional text embeddings, avoiding the need for extensive training data and fine-tuning, while achieving competitive performance in text similarity and clustering tasks.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of high labor and resource costs associated with fine-tuning models on large datasets for conditional text embedding.

**Method:** PonTE utilizes a causal large language model and conditional prompts to create unsupervised conditional text embeddings.

**Result:** Experiments show that PonTE performs comparably to supervised methods in conditional semantic text similarity and text clustering, while also providing insights into the interpretability of text embeddings.

**Conclusion:** PonTE offers a valuable approach to conditional text embedding that is efficient and interpretable, making it a strong alternative to traditional supervised methods.

**Abstract:** Conditional text embedding is a proposed representation that captures the shift in perspective on texts when conditioned on a specific aspect. Previous methods have relied on extensive training data for fine-tuning models, leading to challenges in terms of labor and resource costs. We propose PonTE, a novel unsupervised conditional text embedding method that leverages a causal large language model and a conditional prompt. Through experiments on conditional semantic text similarity and text clustering, we demonstrate that PonTE can generate useful conditional text embeddings and achieve performance comparable to supervised methods without fine-tuning. We also show the interpretability of text embeddings with PonTE by analyzing word generation following prompts and embedding visualization.

</details>


### [83] [Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study](https://arxiv.org/abs/2504.16414)

*Mohammad Khodadad, Ali Shiraee Kasmaee, Mahdi Astaraki, Nicholas Sherck, Hamidreza Mahyar, Soheila Samiee*

**Main category:** cs.CL

**TL;DR:** The study presents a benchmark for assessing the compositional reasoning capabilities of language models in chemistry, highlighting challenges and proposing an automated pipeline for data generation and evaluation.

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the compositional reasoning capabilities of large language models specifically within the chemistry domain and identify their limitations.

**Method:** A new benchmark with a curated dataset and evaluation process was introduced, using an automated pipeline that combines OpenAI reasoning models with NER systems to extract chemical entities and create a knowledge graph for generating multi-hop questions.

**Result:** Experiments showed that state-of-the-art language models struggle with multi-hop compositional reasoning, demonstrating that even with perfect retrieval, reasoning errors persist, emphasizing the need for document retrieval augmentation.

**Conclusion:** The research elucidates the limitations of current LLMs in reasoning tasks, proposes a novel data generation pipeline for challenging datasets, and enhances understanding of reasoning in computational linguistics.

**Abstract:** In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a fully automated pipeline, verified by subject matter experts, to facilitate this task. Our approach integrates OpenAI reasoning models with named entity recognition (NER) systems to extract chemical entities from recent literature, which are then augmented with external knowledge bases to form a comprehensive knowledge graph. By generating multi-hop questions across these graphs, we assess LLM performance in both context-augmented and non-context augmented settings. Our experiments reveal that even state-of-the-art models face significant challenges in multi-hop compositional reasoning. The results reflect the importance of augmenting LLMs with document retrieval, which can have a substantial impact on improving their performance. However, even perfect retrieval accuracy with full context does not eliminate reasoning errors, underscoring the complexity of compositional reasoning. This work not only benchmarks and highlights the limitations of current LLMs but also presents a novel data generation pipeline capable of producing challenging reasoning datasets across various domains. Overall, this research advances our understanding of reasoning in computational linguistics.

</details>


### [84] [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)

*Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Jinchao Zhang, Jie Zhou, Haige Zhu*

**Main category:** cs.CL

**TL;DR:** The paper introduces MMLA, a benchmark for evaluating multimodal large language models (MLLMs) in understanding cognitive-level semantics, revealing their current limitations.

<details>
  <summary>Details</summary>

**Motivation:** There is limited research on the ability of MLLMs to comprehend cognitive-level semantics in human conversational utterances, despite the growing importance of multimodal language analysis.

**Method:** MMLA consists of over 61K multimodal utterances covering six dimensions of multimodal semantics, and eight LLMs and MLLMs were evaluated using zero-shot inference, supervised fine-tuning, and instruction tuning methods.

**Result:** Even fine-tuned MLLMs achieve only about 60% to 70% accuracy, highlighting their limitations in understanding complex human language.

**Conclusion:** MMLA provides a valuable resource for advancing the field of multimodal language analysis and exploring the potential of large language models.

**Abstract:** Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.

</details>


### [85] [EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records](https://arxiv.org/abs/2504.16448)

*Shuguang Zhao, Qiangzhong Feng, Zhiyang He, Peipei Sun, Yingying Wang, Xiaodong Tao, Xiaoliang Lu, Mei Cheng, Xinyue Wu, Yanyan Wang, Wei Liang*

**Main category:** cs.CL

**TL;DR:** EMRModel effectively utilizes LoRA-based fine-tuning and code-style prompts to convert unstructured medical dialogues into structured EMRs, achieving significant performance improvements.

<details>
  <summary>Details</summary>

**Motivation:** The unstructured nature of medical consultation dialogues limits their effective use in clinical diagnosis and treatment.

**Method:** The proposed EMRModel integrates LoRA-based fine-tuning with code-style prompt design to extract structured information from medical dialogues, supported by a high-quality annotated dataset and a fine-grained evaluation benchmark.

**Result:** EMRModel achieves an F1 score of 88.1%, a 49.5% improvement over standard pre-trained models, and outperforms traditional LoRA fine-tuning methods.

**Conclusion:** The effectiveness of EMRModel demonstrates its potential in optimizing the extraction of structured information from medical consultation dialogues.

**Abstract:** Medical consultation dialogues contain critical clinical information, yet their unstructured nature hinders effective utilization in diagnosis and treatment. Traditional methods, relying on rule-based or shallow machine learning techniques, struggle to capture deep and implicit semantics. Recently, large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight fine-tuning method, have shown promise for structured information extraction. We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning with code-style prompt design, aiming to efficiently convert medical consultation dialogues into structured electronic medical records (EMRs). Additionally, we construct a high-quality, realistically grounded dataset of medical consultation dialogues with detailed annotations. Furthermore, we introduce a fine-grained evaluation benchmark for medical consultation information extraction and provide a systematic evaluation methodology, advancing the optimization of medical natural language processing (NLP) models. Experimental results show EMRModel achieves an F1 score of 88.1%, improving by49.5% over standard pre-trained models. Compared to traditional LoRA fine-tuning methods, our model shows superior performance, highlighting its effectiveness in structured medical record extraction tasks.

</details>


### [86] [T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning](https://arxiv.org/abs/2504.16460)

*Vignesh Ethiraj, Sidhanth Menon, Divya Vijay*

**Main category:** cs.CL

**TL;DR:** T-VEC is a specialized embedding model for the telecommunications industry that significantly improves performance on telecom-specific tasks by deeply fine-tuning a state-of-the-art model.

<details>
  <summary>Details</summary>

**Motivation:** Standard NLP models struggle with the unique vocabulary and concepts in telecommunications, leading to subpar performance on related tasks.

**Method:** T-VEC is developed by fine-tuning the gte-Qwen2-1.5B-instruct model using triplet loss on a large, curated telecom dataset, modifying weights across 338 layers to achieve deep integration of domain knowledge.

**Result:** T-VEC achieves a leading average MTEB score of 0.825 and vastly outperforms existing models on a telecom triplet evaluation benchmark with a score of 0.9380 compared to less than 0.07.

**Conclusion:** The introduction of T-VEC, along with an open-sourced telecom-specific tokenizer, establishes NetoAI as a leader in telecom AI innovation, offering a powerful tool for the community.

**Abstract:** The specialized vocabulary and complex concepts of the telecommunications industry present significant challenges for standard Natural Language Processing models. Generic text embeddings often fail to capture telecom-specific semantics, hindering downstream task performance. We introduce T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet loss objective on a meticulously curated, large-scale dataset of telecom-specific data. Crucially, this process involved substantial modification of weights across 338 layers of the base model, ensuring deep integration of domain knowledge, far exceeding superficial adaptation techniques. We quantify this deep change via weight difference analysis. A key contribution is the development and open-sourcing (MIT License) of the first dedicated telecom-specific tokenizer, enhancing the handling of industry jargon. T-VEC achieves a leading average MTEB score (0.825) compared to established models and demonstrates vastly superior performance (0.9380 vs. less than 0.07) on our internal telecom-specific triplet evaluation benchmark, indicating an exceptional grasp of domain-specific nuances, visually confirmed by improved embedding separation. This work positions NetoAI at the forefront of telecom AI innovation, providing the community with a powerful, deeply adapted, open-source tool.

</details>


### [87] [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](https://arxiv.org/abs/2504.16511)

*Fengze Liu, Weidong Zhou, Binbin Liu, Zhimiao Yu, Yifan Zhang, Haobin Lin, Yifeng Yu, Xiaohuan Zhou, Taifeng Wang, Yong Cao*

**Main category:** cs.CL

**TL;DR:** The paper presents QuaDMix, a unified framework for optimizing data quality and diversity in training large language models, demonstrating significant performance improvements.

<details>
  <summary>Details</summary>

**Motivation:** Current approaches optimize quality and diversity metrics separately, neglecting their inherent trade-off; a unified approach is needed to effectively balance both aspects in LLM pretraining.

**Method:** The QuaDMix framework introduces multiple criteria for data quality measurement and employs domain classification for diversity, using a parameterized sampling function to optimize data distribution.

**Result:** Experiments reveal that QuaDMix outperforms existing independent strategies by achieving an average performance improvement of 7.2% across various benchmarks and datasets.

**Conclusion:** The results underscore the importance of jointly considering quality and diversity in data selection for LLM training, confirming QuaDMix's effectiveness in enhancing model performance.

**Abstract:** Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.

</details>


### [88] [Transformers for Complex Query Answering over Knowledge Hypergraphs](https://arxiv.org/abs/2504.16537)

*Hong Ting Tsang, Zihao Wang, Yangqiu Song*

**Main category:** cs.CL

**TL;DR:** The paper presents the Logical Knowledge Hypergraph Transformer (LKHGT), a model designed for Complex Query Answering (CQA) that effectively utilizes hyper-relational graphs and offers improved performance on new datasets.

<details>
  <summary>Details</summary>

**Motivation:** There is a need for more sophisticated models in CQA that can better represent real-world data complexities, as classic triple KGs are limited in their representation capabilities.

**Method:** The authors propose a two-stage transformer model, LKHGT, which includes a Projection Encoder for atomic queries and a Logical Encoder for handling complex logical operations, both leveraging Type Aware Bias for enhanced token interaction.

**Result:** LKHGT achieves state-of-the-art performance in answering queries from newly created CQA datasets, demonstrating the ability to generalize across various query types, including those that are out-of-distribution.

**Conclusion:** The study confirms that LKHGT effectively addresses limitations in existing CQA models and provides a robust approach for handling complex queries in knowledge hypergraphs.

**Abstract:** Complex Query Answering (CQA) has been extensively studied in recent years. In order to model data that is closer to real-world distribution, knowledge graphs with different modalities have been introduced. Triple KGs, as the classic KGs composed of entities and relations of arity 2, have limited representation of real-world facts. Real-world data is more sophisticated. While hyper-relational graphs have been introduced, there are limitations in representing relationships of varying arity that contain entities with equal contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and M-FB15k-HCQA. Each dataset contains various query types that include logical operations such as projection, negation, conjunction, and disjunction. In order to answer knowledge hypergraph (KHG) existential first-order queries, we propose a two-stage transformer model, the Logical Knowledge Hypergraph Transformer (LKHGT), which consists of a Projection Encoder for atomic projection and a Logical Encoder for complex logical operations. Both encoders are equipped with Type Aware Bias (TAB) for capturing token interactions. Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA method over KHG and is able to generalize to out-of-distribution query types.

</details>


### [89] [PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression](https://arxiv.org/abs/2504.16574)

*Lizhe Chen, Binjia Zhou, Yuyao Ge, Jiayi Chen, Shiguang NI*

**Main category:** cs.CL

**TL;DR:** This paper introduces Prompt Importance Sampling (PIS), a novel framework for dynamically compressing prompts for large language models (LLMs) by using attention scores to enhance efficiency and effectiveness.

<details>
  <summary>Details</summary>

**Motivation:** The high costs associated with the performance of LLMs restrict their widespread use, demonstrating a need for more efficient prompt compression methods.

**Method:** PIS uses a dual-level compression approach involving a reinforcement learning network to assess token importance and a Russian roulette strategy for semantic-level sampling.

**Result:** PIS shows state-of-the-art performance in prompt compression across various benchmarks and inadvertently improves reasoning efficiency by optimizing context structure.

**Conclusion:** The framework enhances prompt engineering by merging theoretical foundations with practical strategies, improving context management in LLM applications.

**Abstract:** Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs.

</details>


### [90] [Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study](https://arxiv.org/abs/2504.16601)

*Andy Li, Wei Zhou, Rashina Hoda, Chris Bain, Peter Poon*

**Main category:** cs.CL

**TL;DR:** This study compares the translation quality of large language models (LLMs) and traditional machine translation (MT) tools for medical consultation summaries across three languages.

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of LLMs versus traditional MT tools in translating medical consultation summaries, since accurate medical translation is crucial for patient care.

**Method:** The study employed standard automated metrics to assess translations of patient-friendly and clinician-focused texts from English into Arabic, Chinese, and Vietnamese.

**Result:** Traditional MT tools generally outperformed LLMs, particularly for complex texts, while LLMs performed better on simpler Vietnamese and Chinese summaries; Arabic translations improved with complexity due to morphological factors.

**Conclusion:** LLMs show potential but are inconsistent and current evaluation metrics are inadequate for assessing clinical relevance; hence, there is a need for domain-specific training, better evaluation methods, and human oversight.

**Abstract:** This study evaluates how well large language models (LLMs) and traditional machine translation (MT) tools translate medical consultation summaries from English into Arabic, Chinese, and Vietnamese. It assesses both patient, friendly and clinician, focused texts using standard automated metrics. Results showed that traditional MT tools generally performed better, especially for complex texts, while LLMs showed promise, particularly in Vietnamese and Chinese, when translating simpler summaries. Arabic translations improved with complexity due to the language's morphology. Overall, while LLMs offer contextual flexibility, they remain inconsistent, and current evaluation metrics fail to capture clinical relevance. The study highlights the need for domain-specific training, improved evaluation methods, and human oversight in medical translation.

</details>


### [91] [Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories](https://arxiv.org/abs/2504.16604)

*Mareike Lisker, Christina Gottschalk, Helena Mihaljević*

**Main category:** cs.CL

**TL;DR:** This paper evaluates the effectiveness of Large Language Models in generating counterspeech against conspiracy theories, revealing significant limitations in their outputs.

<details>
  <summary>Details</summary>

**Motivation:** Addressing the challenge of scaling expert-driven counterspeech efforts against harmful online content, particularly conspiracy theories, which lack existing datasets for this purpose.

**Method:** The study assesses GPT-4o, Llama 3, and Mistral to generate counterspeech using structured prompts based on psychological research.

**Result:** The models often produced generic, repetitive, or superficial responses and tended to over-acknowledge fear while hallucinating facts, sources, or figures.

**Conclusion:** The findings indicate that using prompt-based strategies with these models for practical counterspeech applications is problematic due to their limitations.

**Abstract:** Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.

</details>


### [92] [TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval](https://arxiv.org/abs/2504.16627)

*Prasanna Devadiga, Arya Suneesh, Pawan Kumar Rajpoot, Bharatdeep Hazarika, Aditya U Baliga*

**Main category:** cs.CL

**TL;DR:** This paper presents a two-stage approach to retrieving fact-checked claims in both monolingual and crosslingual contexts, utilizing a fine-tuned embedding model and an LLM-based reranker, achieving high success rates.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the urgent challenge of effectively retrieving previously fact-checked claims to combat misinformation on a global scale.

**Method:** The proposed method employs a two-stage strategy consisting of a baseline retrieval system augmented by a fine-tuned embedding model, followed by an LLM-based reranker to enhance accuracy and efficiency.

**Result:** The integrated system achieved a success@10 score of 0.938 for monolingual tasks and 0.81025 for crosslingual tasks, indicating strong performance in both contexts.

**Conclusion:** The study demonstrates that LLM-based translation can significantly improve multilingual information retrieval and ensures that the system can be operated on consumer-grade GPUs.

**Abstract:** We address the challenge of retrieving previously fact-checked claims in monolingual and crosslingual settings - a critical task given the global prevalence of disinformation. Our approach follows a two-stage strategy: a reliable baseline retrieval system using a fine-tuned embedding model and an LLM-based reranker. Our key contribution is demonstrating how LLM-based translation can overcome the hurdles of multilingual information retrieval. Additionally, we focus on ensuring that the bulk of the pipeline can be replicated on a consumer GPU. Our final integrated system achieved a success@10 score of 0.938 and 0.81025 on the monolingual and crosslingual test sets, respectively.

</details>


### [93] [A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics](https://arxiv.org/abs/2504.16677)

*Luisa Shimabucoro, Ahmet Ustun, Marzieh Fadaee, Sebastian Ruder*

**Main category:** cs.CL

**TL;DR:** This study investigates the dynamics of cross-lingual transfer in large language models fine-tuned on multilingual data for various generative tasks.

<details>
  <summary>Details</summary>

**Motivation:** To enhance the utility of large language models globally through effective cross-lingual transfer while clarifying its dynamics.

**Method:** Analysis of two model families with up to 35B parameters trained on controlled multilingual data mixtures across three generative tasks, evaluating single-task and multi-task instruction tuning settings.

**Result:** Cross-lingual transfer dynamics and multilingual performance are influenced by the interaction of post-training settings rather than isolated variables.

**Conclusion:** Identified conditions for effective cross-lingual transfer in practice, highlighting the complexity of the post-training dynamics.

**Abstract:** In order for large language models to be useful across the globe, they are fine-tuned to follow instructions on multilingual data. Despite the ubiquity of such post-training, a clear understanding of the dynamics that enable cross-lingual transfer remains elusive. This study examines cross-lingual transfer (CLT) dynamics in realistic post-training settings. We study two model families of up to 35B parameters in size trained on carefully controlled mixtures of multilingual data on three generative tasks with varying levels of complexity (summarization, instruction following, and mathematical reasoning) in both single-task and multi-task instruction tuning settings. Overall, we find that the dynamics of cross-lingual transfer and multilingual performance cannot be explained by isolated variables, varying depending on the combination of post-training settings. Finally, we identify the conditions that lead to effective cross-lingual transfer in practice.

</details>


### [94] [HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations](https://arxiv.org/abs/2504.16754)

*Kwangseob Ahn*

**Main category:** cs.CL

**TL;DR:** HEMA is a dual-memory architecture that enables large language models to maintain coherent dialogues beyond 300 turns by combining global narrative coherence and episodic memory.

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with coherence in extended conversations despite performing well within their context windows.

**Method:** HEMA integrates Compact Memory for a continuous one-sentence summary and Vector Memory for querying episodic chunk embeddings, implemented with a 6B-parameter transformer.

**Result:** Factual recall accuracy improved from 41% to 87%, human-rated coherence increased from 2.7 to 4.3 on a 5-point scale, and Vector Memory achieved performance metrics indicating significant gains in precision and recall.

**Conclusion:** HEMA provides a practical solution for privacy-aware conversational AI, capable of engaging in month-long dialogues without retraining, enhancing both recall and coherence.

**Abstract:** Large language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining.

</details>


### [95] [How Effective are Generative Large Language Models in Performing Requirements Classification?](https://arxiv.org/abs/2504.16768)

*Waad Alhoshan, Alessio Ferrari, Liping Zhao*

**Main category:** cs.CL

**TL;DR:** The study evaluates the effectiveness of generative large language models (LLMs) in requirements classification, addressing a gap in their application compared to non-generative models.

<details>
  <summary>Details</summary>

**Motivation:** There is limited exploration of generative LLMs in requirements classification, despite their potential for context-aware text generation, leading to an important inquiry into their performance in this domain.

**Method:** An extensive experimental study was conducted involving over 400 experiments with three generative LLMs (Bloom, Gemma, and Llama) across three datasets (PROMISE NFR, Functional-Quality, SecReq).

**Result:** The study found that prompt design and LLM architecture are universally important factors, while dataset variations influence performance based on the classification task complexity.

**Conclusion:** Insights from this research can improve future model development by focusing on optimizing prompt structures and aligning model architectures with specific task requirements.

**Abstract:** In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance.

</details>


### [96] [Evaluation Framework for AI Systems in "the Wild"](https://arxiv.org/abs/2504.16778)

*Sarah Jabbour, Trenton Chang, Anindya Das Antar, Joseph Peper, Insu Jang, Jiachen Liu, Jae-Won Chung, Shiqi He, Michael Wellman, Bryan Goodman, Elizabeth Bondi-Kelly, Kevin Samy, Rada Mihalcea, Mosharaf Chowhury, David Jurgens, Lu Wang*

**Main category:** cs.CL

**TL;DR:** The paper proposes a new framework for evaluating Generative AI systems that reflects real-world performance and emphasizes continuous, holistic assessments.

<details>
  <summary>Details</summary>

**Motivation:** Current evaluation methods for Generative AI fail to address the practical applications and real-world performance of these models, often relying on outdated benchmarks.

**Method:** The authors propose a comprehensive evaluation framework that integrates diverse input assessments, emphasizing ongoing evaluations that incorporate both human and automated methods with a focus on performance, fairness, and ethics.

**Result:** The proposed framework aims to accurately reflect the real-time capabilities of GenAI systems and provides guidance for practitioners and policymakers to enhance the evaluation and regulation of these technologies.

**Conclusion:** Adopting a holistic and dynamic evaluation approach ensures Generative AI models are technically proficient, ethically responsible, and impactful in practice.

**Abstract:** Generative AI (GenAI) models have become vital across industries, yet current evaluation methods have not adapted to their widespread use. Traditional evaluations often rely on benchmarks and fixed datasets, frequently failing to reflect real-world performance, which creates a gap between lab-tested outcomes and practical applications. This white paper proposes a comprehensive framework for how we should evaluate real-world GenAI systems, emphasizing diverse, evolving inputs and holistic, dynamic, and ongoing assessment approaches. The paper offers guidance for practitioners on how to design evaluation methods that accurately reflect real-time capabilities, and provides policymakers with recommendations for crafting GenAI policies focused on societal impacts, rather than fixed performance numbers or parameter sizes. We advocate for holistic frameworks that integrate performance, fairness, and ethics and the use of continuous, outcome-oriented methods that combine human and automated assessments while also being transparent to foster trust among stakeholders. Implementing these strategies ensures GenAI models are not only technically proficient but also ethically responsible and impactful.

</details>


### [97] [MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores](https://arxiv.org/abs/2504.16786)

*Fengwei Zhou, Jiafei Song, Wenjin Jason Li, Gengjian Xue, Zhikang Zhao, Yichao Lu, Bailin Na*

**Main category:** cs.CL

**TL;DR:** The paper presents MOOSComp, a method for long-context compression in large language models that improves efficiency and performance by addressing over-smoothing and utilizing outlier scores.

<details>
  <summary>Details</summary>

**Motivation:** To enhance the practical applications of language models in resource-constrained environments where inference time and resource consumption are critical issues.

**Method:** The proposed method employs a token-classification approach combined with an inter-class cosine similarity loss during training to improve token representation accuracy and incorporates outlier scores during compression to retain important tokens.

**Result:** MOOSComp achieves superior performance across various compression ratios on long-context benchmarks and demonstrates a 3.3x speedup at a 4x compression ratio on mobile devices.

**Conclusion:** The method significantly enhances the efficiency and effectiveness of long-context processing in language models, making it viable for use in limited-resource environments.

**Abstract:** Recent advances in large language models have significantly improved their ability to process long-context input, but practical applications are challenged by increased inference time and resource consumption, particularly in resource-constrained environments. To address these challenges, we propose MOOSComp, a token-classification-based long-context compression method that enhances the performance of a BERT-based compressor by mitigating the over-smoothing problem and incorporating outlier scores. In the training phase, we add an inter-class cosine similarity loss term to penalize excessively similar token representations, thereby improving the token classification accuracy. During the compression phase, we introduce outlier scores to preserve rare but critical tokens that are prone to be discarded in task-agnostic compression. These scores are integrated with the classifier's output, making the compressor more generalizable to various tasks. Superior performance is achieved at various compression ratios on long-context understanding and reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x compression ratio on a resource-constrained mobile device.

</details>


### [98] [Credible plan-driven RAG method for Multi-hop Question Answering](https://arxiv.org/abs/2504.16787)

*Ningning Zhang, Chi Zhang, Zhizhong Tan, Xingxing Yang, Weiping Deng, Wenyong Wang*

**Main category:** cs.CL

**TL;DR:** The PAR RAG framework improves multi-hop question answering by structured problem decomposition and error management, outperforming existing methods.

<details>
  <summary>Details</summary>

**Motivation:** Multi-hop QA is challenging due to errors in reasoning paths and intermediate results in current RAG methods, which lead to reduced answer accuracy.

**Method:** The PAR RAG framework utilizes a three-stage process: planning through top-down decomposition, execution with multi-granularity verification, and review to ensure accuracy and manage error propagation.

**Result:** Experiments show that PAR RAG significantly outperforms state-of-the-art methods in metrics like EM and F1 scores on multi-hop QA datasets.

**Conclusion:** The PAR RAG framework provides a reliable and interpretable approach to multi-hop QA, effectively mitigating the issues related to error propagation.

**Abstract:** Multi-hop question answering (QA) presents a considerable challenge for Retrieval-Augmented Generation (RAG), requiring the structured decomposition of complex queries into logical reasoning paths and the generation of dependable intermediate results. However, deviations in reasoning paths or errors in intermediate results, which are common in current RAG methods, may propagate and accumulate throughout the reasoning process, diminishing the accuracy of the answer to complex queries. To address this challenge, we propose the Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key stages: planning, act, and review, and aims to offer an interpretable and incremental reasoning paradigm for accurate and reliable multi-hop question answering by mitigating error propagation.PAR RAG initially applies a top-down problem decomposition strategy, formulating a comprehensive plan that integrates multiple executable steps from a holistic viewpoint. This approach avoids the pitfalls of local optima common in traditional RAG methods, ensuring the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a plan execution mechanism based on multi-granularity verification. By utilizing both coarse-grained similarity information and fine-grained relevant data, the framework thoroughly checks and adjusts intermediate results, ensuring process accuracy while effectively managing error propagation and amplification. Experimental results on multi-hop QA datasets demonstrate that the PAR RAG framework substantially outperforms existing state-of-the-art methods in key metrics, including EM and F1 scores.

</details>


### [99] [Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention](https://arxiv.org/abs/2504.16795)

*Xiang Hu, Jiaqi Leng, Jun Zhao, Kewei Tu, Wei Wu*

**Main category:** cs.CL

**TL;DR:** HSA enhances RNNs with efficient long-range attention, achieving high accuracy in context retrieval while maintaining efficiency.

<details>
  <summary>Details</summary>

**Motivation:** To address RNNs' inability to access historical context and the potential inefficiency of integrating attention mechanisms.

**Method:** Introducing Hierarchical Sparse Attention (HSA) that organizes inputs into chunks and learns token-to-chunk relevance, along with a hardware-aligned kernel design to ensure efficiency.

**Result:** RAMba, which incorporates HSA, achieves perfect accuracy in passkey retrieval across 64 million contexts and shows significant improvements in various downstream tasks with a nearly constant memory footprint.

**Conclusion:** RAMba demonstrates a promising advancement in efficient long-context modeling, leveraging the benefits of both RNNs and an innovative attention mechanism.

**Abstract:** A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.

</details>


### [100] [LLM-assisted Graph-RAG Information Extraction from IFC Data](https://arxiv.org/abs/2504.16813)

*Sima Iranmanesh, Hadeel Saadany, Edlira Vakaj*

**Main category:** cs.CL

**TL;DR:** The paper explores the use of LLMs with Graph Retrieval-Augmented Generation (Graph-RAG) to simplify the parsing of complex IFC data in construction, enhancing natural language interactions.

<details>
  <summary>Details</summary>

**Motivation:** The need for effective extraction and representation of building information from complex IFC data formats used in collaborative construction work.

**Method:** Utilization of LLMs combined with the Graph-RAG technique to parse IFC data and retrieve object properties and their relationships.

**Result:** The approach improves the performance of generative models like GPT-4o in handling IFC data by incorporating graph-based knowledge, allowing for effective query-response generation.

**Conclusion:** Graph-RAG parsing provides a method to enhance interaction with IFC data, mitigating the complications of its hierarchical structure and reducing the need for elaborate systems.

**Abstract:** IFC data has become the general building information standard for collaborative work in the construction industry. However, IFC data can be very complicated because it allows for multiple ways to represent the same product information. In this research, we utilise the capabilities of LLMs to parse the IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to retrieve building object properties and their relations. We will show that, despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG parsing enhances generative LLMs like GPT-4o with graph-based knowledge, enabling natural language query-response retrieval without the need for a complex pipeline.

</details>


### [101] [GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning](https://arxiv.org/abs/2504.16832)

*Luu Quy Tung, Hoang Quoc Viet, Vo Trong Thu*

**Main category:** cs.CL

**TL;DR:** GreenMind-Medium-14B-R1 is a Vietnamese reasoning model that improves LLM tasks requiring intermediate reasoning by addressing language mixing and factual correctness.

<details>
  <summary>Details</summary>

**Motivation:** To develop a robust Vietnamese reasoning model that overcomes limitations in existing language models, particularly in intermediate reasoning tasks and linguistic consistency.

**Method:** The model is based on Group Relative Policy Optimization and utilizes a high-quality synthesized reasoning dataset, with two designed reward functions to reduce language mixing and ensure the generated content's factual correctness using Sentence Transformer models.

**Result:** The model outperforms previous works on the Vietnamese VLSP 2023 Challenge dataset and improves linguistic consistency in generated responses. It also shows effectiveness in the SeaExam multilingual multiple-choice dataset compared to few-shot prompting techniques.

**Conclusion:** GreenMind-Medium-14B-R1 demonstrates enhanced performance and consistency in reasoning tasks, marking a significant advancement in Vietnamese language processing using LLM.

**Abstract:** Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques.

</details>


### [102] [Monte Carlo Planning with Large Language Model for Text-Based Game Agents](https://arxiv.org/abs/2504.16855)

*Zijing Shi, Meng Fang, Ling Chen*

**Main category:** cs.CL

**TL;DR:** The MC-DML algorithm enhances the planning performance of language-based autonomous agents in text-based games by integrating Monte Carlo Tree Search and Large Language Models with memory mechanisms.

<details>
  <summary>Details</summary>

**Motivation:** Text-based games serve as useful environments for developing language-based autonomous agents, but existing planning-then-learning methods are slow and lack effective language understanding and reasoning.

**Method:** The MC-DML algorithm combines Monte Carlo Tree Search with Large Language Models, incorporating in-trial and cross-trial memory mechanisms to allow the model to learn from past experiences and improve action evaluations during planning.

**Result:** Experiments on text-based games from the Jericho benchmark show that the MC-DML algorithm significantly improves performance during the initial planning phase, surpassing established methods that use multiple iterations.

**Conclusion:** The findings indicate that MC-DML is an effective approach for enhancing language-grounded planning in complex environments, offering a more efficient alternative to traditional methods.

**Abstract:** Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.

</details>


### [103] [Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification](https://arxiv.org/abs/2504.16856)

*Alexander Shvets*

**Main category:** cs.CL

**TL;DR:** This paper presents an LLM-based data synthesis pipeline that generates context-rich sentiment analysis examples, enhancing emotion understanding across multiple classes.

<details>
  <summary>Details</summary>

**Motivation:** Existing sentiment analysis datasets lack contextual information and breadth of emotion categories, leading to suboptimal performance in understanding varied emotional expressions.

**Method:** The authors designed a data synthesis pipeline using the Mistral-7b model to generate context-rich training examples, resulting in a dataset of 100K contextual and 300K context-less examples, which was used to fine-tune pre-trained BERT-type encoders.

**Result:** The fine-tuned Emo Pillars models demonstrated high adaptability to new domains and achieved state-of-the-art (SOTA) performance on several sentiment analysis tasks, including GoEmotions, ISEAR, and IEMOCAP.

**Conclusion:** The study validates the generated dataset's effectiveness in utterance diversification and context personalization, while highlighting the need for improved management of out-of-taxonomy labels within the data synthesis process.

**Abstract:** Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline.

</details>


### [104] [Planning with Diffusion Models for Target-Oriented Dialogue Systems](https://arxiv.org/abs/2504.16858)

*Hanwen Du, Bo Peng, Xia Ning*

**Main category:** cs.CL

**TL;DR:** DiffTOD is a novel dialogue planning framework that uses diffusion models for non-sequential target-oriented dialogue planning, overcoming limitations of traditional sequential methods.

<details>
  <summary>Details</summary>

**Motivation:** To improve dialogue planning by addressing the pitfalls of existing step-by-step methods, such as compounding errors and myopic actions.

**Method:** DiffTOD formulates dialogue planning as a trajectory generation problem and uses a diffusion language model for estimating the likelihood of dialogue trajectories, incorporating tailored guidance mechanisms for various target types.

**Result:** Extensive experiments indicate that DiffTOD enables effective exploration and optimization of action strategies over longer horizons, showing strong flexibility in diverse dialogue scenarios.

**Conclusion:** DiffTOD significantly enhances non-myopic dialogue planning capabilities and provides a robust solution for various target-oriented dialogue challenges.

**Abstract:** Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance towards diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through https://anonymous.4open.science/r/DiffTOD.

</details>


### [105] [Do Large Language Models know who did what to whom?](https://arxiv.org/abs/2504.16884)

*Joseph M. Denning, Xiaohan, Guo, Bryor Snefjella, Idan A. Blank*

**Main category:** cs.CL

**TL;DR:** This paper examines the capacity of Large Language Models (LLMs) to infer thematic roles in sentences, revealing that while LLMs can capture these roles, they do so with less influence compared to humans.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address critiques of LLMs regarding their understanding of language, specifically focusing on the specific cognitive ability of inferring thematic roles in sentence structures.

**Method:** Two experiments were conducted to evaluate sentence representations in four different LLMs, analyzing how these models process thematic roles in relation to syntactic similarity.

**Result:** The findings indicated that LLMs showed representational similarities based on syntax rather than thematic role assignments, and evidence of thematic role information was limited across hidden units, though some attention heads effectively captured thematic roles.

**Conclusion:** While LLMs possess the ability to extract thematic roles, this capability is less robust compared to human understanding, suggesting a weaker influence of thematic information on their sentence representations.

**Abstract:** Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.

</details>


### [106] [Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text](https://arxiv.org/abs/2504.16913)

*Shifali Agrahari, Sanasam Ranbir Singh*

**Main category:** cs.CL

**TL;DR:** COT Fine-tuned is a new framework for detecting AI-generated text and identifying the language model responsible, leveraging Chain-of-Thought reasoning for enhanced interpretability.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the growing need for detecting AI-generated text to ensure academic integrity, combat misinformation, and promote ethical AI use.

**Method:** The proposed dual-task approach includes classifying text as AI-generated or human-written (Task A) and identifying the language model responsible for the text (Task B), utilizing Chain-of-Thought reasoning for explanations.

**Result:** COT Fine-tuned achieves high accuracy in classifying texts and identifying language models, demonstrating strong performance in both tasks.

**Conclusion:** The Chain-of-Thought reasoning process significantly enhances the model's effectiveness and interpretability, making it a valuable tool for detecting AI-generated text.

**Abstract:** In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability.

</details>


### [107] [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)

*Raghav Thind, Youran Sun, Ling Liang, Haizhao Yang*

**Main category:** cs.CL

**TL;DR:** OptimAI is a framework that uses LLM-powered AI agents to optimize natural language described problems, significantly outperforming existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The need for domain expertise in formulating natural language optimization problems into mathematical expressions and selecting appropriate solvers hinders scientific research and practical applications.

**Method:** The framework includes a formulator for translating descriptions, a planner for high-level strategies, and roles for coding and critiquing to refine actions, with an emphasis on multi-agent collaboration and dynamic planning using UCB-based debug scheduling.

**Result:** OptimAI achieved 88.1% accuracy on the NLP4LP dataset and 71.2% on the Optibench subset, reducing error rates by 58% and 50% compared to previous best results.

**Conclusion:** The framework demonstrates superior performance and scalability in optimizing natural language problems, highlighting the importance of collaboration among AI agents.

**Abstract:** Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \textbf{OptimAI}, a framework for solving \underline{Optim}ization problems described in natural language by leveraging LLM-powered \underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \emph{coder} and a \emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\% accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\% and 50\% respectively over prior best results.

</details>


### [108] [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921)

*José Ángel González, Ian Borrego Obrador, Álvaro Romo Herrero, Areg Mikael Sarvazyan, Mara Chinea-Ríos, Angelo Basile, Marc Franco-Salvador*

**Main category:** cs.CL

**TL;DR:** IberBench is a new benchmark for evaluating large language models (LLMs) on diverse, industry-relevant NLP tasks across Iberian languages, addressing limitations of existing English-centric benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** To create a comprehensive evaluation framework for LLMs that goes beyond English-centric benchmarks, incorporates linguistic diversity, focuses on industry-relevant tasks, and allows for continual updates.

**Method:** IberBench integrates 101 datasets spanning 22 task categories and enables community-driven contributions for assessment, evaluating 23 LLMs with a focus on performance across diverse languages in the Iberian Peninsula and Ibero-America.

**Result:** The evaluation reveals that LLMs are generally weaker on industry-relevant tasks than on fundamental tasks, with Galician and Basque showing the lowest performance; some tasks yield near-random results, while others fall short of specialized task systems.

**Conclusion:** IberBench provides an open-source, adaptable framework for assessing LLMs, highlighting performance disparities and setting a foundation for future improvements in multilingual NLP evaluation.

**Abstract:** Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fall short in several key areas: they overlook the diversity of language varieties, prioritize fundamental Natural Language Processing (NLP) capabilities over tasks of industrial relevance, and are static. With these aspects in mind, we present IberBench, a comprehensive and extensible benchmark designed to assess LLM performance on both fundamental and industry-relevant NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America. IberBench integrates 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment and emotion analysis, toxicity detection, and summarization. The benchmark addresses key limitations in current evaluation practices, such as the lack of linguistic diversity and static evaluation setups by enabling continual updates and community-driven model and dataset submissions moderated by a committee of experts. We evaluate 23 LLMs ranging from 100 million to 14 billion parameters and provide empirical insights into their strengths and limitations. Our findings indicate that (i) LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii) performance is on average lower for Galician and Basque, (iii) some tasks show results close to random, and (iv) in other tasks LLMs perform above random but below shared task systems. IberBench offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard.

</details>
