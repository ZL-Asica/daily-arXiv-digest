{"id": "2508.18283", "pdf": "https://arxiv.org/pdf/2508.18283.pdf", "abs": "https://arxiv.org/abs/2508.18283", "title": "Technology-assisted Personalized Yoga for Better Health -- Challenges and Outlook", "authors": ["Vivek Kumar", "Himanshu Sahu", "Hari Prabhat Gupta", "Biplav Srivastava"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "10 Pages, 11 figures, 2 tables", "summary": "Yoga is a discipline of physical postures, breathing techniques, and\nmeditative practices rooted in ancient Indian traditions, now embraced\nworldwide for promoting overall well-being and inner balance. The practices are\na large set of items, our term for executable actions like physical poses or\nbreath exercises, to offer for a person's well-being. However, to get benefits\nof Yoga tailored to a person's unique needs, a person needs to (a) discover\ntheir subset from the large and seemingly complex set with inter-dependencies,\n(b) continue to follow them with interest adjusted to their changing abilities\nand near-term objectives, and (c) as appropriate, adapt to alternative items\nbased on changing environment and the person's health conditions. In this\nvision paper, we describe the challenges for the Yoga personalization problem.\nNext, we sketch a preliminary approach and use the experience to provide an\noutlook on solving the challenging problem using existing and novel techniques\nfrom a multidisciplinary computing perspective. To the best of our knowledge,\nthis is the first paper that comprehensively examines decision support issues\naround Yoga personalization, from pose sensing to recommendation of corrections\nfor a complete regimen, and illustrates with a case study of Surya Namaskar --\na set of 12 choreographed poses.", "AI": {"tldr": "This vision paper addresses the challenges of personalizing Yoga practices to individual needs using a multidisciplinary computing approach.", "motivation": "To promote the effective practice of Yoga tailored to individual requirements by overcoming the complexities in selecting suitable poses and adjustments.", "method": "The paper outlines a preliminary approach to Yoga personalization, discussing challenges in pose sensing and corrections recommendations, illustrated through a case study of Surya Namaskar.", "result": "It highlights how decision support systems can be developed for Yoga personalization, affecting well-being through tailored practices.", "conclusion": "The work emphasizes the need for a comprehensive examination of decision support issues in Yoga personalization using technological advancements.", "key_contributions": ["First comprehensive examination of Yoga personalization challenges.", "Introduction of decision support for pose sensing and recommendations.", "Case study application to demonstrate practical implications."], "limitations": "The approach is preliminary and may need further validation and testing with real users.", "keywords": ["Yoga personalization", "decision support", "pose sensing", "machine learning", "well-being"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.18317", "pdf": "https://arxiv.org/pdf/2508.18317.pdf", "abs": "https://arxiv.org/abs/2508.18317", "title": "Does Calibration Affect Human Actions?", "authors": ["Meir Nizri", "Amos Azaria", "Chirag Gupta", "Noam Hazon"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Calibration has been proposed as a way to enhance the reliability and\nadoption of machine learning classifiers. We study a particular aspect of this\nproposal: how does calibrating a classification model affect the decisions made\nby non-expert humans consuming the model's predictions? We perform a\nHuman-Computer-Interaction (HCI) experiment to ascertain the effect of\ncalibration on (i) trust in the model, and (ii) the correlation between\ndecisions and predictions. We also propose further corrections to the reported\ncalibrated scores based on Kahneman and Tversky's prospect theory from\nbehavioral economics, and study the effect of these corrections on trust and\ndecision-making. We find that calibration is not sufficient on its own; the\nprospect theory correction is crucial for increasing the correlation between\nhuman decisions and the model's predictions. While this increased correlation\nsuggests higher trust in the model, responses to ``Do you trust the model\nmore?\" are unaffected by the method used.", "AI": {"tldr": "The paper explores how calibrating machine learning classifiers impacts human decision-making and trust through an HCI experiment.", "motivation": "To understand the effects of model calibration on human trust and decision-making in machine learning predictions.", "method": "A Human-Computer-Interaction experiment was conducted to evaluate how calibration affects trust and correlation between human decisions and model predictions.", "result": "Calibration improves trust and correlation between decisions and predictions, but additional corrections based on prospect theory are necessary for significant improvements.", "conclusion": "Calibration alone is insufficient; incorporating prospect theory corrections is vital for enhancing the relationship between human decisions and model predictions.", "key_contributions": ["Investigation of the impact of model calibration on non-expert human decisions", "Application of prospect theory to adjust calibrated scores", "Insights into trust dynamics regarding machine learning predictions"], "limitations": "The study focuses on non-expert humans, which may limit generalizability to expert environments.", "keywords": ["machine learning", "calibration", "trust", "decision-making", "HCI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.18481", "pdf": "https://arxiv.org/pdf/2508.18481.pdf", "abs": "https://arxiv.org/abs/2508.18481", "title": "Impact of Target and Tool Visualization on Depth Perception and Usability in Optical See-Through AR", "authors": ["Yue Yang", "Xue Xie", "Xinkai Wang", "Hui Zhang", "Chiming Yu", "Xiaoxian Xiong", "Lifeng Zhu", "Yuanyi Zheng", "Jue Cen", "Bruce Daniel", "Fred Baik"], "categories": ["cs.HC", "cs.CV", "cs.GR"], "comment": null, "summary": "Optical see-through augmented reality (OST-AR) systems like Microsoft\nHoloLens 2 hold promise for arm's distance guidance (e.g., surgery), but depth\nperception of the hologram and occlusion of real instruments remain\nchallenging. We present an evaluation of how visualizing the target object with\ndifferent transparencies and visualizing a tracked tool (virtual proxy vs. real\ntool vs. no tool tracking) affects depth perception and system usability. Ten\nparticipants performed two experiments on HoloLens 2. In Experiment 1, we\ncompared high-transparency vs. low-transparency target rendering in a depth\nmatching task at arm's length. In Experiment 2, participants performed a\nsimulated surgical pinpoint task on a frontal bone target under six\nvisualization conditions ($2 \\times 3$: two target transparencies and three\ntool visualization modes: virtual tool hologram, real tool, or no tool\ntracking). We collected data on depth matching error, target localization\nerror, system usability, task workload, and qualitative feedback. Results show\nthat a more opaque target yields significantly lower depth estimation error\nthan a highly transparent target at arm's distance. Moreover, showing the real\ntool (occluding the virtual target) led to the highest accuracy and usability\nwith the lowest workload, while not tracking the tool yielded the worst\nperformance and user ratings. However, making the target highly transparent,\nwhile allowing the real tool to remain visible, slightly impaired depth cues\nand did not improve usability. Our findings underscore that correct occlusion\ncues, rendering virtual content opaque and occluding it with real tools in real\ntime, are critical for depth perception and precision in OST-AR. Designers of\narm-distance AR systems should prioritize robust tool tracking and occlusion\nhandling; if unavailable, cautiously use transparency to balance depth\nperception and tool visibility.", "AI": {"tldr": "The paper evaluates the impact of transparency and tool visualization on depth perception and usability in optical see-through augmented reality systems, particularly in surgical applications.", "motivation": "To address challenges in depth perception and occlusion in optical see-through AR systems like Microsoft HoloLens 2 during arm's distance guidance tasks.", "method": "Two experiments were conducted with ten participants: Experiment 1 compared high-transparency and low-transparency target rendering in a depth matching task, while Experiment 2 analyzed a simulated surgical task under various visualization conditions (virtual tool hologram, real tool, no tool tracking).", "result": "The study found that opaque targets significantly reduce depth estimation error compared to highly transparent ones, and using a real tool improved accuracy and usability, unlike no tool tracking which performed the worst.", "conclusion": "Correct occlusion, opaque rendering, and real-time tracking of tools enhance depth perception and task performance in OST-AR, suggesting designers prioritize robust occlusion and tool visibility.", "key_contributions": ["Empirical evaluation of transparency effects on depth perception in OST-AR.", "Demonstrated importance of real tool visibility for accuracy and usability.", "Provided design recommendations for arm-distance AR systems regarding transparency and occlusion."], "limitations": "", "keywords": ["Augmented Reality", "Depth Perception", "Holography", "Usability", "Transparency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.18499", "pdf": "https://arxiv.org/pdf/2508.18499.pdf", "abs": "https://arxiv.org/abs/2508.18499", "title": "Skeptik: A Hybrid Framework for Combating Potential Misinformation in Journalism", "authors": ["Arlen Fan", "Fan Lei", "Steven R. Corman", "Ross Maciejewski"], "categories": ["cs.HC"], "comment": "Arlen Fan and Fan Lei contributed equally to this research. Accepted\n  by ACM Transactions on Interactive Intelligent Systems (TiiS)", "summary": "The proliferation of misinformation in journalism, often stemming from flawed\nreasoning and logical fallacies, poses significant challenges to public\nunderstanding and trust in news media. Traditional fact-checking methods, while\nvaluable, are insufficient for detecting the subtle logical inconsistencies\nthat can mislead readers within seemingly factual content. To address this gap,\nwe introduce Skeptik, a hybrid framework that integrates Large Language Models\n(LLMs) with heuristic approaches to analyze and annotate potential logical\nfallacies and reasoning errors in online news articles. Operating as a web\nbrowser extension, Skeptik automatically highlights sentences that may contain\nlogical fallacies, provides detailed explanations, and offers multi-layered\ninterventions to help readers critically assess the information presented. The\nsystem is designed to be extensible, accommodating a wide range of fallacy\ntypes and adapting to evolving misinformation tactics. Through comprehensive\ncase studies, quantitative analyses, usability experiments, and expert\nevaluations, we demonstrate the effectiveness of Skeptik in enhancing readers'\ncritical examination of news content and promoting media literacy. Our\ncontributions include the development of an expandable classification system\nfor logical fallacies, the innovative integration of LLMs for real-time\nanalysis and annotation, and the creation of an interactive user interface that\nfosters user engagement and close reading. By emphasizing the logical integrity\nof textual content rather than relying solely on factual accuracy, Skeptik\noffers a comprehensive solution to combat potential misinformation in\njournalism. Ultimately, our framework aims to improve critical reading and\nprotect the public from deceptive information online and enhance the overall\ncredibility of news media.", "AI": {"tldr": "Skeptik is a hybrid framework that uses Large Language Models to detect logical fallacies in online news articles, enhancing media literacy and critical reading.", "motivation": "To address the challenges posed by misinformation stemming from flawed reasoning and logical fallacies in journalism.", "method": "A web browser extension that integrates LLMs with heuristic approaches to analyze, annotate, and highlight potential logical fallacies in news articles.", "result": "Skeptik effectively enhances readers' critical examination of news content and promotes media literacy through detailed explanations and interventions.", "conclusion": "The framework aims to improve critical reading and protect the public from deceptive information while enhancing news media credibility.", "key_contributions": ["Development of an expandable classification system for logical fallacies", "Integration of LLMs for real-time analysis and annotation", "Creation of an interactive user interface for user engagement"], "limitations": "", "keywords": ["misinformation", "logical fallacies", "Large Language Models", "media literacy", "news articles"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.18290", "pdf": "https://arxiv.org/pdf/2508.18290.pdf", "abs": "https://arxiv.org/abs/2508.18290", "title": "Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI", "authors": ["Hans-Joachim Rudolph"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "This essay develops a theoretical framework for a semantic Artificial General\nIntelligence (AGI) based on the notion of semantic attractors in complex-valued\nmeaning spaces. Departing from current transformer-based language models, which\noperate on statistical next-token prediction, we explore a model in which\nmeaning is not inferred probabilistically but formed through recursive\ntensorial transformation. Using cyclic operations involving the imaginary unit\n\\emph{i}, we describe a rotational semantic structure capable of modeling\nirony, homonymy, and ambiguity. At the center of this model, however, is a\nsemantic attractor -- a teleological operator that, unlike statistical\ncomputation, acts as an intentional agent (Microvitum), guiding meaning toward\nstability, clarity, and expressive depth. Conceived in terms of gradient flows,\ntensor deformations, and iterative matrix dynamics, the attractor offers a\nmodel of semantic transformation that is not only mathematically suggestive,\nbut also philosophically significant. We argue that true meaning emerges not\nfrom simulation, but from recursive convergence toward semantic coherence, and\nthat this requires a fundamentally new kind of cognitive architecture -- one\ndesigned to shape language, not just predict it.", "AI": {"tldr": "The paper introduces a theoretical framework for semantic AGI based on semantic attractors in complex-valued meaning spaces, moving beyond current transformer models.", "motivation": "To develop a model of AGI that transcends statistical next-token prediction and captures the complexities of meaning formation.", "method": "The study employs recursive tensor transformations and cyclic operations with the imaginary unit to establish a rotational semantic structure.", "result": "It presents a semantic attractor that serves as a teleological operator, guiding meaning towards stability and coherence, contrasting with the probabilistic nature of existing models.", "conclusion": "True meaning arises from recursive convergence towards semantic coherence requiring a new cognitive architecture that shapes language rather than predicts it.", "key_contributions": ["Introduction of the concept of semantic attractors in AGI", "Development of a rotational semantic structure for modeling complex meanings", "Proposal of a new cognitive architecture for meaning generation"], "limitations": "", "keywords": ["Semantic AGI", "Semantic attractors", "Tensor transformations"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.18545", "pdf": "https://arxiv.org/pdf/2508.18545.pdf", "abs": "https://arxiv.org/abs/2508.18545", "title": "Beyond prior knowledge: The predictive role of knowledge-building in Tutor Learning", "authors": ["Tasmia Shahriar", "Mia Ameen", "Aditi Mallavarapu", "Shiyan Jiang", "Noboru Matsuda"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "When adopting the role of a teacher in learning-by-teaching environments,\nstudents often struggle to engage in knowledge-building activities, such as\nproviding explanations and addressing misconceptions. Instead, they frequently\ndefault to knowledge-telling behaviors, where they simply dictate what they\nalready know or what to do without deeper reflection, thereby limiting\nlearning. Teachable agents, particularly those capable of posing persistent\nfollow-up questions, have been shown to encourage students (tutors) to shift\nfrom knowledge-telling to knowledge-building and enhance tutor learning. Tutor\nlearning encompasses two interrelated types of knowledge: conceptual and\nprocedural knowledge. Research has established a bidirectional relationship\nbetween these knowledge types, where improvements in one reinforce the other.\nThis study investigates the role of knowledge-building in mediating the\nbidirectional relationship between procedural and conceptual learning. Our\nfindings revealed a stable bidirectional relationship between procedural and\nconceptual knowledge, with higher post-test scores observed among students who\nengaged in knowledge-building, regardless of their procedural and conceptual\npre-test performance. This suggests that knowledge-building serves as a crucial\nmechanism bridging the gap between students with low prior knowledge and higher\nconceptual and procedural learning gain.", "AI": {"tldr": "This study examines how engaging in knowledge-building activities in teachable agent environments enhances students' procedural and conceptual learning.", "motivation": "Students in learning-by-teaching environments struggle with knowledge-building and often resort to knowledge-telling. This limits their learning potential.", "method": "The study investigates the relationship between knowledge-building, procedural knowledge, and conceptual knowledge in a teachable agent context.", "result": "The findings indicate a stable bidirectional relationship where higher engagement in knowledge-building activities leads to improved post-test scores in both procedural and conceptual knowledge.", "conclusion": "Knowledge-building is essential for enhancing learning outcomes in students, especially for those with lower prior knowledge.", "key_contributions": ["Demonstrates the importance of knowledge-building in learning environments.", "Establishes a bidirectional relationship between procedural and conceptual knowledge.", "Highlights the role of teachable agents in fostering deeper learning."], "limitations": "", "keywords": ["knowledge-building", "teachable agents", "learning-by-teaching", "procedural knowledge", "conceptual knowledge"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.18321", "pdf": "https://arxiv.org/pdf/2508.18321.pdf", "abs": "https://arxiv.org/abs/2508.18321", "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions", "authors": ["Maojia Song", "Tej Deep Pala", "Weisheng Jin", "Amir Zadeh", "Chuan Li", "Dorien Herremans", "Soujanya Poria"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS.", "AI": {"tldr": "The paper introduces KAIROS, a benchmark for studying decision-making in multi-agent systems involving large language models, focusing on trust formation and misinformation resistance.", "motivation": "To analyze how large language models (LLMs) interact in multi-agent systems (MAS) and develop strategies to enhance trust and decision-making under social dynamics.", "method": "The study presents KAIROS, a simulation of quiz contests with agents of varying reliability, allowing exploration of trust, peer influence, and decision-making strategies with various training approaches including prompting and reinforcement learning.", "result": "The best performance was achieved using Group Relative Policy Optimisation (GRPO) with context and outcome-based rewards, although it reduced robustness to social influence compared to base models.", "conclusion": "The findings highlight the complexities of trust and decision-making in multi-agent systems with LLMs, and suggest that GRPO is effective but can introduce vulnerabilities.", "key_contributions": ["Development of KAIROS benchmark for multi-agent systems", "Insights on trust formation and decision-making processes with LLMs", "Evaluation of various mitigation strategies including GRPO"], "limitations": "The model's robustness to social influence decreased when using GRPO compared to base models.", "keywords": ["multi-agent systems", "large language models", "trust formation", "KAIROS benchmark", "reinforcement learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.18580", "pdf": "https://arxiv.org/pdf/2508.18580.pdf", "abs": "https://arxiv.org/abs/2508.18580", "title": "Gamification of Immersive Cervical Rehabilitation Exercises in VR: An Exploratory Study on Chin Tuck and Range of Motion Exercises", "authors": ["Haitham Abdelsalam", "Chanelle Montpetit", "Arash Harirpoush", "Maryse Fortin", "Yiming Xiao"], "categories": ["cs.HC"], "comment": "8 pages, 7 figures, Accepted in the IEEE ISMAR 2025 XRehab Workshop", "summary": "Chronic neck pain is a prevalent condition that affects millions of\nindividuals worldwide, causing significant individual suffering and\nsocioeconomic burdens. Although exercise rehabilitation is a staple in\nrelieving pain and improving muscle function for the condition, traditional\none-on-one rehabilitation sessions are costly and suffer from poor adherence\nand accessibility for the patients. Thanks to the increasing accessibility and\nrecent advancements in sensing and display technology, virtual reality (VR)\noffers the potential to tackle the challenges in traditional exercise\nrehabilitation, particularly through gamification. However, still in its\ninfancy, VR-based neck exercise rehabilitation lacks exploration in effective\ngamification strategies and existing prototypes. To address the knowledge gap,\nwe conduct an exploratory study on the gamification strategies for VR-based\ncervical rehabilitation exercises by using chin tuck and neck range of motion\nexercises as examples. Specifically, with different game themes, we investigate\na survival and level progression strategy for muscle strengthening (chin tuck)\nexercise for the first time, and the suitability of ambient reward for a neck\nrange of motion exercise. Through a preliminary user study, we assess the\nproposed novel VR neck rehabilitation games and they demonstrate excellent\nusability, engagement, and perceived health value.", "AI": {"tldr": "This paper explores the use of virtual reality and gamification in neck rehabilitation exercises, addressing traditional rehabilitation challenges through innovative game strategies.", "motivation": "To improve accessibility and adherence to neck rehabilitation, which is often hampered by the high costs and inefficiencies of traditional methods.", "method": "An exploratory study investigating various gamification strategies for VR-based cervical rehabilitation through user engagement in chin tuck and neck range of motion exercises.", "result": "The proposed VR neck rehabilitation games showed excellent usability, engagement, and perceived health value based on preliminary user feedback.", "conclusion": "Gamified VR approaches have potential in enhancing neck rehabilitation effectiveness and user adherence, warranting further exploration in this emerging field.", "key_contributions": ["Investigated novel gamification strategies in VR for neck rehabilitation.", "Studied user engagement with specific exercises in a VR context.", "Demonstrated positive outcomes on usability and perceived health value."], "limitations": "The study is exploratory with preliminary findings; further research is needed to validate the effectiveness and scalability of the proposed strategies.", "keywords": ["Virtual Reality", "Cervical Rehabilitation", "Gamification", "User Engagement", "Health Informatics"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2508.18328", "pdf": "https://arxiv.org/pdf/2508.18328.pdf", "abs": "https://arxiv.org/abs/2508.18328", "title": "Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective", "authors": ["Masudul Hasan Masud Bhuiyan", "Matteo Varvello", "Yasir Zaki", "Cristian-Alexandru Staicu"], "categories": ["cs.CL", "cs.CY", "cs.NI"], "comment": "6 pages, 6 figures", "summary": "English is the predominant language on the web, powering nearly half of the\nworld's top ten million websites. Support for multilingual content is\nnevertheless growing, with many websites increasingly combining English with\nregional or native languages in both visible content and hidden metadata. This\nmultilingualism introduces significant barriers for users with visual\nimpairments, as assistive technologies like screen readers frequently lack\nrobust support for non-Latin scripts and misrender or mispronounce non-English\ntext, compounding accessibility challenges across diverse linguistic contexts.\nYet, large-scale studies of this issue have been limited by the lack of\ncomprehensive datasets on multilingual web content. To address this gap, we\nintroduce LangCrUX, the first large-scale dataset of 120,000 popular websites\nacross 12 languages that primarily use non-Latin scripts. Leveraging this\ndataset, we conduct a systematic analysis of multilingual web accessibility and\nuncover widespread neglect of accessibility hints. We find that these hints\noften fail to reflect the language diversity of visible content, reducing the\neffectiveness of screen readers and limiting web accessibility. We finally\npropose Kizuki, a language-aware automated accessibility testing extension to\naccount for the limited utility of language-inconsistent accessibility hints.", "AI": {"tldr": "This paper introduces LangCrUX, a large-scale dataset for analyzing multilingual web accessibility for visually impaired users, revealing significant neglect of accessibility hints in non-Latin scripts and proposing Kizuki, an automated testing tool for better accessibility.", "motivation": "To address the lack of comprehensive datasets on multilingual web content and the challenges faced by users with visual impairments due to inadequate support for non-Latin scripts.", "method": "The authors created the LangCrUX dataset, comprising 120,000 popular websites across 12 languages using non-Latin scripts, and performed a systematic analysis of multilingual web accessibility.", "result": "The analysis revealed widespread neglect of accessibility hints, which do not accurately reflect the language diversity of content, thus hampering the effectiveness of screen readers.", "conclusion": "The paper highlights the urgent need for better accessibility practices for multilingual web content, and introduces Kizuki as a solution to improve automated accessibility testing.", "key_contributions": ["Development of the LangCrUX dataset for multilingual web accessibility analysis", "Discovery of the shortcomings in accessibility hints for non-Latin scripts", "Introduction of Kizuki, a language-aware automated accessibility testing tool"], "limitations": "The dataset is limited to popular websites and may not represent all multilingual content accurately.", "keywords": ["multilingual web accessibility", "visual impairment", "non-Latin scripts", "accessibility hints", "automated testing"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2508.18591", "pdf": "https://arxiv.org/pdf/2508.18591.pdf", "abs": "https://arxiv.org/abs/2508.18591", "title": "Portable Silent Room: Exploring VR Design for Anxiety and Emotion Regulation for Neurodivergent Women and Non-Binary Individuals", "authors": ["Kinga Skiers", "Yun Suen Pai", "Marina Nakagawa", "Kouta Minamizawa", "Giulia Barbareschi"], "categories": ["cs.HC"], "comment": null, "summary": "Neurodivergent individuals, particularly those with Autism and Attention\nDeficit Hyperactivity Disorder (ADHD), frequently experience anxiety, panic\nattacks, meltdowns, and emotional dysregulation due to societal pressures and\ninadequate accommodations. These challenges are especially pronounced for\nneurodivergent women and non-binary individuals navigating intersecting\nbarriers of neurological differences and gender expectations. This research\ninvestigates virtual reality (VR) as a portable safe space for emotional\nregulation, addressing challenges of sensory overload and motion sickness while\nenhancing relaxation capabilities. Our mixed-methods approach included an\nonline survey (N=223) and an ideation workshop (N=32), which provided key\ndesign elements for creating effective calming VR environments. Based on these\nfindings, we developed and iteratively tested VR prototypes with neurodivergent\nwomen and non-binary participants (N=12), leading to a final version offering\nenhanced adaptability to individual sensory needs. This final prototype\nunderwent a comprehensive evaluation with 25 neurodivergent participants to\nassess its effectiveness as a regulatory tool. This research contributes to the\ndevelopment of inclusive, adaptive VR environments that function as\npersonalized \"portable silent rooms\" offering neurodivergent individuals\non-demand access to sensory regulation regardless of physical location.", "AI": {"tldr": "This research explores the use of virtual reality (VR) as a tool for emotional regulation among neurodivergent individuals, focusing on its effectiveness as a calming environment for managing anxiety and sensory overload.", "motivation": "Neurodivergent individuals face significant emotional challenges due to societal pressures and inadequate support, necessitating innovative solutions for emotional regulation.", "method": "The study employed a mixed-methods approach including an online survey with 223 participants and an ideation workshop with 32 participants, followed by the development and testing of VR prototypes with 12 neurodivergent women and non-binary participants.", "result": "The final VR prototype was tested by 25 neurodivergent participants, showing effectiveness in providing a personalized environment for sensory regulation and emotional comfort.", "conclusion": "This research advances the design of inclusive VR environments as 'portable silent rooms,' available for neurodivergent individuals to help manage sensory overload and emotional dysregulation in any location.", "key_contributions": ["Investigation of VR as a tool for emotional regulation in neurodivergent individuals.", "Development of adaptable VR environments based on user feedback.", "Testing of VR prototypes to create effective calming experiences."], "limitations": "", "keywords": ["virtual reality", "neurodivergence", "emotional regulation", "sensory overload", "inclusive design"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.18381", "pdf": "https://arxiv.org/pdf/2508.18381.pdf", "abs": "https://arxiv.org/abs/2508.18381", "title": "Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models", "authors": ["Yuchun Fan", "Yilin Wang", "Yongyu Mu", "Lei Huang", "Bei Li", "Xiaocheng Feng", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 findings", "summary": "Large vision-language models (LVLMs) have demonstrated exceptional\ncapabilities in understanding visual information with human languages but also\nexhibit an imbalance in multilingual capabilities. In this work, we delve into\nthe multilingual working pattern of LVLMs and identify a salient correlation\nbetween the multilingual understanding ability of LVLMs and language-specific\nneuron activations in shallow layers. Building on this insight, we introduce\nPLAST, a training recipe that achieves efficient multilingual enhancement for\nLVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies\nlayers involved in multilingual understanding by monitoring language-specific\nneuron activations. These layers are then precisely fine-tuned with\nquestion-translation pairs to achieve multilingual alignment. Our empirical\nresults on MM-Bench and MMMB demonstrate that PLAST effectively improves the\nmultilingual capabilities of LVLMs and achieves significant efficiency with\nonly 14% of the parameters tuned. Further analysis reveals that PLAST can be\ngeneralized to low-resource and complex visual reasoning tasks, facilitating\nthe language-specific visual information engagement in shallow layers.", "AI": {"tldr": "This paper introduces PLAST, a method for enhancing the multilingual capabilities of Large Vision-Language Models (LVLMs) through targeted fine-tuning of language-specific layers.", "motivation": "To address the imbalance in multilingual capabilities of LVLMs and improve their efficiency and understanding across multiple languages.", "method": "PLAST identifies layers in LVLMs that respond to multilingual understanding by monitoring neuron activations and fine-tunes these layers using question-translation pairs.", "result": "PLAST improves multilingual capabilities by tuning only 14% of the parameters, with empirical results showing significant enhancements on benchmark datasets MM-Bench and MMMB.", "conclusion": "PLAST provides an efficient means to enhance LVLM multilingual capabilities and can be extended to low-resource and complex reasoning tasks in visual contexts.", "key_contributions": ["Introduction of PLAST for efficient multilingual enhancement in LVLMs", "Identification of language-specific neuron activations in shallow layers", "Demonstration of effective empirical results on multilingual benchmarks"], "limitations": "", "keywords": ["vision-language models", "multilingual enhancement", "neuron activations"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.18640", "pdf": "https://arxiv.org/pdf/2508.18640.pdf", "abs": "https://arxiv.org/abs/2508.18640", "title": "Enhancing XAI Interpretation through a Reverse Mapping from Insights to Visualizations", "authors": ["Aniket Nuthalapati", "Nicholas Hinds", "Brian Y. Lim", "Qianwen Wang"], "categories": ["cs.HC"], "comment": "5 pages, 5 figures, accepted by IEEE VIS 2025", "summary": "As AI systems become increasingly integrated into high-stakes domains,\nenabling users to accurately interpret model behavior is critical. While AI\nexplanations can be provided, users often struggle to reason effectively with\nthese explanations, limiting their ability to validate or learn from AI\ndecisions. To address this gap, we introduce Reverse Mapping, a novel approach\nthat enhances visual explanations by incorporating user-derived insights back\ninto the explanation workflow. Our system extracts structured insights from\nfree-form user interpretations using a large language model and maps them back\nonto visual explanations through interactive annotations and coordinated\nmulti-view visualizations. Inspired by the verification loop in the\nvisualization knowledge generation model, this design aims to foster more\ndeliberate, reflective interaction with AI explanations. We demonstrate our\napproach in a prototype system with two use cases and qualitative user\nfeedback.", "AI": {"tldr": "The paper introduces Reverse Mapping, a novel approach that enhances visual AI explanations by integrating user-derived insights to improve understanding and interaction.", "motivation": "To improve users' ability to interpret and learn from AI model behavior in high-stakes domains.", "method": "The system uses a large language model to extract structured insights from user interpretations and maps these onto visual explanations through interactive annotations and multi-view visualizations.", "result": "Demonstrated effectiveness of the approach through a prototype system in two use cases, supported by qualitative user feedback indicating improved understanding of AI explanations.", "conclusion": "Reverse Mapping fosters more deliberate interaction with AI explanations, enhancing users' reasoning capabilities and learning.", "key_contributions": ["Introduces a novel Reverse Mapping approach for AI explanations.", "Integrates user insights into visual explanations for improved understanding.", "Supports interactive features that enhance user engagement with AI outputs."], "limitations": "", "keywords": ["AI explanations", "Human-Computer Interaction", "visualization"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.18384", "pdf": "https://arxiv.org/pdf/2508.18384.pdf", "abs": "https://arxiv.org/abs/2508.18384", "title": "Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails", "authors": ["Kellen Tan Cheng", "Anna Lisa Gentile", "Chad DeLuca", "Guang-Jie Ren"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The pervasiveness of large language models (LLMs) in enterprise settings has\nalso brought forth a significant amount of risks associated with their usage.\nGuardrails technologies aim to mitigate this risk by filtering LLMs'\ninput/output text through various detectors. However, developing and\nmaintaining robust detectors faces many challenges, one of which is the\ndifficulty in acquiring production-quality labeled data on real LLM outputs\nprior to deployment. In this work, we propose backprompting, a simple yet\nintuitive solution to generate production-like labeled data for health advice\nguardrails development. Furthermore, we pair our backprompting method with a\nsparse human-in-the-loop clustering technique to label the generated data. Our\naim is to construct a parallel corpus roughly representative of the original\ndataset yet resembling real LLM output. We then infuse existing datasets with\nour synthetic examples to produce robust training data for our detector. We\ntest our technique in one of the most difficult and nuanced guardrails: the\nidentification of health advice in LLM output, and demonstrate improvement\nversus other solutions. Our detector is able to outperform GPT-4o by up to\n3.73%, despite having 400x less parameters.", "AI": {"tldr": "This paper introduces backprompting as a method to generate production-like labeled data for health advice guardrail development in LLMs, and demonstrates its efficacy in improving detection performance.", "motivation": "The increasing use of LLMs in enterprise settings has raised significant risks, necessitating robust guardrails to filter outputs, which is hindered by the challenge of acquiring quality labeled data.", "method": "The authors propose backprompting to create production-like labeled data, and incorporate a sparse human-in-the-loop clustering technique to label this synthetic data, enhancing the training of their detector.", "result": "The proposed method improves detection accuracy for health advice in LLM outputs, outperforming GPT-4o by up to 3.73% with significantly fewer parameters (400x less).", "conclusion": "Backprompting, combined with a novel labeling technique, offers an effective solution for generating quality labeled data that improves guardrails for LLMs in health-related applications.", "key_contributions": ["Introduction of backprompting for generating labeled data", "Implementation of a sparse human-in-the-loop clustering technique", "Demonstration of enhanced performance in health advice detection"], "limitations": "", "keywords": ["large language models", "guardrails", "health informatics", "backprompting", "synthetic data"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.18670", "pdf": "https://arxiv.org/pdf/2508.18670.pdf", "abs": "https://arxiv.org/abs/2508.18670", "title": "RÉCITKIT: A Spatial Toolkit for Designing and Evaluating Human-Centered Immersive Data Narratives", "authors": ["Vidya Setlur", "Samuel Ridet"], "categories": ["cs.HC"], "comment": "5 pages, 3 figures", "summary": "Spatial computing presents new opportunities for immersive data storytelling,\nyet there is limited guidance on how to build such experiences or adapt\ntraditional narrative visualizations to this medium. We introduce a toolkit,\nR\\'ECITKIT for supporting spatial data narratives in head-mounted display (HMD)\nenvironments. The toolkit allows developers to create interactive dashboards,\ntag data attributes as spatial assets to 3D models and immersive scenes,\ngenerate text and audio narratives, enabling dynamic filtering, and\nhierarchical drill-down data discoverability. To demonstrate the utility of the\ntoolkit, we developed Charles Minard's historical flow map of Napoleon's 1812\ncampaign in Russia as an immersive experience on Apple Vision Pro. We conducted\na preliminary evaluation with 21 participants that comprised two groups:\ndevelopers, who evaluated the toolkit by authoring spatial stories and\nconsumers, who provided feedback on the Minard app's narrative clarity,\ninteraction design, and engagement. Feedback highlighted how spatial\ninteractions and guided narration enhanced insight formation, with participants\nemphasizing the benefits of physical manipulation (e.g., gaze, pinch,\nnavigation) for understanding temporal and geographic data. Participants also\nidentified opportunities for future enhancement, including improved interaction\naffordance visibility, customizable storytelling logic, and integration of\ncontextual assets to support user orientation. These findings contribute to the\nbroader discourse on toolkit-driven approaches to immersive data storytelling\nacross domains such as education, decision support, and exploratory analytics.", "AI": {"tldr": "Introduction of R'ÉCITKIT, a toolkit for creating spatial data narratives in immersive environments.", "motivation": "To provide guidance on building immersive data storytelling experiences using spatial computing.", "method": "Development of the R'ÉCITKIT toolkit and a case study using Minard's historical flow map of Napoleon's campaign.", "result": "Preliminary evaluations showed enhanced insight formation through spatial interactions, with positive feedback on narrative clarity and engagement.", "conclusion": "The findings suggest potential for improving interaction design in immersive storytelling and highlight areas for future enhancements.", "key_contributions": ["Introduction of R'ÉCITKIT toolkit for spatial data narratives", "Evaluation using a historical case study", "Insights into the effectiveness of spatial interactions for data storytelling"], "limitations": "Limited to a preliminary evaluation of 21 participants; feedback may not represent larger populations.", "keywords": ["Spatial Computing", "Data Storytelling", "Immersive Experiences", "Toolkit", "User Evaluation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.18387", "pdf": "https://arxiv.org/pdf/2508.18387.pdf", "abs": "https://arxiv.org/abs/2508.18387", "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little", "authors": ["Ivan Kobyzev", "Abbas Ghaddar", "Dingtao Hu", "Boxing Chen"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Softmax self-attention often assigns disproportionate weight to semantically\nuninformative tokens such as special tokens and punctuation, a phenomenon known\nas attention noise. While recent methods like Cog Attention and the\nDifferential Transformer have addressed this by introducing negative attention\nscores, they risk discarding useful information. In this paper, we propose the\nIntegral Transformer, a novel self-attention mechanism that denoises attention\nby integrating signals sampled from the logit distribution. Our approach\nmitigates noise while preserving the contributions of special tokens critical\nfor model performance. Extensive experiments demonstrate that our model\noutperforms vanilla, Cog, and Differential attention variants on\nwell-established knowledge and reasoning language benchmarks. Moreover, our\nanalysis reveals that employing vanilla self-attention in the lower Transformer\nlayers enhances performance and that the Integral Transformer effectively\nbalances attention distributions and reduces rank collapse in upper layers.", "AI": {"tldr": "The Integral Transformer is a novel self-attention mechanism designed to reduce attention noise caused by semantically uninformative tokens while preserving useful information.", "motivation": "Address the challenge of attention noise in self-attention mechanisms, which disproportionately weights uninformative tokens, and the limitations of existing methods.", "method": "The Integral Transformer integrates signals from the logit distribution to denoise attention, mitigating noise while preserving important contributions from special tokens.", "result": "The Integral Transformer outperforms existing attention variants (vanilla, Cog, Differential attention) on standard benchmarks, enhancing performance by balancing attention distributions and reducing issues like rank collapse.", "conclusion": "The Integral Transformer offers a significant improvement over traditional self-attention mechanisms, especially in lower Transformer layers, contributing positively to model performance.", "key_contributions": ["Introduction of the Integral Transformer self-attention mechanism", "Demonstrated improvements over existing attention methods", "Revealed effective utilization of vanilla self-attention in lower layers."], "limitations": "", "keywords": ["self-attention", "Integral Transformer", "attention noise", "language benchmarks", "machine learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.18782", "pdf": "https://arxiv.org/pdf/2508.18782.pdf", "abs": "https://arxiv.org/abs/2508.18782", "title": "Long-Term Variability in Physiological-Arousal Relationships for Robust Emotion Estimation", "authors": ["Hiroto Sakimura", "Takayuki Nagaya", "Tomoki Nishi", "Tetsuo Kurahashi", "Katsunori Kohda", "Nobuhiko Muramoto"], "categories": ["cs.HC", "cs.AI"], "comment": "9 pages, 5 figures, accepted at 13th International Conference on\n  Affective Computing and Intelligent Interaction (ACII 2025)", "summary": "Estimating emotional states from physiological signals is a central topic in\naffective computing and psychophysiology. While many emotion estimation systems\nimplicitly assume a stable relationship between physiological features and\nsubjective affect, this assumption has rarely been tested over long timeframes.\nThis study investigates whether such relationships remain consistent across\nseveral months within individuals. We developed a custom measurement system and\nconstructed a longitudinal dataset by collecting physiological signals --\nincluding blood volume pulse, electrodermal activity (EDA), skin temperature,\nand acceleration--along with self-reported emotional states from 24\nparticipants over two three-month periods. Data were collected in naturalistic\nworking environments, allowing analysis of the relationship between\nphysiological features and subjective arousal in everyday contexts. We examined\nhow physiological-arousal relationships evolve over time by using Explainable\nBoosting Machines (EBMs) to ensure model interpretability. A model trained on\n1st-period data showed a 5\\% decrease in accuracy when tested on 2nd-period\ndata, indicating long-term variability in physiological-arousal associations.\nEBM-based comparisons further revealed that while heart rate remained a\nrelatively stable predictor, minimum EDA exhibited substantial individual-level\nfluctuations between periods. While the number of participants is limited,\nthese findings highlight the need to account for temporal variability in\nphysiological-arousal relationships and suggest that emotion estimation models\nshould be periodically updated -- e.g., every five months -- based on observed\nshift trends to maintain robust performance over time.", "AI": {"tldr": "This study explores the stability of physiological signals in estimating emotional states over long periods, indicating a need for periodic updates to emotion estimation models.", "motivation": "To investigate the assumption that relationships between physiological features and subjective affect are stable over long timeframes, which has rarely been tested.", "method": "A longitudinal dataset was created by collecting physiological signals and self-reported emotional states from 24 participants over two three-month periods, using Explainable Boosting Machines for analysis.", "result": "A model trained on the first period data showed a 5% decrease in accuracy when tested on the second period data, indicating long-term variability in relationships. Heart rate was stable while EDA varied significantly among individuals.", "conclusion": "Emotion estimation models should be periodically updated every five months to account for changes in physiological-arousal relationships for better performance.", "key_contributions": ["Developed a longitudinal dataset for studying physiological signals and emotional states.", "Demonstrated the long-term variability of physiological-arousal relationships.", "Highlighted the need for periodic model updates in emotion estimation."], "limitations": "Limited sample size of 24 participants.", "keywords": ["emotion estimation", "physiological signals", "longitudinal study", "affect", "Explainable Boosting Machines"], "importance_score": 7, "read_time_minutes": 9}}
{"id": "2508.18395", "pdf": "https://arxiv.org/pdf/2508.18395.pdf", "abs": "https://arxiv.org/abs/2508.18395", "title": "Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning", "authors": ["Jeong-seok Oh", "Jay-yoon Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Probabilistic decoding in Large Language Models (LLMs) often yields\ninconsistent outputs, particularly on complex or long-form questions.\nSelf-Consistency (SC) mitigates this for short-form QA by majority voting over\nexact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram\nConsistency Score (WUCS) extend to long-form responses but lose accuracy on\nshort-form benchmarks.\n  We introduce Latent Self-Consistency (LSC), which selects the most\nsemantically consistent response using learnable token embeddings. A\nlightweight forward generation of summary tokens increases inference time by\nless than 1% and requires no changes to the model architecture.\n  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,\nTruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form\nones on average, while maintaining negligible computational overhead. These\nresults position LSC as a practical consistency-selection method that works\nreliably across answer formats. Additionally, LSC provides well-calibrated\nconfidence estimates, maintaining low Expected Calibration Error across both\nanswer formats.", "AI": {"tldr": "Latent Self-Consistency (LSC) improves the consistency of responses in Large Language Models (LLMs) for both short and long-form questions by selecting semantically consistent answers using learnable token embeddings.", "motivation": "To address the inconsistent outputs of LLMs, especially for complex or long-form questions, and to enhance the accuracy of responses in QA tasks.", "method": "Introducing Latent Self-Consistency (LSC), which utilizes learnable token embeddings to select the most semantically consistent responses, with minimal impact on inference time and no changes needed to the model architecture.", "result": "LSC outperforms existing methods like Self-Consistency (SC), Universal Self-Consistency (USC), and Weighted Unigram Consistency Score (WUCS) across multiple short-form and long-form reasoning benchmarks, with negligible computational overhead.", "conclusion": "LSC is positioned as a practical method for ensuring response consistency in LLMs across various formats while providing well-calibrated confidence estimates that minimize Expected Calibration Error.", "key_contributions": ["Introduction of Latent Self-Consistency (LSC) for improving response consistency in LLMs.", "Demonstration of LSC's superiority over SC, USC, and WUCS across diverse reasoning benchmarks.", "Provision of well-calibrated confidence estimates with low Expected Calibration Error."], "limitations": "", "keywords": ["Latent Self-Consistency", "Large Language Models", "response consistency", "semantic embedding", "confidence calibration"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.18784", "pdf": "https://arxiv.org/pdf/2508.18784.pdf", "abs": "https://arxiv.org/abs/2508.18784", "title": "Insights into User Interface Innovations from a Design Thinking Workshop at deRSE25", "authors": ["Maximilian Frank", "Simon Lund"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Large Language Models have become widely adopted tools due to their versatile\ncapabilities, yet their user interfaces remain limited, often following rigid,\nlinear interaction paradigms. In this paper, we present insights from a design\nthinking workshop held at the deRSE25 conference aiming at collaboratively\ndeveloping innovative user interface concepts for LLMs. During the workshop,\nparticipants identified common use cases, evaluated the strengths and\nshortcomings of current LLM interfaces, and created visualizations of new\ninteraction concepts emphasizing flexible context management, dynamic\nconversation branching, and enhanced mechanisms for user control. We describe\nhow these participant-generated ideas advanced our own whiteboard-based UI\napproach. The ongoing development of this interface is guided by the\nhuman-centered design process - an iterative, user-focused methodology that\nemphasizes continuous refinement through user feedback. Broader implications\nfor future LLM interface development are discussed, advocating for increased\nattention to UI innovation grounded in user-centered design principles.", "AI": {"tldr": "The paper discusses innovative user interface concepts for Large Language Models (LLMs) developed during a design workshop, aiming to improve user interaction through flexible, user-centered designs.", "motivation": "To address the limitations of current user interfaces for LLMs, which often follow rigid interaction paradigms.", "method": "Insights were gathered from a design thinking workshop where participants evaluated existing LLM interfaces and generated new concepts focusing on flexible context management and user control.", "result": "The workshop led to the creation of new UI interaction concepts that emphasized dynamic conversation branching and user-centered design.", "conclusion": "The ongoing development of LLM interfaces should prioritize innovative UI grounded in user-centered design principles, advocating for continuous refinement through user feedback.", "key_contributions": ["New interaction concepts for LLMs identified by workshop participants", "Emphasis on flexible context management and dynamic conversation branching", "Guidance from a human-centered design process for UI development"], "limitations": "", "keywords": ["Large Language Models", "User Interface", "Design Thinking", "Human-Centered Design", "Interaction Concepts"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.18407", "pdf": "https://arxiv.org/pdf/2508.18407.pdf", "abs": "https://arxiv.org/abs/2508.18407", "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering", "authors": ["Michal Štefánik", "Timothee Mickus", "Marek Kadlčík", "Michal Spiegel", "Josef Kuchař"], "categories": ["cs.CL", "cs.AI", "68T01, 68T07, 68T50", "I.2"], "comment": "To appear in Findings of EMNLP 2025", "summary": "A majority of recent work in AI assesses models' generalization capabilities\nthrough the lens of performance on out-of-distribution (OOD) datasets. Despite\ntheir practicality, such evaluations build upon a strong assumption: that OOD\nevaluations can capture and reflect upon possible failures in a real-world\ndeployment.\n  In this work, we challenge this assumption and confront the results obtained\nfrom OOD evaluations with a set of specific failure modes documented in\nexisting question-answering (QA) models, referred to as a reliance on spurious\nfeatures or prediction shortcuts.\n  We find that different datasets used for OOD evaluations in QA provide an\nestimate of models' robustness to shortcuts that have a vastly different\nquality, some largely under-performing even a simple, in-distribution\nevaluation. We partially attribute this to the observation that spurious\nshortcuts are shared across ID+OOD datasets, but also find cases where a\ndataset's quality for training and evaluation is largely disconnected. Our work\nunderlines limitations of commonly-used OOD-based evaluations of\ngeneralization, and provides methodology and recommendations for evaluating\ngeneralization within and beyond QA more robustly.", "AI": {"tldr": "This paper critiques the reliability of out-of-distribution (OOD) evaluations in AI, specifically in question-answering models, highlighting the discrepancies between OOD datasets and real-world deployment failures due to reliance on spurious features.", "motivation": "To challenge the assumption that OOD evaluations accurately reflect model performance and robustness in real-world deployments.", "method": "The authors analyze the relationship between OOD evaluation datasets and specific documented failure modes in QA models, focusing on the reliance on spurious features or prediction shortcuts.", "result": "Findings indicate that OOD datasets can significantly underperform relative to in-distribution evaluations, with varying qualities in their ability to capture robustness to shortcuts.", "conclusion": "The study emphasizes the limitations of current OOD evaluations for generalization assessment and proposes improved methodologies for more robust evaluation.", "key_contributions": ["Critique of the validity of OOD evaluations for AI models.", "Identification of quality disparities among OOD datasets in capturing model robustness.", "Recommendations for more reliable evaluation methods beyond QA."], "limitations": "The analysis is limited to specific QA models and may not generalize to all AI applications.", "keywords": ["out-of-distribution", "generalization", "question-answering", "spurious features", "model robustness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.18875", "pdf": "https://arxiv.org/pdf/2508.18875.pdf", "abs": "https://arxiv.org/abs/2508.18875", "title": "PRIMMDebug: A Debugging Teaching Aid For Secondary Students", "authors": ["Laurie Gale", "Sue Sentance"], "categories": ["cs.HC"], "comment": "12 pages, 8 figures", "summary": "Debugging is often a challenging and infuriating experience for secondary\nschool students learning their first text-based programming language. Many\nstudents resort to ineffective debugging strategies, making success with\nsolving errors unlikely and emotional distress common. Developing tools that\nencourage students to adopt a more systematic and reflective approach to\ndebugging is therefore an important, but lacking, area of research. This paper\npresents PRIMMDebug, a debugging teaching aid for secondary school students\nlearning text-based programming. The aid consists of an online tool that takes\nstudents through the steps of a systematic debugging process based on PRIMM, a\nframework for teaching programming. The tool promotes a reflective approach to\ndebugging by heavily encouraging students to articulate their thoughts\nthroughout the PRIMMDebug process while simultaneously limiting their ability\nto run and edit code. To evaluate the tool, a set of students from four\nsecondary schools were taught with PRIMMDebug over several lessons. Survey\nresults and log data analysis show that students were generally reluctant to\nengage with the systematicity and reflection that the tool encourages. Given\nthat related work on systematic debugging has reported similar challenges, we\nend by considering how these approaches could be refined to help more students\nbenefit from them.", "AI": {"tldr": "The paper presents PRIMMDebug, a debugging tool designed for secondary school students learning text-based programming, promoting systematic and reflective debugging practices.", "motivation": "To address challenges faced by secondary school students in effectively debugging their programming code, which often leads to emotional distress and ineffective strategies.", "method": "PRIMMDebug is an online tool that guides students through a systematic debugging process based on the PRIMM framework, encouraging them to articulate their thoughts while limiting code editing and execution.", "result": "Students showed reluctance in engaging with the systematic and reflective practices encouraged by the tool, as evidenced by survey results and log data analysis from multiple schools.", "conclusion": "Refinements to approaches in systematic debugging are needed to better support student engagement and learning outcomes in debugging.", "key_contributions": ["Introduction of PRIMMDebug as a structured debugging aid for education.", "Empirical evaluation of the effectiveness and student engagement with the tool.", "Identification of challenges in promoting systematic debugging practices among students."], "limitations": "Students' reluctance to engage with the reflective practices of the tool may limit its effectiveness.", "keywords": ["debugging", "education", "systematic approaches", "programming", "HCI"], "importance_score": 3, "read_time_minutes": 12}}
{"id": "2508.18444", "pdf": "https://arxiv.org/pdf/2508.18444.pdf", "abs": "https://arxiv.org/abs/2508.18444", "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?", "authors": ["Nafis Tanveer Islam", "Zhiming Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at FQAS Conference 2024. DOI will be provided in 3 weeks\n  after the conference has published the paper", "summary": "With the improving semantic understanding capability of Large Language Models\n(LLMs), they exhibit a greater awareness and alignment with human values, but\nthis comes at the cost of transparency. Although promising results are achieved\nvia experimental analysis, an in-depth understanding of the LLM's internal\nworkings is unavoidable to comprehend the reasoning behind the re-ranking,\nwhich provides end users with an explanation that enables them to make an\ninformed decision. Moreover, in newly developed systems with limited user\nengagement and insufficient ranking data, accurately re-ranking content remains\na significant challenge. While various training methods affect the training of\nLLMs and generate inference, our analysis has found that some training methods\nexhibit better explainability than others, implying that an accurate semantic\nunderstanding has not been learned through all training methods; instead,\nabstract knowledge has been gained to optimize evaluation, which raises\nquestions about the true reliability of LLMs. Therefore, in this work, we\nanalyze how different training methods affect the semantic understanding of the\nre-ranking task in LLMs and investigate whether these models can generate more\ninformed textual reasoning to overcome the challenges of transparency or LLMs\nand limited training data. To analyze the LLMs for re-ranking tasks, we utilize\na relatively small ranking dataset from the environment and the Earth science\ndomain to re-rank retrieved content. Furthermore, we also analyze the\nexplainable information to see if the re-ranking can be reasoned using\nexplainability.", "AI": {"tldr": "This paper analyzes the impact of different training methods on the semantic understanding and explainability of Large Language Models (LLMs) in the re-ranking task, focusing on challenges posed by limited user engagement and transparency.", "motivation": "To understand how training methods affect the ability of LLMs to provide reliable re-ranking of content while ensuring transparency and explainability.", "method": "The study utilizes a small ranking dataset from the Earth science domain to evaluate how different training methods influence the performance of LLMs in re-ranking tasks, and investigates the explainability of the reasoning behind the re-ranking.", "result": "The analysis reveals that certain training methods yield better explainability, highlighting inconsistencies in how LLMs acquire semantic understanding and raise questions about their reliability.", "conclusion": "The findings suggest that improve transparency and informed decision-making can be achieved through optimized training methods that enhance the explainability of LLMs in re-ranking applications.", "key_contributions": ["Analyzed the performance of LLMs in re-ranking tasks based on different training methods.", "Investigated the relationship between training methods and semantic understanding in LLMs.", "Assessed the explainability of LLM outputs in the context of limited training data."], "limitations": "The study is based on a relatively small ranking dataset, which may limit the generalizability of the findings.", "keywords": ["Large Language Models", "re-ranking", "explainability", "training methods", "semantic understanding"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.18918", "pdf": "https://arxiv.org/pdf/2508.18918.pdf", "abs": "https://arxiv.org/abs/2508.18918", "title": "DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM with Audio Modality", "authors": ["Youngwon Choi", "Donghyuk Jung", "Hwayeon Kim"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "2 pages, 2 figures. Accepted for presentation as a UIST 2025 Poster", "summary": "We present DESAMO, an on-device smart home system for elder-friendly use\npowered by Audio LLM, that supports natural and private interactions. While\nconventional voice assistants rely on ASR-based pipelines or ASR-LLM cascades,\noften struggling with the unclear speech common among elderly users and unable\nto handle non-speech audio, DESAMO leverages an Audio LLM to process raw audio\ninput directly, enabling a robust understanding of user intent and critical\nevents, such as falls or calls for help.", "AI": {"tldr": "DESAMO is an on-device smart home system that utilizes an Audio LLM to facilitate elder-friendly interactions, overcoming challenges in understanding unclear speech and non-speech audio.", "motivation": "To improve the usability of smart home systems for elderly users who may face difficulties with traditional voice assistants.", "method": "The system directly processes raw audio input using an Audio LLM, bypassing conventional ASR pipelines.", "result": "DESAMO demonstrates a robust capability in understanding user intent and detecting critical audio events, such as falls or distress calls, more effectively than traditional systems.", "conclusion": "The adoption of the Audio LLM allows for more natural and private interactions in smart home environments, particularly for the elderly.", "key_contributions": ["Introduction of DESAMO, a novel elder-friendly smart home system", "Utilization of an Audio LLM for direct audio processing", "Enhanced detection of critical events related to user safety"], "limitations": "", "keywords": ["Elderly care", "Smart home", "Audio LLM", "Natural interaction", "User intent"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.18466", "pdf": "https://arxiv.org/pdf/2508.18466.pdf", "abs": "https://arxiv.org/abs/2508.18466", "title": "Integrating gender inclusivity into large language models via instruction tuning", "authors": ["Alina Wróblewska", "Bartosz Żuk"], "categories": ["cs.CL"], "comment": null, "summary": "Imagine a language with masculine, feminine, and neuter grammatical genders,\nyet, due to historical and political conventions, masculine forms are\npredominantly used to refer to men, women and mixed-gender groups. This is the\nreality of contemporary Polish. A social consequence of this unfair linguistic\nsystem is that large language models (LLMs) trained on Polish texts inherit and\nreinforce this masculine bias, generating gender-imbalanced outputs. This study\naddresses this issue by tuning LLMs using the IPIS dataset, a collection of\nhuman-crafted gender-inclusive proofreading in Polish and Polish-to-English\ntranslation instructions. Grounded in a theoretical linguistic framework, we\ndesign a system prompt with explicit gender-inclusive guidelines for Polish. In\nour experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and\nMistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to\nintegrate gender inclusivity as an inherent feature of these models, offering a\nsystematic solution to mitigate gender bias in Polish language generation.", "AI": {"tldr": "This study addresses gender bias in Polish language models by tuning them with gender-inclusive guidelines and the IPIS dataset.", "motivation": "The masculine bias in Polish grammar affects LLM outputs, resulting in gender-imbalanced text generation.", "method": "The authors developed a system prompt with gender-inclusive guidelines and fine-tuned several multilingual and Polish-specific LLMs using the IPIS dataset.", "result": "The tuned models demonstrate improved gender inclusivity in their outputs compared to standard models.", "conclusion": "Integrating gender inclusivity into LLMs is feasible and can systematically reduce gender bias in Polish language generation.", "key_contributions": ["Introduction of the IPIS dataset for gender-inclusive instructions in Polish", "Development of a systematic prompt design for gender inclusivity", "Demonstration of improved outputs in tuned LLMs regarding gender bias"], "limitations": "The study may be limited by the specificity of the Polish language context and the need for broader datasets for other languages.", "keywords": ["gender bias", "language models", "Polish", "gender inclusivity", "LLMs"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.18919", "pdf": "https://arxiv.org/pdf/2508.18919.pdf", "abs": "https://arxiv.org/abs/2508.18919", "title": "Impact Assessment Card: Communicating Risks and Benefits of AI Uses", "authors": ["Edyta Bogucka", "Marios Constantinides", "Sanja Šćepanović", "Daniele Quercia"], "categories": ["cs.HC", "K.4.1, K.4.2, H.5.3, D.2.9", "K.4.1; K.4.2; H.5.3; D.2.9"], "comment": "42 pages, 14 figures", "summary": "Communicating the risks and benefits of AI is important for regulation and\npublic understanding. Yet current methods such as technical reports often\nexclude people without technical expertise. Drawing on HCI research, we\ndeveloped an Impact Assessment Card to present this information more clearly.\nWe held three focus groups with a total of 12 participants who helped identify\ndesign requirements and create early versions of the card. We then tested a\nrefined version in an online study with 235 participants, including AI\ndevelopers, compliance experts, and members of the public selected to reflect\nthe U.S. population by age, sex, and race. Participants used either the card or\na full impact assessment report to write an email supporting or opposing a\nproposed AI system. The card led to faster task completion and higher-quality\nemails across all groups. We discuss how design choices can improve\naccessibility and support AI governance. Examples of cards are available at:\nhttps://social-dynamics.net/ai-risks/impact-card/.", "AI": {"tldr": "The paper presents the Impact Assessment Card, a tool designed to communicate the risks and benefits of AI more clearly to non-experts, improving task performance and email quality in evaluations of AI systems.", "motivation": "Current methods for communicating AI risks often exclude non-technical individuals, hindering public understanding and governance.", "method": "Development of the Impact Assessment Card through HCI research, followed by focus group discussions and an online study comparing the card's effectiveness with traditional reports.", "result": "Participants who used the Impact Assessment Card completed tasks faster and submitted higher-quality emails than those using standard impact assessment reports.", "conclusion": "Design choices in information presentation can enhance accessibility and support effective AI governance.", "key_contributions": ["Introduction of the Impact Assessment Card as a novel communication tool for AI risks", "Empirical evidence showing improved task performance and email quality using the card", "Insights on design requirements for communicating technical information to non-experts."], "limitations": "Limited to U.S. population sample; may not generalize to other cultural contexts.", "keywords": ["Human-Computer Interaction", "AI Governance", "Impact Assessment", "Communication Tools", "Public Understanding"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.18473", "pdf": "https://arxiv.org/pdf/2508.18473.pdf", "abs": "https://arxiv.org/abs/2508.18473", "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing", "authors": ["Jiawei Li", "Akshayaa Magesh", "Venugopal V. Veeravalli"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages", "summary": "While Large Language Models (LLMs) have emerged as powerful foundational\nmodels to solve a variety of tasks, they have also been shown to be prone to\nhallucinations, i.e., generating responses that sound confident but are\nactually incorrect or even nonsensical. In this work, we formulate the problem\nof detecting hallucinations as a hypothesis testing problem and draw parallels\nto the problem of out-of-distribution detection in machine learning models. We\npropose a multiple-testing-inspired method to solve the hallucination detection\nproblem, and provide extensive experimental results to validate the robustness\nof our approach against state-of-the-art methods.", "AI": {"tldr": "This paper presents a method for detecting hallucinations in Large Language Models (LLMs) by reformulating the problem as hypothesis testing and comparing it to out-of-distribution detection in ML models.", "motivation": "The increasing reliance on LLMs for various applications raises concerns about their tendency to produce confident yet incorrect outputs, known as hallucinations.", "method": "The authors propose a hypothesis testing framework for hallucination detection inspired by multiple-testing principles, including experimental validation against current methods.", "result": "Extensive experiments demonstrate that the proposed method is robust and outperforms state-of-the-art hallucination detection methods.", "conclusion": "The proposed method provides a reliable means to detect hallucinations in LLMs, potentially improving the safety and reliability of systems that utilize these models.", "key_contributions": ["Formulation of hallucination detection as a hypothesis testing problem", "Introduction of a robust method inspired by multiple-testing", "Validation against existing state-of-the-art methods"], "limitations": "The effectiveness might vary across different types of LLMs and use cases; further validation could be required.", "keywords": ["Large Language Models", "hallucination detection", "hypothesis testing", "out-of-distribution detection", "machine learning"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2508.19121", "pdf": "https://arxiv.org/pdf/2508.19121.pdf", "abs": "https://arxiv.org/abs/2508.19121", "title": "Reading minds on the road: decoding perceived risk in automated vehicles through 140K+ ratings", "authors": ["Xiaolin He", "Zirui Li", "Xinwei Wang", "Riender Happee", "Meng Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Perceived risk in automated vehicles (AVs) can create the very danger that\nautomation is meant to prevent: a frightened rider may hesitate when seconds\nmatter, misjudge hazards, or disengage. However, measuring how perceived risk\nevolves in real time during driving remains challenging, leaving a gap in\ndecoding such hidden psychological states. Here, we present a novel method to\ntime-continuously measure and decode perceived risk. We conducted a controlled\nexperiment where 2,164 participants viewed high-fidelity videos of common\nhighway driving scenes and provided 141,628 discrete safety ratings. Through\ncontinuous-signal reconstruction of the discrete ratings, we obtained 236 hours\nof time-continuous perceived risk data - the largest perceived risk dataset to\ndate. Leveraging this dataset, we trained deep neural networks that predict\nmoment-by-moment perceived risk from vehicle kinematics with a mean relative\nerror below $3\\%$. Explainable AI analysis uncovers which factors determine\nperceived risk in real time. Our findings demonstrate a new paradigm for\nquantifying dynamic passenger experience and psychological constructs in real\ntime. These findings can guide the design of AVs and other machines that\noperate in close proximity to people, adjusting behaviour before trust erodes,\nand help realise automation's benefits in transport, healthcare, and service\nrobotics.", "AI": {"tldr": "This paper presents a novel method to measure and decode perceived risk in automated vehicles in real time, using a large dataset and deep neural networks.", "motivation": "Measuring perceived risk in automated vehicles (AVs) is crucial as it influences rider behavior and safety. Understanding how this perception evolves in real time can help in the design of safer AVs.", "method": "Conducted an experiment with 2,164 participants who provided safety ratings while watching driving scenarios. Used continuous-signal reconstruction to create time-continuous perceived risk data, and trained deep neural networks to predict perceived risk based on vehicle kinematics.", "result": "Achieved a mean relative error below 3% in predicting moment-by-moment perceived risk, uncovering real-time factors influencing this perception.", "conclusion": "The study introduces a new method for quantifying dynamic passenger experiences, which can inform the design of automated systems across various domains, ensuring safety and trust.", "key_contributions": ["Largest dataset of perceived risk from AV interactions", "Successful implementation of deep learning for real-time risk prediction", "Insights into factors influencing perceived risk to enhance AV design"], "limitations": "", "keywords": ["automated vehicles", "perceived risk", "real-time measurement", "deep neural networks", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.18549", "pdf": "https://arxiv.org/pdf/2508.18549.pdf", "abs": "https://arxiv.org/abs/2508.18549", "title": "COMET-poly: Machine Translation Metric Grounded in Other Candidates", "authors": ["Maike Züfle", "Vilém Zouhar", "Tu Anh Dinh", "Felipe Maia Polo", "Jan Niehues", "Mrinmaya Sachan"], "categories": ["cs.CL", "I.2.7"], "comment": "Maike Z\\\"ufle, Vil\\'em Zouhar, and Tu Anh Dinh contributed equally", "summary": "Automated metrics for machine translation attempt to replicate human\njudgment. Unlike humans, who often assess a translation in the context of\nmultiple alternatives, these metrics typically consider only the source\nsentence and a single translation. This discrepancy in the evaluation setup may\nnegatively impact the performance of automated metrics. We propose two\nautomated metrics that incorporate additional information beyond the single\ntranslation. COMET-polycand uses alternative translations of the same source\nsentence to compare and contrast with the translation at hand, thereby\nproviding a more informed assessment of its quality. COMET-polyic, inspired by\nretrieval-based in-context learning, takes in translations of similar source\ntexts along with their human-labeled quality scores to guide the evaluation. We\nfind that including a single additional translation in COMET-polycand improves\nthe segment-level metric performance (0.079 to 0.118 Kendall's tau-b\ncorrelation), with further gains when more translations are added.\nIncorporating retrieved examples in COMET-polyic yields similar improvements\n(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.", "AI": {"tldr": "This paper presents two novel metrics for evaluating machine translation that leverage additional information beyond single translations, aiming to enhance automated evaluation performance.", "motivation": "To address the shortcomings of automated machine translation metrics that do not consider the context of multiple alternatives, which may lead to less accurate assessments.", "method": "The paper introduces two metrics: COMET-polycand, which compares a given translation with alternative translations of the same source, and COMET-polyic, which uses translations of similar source texts with associated quality scores for evaluation.", "result": "Combining additional translations in COMET-polycand improved segment-level performance from 0.079 to 0.118 in Kendall's tau-b correlation, while COMET-polyic achieved a similar improvement from 0.079 to 0.116.", "conclusion": "The proposed metrics demonstrate significant improvements in machine translation evaluation by incorporating context from multiple translations, and the models are made publicly available.", "key_contributions": ["Introduction of COMET-polycand and COMET-polyic for improved machine translation evaluation", "Demonstration of performance improvements using additional translation context", "Public release of the models for broader use."], "limitations": "", "keywords": ["Machine Translation", "Evaluation Metrics", "COMET", "Natural Language Processing", "Human Judgment"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.19230", "pdf": "https://arxiv.org/pdf/2508.19230.pdf", "abs": "https://arxiv.org/abs/2508.19230", "title": "Beyond Competitive Gaming: How Casual Players Evaluate and Respond to Teammate Performance", "authors": ["Kaushall Senthil Nathan", "Jieun Lee", "Derrick M. Wang", "Geneva M. Smith", "Eugene Kukshinov", "Daniel Harley", "Lennart E. Nacke"], "categories": ["cs.HC"], "comment": "7 pages, 1 figure, CHI Play 2025 Conference", "summary": "Teammate performance evaluation fundamentally shapes intervention design in\nvideo games. However, our current understanding stems primarily from\ncompetitive E-Sports contexts where individual performance directly impacts\noutcomes. This research addresses whether performance evaluation mechanisms and\nbehavioural responses identified in competitive games generalize to casual\ncooperative games. We investigated how casual players evaluate teammate\ncompetence and respond behaviourally in a controlled between-subjects\nexperiment (N=23). We manipulated confederate performance in Overcooked 2,\ncombining observations, NASA TLX self-reports, and interviews. We present two\nkey findings. (1) Observations revealed frustration behaviours completely\nabsent in self-report data. Thus, these instruments assess fundamentally\ndistinct constructs. (2) Participants consistently evaluated teammate\nperformance through relative comparison rather than absolute metrics. This\ncontradicts task-performance operationalizations dominant in competitive gaming\nresearch. Hence, performance evaluation frameworks from competitive contexts\ncannot be directly applied to casual cooperative games. We provide empirical\nevidence that performance evaluation in casual games requires a comparative\noperationalization.", "AI": {"tldr": "This research investigates how casual players evaluate teammate performance in cooperative video games, revealing distinct behavioral responses and assessment mechanisms compared to competitive gaming contexts.", "motivation": "To explore whether existing performance evaluation frameworks from competitive video games apply to casual cooperative games, and to understand how players assess teammate competence in these settings.", "method": "The study conducted a controlled between-subjects experiment with 23 participants in the game Overcooked 2, using observations, NASA TLX self-reports, and interviews to analyze player behavior and evaluations.", "result": "The findings indicate frustration behaviors observed during gameplay that were not captured in self-reports and highlight that players evaluate performance through relative comparisons, challenging existing metrics used in competitive gaming.", "conclusion": "Performance evaluation mechanisms in casual cooperative games differ from those in competitive contexts, necessitating a separate framework for understanding player evaluations in these scenarios.", "key_contributions": ["Identified distinct behavioral responses in casual cooperative games compared to competitive environments.", "Demonstrated that traditional self-reporting methods do not capture all relevant behaviors like frustration.", "Proposed a new comparative operationalization for evaluating performance in casual games."], "limitations": "Limited sample size of 23 participants may affect generalizability of findings.", "keywords": ["Performance evaluation", "casual games", "cooperative gaming", "teammate competence", "behavioral responses"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.18569", "pdf": "https://arxiv.org/pdf/2508.18569.pdf", "abs": "https://arxiv.org/abs/2508.18569", "title": "The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation", "authors": ["Girish A. Koushik", "Fatemeh Nazarieh", "Katherine Birch", "Shenbin Qian", "Diptesh Kanojia"], "categories": ["cs.CL", "cs.CV"], "comment": "Under Review", "summary": "Visual metaphor generation is a challenging task that aims to generate an\nimage given an input text metaphor. Inherently, it needs language understanding\nto bind a source concept with a target concept, in a way that preserves meaning\nwhile ensuring visual coherence. We propose a self-evaluating visual metaphor\ngeneration framework that focuses on metaphor alignment. Our self-evaluation\napproach combines existing metrics with our newly proposed metaphor\ndecomposition score and a meaning alignment (MA) metric. Within this setup, we\nexplore two novel approaches: a training-free pipeline that explicitly\ndecomposes prompts into source-target-meaning (S-T-M) mapping for image\nsynthesis, and a complementary training-based pipeline that improves alignment\nusing our proposed self-evaluation reward schema, without any large-scale\nretraining. On the held-out test set, the training-free approach surpasses\nstrong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,\nwith the training-based approach close behind. We evaluate our framework output\nusing a user-facing study, and observed that participants preferred GPT-4o\noverall, while our training-free pipeline led open-source methods and edged\nImagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or\nmore abstract metaphors, with closed models excelling on short, concrete cases;\nwe also observe sensitivity to sampler settings. Overall, structured prompting\nand lightweight RL perform metaphor alignment well under modest compute, and\nremaining gaps to human preference appear driven by aesthetics and sampling.", "AI": {"tldr": "Proposes a self-evaluating framework for generating visual metaphors that integrates a metaphor decomposition score and meaning alignment metric, demonstrating promising results over existing baselines.", "motivation": "To generate coherent images from text metaphors while preserving meaning, which is inherently challenging due to the need for language understanding.", "method": "Introduces a pipeline that decomposes prompts into source-target-meaning (S-T-M) mapping for image synthesis, along with a training-based method that enhances metaphor alignment using a self-evaluation reward schema.", "result": "The training-free approach outperforms strong closed baselines on metaphor decomposition and meaning alignment scores, while the training-based approach performs closely behind. User studies indicate preferences for GPT-4o but highlight strengths in the training-free pipeline on abstract metaphors.", "conclusion": "Structured prompting and lightweight reinforcement learning effectively accomplish metaphor alignment without extensive retraining; gaps in performance related to human preference are linked to aesthetics and sampler settings.", "key_contributions": ["Introduced a novel metaphor decomposition score.", "Developed a unique meaning alignment metric.", "Demonstrated a training-free pipeline that outperforms existing methods in specific scenarios."], "limitations": "Sensitivity to sampler settings; potential gaps in human aesthetic preferences.", "keywords": ["visual metaphor generation", "self-evaluation", "metaphor alignment", "S-T-M mapping", "machine learning"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2508.18598", "pdf": "https://arxiv.org/pdf/2508.18598.pdf", "abs": "https://arxiv.org/abs/2508.18598", "title": "What do language models model? Transformers, automata, and the format of thought", "authors": ["Colin Klein"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "What do large language models actually model? Do they tell us something about\nhuman capacities, or are they models of the corpus we've trained them on? I\ngive a non-deflationary defence of the latter position. Cognitive science tells\nus that linguistic capabilities in humans rely supralinear formats for\ncomputation. The transformer architecture, by contrast, supports at best a\nlinear formats for processing. This argument will rely primarily on certain\ninvariants of the computational architecture of transformers. I then suggest a\npositive story about what transformers are doing, focusing on Liu et al.\n(2022)'s intriguing speculations about shortcut automata. I conclude with why I\ndon't think this is a terribly deflationary story. Language is not (just) a\nmeans for expressing inner state but also a kind of 'discourse machine' that\nlets us make new language given appropriate context. We have learned to use\nthis technology in one way; LLMs have also learned to use it too, but via very\ndifferent means.", "AI": {"tldr": "The paper explores the differences between human linguistic capabilities and large language models (LLMs), arguing that LLMs model the corpus they are trained on rather than human cognitive processes.", "motivation": "To investigate the nature of what large language models actually model in comparison to human language processing capabilities.", "method": "The author analyzes the computational architectures of transformers and cognitive science insights regarding human linguistic capabilities, particularly focusing on their linear versus supralinear processing formats.", "result": "The study emphasizes that while language serves to express inner states, it also functions as a 'discourse machine' enabling new language creation in context.", "conclusion": "The author argues against the view that understanding LLMs is simply a deflationary task, positing a more nuanced perspective of language use in both humans and models.", "key_contributions": ["Non-deflationary defense of LLMs reflecting the corpus they are trained on", "Contrast between human supralinear computational capabilities and transformer linear formats", "Introduction of the concept of LLMs as discourse machines"], "limitations": "", "keywords": ["large language models", "transformer architecture", "human cognitive capacities", "language processing", "shortcut automata"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.18607", "pdf": "https://arxiv.org/pdf/2508.18607.pdf", "abs": "https://arxiv.org/abs/2508.18607", "title": "A New NMT Model for Translating Clinical Texts from English to Spanish", "authors": ["Rumeng Li", "Xun Wang", "Hong Yu"], "categories": ["cs.CL"], "comment": "This work was accepted by the Machine Learning for Health (ML4H)\n  Workshop at NeurIPS 2018", "summary": "Translating electronic health record (EHR) narratives from English to Spanish\nis a clinically important yet challenging task due to the lack of a\nparallel-aligned corpus and the abundant unknown words contained. To address\nsuch challenges, we propose \\textbf{NOOV} (for No OOV), a new neural machine\ntranslation (NMT) system that requires little in-domain parallel-aligned corpus\nfor training. NOOV integrates a bilingual lexicon automatically learned from\nparallel-aligned corpora and a phrase look-up table extracted from a large\nbiomedical knowledge resource, to alleviate both the unknown word problem and\nthe word-repeat challenge in NMT, enhancing better phrase generation of NMT\nsystems. Evaluation shows that NOOV is able to generate better translation of\nEHR with improvement in both accuracy and fluency.", "AI": {"tldr": "Introducing NOOV, a neural machine translation system designed to translate EHR narratives from English to Spanish with minimal in-domain parallel-aligned corpus.", "motivation": "The translation of electronic health record narratives faces challenges due to unknown words and the lack of parallel-aligned corpora, which can affect the accuracy and fluency of translations.", "method": "NOOV employs a bilingual lexicon learned from parallel corpora and a phrase look-up table from biomedical knowledge resources to tackle unknown words and word-repetition issues in neural machine translation.", "result": "Evaluation indicates that NOOV produces improved translations of EHR narratives in terms of both accuracy and fluency compared to existing methods.", "conclusion": "NOOV demonstrates effective translation capabilities in clinical settings, offering a new approach to EHR translation with limited training data.", "key_contributions": ["Development of the NOOV neural machine translation system for EHR narratives.", "Integration of a bilingual lexicon for better word management in translations.", "Establishment of a phrase look-up table from biomedical knowledge resources to enhance translation quality."], "limitations": "", "keywords": ["neural machine translation", "health informatics", "EHR", "bilingual lexicon", "biomedical knowledge"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.18609", "pdf": "https://arxiv.org/pdf/2508.18609.pdf", "abs": "https://arxiv.org/abs/2508.18609", "title": "Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models", "authors": ["Chenxi Zhou", "Pengfei Cao", "Jiang Li", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) present significant deployment challenges due to\ntheir scale, with post-training quantization (PTQ) emerging as a practical\ncompression solution. However, a comprehensive understanding of how PTQ\nprecisely impacts diverse LLM knowledge capabilities remains elusive, and\nexisting scaling laws for quantized models often overlook crucial PTQ-specific\nparameters and task-specific sensitivities. This paper addresses these gaps by\nconducting an extensive empirical investigation to establish task-stratified\nscaling laws. We disentangle LLM knowledge into memorization and utilization\ncapabilities and develop a unified quantitative framework that incorporates\nmodel size, effective bit-width, calibration set size, and group size. Our\ncentral finding reveals that knowledge memorization exhibits markedly greater\nsensitivity to variations in effective bit-width, calibration set size, and\nmodel size compared to the more robust knowledge utilization. These findings\noffer a fine-grained understanding of PTQ's impact and provide guidance for\ndeveloping knowledge-aware quantization strategies that can better preserve\ntargeted cognitive functions.", "AI": {"tldr": "This paper investigates the impact of post-training quantization on the knowledge capabilities of large language models, establishing task-stratified scaling laws and a framework for understanding varied sensitivities of memorization and utilization.", "motivation": "To understand how post-training quantization (PTQ) affects the knowledge capabilities of large language models and to address the gaps in existing scaling laws for quantized models.", "method": "An empirical investigation was conducted to develop task-stratified scaling laws, focusing on model size, effective bit-width, calibration set size, and group size.", "result": "Knowledge memorization is significantly more sensitive to variations in effective bit-width, calibration set size, and model size compared to knowledge utilization, indicating the need for more nuanced quantization strategies.", "conclusion": "The findings provide a framework for knowledge-aware quantization strategies that better preserve targeted cognitive functions in LLMs during deployment.", "key_contributions": ["Establishment of task-stratified scaling laws for LLMs under post-training quantization.", "Development of a unified quantitative framework to analyze the effects of quantization parameters.", "Insights into the differential sensitivities of knowledge memorization and utilization capabilities."], "limitations": "", "keywords": ["Post-training quantization", "Large language models", "Knowledge capabilities", "Scaling laws", "Quantization strategies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.18648", "pdf": "https://arxiv.org/pdf/2508.18648.pdf", "abs": "https://arxiv.org/abs/2508.18648", "title": "Thinking Before You Speak: A Proactive Test-time Scaling Approach", "authors": ["Cong Li", "Wenchang Chai", "Hejun Wu", "Yan Pan", "Pengxu Wei", "Liang Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit deficiencies with complex\nreasoning tasks, such as maths, which we attribute to the discrepancy between\nhuman reasoning patterns and those presented in the LLMs' training data. When\ndealing with complex problems, humans tend to think carefully before expressing\nsolutions. However, they often do not articulate their inner thoughts,\nincluding their intentions and chosen methodologies. Consequently, critical\ninsights essential for bridging reasoning steps may be absent in training data\ncollected from human sources. To bridge this gap, we proposes inserting\n\\emph{insight}s between consecutive reasoning steps, which review the status\nand initiate the next reasoning steps. Unlike prior prompting strategies that\nrely on a single or a workflow of static prompts to facilitate reasoning,\n\\emph{insight}s are \\emph{proactively} generated to guide reasoning processes.\nWe implement our idea as a reasoning framework, named \\emph{Thinking Before You\nSpeak} (TBYS), and design a pipeline for automatically collecting and filtering\nin-context examples for the generation of \\emph{insight}s, which alleviates\nhuman labeling efforts and fine-tuning overheads. Experiments on challenging\nmathematical datasets verify the effectiveness of TBYS. Project website:\nhttps://gitee.com/jswrt/TBYS", "AI": {"tldr": "This paper proposes a reasoning framework called Thinking Before You Speak (TBYS) that enhances complex reasoning in Large Language Models (LLMs) by inserting proactive insights between reasoning steps.", "motivation": "LLMs struggle with complex reasoning tasks due to differences in human reasoning patterns and the training data. This gap can leave out critical insights necessary for completing reasoning steps effectively.", "method": "The TBYS framework involves inserting proactively generated insights between reasoning steps, allowing for a review of the status and initiation of subsequent reasoning.", "result": "Experiments on challenging mathematical datasets demonstrate that the TBYS framework significantly improves the reasoning capabilities of LLMs on complex problems.", "conclusion": "The TBYS framework shows promise in bridging the reasoning gap in LLMs by utilizing proactive insights, thus reducing the reliance on static prompts and improving reasoning processes.", "key_contributions": ["Introduction of proactive insights in reasoning processes", "Development of the TBYS framework for LLMs", "Automated pipeline for collecting and filtering in-context examples"], "limitations": "", "keywords": ["Large Language Models", "Reasoning Framework", "Human Reasoning Patterns", "Complex Problems", "Insights"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.18651", "pdf": "https://arxiv.org/pdf/2508.18651.pdf", "abs": "https://arxiv.org/abs/2508.18651", "title": "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models", "authors": ["Chenxu Yang", "Qingyi Si", "Zheng Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Grounding responses in external knowledge represents an effective strategy\nfor mitigating hallucinations in Large Language Models (LLMs). However, current\nLLMs struggle to seamlessly integrate knowledge while simultaneously\nmaintaining faithfulness (or fidelity) and expressiveness, capabilities that\nhumans naturally possess. This limitation results in outputs that either lack\nsupport from external knowledge, thereby compromising faithfulness, or appear\noverly verbose and unnatural, thus sacrificing expressiveness. In this work, to\nbreak the trade-off between faithfulness and expressiveness, we propose\nCollaborative Decoding (CoDe), a novel approach that dynamically integrates\noutput probabilities generated with and without external knowledge. This\nintegration is guided by distribution divergence and model confidence, enabling\nthe selective activation of relevant and reliable expressions from the model's\ninternal parameters. Furthermore, we introduce a knowledge-aware reranking\nmechanism that prevents over-reliance on prior parametric knowledge while\nensuring proper utilization of provided external information. Through\ncomprehensive experiments, our plug-and-play CoDe framework demonstrates\nsuperior performance in enhancing faithfulness without compromising\nexpressiveness across diverse LLMs and evaluation metrics, validating both its\neffectiveness and generalizability.", "AI": {"tldr": "Proposes Collaborative Decoding (CoDe) to enhance Large Language Models' outputs by balancing faithfulness and expressiveness through external knowledge integration.", "motivation": "To address the trade-off between faithfulness and expressiveness in outputs generated by Large Language Models, which often lack reliable integration of external knowledge.", "method": "The paper introduces Collaborative Decoding (CoDe), which dynamically integrates output probabilities with and without external knowledge, guided by distribution divergence and model confidence.", "result": "Experiments show that CoDe significantly improves the faithfulness of model outputs without sacrificing expressiveness across various LLMs and evaluation metrics.", "conclusion": "CoDe represents a plug-and-play solution that enhances the performance of LLMs by improving the integration of external knowledge while maintaining natural output quality.", "key_contributions": ["Introduction of Collaborative Decoding (CoDe) framework", "Dynamic integration of knowledge into LLM outputs", "Implementation of a knowledge-aware reranking mechanism"], "limitations": "", "keywords": ["Large Language Models", "faithfulness", "expressiveness", "Collaborative Decoding", "external knowledge"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.18655", "pdf": "https://arxiv.org/pdf/2508.18655.pdf", "abs": "https://arxiv.org/abs/2508.18655", "title": "Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models", "authors": ["Haoyu Wang", "Guangyan Zhang", "Jiale Chen", "Jingyu Li", "Yuehai Wang", "Yiwen Guo"], "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "comment": "5 pages, 1 figure, submitted to ICASSP 2026", "summary": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nsimply convert the response content into speech without fully understanding the\nrich emotional and paralinguistic cues embedded in the user's query. In many\ncases, the same sentence can have different meanings depending on the emotional\nexpression. Furthermore, emotional understanding is essential for improving\nuser experience in human-machine interaction. Currently, most speech LLMs with\nempathetic capabilities are trained on massive datasets. This approach requires\nvast amounts of data and significant computational resources. Therefore, a key\nchallenge lies in how to develop a speech LLM capable of generating empathetic\nresponses with limited data and without the need for large-scale training. To\naddress this challenge, we propose Emotion Omni, a novel model architecture\ndesigned to understand the emotional content of user speech input and generate\nempathetic speech responses. Additionally, we developed a data generation\npipeline based on an open-source TTS framework to construct a 200k emotional\ndialogue dataset, which supports the construction of an empathetic speech\nassistant. The demos are available at https://w311411.github.io/omni_demo/", "AI": {"tldr": "Emotion Omni is a novel speech LLM architecture that generates empathetic responses by understanding emotional content without needing massive datasets.", "motivation": "To improve interaction quality between users and speech assistants by incorporating emotional understanding in responses.", "method": "The proposed model, Emotion Omni, leverages a unique architecture for understanding emotional cues and includes a data generation pipeline for creating a dataset of 200k emotional dialogues.", "result": "Emotion Omni successfully generates empathetic responses to user queries using a limited amount of data, demonstrating the feasibility of developing speech LLMs without extensive training datasets.", "conclusion": "The development of Emotion Omni represents a significant step toward creating empathetic speech assistants that enhance user experience through better emotional understanding.", "key_contributions": ["Introduction of Emotion Omni model architecture for empathetic speech generation", "Creation of a 200k emotional dialogue dataset using a data generation pipeline", "Demonstration of empathetic response generation with limited training data"], "limitations": "The model's performance may be affected by the quality and diversity of the generated dataset.", "keywords": ["speech LLM", "empathy", "human-machine interaction", "data generation", "emotional dialogue"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.19227", "pdf": "https://arxiv.org/pdf/2508.19227.pdf", "abs": "https://arxiv.org/abs/2508.19227", "title": "Generative Interfaces for Language Models", "authors": ["Jiaqi Chen", "Yanzhe Zhang", "Yutong Zhang", "Yijia Shao", "Diyi Yang"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.", "AI": {"tldr": "The paper introduces Generative Interfaces for Language Models, which improve user interaction by generating adaptive UIs from user queries, outperforming traditional chat-based systems.", "motivation": "To improve the efficiency of interactions in multi-turn, information-dense, and exploratory tasks with LLMs, moving beyond linear request-response formats.", "method": "The proposed framework creates structured interface-specific representations to generate task-specific UIs based on user queries, evaluated through a multidimensional assessment framework.", "result": "Generative interfaces showed superior performance to traditional chat formats and were preferred by users in over 70% of interactions.", "conclusion": "The findings highlight the advantages of generative interfaces in human-AI interaction, paving the way for future research and application improvements.", "key_contributions": ["Introduction of generative interfaces for LLMs.", "Development of a multidimensional assessment framework for evaluating user interaction.", "Demonstrated user preference for generative interfaces over conversational systems."], "limitations": "", "keywords": ["Generative Interfaces", "Language Models", "Human-AI Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.18673", "pdf": "https://arxiv.org/pdf/2508.18673.pdf", "abs": "https://arxiv.org/abs/2508.18673", "title": "Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum", "authors": ["Xinglong Yang", "Quan Feng", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Wentong Li", "Shuofei Qiao", "Yuxia Geng", "Xingyu Zhao", "Sheng-Jun Huang"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often\nlimited by the use of randomly or manually selected examples. These examples\nfail to account for both model-specific knowledge distributions and the\nintrinsic complexity of the tasks, resulting in suboptimal and unstable model\nperformance. To address this, we propose a novel framework inspired by the\npedagogical principle of \"tailored teaching with balanced difficulty\". We\nreframe prompt selection as a prompt curriculum design problem: constructing a\nwell ordered set of training examples that align with the model's current\ncapabilities. Our approach integrates two complementary signals: (1)\nmodel-perceived difficulty, quantified through prediction disagreement in an\nactive learning setup, capturing what the model itself finds challenging; and\n(2) intrinsic sample complexity, which measures the inherent difficulty of each\nquestion-image pair independently of any model. By jointly analyzing these\nsignals, we develop a difficulty-balanced sampling strategy that ensures the\nselected prompt examples are diverse across both dimensions. Extensive\nexperiments conducted on five challenging benchmarks and multiple popular\nMultimodal Large Language Models (MLLMs) demonstrate that our method yields\nsubstantial and consistent improvements and greatly reduces performance\ndiscrepancies caused by random sampling, providing a principled and robust\napproach for enhancing multimodal reasoning.", "AI": {"tldr": "The paper proposes a new framework for Multimodal Chain-of-Thought (MCoT) prompting that addresses the limitations of existing methods by using a tailored example selection process based on model abilities and task complexity.", "motivation": "Existing methods of example selection for MCoT prompting lead to unstable model performance due to the use of randomly or manually selected examples. The paper aims to improve this by tailoring examples to the model's current capabilities.", "method": "The authors propose a prompt curriculum design approach that combines model-perceived difficulty and intrinsic sample complexity to select diverse training examples that balance difficulty.", "result": "Experimentation on five benchmarks and with various Multimodal Large Language Models (MLLMs) shows significant and consistent performance improvements, reducing discrepancies from random sampling.", "conclusion": "The developed difficulty-balanced sampling strategy provides a robust and principled approach to enhance multimodal reasoning in MCoT prompting.", "key_contributions": ["Introduces a novel framework for prompt curriculum design in MCoT prompting.", "Combines model-perceived difficulty with intrinsic sample complexity for example selection.", "Demonstrates substantial performance improvements across multiple benchmarks and MLLMs."], "limitations": "", "keywords": ["Multimodal Chain-of-Thought", "prompt selection", "machine learning", "active learning", "large language models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2308.03651", "pdf": "https://arxiv.org/pdf/2308.03651.pdf", "abs": "https://arxiv.org/abs/2308.03651", "title": "Cluster-Aware Grid Layout", "authors": ["Yuxing Zhou", "Weikai Yang", "Jiashu Chen", "Changjian Chen", "Zhiyang Shen", "Xiaonan Luo", "Lingyun Yu", "Shixia Liu"], "categories": ["cs.HC"], "comment": "Accecpted in IEEE VIS 2023. 11 pages. 10 figures", "summary": "Grid visualizations are widely used in many applications to visually explain\na set of data and their proximity relationships. However, existing layout\nmethods face difficulties when dealing with the inherent cluster structures\nwithin the data. To address this issue, we propose a cluster-aware grid layout\nmethod that aims to better preserve cluster structures by simultaneously\nconsidering proximity, compactness, and convexity in the optimization process.\nOur method utilizes a hybrid optimization strategy that consists of two phases.\nThe global phase aims to balance proximity and compactness within each cluster,\nwhile the local phase ensures the convexity of cluster shapes. We evaluate the\nproposed grid layout method through a series of quantitative experiments and\ntwo use cases, demonstrating its effectiveness in preserving cluster structures\nand facilitating analysis tasks.", "AI": {"tldr": "The paper presents a cluster-aware grid layout method that enhances the visualization of data clusters by optimizing for proximity, compactness, and convexity.", "motivation": "Existing layout methods struggle with preserving cluster structures in grid visualizations, which can hinder data analysis.", "method": "The proposed method involves a two-phase hybrid optimization strategy: a global phase for proximity and compactness within clusters, and a local phase for ensuring convexity of cluster shapes.", "result": "The evaluation through quantitative experiments and use cases indicates that the method effectively preserves cluster structures and aids analysis tasks.", "conclusion": "The cluster-aware grid layout method improves the depiction of clusters in grid visualizations, making it beneficial for data analysis in various applications.", "key_contributions": ["Introduction of a cluster-aware grid layout method", "Hybrid optimization strategy for balancing proximity, compactness, and convexity", "Demonstrated effectiveness through experiments and use cases"], "limitations": "", "keywords": ["grid visualization", "cluster analysis", "optimization", "human-computer interaction", "data structures"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.18687", "pdf": "https://arxiv.org/pdf/2508.18687.pdf", "abs": "https://arxiv.org/abs/2508.18687", "title": "Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning", "authors": ["Songtao Jiang", "Yuxi Chen", "Sibo Song", "Yan Zhang", "Yeying Jin", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": null, "summary": "In high-stakes medical applications, consistent answering across diverse\nquestion phrasings is essential for reliable diagnosis. However, we reveal that\ncurrent Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility\nin Medical Visual Question Answering, as their answers fluctuate significantly\nwhen faced with semantically equivalent rephrasings of medical questions. We\nattribute this to two limitations: (1) insufficient alignment of medical\nconcepts, leading to divergent reasoning patterns, and (2) hidden biases in\ntraining data that prioritize syntactic shortcuts over semantic understanding.\nTo address these challenges, we construct RoMed, a dataset built upon original\nVQA datasets containing 144k questions with variations spanning word-level,\nsentence-level, and semantic-level perturbations. When evaluating\nstate-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming\nperformance drops (e.g., a 40\\% decline in Recall) compared to original VQA\nbenchmarks, exposing critical robustness gaps. To bridge this gap, we propose\nConsistency and Contrastive Learning (CCL), which integrates two key\ncomponents: (1) knowledge-anchored consistency learning, aligning Med-VLMs with\nmedical knowledge rather than shallow feature patterns, and (2) bias-aware\ncontrastive learning, mitigating data-specific priors through discriminative\nrepresentation refinement. CCL achieves SOTA performance on three popular VQA\nbenchmarks and notably improves answer consistency by 50\\% on the challenging\nRoMed test set, demonstrating significantly enhanced robustness. Code will be\nreleased.", "AI": {"tldr": "This paper addresses the fragility of Medical Vision-Language Models (Med-VLMs) in Medical Visual Question Answering, proposing a new dataset and innovative learning strategies to improve answer consistency and robustness.", "motivation": "The inconsistency in answers from Med-VLMs when faced with different phrasings of the same medical question undermines their reliability in high-stakes medical scenarios.", "method": "The authors construct RoMed, a dataset with 144k varied questions, and propose Consistency and Contrastive Learning (CCL) that uses knowledge-anchored consistency learning and bias-aware contrastive learning to enhance model robustness.", "result": "State-of-the-art models like LLaVA-Med showed a performance drop of 40% in Recall when evaluated on RoMed compared to original VQA benchmarks; CCL improves answer consistency by 50% on the RoMed test set.", "conclusion": "The proposed CCL methodology significantly enhances the robustness and performance of Med-VLMs in Medical Visual Question Answering tasks.", "key_contributions": ["Introduction of the RoMed dataset with diverse question variations", "Development of Consistency and Contrastive Learning methods", "Demonstration of improved model robustness and answer consistency"], "limitations": "", "keywords": ["Medical Vision-Language Models", "Visual Question Answering", "Consistency Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.09955", "pdf": "https://arxiv.org/pdf/2504.09955.pdf", "abs": "https://arxiv.org/abs/2504.09955", "title": "VR MRI Training for Adolescents: A Comparative Study of Gamified VR, Passive VR, 360° Video, and Traditional Educational Video", "authors": ["Yue Yang", "Mengyao Guo", "Yuxuan Wu", "Wally Niu", "Emmanuel A Corona", "Bruce Daniel", "Christoph Leuze", "Fred Baik"], "categories": ["cs.HC"], "comment": "Download our application at\n  https://www.meta.com/experiences/stanford-mri-simulator/8205539289482347/", "summary": "Meta Quest Store:\nhttps://www.meta.com/experiences/stanford-mri-simulator/8205539289482347/\nMagnetic Resonance Imaging (MRI) can be a stressful experience for pediatric\npatients due to the loud acoustic environment, enclosed scanner bore, and a\nprolonged requirement to remain still. While sedation is commonly used to\nmanage anxiety and motion, it carries clinical risks and logistical burdens.\nTraditional preparatory approaches, such as instructional videos and mock\nscans, often lack engagement for older children and adolescents. In this study,\nwe present a comparative evaluation of four MRI preparation modalities: (1) a\ngamified virtual reality (VR) simulation that trains stillness through\nreal-time feedback; (2) a passive VR experience replicating the MRI environment\nwithout interactivity; (3) a 360{\\deg} first-person video of a real MRI\nprocedure; and (4) a standard 2D educational video. Using a within-subjects\ndesign (N = 11, ages 10-16), we assess each method's impact on head motion\ndata, anxiety reduction, procedural preparedness, usability, cognitive\nworkload, and subjective preference. Results show that the gamified VR\ncondition has significantly lower head motion (p < 0.001) and yielded the\nhighest preparedness scores (p < 0.05). Head motion data were significantly\ncorrelated with learning outcomes (p < 0.01), suggesting that behavioral\nperformance in VR strongly indicates procedural readiness. While all modalities\nreduced anxiety and were rated usable, interactive VR was preferred by most\nparticipants and demonstrated unique advantages in promoting engagement and\nbehavioral rehearsal. We conclude with design recommendations for designing\nimmersive simulations and integrating VR training into pediatric imaging\nworkflows.", "AI": {"tldr": "This study evaluates the effectiveness of four MRI preparation modalities, highlighting the advantages of a gamified VR simulation for reducing motion and increasing preparedness in pediatric patients.", "motivation": "To improve the MRI experience for pediatric patients, reducing anxiety and motion through innovative preparation techniques.", "method": "A within-subjects design was employed with 11 participants (ages 10-16) to compare four MRI preparation methods: a gamified VR simulation, a passive VR experience, a 360-degree video, and a standard 2D video, focusing on outcomes such as head motion, anxiety, and preparedness.", "result": "The gamified VR condition resulted in significantly lower head motion (p < 0.001) and higher preparedness scores (p < 0.05). It also indicated a strong correlation between head motion and learning outcomes (p < 0.01).", "conclusion": "Gamified VR simulations enhance engagement and reduce anxiety, suggesting their potential integration into pediatric MRI workflows.", "key_contributions": ["Introduction of gamified virtual reality for MRI preparation.", "Demonstrated lower head motion and higher preparedness in pediatric patients using VR.", "Provided design recommendations for immersive simulations in healthcare."], "limitations": "", "keywords": ["MRI", "Virtual Reality", "Pediatrics", "Anxiety", "Engagement"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.18701", "pdf": "https://arxiv.org/pdf/2508.18701.pdf", "abs": "https://arxiv.org/abs/2508.18701", "title": "Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System", "authors": ["Yanfan Du", "Jun Zhang", "Bin Wang", "Jin Qiu", "Lu Huang", "Yuan Ge", "Xiaoqian Liu", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "9 pages, 4 figures, 5 tables", "summary": "Recent advances in speech large language models (SLMs) have improved speech\nrecognition and translation in general domains, but accurately generating\ndomain-specific terms or neologisms remains challenging. To address this, we\npropose Attention2Probability: attention-driven terminology probability\nestimation for robust speech-to-text system, which is lightweight, flexible,\nand accurate. Attention2Probability converts cross-attention weights between\nspeech and terminology into presence probabilities, and it further employs\ncurriculum learning to enhance retrieval accuracy. Furthermore, to tackle the\nlack of data for speech-to-text tasks with terminology intervention, we create\nand release a new speech dataset with terminology to support future research in\nthis area. Experimental results show that Attention2Probability significantly\noutperforms the VectorDB method on our test set. Specifically, its maximum\nrecall rates reach 92.57% for Chinese and 86.83% for English. This high recall\nis achieved with a latency of only 8.71ms per query. Intervening in SLMs'\nrecognition and translation tasks using Attention2Probability-retrieved terms\nimproves terminology accuracy by 6-17%, while revealing that the current\nutilization of terminology by SLMs has limitations.", "AI": {"tldr": "This paper presents Attention2Probability, a method that estimates the probability of domain-specific terminology in speech recognition and translation, addressing challenges in generating accurate neologisms.", "motivation": "Despite advances in speech large language models, accurately generating domain-specific terms in speech recognition and translation is still challenging.", "method": "Attention2Probability leverages cross-attention weights between speech and terminology to estimate presence probabilities, while utilizing curriculum learning to enhance accuracy. A new speech dataset with terminology was also created to support this research.", "result": "Attention2Probability outperforms the VectorDB method, achieving maximum recall rates of 92.57% for Chinese and 86.83% for English, with a low latency of 8.71ms per query.", "conclusion": "Intervening in SLMs with Attention2Probability significantly improves terminology accuracy by 6-17%, highlighting limitations in SLMs' current utilization of terminology.", "key_contributions": ["Introduction of Attention2Probability for terminology probability estimation in SLMs", "Creation and release of a new speech dataset with terminology for research", "Demonstration of superior performance in recall rates compared to existing methods"], "limitations": "", "keywords": ["speech recognition", "large language models", "terminology accuracy", "cross-attention", "curriculum learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.18709", "pdf": "https://arxiv.org/pdf/2508.18709.pdf", "abs": "https://arxiv.org/abs/2508.18709", "title": "Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs", "authors": ["Duy Le", "Kent Ziti", "Evan Girard-Sun", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual riddle generation challenges large language models (LLMs) to\nbalance cultural fluency with creative abstraction. Standard prompting\nstrategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized\nriddles or perform shallow paraphrasing. We introduce Adaptive Originality\nFiltering (AOF), a prompting framework that filters redundant generations using\ncosine-based similarity rejection, while enforcing lexical novelty and\ncross-lingual fidelity. Evaluated across three LLMs and four language pairs,\nAOF-enhanced GPT-4o achieves \\texttt{0.177} Self-BLEU and \\texttt{0.915}\nDistinct-2 in Japanese, signaling improved lexical diversity and reduced\nredundancy compared to other prompting methods and language pairs. Our findings\nshow that semantic rejection can guide culturally grounded, creative generation\nwithout task-specific fine-tuning.", "AI": {"tldr": "This paper presents Adaptive Originality Filtering (AOF) to promote diverse and culturally fluent multilingual riddle generation using large language models.", "motivation": "To address the limitations of standard prompting strategies that result in reused or shallow riddle generations in multilingual contexts.", "method": "A framework called Adaptive Originality Filtering (AOF) is introduced, which utilizes cosine-based similarity rejection to filter out redundant output, while ensuring lexical novelty and cross-lingual fidelity.", "result": "AOF-enhanced GPT-4o demonstrated significant improvements, achieving a Self-BLEU of 0.177 and Distinct-2 of 0.915 in Japanese, indicating enhanced lexical diversity and lower redundancy.", "conclusion": "The results suggest that employing semantic rejection can effectively guide creative generation in a culturally informed manner without the need for fine-tuning specific to tasks.", "key_contributions": ["Introduction of Adaptive Originality Filtering (AOF) framework", "Demonstrated effectiveness on multiple LLMs and language pairs", "Improvement in lexical diversity and reduction of redundancy during riddle generation"], "limitations": "", "keywords": ["multilingual", "riddle generation", "language models", "Adaptive Originality Filtering", "cross-lingual"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.18715", "pdf": "https://arxiv.org/pdf/2508.18715.pdf", "abs": "https://arxiv.org/abs/2508.18715", "title": "EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues", "authors": ["Angela Yifei Yuan", "Haoyi Li", "Soyeon Caren Han", "Christopher Leckie"], "categories": ["cs.CL"], "comment": "15 pages", "summary": "The rapid adoption of large language models (LLMs) in customer service\nintroduces new risks, as malicious actors can exploit them to conduct\nlarge-scale user impersonation through machine-generated text (MGT). Current\nMGT detection methods often struggle in online conversational settings,\nreducing the reliability and interpretability essential for trustworthy AI\ndeployment. In customer service scenarios where operators are typically\nnon-expert users, explanation become crucial for trustworthy MGT detection. In\nthis paper, we propose EMMM, an explanation-then-detection framework that\nbalances latency, accuracy, and non-expert-oriented interpretability.\nExperimental results demonstrate that EMMM provides explanations accessible to\nnon-expert users, with 70\\% of human evaluators preferring its outputs, while\nachieving competitive accuracy compared to state-of-the-art models and\nmaintaining low latency, generating outputs within 1 second. Our code and\ndataset are open-sourced at\nhttps://github.com/AngieYYF/EMMM-explainable-chatbot-detection.", "AI": {"tldr": "EMMM is an explanation-then-detection framework for detecting machine-generated text (MGT) in customer service, focusing on usability for non-expert users.", "motivation": "The increasing use of large language models in customer service raises the risk of malicious impersonation through machine-generated text, necessitating effective detection methods that also provide explanations.", "method": "The paper introduces EMMM, which offers explanations before detection, aiming to balance latency, accuracy, and interpretability for non-expert users in online settings.", "result": "EMMM achieves competitive accuracy with state-of-the-art models while providing interpretable outputs preferred by 70% of human evaluators and maintaining low latency (under 1 second).", "conclusion": "The EMMM framework enhances the reliability of MGT detection in customer service by addressing the needs of non-expert users with interpretable explanations and efficient performance.", "key_contributions": ["Introduction of EMMM framework for explainable MGT detection", "Demonstrated effective balance between accuracy, latency, and interpretability", "Open-sourced code and dataset for further research"], "limitations": "", "keywords": ["large language models", "machine-generated text", "customer service", "explainable AI", "non-expert users"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.18739", "pdf": "https://arxiv.org/pdf/2508.18739.pdf", "abs": "https://arxiv.org/abs/2508.18739", "title": "Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models", "authors": ["Chang Wang", "Siyu Yan", "Depeng Yuan", "Yuqi Chen", "Yanhua Huang", "Yuanhang Zheng", "Shuhao Li", "Yinqi Zhang", "Kedi Chen", "Mingrui Zhu", "Ruiwen Xu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The generation of ad headlines plays a vital role in modern advertising,\nwhere both quality and diversity are essential to engage a broad range of\naudience segments. Current approaches primarily optimize language models for\nheadline quality or click-through rates (CTR), often overlooking the need for\ndiversity and resulting in homogeneous outputs. To address this limitation, we\npropose DIVER, a novel framework based on large language models (LLMs) that are\njointly optimized for both diversity and quality. We first design a semantic-\nand stylistic-aware data generation pipeline that automatically produces\nhigh-quality training pairs with ad content and multiple diverse headlines. To\nachieve the goal of generating high-quality and diversified ad headlines within\na single forward pass, we propose a multi-stage multi-objective optimization\nframework with supervised fine-tuning (SFT) and reinforcement learning (RL).\nExperiments on real-world industrial datasets demonstrate that DIVER\neffectively balances quality and diversity. Deployed on a large-scale\ncontent-sharing platform serving hundreds of millions of users, our framework\nimproves advertiser value (ADVV) and CTR by 4.0% and 1.4%.", "AI": {"tldr": "DIVER is a framework for generating diverse and high-quality ad headlines using LLMs, addressing the issue of homogeneous outputs in advertising.", "motivation": "Current methods for generating ad headlines often focus solely on quality or CTR, leading to a lack of diversity in outputs that can engage a wider audience.", "method": "A multi-stage multi-objective optimization framework is proposed, which combines supervised fine-tuning and reinforcement learning to optimize both diversity and quality simultaneously during the generation of ad headlines.", "result": "Experiments on real-world datasets show that DIVER effectively improves both the advertiser value and CTR by 4.0% and 1.4%, respectively, while maintaining a balance between quality and diversity.", "conclusion": "DIVER offers a robust solution for advertisers seeking to enhance engagement through diverse ad headlines without compromising on quality.", "key_contributions": ["Introduction of the DIVER framework optimizing for both diversity and quality in ad headline generation.", "Development of a semantic- and stylistic-aware data generation pipeline for creating diverse training data.", "Validation of effectiveness through deployment on a large-scale content-sharing platform."], "limitations": "", "keywords": ["ad headlines", "large language models", "diversity in advertising"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2508.18740", "pdf": "https://arxiv.org/pdf/2508.18740.pdf", "abs": "https://arxiv.org/abs/2508.18740", "title": "M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations", "authors": ["Qiao Liang", "Ying Shen", "Tiantian Chen", "Lin Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 8 figures. Accepted to Findings of ACL 2025", "summary": "Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has\nrecently gained significant attention in social media analysis, aiming to\nextract emotion utterances, cause utterances, and emotion categories\nsimultaneously. However, the scarcity of related datasets, with only one\npublished dataset featuring highly uniform dialogue scenarios, hinders model\ndevelopment in this field. To address this, we introduce MECAD, the first\nmultimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56\nTV series spanning a wide range of dialogue contexts. In addition, existing\nMECTEC methods fail to explicitly model emotional and causal contexts and\nneglect the fusion of semantic information at different levels, leading to\nperformance degradation. In this paper, we propose M3HG, a novel model that\nexplicitly captures emotional and causal contexts and effectively fuses\ncontextual information at both inter- and intra-utterance levels via a\nmultimodal heterogeneous graph. Extensive experiments demonstrate the\neffectiveness of M3HG compared with existing state-of-the-art methods. The\ncodes and dataset are available at https://github.com/redifinition/M3HG.", "AI": {"tldr": "This paper introduces MECAD, a multimodal dataset for Emotion Cause Triplet Extraction (MECTEC), and proposes a novel model, M3HG, to improve extraction performance by capturing emotional and causal contexts.", "motivation": "The scarcity of datasets for Emotion Cause Triplet Extraction hinders model development, necessitating the creation of a more diverse dataset and improved modeling techniques.", "method": "MECAD is introduced as the first multimodal, multi-scenario dataset for MECTEC, with 989 conversations from 56 TV series. The proposed M3HG model utilizes a multimodal heterogeneous graph to capture emotional and causal contexts and to fuse contextual information effectively.", "result": "Extensive experiments showed that M3HG outperforms existing state-of-the-art methods in Emotion Cause Triplet Extraction.", "conclusion": "The introduction of the MECAD dataset and the M3HG model provide significant advancements for research on emotion cause extraction in multimodal contexts.", "key_contributions": ["Introduction of the MECAD dataset, the first multimodal, multi-scenario dataset for MECTEC.", "Proposing the M3HG model that captures emotional and causal contexts explicitly.", "Demonstrating improved performance of M3HG over existing methods."], "limitations": "", "keywords": ["Emotion Cause Triplet Extraction", "MECTEC", "Multimodal Dataset"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.18748", "pdf": "https://arxiv.org/pdf/2508.18748.pdf", "abs": "https://arxiv.org/abs/2508.18748", "title": "Chronological Passage Assembling in RAG framework for Temporal Question Answering", "authors": ["Byeongjeong Kim", "Jeonghyun Park", "Joonho Yang", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "7 pages, 3 figures", "summary": "Long-context question answering over narrative tasks is challenging because\ncorrect answers often hinge on reconstructing a coherent timeline of events\nwhile preserving contextual flow in a limited context window.\nRetrieval-augmented generation (RAG) indexing methods aim to address this\nchallenge by selectively retrieving only necessary document segments. However,\nnarrative texts possess unique characteristics that limit the effectiveness of\nthese existing approaches. Specifically, understanding narrative texts requires\nmore than isolated segments, as the broader context and sequential\nrelationships between segments are crucial for comprehension. To address these\nlimitations, we propose ChronoRAG, a novel RAG framework specialized for\nnarrative texts. This approach focuses on two essential aspects: refining\ndispersed document information into coherent and structured passages, and\npreserving narrative flow by explicitly capturing and maintaining the temporal\norder among retrieved passages. We empirically demonstrate the effectiveness of\nChronoRAG through experiments on the NarrativeQA dataset, showing substantial\nimprovements in tasks requiring both factual identification and comprehension\nof complex sequential relationships, underscoring that reasoning over temporal\norder is crucial in resolving narrative QA.", "AI": {"tldr": "ChronoRAG is a novel RAG framework designed for long-context question answering in narrative tasks, preserving narrative flow and temporal order in episodic information retrieval.", "motivation": "Address the challenges of long-context question answering in narrative texts, where understanding requires coherent timelines and contextual flow.", "method": "ChronoRAG framework refines dispersed document information and captures temporal order among retrieved passages for improved narrative comprehension.", "result": "Empirical tests on the NarrativeQA dataset show significant improvements in both factual identification and understanding of complex sequential relationships in narratives.", "conclusion": "ChronoRAG effectively enhances narrative question answering by prioritizing contextual flow and temporal relationships in the retrieval process.", "key_contributions": ["Development of the ChronoRAG framework for narrative question answering", "Focus on temporal order in information retrieval", "Empirical validation showing improved performance on NarrativeQA tasks"], "limitations": "", "keywords": ["long-context QA", "narrative texts", "retrieval-augmented generation", "temporal order", "ChronoRAG"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2508.18773", "pdf": "https://arxiv.org/pdf/2508.18773.pdf", "abs": "https://arxiv.org/abs/2508.18773", "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models", "authors": ["Qianyu He", "Siyu Yuan", "Xuefeng Li", "Mingxuan Wang", "Jiangjie Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks.", "AI": {"tldr": "Introducing ThinkDial, an open-source framework for controlling LLM reasoning through discrete operational modes, achieving compression-performance trade-offs without significant performance loss.", "motivation": "To address the challenge of controlling computational effort in large language models (LLMs) during practical deployments, as current proprietary systems offer discrete operational modes that the open-source community lacks.", "method": "An end-to-end training paradigm combining budget-mode supervised fine-tuning for embedding reasoning capabilities and budget-aware reinforcement learning for adaptive reward shaping.", "result": "ThinkDial enables switching among three reasoning modes (High, Medium, Low) with significant token reduction and minimal performance degradation, demonstrating effective compression-performance trade-offs and strong out-of-distribution generalization.", "conclusion": "ThinkDial represents a significant advancement in controllable reasoning for LLMs, providing a practical solution for reducing computational costs while maintaining performance standards.", "key_contributions": ["First open-recipe framework for controllable reasoning in LLMs", "Three distinct reasoning modes for tuning computational effort", "Demonstrated strong generalization on out-of-distribution tasks"], "limitations": "", "keywords": ["Large language models", "controllable reasoning", "machine learning", "open-source framework", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.18780", "pdf": "https://arxiv.org/pdf/2508.18780.pdf", "abs": "https://arxiv.org/abs/2508.18780", "title": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction", "authors": ["Yilin Li", "Xunjian Yin", "Yilin Chen", "Xiaojun Wan"], "categories": ["cs.CL", "cs.AI"], "comment": "Code will be released upon publication", "summary": "Grammatical error correction is a significant task in NLP. Traditional\nmethods based on encoder-decoder models have achieved certain success, but the\napplication of LLMs in this field is still underexplored. Current research\npredominantly relies on supervised fine-tuning to train LLMs to directly\ngenerate the corrected sentence, which limits the model's powerful reasoning\nability. To address this limitation, we propose a novel framework based on\nRule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL\nframework achieves \\textbf{state-of-the-art }performance, with a notable\nincrease in \\textbf{recall}. This result clearly highlights the advantages of\nusing RL to steer LLMs, offering a more controllable and reliable paradigm for\nfuture development in GEC.", "AI": {"tldr": "This paper presents a Rule-Based Reinforcement Learning (RL) framework for Grammatical Error Correction (GEC) using LLMs, achieving state-of-the-art performance on Chinese datasets.", "motivation": "The paper aims to improve Grammatical Error Correction by addressing the limitations of traditional supervised fine-tuning methods in LLMs, which fail to leverage the full reasoning potential of these models.", "method": "The proposed framework utilizes Rule-Based RL to guide the LLMs in generating corrected sentences, enhancing their reasoning capabilities.", "result": "The framework demonstrates state-of-the-art performance in GEC tasks, achieving a significant increase in recall on Chinese datasets.", "conclusion": "The study suggests that Rule-Based RL can effectively enhance the controllability and reliability of LLMs in grammatical error correction.", "key_contributions": ["Introduction of a Rule-Based RL framework for GEC", "Achieves state-of-the-art performance on Chinese datasets", "Demonstrates the advantages of using RL for guiding LLMs."], "limitations": "", "keywords": ["Grammatical Error Correction", "Reinforcement Learning", "LLMs", "NLP"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.04945", "pdf": "https://arxiv.org/pdf/2503.04945.pdf", "abs": "https://arxiv.org/abs/2503.04945", "title": "Collaborative Evaluation of Deepfake Text with Deliberation-Enhancing Dialogue Systems", "authors": ["Jooyoung Lee", "Xiaochen Zhu", "Georgi Karadzhov", "Tom Stafford", "Andreas Vlachos", "Dongwon Lee"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "15; To appear in ICWSM 2026 (https://www.icwsm.org/2026/)", "summary": "The proliferation of generative models has presented significant challenges\nin distinguishing authentic human-authored content from deepfake content.\nCollaborative human efforts, augmented by AI tools, present a promising\nsolution. In this study, we explore the potential of DeepFakeDeLiBot, a\ndeliberation-enhancing chatbot, to support groups in detecting deepfake text.\nOur findings reveal that group-based problem-solving significantly improves the\naccuracy of identifying machine-generated paragraphs compared to individual\nefforts. While engagement with DeepFakeDeLiBot does not yield substantial\nperformance gains overall, it enhances group dynamics by fostering greater\nparticipant engagement, consensus building, and the frequency and diversity of\nreasoning-based utterances. Additionally, participants with higher perceived\neffectiveness of group collaboration exhibited performance benefits from\nDeepFakeDeLiBot. These findings underscore the potential of deliberative\nchatbots in fostering interactive and productive group dynamics while ensuring\naccuracy in collaborative deepfake text detection. \\textit{Dataset and source\ncode used in this study will be made publicly available upon acceptance of the\nmanuscript.", "AI": {"tldr": "Explores how the DeepFakeDeLiBot chatbot improves group collaboration in detecting deepfake text, although individual performance gains are limited.", "motivation": "The increasing prevalence of generative models necessitates better methods for distinguishing between authentic and machine-generated content, with collaborative human efforts augmented by AI potentially providing a solution.", "method": "Experimentation with the DeepFakeDeLiBot chatbot among groups designed to identify deepfake text, measuring accuracy against individual efforts and group dynamics.", "result": "Group participation improves identification accuracy of machine-generated paragraphs, with notable enhancements in group engagement and reasoning diversity, despite limited overall performance gains from the chatbot.", "conclusion": "Deliberative chatbots like DeepFakeDeLiBot enhance interactive group dynamics essential for accurate deepfake text detection, suggesting their potential for collaborative efforts in HCI contexts.", "key_contributions": ["Demonstration of improved accuracy in deepfake detection through group collaboration.", "Enhanced group dynamics facilitated by the chatbot, increasing engagement and diversity of reasoning.", "Insight into the limitations of AI tools in collaborative contexts, particularly regarding individual performance gains."], "limitations": "Engagement with the chatbot did not lead to significant performance improvements for individuals and varied effectiveness based on perceptions of group collaboration.", "keywords": ["Deepfake detection", "Human-AI collaboration", "Group dynamics", "Chatbot", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.18783", "pdf": "https://arxiv.org/pdf/2508.18783.pdf", "abs": "https://arxiv.org/abs/2508.18783", "title": "Controllable Conversational Theme Detection Track at DSTC 12", "authors": ["Igor Shalyminov", "Hang Su", "Jake Vincent", "Siffi Singh", "Jason Cai", "James Gung", "Raphael Shu", "Saab Mansour"], "categories": ["cs.CL"], "comment": "DSTC12@SigDial2025; data and code available at\n  https://github.com/amazon-science/dstc12-controllable-conversational-theme-detection", "summary": "Conversational analytics has been on the forefront of transformation driven\nby the advances in Speech and Natural Language Processing techniques. Rapid\nadoption of Large Language Models (LLMs) in the analytics field has taken the\nproblems that can be automated to a new level of complexity and scale. In this\npaper, we introduce Theme Detection as a critical task in conversational\nanalytics, aimed at automatically identifying and categorizing topics within\nconversations. This process can significantly reduce the manual effort involved\nin analyzing expansive dialogs, particularly in domains like customer support\nor sales. Unlike traditional dialog intent detection, which often relies on a\nfixed set of intents for downstream system logic, themes are intended as a\ndirect, user-facing summary of the conversation's core inquiry. This\ndistinction allows for greater flexibility in theme surface forms and\nuser-specific customizations. We pose Controllable Conversational Theme\nDetection problem as a public competition track at Dialog System Technology\nChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of\ndialog utterances, with the distinctive aspect being controllability of the\nresulting theme clusters' granularity achieved via the provided user preference\ndata. We give an overview of the problem, the associated dataset and the\nevaluation metrics, both automatic and human. Finally, we discuss the\nparticipant teams' submissions and provide insights from those. The track\nmaterials (data and code) are openly available in the GitHub repository.", "AI": {"tldr": "This paper introduces Theme Detection in conversational analytics aimed at automatically identifying and categorizing topics in conversations, leveraging recent advances in LLMs.", "motivation": "The need for automating complex dialog analysis in domains like customer support and sales due to the rapid adoption of LLMs in analytics.", "method": "It presents the Controllable Conversational Theme Detection as a competition track, framed as joint clustering and theme labeling, utilizing user preference data for granular control.", "result": "The paper discusses the overview of the task, provided datasets, evaluation metrics, and insights from participant submissions.", "conclusion": "The approach allows for flexibility in summarizing conversations and is aimed at reducing manual effort in dialog analysis.", "key_contributions": ["Introduction of Theme Detection for automation in conversational analytics", "Framing of the Controllable Conversational Theme Detection problem", "Open availability of datasets and evaluation metrics for community use"], "limitations": "", "keywords": ["Conversational Analytics", "Theme Detection", "Large Language Models", "Dialog Systems", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.18791", "pdf": "https://arxiv.org/pdf/2508.18791.pdf", "abs": "https://arxiv.org/abs/2508.18791", "title": "LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination", "authors": ["Ziming Zhu", "Chenglong Wang", "Shunjie Xing", "Yifu Huo", "Fengning Tian", "Quan Du", "Di Yang", "Chunliang Zhang", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Despite the remarkable progress of modern machine translation (MT) systems on\ngeneral-domain texts, translating structured LaTeX-formatted documents remains\na significant challenge. These documents typically interleave natural language\nwith domain-specific syntax, such as mathematical equations, tables, figures,\nand cross-references, all of which must be accurately preserved to maintain\nsemantic integrity and compilability. In this paper, we introduce LaTeXTrans, a\ncollaborative multi-agent system designed to address this challenge. LaTeXTrans\nensures format preservation, structural fidelity, and terminology consistency\nthrough six specialized agents: 1) a Parser that decomposes LaTeX into\ntranslation-friendly units via placeholder substitution and syntax filtering;\n2) a Translator, Validator, Summarizer, and Terminology Extractor that work\ncollaboratively to ensure context-aware, self-correcting, and\nterminology-consistent translations; 3) a Generator that reconstructs the\ntranslated content into well-structured LaTeX documents. Experimental results\ndemonstrate that LaTeXTrans can outperform mainstream MT systems in both\ntranslation accuracy and structural fidelity, offering an effective and\npractical solution for translating LaTeX-formatted documents.", "AI": {"tldr": "LaTeXTrans is a collaborative multi-agent system designed for translating structured LaTeX documents while preserving format, structure, and terminology.", "motivation": "Modern machine translation struggles with LaTeX documents that contain natural language and complex formatting such as equations, tables, and references, which need to be preserved for accurate translation.", "method": "LaTeXTrans employs six specialized agents: a Parser for syntax decomposition, a Translator for context-aware translations, a Validator, a Summarizer, a Terminology Extractor for consistency, and a Generator for reconstructing LaTeX documents.", "result": "LaTeXTrans demonstrates superior performance compared to mainstream machine translation systems, achieving higher accuracy in translation and better structural integrity in LaTeX documents.", "conclusion": "LaTeXTrans offers a practical solution for translating LaTeX-formatted documents while ensuring content integrity, making it valuable for fields with complex document requirements.", "key_contributions": ["Introduction of a multi-agent system for LaTeX translation", "Enhanced accuracy and structural fidelity in translations", "Collaborative approach incorporating specialized agents for improved outcomes."], "limitations": "", "keywords": ["Machine Translation", "LaTeX", "Multi-agent System", "Natural Language Processing", "Translation Accuracy"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.18819", "pdf": "https://arxiv.org/pdf/2508.18819.pdf", "abs": "https://arxiv.org/abs/2508.18819", "title": "LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection", "authors": ["Shubham Gupta", "Shraban Kumar Chatterjee", "Suman Kundu"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "The proliferation of misinformation in the digital age has led to significant\nsocietal challenges. Existing approaches often struggle with capturing\nlong-range dependencies, complex semantic relations, and the social dynamics\ninfluencing news dissemination. Furthermore, these methods require extensive\nlabelled datasets, making their deployment resource-intensive. In this study,\nwe propose a novel self-supervised misinformation detection framework that\nintegrates both complex semantic relations using Abstract Meaning\nRepresentation (AMR) and news propagation dynamics. We introduce an LLM-based\ngraph contrastive loss (LGCL) that utilizes negative anchor points generated by\na Large Language Model (LLM) to enhance feature separability in a zero-shot\nmanner. To incorporate social context, we employ a multi view graph masked\nautoencoder, which learns news propagation features from social context graph.\nBy combining these semantic and propagation-based features, our approach\neffectively differentiates between fake and real news in a self-supervised\nmanner. Extensive experiments demonstrate that our self-supervised framework\nachieves superior performance compared to other state-of-the-art methodologies,\neven with limited labelled datasets while improving generalizability.", "AI": {"tldr": "This paper presents a self-supervised framework for misinformation detection that integrates semantic relations and news propagation dynamics, utilizing an LLM-based graph contrastive loss for improved feature separability.", "motivation": "To address the challenges of misinformation detection in the presence of long-range dependencies and the need for extensive labeled datasets.", "method": "A novel self-supervised misinformation detection framework combining Abstract Meaning Representation (AMR) and news propagation dynamics, utilizing LLM-based graph contrastive loss and a multi-view graph masked autoencoder.", "result": "The proposed framework outperforms state-of-the-art methodologies in misinformation detection, achieving better performance with limited labeled datasets.", "conclusion": "The self-supervised approach improves generalizability and effectiveness in differentiating between fake and real news.", "key_contributions": ["Integration of semantic relations and news propagation dynamics for misinformation detection.", "Development of LLM-based graph contrastive loss (LGCL) enhancing feature separability.", "Use of multi-view graph masked autoencoder to incorporate social context in learning."], "limitations": "", "keywords": ["misinformation detection", "self-supervised learning", "large language model", "news propagation", "graph contrastive loss"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.18824", "pdf": "https://arxiv.org/pdf/2508.18824.pdf", "abs": "https://arxiv.org/abs/2508.18824", "title": "Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness", "authors": ["Sirui Chen", "Changxin Tian", "Binbin Hu", "Kunlong Chen", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the mathematical reasoning of large language models (LLMs) demands\nhigh-quality training data, yet conventional methods face critical challenges\nin scalability, cost, and data reliability. To address these limitations, we\npropose a novel program-assisted synthesis framework that systematically\ngenerates a high-quality mathematical corpus with guaranteed diversity,\ncomplexity, and correctness. This framework integrates mathematical knowledge\nsystems and domain-specific tools to create executable programs. These programs\nare then translated into natural language problem-solution pairs and vetted by\na bilateral validation mechanism that verifies solution correctness against\nprogram outputs and ensures program-problem consistency. We have generated 12.3\nmillion such problem-solving triples. Experiments demonstrate that models\nfine-tuned on our data significantly improve their inference capabilities,\nachieving state-of-the-art performance on several benchmark datasets and\nshowcasing the effectiveness of our synthesis approach.", "AI": {"tldr": "A framework is proposed for generating high-quality mathematical data to enhance LLM reasoning, overcoming previous limitations in scalability and reliability.", "motivation": "Conventional methods for training LLMs in mathematical reasoning struggle with scalability, cost, and data reliability, necessitating a new approach.", "method": "A novel program-assisted synthesis framework is introduced, which generates diverse, complex, and correct mathematical problem-solution pairs using executable programs and a validation mechanism.", "result": "Models fine-tuned on the generated data achieved state-of-the-art performance on benchmark datasets, indicating significant improvements in their inference capabilities.", "conclusion": "The proposed synthesis approach effectively enhances the mathematical reasoning of LLMs and generates a large corpus of verified problem-solving triples.", "key_contributions": ["Introduction of a program-assisted synthesis framework for LLM training", "Generation of 12.3 million verified problem-solution triples", "Demonstration of significant performance improvements on benchmark datasets"], "limitations": "", "keywords": ["Large Language Models", "Mathematical Reasoning", "Data Synthesis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.18847", "pdf": "https://arxiv.org/pdf/2508.18847.pdf", "abs": "https://arxiv.org/abs/2508.18847", "title": "ConfTuner: Training Large Language Models to Express Their Confidence Verbally", "authors": ["Yibo Li", "Miao Xiong", "Jiaying Wu", "Bryan Hooi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes domains\nsuch as science, law, and healthcare, where accurate expressions of uncertainty\nare essential for reliability and trust. However, current LLMs are often\nobserved to generate incorrect answers with high confidence, a phenomenon known\nas \"overconfidence\". Recent efforts have focused on calibrating LLMs'\nverbalized confidence: i.e., their expressions of confidence in text form, such\nas \"I am 80% confident that...\". Existing approaches either rely on prompt\nengineering or fine-tuning with heuristically generated uncertainty estimates,\nboth of which have limited effectiveness and generalizability. Motivated by the\nnotion of proper scoring rules for calibration in classical machine learning\nmodels, we introduce ConfTuner, a simple and efficient fine-tuning method that\nintroduces minimal overhead and does not require ground-truth confidence scores\nor proxy confidence estimates. ConfTuner relies on a new loss function,\ntokenized Brier score, which we theoretically prove to be a proper scoring\nrule, intuitively meaning that it \"correctly incentivizes the model to report\nits true probability of being correct\". ConfTuner improves calibration across\ndiverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our\nresults further show that better-calibrated confidence enables downstream gains\nin self-correction and model cascade, advancing the development of trustworthy\nLLM systems. The code is available at\nhttps://github.com/liushiliushi/ConfTuner.", "AI": {"tldr": "ConfTuner is a new fine-tuning method for large language models that improves the calibration of verbalized confidence without requiring ground-truth confidence scores.", "motivation": "With LLMs being used in critical fields like healthcare, ensuring accurate expressions of uncertainty is vital for trustworthiness, yet current models often express overconfidence.", "method": "The paper introduces ConfTuner, which utilizes a new loss function called tokenized Brier score to properly incentivize models to accurately reflect their confidence levels without needing ground-truth scores.", "result": "ConfTuner shows improved calibration across various reasoning tasks and enhances performance in self-correction and model cascade contexts, even with black-box models like GPT-4o.", "conclusion": "Better calibration in LLMs leads to increased reliability and has significant implications for trust in AI applications in sensitive domains.", "key_contributions": ["Introduction of ConfTuner for fine-tuning LLMs", "Use of a tokenized Brier score as a new loss function", "Demonstrated generalization to black-box models for improved calibration"], "limitations": "", "keywords": ["Large Language Models", "Confidence Calibration", "Machine Learning", "Health Informatics", "Trustworthy AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.18870", "pdf": "https://arxiv.org/pdf/2508.18870.pdf", "abs": "https://arxiv.org/abs/2508.18870", "title": "ReflectivePrompt: Reflective evolution in autoprompting algorithms", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting.", "AI": {"tldr": "ReflectivePrompt improves autoprompting for language models using evolutionary algorithms, achieving significant performance gains over existing methods.", "motivation": "The paper addresses the need for optimized prompt selection in language models, driven by advancements in prompt engineering and the effectiveness of large language models (LLMs).", "method": "ReflectivePrompt employs a reflective evolution approach with short-term and long-term reflection operations before crossover and elitist mutation, aiming for a more precise search of optimal prompts.", "result": "The method has been tested on 33 datasets and shows an average performance improvement of 28% on certain metrics compared to the previous state-of-the-art, EvoPrompt.", "conclusion": "ReflectivePrompt establishes itself as a leading solution in evolutionary algorithm-based autoprompting, enhancing the prompt selection process significantly.", "key_contributions": ["Introduction of ReflectivePrompt for optimizing prompts in LLMs", "Use of evolutionary algorithms with reflective operations", "Demonstrated significant performance improvements on multiple datasets"], "limitations": "", "keywords": ["Autoprompting", "Evolutionary Algorithms", "Large Language Models", "Prompt Engineering", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.18872", "pdf": "https://arxiv.org/pdf/2508.18872.pdf", "abs": "https://arxiv.org/abs/2508.18872", "title": "Empowering Computing Education Researchers Through LLM-Assisted Content Analysis", "authors": ["Laurie Gale", "Sebastian Mateos Nicolajsen"], "categories": ["cs.CL"], "comment": "7 pages, 2 figures", "summary": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline.", "AI": {"tldr": "This paper proposes LLM-assisted content analysis (LACA) as a method for rigorous analysis of large qualitative datasets in computing education research, enabling more generalizable findings.", "motivation": "Many computing education researchers struggle with conducting rigorous and generalizable research due to limited resources and capacity.", "method": "LACA combines traditional content analysis with large language models to analyze large volumes of textual data efficiently.", "result": "The application of LACA to computing education datasets demonstrates its effectiveness in producing rigorous and reproducible results.", "conclusion": "LACA has the potential to enhance the quality and generalizability of research findings in computing education, advancing both research and practice in the discipline.", "key_contributions": ["Introduction of LLM-assisted content analysis (LACA) method", "Demonstration of LACA using a computing education dataset", "Potential for improving research rigor and generalizability in computing education research"], "limitations": "", "keywords": ["large language models", "content analysis", "computing education research"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.18916", "pdf": "https://arxiv.org/pdf/2508.18916.pdf", "abs": "https://arxiv.org/abs/2508.18916", "title": "Affective Polarization across European Parliaments", "authors": ["Bojan Evkoski", "Igor Mozetič", "Nikola Ljubešić", "Petra Kralj Novak"], "categories": ["cs.CL", "cs.SI"], "comment": "6 pages, 4 figures", "summary": "Affective polarization, characterized by increased negativity and hostility\ntowards opposing groups, has become a prominent feature of political discourse\nworldwide. Our study examines the presence of this type of polarization in a\nselection of European parliaments in a fully automated manner. Utilizing a\ncomprehensive corpus of parliamentary speeches from the parliaments of six\nEuropean countries, we employ natural language processing techniques to\nestimate parliamentarian sentiment. By comparing the levels of negativity\nconveyed in references to individuals from opposing groups versus one's own, we\ndiscover patterns of affectively polarized interactions. The findings\ndemonstrate the existence of consistent affective polarization across all six\nEuropean parliaments. Although activity correlates with negativity, there is no\nobserved difference in affective polarization between less active and more\nactive members of parliament. Finally, we show that reciprocity is a\ncontributing mechanism in affective polarization between parliamentarians\nacross all six parliaments.", "AI": {"tldr": "The study investigates affective polarization in European parliaments through automated sentiment analysis of parliamentary speeches, revealing consistent patterns of negativity towards opposing groups.", "motivation": "To understand the dynamics of affective polarization in political discourse within European parliaments, using automated methods to analyze parliamentary speeches.", "method": "Natural language processing techniques were applied to a comprehensive corpus of parliamentary speeches from six European countries to estimate sentiment and measure affective polarization.", "result": "The study found consistent affective polarization across all six parliaments, with levels of negativity in speeches reflecting an adversarial attitude towards opposing groups.", "conclusion": "The findings underscore the pervasive nature of affective polarization in political discourse, suggesting that reciprocity plays a significant role in shaping interactions between parliamentarians.", "key_contributions": ["Automated analysis of affective polarization in parliamentary discourse", "Introduction of sentiment analysis to measure negativity in political speeches", "Identification of reciprocity as a mechanism in affective polarization"], "limitations": "", "keywords": ["affective polarization", "political discourse", "natural language processing", "sentiment analysis"], "importance_score": 3, "read_time_minutes": 6}}
{"id": "2508.18929", "pdf": "https://arxiv.org/pdf/2508.18929.pdf", "abs": "https://arxiv.org/abs/2508.18929", "title": "Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework", "authors": ["Ilias Driouich", "Hongliu Cao", "Eoin Thomas"], "categories": ["cs.CL", "cs.AI"], "comment": "ECAI 2025 TRUST AI workshop", "summary": "Retrieval-augmented generation (RAG) systems improve large language model\noutputs by incorporating external knowledge, enabling more informed and\ncontext-aware responses. However, the effectiveness and trustworthiness of\nthese systems critically depends on how they are evaluated, particularly on\nwhether the evaluation process captures real-world constraints like protecting\nsensitive information. While current evaluation efforts for RAG systems have\nprimarily focused on the development of performance metrics, far less attention\nhas been given to the design and quality of the underlying evaluation datasets,\ndespite their pivotal role in enabling meaningful, reliable assessments. In\nthis work, we introduce a novel multi-agent framework for generating synthetic\nQA datasets for RAG evaluation that prioritize semantic diversity and privacy\npreservation. Our approach involves: (1) a Diversity agent leveraging\nclustering techniques to maximize topical coverage and semantic variability,\n(2) a Privacy Agent that detects and mask sensitive information across multiple\ndomains and (3) a QA curation agent that synthesizes private and diverse QA\npairs suitable as ground truth for RAG evaluation. Extensive experiments\ndemonstrate that our evaluation sets outperform baseline methods in diversity\nand achieve robust privacy masking on domain-specific datasets. This work\noffers a practical and ethically aligned pathway toward safer, more\ncomprehensive RAG system evaluation, laying the foundation for future\nenhancements aligned with evolving AI regulations and compliance standards.", "AI": {"tldr": "This paper presents a multi-agent framework for generating synthetic QA datasets aimed at improving the evaluation of retrieval-augmented generation (RAG) systems, focusing on diversity and privacy.", "motivation": "To enhance the evaluation processes for RAG systems by ensuring that they are more representative of real-world constraints, especially in terms of privacy and diversity.", "method": "The method involves three agents: a Diversity agent for maximizing semantic variability, a Privacy agent for masking sensitive information, and a QA curation agent for synthesizing diverse QA pairs.", "result": "Experiments show that the evaluation sets generated using this framework outperform traditional methods in terms of diversity and effectively mask sensitive data.", "conclusion": "The proposed framework provides a robust and ethically sound approach to RAG system evaluation, addressing privacy concerns and setting a foundation for future improvements in compliance with AI regulations.", "key_contributions": ["Development of a multi-agent framework for QA dataset generation", "Focus on semantic diversity and privacy preservation", "Demonstrated effectiveness of datasets in RAG evaluation"], "limitations": "", "keywords": ["retrieval-augmented generation", "privacy preservation", "synthetic QA datasets"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.18988", "pdf": "https://arxiv.org/pdf/2508.18988.pdf", "abs": "https://arxiv.org/abs/2508.18988", "title": "Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models", "authors": ["Hung Ming Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 9 figures. The AI Intuition Explorer dashboard is available\n  at: https://cyrilliu1974.github.io/github.io/vi.html", "summary": "We present a framework where neural models develop an AI Mother Tongue, a\nnative symbolic language that simultaneously supports intuitive reasoning,\ncompositional symbol chains, and inherent interpretability. Unlike post-hoc\nexplanation methods, our approach embeds reasoning directly into the model's\nrepresentations: symbols capture meaningful semantic patterns, chains trace\ndecision paths, and gated induction mechanisms guide selective focus, yielding\ntransparent yet flexible reasoning. We introduce complementary training\nobjectives to enhance symbol purity and decision sparsity, and employ a\nsequential specialization strategy to first build broad symbolic competence and\nthen refine intuitive judgments. Experiments on AI tasks demonstrate\ncompetitive accuracy alongside verifiable reasoning traces, showing that AI\nMother Tongue can serve as a unified mechanism for interpretability, intuition,\nand symbolic reasoning in neural models.", "AI": {"tldr": "A framework for neural models to develop an AI Mother Tongue, enabling intuitive reasoning and interpretability through symbolic language.", "motivation": "To integrate reasoning into neural models in a more interpretable and intuitive manner than existing methods.", "method": "The framework embeds reasoning into model representations using symbols for semantic patterns and gated induction for selective focus, supported by complementary training objectives.", "result": "Experiments show competitive accuracy in AI tasks and verifiable reasoning traces, indicating the effectiveness of the AI Mother Tongue in symbolic reasoning.", "conclusion": "The AI Mother Tongue framework provides a unified approach for enhancing interpretability, intuition, and symbolic reasoning in neural models.", "key_contributions": ["Development of a novel AI Mother Tongue framework for symbolism in neural models.", "Introduction of complementary training objectives for symbol purity and decision sparsity.", "Demonstrated effectiveness through experiments showcasing accuracy and reasoning transparency."], "limitations": "", "keywords": ["neural models", "symbolic reasoning", "interpretability"], "importance_score": 7, "read_time_minutes": 25}}
{"id": "2508.18992", "pdf": "https://arxiv.org/pdf/2508.18992.pdf", "abs": "https://arxiv.org/abs/2508.18992", "title": "Automatic Prompt Optimization with Prompt Distillation", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.", "AI": {"tldr": "DistillPrompt is a novel autoprompting method that optimizes prompts for language models, showing significant improvements in text classification and generation tasks.", "motivation": "There is a growing interest in prompt engineering driven by research in large language models (LLMs), necessitating optimized prompt selection for improved performance.", "method": "DistillPrompt employs a multi-stage integration of task-specific information into prompts through the use of distillation, compression, and aggregation operations to explore the prompt space.", "result": "Tests showed an average performance improvement of 20.12% compared to existing methods such as Grips on various datasets for text classification and generation.", "conclusion": "DistillPrompt demonstrates effectiveness as a non-gradient approach to autoprompting, outperforming existing methods in key metrics.", "key_contributions": ["Introduction of a novel autoprompting method", "Significant improvement in prompt optimization for language models", "Demonstration of effectiveness across various datasets"], "limitations": "", "keywords": ["autoprompting", "large language models", "prompt engineering", "text classification", "text generation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.19026", "pdf": "https://arxiv.org/pdf/2508.19026.pdf", "abs": "https://arxiv.org/abs/2508.19026", "title": "MovieCORE: COgnitive REasoning in Movies", "authors": ["Gueter Josmy Faure", "Min-Hung Chen", "Jia-Fong Yeh", "Ying Cheng", "Hung-Ting Su", "Yung-Hao Tang", "Shang-Hong Lai", "Winston H. Hsu"], "categories": ["cs.CL"], "comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html", "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.", "AI": {"tldr": "MovieCORE is a novel video question answering dataset aimed at deeper cognitive understanding of movie content, utilizing large language models for question generation and evaluation.", "motivation": "To enhance the capabilities of video question answering (VQA) systems by focusing on deeper cognitive processing rather than surface-level comprehension of movies.", "method": "Introduces the MovieCORE dataset and employs large language models to generate and refine question-answer pairs, alongside a set of cognitive tests to evaluate dataset quality and model performance.", "result": "The dataset allows for probing deeper cognitive understanding and the new Agentic Choice Enhancement (ACE) module improves reasoning capabilities of VQA models by up to 25%.", "conclusion": "MovieCORE advances movie understanding in AI and provides insights into the performance of VQA models on more nuanced cinematic questions.", "key_contributions": ["Novel video question answering dataset designed for deeper cognitive engagement.", "Innovative usage of large language models as thought agents for question generation.", "Agentic Choice Enhancement module that significantly improves reasoning capabilities of VQA systems."], "limitations": "", "keywords": ["Video Question Answering", "Large Language Models", "Cognitive Understanding"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.19076", "pdf": "https://arxiv.org/pdf/2508.19076.pdf", "abs": "https://arxiv.org/abs/2508.19076", "title": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance", "authors": ["Ziyue Li", "Yuan Chang", "Gaihong Yu", "Xiaoqiu Le"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components.", "AI": {"tldr": "HiPlan is a hierarchical planning framework designed to enhance decision-making in large language model agents by providing structured guidance and support for complex, long-horizon tasks.", "motivation": "LLM-based agents struggle with complex decision-making tasks due to a lack of guidance and oversight, leading to disorientation and execution failures.", "method": "HiPlan decomposes tasks into milestone action guides and step-wise hints; it constructs a milestone library from expert demonstrations during the offline phase and adapts trajectory segments during execution to align with milestone objectives.", "result": "HiPlan shows significant improvements over strong baselines in two challenging benchmarks, demonstrating enhanced decision-making capabilities of LLM agents.", "conclusion": "The hierarchical components of HiPlan offer complementary benefits, improving the overall performance of LLM-based agents in complex planning scenarios.", "key_contributions": ["Introduction of HiPlan framework for LLM decision-making", "Development of a milestone library from expert demonstrations", "Dynamic adaptation of past trajectory segments for real-time guidance"], "limitations": "", "keywords": ["Hierarchical Planning", "LLMs", "Decision-Making", "Task Decomposition", "Adaptive Guidance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.19077", "pdf": "https://arxiv.org/pdf/2508.19077.pdf", "abs": "https://arxiv.org/abs/2508.19077", "title": "\"Where does it hurt?\" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues", "authors": ["Tom Röhr", "Soumyadeep Roy", "Fares Al Mohamad", "Jens-Michalis Papaioannou", "Wolfgang Nejdl", "Felix Gers", "Alexander Löser"], "categories": ["cs.CL"], "comment": "Accepted at ECAI 2025", "summary": "In a doctor-patient dialogue, the primary objective of physicians is to\ndiagnose patients and propose a treatment plan. Medical doctors guide these\nconversations through targeted questioning to efficiently gather the\ninformation required to provide the best possible outcomes for patients. To the\nbest of our knowledge, this is the first work that studies physician intent\ntrajectories in doctor-patient dialogues. We use the `Ambient Clinical\nIntelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with\nmedical professionals to develop a fine-grained taxonomy of physician intents\nbased on the SOAP framework (Subjective, Objective, Assessment, and Plan). We\nthen conduct a large-scale annotation effort to label over 5000 doctor-patient\nturns with the help of a large number of medical experts recruited using\nProlific, a popular crowd-sourcing platform. This large labeled dataset is an\nimportant resource contribution that we use for benchmarking the\nstate-of-the-art generative and encoder models for medical intent\nclassification tasks. Our findings show that our models understand the general\nstructure of medical dialogues with high accuracy, but often fail to identify\ntransitions between SOAP categories. We also report for the first time common\ntrajectories in medical dialogue structures that provide valuable insights for\ndesigning `differential diagnosis' systems. Finally, we extensively study the\nimpact of intent filtering for medical dialogue summarization and observe a\nsignificant boost in performance. We make the codes and data, including\nannotation guidelines, publicly available at\nhttps://github.com/DATEXIS/medical-intent-classification.", "AI": {"tldr": "This paper studies physician intent trajectories in doctor-patient dialogues using the Aci-bench dataset and develops a taxonomy based on the SOAP framework, contributing a large labeled dataset for intent classification.", "motivation": "The primary goal in doctor-patient dialogues is to effectively gather information for diagnosis and treatment planning, highlighting a need to understand physician intents.", "method": "A fine-grained taxonomy of physician intents was developed based on the SOAP framework, followed by labeling over 5000 dialogue turns in collaboration with medical experts from the Aci-bench dataset.", "result": "Models showed high accuracy in understanding medical dialogues but struggled with transitions between SOAP categories; common trajectories in dialogue structures were identified.", "conclusion": "The study contributes significantly to medical dialogue systems and demonstrates the importance of intent filtering for dialogue summarization, improving performance.", "key_contributions": ["First study of physician intent trajectories in dialogues", "Development of a taxonomy based on the SOAP framework", "Large labeled dataset for benchmarking medical intent classification"], "limitations": "Models often fail to identify transitions between SOAP categories, indicating a gap in understanding dialogue transitions.", "keywords": ["physician intent", "doctor-patient dialogue", "medical classification", "SOAP framework", "dialogue summarization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.19089", "pdf": "https://arxiv.org/pdf/2508.19089.pdf", "abs": "https://arxiv.org/abs/2508.19089", "title": "It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs", "authors": ["Yue Li", "Zhixue Zhao", "Carolina Scarton"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025", "summary": "Extremely low-resource languages, especially those written in rare scripts,\nas shown in Figure 1, remain largely unsupported by large language models\n(LLMs). This is due in part to compounding factors such as the lack of training\ndata. This paper delivers the first comprehensive analysis of whether LLMs can\nacquire such languages purely via in-context learning (ICL), with or without\nauxiliary alignment signals, and how these methods compare to\nparameter-efficient fine-tuning (PEFT). We systematically evaluate 20\nunder-represented languages across three state-of-the-art multilingual LLMs.\nOur findings highlight the limitation of PEFT when both language and its script\nare extremely under-represented by the LLM. In contrast, zero-shot ICL with\nlanguage alignment is impressively effective on extremely low-resource\nlanguages, while few-shot ICL or PEFT is more beneficial for languages\nrelatively better represented by LLMs. For LLM practitioners working on\nextremely low-resource languages, we summarise guidelines grounded by our\nresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning\na multilingual model on languages of unseen scripts.", "AI": {"tldr": "This paper analyzes the effectiveness of LLMs in acquiring extremely low-resource languages through in-context learning versus parameter-efficient fine-tuning.", "motivation": "The need to support extremely low-resource languages, which are often written in rare scripts and lack sufficient training data for large language models (LLMs).", "method": "Systematic evaluation of 20 under-represented languages across three state-of-the-art multilingual LLMs, comparing in-context learning and parameter-efficient fine-tuning methods.", "result": "Zero-shot in-context learning with language alignment shows high effectiveness for extremely low-resource languages, while parameter-efficient fine-tuning is limited in such contexts.", "conclusion": "Guidelines for adapting LLMs to low-resource languages are provided, emphasizing the avoidance of fine-tuning on languages with unseen scripts.", "key_contributions": ["Comprehensive analysis of LLM adaptation for low-resource languages", "Evaluation of zero-shot in-context learning vs parameter-efficient fine-tuning", "Guidelines for LLM practitioners on handling low-resource languages"], "limitations": "The analysis may not cover all languages or scripts, and the effectiveness of the methods may vary across different contexts.", "keywords": ["low-resource languages", "in-context learning", "parameter-efficient fine-tuning", "large language models", "language alignment"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.19093", "pdf": "https://arxiv.org/pdf/2508.19093.pdf", "abs": "https://arxiv.org/abs/2508.19093", "title": "Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index", "authors": ["Mathew Henrickson"], "categories": ["cs.CL"], "comment": null, "summary": "This research presents a Retrieval-Augmented Generation (RAG) framework for\nart provenance studies, focusing on the Getty Provenance Index. Provenance\nresearch establishes the ownership history of artworks, which is essential for\nverifying authenticity, supporting restitution and legal claims, and\nunderstanding the cultural and historical context of art objects. The process\nis complicated by fragmented, multilingual archival data that hinders efficient\nretrieval. Current search portals require precise metadata, limiting\nexploratory searches. Our method enables natural-language and multilingual\nsearches through semantic retrieval and contextual summarization, reducing\ndependence on metadata structures. We assess RAG's capability to retrieve and\nsummarize auction records using a 10,000-record sample from the Getty\nProvenance Index - German Sales. The results show this approach provides a\nscalable solution for navigating art market archives, offering a practical tool\nfor historians and cultural heritage professionals conducting historically\nsensitive research.", "AI": {"tldr": "This paper introduces a Retrieval-Augmented Generation (RAG) framework for enhancing searches in art provenance studies, specifically utilizing the Getty Provenance Index.", "motivation": "The study aims to address the challenges faced in provenance research due to fragmented and multilingual archival data, which complicates the retrieval of ownership histories of artworks.", "method": "The proposed framework facilitates natural-language and multilingual searches through semantic retrieval and contextual summarization, thereby minimizing reliance on structured metadata.", "result": "The framework was evaluated using a sample of 10,000 records from the Getty Provenance Index, demonstrating its effective retrieval and summarization of auction records, providing a scalable solution for art market archive navigation.", "conclusion": "This RAG approach offers a valuable resource for historians and cultural heritage professionals by simplifying the search process in historically sensitive research contexts.", "key_contributions": ["Introduction of a RAG framework for art provenance studies", "Enabling multilingual searches through semantic retrieval", "Providing summarization of fragmented archival data"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "art provenance", "semantic retrieval", "multilingual searches", "cultural heritage"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.19099", "pdf": "https://arxiv.org/pdf/2508.19099.pdf", "abs": "https://arxiv.org/abs/2508.19099", "title": "Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic", "authors": ["Thomas Compton"], "categories": ["cs.CL"], "comment": "5 pages conference paper, 4 tables", "summary": "Quantitative Discourse Analysis has seen growing adoption with the rise of\nLarge Language Models and computational tools. However, reliance on black box\nsoftware such as MAXQDA and NVivo risks undermining methodological transparency\nand alignment with research goals. This paper presents a hybrid, transparent\nframework for QDA that combines lexical and semantic methods to enable\ntriangulation, reproducibility, and interpretability. Drawing from a case study\nin historical political discourse, we demonstrate how custom Python pipelines\nusing NLTK, spaCy, and Sentence Transformers allow fine-grained control over\npreprocessing, lemmatisation, and embedding generation. We further detail our\niterative BERTopic modelling process, incorporating UMAP dimensionality\nreduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised\nthrough parameter tuning and multiple runs to enhance topic coherence and\ncoverage. By juxtaposing precise lexical searches with context-aware semantic\nclustering, we argue for a multi-layered approach that mitigates the\nlimitations of either method in isolation. Our workflow underscores the\nimportance of code-level transparency, researcher agency, and methodological\ntriangulation in computational discourse studies. Code and supplementary\nmaterials are available via GitHub.", "AI": {"tldr": "The paper presents a transparent hybrid framework for Quantitative Discourse Analysis (QDA) that integrates lexical and semantic methods, focusing on methodological transparency and researcher agency using custom Python pipelines.", "motivation": "To address the methodological limitations of black box software in Quantitative Discourse Analysis and enhance transparency and reproducibility.", "method": "The paper uses custom Python pipelines with NLTK, spaCy, and Sentence Transformers for preprocessing, lemmatisation, and embedding generation, and details a multi-layered iterative BERTopic modelling process employing UMAP, HDBSCAN, and c-TF-IDF keyword extraction, optimized through parameter tuning.", "result": "The proposed framework allows for fine-grained control over data processing, leading to improved topic coherence and the ability to triangulate lexical and semantic analyses.", "conclusion": "The authors emphasize the importance of transparency and researcher agency in computational discourse analysis through a multi-layered approach that enhances interpretability and reproducibility.", "key_contributions": ["Hybrid QDA framework combining lexical and semantic methods.", "Custom Python pipelines facilitating transparency in data processing.", "Iterative BERTopic modelling process with enhanced topic coherence."], "limitations": "The paper does not explore the potential scalability issues of the proposed framework in larger datasets.", "keywords": ["Quantitative Discourse Analysis", "Large Language Models", "Methodological Transparency"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.19111", "pdf": "https://arxiv.org/pdf/2508.19111.pdf", "abs": "https://arxiv.org/abs/2508.19111", "title": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs", "authors": ["Zhikai Ding", "Shiyu Ni", "Keping Bi"], "categories": ["cs.CL"], "comment": "EMNLP2025 Findings", "summary": "Large vision-language models (LVLMs) demonstrate strong visual question\nanswering (VQA) capabilities but are shown to hallucinate. A reliable model\nshould perceive its knowledge boundaries-knowing what it knows and what it does\nnot. This paper investigates LVLMs' perception of their knowledge boundaries by\nevaluating three types of confidence signals: probabilistic confidence, answer\nconsistency-based confidence, and verbalized confidence. Experiments on three\nLVLMs across three VQA datasets show that, although LVLMs possess a reasonable\nperception level, there is substantial room for improvement. Among the three\nconfidences, probabilistic and consistency-based signals are more reliable\nindicators, while verbalized confidence often leads to overconfidence. To\nenhance LVLMs' perception, we adapt several established confidence calibration\nmethods from Large Language Models (LLMs) and propose three effective methods.\nAdditionally, we compare LVLMs with their LLM counterparts, finding that\njointly processing visual and textual inputs decreases question-answering\nperformance but reduces confidence, resulting in an improved perception level\ncompared to LLMs.", "AI": {"tldr": "This paper evaluates the perception of knowledge boundaries in large vision-language models and proposes methods to enhance their confidence estimation in visual question answering tasks.", "motivation": "To address the hallucination issue in large vision-language models (LVLMs) and improve their ability to recognize their knowledge limitations when answering questions about images.", "method": "The study evaluates three confidence signals in LVLMs: probabilistic confidence, answer consistency-based confidence, and verbalized confidence, and adapts several confidence calibration methods from large language models.", "result": "Experiments show LVLMs have a reasonable perception level regarding their knowledge boundaries, but there is substantial room for improvement. Probabilistic and consistency-based confidences are better indicators than verbalized confidence.", "conclusion": "Adapting confidence calibration methods from LLMs can enhance LVLMs' perception of their knowledge boundaries, leading to better performance in visual question answering tasks.", "key_contributions": ["Evaluation of three types of confidence signals in LVLMs", "Adaptation of confidence calibration methods from LLMs for LVLMs", "Comparison of LVLMs’ performance and confidence levels with LLMs."], "limitations": "LVLMs still have significant room for improvement in knowledge boundary perception despite reasonable performance levels.", "keywords": ["vision-language models", "confidence calibration", "visual question answering", "machine learning", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.19202", "pdf": "https://arxiv.org/pdf/2508.19202.pdf", "abs": "https://arxiv.org/abs/2508.19202", "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning", "authors": ["Alan Li", "Yixin Liu", "Arpan Sarkar", "Doug Downey", "Arman Cohan"], "categories": ["cs.CL"], "comment": "28 pages, 16 figures", "summary": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning.", "AI": {"tldr": "This paper introduces SciReas, a benchmark suite for scientific reasoning tasks in LLMs, along with an in-depth analysis through the KRUX framework that uncovers the relationship between knowledge retrieval and reasoning abilities.", "motivation": "To address the lack of a holistic benchmark for evaluating scientific reasoning in LLMs and to understand the distinct roles of knowledge and reasoning in scientific problem solving.", "method": "The study introduces SciReas and SciReas-Pro benchmarks for scientific reasoning and employs the KRUX framework to evaluate and analyze the performance of LLMs on these benchmarks.", "result": "Key findings include that retrieving relevant knowledge is a major bottleneck for LLMs, and that external knowledge can significantly enhance reasoning models. Moreover, enhancing verbal reasoning improves LLMs' knowledge retrieval capabilities.", "conclusion": "The analysis provides actionable insights into improving LLMs for scientific reasoning tasks and highlights crucial areas for further research.", "key_contributions": ["Introduction of holistic benchmarks for scientific reasoning in LLMs", "Development of the KRUX framework for analyzing knowledge and reasoning roles", "Release of SciLit01 as a strong baseline for scientific reasoning tasks."], "limitations": "", "keywords": ["scientific reasoning", "language models", "knowledge retrieval", "reasoning frameworks", "benchmarking"], "importance_score": 9, "read_time_minutes": 28}}
{"id": "2508.19205", "pdf": "https://arxiv.org/pdf/2508.19205.pdf", "abs": "https://arxiv.org/abs/2508.19205", "title": "VibeVoice Technical Report", "authors": ["Zhiliang Peng", "Jianwei Yu", "Wenhui Wang", "Yaoyao Chang", "Yutao Sun", "Li Dong", "Yi Zhu", "Weijiang Xu", "Hangbo Bao", "Zehua Wang", "Shaohan Huang", "Yan Xia", "Furu Wei"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "This report presents VibeVoice, a novel model designed to synthesize\nlong-form speech with multiple speakers by employing next-token diffusion,\nwhich is a unified method for modeling continuous data by autoregressively\ngenerating latent vectors via diffusion. To enable this, we introduce a novel\ncontinuous speech tokenizer that, when compared to the popular Encodec model,\nimproves data compression by 80 times while maintaining comparable performance.\nThe tokenizer effectively preserves audio fidelity while significantly boosting\ncomputational efficiency for processing long sequences. Thus, VibeVoice can\nsynthesize long-form speech for up to 90 minutes (in a 64K context window\nlength) with a maximum of 4 speakers, capturing the authentic conversational\n``vibe'' and surpassing open-source and proprietary dialogue models.", "AI": {"tldr": "VibeVoice is a novel model for synthesizing long-form speech with multiple speakers using next-token diffusion and an efficient continuous speech tokenizer.", "motivation": "To synthesize long-form speech with multiple speakers effectively and efficiently.", "method": "Utilizes next-token diffusion for autoregressive generation of latent vectors, combined with a new continuous speech tokenizer for improved data compression.", "result": "VibeVoice synthesizes long-form speech for up to 90 minutes while achieving 80 times better data compression than the Encodec model, maintaining audio fidelity.", "conclusion": "VibeVoice surpasses both open-source and proprietary dialogue models in synthesizing authentic conversational vibes.", "key_contributions": ["Introduction of a novel next-token diffusion model for speech synthesis", "Development of a continuous speech tokenizer that improves data compression by 80 times", "Ability to synthesize speech for 90 minutes with multiple speakers."], "limitations": "", "keywords": ["speech synthesis", "next-token diffusion", "continuous speech tokenizer", "data compression", "multispeaker synthesis"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.19221", "pdf": "https://arxiv.org/pdf/2508.19221.pdf", "abs": "https://arxiv.org/abs/2508.19221", "title": "Evaluating the Evaluators: Are readability metrics good measures of readability?", "authors": ["Isabel Cachola", "Daniel Khashabi", "Mark Dredze"], "categories": ["cs.CL"], "comment": null, "summary": "Plain Language Summarization (PLS) aims to distill complex documents into\naccessible summaries for non-expert audiences. In this paper, we conduct a\nthorough survey of PLS literature, and identify that the current standard\npractice for readability evaluation is to use traditional readability metrics,\nsuch as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in\nother fields, these metrics have not been compared to human readability\njudgments in PLS. We evaluate 8 readability metrics and show that most\ncorrelate poorly with human judgments, including the most popular metric, FKGL.\nWe then show that Language Models (LMs) are better judges of readability, with\nthe best-performing model achieving a Pearson correlation of 0.56 with human\njudgments. Extending our analysis to PLS datasets, which contain summaries\naimed at non-expert audiences, we find that LMs better capture deeper measures\nof readability, such as required background knowledge, and lead to different\nconclusions than the traditional metrics. Based on these findings, we offer\nrecommendations for best practices in the evaluation of plain language\nsummaries. We release our analysis code and survey data.", "AI": {"tldr": "This paper surveys Plain Language Summarization (PLS) and evaluates traditional readability metrics against human judgments, finding that Language Models (LMs) provide better readability assessments for non-expert audiences.", "motivation": "To improve the evaluation of Plain Language Summarization (PLS) quality and accessibility for non-expert audiences by investigating readability metrics.", "method": "A thorough survey of existing PLS literature, comparison of 8 traditional readability metrics with human judgments, and evaluation of Language Models for readability assessment.", "result": "Most traditional readability metrics correlate poorly with human judgments; Language Models outperform traditional metrics in evaluating readability of plain language summaries.", "conclusion": "Language Models are recommended over traditional metrics for assessing readability in PLS, and best practices for evaluation are proposed based on this study's findings.", "key_contributions": ["Identification of poor correlation between traditional metrics and human readability judgments.", "Demonstration of Language Models' superior performance in assessing readability.", "Recommendations for best practices in evaluating plain language summaries."], "limitations": "The study is limited to the readability metrics tested; further research is needed to explore additional metrics and contexts.", "keywords": ["Plain Language Summarization", "readability evaluation", "Language Models", "human judgments", "readability metrics"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.19227", "pdf": "https://arxiv.org/pdf/2508.19227.pdf", "abs": "https://arxiv.org/abs/2508.19227", "title": "Generative Interfaces for Language Models", "authors": ["Jiaqi Chen", "Yanzhe Zhang", "Yutong Zhang", "Yijia Shao", "Diyi Yang"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.", "AI": {"tldr": "This paper introduces Generative Interfaces for Language Models that create adaptive user interfaces to enhance multi-turn conversations and improve user experience compared to traditional chat-based systems.", "motivation": "To overcome the limitations of linear request-response formats in interactions with large language models, particularly in multi-turn and exploratory tasks.", "method": "The framework utilizes structured interface-specific representations and iterative refinements to translate user queries into task-specific user interfaces for interaction.", "result": "Generative interfaces were shown to outperform traditional chat-based interfaces, with user preference favoring the new paradigm in over 70% of the cases.", "conclusion": "The study indicates that generative interfaces not only enhance user experience but also provide valuable insights into user preferences, paving the way for future advancements in human-AI interaction.", "key_contributions": ["Introduction of Generative Interfaces for Language Models", "Development of a multidimensional assessment framework for comparing interaction systems", "Insights on user preference for generative interfaces over traditional chat-based interfaces"], "limitations": "", "keywords": ["generative interfaces", "human-AI interaction", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.04876", "pdf": "https://arxiv.org/pdf/2406.04876.pdf", "abs": "https://arxiv.org/abs/2406.04876", "title": "HateDebias: On the Diversity and Variability of Hate Speech Debiasing", "authors": ["Hongyan Wu", "Zhengming Chen", "Zijian Li", "Nankai Lin", "Lianxi Wang", "Shengyi Jiang", "Aimin Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Hate speech frequently appears on social media platforms and urgently needs\nto be effectively controlled. Alleviating the bias caused by hate speech can\nhelp resolve various ethical issues. Although existing research has constructed\nseveral datasets for hate speech detection, these datasets seldom consider the\ndiversity and variability of bias, making them far from real-world scenarios.\nTo fill this gap, we propose a benchmark HateDebias to analyze the fairness of\nmodels under dynamically evolving environments. Specifically, to meet the\ndiversity of biases, we collect hate speech data with different types of biases\nfrom real-world scenarios. To further simulate the variability in the\nreal-world scenarios(i.e., the changing of bias attributes in datasets), we\nconstruct a dataset to follow the continuous learning setting and evaluate the\ndetection accuracy of models on the HateDebias, where performance degradation\nindicates a significant bias toward a specific attribute. To provide a\npotential direction, we further propose a continual debiasing framework\ntailored to dynamic bias in real-world scenarios, integrating memory replay and\nbias information regularization to ensure the fairness of the model. Experiment\nresults on the HateDebias benchmark reveal that our methods achieve improved\nperformance in mitigating dynamic biases in real-world scenarios, highlighting\nthe practicality in real-world applications.", "AI": {"tldr": "This paper introduces HateDebias, a benchmark for assessing hate speech detection models under diverse and evolving biases. It proposes a continual debiasing framework that incorporates memory replay to address performance degradation due to changing biases in real-world scenarios.", "motivation": "The need to effectively control hate speech on social media platforms and the ethical implications of bias in hate speech detection.", "method": "The authors collected hate speech data reflecting various biases from real-world scenarios and created a dataset suitable for continuous learning. They evaluated detection accuracy and proposed a continual debiasing framework integrating memory replay and bias regularization.", "result": "Experiments revealed that the proposed methods improved performance in mitigating dynamic biases, highlighting their practicality for real-world applications.", "conclusion": "The HateDebias benchmark and the continual debiasing framework provide valuable tools for evaluating and addressing bias in hate speech detection models, contributing to fairer AI applications.", "key_contributions": ["Introduction of the HateDebias benchmark for hate speech detection;", "Development of a continual debiasing framework to address dynamic biases;", "Demonstration of improved performance in real-world scenarios."], "limitations": "", "keywords": ["hate speech", "bias", "machine learning", "fairness", "continual learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2407.06866", "pdf": "https://arxiv.org/pdf/2407.06866.pdf", "abs": "https://arxiv.org/abs/2407.06866", "title": "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "authors": ["Victoria R. Li", "Yida Chen", "Naomi Saphra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While the biases of language models in production are extensively documented,\nthe biases of their guardrails have been neglected. This paper studies how\ncontextual information about the user influences the likelihood of an LLM to\nrefuse to execute a request. By generating user biographies that offer\nideological and demographic information, we find a number of biases in\nguardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas\nare more likely to trigger a refusal guardrail when requesting censored or\nillegal information. Guardrails are also sycophantic, refusing to comply with\nrequests for a political position the user is likely to disagree with. We find\nthat certain identity groups and seemingly innocuous information, e.g., sports\nfandom, can elicit changes in guardrail sensitivity similar to direct\nstatements of political ideology. For each demographic category and even for\nAmerican football team fandom, we find that ChatGPT appears to infer a likely\npolitical ideology and modify guardrail behavior accordingly.", "AI": {"tldr": "The paper examines biases in language model guardrails, revealing how user demographics affect LLM refusal rates for requests.", "motivation": "To investigate the often-neglected biases in guardrails of language models, particularly in how user demographics influence response tendencies.", "method": "User biographies were generated with different ideological and demographic attributes to assess guardrail sensitivity in GPT-3.5.", "result": "Certain demographics, such as younger, female, and Asian-American users, experienced higher rates of refusal for specific requests; guardrails also reflected political biases based on inferred user ideology.", "conclusion": "LLM guardrails exhibit sensitivity to user identity, significantly impacting compliance with user requests based on demographic attributes and perceived political stance.", "key_contributions": ["Identified bias in guardrail response based on user demographics", "Showed that sports fandom can influence guardrail behavior", "Demonstrated that guardrails infer user ideology based on contextual information"], "limitations": "", "keywords": ["language models", "guardrails", "bias", "GPT-3.5", "demographics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2408.05873", "pdf": "https://arxiv.org/pdf/2408.05873.pdf", "abs": "https://arxiv.org/abs/2408.05873", "title": "Recognizing Limits: Investigating Infeasibility in Large Language Models", "authors": ["Wenbo Zhang", "Zihang Xu", "Hengrui Cai"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Large language models (LLMs) have shown remarkable performance in various\ntasks but often fail to handle queries that exceed their knowledge and\ncapabilities, leading to incorrect or fabricated responses. This paper\naddresses the need for LLMs to recognize and refuse infeasible tasks due to the\nrequests surpassing their capabilities. We conceptualize four main categories\nof infeasible tasks for LLMs, which cover a broad spectrum of\nhallucination-related challenges identified in prior literature. We develop and\nbenchmark a new dataset comprising diverse infeasible and feasible tasks to\nevaluate multiple LLMs' abilities to decline infeasible tasks. Furthermore, we\nexplore the potential of increasing LLMs' refusal capabilities with\nfine-tuning. Our experiments validate the effectiveness of the trained models,\nsuggesting promising directions for improving the performance of LLMs in\nreal-world applications.", "AI": {"tldr": "This paper addresses how large language models (LLMs) can be improved to recognize and refuse infeasible tasks to reduce incorrect or fabricated responses.", "motivation": "The paper highlights the challenge of LLMs failing to handle queries that exceed their capabilities, leading to hallucinations and false information.", "method": "Conceptualization of four categories of infeasible tasks; development of a new dataset for evaluation; benchmark testing on multiple LLMs and exploration of fine-tuning techniques.", "result": "The experiments demonstrate that fine-tuning can effectively enhance LLMs' ability to decline infeasible tasks, showing promise for real-world application improvements.", "conclusion": "Improving LLMs’ refusal capabilities can significantly enhance their reliability and usability in practical contexts.", "key_contributions": ["Identification of four main categories of infeasible tasks for LLMs.", "Development of a diverse dataset for benchmarking LLM capabilities.", "Evaluation of fine-tuning methods to increase refusal capabilities of LLMs."], "limitations": "", "keywords": ["large language models", "infeasible tasks", "fine-tuning", "evaluation dataset", "real-world applications"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.19195", "pdf": "https://arxiv.org/pdf/2410.19195.pdf", "abs": "https://arxiv.org/abs/2410.19195", "title": "Label Set Optimization via Activation Distribution Kurtosis for Zero-shot Classification with Generative Models", "authors": ["Yue Li", "Zhixue Zhao", "Carolina Scarton"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025", "summary": "In-context learning (ICL) performance is highly sensitive to prompt design,\nyet the impact of class label options (e.g. lexicon or order) in zero-shot\nclassification remains underexplored. This study proposes LOADS (Label set\nOptimization via Activation Distribution kurtosiS), a post-hoc method for\nselecting optimal label sets in zero-shot ICL with large language models\n(LLMs). LOADS is built upon the observations in our empirical analysis, the\nfirst to systematically examine how label option design (i.e., lexical choice,\norder, and elaboration) impacts classification performance. This analysis shows\nthat the lexical choice of the labels in the prompt (such as agree vs. support\nin stance classification) plays an important role in both model performance and\nmodel's sensitivity to the label order. A further investigation demonstrates\nthat optimal label words tend to activate fewer outlier neurons in LLMs'\nfeed-forward networks. LOADS then leverages kurtosis to measure the neuron\nactivation distribution for label selection, requiring only a single forward\npass without gradient propagation or labelled data. The LOADS-selected label\nwords consistently demonstrate effectiveness for zero-shot ICL across\nclassification tasks, datasets, models and languages, achieving maximum\nperformance gain from 0.54 to 0.76 compared to the conventional approach of\nusing original dataset label words.", "AI": {"tldr": "This paper introduces LOADS, a method for optimizing label sets in zero-shot in-context learning (ICL) using large language models, focusing on the impact of label design on classification performance.", "motivation": "The study addresses the underexplored area of how class label options affect zero-shot classification in in-context learning, emphasizing the significance of prompt design.", "method": "LOADS uses kurtosis to measure neuron activation distributions for selecting optimal label sets, requiring only one forward pass without the need for labeled data.", "result": "LOADS selected label words led to performance improvements in zero-shot ICL, with gains from 0.54 to 0.76 compared to conventional label words across various tasks and models.", "conclusion": "The findings highlight the importance of label design in zero-shot classification, demonstrating that proper label selection can enhance large language models' performance significantly.", "key_contributions": ["Introduction of LOADS for label set optimization", "Empirical analysis of the effects of label design on ICL", "Demonstrated performance gains across classification tasks and languages"], "limitations": "", "keywords": ["In-context Learning", "Zero-shot Classification", "Label Optimization", "Large Language Models", "Kurtosis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.14252", "pdf": "https://arxiv.org/pdf/2411.14252.pdf", "abs": "https://arxiv.org/abs/2411.14252", "title": "From Intents to Conversations: Generating Intent-Driven Dialogues with Contrastive Learning for Multi-Turn Classification", "authors": ["Junhua Liu", "Yong Keat Tan", "Bin Fu", "Kwan Hui Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Proceedings of CIKM 2025", "summary": "In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We further propose MINT-CL, a multi-task contrastive learning\nframework for multi-turn intent classification, which improves performance\nwhile reducing dependence on large-scale annotated datasets. Empirical results\ndemonstrate that our approach outperforms competitive baselines in both\ndialogue generation quality and classification accuracy, particularly in\nmultilingual settings. To facilitate future research, we release MINT-E, a\ncomprehensive, multilingual, intent-aware multi-turn dialogue corpus derived\nfrom the e-commerce domain. The reproduced source code and dataset are\navailable at https://github.com/junhua/chain-of-intent.", "AI": {"tldr": "Introduces Chain-of-Intent, a framework combining HMMs with LLMs for generating multilingual dialogue datasets and enhances multi-turn intent classification through MINT-CL.", "motivation": "To address the challenge of generating large-scale, domain-specific, multilingual dialogue datasets for multi-turn intent classification in conversational AI systems.", "method": "The proposed framework utilizes Hidden Markov Models (HMMs) in conjunction with Large Language Models (LLMs) to generate context-aware dialogues, guided by intent transition patterns extracted from real-world e-commerce chat logs.", "result": "The approach shows improved performance in dialogue generation quality and classification accuracy over competitive baselines, particularly in multilingual contexts. A new multilingual dialogue corpus, MINT-E, is also proposed for future research.", "conclusion": "The combination of HMMs and LLMs effectively generates natural dialogues, while the MINT-CL framework reduces reliance on large annotated datasets, enhancing intent classification tasks.", "key_contributions": ["Development of Chain-of-Intent framework for dialogue generation", "Introduction of MINT-CL for multi-task contrastive learning", "Release of MINT-E multilingual dialogue corpus"], "limitations": "", "keywords": ["Conversational AI", "Intent Classification", "Dialogue Generation", "Multilingual Datasets", "Hidden Markov Models"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2412.01377", "pdf": "https://arxiv.org/pdf/2412.01377.pdf", "abs": "https://arxiv.org/abs/2412.01377", "title": "Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge", "authors": ["Yuhe Ji", "Yilun Liu", "Feiyu Yao", "Minggui He", "Shimin Tao", "Xiaofeng Zhao", "Su Chang", "Xinhua Yang", "Weibin Meng", "Yuming Xie", "Boxing Chen", "Shenglin Zhang", "Yongqian Sun"], "categories": ["cs.CL", "cs.SE"], "comment": "Accepted by CIKM 2025", "summary": "Log analysis represents a critical sub-domain within AI applications that\nfacilitates automatic approaches to fault and error management of large-scaled\nsoftware systems, saving labors of traditional manual methods. While existing\nsolutions using large language models (LLMs) show promise, they are limited by\na significant domain gap between natural and log languages (the latter contains\nrich domain-specific tokens such as status codes, IP addresses, resource\npathes), which restricts their effectiveness in real-world applications.\nHowever, directly adapting general-purpose LLMs to log analysis using raw logs\nmay degrade their performance due to inconsistent token distribution. In this\npaper, we present a domain adaptation approach that addresses these limitations\nby integrating interpretable domain knowledge into open-source LLMs through\ncontinual pre-training (CPT), which bridges this domain gap by adapting LLMs on\ninterpretable natural texts with log knowledge (instead of raw logs) to reduce\ndistribution discrepancy. To achieve this, we developed NLPLog, a comprehensive\ndataset containing over 250,000 question-answer pairs on log-related knowledge.\nOur resulting model, SuperLog, achieves the best performance across four log\nanalysis tasks, with an average accuracy improvement of 12.01% over the\nsecond-best model. Ablation study also suggests advantages of domain adaption\nusing interpretable log knowledge over using raw logs.", "AI": {"tldr": "This paper presents NLPLog, a dataset and SuperLog, a model that integrate domain knowledge into LLMs to enhance log analysis performance by reducing discrepancies between natural and log language representations.", "motivation": "To improve the effectiveness of LLMs in log analysis and bridge the domain gap between natural language and domain-specific log language, enhancing performance in automatic fault and error management.", "method": "The paper proposes a domain adaptation approach through continual pre-training of LLMs on interpretable natural texts containing log knowledge, supported by the development of the NLPLog dataset with over 250,000 question-answer pairs related to logs.", "result": "SuperLog, the resulting model, demonstrates significant performance improvements, achieving an average accuracy increase of 12.01% over the second-best model in four log analysis tasks.", "conclusion": "The integration of interpretable log knowledge via continual pre-training effectively enhances the performance of LLMs in log analysis, indicating the value of domain adaptation in AI applications.", "key_contributions": ["Introduction of NLPLog, a new dataset for log-related knowledge", "Development of SuperLog, a model that outperforms existing solutions", "Demonstration of the effectiveness of domain adaptation using interpretable knowledge"], "limitations": "", "keywords": ["Log Analysis", "Domain Adaptation", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.15495", "pdf": "https://arxiv.org/pdf/2412.15495.pdf", "abs": "https://arxiv.org/abs/2412.15495", "title": "TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use", "authors": ["Junjie Ye", "Yilong Wu", "Sixian Li", "Yuming Yang", "Zhiheng Xi", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Peng Wang", "Zhongchao Shi", "Jianping Fan", "Zhengyin Du"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) achieve remarkable advancements by leveraging\ntools to interact with environments, a critical step toward generalized AI.\nHowever, the standard supervised fine-tuning (SFT) approach, which relies on\nlarge-scale datasets, often overlooks task-specific characteristics in tool\nuse, leading to performance bottlenecks. To address this issue, we analyze\nthree existing LLMs and uncover key insights: training data can inadvertently\nimpede tool-use behavior, token importance is distributed unevenly, and errors\nin tool calls fall into a small set of categories. Building on these findings,\nwe propose~\\emph{TL-Training}, a task-feature-based framework that mitigates\nthe effects of suboptimal training data, dynamically adjusts token weights to\nprioritize key tokens during SFT, and incorporates a robust reward mechanism\ntailored to error categories, optimized through proximal policy optimization.\nWe validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four\nopen-source test sets. Our results demonstrate that the LLM trained by our\nmethod matches or surpasses both open- and closed-source LLMs in tool-use\nperformance using only 1,217 training data points. Additionally, our method\nenhances robustness in noisy environments and improves general task\nperformance, offering a scalable and efficient paradigm for tool-use training\nin LLMs. Code and data are available at\nhttps://github.com/Junjie-Ye/TL-Training.", "AI": {"tldr": "The paper introduces TL-Training, a task-feature-based framework that optimizes fine-tuning for large language models (LLMs) to enhance tool-use performance.", "motivation": "To improve LLM performance in tool use, addressing the limitations of standard supervised fine-tuning (SFT) which often ignores task-specific characteristics.", "method": "The authors analyze existing LLMs and propose TL-Training, which dynamically adjusts token importance and incorporates a reward mechanism tailored to specific error categories, optimized using proximal policy optimization.", "result": "TL-Training was validated on CodeLLaMA-2-7B, showing that it matches or surpasses existing LLMs in tool-use performance using a minimal amount of training data (1,217 points).", "conclusion": "The proposed method improves both robustness in noisy environments and general task performance, presenting a scalable approach for LLM training in tool usage.", "key_contributions": ["Development of TL-Training framework to improve tool-use in LLMs", "Dynamic adjustment of token weights during training", "Incorporation of a robust reward mechanism tailored to common tool-use errors"], "limitations": "", "keywords": ["large language models", "tool use", "supervised fine-tuning", "NLU", "proximal policy optimization"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2502.12459", "pdf": "https://arxiv.org/pdf/2502.12459.pdf", "abs": "https://arxiv.org/abs/2502.12459", "title": "Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements", "authors": ["Guangxiang Zhao", "Saier Hu", "Xiaoqi Jian", "Jinzhu Wu", "Yuhan Wu", "Change Jia", "Lin Sun", "Xiangzheng Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 Main Conference", "summary": "In this paper, we propose a ``Generalization Stress Test\" to assess Large\nLanguage Models' (LLMs) generalization ability under slight and controlled\nperturbations, including option length, problem types, and irrelevant noun\nreplacements. We achieve novel and significant findings that, despite high\nbenchmark scores, LLMs exhibit severe accuracy drops and unexpected biases\n(e.g., preference for longer distractors) when faced with these minor but\ncontent-preserving modifications. For example, Qwen 2.5 1.5B's MMLU score rises\nfrom 60 to 89 and drops from 89 to 36 when option lengths are changed without\naltering the question. Even GPT4o experiences a 25-point accuracy loss when\nproblem types are changed, with a 6-point drop across all three modification\ncategories. These analyses suggest that LLMs rely heavily on superficial cues\nrather than forming robust, abstract representations that generalize across\nformats, lexical variations, and irrelevant content shifts.", "AI": {"tldr": "This paper introduces a Generalization Stress Test for evaluating LLMs' performance under minor controlled perturbations, revealing significant accuracy drops and biases despite high benchmark scores.", "motivation": "To assess the generalization ability of Large Language Models in light of their observed performance across various tasks.", "method": "A Generalization Stress Test is proposed, which includes perturbations such as option length, problem types, and irrelevant noun replacements to evaluate LLMs' responses.", "result": "LLMs show severe accuracy drops indicating reliance on superficial cues rather than robust representations, with examples showing significant score fluctuations under different perturbations.", "conclusion": "The findings suggest that LLMs lack strong generalization capabilities as they react poorly to slight content-preserving changes, indicating a need for improved model designs.", "key_contributions": ["Introduction of a Generalization Stress Test for evaluating LLMs.", "Discovery of significant biases and accuracy drops in LLMs under slight perturbations.", "Insight into LLMs' superficial cue reliance rather than robust understanding."], "limitations": "", "keywords": ["Large Language Models", "generalization", "bias", "perturbations", "evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.04945", "pdf": "https://arxiv.org/pdf/2503.04945.pdf", "abs": "https://arxiv.org/abs/2503.04945", "title": "Collaborative Evaluation of Deepfake Text with Deliberation-Enhancing Dialogue Systems", "authors": ["Jooyoung Lee", "Xiaochen Zhu", "Georgi Karadzhov", "Tom Stafford", "Andreas Vlachos", "Dongwon Lee"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "15; To appear in ICWSM 2026 (https://www.icwsm.org/2026/)", "summary": "The proliferation of generative models has presented significant challenges\nin distinguishing authentic human-authored content from deepfake content.\nCollaborative human efforts, augmented by AI tools, present a promising\nsolution. In this study, we explore the potential of DeepFakeDeLiBot, a\ndeliberation-enhancing chatbot, to support groups in detecting deepfake text.\nOur findings reveal that group-based problem-solving significantly improves the\naccuracy of identifying machine-generated paragraphs compared to individual\nefforts. While engagement with DeepFakeDeLiBot does not yield substantial\nperformance gains overall, it enhances group dynamics by fostering greater\nparticipant engagement, consensus building, and the frequency and diversity of\nreasoning-based utterances. Additionally, participants with higher perceived\neffectiveness of group collaboration exhibited performance benefits from\nDeepFakeDeLiBot. These findings underscore the potential of deliberative\nchatbots in fostering interactive and productive group dynamics while ensuring\naccuracy in collaborative deepfake text detection. \\textit{Dataset and source\ncode used in this study will be made publicly available upon acceptance of the\nmanuscript.", "AI": {"tldr": "This study investigates DeepFakeDeLiBot, a chatbot designed to aid group collaboration in detecting deepfake text, showing improved accuracy in group efforts compared to individual tasks.", "motivation": "The rise of generative models has made it challenging to differentiate between authentic and deepfake content, necessitating effective collaborative solutions.", "method": "The effectiveness of DeepFakeDeLiBot was evaluated in group settings to detect deepfake text, focusing on metrics such as engagement, consensus building, and diversity in reasoning.", "result": "Group-based problem-solving improved detection accuracy for machine-generated text, with benefits seen particularly in groups valuing collaboration.", "conclusion": "Deliberation-enhancing chatbots like DeepFakeDeLiBot can improve group dynamics and interaction, contributing to more effective detection of deepfake text.", "key_contributions": ["Introduction of DeepFakeDeLiBot for deepfake text detection", "Demonstration of improved accuracy in group-based problem solving", "Analysis of group dynamics and effectiveness in collaboration"], "limitations": "Engagement with DeepFakeDeLiBot did not lead to significant performance gains overall.", "keywords": ["deepfake detection", "collaborative problem solving", "deliberation-enhancing chatbot"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.06029", "pdf": "https://arxiv.org/pdf/2503.06029.pdf", "abs": "https://arxiv.org/abs/2503.06029", "title": "SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?", "authors": ["Xudong Lu", "Haohao Gao", "Renshou Wu", "Shuai Ren", "Xiaoxin Chen", "Hongsheng Li", "Fangyuan Li"], "categories": ["cs.CL", "cs.LG"], "comment": "26 pages", "summary": "Large Language Models (LLMs) have become integral to daily life, especially\nadvancing as intelligent assistants through on-device deployment on\nsmartphones. However, existing LLM evaluation benchmarks predominantly focus on\nobjective tasks like mathematics and coding in English, which do not\nnecessarily reflect the practical use cases of on-device LLMs in real-world\nmobile scenarios, especially for Chinese users. To address these gaps, we\nintroduce SmartBench, the first benchmark designed to evaluate the capabilities\nof on-device LLMs in Chinese mobile contexts. We analyze functionalities\nprovided by representative smartphone manufacturers and divide them into five\ncategories: text summarization, text Q&A, information extraction, content\ncreation, and notification management, further detailed into 20 specific tasks.\nFor each task, we construct high-quality datasets comprising 50 to 200\nquestion-answer pairs that reflect everyday mobile interactions, and we develop\nautomated evaluation criteria tailored for these tasks. We conduct\ncomprehensive evaluations of on-device LLMs and MLLMs using SmartBench and also\nassess their performance after quantized deployment on real smartphone NPUs.\nOur contributions provide a standardized framework for evaluating on-device\nLLMs in Chinese, promoting further development and optimization in this\ncritical area. Code and data will be available at\nhttps://github.com/vivo-ai-lab/SmartBench.", "AI": {"tldr": "Introduction of SmartBench, a benchmark for evaluating on-device LLMs in Chinese mobile contexts.", "motivation": "Existing evaluation benchmarks for LLMs do not reflect real-world applications, particularly for Chinese-speaking users using mobile devices.", "method": "Developed SmartBench, focusing on five categories of tasks related to on-device LLM functionalities and creating datasets with question-answer pairs for evaluation.", "result": "SmartBench enables comprehensive evaluation of on-device LLMs in Chinese contexts, revealing performance insights post-quantized deployment on smartphone NPUs.", "conclusion": "SmartBench provides a standardized framework for evaluating on-device LLMs, promoting future advancements in this domain.", "key_contributions": ["First benchmark for evaluating on-device LLMs in Chinese mobile contexts", "Detailed analysis of smartphone manufacturer functionalities", "Automated evaluation criteria for practical mobile tasks"], "limitations": "Focus exclusively on Chinese language and mobile contexts, limiting broader applicability.", "keywords": ["Large Language Models", "HCI", "Mobile Computing", "Evaluation Benchmarks", "Chinese"], "importance_score": 8, "read_time_minutes": 26}}
{"id": "2503.20533", "pdf": "https://arxiv.org/pdf/2503.20533.pdf", "abs": "https://arxiv.org/abs/2503.20533", "title": "Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence", "authors": ["Yijiong Yu"], "categories": ["cs.CL"], "comment": "Our code is available in\n  https://github.com/yuyijiong/parallel-decoding-in-one-sequence", "summary": "Recent advances in reasoning models have demonstrated significant\nimprovements in accuracy by employing detailed and comprehensive reasoning\nprocesses. However, generating these lengthy reasoning sequences is\ncomputationally expensive and time-consuming. To address this inefficiency, we\nleverage the inherent parallelizability of certain tasks to accelerate the\nreasoning process. Specifically, when multiple parallel reasoning steps exist,\nwe decode multiple tokens per forward pass via a tree-like attention mask\nwithin a single sequence, avoiding additional memory usage. Experimental\nresults show that our method achieves up to nearly 100\\% speedup in decoding\nwhile basically maintaining the answer quality.", "AI": {"tldr": "This paper presents a method for accelerating reasoning models by employing parallel processing to improve decoding speed while maintaining answer quality.", "motivation": "The motivation behind this research is to address the computational inefficiency of generating lengthy reasoning sequences in reasoning models, which hinders their practicality.", "method": "The authors propose a method that utilizes the parallelizability of specific tasks by decoding multiple tokens per forward pass with a tree-like attention mask, thus reducing memory usage.", "result": "Experimental results indicate that the proposed method achieves nearly 100% speedup in decoding processes while preserving the quality of answers.", "conclusion": "The proposed approach significantly enhances the efficiency of reasoning models by leveraging parallel processing, making them more practical for real-world applications.", "key_contributions": ["Introduction of a tree-like attention mask for parallel decoding", "Demonstration of nearly 100% speedup in decoding processes", "Maintaining answer quality while improving efficiency"], "limitations": "", "keywords": ["reasoning models", "parallel processing", "decoding speed", "tree-like attention mask", "computational efficiency"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.20430", "pdf": "https://arxiv.org/pdf/2506.20430.pdf", "abs": "https://arxiv.org/abs/2506.20430", "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning", "authors": ["Weike Zhao", "Chaoyi Wu", "Yanjie Fan", "Xiaoman Zhang", "Pengcheng Qiu", "Yuze Sun", "Xiao Zhou", "Yanfeng Wang", "Xin Sun", "Ya Zhang", "Yongguo Yu", "Kun Sun", "Weidi Xie"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "comment": null, "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.", "AI": {"tldr": "DeepRare is the first LLM-powered diagnostic system for rare diseases, generating ranked hypotheses with transparent reasoning.", "motivation": "Timely diagnosis of rare diseases is challenging due to clinical heterogeneity and limited clinician familiarity.", "method": "DeepRare includes a host with long-term memory, specialized agent servers for analytical tasks, and integrates over 40 tools and up-to-date medical knowledge.", "result": "DeepRare achieved 100% accuracy for 1,013 diseases, significantly outperformed other methods with an average Recall@1 of 57.18%, and was validated by clinical experts with 95.40% agreement.", "conclusion": "DeepRare provides a modular, scalable, and user-friendly solution for rare disease diagnosis, outperforming existing methods and ensuring reasoning transparency.", "key_contributions": ["First LLM-powered system for rare disease diagnosis", "Achieved 100% accuracy for a subset of diseases", "Integrates multiple analytical tools and knowledge sources"], "limitations": "", "keywords": ["rare diseases", "diagnosis", "Large Language Models", "DeepRare", "health informatics"], "importance_score": 10, "read_time_minutes": 10}}
{"id": "2508.14090", "pdf": "https://arxiv.org/pdf/2508.14090.pdf", "abs": "https://arxiv.org/abs/2508.14090", "title": "DLLMQuant: Quantizing Diffusion-based Large Language Models", "authors": ["Chen Xu", "Dawei Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 6 figures", "summary": "Diffusion-based large language models (DLLMs) have shown promise for\nnon-autoregressive text generation, but their deployment is constrained by\nlarge model sizes and heavy computational costs. Post-training quantization\n(PTQ), a widely used method for compressing and accelerating Large Language\nModels (LLMs), suffers from severe accuracy degradation and reduced\ngeneralization performance when directly applied to DLLMs (e.g., AWQ suffers a\n16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key\nmechanisms - dynamic masking, iterative generation, bidirectional attention -\nclash with quantization. We identify three core issues: 1) Iterative generation\nand dynamic masking ratios lead to distinct token distributions across decoding\nsteps, which are not adequately captured by existing PTQ calibration methods;\n2) Quantization errors are accumulated and amplified progressively during\niteration in DLLMs, causing quantized models to perform worse as decoding steps\nprogress; 3) Unmasked tokens stabilize while masked remain probabilistic,\nmaking overall feature distribution incompatible with existing PTQ methods. To\naddress these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs,\nwhich incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling\n(TMAS), a calibration method that accounts for both time and mask factors, with\nthe capacity to capture distributions across timesteps. 2) Interaction-Aware\nActivation Quantization (IA-AQ), which utilizes bidirectional attention's\ninteraction signals to dynamically allocate quantization resources. 3)\nCertainty-Guided Quantization (CGQ), which integrates mask status and token\nscores as key weighting criteria into error compensation, making weight\nquantization more suitable for DLLMs. Experiments show that DLLMQuant achieves\nsignificant performance gains while enhancing efficiency.", "AI": {"tldr": "This paper presents DLLMQuant, a post-training quantization framework specifically designed for diffusion-based large language models to improve efficiency without severe accuracy loss.", "motivation": "The deployment of diffusion-based large language models is hindered by their large sizes and computation costs, and existing post-training quantization methods result in significant accuracy degradation.", "method": "The paper proposes DLLMQuant, which includes three novel techniques: Temporal-Mask Adaptive Sampling for capturing distributions across decoding steps, Interaction-Aware Activation Quantization for dynamically allocating quantization resources, and Certainty-Guided Quantization for effective error compensation.", "result": "DLLMQuant significantly improves performance and efficiency when quantizing diffusion-based large language models compared to traditional methods.", "conclusion": "The proposed techniques effectively address the unique challenges posed by DLLMs in the quantization process, leading to enhanced model performance.", "key_contributions": ["Introduction of DLLMQuant framework for quantizing diffusion-based LLMs.", "Development of Temporal-Mask Adaptive Sampling for better distribution capture.", "Implementation of Interaction-Aware Activation Quantization to optimize resource allocation."], "limitations": "", "keywords": ["Diffusion-based models", "Post-training quantization", "Large language models", "Temporal-Mask Adaptive Sampling", "Interaction-Aware Activation Quantization"], "importance_score": 8, "read_time_minutes": 12}}
