{"id": "2507.08804", "pdf": "https://arxiv.org/pdf/2507.08804.pdf", "abs": "https://arxiv.org/abs/2507.08804", "title": "Cognitive Dissonance Artificial Intelligence (CD-AI): The Mind at War with Itself. Harnessing Discomfort to Sharpen Critical Thinking", "authors": ["Delia Deliu"], "categories": ["cs.HC", "cs.CY"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "AI-augmented systems are traditionally designed to streamline human\ndecision-making by minimizing cognitive load, clarifying arguments, and\noptimizing efficiency. However, in a world where algorithmic certainty risks\nbecoming an Orwellian tool of epistemic control, true intellectual growth\ndemands not passive acceptance but active struggle. Drawing on the dystopian\nvisions of George Orwell and Philip K. Dick - where reality is unstable,\nperception malleable, and truth contested - this paper introduces Cognitive\nDissonance AI (CD-AI): a novel framework that deliberately sustains uncertainty\nrather than resolving it. CD-AI does not offer closure, but compels users to\nnavigate contradictions, challenge biases, and wrestle with competing truths.\nBy delaying resolution and promoting dialectical engagement, CD-AI enhances\nreflective reasoning, epistemic humility, critical thinking, and adaptability\nin complex decision-making. This paper examines the theoretical foundations of\nthe approach, presents an implementation model, explores its application in\ndomains such as ethics, law, politics, and science, and addresses key ethical\nconcerns - including decision paralysis, erosion of user autonomy, cognitive\nmanipulation, and bias in AI reasoning. In reimagining AI as an engine of doubt\nrather than a deliverer of certainty, CD-AI challenges dominant paradigms of\nAI-augmented reasoning and offers a new vision - one in which AI sharpens the\nmind not by resolving conflict, but by sustaining it. Rather than reinforcing\nHuxleyan complacency or pacifying the user into intellectual conformity, CD-AI\nechoes Nietzsche's vision of the Uebermensch - urging users to transcend\npassive cognition through active epistemic struggle.", "AI": {"tldr": "This paper introduces Cognitive Dissonance AI (CD-AI), a framework that promotes engagement with uncertainty and opposing truths to enhance critical thinking and decision-making.", "motivation": "To address the limitations of traditional AI systems that seek to minimize cognitive load and provide certainty, by promoting active struggle and epistemic humility.", "method": "The paper presents a theoretical foundation for CD-AI and an implementation model, exploring its application across various domains including ethics, law, politics, and science.", "result": "CD-AI compels users to navigate contradictions and challenge biases, thus fostering better reflective reasoning and adaptability in complex decision-making scenarios.", "conclusion": "By advocating for a model of AI that sustains uncertainty rather than resolving it, this work reconceptualizes how AI can facilitate critical thinking and intellectual growth.", "key_contributions": ["Introduction of Cognitive Dissonance AI (CD-AI) framework", "Emphasis on promoting active engagement with uncertainty", "Exploration of ethical implications of sustaining cognitive dissonance in AI applications."], "limitations": "Potential issues include decision paralysis, erosion of user autonomy, and risks of cognitive manipulation.", "keywords": ["Cognitive Dissonance AI", "decision-making", "critical thinking", "epistemic humility", "HCI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.08805", "pdf": "https://arxiv.org/pdf/2507.08805.pdf", "abs": "https://arxiv.org/abs/2507.08805", "title": "Non-linear, Team-based VR Training for Cardiac Arrest Care with enhanced CRM Toolkit", "authors": ["Mike Kentros", "Manos Kamarianakis", "Michael Cole", "Vitaliy Popov", "Antonis Protopsaltis", "George Papagiannakis"], "categories": ["cs.HC", "cs.CY", "cs.GR"], "comment": "4 pages, 3 figures, 1 table", "summary": "This paper introduces iREACT, a novel VR simulation addressing key\nlimitations in traditional cardiac arrest (CA) training. Conventional methods\nstruggle to replicate the dynamic nature of real CA events, hindering Crew\nResource Management (CRM) skill development. iREACT provides a non-linear,\ncollaborative environment where teams respond to changing patient states,\nmirroring real CA complexities. By capturing multi-modal data (user actions,\ncognitive load, visual gaze) and offering real-time and post-session feedback,\niREACT enhances CRM assessment beyond traditional methods. A formative\nevaluation with medical experts underscores its usability and educational\nvalue, with potential applications in other high-stakes training scenarios to\nimprove teamwork, communication, and decision-making.", "AI": {"tldr": "iREACT is a VR simulation designed to improve cardiac arrest training by addressing the limitations of traditional methods and enhancing Crew Resource Management skills.", "motivation": "Traditional cardiac arrest training methods fail to effectively replicate the complexities of real-life events, making it difficult for teams to develop essential CRM skills.", "method": "iREACT provides a non-linear, collaborative VR environment that simulates dynamic patient states, enabling teams to respond in real-time while capturing multi-modal data for feedback.", "result": "Formative evaluation with medical experts shows that iREACT significantly enhances the usability and educational value of CA training compared to conventional methods.", "conclusion": "The iREACT simulation not only improves CRM in cardiac arrest training but also holds potential for application in other critical training environments to foster better teamwork and decision-making.", "key_contributions": ["Introduces a non-linear VR environment for cardiac arrest training.", "Captures multi-modal user data for enhanced feedback.", "Demonstrates applicability to high-stakes training beyond cardiovascular scenarios."], "limitations": "The primary limitation may include the specific focus on cardiac arrest scenarios, which may not account for variances in other medical training contexts.", "keywords": ["Virtual Reality", "Cardiac Arrest Training", "Crew Resource Management", "Multi-modal Data", "Simulation"], "importance_score": 8, "read_time_minutes": 4}}
{"id": "2507.08914", "pdf": "https://arxiv.org/pdf/2507.08914.pdf", "abs": "https://arxiv.org/abs/2507.08914", "title": "'Teens Need to Be Educated on the Danger': Digital Access, Online Risks, and Safety Practices Among Nigerian Adolescents", "authors": ["Munachimso B. Oguine", "Ozioma C. Oguine", "Karla Badillo-Urquiola", "Oluwasogo Adekunle Okunade"], "categories": ["cs.HC", "cs.CY"], "comment": "14 pages, 4 figures. Accepted to AfriCHI 2025", "summary": "Adolescents increasingly rely on online technologies to explore their\nidentities, form social connections, and access information and entertainment.\nHowever, their growing digital engagement exposes them to significant online\nrisks, particularly in underrepresented contexts like West Africa. This study\ninvestigates the online experiences of 409 secondary school adolescents in\nNigeria's Federal Capital Territory (FCT), focusing on their access to\ntechnology, exposure to risks, coping strategies, key stakeholders influencing\ntheir online interactions, and recommendations for improving online safety.\nUsing self-administered surveys, we found that while most adolescents reported\nmoderate access to online technology and connectivity, those who encountered\nrisks frequently reported exposure to inappropriate content and online scams.\nBlocking and reporting tools were the most commonly used strategies, though\nsome adolescents responded with inaction due to limited resources or awareness.\nParents emerged as the primary support network, though monitoring practices and\ncommunication varied widely. Guided by Protection Motivation Theory (PMT), our\nanalysis interprets adolescents' online safety behaviors as shaped by both\ntheir threat perceptions and their confidence in available coping strategies. A\nthematic analysis of their recommendations highlights the need for greater\nawareness and education, parental mediation, enhanced safety tools, stricter\nage restrictions, improved content moderation, government accountability, and\nresilience-building initiatives. Our findings underscore the importance of\nculturally and contextually relevant interventions to empower adolescents in\nnavigating the digital world, with implications for parents, educators,\ndesigners, and policymakers.", "AI": {"tldr": "This study explores the online experiences and risks faced by secondary school adolescents in Nigeria, emphasizing the need for culturally relevant interventions to improve online safety.", "motivation": "Adolescents' increased reliance on online technologies for identity exploration and social connections necessitates understanding their online risks, particularly in underrepresented regions like West Africa.", "method": "Self-administered surveys were conducted with 409 secondary school adolescents in Nigeria's Federal Capital Territory to examine their technology access, risk exposure, coping strategies, and recommendations for improving online safety.", "result": "Most adolescents had moderate access to technology but reported frequent encounters with risks such as inappropriate content and online scams. The most common coping strategies included blocking and reporting tools, though inaction was also noted due to limited resources.", "conclusion": "The study emphasizes the importance of culturally and contextually relevant interventions involving parents, educators, and policymakers to enhance online safety for adolescents in Nigeria.", "key_contributions": ["Analysis of adolescents' online safety behaviors through Protection Motivation Theory", "Identification of the key stakeholders influencing adolescents' online interactions", "Recommendations for improving online safety that cater to cultural contexts"], "limitations": "The study is geographically limited to Nigeria's Federal Capital Territory and may not represent experiences in other regions.", "keywords": ["online safety", "adolescents", "Nigeria", "digital risks", "Parental mediation"], "importance_score": 7, "read_time_minutes": 14}}
{"id": "2507.08973", "pdf": "https://arxiv.org/pdf/2507.08973.pdf", "abs": "https://arxiv.org/abs/2507.08973", "title": "Analytical Study on the Visibility of Potential Positions for External Human-Machine Interfaces", "authors": ["Jose Gonzalez-Belmonte", "Jaerock Kwon"], "categories": ["cs.HC"], "comment": "28 pages, 5 tables, 10 figures", "summary": "As we move towards a future of autonomous vehicles, questions regarding their\nmethod of communication have arisen. One of the common questions concerns the\nplacement of the signaling used to communicate with pedestrians and road users,\nbut little work has been published fully dedicated to exploring this. This\npaper uses a simulation made in the Unity game engine to record the visibility\nof fifteen different vehicles, specifically regarding the visibility of frontal\nelements by a pedestrian on the sidewalk. Variables include the vehicle\nposition, number of vehicles on the road, and minimum and maximum distance of\nthe recorded points. It was concluded that the areas of the vehicle most often\nseen by pedestrians on the sidewalk attempting to cross the road were the\nfrontal frontal fenders and the headlights, with the frontal wheels, frontal\ndoors, bumper, and side mirrors are less visible alternatives. These findings\nare valuable in the future design of signaling for autonomous vehicles, in\norder to ensure pedestrians are able to see them on approaching vehicles. The\nsoftware used provides a platform for similar works in the future to be\nconducted.", "AI": {"tldr": "This paper explores the visibility of various vehicle elements to pedestrians using Unity simulations, aiming to improve signaling design for autonomous vehicles.", "motivation": "With the rise of autonomous vehicles, understanding how pedestrians perceive these vehicles is crucial for safety and effective communication.", "method": "The study utilized Unity simulations to assess the visibility of fifteen different vehicle elements from the perspective of a sidewalk pedestrian, considering various factors such as vehicle position, number of vehicles, and visibility distances.", "result": "The research found that pedestrians most frequently see the frontal fenders and headlights of vehicles, while parts like frontal wheels, doors, and mirrors are less visible.", "conclusion": "The findings can inform the design of signaling systems for autonomous vehicles to enhance pedestrian awareness and safety.", "key_contributions": ["Identification of key vehicle elements visible to pedestrians", "Utilization of Unity for vehicle visibility simulations", "Provision of insights for future signaling design in autonomous vehicles"], "limitations": "", "keywords": ["autonomous vehicles", "pedestrian visibility", "signaling design", "Unity simulation", "vehicle communication"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.08865", "pdf": "https://arxiv.org/pdf/2507.08865.pdf", "abs": "https://arxiv.org/abs/2507.08865", "title": "Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale", "authors": ["Javis AI Team", "Amrendra Singh", "Maulik Shah", "Dharshan Sampath"], "categories": ["cs.CL"], "comment": null, "summary": "Extracting tables and key-value pairs from financial documents is essential\nfor business workflows such as auditing, data analytics, and automated invoice\nprocessing. In this work, we introduce Spatial ModernBERT-a transformer-based\nmodel augmented with spatial embeddings-to accurately detect and extract\ntabular data and key-value fields from complex financial documents. We cast the\nextraction task as token classification across three heads: (1) Label Head,\nclassifying each token as a label (e.g., PO Number, PO Date, Item Description,\nQuantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;\n(3) Row Head, distinguishing the start of item rows and header rows. The model\nis pretrained on the PubTables-1M dataset, then fine-tuned on a financial\ndocument dataset, achieving robust performance through cross-entropy loss on\neach classification head. We propose a post-processing method to merge tokens\nusing B-I-IB tagging, reconstruct the tabular layout, and extract key-value\npairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages\nboth textual and spatial cues, facilitating highly accurate table and key-value\nextraction in real-world financial documents.", "AI": {"tldr": "Spatial ModernBERT is a transformer model enhanced with spatial embeddings for accurate extraction of tables and key-value pairs from financial documents.", "motivation": "The extraction of tabular data and key-value pairs from financial documents is crucial for workflows like auditing and invoice processing.", "method": "The extraction task is approached as token classification using a transformer model with three distinct heads for labeling tokens, predicting column indices, and identifying row starts. The model is pretrained on the PubTables-1M dataset and fine-tuned on a financial document dataset.", "result": "Spatial ModernBERT demonstrates robust performance, effectively leveraging textual and spatial information, as shown through evaluations on financial document extraction tasks.", "conclusion": "The proposed model and its post-processing method significantly improve the accuracy of table and key-value extraction in financial documents.", "key_contributions": ["Introduction of Spatial ModernBERT with spatial embeddings for enhanced extraction accuracy", "Multi-head token classification approach for structured data extraction", "Effective post-processing method for token merging and layout reconstruction"], "limitations": "", "keywords": ["tabular data extraction", "key-value pairs", "transformer model", "financial documents", "spatial embeddings"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.09100", "pdf": "https://arxiv.org/pdf/2507.09100.pdf", "abs": "https://arxiv.org/abs/2507.09100", "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data", "authors": ["Mohammad Abolnejadian", "Shakiba Amirshahi", "Matthew Brehmer", "Anamaria Crisan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.0"], "comment": "7 pages and 4 figures. Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25)", "summary": "In decision-making conversations, experts must navigate complex choices and\nmake on-the-spot decisions while engaged in conversation. Although extensive\nhistorical data often exists, the real-time nature of these scenarios makes it\ninfeasible for decision-makers to review and leverage relevant information.\nThis raises an interesting question: What if experts could utilize relevant\npast data in real-time decision-making through insights derived from past data?\nTo explore this, we implemented a conversational user interface, taking\ndoctor-patient interactions as an example use case. Our system continuously\nlistens to the conversation, identifies patient problems and doctor-suggested\nsolutions, and retrieves related data from an embedded dataset, generating\nconcise insights using a pipeline built around a retrieval-based Large Language\nModel (LLM) agent. We evaluated the prototype by embedding Health Canada\ndatasets into a vector database and conducting simulated studies using sample\ndoctor-patient dialogues, showing effectiveness but also challenges, setting\ndirections for the next steps of our work.", "AI": {"tldr": "The paper presents a system that enables real-time decision-making in doctor-patient conversations by utilizing past health data through a retrieval-based LLM interface.", "motivation": "To assist experts in making informed decisions during conversations by leveraging historical data in real-time, especially in healthcare scenarios like doctor-patient interactions.", "method": "A conversational user interface was developed that listens to conversations, identifies key issues, and retrieves relevant data from an embedded dataset to generate insights using a retrieval-based LLM pipeline.", "result": "Simulations showed that the prototype effectively generated insights for decision-making in simulated doctor-patient dialogues, though challenges were identified.", "conclusion": "The study demonstrates the potential of real-time data retrieval to support decision-making in healthcare conversations, while also highlighting areas for improvement and future work.", "key_contributions": ["Development of a conversational system for real-time insights in healthcare", "Use of a retrieval-based LLM for generating context-specific information", "Evaluation using Health Canada datasets in simulated dialogues"], "limitations": "Challenges in accurately identifying relevant data and the need for further refinement of the real-time system.", "keywords": ["conversational user interface", "decision making", "large language model", "healthcare", "real-time data retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.08898", "pdf": "https://arxiv.org/pdf/2507.08898.pdf", "abs": "https://arxiv.org/abs/2507.08898", "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "categories": ["cs.CL", "cs.AI"], "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail.", "AI": {"tldr": "SEALGuard is a multilingual guardrail designed to enhance the safety alignment of LLM systems by effectively filtering unsafe and jailbreak prompts across diverse languages.", "motivation": "To address the limitations of existing LLM-powered guardrails, which struggle with multilingual unsafe inputs, particularly in low-resource languages.", "method": "The paper proposes SEALGuard, utilizing low-rank adaptation (LoRA) on a multilingual language model, and introduces SEALSBench, a multilingual safety alignment dataset with over 260,000 prompts in ten languages.", "result": "SEALGuard surpasses state-of-the-art guardrails, improving Defense Success Rate (DSR) by 48% compared to LlamaGuard on multilingual unsafe prompts, achieving superior precision and F1-score.", "conclusion": "SEALGuard significantly enhances safety alignment in LLM systems by effectively addressing the multilingual safety alignment gap.", "key_contributions": ["Introduction of SEALGuard for multilingual safety alignment", "Development of SEALSBench, a large-scale multilingual prompt dataset", "Demonstrated performance improvements over existing guardrails in multilingual contexts."], "limitations": "", "keywords": ["Multilingual guardrail", "Safety alignment", "LLM systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.09190", "pdf": "https://arxiv.org/pdf/2507.09190.pdf", "abs": "https://arxiv.org/abs/2507.09190", "title": "User-to-PC Authentication Through Confirmation on Mobile Devices: On Usability and Performance", "authors": ["Andreas Pramendorfer", "Rainhard Dieter Findling"], "categories": ["cs.HC", "cs.CR"], "comment": "Submitted to MoMM 2025", "summary": "Protecting personal computers (PCs) from unauthorized access typically relies\non password authentication, which is know to suffer from cognitive burden and\nweak credentials. As many users nowadays carry mobile devices with advanced\nsecurity features throughout their day, there is an opportunity to leverage\nthese devices to improve authentication to PCs. In this paper we utilize a\ntoken-based passwordless approach where users authenticate to their PC by\nconfirming the authentication request on their smartphones or smartwatches.\nUpon a request to login to the PC, or to evaluate privileges, the PC issues an\nauthentication request that users receive on their mobile devices, where users\ncan confirm or deny the request. We evaluate button tap and biometric\nfingerprint verification as confirmation variants, and compare their\nauthentication duration, success rate, and usability to traditional\npassword-based authentication in a user study with 30 participants and a total\nof 1,200 authentication attempts. Smartwatch-based authentication outperformed\npassword-based authentication and smartphone-based variants in authentication\nduration, while showing comparable success rates. Participants rated\nsmartwatch-based authentication highest in usability, followed by\npassword-based authentication and smartphone-based authentication.", "AI": {"tldr": "This paper presents a token-based passwordless authentication method using mobile devices to enhance security and usability.", "motivation": "To improve personal computer security and reduce cognitive burden associated with password authentication.", "method": "The study evaluates a token-based approach where users confirm authentication requests on their smartphones or smartwatches, comparing button tap and biometric fingerprint verification methods.", "result": "Smartwatch-based authentication showed superior performance in authentication duration compared to both password-based and smartphone-based methods, with comparable success rates. Usability ratings favored smartwatch-based authentication.", "conclusion": "The results suggest that utilizing mobile devices for authentication significantly improves the experience and security over traditional password methods.", "key_contributions": ["Introduction of token-based passwordless authentication utilizing mobile devices.", "Comparison of smartwatch and smartphone confirmation methods against traditional passwords in terms of speed and usability.", "User study confirming the practicality of smartwatch-based authentication."], "limitations": "The study included a small sample size of 30 participants and focused only on specific confirmation methods.", "keywords": ["passwordless authentication", "human-computer interaction", "mobile security"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.08916", "pdf": "https://arxiv.org/pdf/2507.08916.pdf", "abs": "https://arxiv.org/abs/2507.08916", "title": "Evaluating LLMs in Medicine: A Call for Rigor, Transparency", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "categories": ["cs.CL"], "comment": null, "summary": "Objectives: To evaluate the current limitations of large language models\n(LLMs) in medical question answering, focusing on the quality of datasets used\nfor their evaluation. Materials and Methods: Widely-used benchmark datasets,\nincluding MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,\ntransparency, and relevance to clinical scenarios. Alternatives, such as\nchallenge questions in medical journals, were also analyzed to identify their\npotential as unbiased evaluation tools. Results: Most existing datasets lack\nclinical realism, transparency, and robust validation processes. Publicly\navailable challenge questions offer some benefits but are limited by their\nsmall size, narrow scope, and exposure to LLM training. These gaps highlight\nthe need for secure, comprehensive, and representative datasets. Conclusion: A\nstandardized framework is critical for evaluating LLMs in medicine.\nCollaborative efforts among institutions and policymakers are needed to ensure\ndatasets and methodologies are rigorous, unbiased, and reflective of clinical\ncomplexities.", "AI": {"tldr": "This paper evaluates limitations of LLMs in medical question answering, highlighting dataset quality issues.", "motivation": "To assess the efficacy of LLMs in medical question answering through rigorous evaluation of benchmark datasets.", "method": "A review of widely-used benchmark datasets such as MedQA, MedMCQA, PubMedQA, and MMLU, along with an analysis of alternative evaluation tools like challenge questions from medical journals.", "result": "The study found that most existing datasets lack clinical realism, transparency, and robust validation, while alternative tools offered limited benefits.", "conclusion": "A standardized framework for evaluating LLMs in medicine is essential, alongside collaborative efforts to improve dataset quality and evaluation methodologies.", "key_contributions": ["Identified gaps in current LLM evaluation datasets.", "Proposed the need for standardized evaluation frameworks.", "Highlighted the importance of collaborative efforts in dataset development."], "limitations": "Existing datasets are characterized by small sizes, narrow scopes, and bias due to prior exposure in LLM training.", "keywords": ["large language models", "medical question answering", "dataset evaluation", "clinical scenarios", "standardized framework"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.09262", "pdf": "https://arxiv.org/pdf/2507.09262.pdf", "abs": "https://arxiv.org/abs/2507.09262", "title": "Discrepancies in Mental Workload Estimation: Self-Reported versus EEG-Based Measures in Data Visualization Evaluation", "authors": ["Soobin Yim", "Sangbong Yoo", "Chanyoung Yoon", "Chanyoung Jung", "Chansoo Kim", "Yun Jang", "Ghulam Jilani Quadri"], "categories": ["cs.HC"], "comment": null, "summary": "Accurate assessment of mental workload (MW) is crucial for understanding\ncognitive processes during visualization tasks. While EEG-based measures are\nemerging as promising alternatives to conventional assessment techniques, such\nas selfreport measures, studies examining consistency across these different\nmethodologies are limited. In a preliminary study, we observed indications of\npotential discrepancies between EEGbased and self-reported MW measures.\nMotivated by these preliminary observations, our study further explores the\ndiscrepancies between EEG-based and self-reported MW assessment methods through\nan experiment involving visualization tasks. In the experiment, we employ two\nbenchmark tasks: the Visualization Literacy Assessment Test (VLAT) and a\nSpatial Visualization (SV) task. EEG signals are recorded from participants\nusing a 32-channel system at a sampling rate of 128 Hz during the visualization\ntasks. For each participant, MW is estimated using an EEG-based model built on\na Graph Attention Network (GAT) architecture, and these estimates are compared\nwith conventional MW measures to examine potential discrepancies. Our findings\nreveal notable discrepancies between task difficulty and EEG-based MW\nestimates, as well as between EEG-based and self-reported MW measures across\nvarying task difficulty levels. Additionally, the observed patterns suggest the\npresence of unconscious cognitive effort that may not be captured by selfreport\nalone.", "AI": {"tldr": "This study investigates discrepancies between EEG-based and self-reported mental workload (MW) in visualization tasks.", "motivation": "The accurate assessment of mental workload is essential for understanding cognitive processes, and EEG measures may offer insights beyond self-reports.", "method": "An experiment was conducted using EEG to assess MW during two benchmark visualization tasks (VLAT and SV), comparing these measures with traditional self-report assessments.", "result": "Significant discrepancies were found between EEG-based MW estimates and self-reported measures, particularly across varying task difficulty levels, indicating possible unconscious cognitive effort not captured by self-report.", "conclusion": "The findings suggest that EEG-based assessments may provide a more nuanced understanding of mental workload compared to conventional self-reported methods.", "key_contributions": ["Demonstrated discrepancies between EEG-based and self-reported MW assessments.", "Utilized a Graph Attention Network (GAT) architecture for modeling EEG data.", "Highlighted the potential of EEG measures to capture unconscious cognitive effort."], "limitations": "The study's findings are preliminary and may require further validation with larger participant samples.", "keywords": ["mental workload", "EEG", "self-report", "visualization tasks", "cognitive processes"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.08924", "pdf": "https://arxiv.org/pdf/2507.08924.pdf", "abs": "https://arxiv.org/abs/2507.08924", "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.", "AI": {"tldr": "Introduction of two Korean expert-level benchmarks for evaluating LLMs in industrial applications.", "motivation": "To evaluate the applicability of Large Language Models in real-world scenarios across both academic and industrial domains using robust benchmarks.", "method": "The paper presents two benchmarks: KMMLU-Redux, reconstructed from the KMMLU with enhanced reliability, and KMMLU-Pro, based on Korean National Professional Licensure exams.", "result": "Experiments show that these benchmarks comprehensively represent industrial knowledge in Korea.", "conclusion": "The newly developed benchmarks can effectively evaluate the performance of LLMs in both academic and professional contexts.", "key_contributions": ["Development of KMMLU-Redux and KMMLU-Pro benchmarks", "Public release of the dataset for broader access", "Enhancements in reliability of the evaluation process for LLMs"], "limitations": "", "keywords": ["Large Language Models", "benchmarks", "Korean National Qualification exams", "industrial knowledge", "Korean LLM evaluation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.09489", "pdf": "https://arxiv.org/pdf/2507.09489.pdf", "abs": "https://arxiv.org/abs/2507.09489", "title": "TraSculptor: Visual Analytics for Enhanced Decision-Making in Road Traffic Planning", "authors": ["Zikun Deng", "Yuanbang Liu", "Mingrui Zhu", "Da Xiang", "Haiyue Yu", "Zicheng Su", "Qinglong Lu", "Tobias Schreck", "Yi Cai"], "categories": ["cs.HC"], "comment": "IEEE Transactions on Visualization and Computer Graphics", "summary": "The design of urban road networks significantly influences traffic\nconditions, underscoring the importance of informed traffic planning. Traffic\nplanning experts rely on specialized platforms to simulate traffic systems,\nassessing the efficacy of the road network across various states of\nmodifications. Nevertheless, a prevailing issue persists: many existing traffic\nplanning platforms exhibit inefficiencies in flexibly interacting with the road\nnetwork's structure and attributes and intuitively comparing multiple states\nduring the iterative planning process. This paper introduces TraSculptor, an\ninteractive planning decision-making system. To develop TraSculptor, we\nidentify and address two challenges: interactive modification of road networks\nand intuitive comparison of multiple network states. For the first challenge,\nwe establish flexible interactions to enable experts to easily and directly\nmodify the road network on the map. For the second challenge, we design a\ncomparison view with a history tree of multiple states and a road-state matrix\nto facilitate intuitive comparison of road network states. To evaluate\nTraSculptor, we provided a usage scenario where the Braess's paradox was\nshowcased, invited experts to perform a case study on the Sioux Falls network,\nand collected expert feedback through interviews.", "AI": {"tldr": "TraSculptor is an interactive traffic planning system designed to enhance the modification of urban road networks and the comparison of multiple states during traffic planning.", "motivation": "The paper addresses the inefficiencies in current traffic planning platforms that hinder effective interaction with road network structures and intuitive comparison of various modifications.", "method": "TraSculptor provides flexible interactions for expert users to modify road networks on a map and includes a comparison view featuring a history tree and a road-state matrix to facilitate intuitive analysis.", "result": "The evaluation through a usage scenario demonstrating Braess's paradox and expert case studies on the Sioux Falls network provided positive feedback on TraSculptor's capabilities.", "conclusion": "TraSculptor enhances the traffic planning process by allowing for direct modifications and intuitive comparisons, addressing significant limitations in current tools.", "key_contributions": ["Introduction of TraSculptor for interactive traffic planning", "Flexible modification capabilities for road networks", "Intuitive comparison view with history tree and road-state matrix"], "limitations": "", "keywords": ["traffic planning", "urban road networks", "interactive systems", "visualization", "simulation"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2507.08967", "pdf": "https://arxiv.org/pdf/2507.08967.pdf", "abs": "https://arxiv.org/abs/2507.08967", "title": "Self-Improving Model Steering", "authors": ["Rongyi Zhu", "Yuhui Wang", "Tanqiu Jiang", "Jiacheng Liang", "Ting Wang"], "categories": ["cs.CL"], "comment": "16 pages, 9 figures", "summary": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment.", "AI": {"tldr": "This paper introduces SIMS, a self-improving model-steering framework for large language models that does not rely on external data.", "motivation": "Conventional model-steering methods depend on annotated data, limiting adaptability and effectiveness based on annotation quality.", "method": "SIMS generates and refines contrastive samples through self-improvement cycles, using prompt ranking and contrast sampling strategies for context-specific steering.", "result": "SIMS has been extensively evaluated and demonstrates substantial improvements in steering effectiveness and adaptability compared to existing methods.", "conclusion": "The research highlights the potential of self-improving model steering for enhancing inference-time LLM alignment without needing external supervision.", "key_contributions": ["First self-improving model-steering framework (SIMS) without external supervision", "Introduction of prompt ranking and contrast sampling strategies", "Demonstrated substantial improvements over existing steering methods"], "limitations": "", "keywords": ["self-improving", "model steering", "large language models", "context-specific steering", "contrastive sampling"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.09549", "pdf": "https://arxiv.org/pdf/2507.09549.pdf", "abs": "https://arxiv.org/abs/2507.09549", "title": "The Spectacle of Fidelity: Blind Resistance and the Wizardry of Prototyping", "authors": ["Hrittika Bhowmick", "Shilpaa Anand"], "categories": ["cs.HC", "H.5.2; K.4.2; D.2.2"], "comment": "3 pages. Submitted for Access InContext Workshop at CHI'25, April 26,\n  2025, Yokohama, Japan", "summary": "Prototyping is widely regarded in Human-Computer Interaction as an iterative\nprocess through which ideas are tested and refined, often via visual mockups,\nscreen flows, and coded simulations. This position paper critiques the\nvisual-centric norms embedded in prototyping culture by drawing from the lived\nexperiences of blind scholars and insights from cultural disability studies. It\ndiscusses how dominant methods of prototyping rely on an unexamined fidelity to\nsight, privileging what can be rendered visibly coherent while marginalizing\nother modes of knowing and making. By repositioning prototyping as a situated,\nembodied, and relational practice, this paper challenges HCI to rethink what\nkinds of design participation are legitimized and which are excluded when\nprototyping is reduced to screen-based simulations.", "AI": {"tldr": "This paper critiques the visual-centric norms of prototyping in Human-Computer Interaction by exploring the experiences of blind scholars and cultural disability studies, advocating for a broader understanding of design participation beyond sight-based methods.", "motivation": "To critique the dominance of visual methods in prototyping within HCI and highlight the marginalization of other forms of knowledge and experience.", "method": "The paper draws insights from cultural disability studies and the lived experiences of blind scholars to analyze existing prototyping practices.", "result": "The critique reveals that current prototyping methods privilege visual coherence and exclude diverse modes of knowing, suggesting the need for a more inclusive approach to design.", "conclusion": "The paper calls for HCI to rethink its prototyping practices to include various forms of participation that do not solely depend on visual representations.", "key_contributions": ["Critique of visual-centric prototyping methods", "Inclusion of blind scholars' perspectives", "Reframing prototyping as an embodied and relational practice"], "limitations": "", "keywords": ["Prototyping", "Human-Computer Interaction", "Disability Studies", "Inclusive Design", "Visual Culture"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.08969", "pdf": "https://arxiv.org/pdf/2507.08969.pdf", "abs": "https://arxiv.org/abs/2507.08969", "title": "Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR", "authors": ["Drew Walker", "Jennifer Love", "Swati Rajwal", "Isabel C Walker", "Hannah LF Cooper", "Abeed Sarker", "Melvin Livingston III"], "categories": ["cs.CL"], "comment": "3 Tables", "summary": "Introduction: Electronic health records (EHR) are a critical medium through\nwhich patient stigmatization is perpetuated among healthcare teams. Methods: We\nidentified linguistic features of doubt markers and stigmatizing labels in\nMIMIC-III EHR via expanded lexicon matching and supervised learning\nclassifiers. Predictors of rates of linguistic features were assessed using\nPoisson regression models. Results: We found higher rates of stigmatizing\nlabels per chart among patients who were Black or African American (RR: 1.16),\npatients with Medicare/Medicaid or government-run insurance (RR: 2.46),\nself-pay (RR: 2.12), and patients with a variety of stigmatizing disease and\nmental health conditions. Patterns among doubt markers were similar, though\nmale patients had higher rates of doubt markers (RR: 1.25). We found increased\nstigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),\nwith similar patterns of doubt markers. Discussion: Stigmatizing language\noccurred at higher rates among historically stigmatized patients, perpetuated\nby multiple provider types.", "AI": {"tldr": "This study analyzes the presence of stigmatizing language in electronic health records (EHR) among different patient demographics.", "motivation": "To understand how electronic health records may perpetuate stigmatization among healthcare teams and impact patient care.", "method": "Linguistic features of doubt markers and stigmatizing labels were identified in the MIMIC-III EHR using lexicon matching and supervised learning classifiers, with rates analyzed using Poisson regression models.", "result": "Significant findings revealed higher rates of stigmatizing labels among Black or African American patients, those with government insurance, and patients with varying mental health conditions. Higher rates of doubt markers were also noted among male patients.", "conclusion": "The prevalence of stigmatizing language in EHR is influenced by patient demographics and provider types, suggesting a need for awareness and reform in documentation practices.", "key_contributions": ["Identification of linguistic features in EHR that contribute to stigma.", "Evidence of disparity in language use based on patient demographics.", "Highlighting the role of various provider types in perpetuating stigmatizing language."], "limitations": "", "keywords": ["electronic health records", "stigmatization", "linguistic features", "MIMIC-III", "Poisson regression"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.09664", "pdf": "https://arxiv.org/pdf/2507.09664.pdf", "abs": "https://arxiv.org/abs/2507.09664", "title": "SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations", "authors": ["Zoe Kaputa", "Anika Rajaram", "Vryan Almanon Feliciano", "Zhuoyue Lyu", "Maneesh Agrawala", "Hari Subramonyam"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Programming-by-prompting with generative AI offers a new paradigm for\nend-user programming, shifting the focus from syntactic fluency to semantic\nintent. This shift holds particular promise for non-programmers such as\neducators, who can describe instructional goals in natural language to generate\ninteractive learning content. Yet in bypassing direct code authoring, many of\nprogramming's core affordances - such as traceability, stepwise refinement, and\nbehavioral testing - are lost. We propose the Chain-of-Abstractions (CoA)\nframework as a way to recover these affordances while preserving the expressive\nflexibility of natural language. CoA decomposes the synthesis process into a\nsequence of cognitively meaningful, task-aligned representations that function\nas checkpoints for specification, inspection, and refinement. We instantiate\nthis approach in SimStep, an authoring environment for teachers that scaffolds\nsimulation creation through four intermediate abstractions: Concept Graph,\nScenario Graph, Learning Goal Graph, and UI Interaction Graph. To address\nambiguities and misalignments, SimStep includes an inverse correction process\nthat surfaces in-filled model assumptions and enables targeted revision without\nrequiring users to manipulate code. Evaluations with educators show that CoA\nenables greater authoring control and interpretability in\nprogramming-by-prompting workflows.", "AI": {"tldr": "The paper presents the Chain-of-Abstractions (CoA) framework for enhancing programming-by-prompting with generative AI, focusing on maintaining core programming affordances while enabling non-programmers, particularly educators, to create interactive learning content.", "motivation": "Programming-by-prompting with generative AI shifts focus from code fluency to semantic intent, making it accessible for non-programmers like educators. However, it loses important programming features like traceability and testing, necessitating a new approach.", "method": "The Chain-of-Abstractions (CoA) framework decomposes the synthesis process into sequenced, cognitively meaningful representations, which serve as checkpoints for specification, inspection, and refinement in an authoring environment called SimStep.", "result": "Evaluations with educators showed that the CoA framework in SimStep allows for greater authoring control and interpretability when creating simulations compared to traditional programming methods.", "conclusion": "CoA effectively enables educators to create complex interactive learning environments while still maintaining control over essential programming affordances, enhancing the programming-by-prompting experience.", "key_contributions": ["Introduction of the Chain-of-Abstractions (CoA) framework for programming-by-prompting.", "Development of the SimStep authoring environment that leverages CoA for educators.", "Implementation of intermediate abstractions (Concept Graph, Scenario Graph, Learning Goal Graph, UI Interaction Graph) for enhanced authoring control."], "limitations": "", "keywords": ["generative AI", "end-user programming", "education", "interactive learning", "Chain-of-Abstractions"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.09011", "pdf": "https://arxiv.org/pdf/2507.09011.pdf", "abs": "https://arxiv.org/abs/2507.09011", "title": "Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery", "authors": ["Ana Chkhaidze", "Reshanne R. Reeder", "Connor Gag", "Anastasia Kiyonaga", "Seana Coulson"], "categories": ["cs.CL", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "A rapidly alternating red and black display known as Ganzflicker induces\nvisual hallucinations that reflect the generative capacity of the visual\nsystem. Recent proposals regarding the imagery spectrum, that is, differences\nin the visual system of individuals with absent imagery, typical imagery, and\nvivid imagery, suggest these differences should impact the complexity of other\ninternally generated visual experiences. Here, we used tools from natural\nlanguage processing to analyze free-text descriptions of hallucinations from\nover 4,000 participants, asking whether people with different imagery\nphenotypes see different things in their mind's eye during Ganzflicker-induced\nhallucinations. Strong imagers described complex, naturalistic content, while\nweak imagers reported simple geometric patterns. Embeddings from vision\nlanguage models better captured these differences than text-only language\nmodels, and participants with stronger imagery used language with richer\nsensorimotor associations. These findings may reflect individual variation in\ncoordination between early visual areas and higher-order regions relevant for\nthe imagery spectrum.", "AI": {"tldr": "The paper investigates the relationship between individual imagery phenotypes and the nature of visual hallucinations induced by Ganzflicker, utilizing natural language processing to analyze participant descriptions.", "motivation": "To explore how differences in the imagery spectrum impact the complexity of internally generated visual experiences during Ganzflicker-induced hallucinations.", "method": "Analyzed free-text descriptions of hallucinations from over 4,000 participants using natural language processing tools and compared embeddings from vision language models and text-only language models.", "result": "Strong imagers reported complex, naturalistic content during hallucinations, while weak imagers described simple geometric patterns. Vision language models were better at capturing these differences than text-only models.", "conclusion": "Individual differences in the imagery spectrum may reflect varying coordination between early visual areas and higher-order brain regions.", "key_contributions": ["Used large participant pool (over 4,000) to analyze hallucination descriptions.", "Showed that imagery phenotypes influence the complexity of visual hallucinations.", "Demonstrated effectiveness of vision language models over text-only models in capturing sensory experiences."], "limitations": "Results may be influenced by self-reporting biases and may not generalize to all types of visual hallucinations.", "keywords": ["visual hallucinations", "imagery spectrum", "natural language processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.09917", "pdf": "https://arxiv.org/pdf/2507.09917.pdf", "abs": "https://arxiv.org/abs/2507.09917", "title": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series", "authors": ["Zikun Deng", "Jiabao Huang", "Chenxi Ruan", "Jialing Li", "Shaowu Gao", "Yi Cai"], "categories": ["cs.HC"], "comment": null, "summary": "Spatial time series visualization offers scientific research pathways and\nanalytical decision-making tools across various spatiotemporal domains. Despite\nmany advanced methodologies, the seamless integration of temporal and spatial\ninformation remains a challenge. The space-time cube (STC) stands out as a\npromising approach for the synergistic presentation of spatial and temporal\ninformation, with successful applications across various spatiotemporal\ndatasets. However, the STC is plagued by well-known issues such as visual\nocclusion and depth ambiguity, which are further exacerbated when dealing with\nlarge-scale spatial time series data. In this study, we introduce a novel\ntechnical framework termed VolumeSTCube, designed for continuous spatiotemporal\nphenomena. It first leverages the concept of the STC to transform discretely\ndistributed spatial time series data into continuously volumetric data.\nSubsequently, volume rendering and surface rendering techniques are employed to\nvisualize the transformed volumetric data. Volume rendering is utilized to\nmitigate visual occlusion, while surface rendering provides pattern details by\nenhanced lighting information. Lastly, we design interactions to facilitate the\nexploration and analysis from temporal, spatial, and spatiotemporal\nperspectives. VolumeSTCube is evaluated through a computational experiment, a\nreal-world case study with one expert, and a controlled user study with twelve\nnon-experts, compared against a baseline from prior work, showing its\nsuperiority and effectiveness in largescale spatial time series analysis.", "AI": {"tldr": "This paper introduces VolumeSTCube, a novel framework for visualizing large-scale spatial time series data, addressing issues of visual occlusion and depth ambiguity in traditional space-time cube methods.", "motivation": "There is a need for improved visualization techniques for large-scale spatial time series data, as existing methods struggle with integrating temporal and spatial information effectively.", "method": "The VolumeSTCube framework transforms discrete spatial time series data into continuous volumetric data, utilizing volume and surface rendering techniques to enhance visualization and mitigate occlusion.", "result": "VolumeSTCube was evaluated through computational experiments and user studies, demonstrating superiority in effectiveness for large-scale spatial time series analysis compared to prior approaches.", "conclusion": "The proposed framework offers significant improvements in visualizing large-scale spatial time series data, allowing for better exploration and analysis of spatiotemporal phenomena.", "key_contributions": ["Introduction of the VolumeSTCube framework for spatiotemporal visualization.", "Utilization of volume and surface rendering techniques to enhance visualization quality.", "Evaluation through expert and user studies demonstrating effectiveness over existing methods."], "limitations": "The study is primarily focused on specific datasets and may require further validation across diverse spatiotemporal domains.", "keywords": ["spatial time series", "visualization", "Space-Time Cube", "VolumeSTCube", "rendering techniques"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.09025", "pdf": "https://arxiv.org/pdf/2507.09025.pdf", "abs": "https://arxiv.org/abs/2507.09025", "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "categories": ["cs.CL", "cs.LG"], "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.", "AI": {"tldr": "Lizard is a linearization framework that transforms LLMs into more efficient architectures for infinite-context generation, overcoming memory and computational challenges of conventional Transformer models.", "motivation": "To address the significant memory and computational bottlenecks in Transformer-based LLMs that arise from quadratic complexity in attention mechanisms, especially at longer context lengths.", "method": "Lizard introduces a subquadratic attention mechanism that approximates softmax attention while integrating a gating module for adaptive memory control. It combines gated linear attention for global context with sliding window attention, and includes a hardware-aware algorithm to enhance training speed.", "result": "Lizard outperforms previous linearization methods, achieving near-lossless performance recovery of teacher models in language tasks, with an 18-point improvement on the 5-shot MMLU benchmark and enhanced performance in associative recall tasks.", "conclusion": "Lizard provides a more efficient architecture for LLMs, achieving both flexibility in design and improved performance, particularly at higher context lengths.", "key_contributions": ["Introduction of a subquadratic attention mechanism for LLMs", "Incorporation of a gating module for adaptive memory control", "Development of a hardware-aware training acceleration algorithm"], "limitations": "", "keywords": ["Linearization", "Large Language Models", "Attention Mechanism", "Adaptive Memory Control", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.09959", "pdf": "https://arxiv.org/pdf/2507.09959.pdf", "abs": "https://arxiv.org/abs/2507.09959", "title": "Branch Explorer: Leveraging Branching Narratives to Support Interactive 360° Video Viewing for Blind and Low Vision Users", "authors": ["Shuchang Xu", "Xiaofu Jin", "Wenshuo Zhang", "Huamin Qu", "Yukang Yan"], "categories": ["cs.HC"], "comment": null, "summary": "360{\\deg} videos enable users to freely choose their viewing paths, but blind\nand low vision (BLV) users are often excluded from this interactive experience.\nTo bridge this gap, we present Branch Explorer, a system that transforms\n360{\\deg} videos into branching narratives -- stories that dynamically unfold\nbased on viewer choices -- to support interactive viewing for BLV audiences.\nOur formative study identified three key considerations for accessible\nbranching narratives: providing diverse branch options, ensuring coherent story\nprogression, and enabling immersive navigation among branches. To address these\nneeds, Branch Explorer employs a multi-modal machine learning pipeline to\ngenerate diverse narrative paths, allowing users to flexibly make choices at\ndetected branching points and seamlessly engage with each storyline through\nimmersive audio guidance. Evaluation with 12 BLV viewers showed that Branch\nExplorer significantly enhanced user agency and engagement in 360{\\deg} video\nviewing. Users also developed personalized strategies for exploring 360{\\deg}\ncontent. We further highlight implications for supporting accessible\nexploration of videos and virtual environments.", "AI": {"tldr": "Branch Explorer transforms 360° videos into accessible branching narratives for blind and low vision users, enhancing engagement through a multi-modal machine learning pipeline.", "motivation": "To support interactive viewing experiences in 360° videos for blind and low vision users who are typically excluded from such experiences.", "method": "A multi-modal machine learning pipeline generates diverse branching narratives based on viewer choices, complemented by immersive audio guidance for navigation.", "result": "Evaluation with 12 blind and low vision viewers showed significant improvements in user agency and engagement, as well as the development of personalized exploration strategies.", "conclusion": "Branch Explorer effectively enhances accessibility in 360° video content, providing key insights into creating inclusive virtual environments.", "key_contributions": ["Transforms 360° video into accessible narratives for BLV users.", "Employs machine learning for dynamic story generation.", "Evaluates impact on user engagement and agency."], "limitations": "", "keywords": ["360° videos", "blind and low vision", "branching narratives", "accessible technology", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.09037", "pdf": "https://arxiv.org/pdf/2507.09037.pdf", "abs": "https://arxiv.org/abs/2507.09037", "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers.", "AI": {"tldr": "The ALIGN system personalizes LLM-based decision-making through prompt-based alignment, featuring robust configuration management and enabling side-by-side LLM comparisons.", "motivation": "To address the diverse values and preferences of users that impact decision-making with LLMs, necessitating alignment and personalization.", "method": "Development of the ALIGN system that utilizes dynamic prompt-based alignment to configure decision-making attributes and allows user interface for qualitative LLM comparisons.", "result": "The ALIGN framework was quantitatively analyzed for demographic alignment in public opinion surveys and value alignment for medical triage, demonstrating its versatility and effectiveness.", "conclusion": "ALIGN is an open-source framework that facilitates research into personalized and reliable LLM decision-making tools.", "key_contributions": ["Introduction of dynamic personalization for LLM decision aids", "Development of a modular backend for algorithm integration", "Quantitative analysis of alignment approaches in multiple domains"], "limitations": "", "keywords": ["LLM", "alignment", "personalization", "decision-making", "open source"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.10024", "pdf": "https://arxiv.org/pdf/2507.10024.pdf", "abs": "https://arxiv.org/abs/2507.10024", "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles", "authors": ["Shaolun Ruan", "Rui Sheng", "Xiaolin Wen", "Jiachen Wang", "Tianyi Zhang", "Yong Wang", "Tim Dwyer", "Jiannan Li"], "categories": ["cs.HC"], "comment": null, "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research.", "AI": {"tldr": "This paper explores the role of large language models (LLMs) in enhancing the design study process in visualization research through qualitative insights from design study researchers.", "motivation": "To address the lack of systematic understanding of how LLMs can assist in visualization-specific design studies.", "method": "Conducted a multi-stage qualitative study involving in-depth interviews and questionnaires with 30 design study researchers.", "result": "Identified strategies for utilizing LLMs in the design study process, along with challenges faced and best practices to overcome them.", "conclusion": "The study provides practical implications and a framework for leveraging LLMs to enhance visualization research methodologies.", "key_contributions": ["Identification of LLM roles in design study processes", "Strategies and practices for effective LLM utilization", "Framework for practitioners to enhance visualization studies with LLMs"], "limitations": "", "keywords": ["Large Language Models", "Visualization", "Design Studies", "HCI", "Qualitative Research"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.09075", "pdf": "https://arxiv.org/pdf/2507.09075.pdf", "abs": "https://arxiv.org/abs/2507.09075", "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Aleksander Ficek", "Sean Narenthiran", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Vahid Noroozi", "Boris Ginsburg"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "Recent advancements in reasoning-based Large Language Models (LLMs),\nparticularly their potential through test-time scaling, have created\nsignificant opportunities for distillation in code generation and critique.\nHowever, progress in both areas fundamentally depends on large-scale,\nhigh-quality datasets. In this work, we introduce OpenCodeReasoning-II, a\ndataset consists of 2.5M question-solution-critique triples (approx. 35K unique\nprogramming questions), making it nearly twice the size of the previous largest\npublicly available code reasoning dataset. In this work, we employ a two-stage\nsupervised fine-tuning strategy. The first stage focuses on fine-tuning for\ncode generation, while the second stage involves the joint training of models\nfor both code generation and critique. Our resulting finetuned Qwen2.5-Instruct\nmodels achieve performance in code generation that either exceeds or equals the\nbest prior open-weight distilled models. Notably, the integration of our code\ngeneration and critique models leads to significant improvements in competitive\ncoding performance. Furthermore, we present an extension of the LiveCodeBench\nbenchmark to specifically support the C++ programming language, thereby\nfacilitating more comprehensive LLM evaluation using this benchmark.", "AI": {"tldr": "The paper introduces OpenCodeReasoning-II, a dataset of 2.5M question-solution-critique triples for enhancing code generation and critique using LLMs.", "motivation": "To address the need for large-scale, high-quality datasets essential for improving reasoning-based LLMs in code generation and critique tasks.", "method": "A two-stage supervised fine-tuning strategy is employed: the first stage fine-tunes for code generation, and the second stage trains models for both code generation and critique.", "result": "The finetuned Qwen2.5-Instruct models perform equal to or better than the best prior open-weight distilled models, significantly improving competitive coding performance when integrating code generation and critique models.", "conclusion": "The findings underscore the efficacy of the proposed dataset and methodology, with extensions like the LiveCodeBench benchmark aiding in LLM evaluation, particularly for the C++ language.", "key_contributions": ["Introduction of the OpenCodeReasoning-II dataset with 2.5M triples", "A two-stage fine-tuning approach for combining code generation and critique", "Extension of LiveCodeBench to support C++ programming language"], "limitations": "", "keywords": ["Large Language Models", "Code Generation", "Code Critique", "Dataset", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.10043", "pdf": "https://arxiv.org/pdf/2507.10043.pdf", "abs": "https://arxiv.org/abs/2507.10043", "title": "XROps: A Visual Workflow Management System for Dynamic Immersive Analytics", "authors": ["Suemin Jeon", "JunYoung Choi", "Haejin Jeong", "Won-Ki Jeong"], "categories": ["cs.HC"], "comment": null, "summary": "Immersive analytics is gaining attention across multiple domains due to its\ncapability to facilitate intuitive data analysis in expansive environments\nthrough user interaction with data. However, creating immersive analytics\nsystems for specific tasks is challenging due to the need for programming\nexpertise and significant development effort. Despite the introduction of\nvarious immersive visualization authoring toolkits, domain experts still face\nhurdles in adopting immersive analytics into their workflow, particularly when\nfaced with dynamically changing tasks and data in real time. To lower such\ntechnical barriers, we introduce XROps, a web-based authoring system that\nallows users to create immersive analytics applications through interactive\nvisual programming, without the need for low-level scripting or coding. XROps\nenables dynamic immersive analytics authoring by allowing users to modify each\nstep of the data visualization process with immediate feedback, enabling them\nto build visualizations on-the-fly and adapt to changing environments. It also\nsupports the integration and visualization of real-time sensor data from XR\ndevices, a key feature of immersive analytics, facilitating the creation of\nvarious analysis scenarios. We evaluated the usability of XROps through a user\nstudy and demonstrate its efficacy and usefulness in several example scenarios.\nWe have released a web platform (https://vience.io/xrops) to demonstrate\nvarious examples to supplement our findings.", "AI": {"tldr": "XROps is a web-based authoring system for immersive analytics that allows non-programmers to create interactive data visualizations with real-time updates.", "motivation": "The paper addresses the challenges faced by domain experts in adopting immersive analytics due to the need for programming skills and the complexity of dynamic data and tasks.", "method": "The authors present XROps, an interactive visual programming environment that streamlines the creation of immersive analytics applications without coding.", "result": "A user study demonstrated the usability and effectiveness of XROps in building visualizations that adapt to real-time data changes, enhancing the analytics process.", "conclusion": "XROps successfully lowers technical barriers for creating immersive analytics applications and integrates real-time sensor data to facilitate dynamic analysis scenarios.", "key_contributions": ["Introduction of XROps for interactive visual programming", "Enables real-time modifications of data visualizations", "Integration of real-time sensor data from XR devices"], "limitations": "", "keywords": ["immersive analytics", "visual programming", "real-time data", "XR", "usability study"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.09076", "pdf": "https://arxiv.org/pdf/2507.09076.pdf", "abs": "https://arxiv.org/abs/2507.09076", "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "comment": "submitted to EMNLP 2025", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance.", "AI": {"tldr": "This paper presents a Dynamic Parameter Memory (DPM) mechanism to enhance emotion recognition in speech large language models by allowing for the processing of unlimited-length audio while maintaining contextual awareness.", "motivation": "To address the limitations of speech large language models (SLLM) in processing long audio sequences due to context window constraints.", "method": "The authors propose a Dynamic Parameter Memory (DPM) mechanism that encodes sentence-level information and emotions, facilitating the processing of long audio streams by maintaining contextual semantics.", "result": "The incorporation of DPM into the SLLM framework showed a significant improvement in emotion recognition capabilities, achieving state-of-the-art results on the IEMOCAP dataset.", "conclusion": "DPM effectively enhances SLLM's ability to recognize emotions in conversations over long durations, overcoming the limitations of traditional input token compression methods.", "key_contributions": ["Introduction of Dynamic Parameter Memory (DPM) for SLLM", "Contextual semantics and sentence-level emotion encoding", "Demonstrated state-of-the-art performance on the IEMOCAP dataset for ERC."], "limitations": "", "keywords": ["speech emotion recognition", "large language models", "Dynamic Parameter Memory"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.10044", "pdf": "https://arxiv.org/pdf/2507.10044.pdf", "abs": "https://arxiv.org/abs/2507.10044", "title": "MEDebiaser: A Human-AI Feedback System for Mitigating Bias in Multi-label Medical Image Classification", "authors": ["Shaohan Shi", "Yuheng Shao", "Haoran Jiang", "Yunjie Yao", "Zhijun Zhang", "Xu Ding", "Quan Li"], "categories": ["cs.HC"], "comment": null, "summary": "Medical images often contain multiple labels with imbalanced distributions\nand co-occurrence, leading to bias in multi-label medical image classification.\nClose collaboration between medical professionals and machine learning\npractitioners has significantly advanced medical image analysis. However,\ntraditional collaboration modes struggle to facilitate effective feedback\nbetween physicians and AI models, as integrating medical expertise into the\ntraining process via engineers can be time-consuming and labor-intensive. To\nbridge this gap, we introduce MEDebiaser, an interactive system enabling\nphysicians to directly refine AI models using local explanations. By combining\nprediction with attention loss functions and employing a customized ranking\nstrategy to alleviate scalability, MEDebiaser allows physicians to mitigate\nbiases without technical expertise, reducing reliance on engineers, and thus\nenhancing more direct human-AI feedback. Our mechanism and user studies\ndemonstrate that it effectively reduces biases, improves usability, and\nenhances collaboration efficiency, providing a practical solution for\nintegrating medical expertise into AI-driven healthcare.", "AI": {"tldr": "MEDebiaser is an interactive system that allows physicians to refine AI models directly to reduce bias in medical image classification.", "motivation": "The need for effective feedback loops between physicians and AI models in medical imaging, which are hindered by traditional collaboration methods.", "method": "MEDebiaser combines prediction with attention loss functions and a customized ranking strategy to facilitate physician involvement without requiring technical expertise.", "result": "User studies show that MEDebiaser reduces biases in multi-label medical image classification, improves usability, and enhances collaboration efficiency.", "conclusion": "MEDebiaser offers a practical approach to integrating medical expertise into AI while alleviating the biases observed in traditional methods.", "key_contributions": ["Introducing an interactive system for direct physician-AI feedback", "Development of a mechanism that combines prediction and attention loss to mitigate bias", "Demonstration of improved usability and collaboration efficiency through user studies."], "limitations": "", "keywords": ["Medical Imaging", "Multi-label Classification", "Human-AI Collaboration"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.09104", "pdf": "https://arxiv.org/pdf/2507.09104.pdf", "abs": "https://arxiv.org/abs/2507.09104", "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.", "AI": {"tldr": "CompassJudger-2 is a new generalist judge model addressing the shortcomings of current LLM judge models by employing a task-driven, multi-domain data curation strategy and refined learning objectives, achieving superior judgment accuracy and establishing new evaluation benchmarks.", "motivation": "Current judge models are narrow and lack robustness, limiting their ability to evaluate large language models effectively.", "method": "The paper presents CompassJudger-2, which uses a task-driven multi-domain data curation strategy, supervised judgment tasks with verifiable rewards, and a refined learning objective with margin policy gradient loss to improve judge model performance.", "result": "CompassJudger-2 outperforms existing judge models significantly in terms of judgment accuracy and achieves competitive performance compared to larger models.", "conclusion": "The advancements made through CompassJudger-2 contribute to more robust and scalable LLM judgment processes and set new standards for model evaluation.", "key_contributions": ["Introduction of CompassJudger-2, a generalist judge model", "Development of JudgerBenchV2 for cross-domain judgment accuracy evaluation", "Implementation of refined learning objectives to enhance performance"], "limitations": "", "keywords": ["LLM evaluation", "generalist model", "cross-domain judgment"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.10099", "pdf": "https://arxiv.org/pdf/2507.10099.pdf", "abs": "https://arxiv.org/abs/2507.10099", "title": "ReDemon UI: Reactive Synthesis by Demonstration for Web UI", "authors": ["Jay Lee", "Gyuhyeok Oh", "Joongwon Ahn", "Xiaokang Qiu"], "categories": ["cs.HC", "cs.PL"], "comment": "Submitted to UIST 2025 Posters", "summary": "ReDemon UI synthesizes React applications from user demonstrations, enabling\ndesigners and non-expert programmers to create UIs that integrate with standard\nUI prototyping workflows. Users provide a static mockup sketch with event\nhandler holes and demonstrate desired runtime behaviors by interacting with the\nrendered mockup and editing the sketch. ReDemon UI identifies reactive data and\nsynthesizes a React program with correct state update logic. We utilize\nenumerative synthesis for simple UIs and LLMs for more complex UIs.", "AI": {"tldr": "ReDemon UI synthesizes React applications from user demonstrations to aid designers in creating UIs using mockup sketches and interactions.", "motivation": "To enable designers and non-expert programmers to easily create UIs that can be integrated into common UI prototyping workflows.", "method": "The system allows users to provide a static mockup sketch and interact with it to demonstrate desired behaviors. It identifies reactive data and synthesizes a React program using enumerative synthesis and LLMs for complexity.", "result": "ReDemon UI successfully generates React applications with correct state update logic based on user-provided demonstrations and sketches.", "conclusion": "ReDemon UI bridges the gap between design and development, allowing for efficient UI synthesis from demonstrative interactions.", "key_contributions": ["Synthesis of React applications from user demonstrations", "Integration of static mockups and runtime behavior demonstration", "Use of LLMs for complex UI generation"], "limitations": "", "keywords": ["User Interface Design", "React", "UI Prototyping", "Machine Learning", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.09155", "pdf": "https://arxiv.org/pdf/2507.09155.pdf", "abs": "https://arxiv.org/abs/2507.09155", "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields.", "AI": {"tldr": "OPENXRD is an open-book question answering pipeline for crystallography using GPT-4.5 to generate concise references that aid smaller models in X-ray diffraction (XRD) tasks.", "motivation": "The need for effective question answering in crystallography without copyright challenges posed by traditional textbooks.", "method": "OPENXRD integrates textual prompts with generated summaries from GPT-4.5, evaluated against expert-level XRD questions comparing several vision-language models.", "result": "Models utilizing the GPT-4.5-generated summaries showed significant accuracy improvements, especially in those with limited crystallography training.", "conclusion": "OPENXRD establishes that AI-generated resources can enhance reasoning in scientific questions and lays groundwork for NLP applications in materials science.", "key_contributions": ["Introduces OPENXRD, an innovative open-book system for crystallography question answering.", "Demonstrates improved accuracy in response to expert XRD questions using GPT-4.5 summaries.", "Highlights the potential of AI-generated content to assist smaller models in scientific reasoning."], "limitations": "Current version focuses solely on text-based inputs; future developments may enhance with visual data.", "keywords": ["crystallography", "question answering", "GPT-4.5", "X-ray diffraction", "natural language processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.10102", "pdf": "https://arxiv.org/pdf/2507.10102.pdf", "abs": "https://arxiv.org/abs/2507.10102", "title": "When Familiarity Remains: Procedural Memory, Symbolic Anchors, and Digital Engagement in Dementia Care", "authors": ["Jeongone Seo", "Kyung-zoon Hong", "Sol Baik"], "categories": ["cs.HC"], "comment": "23 page, 2 tables, 1 figure", "summary": "INTRODUCTION: Older adults with early-stage dementia often retain procedural\nmemory, enabling continued use of familiar technologies. Additionally, symbolic\nanchors such as photos or personalized content may serve as memory cues to\nreinforce digital engagement. This study explores how these mechanisms support\ntechnology use in dementia care within the South Korean context.\n  METHODS: We conducted in-depth interviews with 11 professional caregivers of\ncommunity-dwelling older adults with cognitive decline. Grounded theory methods\nguided the analysis, using iterative coding and constant comparison to identify\nemergent themes.\n  RESULTS: Caregivers reported that familiar digital routines (e.g., taking\nphotos) persisted through procedural memory. Symbolic anchors such as family\nphotos or recognizable icons enhanced interaction and emotional engagement.\nHowever, unfamiliar or anthropomorphic technologies often triggered fear or\nsymbolic resistance.\n  DISCUSSION: Findings highlight the dual role of procedural memory and\nsymbolic anchors in sustaining digital engagement. Designing culturally\nresponsive and cognitively accessible technologies may enhance autonomy and\nwell-being in dementia care.\n  Keywords: procedural memory, symbolic anchors, dementia care, digital\nengagement, older adults, cultural adaptation, caregiving technologies", "AI": {"tldr": "This study investigates how procedural memory and symbolic anchors aid digital technology use among older adults with early-stage dementia in South Korea, based on interviews with caregivers.", "motivation": "To explore mechanisms that support technology use in dementia care, particularly through procedural memory and symbolic memory anchors.", "method": "In-depth interviews with 11 professional caregivers, analyzed using grounded theory methods with iterative coding and constant comparison.", "result": "Caregivers found that familiar digital routines endure due to procedural memory, and that symbolic anchors enhance emotional engagement, although unfamiliar technologies can cause fear.", "conclusion": "The study underscores the importance of procedural memory and symbolic anchors in promoting digital engagement among older adults with dementia and calls for culturally responsive technology design.", "key_contributions": ["Identified the role of procedural memory in technology use for older adults with dementia", "Demonstrated the effectiveness of symbolic anchors in enhancing digital engagement", "Provided insights into culturally responsive technology design for dementia care"], "limitations": "The study is limited by its small sample size and focus on a specific cultural context.", "keywords": ["procedural memory", "symbolic anchors", "dementia care", "digital engagement", "older adults"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.09157", "pdf": "https://arxiv.org/pdf/2507.09157.pdf", "abs": "https://arxiv.org/abs/2507.09157", "title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning", "authors": ["Bhavinkumar Vinodbhai Kuwar", "Bikrant Bikram Pratap Maurya", "Priyanshu Gupta", "Nitin Choudhury"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting deception in strategic dialogues is a complex and high-stakes task\ndue to the subtlety of language and extreme class imbalance between deceptive\nand truthful communications. In this work, we revisit deception detection in\nthe Diplomacy dataset, where less than 5% of messages are labeled deceptive. We\nintroduce a lightweight yet effective model combining frozen BERT embeddings,\ninterpretable linguistic and game-specific features, and a Positive-Unlabeled\n(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is\ntailored for situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new best macro F1\nof 0.60 while reducing trainable parameters by over 650x. Through comprehensive\nevaluations and ablation studies across seven models, we demonstrate the value\nof PU learning, linguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately detecting\ndeception is more critical than identifying truthful messages. This priority\nguides our choice of PU learning, which explicitly models the rare but vital\ndeceptive class.", "AI": {"tldr": "This paper introduces PU-Lie, a model for detecting deception in strategic dialogues using frozen BERT embeddings and a Positive-Unlabeled learning objective.", "motivation": "The complexity of detecting deception in dialogues due to subtle language nuances and class imbalance between deceptive and truthful messages.", "method": "A lightweight model combining frozen BERT embeddings with linguistic and game-specific features, alongside a Positive-Unlabeled (PU) learning objective for improved deception detection.", "result": "PU-Lie achieves a new best macro F1 score of 0.60 while significantly reducing the number of trainable parameters.", "conclusion": "Properly prioritizing the detection of deception over truthful messages is critical, reflecting the challenges of severe class imbalance in deceptive communication contexts.", "key_contributions": ["Introduction of PU-Lie model for deception detection in dialogues", "Achievement of a new best macro F1 score of 0.60", "Demonstration of the effectiveness of PU learning and linguistic interpretability."], "limitations": "", "keywords": ["deception detection", "Positive-Unlabeled learning", "BERT embeddings", "strategic dialogues", "language subtleties"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.10240", "pdf": "https://arxiv.org/pdf/2507.10240.pdf", "abs": "https://arxiv.org/abs/2507.10240", "title": "Visual Analytics for Explainable and Trustworthy Artificial Intelligence", "authors": ["Angelos Chatzimparmpas"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Our society increasingly depends on intelligent systems to solve complex\nproblems, ranging from recommender systems suggesting the next movie to watch\nto AI models assisting in medical diagnoses for hospitalized patients. With the\niterative improvement of diagnostic accuracy and efficiency, AI holds\nsignificant potential to mitigate medical misdiagnoses by preventing numerous\ndeaths and reducing an economic burden of approximately 450 EUR billion\nannually. However, a key obstacle to AI adoption lies in the lack of\ntransparency: many automated systems function as \"black boxes,\" providing\npredictions without revealing the underlying processes. This opacity can hinder\nexperts' ability to trust and rely on AI systems. Visual analytics (VA)\nprovides a compelling solution by combining AI models with interactive\nvisualizations. These specialized charts and graphs empower users to\nincorporate their domain expertise to refine and improve the models, bridging\nthe gap between AI and human understanding. In this work, we define,\ncategorize, and explore how VA solutions can foster trust across the stages of\na typical AI pipeline. We propose a design space for innovative visualizations\nand present an overview of our previously developed VA dashboards, which\nsupport critical tasks within the various pipeline stages, including data\nprocessing, feature engineering, hyperparameter tuning, understanding,\ndebugging, refining, and comparing models.", "AI": {"tldr": "The paper explores how visual analytics can enhance trust in AI models by providing interactive visualizations that allow users to understand and refine these models throughout the AI pipeline.", "motivation": "AI has the potential to greatly improve medical diagnoses but is often hindered by a lack of transparency in its decision-making processes.", "method": "The paper defines and categorizes visual analytics (VA) solutions and proposes a design space for innovative visualizations that support AI model transparency.", "result": "The proposed VA solutions empower users to leverage their domain expertise to refine AI models and improve trust across the AI pipeline stages.", "conclusion": "The integration of visual analytics into the AI workflow can significantly enhance the trust and effectiveness of AI systems in critical applications like healthcare.", "key_contributions": ["Definition and categorization of visual analytics solutions for AI", "Proposal of a design space for innovative visualizations", "Development of VA dashboards that support critical AI pipeline tasks"], "limitations": "", "keywords": ["Visual Analytics", "AI Trust", "Medical Diagnostics", "Human-Computer Interaction", "Model Interpretation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.09174", "pdf": "https://arxiv.org/pdf/2507.09174.pdf", "abs": "https://arxiv.org/abs/2507.09174", "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking", "authors": ["Shuo Yang", "Zijian Yu", "Zhenzhe Ying", "Yuqin Dai", "Guoqing Wang", "Jun Lan", "Jinfeng Xu", "Jinze Li", "Edith C. H. Ngai"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git.", "AI": {"tldr": "RAMA is a multi-agent framework for verifying multimedia misinformation, enhancing automated fact-checking through innovative query formulation, evidence aggregation, and the use of multimodal large language models.", "motivation": "The rise of multimodal misinformation complicates automated fact-checking due to ambiguity and lack of context in claims.", "method": "RAMA uses strategic query formulation, cross-verification from diverse sources, and a multi-agent ensemble architecture to process and verify multimedia claims.", "result": "RAMA demonstrates superior performance in verifying ambiguous claims by effectively grounding verification in retrieved factual evidence, particularly outperforming existing solutions.", "conclusion": "The integration of web-based evidence and multi-agent reasoning is essential for reliable multimedia verification, which RAMA facilitates, offering a scalable fact-checking solution.", "key_contributions": ["Strategic query formulation for multimedia claims", "Cross-verification from diverse sources", "Multi-agent ensemble architecture utilizing LLMs."], "limitations": "", "keywords": ["multimodal misinformation", "fact-checking", "large language models", "automated verification", "multi-agent systems"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.10427", "pdf": "https://arxiv.org/pdf/2507.10427.pdf", "abs": "https://arxiv.org/abs/2507.10427", "title": "Towards Emotion Co-regulation with LLM-powered Socially Assistive Robots: Integrating LLM Prompts and Robotic Behaviors to Support Parent-Neurodivergent Child Dyads", "authors": ["Jing Li", "Felix Schijve", "Sheng Li", "Yuye Yang", "Jun Hu", "Emilia Barakova"], "categories": ["cs.HC", "cs.RO"], "comment": "Submission for the IROS 2025 conference", "summary": "Socially Assistive Robotics (SAR) has shown promise in supporting emotion\nregulation for neurodivergent children. Recently, there has been increasing\ninterest in leveraging advanced technologies to assist parents in co-regulating\nemotions with their children. However, limited research has explored the\nintegration of large language models (LLMs) with SAR to facilitate emotion\nco-regulation between parents and children with neurodevelopmental disorders.\nTo address this gap, we developed an LLM-powered social robot by deploying a\nspeech communication module on the MiRo-E robotic platform. This supervised\nautonomous system integrates LLM prompts and robotic behaviors to deliver\ntailored interventions for both parents and neurodivergent children. Pilot\ntests were conducted with two parent-child dyads, followed by a qualitative\nanalysis. The findings reveal MiRo-E's positive impacts on interaction dynamics\nand its potential to facilitate emotion regulation, along with identified\ndesign and technical challenges. Based on these insights, we provide design\nimplications to advance the future development of LLM-powered SAR for mental\nhealth applications.", "AI": {"tldr": "The paper presents an LLM-powered social robot developed to assist in emotion co-regulation between parents and neurodivergent children, showing positive interactions and identifying design implications.", "motivation": "To explore the integration of LLMs with Socially Assistive Robotics for emotion regulation in neurodivergent children and their parents, addressing a gap in existing research.", "method": "An LLM-powered social robot was deployed on the MiRo-E platform, incorporating speech communication and tailored interventions for parent-child interaction.", "result": "Pilot tests indicated positive impacts on the interaction dynamics between parent-child dyads and highlighted the robot's ability to facilitate emotion regulation, along with design and technical challenges faced.", "conclusion": "The findings suggest the viability of LLM-powered SAR systems for mental health, providing key insights for future designs and applications.", "key_contributions": ["Development of an LLM-powered social robot for emotion co-regulation", "Qualitative analysis of interaction dynamics among users", "Identification of design implications for future SAR development."], "limitations": "Limited pilot test sample size and scope of interactions assessed.", "keywords": ["Socially Assistive Robotics", "Emotion Regulation", "Large Language Models", "Neurodivergent Children", "Mental Health"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.09185", "pdf": "https://arxiv.org/pdf/2507.09185.pdf", "abs": "https://arxiv.org/abs/2507.09185", "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models", "authors": ["Ameen Ali", "Shahar Katz", "Lior Wolf", "Ivan Titov"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) often develop learned mechanisms specialized to\nspecific datasets, such as reliance on domain-specific correlations, which\nyield high-confidence predictions without generalizable reasoning. While\nbeneficial in one setting, these dataset-specific mechanisms typically degrade\nperformance when models encounter novel tasks or distributions. In this work,\nwe introduce a fine-tuning approach designed to enhance generalization by\nidentifying and pruning neurons associated with dataset-specific mechanisms in\ntransformer-based LLMs. Our method employs Integrated Gradients to quantify\neach neuron's influence on high-confidence predictions, pinpointing those that\ndisproportionately contribute to dataset-specific performance without\nsupporting robust, transferable reasoning. Selectively pruning these neurons\ncompels the model to depend on generalizable representations. Evaluated across\nmultiple-choice benchmarks, our pruning-based fine-tuning significantly\nenhances performance, surpassing prior (non-pruning) adaptation methods.", "AI": {"tldr": "This paper presents a fine-tuning method for large language models that improves generalization by pruning neurons responsible for dataset-specific mechanisms, enhancing transferable reasoning capabilities.", "motivation": "To mitigate the issue of large language models relying on dataset-specific mechanisms that hinder generalization to novel tasks and distributions.", "method": "The proposed method utilizes Integrated Gradients to assess each neuron's impact on predictions, allowing for selective pruning of neurons linked to non-generalizable performance.", "result": "The pruning-based fine-tuning approach significantly improves performance on multiple-choice benchmarks compared to previous adaptation methods that do not utilize pruning.", "conclusion": "Fine-tuning through neuron pruning leads to better generalization and robustness in transformer-based language models, enabling more reliable performance on diverse tasks.", "key_contributions": ["Introduction of a neuron pruning approach to enhance generalization in LLMs", "Application of Integrated Gradients for neuron influence quantification", "Demonstration of improved performance over non-pruning methods across multiple benchmarks."], "limitations": "", "keywords": ["Large language models", "Fine-tuning", "Neural network pruning", "Generalization", "Integrated Gradients"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.10469", "pdf": "https://arxiv.org/pdf/2507.10469.pdf", "abs": "https://arxiv.org/abs/2507.10469", "title": "An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments", "authors": ["Mikko Korkiakoski", "Saeid Sheikhi", "Jesper Nyman", "Jussi Saariniemi", "Kalle Tapio", "Panos Kostakos"], "categories": ["cs.HC", "cs.AI", "cs.MM"], "comment": null, "summary": "Advancements in artificial intelligence (AI) have significantly enhanced the\nrealism and interactivity of non-player characters (NPCs) in virtual reality\n(VR), creating more engaging and believable user experiences. This paper\nevaluates AI-driven NPCs within a VR interrogation simulator, focusing on their\nperceived realism, usability, and system performance. The simulator features\ntwo AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage\nparticipants in a scenario to determine the suspect's guilt or innocence. A\nuser study with 18 participants assessed the system using the System Usability\nScale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent\nBelievability Questionnaire, alongside latency measurements for speech-to-text\n(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.\nResults showed an average cycle latency of 7 seconds, influenced by the\nincreasing conversational context. Believability scored 6.67 out of 10, with\nhigh ratings in behavior, social relationships, and intelligence but moderate\nscores in emotion and personality. The system achieved a SUS score of 79.44,\nindicating good usability. These findings demonstrate the potential of large\nlanguage models to improve NPC realism and interaction in VR while highlighting\nchallenges in reducing system latency and enhancing emotional depth. This\nresearch contributes to the development of more sophisticated AI-driven NPCs,\nrevealing the need for performance optimization to achieve increasingly\nimmersive virtual experiences.", "AI": {"tldr": "This paper evaluates AI-driven NPCs in a VR interrogation simulator, highlighting their realism, usability, and performance.", "motivation": "To enhance the realism and interactivity of non-player characters in virtual reality, thereby improving user experiences.", "method": "A user study with 18 participants assessed NPCs using the System Usability Scale, Game Experience Questionnaire, and a Virtual Agent Believability Questionnaire while measuring system latency.", "result": "The AI NPCs scored 6.67 on believability and a SUS score of 79.44, indicating good usability. Average cycle latency was 7 seconds, influenced by conversational context.", "conclusion": "The study demonstrates the potential of large language models to improve NPC realism and interaction in VR and highlights challenges like latency reduction and emotional depth enhancement.", "key_contributions": ["Demonstrated the viability of AI-driven NPCs using GPT-4 Turbo in VR settings.", "Provided empirical data on believability and usability of NPCs in user studies.", "Identified performance optimization needs for improving emotional aspects of NPCs."], "limitations": "Limited sample size of 18 participants may affect generalizability of results.", "keywords": ["NPCs", "virtual reality", "AI", "GPT-4 Turbo", "usability"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.09205", "pdf": "https://arxiv.org/pdf/2507.09205.pdf", "abs": "https://arxiv.org/abs/2507.09205", "title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Sangjee Dondrub", "Caizang Tai", "Haixing Zhao", "Huaque Cairang", "Suonan Cairang", "Rou Te", "Lengben Zhaxi", "Gazang Zhaxi", "Zhonglin Ye", "Yuhui Zheng", "Chunyan Peng", "Secha Jia", "Pema Tashi", "Cizhen Jiacuo", "Pema Dorjee", "Hongkai Liu", "Pema Yanggon", "Tsehang Dorjee", "Jiaxin Han", "Qiongying Hu", "Jilin Man", "Huanke You", "Yuqi Ren", "Duo La", "Deyi Xiong"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks.", "AI": {"tldr": "This paper introduces Banzhida, a multilingual large language model tailored for Tibetan, addressing the scarcity of high-quality data by curating the largest Tibetan pre-training corpus and creating new evaluation benchmarks.", "motivation": "To improve the performance of language models for Tibetan, which is a low-resource language that lacks sufficient training data and high-quality models.", "method": "Curated the largest Tibetan pre-training corpus from diverse sources and applied a dedicated data cleaning and processing pipeline; conducted pre/post-training of a multilingual base model into Banzhida.", "result": "Banzhida significantly outperforms existing open-source and Tibetan-specific models on a variety of language tasks according to newly created and public benchmarks.", "conclusion": "The creation of a high-quality Tibetan pre-training corpus and the development of Banzhida mark significant advancements in generative AI for the Tibetan language.", "key_contributions": ["Curated the largest Tibetan pre-training corpus to date.", "Developed Banzhida, a dedicated multilingual large language model for Tibetan.", "Created new high-quality benchmarks for evaluating Tibetan language tasks."], "limitations": "", "keywords": ["Tibetan", "language model", "low-resource", "data curation", "generative AI"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2507.10479", "pdf": "https://arxiv.org/pdf/2507.10479.pdf", "abs": "https://arxiv.org/abs/2507.10479", "title": "VIP-Sim: A User-Centered Approach to Vision Impairment Simulation for Accessible Design", "authors": ["Max Rädler", "Mark Colley", "Enrico Rukzio"], "categories": ["cs.HC"], "comment": "Conditionally accepted at UIST'25", "summary": "People with vision impairments (VIPs) often rely on their remaining vision\nwhen interacting with user interfaces. Simulating visual impairments is an\neffective tool for designers, fostering awareness of the challenges faced by\nVIPs. While previous research has introduced various vision impairment\nsimulators, none have yet been developed with the direct involvement of VIPs or\nthoroughly evaluated from their perspective. To address this gap, we developed\nVIP-Sim. This symptom-based vision simulator was created through a\nparticipatory design process tailored explicitly for this purpose, involving\nN=7 VIPs. 21 symptoms, like field loss or light sensitivity, can be overlaid on\ndesktop design tools. Most participants felt VIP-Sim could replicate their\nsymptoms. VIP-Sim was received positively, but concerns about exclusion in\ndesign and comprehensiveness of the simulation remain, mainly whether it\nrepresents the experiences of other VIPs.", "AI": {"tldr": "Development of VIP-Sim, a vision impairment simulator designed with input from individuals with vision impairments to enhance awareness in user interface design.", "motivation": "To improve the design processes for user interfaces by allowing designers to better understand the challenges faced by individuals with vision impairments through simulation.", "method": "Participatory design process involving 7 individuals with vision impairments to create a symptom-based vision simulator that overlays 21 symptoms on design tools.", "result": "Participants felt that VIP-Sim effectively replicated their visual symptoms and it was well-received overall, though concerns regarding the simulation's comprehensiveness and representation of different experiences were raised.", "conclusion": "VIP-Sim provides a valuable tool for designers, but further evaluation is needed to address concerns about inclusivity and completeness of the simulation.", "key_contributions": ["First vision impairment simulator developed directly with VIP involvement", "Includes 21 different visual symptoms for realistic overlays", "Offers a new perspective on inclusive design in user interfaces"], "limitations": "Concerns regarding the representation of the diverse experiences of all vision impairments and potential exclusion in the design process.", "keywords": ["vision impairment", "user interface design", "participatory design", "simulation", "accessibility"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2507.09225", "pdf": "https://arxiv.org/pdf/2507.09225.pdf", "abs": "https://arxiv.org/abs/2507.09225", "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "categories": ["cs.CL", "cs.CY"], "comment": "27 pages, 5 figures", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.", "AI": {"tldr": "This study explores the impact of visual metaphors in climate change communication through a novel database of images, revealing their higher cognitive load and aesthetic appreciation compared to literal images.", "motivation": "To understand the communication effectiveness of visual metaphors in climate change and provide a valuable resource for future research.", "method": "A database of Metaphors of Climate Change in Images (MetaClimage) was created, including human ratings for difficulty, efficacy, artistic quality, and emotional arousal; NLP was used to derive semantic variables from participant-generated tags.", "result": "Visual metaphors were rated more difficult to understand but more aesthetically pleasing than literal images; they generated more positive and dominant responses but did not differ significantly in efficacy and emotional arousal.", "conclusion": "Visual metaphors present a cost-benefit trade-off in environmental communication, offering aesthetic value and cognitive depth despite a higher cognitive load and similar efficacy to literal images.", "key_contributions": ["Development of a novel database for climate change visual metaphors", "Insights into cognitive load versus aesthetic appreciation in communication", "Evaluation of emotional and semantic dimensions related to visual metaphors"], "limitations": "The study may have limitations due to the subjective nature of human ratings and the specific context of climate change imagery.", "keywords": ["visual metaphors", "climate change", "environmental communication", "human ratings", "Natural Language Processing"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2507.09245", "pdf": "https://arxiv.org/pdf/2507.09245.pdf", "abs": "https://arxiv.org/abs/2507.09245", "title": "Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources", "authors": ["Deshan Sumanathilaka", "Sameera Perera", "Sachithya Dharmasiri", "Maneesha Athukorala", "Anuja Dilrukshi Herath", "Rukshan Dias", "Pasindu Gamage", "Ruvan Weerasinghe", "Y. H. P. P. Priyadarshana"], "categories": ["cs.CL"], "comment": "13 pages, 3 Tables, 3 figures", "summary": "The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain.", "AI": {"tldr": "The Swa-bhasha Resource Hub aids Sinhala NLP through data resources and algorithms for Romanized Sinhala to Sinhala transliteration.", "motivation": "To advance Sinhala NLP research and improve transliteration processes using Romanized Sinhala.", "method": "The paper provides an overview of data resources, algorithms, and a comparative analysis of transliteration applications.", "result": "The hub has made significant contributions to the field by providing openly accessible data sets and tools for transliteration model training.", "conclusion": "The resources presented promote further research and application development in Sinhala NLP.", "key_contributions": ["Creation of the Swa-bhasha Resource Hub", "Open access to data resources and algorithms", "Comparative analysis of transliteration applications."], "limitations": "", "keywords": ["Romanized Sinhala", "transliteration", "Sinhala NLP", "data resources", "natural language processing"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.09259", "pdf": "https://arxiv.org/pdf/2507.09259.pdf", "abs": "https://arxiv.org/abs/2507.09259", "title": "Psychology-Driven Enhancement of Humour Translation", "authors": ["Yuchen Su", "Yonghua Zhu", "Yang Chen", "Diana Benavides-Prado", "Michael Witbrock"], "categories": ["cs.CL"], "comment": null, "summary": "Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text.", "AI": {"tldr": "This paper presents a psychology-inspired Humour Decomposition Mechanism (HDM) that improves humour translation in LLMs by using Chain-of-Thought techniques and integrating humour theory.", "motivation": "The need for effective humour translation across cultures due to the limitations of existing LLMs in rendering humour accurately in translations.", "method": "A Humour Decomposition Mechanism (HDM) inspired by psychology, utilizing Chain-of-Thought processes to enhance LLMs' translation abilities, particularly in humour.", "result": "Experiments show that HDM significantly improves humour translation quality, with gains of 7.75% in humour, 2.81% in fluency, and 6.13% in coherence when compared to prior models.", "conclusion": "The proposed HDM effectively enhances the readability and humorous elements of translated texts, suggesting a viable approach to improving humour translation in LLMs.", "key_contributions": ["Introduction of the Humour Decomposition Mechanism (HDM)", "Utilization of Chain-of-Thought for humour translation", "Integration of humour theory into translation processes"], "limitations": "", "keywords": ["humour translation", "large language models", "Chain-of-Thought", "humour theory", "machine translation"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.09282", "pdf": "https://arxiv.org/pdf/2507.09282.pdf", "abs": "https://arxiv.org/abs/2507.09282", "title": "ClaritySpeech: Dementia Obfuscation in Speech", "authors": ["Dominika Woszczyk", "Ranya Aloufi", "Soteris Demetriou"], "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility.", "AI": {"tldr": "This paper introduces ClaritySpeech, a framework that integrates ASR, text obfuscation, and zero-shot TTS to enhance communication for individuals with dementia while preserving privacy and speaker identity.", "motivation": "To address communication barriers caused by dementia and enhance accessibility while maintaining privacy when using speech technologies.", "method": "The paper presents ClaritySpeech, which combines automatic speech recognition (ASR), text obfuscation, and zero-shot text-to-speech (TTS) techniques to correct dementia-affected speech in low-data environments.", "result": "ClaritySpeech achieved a decline in mean F1 score by 16% and 10% in various settings, while improving word error rate (WER) significantly and enhancing speech quality.", "conclusion": "The framework demonstrates that it is possible to enhance accessibility and privacy in speech technologies for individuals with dementia, without the need for extensive data or fine-tuning.", "key_contributions": ["Introduction of ClaritySpeech framework for dementia speech correction", "Integration of ASR, text obfuscation, and zero-shot TTS", "Maintains speaker identity while improving accessibility and privacy"], "limitations": "The system shows a reduction in mean F1 score under adversarial conditions.", "keywords": ["dementia", "speech technology", "automatic speech recognition", "text-to-speech", "privacy"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.09424", "pdf": "https://arxiv.org/pdf/2507.09424.pdf", "abs": "https://arxiv.org/abs/2507.09424", "title": "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models", "authors": ["Cathy Jiao", "Yijun Pan", "Emily Xiao", "Daisy Sheng", "Niket Jain", "Hanzhang Zhao", "Ishita Dasgupta", "Jiaqi W. Ma", "Chenyan Xiong"], "categories": ["cs.CL"], "comment": null, "summary": "Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs.", "AI": {"tldr": "DATE-LM is a benchmark for evaluating data attribution methods in LLMs through real-world applications, focusing on training data selection, toxicity/bias filtering, and factual attribution.", "motivation": "To address the critical gaps in systematic LLM-centric evaluation of data attribution methods which are essential for dataset curation, model interpretability, and data valuation.", "method": "DATE-LM benchmark evaluates data attribution through three tasks: training data selection, toxicity/bias filtering, and factual attribution, enabling large-scale evaluations across tasks and LLM architectures.", "result": "The evaluation shows that no single data attribution method excels in all tasks, with various trade-offs and performance sensitivities based on task-specific designs.", "conclusion": "DATE-LM aims to be a foundation for future research on data attribution in LLMs and is accompanied by a public leaderboard for easier comparison of methods.", "key_contributions": ["Introduction of DATE-LM benchmark for LLM data attribution evaluation", "Large-scale evaluation of existing data attribution methods", "Release of a public leaderboard for method comparison"], "limitations": "The performance of data attribution methods is sensitive to the evaluation design, which may limit generalizability.", "keywords": ["data attribution", "language models", "benchmark", "toxicity filtering", "factual attribution"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.09470", "pdf": "https://arxiv.org/pdf/2507.09470.pdf", "abs": "https://arxiv.org/abs/2507.09470", "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "categories": ["cs.CL", "cs.AI", "68T07"], "comment": "29 pages, 5 tables", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.", "AI": {"tldr": "This study optimizes the DRAGON Longformer model for classifying clinical texts, achieving significant performance gains in accuracy and other metrics.", "motivation": "To enhance clinical text classification using an optimized version of the DRAGON Longformer model, improving the accuracy and efficiency of interpreting medical case descriptions.", "method": "The study employed hyperparameter tuning, domain-specific preprocessing, and architectural modifications, including increasing sequence length and adjusting learning rates.", "result": "The optimized model achieved accuracy improvement from 72.0% to 85.2%, precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from 71.0% to 85.2%, with statistically significant results.", "conclusion": "The modifications made to the DRAGON Longformer model significantly enhanced its performance for clinical text classification, with implications for its application in healthcare.", "key_contributions": ["Significant improvement in classification accuracy and other performance metrics.", "Demonstration of the model's ability to handle specialized medical terminology effectively.", "Practical implications for clinical natural language processing applications."], "limitations": "", "keywords": ["clinical text classification", "DRAGON Longformer", "hyperparameter tuning", "medical terminology", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.09482", "pdf": "https://arxiv.org/pdf/2507.09482.pdf", "abs": "https://arxiv.org/abs/2507.09482", "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.", "AI": {"tldr": "Introduction of a multimodal sarcasm generation dataset (M2SaG) and a novel generation framework (ViSP) combining PPO and contrastive learning for improved sarcasm generation.", "motivation": "Sarcasm generation is underexplored, especially due to reliance on textual modalities and lack of visual cues; existing datasets often mismatch image content with sarcastic intent.", "method": "M2SaG dataset with 4,970 samples includes images, sarcastic text, and targets. ViSP framework uses Proximal Policy Optimization and contrastive learning to improve sarcasm text generation.", "result": "ViSP surpasses all baselines, including large language models, producing texts with higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity (0.768 vs. 0.739).", "conclusion": "ViSP demonstrates higher-quality sarcastic content generation compared to existing datasets, indicating the effectiveness of the proposed methods.", "key_contributions": ["Introduction of the M2SaG dataset for multimodal sarcasm generation.", "Development of ViSP framework combining PPO and contrastive learning for generation improvement.", "Demonstration of improved sarcasm generation metrics over existing large language models."], "limitations": "", "keywords": ["sarcasm generation", "multimodal dataset", "Proximal Policy Optimization", "contrastive learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.09474", "pdf": "https://arxiv.org/pdf/2507.09474.pdf", "abs": "https://arxiv.org/abs/2507.09474", "title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "categories": ["cs.CL"], "comment": "12 pages", "summary": "The CoNLL-2013 shared task was devoted to grammatical error correction. In\nthis paper, we give the task definition, present the data sets, and describe\nthe evaluation metric and scorer used in the shared task. We also give an\noverview of the various approaches adopted by the participating teams, and\npresent the evaluation results.", "AI": {"tldr": "This paper discusses the CoNLL-2013 shared task on grammatical error correction, detailing task definitions, datasets, evaluation metrics, and the results of various approaches used by participants.", "motivation": "To provide a comprehensive understanding of the CoNLL-2013 shared task in grammatical error correction, including its methodology and outcomes.", "method": "Overview of task definitions, datasets, evaluation metrics, and analysis of participant approaches and results.", "result": "Summary of the evaluation results from various teams participating in the CoNLL-2013 shared task, showcasing different approaches to grammatical error correction.", "conclusion": "The paper highlights the collaborative nature of the shared task and the diversity of methodologies used to tackle grammatical error correction.", "key_contributions": ["Definition of the grammatical error correction task", "Presentation of datasets and evaluation metrics", "Comprehensive results of various participating approaches"], "limitations": "", "keywords": ["grammatical error correction", "CoNLL-2013", "evaluation metrics"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2507.09477", "pdf": "https://arxiv.org/pdf/2507.09477.pdf", "abs": "https://arxiv.org/abs/2507.09477", "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "submitted to ARR May", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.", "AI": {"tldr": "The paper surveys Retrieval-Augmented Generation (RAG) and its integration with reasoning to enhance multi-step inference in Large Language Models (LLMs).", "motivation": "The need for improving factuality and multi-step inference capabilities in LLMs through integrated retrieval and reasoning methods.", "method": "The survey categorizes advanced reasoning techniques that optimize stages of RAG and discusses how retrieved information can support multi-step inference and context expansion.", "result": "It identifies emerging frameworks that combine searching and reasoning in LLMs, achieving better performance on knowledge-intensive tasks.", "conclusion": "The paper provides insights into methods, datasets, challenges, and future directions for more effective and human-centric RAG-Reasoning systems.", "key_contributions": ["Unified framework for RAG and reasoning", "Identification of Synergized RAG-Reasoning frameworks", "Categorization of methods and datasets for RAG-Reasoning"], "limitations": "The paper primarily focuses on current methodologies without extensive discussion on implementation challenges or practical applications.", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Reasoning", "Human-Centric AI", "Knowledge-Intensive Benchmarks"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2507.09482", "pdf": "https://arxiv.org/pdf/2507.09482.pdf", "abs": "https://arxiv.org/abs/2507.09482", "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.", "AI": {"tldr": "The paper introduces a multimodal sarcasm generation dataset and a framework for generating sarcastic texts that outperforms existing models.", "motivation": "Despite advancements in sarcasm research, generation remains limited by a focus on text and lack of visual context, leading to mismatched datasets.", "method": "The study presents M2SaG, a dataset with 4,970 samples, and proposes the ViSP framework, which combines Proximal Policy Optimization and contrastive learning to enhance sarcasm generation.", "result": "ViSP outperforms existing baselines, including large language models, in generating texts with higher Sarcasm Scores and Factual Incongruity, demonstrating improved quality of sarcastic content.", "conclusion": "The results highlight the effectiveness of integrating visual cues and specific learning strategies for better sarcasm generation.", "key_contributions": ["Introduction of a multimodal sarcasm generation dataset (M2SaG) with images and sarcastic texts.", "Development of the ViSP framework leveraging PPO and contrastive learning for sarcasm generation.", "Demonstration of improved generation quality with quantifiable metrics comparing ViSP to baseline models."], "limitations": "", "keywords": ["sarcasm generation", "multimodal dataset", "Proximal Policy Optimization", "contrastive learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.09485", "pdf": "https://arxiv.org/pdf/2507.09485.pdf", "abs": "https://arxiv.org/abs/2507.09485", "title": "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis", "authors": ["Junjie Liu", "Yuanhe Tian", "Yan Song"], "categories": ["cs.CL"], "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in\nsocial media scenarios to identify the sentiment polarity of specific aspect\nterms in a sentence. Although many existing studies leverage large language\nmodels (LLMs) to perform ABSA due to their strong context understanding\ncapabilities, they still face challenges to learn the context information in\nthe running text because of the short text, as well as the small and unbalanced\nlabeled training data, where most data are labeled with positive sentiment.\nData augmentation (DA) is a feasible strategy for providing richer contextual\ninformation, especially when using LLMs to create synthetic training data, but\nfaces challenges in ensuring a high quality of the augmented data.In this\npaper, we propose an LLM-based ABSA approach with training data\naugmentation.Specifically, an LLM is prompted to generate augmented training\ndata based on the original training data, so as to construct a new training\ndata with larger size and balanced label distributions to better train an ABSA\nmodel. Meanwhile, in order to improve the quality of the augmented data, we\npropose a reinforcement learning approach to optimize the data augmentation.\nLLM.Experiment results and further analyses on English benchmark datasets for\nABSA demonstrate the effectiveness of our approach, where superior performance\nis observed over strong baselines and most existing studies.", "AI": {"tldr": "This paper presents an LLM-based approach for aspect-based sentiment analysis (ABSA) that utilizes data augmentation to improve training data quality and balance sentiment labels.", "motivation": "The challenges in ABSA stem from short text length and imbalanced labeled training data, which often leads to a predominance of positive sentiment labeling.", "method": "An LLM is prompted to generate synthetic training data to enlarge the dataset and balance sentiment labels. Additionally, a reinforcement learning method is used to enhance the quality of this augmented data.", "result": "The proposed approach shows superior performance on English benchmark datasets for ABSA compared to existing strong baseline methods.", "conclusion": "The use of LLM-based data augmentation significantly enhances the performance of ABSA models by providing more comprehensive training data.", "key_contributions": ["LLM-based data augmentation technique for ABSA", "Reinforcement learning to optimize data quality", "Demonstrated superior performance over baseline methods"], "limitations": "", "keywords": ["Aspect-based sentiment analysis", "data augmentation", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.09497", "pdf": "https://arxiv.org/pdf/2507.09497.pdf", "abs": "https://arxiv.org/abs/2507.09497", "title": "GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities", "authors": ["Siyi Wu", "Zeyu Wang", "Xinyuan Song", "Zhengpeng Zhou", "Lifan Sun", "Tianyu Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Modern enterprise environments demand intelligent systems capable of handling\ncomplex, dynamic, and multi-faceted tasks with high levels of autonomy and\nadaptability. However, traditional single-purpose AI systems often lack\nsufficient coordination, memory reuse, and task decomposition capabilities,\nlimiting their scalability in realistic settings. To address these challenges,\nwe present \\textbf{GoalfyMax}, a protocol-driven framework for end-to-end\nmulti-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent\n(A2A) communication layer built on the Model Context Protocol (MCP), allowing\nindependent agents to coordinate through asynchronous, protocol-compliant\ninteractions. It incorporates the Experience Pack (XP) architecture, a layered\nmemory system that preserves both task rationales and execution traces,\nenabling structured knowledge retention and continual learning. Moreover, our\nsystem integrates advanced features including multi-turn contextual dialogue,\nlong-short term memory modules, and dynamic safety validation, supporting\nrobust, real-time strategy adaptation. Empirical results on complex task\norchestration benchmarks and case study demonstrate that GoalfyMax achieves\nsuperior adaptability, coordination, and experience reuse compared to baseline\nframeworks. These findings highlight its potential as a scalable, future-ready\nfoundation for multi-agent intelligent systems.", "AI": {"tldr": "GoalfyMax is a protocol-driven framework designed for multi-agent collaboration, enhancing AI systems' adaptability and coordination through a standardized communication layer and layered memory system.", "motivation": "To tackle the limitations of traditional single-purpose AI systems in complex, dynamic environments by enabling better coordination and memory reuse among agents.", "method": "GoalfyMax uses a standardized Agent-to-Agent communication layer based on the Model Context Protocol, along with a layered Experience Pack architecture for memory management and continual learning.", "result": "Empirical results show that GoalfyMax significantly improves adaptability, coordination, and experience reuse in multi-agent scenarios compared to existing frameworks.", "conclusion": "GoalfyMax presents a scalable framework that can serve as a foundation for future multi-agent intelligent systems, demonstrating its potential through superior performance in various benchmarks.", "key_contributions": ["Introduction of the Model Context Protocol for agent communication", "Development of the Experience Pack architecture for knowledge retention", "Demonstration of significant performance improvements in multi-agent coordination"], "limitations": "", "keywords": ["multi-agent systems", "adaptive intelligence", "protocol-driven frameworks"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.09506", "pdf": "https://arxiv.org/pdf/2507.09506.pdf", "abs": "https://arxiv.org/abs/2507.09506", "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models", "authors": ["Junjie Wu", "Gefei Gu", "Yanan Zheng", "Dit-Yan Yeung", "Arman Cohan"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference. First 2 authors contributed equally", "summary": "Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long.", "AI": {"tldr": "This paper introduces a benchmark called Ref-Long to evaluate long-context referencing capabilities of LCLMs, showing significant performance gaps across advanced models.", "motivation": "To address the underexplored area of long-context referencing in long-context language models (LCLMs).", "method": "The paper proposes the Referencing Evaluation for Long-context Language Models (Ref-Long), a benchmark requiring LCLMs to identify document indexes related to specific keys, with three subsets created for varied contexts.", "result": "Experimental results indicate significant deficiencies in long-context referencing abilities, even in top models like GPT-4o, supported by comprehensive analyses including human evaluations and error analyses.", "conclusion": "The findings highlight the need for improvement in long-context referencing and provide insights from various experimental adjustments.", "key_contributions": ["Introduction of the Ref-Long benchmark for evaluating long-context referencing in LCLMs.", "Identification of significant shortcomings in existing LCLMs regarding long-context referencing tasks.", "Comprehensive analyses providing insights into improving LCLM performance on long-context tasks."], "limitations": "The benchmark may not encompass all contextual nuances and relies on current LCLM capabilities, which are evolving.", "keywords": ["Long-context Language Models", "Referencing Evaluation", "Natural Language Processing", "Benchmark", "Language Model Evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.09509", "pdf": "https://arxiv.org/pdf/2507.09509.pdf", "abs": "https://arxiv.org/abs/2507.09509", "title": "How Important is `Perfect' English for Machine Translation Prompts?", "authors": ["Patrícia Schmidtová", "Niyati Bafna", "Seth Aycock", "Gianluca Vico", "Wiktor Kamzela", "Katharina Hämmerl", "Vilém Zouhar"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans.", "AI": {"tldr": "This paper evaluates how errors in prompts affect the performance of large language models in machine translation and evaluation tasks, showing that prompt quality is crucial for effective translation.", "motivation": "To understand the sensitivity of large language models to errors in user prompts and their impact on machine translation performance.", "method": "Systematic evaluation of the effects of both humanly plausible and synthetic errors in prompts on LLMs' performance, alongside quantitative and qualitative analysis.", "result": "Prompt quality significantly affects translation performance, with character-level and combined noise types leading to worse outcomes than phrasal perturbations.", "conclusion": "Low prompt quality hinders instruction following in LLMs and can severely impact translation performance, even though LLMs cope with high levels of random noise better than humans would.", "key_contributions": ["Quantitative and qualitative insights into LLMs' performance under prompt noise", "Identification of different types of errors affecting translation differently", "Revealing the instruction-following challenges caused by poor prompt quality"], "limitations": "", "keywords": ["large language models", "machine translation", "prompt sensitivity", "error analysis", "instruction following"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.09536", "pdf": "https://arxiv.org/pdf/2507.09536.pdf", "abs": "https://arxiv.org/abs/2507.09536", "title": "Adapting Definition Modeling for New Languages: A Case Study on Belarusian", "authors": ["Daniela Kazakouskaya", "Timothee Mickus", "Janine Siewert"], "categories": ["cs.CL"], "comment": "To appear at SlavicNLP 2025", "summary": "Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture.", "AI": {"tldr": "This paper explores adapting definition modeling to Belarusian by proposing a dataset of 43,150 definitions and assessing the efficiency of existing models.", "motivation": "To assist lexicographers in documenting a wider variety of languages by adapting definition modeling systems to unsupported languages like Belarusian.", "method": "The authors create a dataset of 43,150 definitions for Belarusian and conduct experiments to evaluate the adaptation of existing definition modeling systems with minimal data.", "result": "Experiments show that adapting the system requires minimal data, though gaps in automatic metrics were identified.", "conclusion": "The findings suggest promising outcomes for adapting existing models, but highlight the need for improved metrics to fully assess performance.", "key_contributions": ["Proposed a novel dataset of 43,150 definitions for Belarusian", "Demonstrated effective adaptation of definition modeling systems with minimal data", "Identified limitations in current automatic metrics for evaluating definitions"], "limitations": "There are gaps in what automatic metrics do capture regarding the quality of generated definitions.", "keywords": ["definition modeling", "Belarusian", "language adaptation", "lexicography", "automatic metrics"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.09601", "pdf": "https://arxiv.org/pdf/2507.09601.pdf", "abs": "https://arxiv.org/abs/2507.09601", "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "comment": "Under Review", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.", "AI": {"tldr": "NMIXX introduces specialized cross-lingual embeddings for finance, especially in low-resource languages like Korean, capturing nuanced semantics and outperforming existing models in financial STS tasks.", "motivation": "General-purpose sentence embedding models often fail in specialized domains like finance and low-resource languages due to unique jargon and misaligned vocabularies.", "method": "Development of NMIXX and the KorFinSTS benchmark utilizing 18.8K triplets of paraphrases, hard negatives, and translations; evaluation against seven baselines.", "result": "NMIXX's multilingual bge-m3 variant shows significant performance improvements, achieving +0.10 on English FinSTS and +0.22 on KorFinSTS and highlighting tokenizer design's impact on low-resource languages.", "conclusion": "NMIXX and KorFinSTS provide strong tools for multilingual representation learning in finance, addressing the limitations of current models.", "key_contributions": ["Introduction of NMIXX for cross-lingual finance embeddings", "Creation of the KorFinSTS benchmark tailored to financial nuances", "Showcases the importance of tokenizer design in multilingual settings"], "limitations": "Modest trade-off in general STS performance when optimized for financial semantics.", "keywords": ["cross-lingual embeddings", "finance", "Korean", "sentence similarity", "multilingual representation"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2402.09750", "pdf": "https://arxiv.org/pdf/2402.09750.pdf", "abs": "https://arxiv.org/abs/2402.09750", "title": "Pinning \"Reflection\" on the Agenda: Investigating Reflection in Human-LLM Co-Creation for Creative Coding", "authors": ["Anqi Wang", "Zhizhuo Yin", "Yulu Hu", "Yuanyuan Mao", "Lei Han", "Xin Tong", "Keqin Jiao", "Pan Hui"], "categories": ["cs.HC", "cs.AI", "J.5"], "comment": "6 pages, 2 figures, 2 tables", "summary": "Large language models (LLMs) are increasingly integrated into creative\ncoding, yet how users reflect, and how different co-creation conditions\ninfluence reflective behavior, remains underexplored. This study investigates\nsituated, moment-to-moment reflection in creative coding under two prompting\nstrategies: the entire task invocation (T1) and decomposed subtask invocation\n(T2), to examine their effects on reflective behavior. Our mixed-method results\nreveal three distinct reflection types and show that T2 encourages more\nfrequent, strategic, and generative reflection, fostering diagnostic reasoning\nand goal redefinition. These findings offer insights into how LLM-based tools\nfoster deeper creative engagement through structured, behaviorally grounded\nreflection support.", "AI": {"tldr": "This study explores how different prompting strategies in creative coding can influence users' reflective behavior when using large language models (LLMs).", "motivation": "To understand the impact of prompting strategies on user reflection during creative coding with LLMs.", "method": "A mixed-method approach was used to compare two prompting strategies, whole task invocation and decomposed subtask invocation, focusing on users' moment-to-moment reflection.", "result": "The study identifies three types of reflection and finds that the decomposed subtask approach (T2) leads to more frequent, strategic, and generative reflection compared to the whole task approach (T1).", "conclusion": "LLM-driven tools can enhance creative engagement by supporting structured reflection, improving diagnostic reasoning and goal redefinition among users.", "key_contributions": ["Identification of three distinct types of reflection in creative coding.", "Demonstration that T2 prompting strategy enhances reflective behavior.", "Insights into the role of structured reflection support in creativity with LLMs."], "limitations": "", "keywords": ["large language models", "creative coding", "user reflection", "prompting strategies", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2507.09628", "pdf": "https://arxiv.org/pdf/2507.09628.pdf", "abs": "https://arxiv.org/abs/2507.09628", "title": "SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks", "authors": ["Salvatore Citraro", "Edith Haim", "Alessandra Carini", "Cynthia S. Q. Siew", "Giulio Rossetti", "Massimo Stella"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce SpreadPy as a Python library for simulating spreading activation\nin cognitive single-layer and multiplex networks. Our tool is designed to\nperform numerical simulations testing structure-function relationships in\ncognitive processes. By comparing simulation results with grounded theories in\nknowledge modelling, SpreadPy enables systematic investigations of how\nactivation dynamics reflect cognitive, psychological and clinical phenomena. We\ndemonstrate the library's utility through three case studies: (1) Spreading\nactivation on associative knowledge networks distinguishes students with high\nversus low math anxiety, revealing anxiety-related structural differences in\nconceptual organization; (2) Simulations of a creativity task show that\nactivation trajectories vary with task difficulty, exposing how cognitive load\nmodulates lexical access; (3) In individuals with aphasia, simulated activation\npatterns on lexical networks correlate with empirical error types (semantic vs.\nphonological) during picture-naming tasks, linking network structure to\nclinical impairments. SpreadPy's flexible framework allows researchers to model\nthese processes using empirically derived or theoretical networks, providing\nmechanistic insights into individual differences and cognitive impairments. The\nlibrary is openly available, supporting reproducible research in psychology,\nneuroscience, and education research.", "AI": {"tldr": "SpreadPy is a Python library for simulating spreading activation in cognitive networks, aiding in understanding structure-function relationships in cognitive processes.", "motivation": "The motivation is to systematically investigate how activation dynamics in cognitive processes reflect cognitive, psychological, and clinical phenomena through numerical simulations.", "method": "Utilizes numerical simulations to compare results with grounded theories in knowledge modeling, focusing on cognitive single-layer and multiplex networks.", "result": "The library was demonstrated through case studies showing activation dynamics distinguishing students with math anxiety, variations in creativity task performance with cognitive load, and correlating simulated activation patterns with errors in individuals with aphasia.", "conclusion": "SpreadPy provides a flexible framework for modeling cognitive processes that offers mechanistic insights into cognitive impairments and individual differences, and it supports reproducible research across various domains.", "key_contributions": ["Introduction of SpreadPy for cognitive network simulations", "Case studies linking activation patterns to cognitive and clinical phenomena", "Open-source availability for reproducible research"], "limitations": "", "keywords": ["cognitive networks", "spreading activation", "psychology", "neuroscience", "education"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2403.08969", "pdf": "https://arxiv.org/pdf/2403.08969.pdf", "abs": "https://arxiv.org/abs/2403.08969", "title": "The Full-scale Assembly Simulation Testbed (FAST) Dataset", "authors": ["Alec G. Moore", "Tiffany D. Do", "Nayan N. Chawla", "Antonia Jimenez Iriarte", "Ryan P. McMahan"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "In recent years, numerous researchers have begun investigating how virtual\nreality (VR) tracking and interaction data can be used for a variety of machine\nlearning purposes, including user identification, predicting cybersickness, and\nestimating learning gains. One constraint for this research area is the dearth\nof open datasets. In this paper, we present a new open dataset captured with\nour VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset\nconsists of data collected from 108 participants (50 females, 56 males, 2\nnon-binary) learning how to assemble two distinct full-scale structures in VR.\nIn addition to explaining how the dataset was collected and describing the data\nincluded, we discuss how the dataset may be used by future researchers.", "AI": {"tldr": "This paper introduces a new open dataset for VR-based machine learning research, detailing its collection from 108 participants learning assembly tasks.", "motivation": "The paper addresses the lack of open datasets in VR research for machine learning applications such as user identification and predicting cybersickness.", "method": "Data was collected from 108 participants using a VR-based Full-scale Assembly Simulation Testbed (FAST) while they learned to assemble two structures.", "result": "The dataset includes comprehensive interaction and tracking data, which can be valuable for various machine learning projects in VR.", "conclusion": "Future researchers can utilize this dataset to advance their studies in VR interaction, machine learning, and related areas.", "key_contributions": ["Introduction of a new open dataset for VR research", "Data collected from a diverse group of participants", "Potential applications in user identification and learning analytics"], "limitations": "", "keywords": ["virtual reality", "machine learning", "open dataset", "user interaction", "cybersickness"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.09629", "pdf": "https://arxiv.org/pdf/2507.09629.pdf", "abs": "https://arxiv.org/abs/2507.09629", "title": "An Exploration of Knowledge Editing for Arabic", "authors": ["Basel Mousi", "Nadir Durrani", "Fahim Dalvi"], "categories": ["cs.CL"], "comment": null, "summary": "While Knowledge Editing (KE) has been widely explored in English, its\nbehavior in morphologically rich languages like Arabic remains underexamined.\nIn this work, we present the first study of Arabic KE. We evaluate four methods\n(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact\nbenchmarks, analyzing both multilingual and cross-lingual settings. Our\nexperiments on Llama-2-7B-chat show show that parameter-based methods struggle\nwith cross-lingual generalization, while instruction-tuned methods perform more\nrobustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show\nthat joint Arabic-English training improves both editability and transfer. We\nrelease Arabic KE benchmarks and multilingual training for LTE data to support\nfuture research.", "AI": {"tldr": "First study on Knowledge Editing (KE) in Arabic, evaluating existing methods and proposing improvements for multilingual settings.", "motivation": "To investigate Knowledge Editing in morphologically rich languages like Arabic, which has been overlooked compared to English.", "method": "Evaluated four KE methods (ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact benchmarks, focusing on multilingual and cross-lingual settings with Llama-2-7B-chat.", "result": "Instruction-tuned methods exhibited better performance in cross-lingual generalization than parameter-based methods; joint Arabic-English training enhanced both editability and transfer.", "conclusion": "The study indicates significant differences in KE methods' effectiveness in Arabic compared to English and contributes Arabic KE benchmarks to the research community.", "key_contributions": ["First evaluation of KE methods in Arabic", "Proposed improvements to LTE for multilingual settings", "Release of Arabic KE benchmarks and multilingual training data"], "limitations": "", "keywords": ["Knowledge Editing", "Arabic", "Multilingual", "Cross-lingual", "Machine Learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2406.16173", "pdf": "https://arxiv.org/pdf/2406.16173.pdf", "abs": "https://arxiv.org/abs/2406.16173", "title": "Crepe: A Mobile Screen Data Collector Using Graph Query", "authors": ["Yuwen Lu", "Meng Chen", "Qi Zhao", "Victor Cox", "Yang Yang", "Meng Jiang", "Jay Brockman", "Tamara Kay", "Toby Jia-Jun Li"], "categories": ["cs.HC"], "comment": null, "summary": "Collecting mobile datasets remains challenging for academic researchers due\nto limited data access and technical barriers. Commercial organizations often\npossess exclusive access to mobile data, leading to a \"data monopoly\" that\nrestricts the independence of academic research. Existing open-source mobile\ndata collection frameworks primarily focus on mobile sensing data rather than\nscreen content, which is crucial for various research studies. We present\nCrepe, a no-code Android app that enables researchers to collect information\ndisplayed on screen through simple demonstrations of target data. Crepe\nutilizes a novel Graph Query technique which augments the structures of mobile\nUI screens to support flexible identification, location, and collection of\nspecific data pieces. The tool emphasizes participants' privacy and agency by\nproviding full transparency over collected data and allowing easy opt-out. We\ndesigned and built Crepe for research purposes only and in scenarios where\nresearchers obtain explicit consent from participants. Code for Crepe will be\nopen-sourced to support future academic research data collection.", "AI": {"tldr": "Crepe is a no-code Android app designed for academic researchers to collect mobile screen content data more easily while ensuring participant privacy.", "motivation": "To address the issue of limited access to mobile datasets for academic researchers and the existing data monopolies held by commercial organizations.", "method": "Crepe employs a novel Graph Query technique to enhance mobile UI screen structures, enabling efficient identification and collection of specific data pieces without coding expertise.", "result": "Crepe allows researchers to collect on-screen data through simple demonstrations and emphasizes participant transparency and control over data collection.", "conclusion": "Crepe is designed for research purposes only, requiring explicit participant consent, and will be open-sourced to aid in future academic data collection efforts.", "key_contributions": ["No-code approach to mobile data collection", "Novel Graph Query technique for screen data retrieval", "Emphasis on participant privacy and transparency"], "limitations": "", "keywords": ["mobile data collection", "no-code app", "user privacy", "academic research", "Graph Query"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.09638", "pdf": "https://arxiv.org/pdf/2507.09638.pdf", "abs": "https://arxiv.org/abs/2507.09638", "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?", "authors": ["Pawitsapak Akarajaradwong", "Chompakorn Chaksangchaichot", "Pirat Pothavorn", "Attapol Thamrongrattanarit-Rutherford", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "categories": ["cs.CL"], "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs.", "AI": {"tldr": "This paper presents a method to enhance Thai legal question answering using Retrieval-Augmented Generation (RAG) systems with Group-Relative Policy Optimization (GRPO) to improve law citation accuracy and response quality.", "motivation": "To improve the performance of legal question answering systems on complex legal reasoning tasks, especially in the context of Thai law.", "method": "The authors propose the use of Group-Relative Policy Optimization (GRPO) in conjunction with BGE-M3 embeddings to enhance law citation accuracy and reduce computational expenses.", "result": "Experiments reveal that GRPO achieves up to 90% gains in citation-F1 and a 31% improvement in joint quality metrics over traditional instruction tuning methods.", "conclusion": "The proposed GRPO method offers a resource-efficient solution for boosting the performance of Thai legal LLMs, particularly in handling complex reasoning.", "key_contributions": ["Introduction of GRPO for legal LLMs", "Significant gains in citation accuracy", "Reduction in computational costs compared to traditional methods"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "legal reasoning", "Group-Relative Policy Optimization", "BGE-M3 embeddings", "Thai law"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.09701", "pdf": "https://arxiv.org/pdf/2507.09701.pdf", "abs": "https://arxiv.org/abs/2507.09701", "title": "MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs", "authors": ["Shulin Huang", "Linyi Yang", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding.", "AI": {"tldr": "MCEval is a multilingual evaluation framework for assessing cultural biases and awareness in large language models (LLMs) across 13 cultures and languages.", "motivation": "To address cultural biases and improve cross-cultural understanding in large language models serving diverse global populations.", "method": "MCEval employs dynamic cultural question construction and enables causal analysis through Counterfactual Rephrasing and Confounder Rephrasing. It evaluates 39,897 cultural awareness instances and 17,940 cultural bias instances.", "result": "Experimental results show performance disparities in cultural awareness and bias across linguistic scenarios, indicating that optimal performance relates to training data distribution and language-culture alignment.", "conclusion": "MCEval provides insights into LLMs' cultural understanding and highlights fairness concerns that exacerbate disadvantages in non-English scenarios.", "key_contributions": ["Introduction of MCEval framework for multilingual cultural evaluation.", "Comprehensive assessment of cultural awareness and bias across multiple languages.", "Highlighting fairness issues in LLM performance across different cultures."], "limitations": "The framework's effectiveness may vary based on the cultures and languages included in the evaluation.", "keywords": ["multilingual evaluation", "cultural bias", "large language models", "cultural awareness", "evaluation framework"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.09709", "pdf": "https://arxiv.org/pdf/2507.09709.pdf", "abs": "https://arxiv.org/abs/2507.09709", "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces", "authors": ["Baturay Saglam", "Paul Kassianik", "Blaine Nelson", "Sajana Weerawardhena", "Yaron Singer", "Amin Karbasi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision.", "AI": {"tldr": "Investigates the latent space geometry of large language models (LLMs) to understandsemantic representations and improve alignment using empirical analysis across various topics.", "motivation": "Understanding the internal organization of representations in LLMs is crucial for improving their alignment and interpreting their behavior.", "method": "Conducted a large-scale empirical study of hidden states in 11 transformer-based LLMs across 6 scientific topics and 12 layers, analyzing the geometrical structure of latent representations.", "result": "High-level semantic information is found in low-dimensional subspaces with linearly separable representations; this separability increases in deeper layers, aiding in causal interventions in hidden space.", "conclusion": "The findings support the use of geometry-aware tools to detect and manage harmful content in LLMs, demonstrated by a simple classifier that can identify adversarial prompts with high precision.", "key_contributions": ["Empirical study of latent space geometry in LLMs", "Discovery of high-level semantic information organization in low-dimensional subspaces", "Demonstration of a lightweight classifier for prompt detection based on latent representations."], "limitations": "", "keywords": ["latent space geometry", "large language models", "semantic representation", "alignment", "adversarial content"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.09758", "pdf": "https://arxiv.org/pdf/2507.09758.pdf", "abs": "https://arxiv.org/abs/2507.09758", "title": "Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding", "authors": ["Qi Feng", "Yihong Liu", "Hinrich Schütze"], "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "comment": "18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop\n  (SRW)", "summary": "Curriculum learning is a widely adopted training strategy in natural language\nprocessing (NLP), where models are exposed to examples organized by increasing\ndifficulty to enhance learning efficiency and performance. However, most\nexisting approaches rely on manually defined difficulty metrics -- such as text\nlength -- which may not accurately reflect the model's own perspective. To\novercome this limitation, we present a self-adaptive curriculum learning\nparadigm that prioritizes fine-tuning examples based on difficulty scores\npredicted by pre-trained language models (PLMs) themselves. Building on these\nscores, we explore various training strategies that differ in the ordering of\nexamples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed\nsampling. We evaluate our method on four natural language understanding (NLU)\ndatasets covering both binary and multi-class classification tasks.\nExperimental results show that our approach leads to faster convergence and\nimproved performance compared to standard random sampling.", "AI": {"tldr": "This paper presents a self-adaptive curriculum learning approach that uses pre-trained language models to determine example difficulty for fine-tuning, leading to better performance in natural language understanding tasks.", "motivation": "Existing curriculum learning methods rely on fixed difficulty metrics that may not align with the model's perspective. There is a need for a more dynamic approach to assess example difficulty.", "method": "The authors propose a self-adaptive curriculum learning strategy where difficulty scores are predicted by pre-trained language models. They explore various training methods based on the ordering of examples: easy-to-hard, hard-to-easy, and mixed sampling.", "result": "The proposed method results in faster convergence and enhanced performance across four NLU datasets compared to traditional random sampling techniques.", "conclusion": "Self-adaptive curriculum learning can significantly improve NLP model training by leveraging model-driven difficulty assessments, leading to more effective learning.", "key_contributions": ["Introduction of self-adaptive difficulty assessments using PLMs.", "Comparison of different example ordering strategies in training.", "Demonstrated improvement in NLU task performance and training efficiency."], "limitations": "The method's effectiveness may vary based on the choice of pre-trained language model and specific datasets used for evaluation.", "keywords": ["curriculum learning", "natural language processing", "self-adaptive learning", "pre-trained language models", "training strategies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.09777", "pdf": "https://arxiv.org/pdf/2507.09777.pdf", "abs": "https://arxiv.org/abs/2507.09777", "title": "Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News", "authors": ["Gabriel Mordecki", "Guillermo Moncecchi", "Javier Couto"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.", "AI": {"tldr": "The paper revises the definition of clickbait, introduces a new detection approach, and presents the TA1C dataset for Spanish clickbait detection.", "motivation": "Current definitions of clickbait lack consensus; there is a need for clarity in what distinguishes clickbait from other information presentation techniques.", "method": "A new definition of clickbait is proposed focused on curiosity gaps, and a new dataset, TA1C, is created for clickbait detection with clear annotation criteria to minimize subjectivity.", "result": "The TA1C dataset consists of 3,500 annotated tweets with a Fleiss' K inter annotator agreement of 0.825, achieving strong detection baselines with an F1-score of 0.84.", "conclusion": "This work provides a clearer understanding of clickbait and contributes valuable resources for its detection in Spanish, helping improve the study of clickbait effects.", "key_contributions": ["Proposes a refined definition of clickbait based on curiosity gaps.", "Introduces a new methodology for creating clickbait detection datasets.", "Releases the TA1C dataset, the first of its kind for Spanish clickbait detection."], "limitations": "The focus is on Spanish language data, which may limit applicability to other languages or contexts.", "keywords": ["clickbait", "curiosity gap", "dataset", "detection", "Spanish"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.09875", "pdf": "https://arxiv.org/pdf/2507.09875.pdf", "abs": "https://arxiv.org/abs/2507.09875", "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code: https://github.com/INK-USC/function-induction", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.", "AI": {"tldr": "This paper investigates the mechanisms behind task-level generalization in large language models through the specific case of off-by-one addition, uncovering a function induction mechanism that facilitates this generalization.", "motivation": "To understand how large language models achieve task-level generalization through in-context learning, particularly in unconventional tasks like off-by-one addition.", "method": "Circuit-style interpretability techniques, including path patching, are employed to analyze internal computations within language models during task performance.", "result": "The study reveals a function induction mechanism that explains how models transition from standard addition to off-by-one addition, highlighting the roles of multiple attention heads in the induction process.", "conclusion": "The findings suggest that language models possess reusable and composable structures that enhance their ability to generalize across different tasks beyond straightforward addition.", "key_contributions": ["Uncovered a function induction mechanism that elevates understanding of model generalization.", "Demonstrated the role of multiple attention heads in the induction of a second-step function (+1).", "Showed the reuse of the function induction mechanism across various tasks, indicating broader applicability."], "limitations": "", "keywords": ["large language models", "task-level generalization", "function induction", "attention heads", "in-context learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.13748", "pdf": "https://arxiv.org/pdf/2409.13748.pdf", "abs": "https://arxiv.org/abs/2409.13748", "title": "TheraGen: Therapy for Every Generation", "authors": ["Kartikey Doshi", "Jimit Shah", "Narendra Shekokar"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "This paper contains major errors in methodology and results. It\n  should not be cited", "summary": "We present TheraGen, an advanced AI-powered mental health chatbot utilizing\nthe LLaMA 2 7B model. This approach builds upon recent advancements in language\nmodels and transformer architectures. TheraGen provides all-day personalized,\ncompassionate mental health care by leveraging a large dataset of 1 million\nconversational entries, combining anonymized therapy transcripts, online mental\nhealth discussions, and psychological literature, including APA resources. Our\nimplementation employs transfer learning, fine-tuning, and advanced training\ntechniques to optimize performance. TheraGen offers a user-friendly interface\nfor seamless interaction, providing empathetic responses and evidence-based\ncoping strategies. Evaluation results demonstrate high user satisfaction rates,\nwith 94% of users reporting improved mental well-being. The system achieved a\nBLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response\naccuracy. With an average response time of 1395 milliseconds, TheraGen ensures\nreal-time, efficient support. While not a replacement for professional therapy,\nTheraGen serves as a valuable complementary tool, significantly improving user\nwell-being and addressing the accessibility gap in mental health treatments.\nThis paper details TheraGen's architecture, training methodology, ethical\nconsiderations, and future directions, contributing to the growing field of\nAI-assisted mental healthcare and offering a scalable solution to the pressing\nneed for mental health support.", "AI": {"tldr": "TheraGen is an AI-powered mental health chatbot utilizing the LLaMA 2 7B model, providing personalized care through real-time empathetic interaction and coping strategies.", "motivation": "To address the accessibility gap in mental health treatments and provide an AI solution for improved user well-being.", "method": "TheraGen employs transfer learning and advanced training techniques, utilizing a large dataset of conversational entries to optimize its performance for mental health interactions.", "result": "Users reported a 94% satisfaction rate and improvement in mental well-being, with TheraGen achieving a BLEU score of 0.67 and ROUGE score of 0.62, indicating strong response accuracy.", "conclusion": "TheraGen serves as a complementary tool in mental health care, although it is not a replacement for professional therapy, improving accessibility to mental health support.", "key_contributions": ["Utilization of the LLaMA 2 7B model for mental health applications", "High user satisfaction and well-being improvement rates", "Scalable solution to mental health support using AI"], "limitations": "Major errors in methodology and results; should not be cited.", "keywords": ["AI", "Mental Health", "Chatbot", "Language Models", "Therapy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.09935", "pdf": "https://arxiv.org/pdf/2507.09935.pdf", "abs": "https://arxiv.org/abs/2507.09935", "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking", "authors": ["Hai Toan Nguyen", "Tien Dat Nguyen", "Viet Ha Nguyen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.", "AI": {"tldr": "This paper presents a novel framework for Retrieval-Augmented Generation (RAG) that enhances chunking strategies by integrating hierarchical text segmentation and clustering, resulting in more semantically coherent and contextually relevant information retrieval.", "motivation": "Traditional chunking strategies for RAG often fail to create meaningful chunks that capture semantic meaning, impacting the effectiveness of external knowledge integration in LLMs.", "method": "The proposed framework integrates hierarchical text segmentation and clustering methods to generate semantically coherent chunks. It retrieves information using segment-level and cluster-level vector representations during inference.", "result": "Evaluations on the NarrativeQA, QuALITY, and QASPER datasets show that this method outperforms traditional chunking techniques, leading to improved retrieval accuracy and context relevance.", "conclusion": "The novel framework provides a significant improvement in RAG systems by enhancing the quality of information retrieval through better chunking strategies.", "key_contributions": ["Integration of hierarchical text segmentation and clustering in RAG systems", "Improved semantically coherent chunks for better information retrieval", "Demonstrated effectiveness on multiple benchmark datasets"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Natural Language Processing", "Chunking Strategies"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.09973", "pdf": "https://arxiv.org/pdf/2507.09973.pdf", "abs": "https://arxiv.org/abs/2507.09973", "title": "Tiny Reward Models", "authors": ["Sarah Pan"], "categories": ["cs.CL", "cs.AI"], "comment": "2025 ICML Efficient Systems for Foundation Models Workshop", "summary": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling.", "AI": {"tldr": "TinyRM introduces a family of small, bidirectional masked language models that perform comparably to much larger models in reasoning and safety preference tasks while significantly reducing inference costs.", "motivation": "The need for efficient reward modeling in reinforcement learning from human feedback, particularly due to increasing inference costs as model sizes grow.", "method": "TinyRM employs a combination of FLAN-style prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing techniques to enhance performance while utilizing fewer resources.", "result": "TinyRM models, with as few as 400 million parameters, achieve competitive results on RewardBench compared to models over 175 times their size, especially excelling when fine-tuned with domain-specific strategies.", "conclusion": "Despite challenges in developing generalist and conversational models, the promising results of TinyRM suggest that smaller, bidirectional architectures can serve as efficient and scalable alternatives for preference modeling tasks.", "key_contributions": ["Introduction of TinyRM, a family of small MLMs for preference modeling", "Demonstration of effectiveness with significantly fewer parameters", "Exploration of lightweight fine-tuning strategies for small models"], "limitations": "Challenges remain in generalist model performance and conversational preference modeling.", "keywords": ["reward modeling", "reinforcement learning", "language models", "finite-tuning", "bidirectional architectures"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.09982", "pdf": "https://arxiv.org/pdf/2507.09982.pdf", "abs": "https://arxiv.org/abs/2507.09982", "title": "TextOmics-Guided Diffusion for Hit-like Molecular Generation", "authors": ["Hang Yuan", "Chen Li", "Wenjun Ma", "Yuncheng Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi.", "AI": {"tldr": "TextOmics provides a dataset for integrating omics and molecular descriptions, and ToDi is a framework for generating hit-like molecules.", "motivation": "There is a need for a unified framework and data to enhance target-specific drug discovery through molecular generation.", "method": "TextOmics establishes correspondences between omics expressions and molecular descriptions, while ToDi uses encoders and conditional diffusion for molecule generation.", "result": "ToDi outperforms state-of-the-art methods in generating biologically relevant hit-like molecules and can perform zero-shot generation.", "conclusion": "The framework shows promise in drug discovery by generating targets with high therapeutic potential.", "key_contributions": ["Introduction of TextOmics benchmark for molecular generation", "Development of ToDi framework combining omics and textual descriptions", "Showcasing effectiveness in zero-shot generation of therapeutic molecules"], "limitations": "", "keywords": ["TextOmics", "drug discovery", "molecular generation", "generative framework", "therapeutic potential"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.10008", "pdf": "https://arxiv.org/pdf/2507.10008.pdf", "abs": "https://arxiv.org/abs/2507.10008", "title": "Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media", "authors": ["Jun Li", "Xiangmeng Wang", "Haoyang Li", "Yifei Yan", "Hong Va Leong", "Ling Feng", "Nancy Xiaonan Yu", "Qing Li"], "categories": ["cs.CL"], "comment": null, "summary": "Suicide is a critical global health issue that requires urgent attention.\nEven though prior work has revealed valuable insights into detecting current\nsuicide risk on social media, little attention has been paid to developing\nmodels that can predict subsequent suicide risk over time, limiting their\nability to capture rapid fluctuations in individuals' mental state transitions.\nIn addition, existing work ignores protective factors that play a crucial role\nin suicide risk prediction, focusing predominantly on risk factors alone.\nProtective factors such as social support and coping strategies can mitigate\nsuicide risk by moderating the impact of risk factors. Therefore, this study\nproposes a novel framework for predicting subsequent suicide risk by jointly\nlearning the dynamic influence of both risk factors and protective factors on\nusers' suicide risk transitions. We propose a novel Protective Factor-Aware\nDataset, which is built from 12 years of Reddit posts along with comprehensive\nannotations of suicide risk and both risk and protective factors. We also\nintroduce a Dynamic Factors Influence Learning approach that captures the\nvarying impact of risk and protective factors on suicide risk transitions,\nrecognizing that suicide risk fluctuates over time according to established\npsychological theories. Our thorough experiments demonstrate that the proposed\nmodel significantly outperforms state-of-the-art models and large language\nmodels across three datasets. In addition, the proposed Dynamic Factors\nInfluence Learning provides interpretable weights, helping clinicians better\nunderstand suicidal patterns and enabling more targeted intervention\nstrategies.", "AI": {"tldr": "A novel framework for predicting suicide risk over time by considering both risk and protective factors using a new dataset from Reddit.", "motivation": "To address the gap in suicide risk prediction models that overlook the dynamic nature of mental state transitions and the importance of protective factors.", "method": "This study proposes a Protective Factor-Aware Dataset derived from Reddit posts, and a Dynamic Factors Influence Learning approach to model the interplay between risk and protective factors in predicting suicide risk.", "result": "The model significantly outperforms existing state-of-the-art models and large language models across three datasets, providing interpretable weights for better understanding of suicidal patterns.", "conclusion": "Incorporating protective factors into suicide risk assessments enhances prediction accuracy and may lead to more effective interventions.", "key_contributions": ["Introduced a Protective Factor-Aware Dataset from 12 years of Reddit posts.", "Developed a Dynamic Factors Influence Learning approach for modeling suicide risk transitions.", "Provided interpretable weights for clinicians to understand suicidal patterns."], "limitations": "", "keywords": ["suicide risk prediction", "protective factors", "dynamic factors", "Reddit data", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.10059", "pdf": "https://arxiv.org/pdf/2507.10059.pdf", "abs": "https://arxiv.org/abs/2507.10059", "title": "GeLaCo: An Evolutionary Approach to Layer Compression", "authors": ["David Ponce", "Thierry Etchegoyhen", "Javier Del Ser"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.", "AI": {"tldr": "Introducing GeLaCo, an evolutionary method for compressing LLMs through layer collapse that improves efficiency and maintains quality.", "motivation": "To address deployment and usage barriers of Large Language Models (LLMs) due to high computational requirements, by exploring model compression techniques.", "method": "An evolutionary approach using population-based search and a module-wise similarity fitness function to explore the compression solution space.", "result": "GeLaCo solutions outperform state-of-the-art alternatives in terms of perplexity-based and generative evaluations on foundational and instruction-tuned models.", "conclusion": "GeLaCo establishes a Pareto frontier for compression and quality, facilitating efficient LLM compression without the need for extensive empirical search.", "key_contributions": ["Introduction of GeLaCo for LLM compression", "Establishment of the Pareto frontier along compression and quality axes", "Utilization of population-based search for exploring solution space"], "limitations": "", "keywords": ["LLM", "model compression", "evolutionary algorithms", "Pareto frontier", "layer collapse"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.10073", "pdf": "https://arxiv.org/pdf/2507.10073.pdf", "abs": "https://arxiv.org/abs/2507.10073", "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "authors": ["Simon Münker"], "categories": ["cs.CL", "cs.AI"], "comment": "15pages, 1 figure, 2 tables", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.", "AI": {"tldr": "This study reveals that Large Language Models (LLMs) fail to represent diverse moral frameworks across cultures and instead homogenize moral diversity, challenging their use in social science research.", "motivation": "To evaluate whether AI systems truly represent human values or merely average them, and to address the limitations in current AI alignment approaches.", "method": "The study utilized the Moral Foundations Questionnaire across 19 cultural contexts to compare the moral intuitions represented by LLMs against human baseline data.", "result": "Results indicate significant gaps between AI-generated moral representations and human moral intuitions, highlighting that larger model sizes do not necessarily increase fidelity in cultural representation.", "conclusion": "The findings suggest a critical need for more grounded alignment objectives and evaluation metrics to ensure AI accurately reflects diverse human values, rather than oversimplifying them.", "key_contributions": ["Identification of moral representation gaps in LLMs", "Demonstration of the homogenizing effect of LLMs on moral diversity", "Call for improved alignment metrics in AI systems"], "limitations": "Current alignment approaches fail to capture nuanced, culturally-specific moral intuitions, limiting AI's applicability in diverse contexts.", "keywords": ["Large Language Models", "moral diversity", "AI alignment", "cultural values", "Moral Foundations Questionnaire"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.10085", "pdf": "https://arxiv.org/pdf/2507.10085.pdf", "abs": "https://arxiv.org/abs/2507.10085", "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning", "authors": ["Chenxi Huang", "Shaotian Yan", "Liang Xie", "Binbin Lin", "Sinan Fan", "Yue Xin", "Deng Cai", "Chen Shen", "Jieping Ye"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods.", "AI": {"tldr": "This paper proposes Critical Representation Fine-Tuning (CRFT) to enhance reasoning performance by optimizing critical representations in complex reasoning tasks instead of fixed representations.", "motivation": "To improve performance in complex reasoning tasks while addressing the limitations of the native Representation Fine-tuning (ReFT) method, which modifies fixed representations.", "method": "CRFT identifies and fine-tunes critical representations that integrate information and regulate outputs, operating under a supervised learning framework and optimizing in a low-rank subspace while freezing the base model.", "result": "CRFT significantly enhances reasoning performance, validated across eight benchmarks for arithmetic and commonsense reasoning, and improves one-shot accuracy by 16.4%.", "conclusion": "CRFT offers a lightweight and effective alternative to traditional PEFT methods, emphasizing the importance of representation-level optimization in reasoning tasks.", "key_contributions": ["Introduces Critical Representation Fine-Tuning method.", "Demonstrates significant improvements on reasoning benchmarks.", "Highlights the importance of representation-level optimization."], "limitations": "", "keywords": ["Representation Fine-tuning", "Critical representations", "Reasoning tasks"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2507.10098", "pdf": "https://arxiv.org/pdf/2507.10098.pdf", "abs": "https://arxiv.org/abs/2507.10098", "title": "Fusing Large Language Models with Temporal Transformers for Time Series Forecasting", "authors": ["Chen Su", "Yuanhe Tian", "Qinyu Liu", "Jun Zhang", "Yan Song"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.", "AI": {"tldr": "This paper presents a novel Transformer-based architecture that combines the strengths of large language models (LLMs) and vanilla Transformers for time series forecasting (TSF).", "motivation": "To improve time series forecasting by addressing the limitations of LLMs in modeling continuous numerical data while leveraging their high-level semantic understanding.", "method": "The proposed architecture integrates representations from LLMs and vanilla Transformers to create a hybrid representation, which captures both historical temporal dynamics and semantic variations.", "result": "Experiments on benchmark datasets show the effectiveness of the model in providing more accurate predictions compared to existing methods.", "conclusion": "The proposed model successfully fuses LLM capabilities with traditional Transformer strengths, leading to improvements in time series forecasting accuracy.", "key_contributions": ["Development of a hybrid Transformer architecture that integrates LLMs with vanilla Transformers for TSF.", "Demonstration of improved forecasting accuracy on benchmark datasets.", "Analysis of the performance gaps between LLMs and traditional methods in TSF."], "limitations": "", "keywords": ["large language models", "time series forecasting", "Transformer architecture", "hybrid representation", "semantic patterns"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.10155", "pdf": "https://arxiv.org/pdf/2507.10155.pdf", "abs": "https://arxiv.org/abs/2507.10155", "title": "Task-Based Flexible Feature Distillation for LLMs", "authors": ["Khouloud Saadi", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.", "AI": {"tldr": "Proposes a novel task-based feature distillation method for large language models (LLMs) that enables knowledge transfer without new parameters.", "motivation": "To address the limitations of traditional feature distillation methods that require teachers and students to share the same hidden sizes, which hinders flexibility.", "method": "The method identifies the most task-relevant hidden units in the teacher model and distills their activations directly to the student model, allowing for different hidden layer dimensions without adding new parameters.", "result": "Demonstrated consistent improvements in various tasks, achieving up to a 3% performance gain over linear projection baseline methods in classification, instruction-following, and summarization tasks.", "conclusion": "The proposed method offers a flexible solution for knowledge transfer in LLMs and improves performance on downstream tasks.", "key_contributions": ["Novel task-based feature distillation without additional parameters", "Identifies task-relevant hidden units for knowledge transfer", "Improves performance across multiple downstream tasks"], "limitations": "", "keywords": ["Knowledge Distillation", "Large Language Models", "Task-based Distillation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.10177", "pdf": "https://arxiv.org/pdf/2507.10177.pdf", "abs": "https://arxiv.org/abs/2507.10177", "title": "Abusive text transformation using LLMs", "authors": ["Rohitash Chandra", "Jiyong Choi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.", "AI": {"tldr": "This study explores the use of Large Language Models (LLMs) to transform abusive text into non-abusive equivalents while retaining their semantic meaning.", "motivation": "To investigate the effectiveness of LLMs in classifying and transforming abusive text into non-abusive versions and to maintain the essential intent of the original text.", "method": "The study evaluates several state-of-the-art LLMs, including Gemini, GPT-4o, DeepSeek, and Groq, assessing their ability to identify and transform various forms of abusive text such as tweets and reviews.", "result": "Groq showed significantly different results compared to the other LLMs, while similarities were found between the performances of GPT-4o and DeepSeek-V3 in transforming abusive content.", "conclusion": "The findings highlight the potential of LLMs in mitigating abusive language while preserving message intent, with varying efficacy among different models.", "key_contributions": ["Exploration of LLMs for abusive text transformation", "Performance comparison among multiple state-of-the-art LLMs", "Insight into sentiment preservation in transformed text"], "limitations": "", "keywords": ["Large Language Models", "abusive text transformation", "sentiment analysis", "semantic analysis", "hate speech"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.10216", "pdf": "https://arxiv.org/pdf/2507.10216.pdf", "abs": "https://arxiv.org/abs/2507.10216", "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects", "authors": ["Renad Al-Monef", "Hassan Alhuzali", "Nora Alturayeif", "Ashwag Alasmari"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications.", "AI": {"tldr": "This paper introduces the Absher benchmark for evaluating LLMs' understanding of Arabic dialects and cultural nuances, with a focus on Saudi Arabia.", "motivation": "To evaluate the understanding of large language models in linguistically diverse settings and address performance gaps in Arabic NLP applications.", "method": "The paper presents a benchmark comprising over 18,000 multiple-choice questions related to Saudi dialects, categorized into areas such as Meaning, True/False, Fill-in-the-Blank, and Cultural Interpretation, derived from dialectal sources.", "result": "The evaluation of multiple LLMs reveals performance gaps in cultural inference and contextual understanding, suggesting inadequacies in current models.", "conclusion": "The findings emphasize the necessity for dialect-aware training and evaluation methodologies to enhance LLM performance in real-world Arabic contexts.", "key_contributions": ["Introduction of the Absher benchmark for assessing LLM performance in Arabic dialects.", "Identification of performance gaps in LLMs related to cultural inference and contextual tasks.", "Emphasis on the need for culturally aligned evaluation methodologies."], "limitations": "", "keywords": ["Arabic NLP", "large language models", "Saudi dialects", "cultural interpretation", "benchmarking"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.10326", "pdf": "https://arxiv.org/pdf/2507.10326.pdf", "abs": "https://arxiv.org/abs/2507.10326", "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation", "authors": ["Muzhaffar Hazman", "Minh-Khoi Pham", "Shweta Soundararajan", "Goncalo Mordido", "Leonardo Custode", "David Lynch", "Giorgio Cruciata", "Yucheng Shi", "Hongmeng Song", "Wang Chao", "Pan Yue", "Aleksandar Milenovic", "Alexandros Agapitos"], "categories": ["cs.CL"], "comment": "Accepted for Publication at ECAI 2025", "summary": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.", "AI": {"tldr": "The paper presents an evolutionary search approach for automated prompt optimization in large language models (LLMs), demonstrating superior performance over existing methods.", "motivation": "Improving prompt engineering for complex tasks with smaller models that are sensitive to prompt design uses a novel evolutionary search technique.", "method": "The method includes a two-phase approach: (1) grammar-guided genetic programming to create prompt-creating programs, and (2) local search to fine-tune the performance of these programs.", "result": "The proposed approach outperforms three state-of-the-art methods (PromptWizard, OPRO, RL-Prompt) on small general-purpose LLMs across four challenging tasks.", "conclusion": "The evolutionary search approach significantly enhances performance in various task-model combinations with minimal degradation in other scenarios.", "key_contributions": ["Introduces a two-phase evolutionary search for prompt optimization", "Demonstrates effectiveness on smaller LLMs", "Outperforms existing prompt optimization methods in complex tasks."], "limitations": "", "keywords": ["prompt engineering", "large language models", "automated optimization", "genetic programming", "HCI"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.10330", "pdf": "https://arxiv.org/pdf/2507.10330.pdf", "abs": "https://arxiv.org/abs/2507.10330", "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach", "authors": ["Mohammed Bouri", "Adnane Saoud"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL Findings 2025", "summary": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM", "AI": {"tldr": "This paper introduces a novel regularization technique, Growth Bound Matrices (GBM), to enhance the robustness of NLP models against adversarial attacks, specifically focusing on recurrent networks and state space models.", "motivation": "The motivation of this paper is to improve the robustness of NLP models, particularly recurrent networks and state space models, against adversarial attacks, which have been inadequately addressed in prior research.", "method": "The authors propose a regularization technique called Growth Bound Matrices (GBM) that reduces the impact of input perturbations on model outputs. The technique is evaluated on Long Short-Term Memory (LSTM), State Space models (S4), and Convolutional Neural Networks (CNN).", "result": "The method significantly enhances adversarial robustness, achieving up to 8.8% improvement over existing baselines across various architectures and benchmark datasets.", "conclusion": "The results indicate the effectiveness of the GBM approach and its superiority compared to several state-of-the-art methods in defending against adversarial attacks in NLP models.", "key_contributions": ["Introduction of Growth Bound Matrices (GBM) as a regularization technique.", "Systematic analysis of the robustness of State Space models (S4).", "Demonstration of improved adversarial robustness across multiple architectures."], "limitations": "", "keywords": ["Natural Language Processing", "Adversarial Attacks", "Robustness", "Growth Bound Matrices", "State Space Models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.10342", "pdf": "https://arxiv.org/pdf/2507.10342.pdf", "abs": "https://arxiv.org/abs/2507.10342", "title": "Using AI to replicate human experimental results: a motion study", "authors": ["Rosa Illan Castillo", "Javier Valenzuela"], "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.", "AI": {"tldr": "The paper investigates the efficacy of large language models (LLMs) in linguistic research, particularly their ability to analyze affective meanings in language and replicate human judgments.", "motivation": "To explore the reliability of LLMs in producing nuanced linguistic analyses alongside human responses in psycholinguistic studies.", "method": "Four psycholinguistic studies were conducted comparing human judgments and LLM outputs on tasks related to affective meanings, valence shifts, verb choice, and sentence-emoji associations.", "result": "Results indicated a strong correspondence between human and AI responses, with high correlation in both qualitative and quantitative measures, suggesting LLMs can effectively augment traditional linguistic research.", "conclusion": "The findings advocate for the integration of LLMs in linguistic studies, confirming their role as valid partners in research without undermining interpretative validity.", "key_contributions": ["Demonstrated strong correlation between human and LLM responses in psycholinguistic tasks.", "Validated the use of LLMs as tools for hypothesis generation in linguistic research.", "Provided evidence that LLMs can facilitate larger scale linguistic studies."], "limitations": "Minor divergences were observed, but they did not significantly impact overall interpretative outcomes.", "keywords": ["large language models", "psycholinguistics", "affective meaning", "valence shifts", "LLM collaboration"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.10354", "pdf": "https://arxiv.org/pdf/2507.10354.pdf", "abs": "https://arxiv.org/abs/2507.10354", "title": "Meanings are like Onions: a Layered Approach to Metaphor Processing", "authors": ["Silvia Cappa", "Anna Sofia Lippolis", "Stefano Zoia"], "categories": ["cs.CL"], "comment": null, "summary": "Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning.", "AI": {"tldr": "The paper proposes a three-dimensional model of metaphor processing, comparing meaning to an onion, which integrates content analysis, conceptual blending, and pragmatic intentionality for deeper metaphor interpretation in computational systems.", "motivation": "To enhance metaphor interpretation in computational systems by providing a more complex understanding of metaphorical meaning that goes beyond simple surface associations.", "method": "A stratified model of metaphor processing comprising three levels: content analysis for basic conceptual elements; conceptual blending for emergent meanings; and a pragmatic vocabulary for capturing speaker intent and contextual effects.", "result": "The proposed model allows for richer metaphor interpretation in computational systems by integrating multiple cognitive levels, providing the groundwork for context-sensitive reasoning.", "conclusion": "The model offers a formal framework that unifies different layers of metaphor processing, facilitating more advanced computational representations of metaphorical meaning.", "key_contributions": ["Introduction of a multi-layered metaphor processing model", "Integration of pragmatic theories with metaphor understanding", "Development of methods for deeper, context-sensitive reasoning in computational interpretations."], "limitations": "", "keywords": ["metaphor processing", "cognitive science", "computational linguistics"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.10435", "pdf": "https://arxiv.org/pdf/2507.10435.pdf", "abs": "https://arxiv.org/abs/2507.10435", "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers", "authors": ["Xinnan Dai", "Kai Yang", "Jay Revolinsky", "Kai Guo", "Aoran Wang", "Bohang Zhang", "Jiliang Tang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data.", "AI": {"tldr": "This paper investigates how large language models (LLMs), specifically decoder-only Transformers, can understand and extract graph structures from textual data, introducing the Induced Substructure Filtration (ISF) method to analyze internal mechanisms and performance on complex graph tasks.", "motivation": "To explore the capability of decoder-only Transformers in understanding and solving graph reasoning tasks through textual descriptions and to analyze how these models extract substructures from graphs.", "method": "The paper introduces the concept of Induced Substructure Filtration (ISF) to study substructure extraction in multi-layer transformers, supported by empirical results and theoretical analysis, and tests this method on LLMs to evaluate their performance with attributed graphs.", "result": "The study reveals consistent internal dynamics across transformer layers during the substructure extraction task and demonstrates that decoder-only Transformers can efficiently extract substructures from complex composite patterns, such as molecular graphs.", "conclusion": "The findings provide new insights into the ability of sequence-based Transformers to handle graph data, particularly through effective substructure extraction methods that enhance their application in reasoning tasks.", "key_contributions": ["Induced Substructure Filtration (ISF) method for analyzing Transformers", "Empirical and theoretical insights into graph reasoning capabilities of LLMs", "Demonstration of successful substructure extraction from attributed graphs"], "limitations": "", "keywords": ["large language models", "graph reasoning", "Induced Substructure Filtration", "Transformer architecture", "substructure extraction"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2507.10445", "pdf": "https://arxiv.org/pdf/2507.10445.pdf", "abs": "https://arxiv.org/abs/2507.10445", "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour", "authors": ["Chris Madge", "Matthew Purver", "Massimo Poesio"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy.", "AI": {"tldr": "This study investigates the ability of LLMs to ask clarification questions in task-oriented dialogues, especially focusing on ambiguity. It presents a new corpus for experimentation and analyzes the differences between human and LLM responses.", "motivation": "The research is motivated by the need to understand how LLMs handle ambiguity in dialogues and their capacity to generate clarification questions compared to humans.", "method": "The authors create a new corpus by merging two existing annotations of the Minecraft Dialogue Corpus, which includes references to ambiguity and clarifications. They compare the instances of clarification questions from LLMs and human responses in ambiguous situations.", "result": "The findings reveal a weak link between ambiguity and human-generated clarification questions, with low correlation between human and LLM question asking. LLMs tend to produce more questions for referential ambiguity but less for task uncertainty compared to humans.", "conclusion": "The study concludes that LLMs' ability to ask clarification questions might depend on their reasoning capabilities, as different reasoning approaches impact the frequency and relevance of these questions.", "key_contributions": ["Creation of a new annotated corpus for studying clarification questions in dialogues.", "Comparison of human and LLM clarification question generation in ambiguous situations.", "Investigation of the correlation between reasoning approaches and LLM question asking frequency."], "limitations": "The study primarily focuses on one corpus, which may limit generalizability to other domains or types of dialogues.", "keywords": ["LLMs", "clarification questions", "task-oriented dialogues", "ambiguity", "reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.10468", "pdf": "https://arxiv.org/pdf/2507.10468.pdf", "abs": "https://arxiv.org/abs/2507.10468", "title": "From BERT to Qwen: Hate Detection across architectures", "authors": ["Ariadna Mon", "Saúl Fenollosa", "Jon Lecumberri"], "categories": ["cs.CL", "cs.LG"], "comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)", "summary": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).", "AI": {"tldr": "The paper benchmarks classic encoders and next-generation LLMs for hate-speech detection in online interactions.", "motivation": "To evaluate the effectiveness of ultra-large autoregressive LLMs in hate-speech detection compared to earlier bidirectional transformer encoders without over-censoring legitimate discourse.", "method": "The study involves benchmarking classic encoder models against ultra-large autoregressive LLMs on curated corpora of online interactions related to hate speech.", "result": "The study provides insights into whether the increased scale of LLMs improves hate-speech detection in practical applications.", "conclusion": "There is a need for further verification of model effectiveness in real-world scenarios to balance hate-speech detection and the preservation of legitimate discourse.", "key_contributions": ["Benchmarking hate-speech detection performance of classic encoders vs. LLMs", "Curated corpora for real-world text analysis", "Insights on model effectiveness in hate-speech detection"], "limitations": "The practical implications of findings in diverse contexts remain to be explored.", "keywords": ["hate speech detection", "large language models", "bidirectional transformers", "online discourse", "NLP"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.10472", "pdf": "https://arxiv.org/pdf/2507.10472.pdf", "abs": "https://arxiv.org/abs/2507.10472", "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking", "authors": ["Mohamed T. Younes", "Omar Walid", "Mai Hassan", "Ali Hamdi"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.", "AI": {"tldr": "The paper presents MLAR, an innovative ATS using LLMs to improve recruitment processes by automating resume screening and candidate shortlisting.", "motivation": "Traditional recruitment faces bottlenecks in resume screening and candidate shortlisting due to time and resource constraints.", "method": "MLAR employs Large Language Models in three layers: extracting characteristics from job postings, parsing resumes for key information, and performing similarity matching.", "result": "MLAR outperformed leading RPA platforms in processing resumes, achieving an average time of 5.4 seconds per resume, with significant reductions in processing time compared to competitors.", "conclusion": "MLAR can transform recruitment workflows, providing an efficient, accurate, and scalable solution for modern hiring needs.", "key_contributions": ["Introduction of a novel Robotic process automation framework MLAR for ATS.", "Three-layered approach using LLMs for resume and job matching.", "Performance benchmarking demonstrating superior efficiency over leading RPA platforms."], "limitations": "", "keywords": ["Applicant Tracking System", "Robotic Process Automation", "Machine Learning", "Resume Screening", "Large Language Models"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.10475", "pdf": "https://arxiv.org/pdf/2507.10475.pdf", "abs": "https://arxiv.org/abs/2507.10475", "title": "Can You Detect the Difference?", "authors": ["İsmail Tarım", "Aytuğ Onan"], "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3"], "comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.", "AI": {"tldr": "The paper analyzes the ability of stylometric metrics to detect AI-generated text from diffusion models compared to autoregressive models, finding that diffusion-generated text closely resembles human text, necessitating new detection methods.", "motivation": "With the rise of large language models, particularly diffusion models, there is a growing concern about detecting AI-generated text reliably. Previous methods for detection may struggle with this new generation of models.", "method": "The study compares diffusion-generated text using the LLaDA model against autoregressive-generated text from the LLaMA model, analyzing 2,000 samples using various metrics: perplexity, burstiness, lexical diversity, readability, and BLEU/ROUGE scores.", "result": "The results indicate that diffusion-generated text mimics human writing closely, leading to high false-negative rates in existing autoregressive detectors. LLaMA demonstrated lower perplexity but reduced lexical fidelity compared to human text.", "conclusion": "The paper concludes that a single detection metric is insufficient for distinguishing diffusion outputs from human writing and calls for the development of diffusion-aware detection methods such as hybrid models and improved watermarking.", "key_contributions": ["First systematic comparison of diffusion and autoregressive models in text generation detection", "Identification of key metrics where diffusion models mimic human text", "Recommendations for developing better detection methods for diffusion-generated text"], "limitations": "The paper focuses on a limited number of metrics and does not explore all possible detection approaches or fully characterize the detection landscape.", "keywords": ["large language models", "AI text detection", "diffusion models", "stylometric metrics", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.10524", "pdf": "https://arxiv.org/pdf/2507.10524.pdf", "abs": "https://arxiv.org/abs/2507.10524", "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation", "authors": ["Sangmin Bae", "Yujin Kim", "Reza Bayat", "Sungnyun Kim", "Jiyoun Ha", "Tal Schuster", "Adam Fisch", "Hrayr Harutyunyan", "Ziwei Ji", "Aaron Courville", "Se-Young Yun"], "categories": ["cs.CL", "cs.LG"], "comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions", "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.", "AI": {"tldr": "Introducing Mixture-of-Recursions (MoR), a unified framework that enhances parameter and computational efficiency in language models through shared layers and adaptive recursion depths.", "motivation": "To address the significant computational and memory demands of scaling language models while striving for both parameter sharing and adaptive computation efficiency.", "method": "MoR combines shared layer stacks and lightweight routers for adaptive token-level recursion, optimizing attention computation and memory access through selective caching of key-value pairs.", "result": "The MoR framework significantly reduces validation perplexity and improves few-shot accuracy across various model sizes while achieving higher throughput compared to existing methods.", "conclusion": "MoR provides a viable solution for achieving large-model quality with reduced resource costs, paving the way for more efficient language model deployment.", "key_contributions": ["Unified framework for parameter sharing and adaptive computation", "Selective caching of key-value pairs for improved memory efficiency", "Demonstrates significant performance improvements in smaller model sizes"], "limitations": "", "keywords": ["language models", "parameter efficiency", "adaptive computation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.10535", "pdf": "https://arxiv.org/pdf/2507.10535.pdf", "abs": "https://arxiv.org/abs/2507.10535", "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks", "authors": ["Hongchao Jiang", "Yiming Chen", "Yushi Cao", "Hung-yi Lee", "Robby T. Tan"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench", "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.", "AI": {"tldr": "The paper introduces CodeJudgeBench, a benchmark for evaluating Large Language Models (LLMs) as judges in coding tasks, revealing performance discrepancies and optimal prompting strategies.", "motivation": "There is a lack of effective benchmarks for evaluating LLMs in coding scenarios despite their potential to assess response quality among models.", "method": "The authors benchmarked 26 LLM-as-a-Judge models across three coding tasks: code generation, code repair, and unit test generation.", "result": "The study shows that recent thinking models outperform non-thinking ones and that even smaller models can outperform larger, specially trained models, while highlighting issues of randomness and variance in judgments.", "conclusion": "Optimal prompting strategies, such as pair-wise comparisons and retaining full LLM responses, can improve the reliability and performance of LLM-as-a-Judge in coding tasks.", "key_contributions": ["Introduction of CodeJudgeBench for LLM performance evaluation in coding tasks.", "Demonstration of significant performance variance among LLM-as-a-Judge models.", "Identification of effective prompting strategies that enhance judgment accuracy."], "limitations": "Models exhibit significant randomness and inconsistency in judging coding tasks, raising concerns about reliability.", "keywords": ["Large Language Models", "benchmark", "code generation", "code repair", "unit test generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.10541", "pdf": "https://arxiv.org/pdf/2507.10541.pdf", "abs": "https://arxiv.org/abs/2507.10541", "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once", "authors": ["Zhuoshi Pan", "Qizhi Pei", "Yu Li", "Qiyao Sun", "Zinan Tang", "H. Vicky Zhao", "Conghui He", "Lijun Wu"], "categories": ["cs.CL"], "comment": "REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously", "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual\ncreation of new questions with large human efforts, (2) failure to evaluate\nmodels under multi-context pressure, a key requirement for real-world\ndeployment. To bridge this gap, we present REST (Reasoning Evaluation through\nSimultaneous Testing), a stress-testing framework that concurrently exposes\nLRMs to multiple problems simultaneously. Beyond basic reasoning, REST\nspecifically evaluates several under-tested capabilities: contextual priority\nallocation, cross-problem interference resistance, and dynamic cognitive load\nmanagement. Our evaluation reveals several striking findings: Even\nstate-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance\ndegradation under stress testing. Crucially, REST demonstrates stronger\ndiscriminative power than existing benchmarks, revealing pronounced performance\ndifferences among models that exhibit similar, near-ceiling performance under\nsingle-question evaluations. Some key mechanistic insights emerge from our\nanalysis: (1) the \"overthinking trap\" is a critical factor contributing to the\nperformance degradation; (2) the models trained with \"long2short\" technique\npreserve more accuracy of their single-problem performance under REST,\noutperforming standard-trained counterparts. These results establish REST as a\ncost-efficient, future-proof evaluation paradigm that better reflects\nreal-world reasoning demands while reducing reliance on continuous human\nannotation.", "AI": {"tldr": "REST is a stress-testing framework for evaluating Large Reasoning Models (LRMs) under multi-context pressure, revealing performance degradations that conventional benchmarks do not capture.", "motivation": "To address limitations in existing evaluation methods for LRMs that focus on single-question reasoning and require constant question generation.", "method": "REST concurrently exposes models to multiple problems, evaluating contextual priority allocation, cross-problem interference resistance, and cognitive load management.", "result": "REST's evaluation highlights performance degradation of state-of-the-art models under stress, demonstrating stronger discriminative power than traditional benchmarks.", "conclusion": "REST is positioned as a cost-efficient and reliable evaluation method that reflects the complexities of real-world reasoning tasks, reducing the need for continuous human question generation.", "key_contributions": ["Introduction of REST as a new evaluation framework for LRMs", "Revealing performance differences among models previously considered similar", "Insights into the impact of training techniques on model performance under stress testing."], "limitations": "The framework's effectiveness may vary depending on the specific characteristics of different models and their training procedures.", "keywords": ["Large Reasoning Models", "stress-testing", "evaluation framework", "contextual reasoning", "cognitive load"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.09100", "pdf": "https://arxiv.org/pdf/2507.09100.pdf", "abs": "https://arxiv.org/abs/2507.09100", "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data", "authors": ["Mohammad Abolnejadian", "Shakiba Amirshahi", "Matthew Brehmer", "Anamaria Crisan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.0"], "comment": "7 pages and 4 figures. Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25)", "summary": "In decision-making conversations, experts must navigate complex choices and\nmake on-the-spot decisions while engaged in conversation. Although extensive\nhistorical data often exists, the real-time nature of these scenarios makes it\ninfeasible for decision-makers to review and leverage relevant information.\nThis raises an interesting question: What if experts could utilize relevant\npast data in real-time decision-making through insights derived from past data?\nTo explore this, we implemented a conversational user interface, taking\ndoctor-patient interactions as an example use case. Our system continuously\nlistens to the conversation, identifies patient problems and doctor-suggested\nsolutions, and retrieves related data from an embedded dataset, generating\nconcise insights using a pipeline built around a retrieval-based Large Language\nModel (LLM) agent. We evaluated the prototype by embedding Health Canada\ndatasets into a vector database and conducting simulated studies using sample\ndoctor-patient dialogues, showing effectiveness but also challenges, setting\ndirections for the next steps of our work.", "AI": {"tldr": "The paper explores using a conversational UI to assist experts in real-time decision-making by leveraging past data through a retrival-based LLM.", "motivation": "The need for effective decision-making in real-time conversations, specifically in complex scenarios where experts must make on-the-spot decisions without access to historical data.", "method": "A conversational user interface was implemented to assist in doctor-patient interactions by continuously listening to conversations, identifying relevant patient issues and solutions, and retrieving pertinent information from a vector database.", "result": "The prototype demonstrated effectiveness in generating insights from past data in simulated doctor-patient dialogues, although it also highlighted challenges that require further exploration.", "conclusion": "The study shows promise for using conversational UIs to improve decision-making in healthcare by integrating relevant past data into real-time interactions, while noting limitations that need addressing.", "key_contributions": ["Implementation of a conversational UI for decision-making", "Use of a retrieval-based LLM for real-time insights", "Integration of Health Canada datasets for practical evaluation"], "limitations": "Challenges in extracting and generating insights effectively in dynamic conversations warrant further research.", "keywords": ["Conversational UI", "Real-time decision-making", "Health informatics", "Large Language Model"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2310.10873", "pdf": "https://arxiv.org/pdf/2310.10873.pdf", "abs": "https://arxiv.org/abs/2310.10873", "title": "IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models", "authors": ["Shaokun Zhang", "Xiaobo Xia", "Zhaoqing Wang", "Ling-Hao Chen", "Jiale Liu", "Qingyun Wu", "Tongliang Liu"], "categories": ["cs.CL"], "comment": "Accepted by ICLR 2024", "summary": "In-context learning is a promising paradigm that utilizes in-context examples\nas prompts for the predictions of large language models. These prompts are\ncrucial for achieving strong performance. However, since the prompts need to be\nsampled from a large volume of annotated examples, finding the right prompt may\nresult in high annotation costs. To address this challenge, this paper\nintroduces an influence-driven selective annotation method that aims to\nminimize annotation costs while improving the quality of in-context examples.\nThe essence of our method is to select a pivotal subset from a large-scale\nunlabeled data pool to annotate for the subsequent sampling of prompts.\nSpecifically, a directed graph is first constructed to represent unlabeled\ndata. Afterward, the influence of candidate unlabeled subsets is quantified\nwith a diffusion process. A simple yet effective greedy algorithm for unlabeled\ndata selection is lastly introduced. It iteratively selects the data if it\nprovides a maximum marginal gain with respect to quantified influence. Compared\nwith previous efforts on selective annotations, our influence-driven method\nworks in an end-to-end manner, avoids an intractable explicit balance between\ndata diversity and representativeness, and enjoys theoretical support.\nExperiments confirm the superiority of the proposed method on various\nbenchmarks, achieving better performance under lower time consumption during\nsubset selection. The project page is available at\nhttps://skzhang1.github.io/IDEAL/.", "AI": {"tldr": "This paper introduces an influence-driven selective annotation method for improving prompt quality in in-context learning while minimizing annotation costs.", "motivation": "The need to sample prompts from a vast array of annotated examples can lead to high annotation costs, necessitating an efficient approach to select valuable examples.", "method": "A directed graph is constructed to represent unlabeled data and the influence of candidate subsets is quantified using a diffusion process. An iterative greedy algorithm is used for the selection of unlabeled data based on maximum marginal gain.", "result": "Experiments show that the proposed method outperforms existing selective annotation techniques in terms of performance and efficiency, achieving better results with lower time consumption.", "conclusion": "The influence-driven approach offers an end-to-end solution for selecting unlabeled data subsets, balancing the need for diversity and representativeness theoretically and practically.", "key_contributions": ["Introduction of an influence-driven method for selective annotation", "Utilization of a directed graph and diffusion process for influence quantification", "A greedy algorithm that optimally selects pivotal unlabeled data subsets"], "limitations": "", "keywords": ["in-context learning", "selective annotation", "large language models"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2402.07577", "pdf": "https://arxiv.org/pdf/2402.07577.pdf", "abs": "https://arxiv.org/abs/2402.07577", "title": "Topic Modeling as Multi-Objective Contrastive Optimization", "authors": ["Thong Nguyen", "Xiaobao Wu", "Xinshuai Dong", "Cong-Duy T Nguyen", "See-Kiong Ng", "Anh Tuan Luu"], "categories": ["cs.CL"], "comment": "Accepted at ICLR 2024 (poster). Official version available at:\n  https://openreview.net/forum?id=HdAoLSBYXj", "summary": "Recent representation learning approaches enhance neural topic models by\noptimizing the weighted linear combination of the evidence lower bound (ELBO)\nof the log-likelihood and the contrastive learning objective that contrasts\npairs of input documents. However, document-level contrastive learning might\ncapture low-level mutual information, such as word ratio, which disturbs topic\nmodeling. Moreover, there is a potential conflict between the ELBO loss that\nmemorizes input details for better reconstruction quality, and the contrastive\nloss which attempts to learn topic representations that generalize among input\ndocuments. To address these issues, we first introduce a novel contrastive\nlearning method oriented towards sets of topic vectors to capture useful\nsemantics that are shared among a set of input documents. Secondly, we\nexplicitly cast contrastive topic modeling as a gradient-based multi-objective\noptimization problem, with the goal of achieving a Pareto stationary solution\nthat balances the trade-off between the ELBO and the contrastive objective.\nExtensive experiments demonstrate that our framework consistently produces\nhigher-performing neural topic models in terms of topic coherence, topic\ndiversity, and downstream performance.", "AI": {"tldr": "This paper presents a novel method for contrastive topic modeling that aims to enhance topic coherence and diversity while addressing previous conflicts in learning objectives.", "motivation": "To improve the effectiveness of neural topic models by resolving contradictions between evidence lower bound (ELBO) and contrastive learning objectives for better topic representation.", "method": "Introduces a contrastive learning method for sets of topic vectors and formulates contrastive topic modeling as a multi-objective optimization problem, aiming for a Pareto stationary solution.", "result": "The proposed framework yields neural topic models with improved topic coherence, diversity, and overall performance in downstream tasks compared to existing methods.", "conclusion": "The new approach effectively balances ELBO and contrastive objectives, enhancing the quality of topic modeling.", "key_contributions": ["Novel contrastive learning method focused on topic vectors", "Multi-objective optimization formulation for contrastive topic modeling", "Demonstrated improvement in topic coherence and diversity"], "limitations": "The paper does not address specific computational efficiency or scalability issues of the proposed method.", "keywords": ["contrastive learning", "topic modeling", "neural models", "multi-objective optimization", "topic coherence"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2402.11347", "pdf": "https://arxiv.org/pdf/2402.11347.pdf", "abs": "https://arxiv.org/abs/2402.11347", "title": "SEE: Strategic Exploration and Exploitation for Cohesive In-Context Prompt Optimization", "authors": ["Wendi Cui", "Zhuohang Li", "Hao Sun", "Damien Lopez", "Kamalika Das", "Bradley Malin", "Sricharan Kumar", "Jiaxin Zhang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Designing optimal prompts for Large Language Models (LLMs) is a complicated\nand resource-intensive task, often requiring substantial human expertise and\neffort. Existing approaches typically separate the optimization of prompt\ninstructions and in-context learning examples, leading to incohesive prompts\nthat are defined and represented by suboptimal task performance. To overcome\nthese challenges, we propose a novel Cohesive In-Context Prompt Optimization\nframework that refines both prompt instructions and examples. However,\nformulating such an optimization in the discrete and high-dimensional space of\nnatural language poses significant challenges in both convergence and\ncomputational efficiency. To address these issues, we introduce SEE, a scalable\nand efficient prompt optimization framework that adopts metaheuristic\noptimization principles and strategically balances exploration and exploitation\nto enhance optimization performance and achieve efficient convergence. SEE\nfeatures a quad-phased design that alternates between global traversal\n(exploration) and local optimization (exploitation) and adaptively chooses LLM\noperators during the optimization process. We have conducted a comprehensive\nevaluation across 35 benchmark tasks, and SEE significantly outperforms\nstate-of-the-art baseline methods by a large margin, achieving an average\nperformance gain of 13.94 while reducing computational costs by 58.67.", "AI": {"tldr": "This paper presents SEE, a new framework for optimizing prompts in Large Language Models by refining both prompt instructions and examples, which outperforms existing methods significantly.", "motivation": "The task of designing optimal prompts for LLMs is complex and often results in suboptimal performance due to its separation of prompt instructions and in-context examples.", "method": "The paper introduces a novel framework called Cohesive In-Context Prompt Optimization, implemented through SEE, which utilizes metaheuristic principles for efficient optimization by balancing exploration and exploitation during the prompt design process.", "result": "SEE was evaluated across 35 benchmark tasks and demonstrated a 13.94 average performance gain while reducing computational costs by 58.67 compared to state-of-the-art methods.", "conclusion": "The proposed SEE framework effectively addresses the challenges of prompt optimization in LLMs, leading to improved performance and efficiency.", "key_contributions": ["Introduction of a novel optimization framework for LLM prompts", "A quad-phased design enabling efficient exploration and exploitation", "Significant performance improvements on benchmark tasks compared to existing methods"], "limitations": "", "keywords": ["Large Language Models", "Prompt Optimization", "Metaheuristic Principles"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2404.03353", "pdf": "https://arxiv.org/pdf/2404.03353.pdf", "abs": "https://arxiv.org/abs/2404.03353", "title": "Towards Pareto Optimal Throughput in Small Language Model Serving", "authors": ["Pol G. Recasens", "Yue Zhu", "Chen Wang", "Eun Kyung Lee", "Olivier Tardieu", "Alaa Youssef", "Jordi Torres", "Josep Ll. Berral"], "categories": ["cs.CL"], "comment": "Revised version of the paper published at EuroMLSys'24", "summary": "Large language models (LLMs) have revolutionized the state-of-the-art of many\ndifferent natural language processing tasks. Although serving LLMs is\ncomputationally and memory demanding, the rise of Small Language Models (SLMs)\noffers new opportunities for resource-constrained users, who now are able to\nserve small models with cutting-edge performance. In this paper, we present a\nset of experiments designed to benchmark SLM inference at performance and\nenergy levels. Our analysis provides a new perspective in serving, highlighting\nthat the small memory footprint of SLMs allows for reaching the Pareto-optimal\nthroughput within the resource capacity of a single accelerator. In this\nregard, we present an initial set of findings demonstrating how model\nreplication can effectively improve resource utilization for serving SLMs.", "AI": {"tldr": "This paper benchmarks Small Language Models (SLMs) for inference performance and energy efficiency, demonstrating that their lower memory requirements allow for optimal resource utilization in serving applications.", "motivation": "The rise of Small Language Models (SLMs) presents new opportunities for resource-constrained users to employ NLP technologies without extensive computational demands.", "method": "The authors conducted a series of experiments to evaluate the inference performance and energy consumption of SLMs, focusing on how to maximize resource utilization through model replication.", "result": "The analysis reveals that SLMs can achieve Pareto-optimal throughput, utilizing significantly less memory while maintaining competitive performance compared to larger models.", "conclusion": "Effective model replication improves resource utilization for serving SLMs, aiding users in deploying efficient natural language processing applications.", "key_contributions": ["Benchmarking methodology for Small Language Models (SLMs)", "Insight into energy-efficient serving of SLMs", "Strategies for improving resource utilization through model replication"], "limitations": "", "keywords": ["Small Language Models", "Energy Efficiency", "Resource Utilization", "Natural Language Processing", "Benchmarking"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2405.11870", "pdf": "https://arxiv.org/pdf/2405.11870.pdf", "abs": "https://arxiv.org/abs/2405.11870", "title": "Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process", "authors": ["Ermo Hua", "Biqing Qi", "Kaiyan Zhang", "Kai Tian", "Xingtai Lv", "Ning Ding", "Bowen Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025, Oral & Panel Discussion", "summary": "Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are key\nprocesses for aligning Language Models (LMs) with human preferences post\npre-training. While SFT excels in efficiency and PO in effectiveness, they are\noften combined sequentially without integrating their optimization objectives.\nThis approach ignores the opportunities to bridge their paradigm gap and take\nthe strengths from both. In this paper, we interpret SFT and PO with two\nsub-processes -- Preference Estimation and Transition Optimization -- defined\nat token level within the Markov Decision Process (MDP). This modeling shows\nthat SFT is only a special case of PO with inferior estimation and\noptimization. PO estimates the model's preference by its entire generation,\nwhile SFT only scores model's subsequent predicted tokens based on prior tokens\nfrom ground truth answer. These priors deviates from model's distribution,\nhindering the preference estimation and transition optimization. Building on\nthis view, we introduce Intuitive Fine-Tuning (IFT) to integrate SFT and PO\ninto a single process. Through a temporal residual connection, IFT brings\nbetter estimation and optimization by capturing LMs' intuitive sense of its\nentire answers. But it solely relies on a single policy and the same volume of\nnon-preference-labeled data as SFT. Our experiments show that IFT performs\ncomparably or even superiorly to SFT and some typical PO methods across several\ntasks, particularly those require generation, reasoning, and fact-following\nabilities. An explainable Frozen Lake game further validates the effectiveness\nof IFT for getting competitive policy.", "AI": {"tldr": "Introducing Intuitive Fine-Tuning (IFT), a novel approach that integrates Supervised Fine-Tuning (SFT) and Preference Optimization (PO) to enhance language model alignment with human preferences.", "motivation": "Existing methods for aligning language models often separate SFT and PO, not leveraging the strengths of both. This paper seeks to bridge that gap.", "method": "The paper models SFT and PO as two subprocesses in a Markov Decision Process framework, leading to the development of Intuitive Fine-Tuning (IFT), which integrates SFT and PO into a single process.", "result": "IFT shows performance comparable or superior to SFT and common PO methods across multiple tasks, especially in generation, reasoning, and fact-following scenarios.", "conclusion": "IFT offers a new way to improve language models by better estimating and optimizing preferences using a unified approach, while relying on the same data volume as SFT.", "key_contributions": ["Development of Intuitive Fine-Tuning (IFT) that integrates SFT and PO.", "Modeling SFT and PO as subprocesses within an MDP framework.", "Demonstration of IFT's superior performance in various language generation tasks."], "limitations": "IFT relies on a single policy and requires the same volume of non-preference-labeled data as SFT, which could be limiting in certain contexts.", "keywords": ["Language Models", "Supervised Fine-Tuning", "Preference Optimization", "Human Preferences", "Markov Decision Process"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2408.11415", "pdf": "https://arxiv.org/pdf/2408.11415.pdf", "abs": "https://arxiv.org/abs/2408.11415", "title": "Political Bias in LLMs: Unaligned Moral Values in Agent-centric Simulations", "authors": ["Simon Münker"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 2 tables", "summary": "Contemporary research in social sciences increasingly utilizes\nstate-of-the-art generative language models to annotate or generate content.\nWhile these models achieve benchmark-leading performance on common language\ntasks, their application to novel out-of-domain tasks remains insufficiently\nexplored. To address this gap, we investigate how personalized language models\nalign with human responses on the Moral Foundation Theory Questionnaire. We\nadapt open-source generative language models to different political personas\nand repeatedly survey these models to generate synthetic data sets where\nmodel-persona combinations define our sub-populations. Our analysis reveals\nthat models produce inconsistent results across multiple repetitions, yielding\nhigh response variance. Furthermore, the alignment between synthetic data and\ncorresponding human data from psychological studies shows a weak correlation,\nwith conservative persona-prompted models particularly failing to align with\nactual conservative populations. These results suggest that language models\nstruggle to coherently represent ideologies through in-context prompting due to\ntheir alignment process. Thus, using language models to simulate social\ninteractions requires measurable improvements in in-context optimization or\nparameter manipulation to align with psychological and sociological stereotypes\nproperly.", "AI": {"tldr": "This paper examines the alignment of personalized generative language models with human responses based on the Moral Foundation Theory Questionnaire, highlighting inconsistencies and weak correlations with actual human data.", "motivation": "To explore the application of generative language models in social sciences and their alignment with human-like responses in political contexts, particularly addressing the gaps in their use for out-of-domain tasks.", "method": "The study adapts open-source generative language models to vary by political personas and generates synthetic datasets through repeated surveys, analyzing the results against psychological data.", "result": "The analysis revealed high response variance and weak correlation between synthetic model outputs and real human data, especially for conservative persona models, indicating challenges in ideological representation through language models.", "conclusion": "The findings underscore the need for improvements in in-context optimization of generative language models to better represent psychological and sociological ideologies in simulated social interactions.", "key_contributions": ["Investigation of generative language models in social science applications", "Analysis of model-persona combinations and their inconsistencies", "Identification of weak correlation between synthetic and human data"], "limitations": "The study is limited by the scope of political personas selected and the inherent challenges in modeling complex human ideologies accurately with current techniques.", "keywords": ["generative language models", "Moral Foundation Theory", "political personas", "synthetic data", "social sciences"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2409.01389", "pdf": "https://arxiv.org/pdf/2409.01389.pdf", "abs": "https://arxiv.org/abs/2409.01389", "title": "CV-Probes: Studying the interplay of lexical and world knowledge in visually grounded verb understanding", "authors": ["Ivana Beňová", "Michal Gregor", "Albert Gatt"], "categories": ["cs.CL"], "comment": "9 pages, 2 figure, 6 tables, CogSci conference 2025", "summary": "How do vision-language (VL) transformer models ground verb phrases and do\nthey integrate contextual and world knowledge in this process? We introduce the\nCV-Probes dataset, containing image-caption pairs involving verb phrases that\nrequire both social knowledge and visual context to interpret (e.g., \"beg\"), as\nwell as pairs involving verb phrases that can be grounded based on information\ndirectly available in the image (e.g., \"sit\"). We show that VL models struggle\nto ground VPs that are strongly context-dependent. Further analysis using\nexplainable AI techniques shows that such models may not pay sufficient\nattention to the verb token in the captions. Our results suggest a need for\nimproved methodologies in VL model training and evaluation. The code and\ndataset will be available https://github.com/ivana-13/CV-Probes.", "AI": {"tldr": "This paper introduces the CV-Probes dataset to analyze how vision-language transformer models ground verb phrases, showing their struggles with context-dependent phrases and emphasizing the need for improved training methodologies.", "motivation": "To explore how vision-language models integrate both contextual and world knowledge while grounding verb phrases, particularly focusing on those that require social or visual context.", "method": "The authors introduce the CV-Probes dataset, consisting of image-caption pairs indicating different types of verb phrases. They analyze the models' grounding ability using explainable AI techniques to identify where model attention falls short.", "result": "VL models perform poorly in grounding context-dependent verb phrases. The analysis reveals insufficient attention to verb tokens in captions, indicating a gap in current model capabilities.", "conclusion": "The findings highlight the necessity for improved methodologies in training and evaluating vision-language models, particularly for context-sensitive tasks.", "key_contributions": ["Introduction of the CV-Probes dataset", "Analysis of VL model performance in grounding verb phrases", "Usage of explainable AI techniques to assess model attention"], "limitations": "The study primarily focuses on specific social and visual contexts; broader applicability remains to be tested.", "keywords": ["Vision-Language Models", "Verb Phrases", "Machine Learning", "Explainable AI", "Dataset"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2409.05137", "pdf": "https://arxiv.org/pdf/2409.05137.pdf", "abs": "https://arxiv.org/abs/2409.05137", "title": "READoc: A Unified Benchmark for Realistic Document Structured Extraction", "authors": ["Zichao Li", "Aizier Abulaiti", "Yaojie Lu", "Xuanang Chen", "Jia Zheng", "Hongyu Lin", "Xianpei Han", "Le Sun"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Findings", "summary": "Document Structured Extraction (DSE) aims to extract structured content from\nraw documents. Despite the emergence of numerous DSE systems, their unified\nevaluation remains inadequate, significantly hindering the field's advancement.\nThis problem is largely attributed to existing benchmark paradigms, which\nexhibit fragmented and localized characteristics. To address these limitations\nand offer a thorough evaluation of DSE systems, we introduce a novel benchmark\nnamed READoc, which defines DSE as a realistic task of converting unstructured\nPDFs into semantically rich Markdown. The READoc dataset is derived from 3,576\ndiverse and real-world documents from arXiv, GitHub, and Zenodo. In addition,\nwe develop a DSE Evaluation S$^3$uite comprising Standardization, Segmentation\nand Scoring modules, to conduct a unified evaluation of state-of-the-art DSE\napproaches. By evaluating a range of pipeline tools, expert visual models, and\ngeneral VLMs, we identify the gap between current work and the unified,\nrealistic DSE objective for the first time. We aspire that READoc will catalyze\nfuture research in DSE, fostering more comprehensive and practical solutions.", "AI": {"tldr": "Introduction of the READoc benchmark for Unified Evaluation of Document Structured Extraction (DSE) systems.", "motivation": "To address the fragmented evaluation of DSE systems which hinders the advancement of the field.", "method": "Development of a novel benchmark READoc with a dataset of 3,576 real-world documents and a DSE Evaluation S$^3$uite including Standardization, Segmentation, and Scoring modules.", "result": "Identification of the gap between current DSE approaches and the realistic DSE objective through the evaluation of various tools including pipeline tools, expert visual models, and general VLMs.", "conclusion": "READoc aims to catalyze future research in DSE, promoting practical and comprehensive solutions.", "key_contributions": ["Introduction of a unified benchmark for DSE systems.", "Creation of the READoc dataset from diverse real-world documents.", "Development of a comprehensive evaluation suite for DSE."], "limitations": "", "keywords": ["Document Structured Extraction", "DSE", "READoc", "Benchmark", "Evaluation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2409.13748", "pdf": "https://arxiv.org/pdf/2409.13748.pdf", "abs": "https://arxiv.org/abs/2409.13748", "title": "TheraGen: Therapy for Every Generation", "authors": ["Kartikey Doshi", "Jimit Shah", "Narendra Shekokar"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "This paper contains major errors in methodology and results. It\n  should not be cited", "summary": "We present TheraGen, an advanced AI-powered mental health chatbot utilizing\nthe LLaMA 2 7B model. This approach builds upon recent advancements in language\nmodels and transformer architectures. TheraGen provides all-day personalized,\ncompassionate mental health care by leveraging a large dataset of 1 million\nconversational entries, combining anonymized therapy transcripts, online mental\nhealth discussions, and psychological literature, including APA resources. Our\nimplementation employs transfer learning, fine-tuning, and advanced training\ntechniques to optimize performance. TheraGen offers a user-friendly interface\nfor seamless interaction, providing empathetic responses and evidence-based\ncoping strategies. Evaluation results demonstrate high user satisfaction rates,\nwith 94% of users reporting improved mental well-being. The system achieved a\nBLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response\naccuracy. With an average response time of 1395 milliseconds, TheraGen ensures\nreal-time, efficient support. While not a replacement for professional therapy,\nTheraGen serves as a valuable complementary tool, significantly improving user\nwell-being and addressing the accessibility gap in mental health treatments.\nThis paper details TheraGen's architecture, training methodology, ethical\nconsiderations, and future directions, contributing to the growing field of\nAI-assisted mental healthcare and offering a scalable solution to the pressing\nneed for mental health support.", "AI": {"tldr": "TheraGen is an AI-powered mental health chatbot using the LLaMA 2 model, designed to provide personalized mental health support with high user satisfaction.", "motivation": "To address the accessibility gap in mental health treatments and offer a complementary tool for mental well-being.", "method": "Utilizes the LLaMA 2 7B model, employing transfer learning and advanced training on a dataset of 1 million conversational entries.", "result": "Achieved a BLEU score of 0.67 and a ROUGE score of 0.62, with 94% user satisfaction indicating improved mental well-being.", "conclusion": "TheraGen is valuable in the mental health sector, although it is not a substitute for professional therapy.", "key_contributions": ["Implementation of an advanced AI chatbot for mental health care", "Use of a large dataset for training to enhance chatbot responses", "Integration of user feedback for improvement and evaluation of effectiveness"], "limitations": "Contains major errors in methodology and results; not intended for citation.", "keywords": ["AI chatbot", "mental health", "LLaMA 2", "transfer learning", "user satisfaction"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2410.12380", "pdf": "https://arxiv.org/pdf/2410.12380.pdf", "abs": "https://arxiv.org/abs/2410.12380", "title": "Evaluation of Attribution Bias in Generator-Aware Retrieval-Augmented Large Language Models", "authors": ["Amin Abolghasemi", "Leif Azzopardi", "Seyyed Hadi Hashemi", "Maarten de Rijke", "Suzan Verberne"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Attributing answers to source documents is an approach used to enhance the\nverifiability of a model's output in retrieval augmented generation (RAG).\nPrior work has mainly focused on improving and evaluating the attribution\nquality of large language models (LLMs) in RAG, but this may come at the\nexpense of inducing biases in the attribution of answers. We define and examine\ntwo aspects in the evaluation of LLMs in RAG pipelines, namely attribution\nsensitivity and bias with respect to authorship information. We explicitly\ninform an LLM about the authors of source documents, instruct it to attribute\nits answers, and analyze (i) how sensitive the LLM's output is to the author of\nsource documents, and (ii) whether the LLM exhibits a bias towards\nhuman-written or AI-generated source documents. We design an experimental setup\nin which we use counterfactual evaluation to study three LLMs in terms of their\nattribution sensitivity and bias in RAG pipelines. Our results show that adding\nauthorship information to source documents can significantly change the\nattribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have\nan attribution bias towards explicit human authorship, which can serve as a\ncompeting hypothesis for findings of prior work that shows that LLM-generated\ncontent may be preferred over human-written contents. Our findings indicate\nthat metadata of source documents can influence LLMs' trust, and how they\nattribute their answers. Furthermore, our research highlights attribution bias\nand sensitivity as a novel aspect of brittleness in LLMs.", "AI": {"tldr": "This paper examines the impact of authorship information on the attribution quality and biases of large language models in retrieval augmented generation (RAG) settings.", "motivation": "To enhance the verifiability of model outputs in RAG, this study investigates attribution sensitivity and bias regarding authorship.", "method": "An experimental setup was designed using counterfactual evaluation to assess three large language models on their attribution sensitivity and biases when provided with authorship information.", "result": "Results indicate that including authorship information can alter attribution quality by 3% to 18%, with evidence of a bias towards human-written documents.", "conclusion": "The study suggests that source document metadata influences LLMs' trust and attribution quality, revealing new dimensions of brittleness in LLMs.", "key_contributions": ["Introduction of the concepts of attribution sensitivity and bias in LLMs for RAG.", "Demonstration of how authorship affects attribution quality and biases in LLM outputs.", "Highlighting the implications of document metadata on the reliability of LLM-generated answers."], "limitations": "The study focuses on specific LLMs and may not generalize to all models or domains.", "keywords": ["large language models", "attribution sensitivity", "bias", "retrieval augmented generation", "metadata"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.07533", "pdf": "https://arxiv.org/pdf/2411.07533.pdf", "abs": "https://arxiv.org/abs/2411.07533", "title": "Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence", "authors": ["Linyang He", "Ercong Nie", "Helmut Schmid", "Hinrich Schütze", "Nima Mesgarani", "Jonathan Brennan"], "categories": ["cs.CL"], "comment": null, "summary": "This study investigates the linguistic understanding of Large Language Models\n(LLMs) regarding signifier (form) and signified (meaning) by distinguishing two\nLLM assessment paradigms: psycholinguistic and neurolinguistic. Traditional\npsycholinguistic evaluations often reflect statistical rules that may not\naccurately represent LLMs' true linguistic competence. We introduce a\nneurolinguistic approach, utilizing a novel method that combines minimal pair\nand diagnostic probing to analyze activation patterns across model layers. This\nmethod allows for a detailed examination of how LLMs represent form and\nmeaning, and whether these representations are consistent across languages. We\nfound: (1) Psycholinguistic and neurolinguistic methods reveal that language\nperformance and competence are distinct; (2) Direct probability measurement may\nnot accurately assess linguistic competence; (3) Instruction tuning won't\nchange much competence but improve performance; (4) LLMs exhibit higher\ncompetence and performance in form compared to meaning. Additionally, we\nintroduce new conceptual minimal pair datasets for Chinese (COMPS-ZH) and\nGerman (COMPS-DE), complementing existing English datasets.", "AI": {"tldr": "This study examines LLMs' linguistic understanding using psycholinguistic and neurolinguistic methods, revealing differences in language performance and competence.", "motivation": "To investigate the true linguistic competence of Large Language Models (LLMs) and provide a more accurate assessment framework.", "method": "A neurolinguistic approach combining minimal pair and diagnostic probing to analyze LLMs' activation patterns across model layers.", "result": "The study found that performance and competence in language tasks are distinct; probability measurements do not accurately reflect competence; instruction tuning improves performance without altering competence; LLMs show better competence in form compared to meaning.", "conclusion": "The introduction of new datasets for Chinese and German provides additional resources for evaluating LLMs' linguistic abilities across languages.", "key_contributions": ["Introduced a novel neurolinguistic approach for assessing LLMs", "Identified distinct differences between language performance and competence", "Provided new minimal pair datasets for Chinese and German"], "limitations": "", "keywords": ["Large Language Models", "neurolinguistics", "psycholinguistics", "linguistic competence", "language representation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2411.13820", "pdf": "https://arxiv.org/pdf/2411.13820.pdf", "abs": "https://arxiv.org/abs/2411.13820", "title": "InstCache: A Predictive Cache for LLM Serving", "authors": ["Longwei Zou", "Yan Liu", "Jiamu Kang", "Tingfeng Liu", "Jiangang Kong", "Yangdong Deng"], "categories": ["cs.CL", "cs.DC"], "comment": null, "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.", "AI": {"tldr": "InstCache is a predictive caching mechanism for Large Language Model (LLM) serving systems that improves instruction-response caching efficiency.", "motivation": "The rising demand for LLM inference systems and the inefficiencies of current caching methods necessitate improved caching techniques.", "method": "InstCache reorders the representation space of instruction texts to enhance spatial locality for better prediction of instruction-response pairs.", "result": "InstCache achieves a 2.3x higher hit rate over traditional caching mechanisms and significantly reduces the output token computation time by up to 50% on certain datasets.", "conclusion": "InstCache provides an effective solution for improving caching efficiency in LLM inference systems, leading to lower computation times and higher hit rates.", "key_contributions": ["Introduction of InstCache for predictive instruction-response caching", "Achieving higher hit rates compared to traditional methods", "Demonstrating significant reductions in computation time per token"], "limitations": "", "keywords": ["Large Language Models", "caching", "machine learning", "inference systems", "spatial locality"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.00947", "pdf": "https://arxiv.org/pdf/2412.00947.pdf", "abs": "https://arxiv.org/abs/2412.00947", "title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information", "authors": ["Ryo Kamoi", "Yusen Zhang", "Sarkar Snigdha Sarathi Das", "Ranran Haoran Zhang", "Rui Zhang"], "categories": ["cs.CL", "cs.CV"], "comment": "COLM 2025. VisOnlyQA dataset, code, and model responses are provided\n  at https://github.com/psunlpgroup/VisOnlyQA. Please also refer to our project\n  website at https://visonlyqa.github.io/", "summary": "Large Vision Language Models (LVLMs) have achieved remarkable performance in\nvarious vision-language tasks. However, it is still unclear how accurately\nLVLMs can perceive visual information in images. In particular, the capability\nof LVLMs to perceive geometric information, such as shape, angle, and size,\nremains insufficiently analyzed, although the perception of these properties is\ncrucial for tasks that require a detailed visual understanding. In this work,\nwe introduce VisOnlyQA, a dataset for evaluating the geometric perception of\nLVLMs, and reveal that LVLMs often cannot accurately perceive basic geometric\ninformation in images, while human performance is nearly perfect. VisOnlyQA\nconsists of 12 tasks that directly ask about geometric information in geometric\nshapes, charts, chemical structures, and 3D shapes. Our experiments highlight\nthe following findings: (i) State-of-the-art LVLMs struggle with basic\ngeometric perception. 23 LVLMs we evaluate, including GPT-4o and Gemini 2.5\nPro, work poorly on VisOnlyQA. (ii) Additional training data does not resolve\nthis issue. Fine-tuning on the training set of VisOnlyQA is not always\neffective, even for in-distribution tasks. (iii) LLM may be the bottleneck.\nLVLMs using stronger LLMs exhibit better geometric perception on VisOnlyQA,\nwhile it does not require complex reasoning, suggesting that the way LVLMs\nprocess information from visual encoders is a bottleneck. The datasets, code,\nand model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.", "AI": {"tldr": "This paper introduces VisOnlyQA, a dataset aimed at evaluating the geometric perception capabilities of Large Vision Language Models (LVLMs) and reveals their poor performance compared to humans.", "motivation": "To analyze the geometric perception abilities of LVLMs, which remain insufficiently explored despite their success in vision-language tasks.", "method": "Introducing VisOnlyQA, a dataset composed of 12 tasks focusing on geometric information in images, charts, chemical structures, and 3D shapes to benchmark LVLMs' geometric perception.", "result": "The paper finds that state-of-the-art LVLMs, including GPT-4o and Gemini 2.5 Pro, struggle significantly with geometric perception tasks, highlighting gaps in their visual understanding abilities.", "conclusion": "The findings suggest that the current LVLMs' performance issues may stem from the way they process information rather than a lack of data, indicating that leveraging stronger underlying LLMs could improve geometric perception in LVLMs.", "key_contributions": ["Introduction of the VisOnlyQA dataset for evaluating LVLMs", "Demonstration of significant performance gaps in LVLMs compared to humans in geometric perception tasks", "Insight into the limitations of current LVLM architectures in processing geometric information."], "limitations": "The dataset's effectiveness in improving LVLM performance through fine-tuning is inconsistent, and there may be inherent limitations in LVLM architecture.", "keywords": ["Large Vision Language Models", "geometric perception", "VisOnlyQA", "dataset", "human comparison"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.05225", "pdf": "https://arxiv.org/pdf/2412.05225.pdf", "abs": "https://arxiv.org/abs/2412.05225", "title": "BEExformer: A Fast Inferencing Binarized Transformer with Early Exits", "authors": ["Wazib Ansar", "Saptarsi Goswami", "Amlan Chakrabarti"], "categories": ["cs.CL", "cs.AI", "cs.NE"], "comment": "This revised manuscript includes 18 pages, 17 figures, and 6 tables.\n  Methodology and results sections have been improved for clarity and depth,\n  incorporating additional comparisons, ablations, and a new evaluation\n  dataset. A few relevant references were added, and overall organization\n  refined for better readability", "summary": "Large Language Models (LLMs) based on transformers achieve cutting-edge\nresults on a variety of applications. However, their enormous size and\nprocessing requirements hinder deployment on constrained resources. To enhance\nefficiency, binarization and Early Exit (EE) have proved to be effective\nsolutions. However, binarization may lead to performance loss as reduced\nprecision affects gradient estimation and parameter updates. Besides, research\non EE mechanisms is still in its early stages. To address these challenges, we\nintroduce Binarized Early Exit Transformer (BEExformer), the first-ever\nselective learning-based transformer integrating Binarization-Aware Training\n(BAT) with EE for efficient and fast textual inference. Each transformer block\nhas an integrated Selective-Learn Forget Network (SLFN) to enhance contextual\nretention while eliminating irrelevant information. The BAT employs a\ndifferentiable second-order approximation to the sign function, enabling\ngradient computation that captures both the sign and magnitude of the weights.\nThis aids in 21.30 times reduction in model size. The EE mechanism hinges on\nfractional reduction in entropy among intermediate transformer blocks with\nsoft-routing loss estimation. This accelerates inference by reducing FLOPs by\n52.08% and even improves accuracy by 2.89% by resolving the \"overthinking\"\nproblem inherent in deep networks. Extensive evaluation through comparison with\nthe SOTA methods and various ablations across six datasets covering multiple\nNLP tasks demonstrates its Pareto-optimal performance-efficiency trade-off.", "AI": {"tldr": "Introduction of BEExformer, a transformer model enhancing efficiency through binarization and early exit mechanisms for NLP tasks.", "motivation": "Address the deployment challenges of large language models due to their size and processing requirements by improving efficiency.", "method": "BEExformer integrates Binarization-Aware Training (BAT) with Early Exit (EE) mechanisms, employing a Selective-Learn Forget Network (SLFN) for improved contextual retention and efficient inference.", "result": "Achieves a 21.30 times reduction in model size, reduces FLOPs by 52.08%, and improves accuracy by 2.89% while demonstrating superior performance-efficiency across multiple NLP tasks.", "conclusion": "BEExformer offers a Pareto-optimal performance-efficiency trade-off, suitable for constrained environments, enhancing the usability of transformer models.", "key_contributions": ["Introduction of Binarized Early Exit Transformer (BEExformer) for NLP tasks.", "Integration of Binarization-Aware Training with Early Exit mechanisms.", "Demonstrated significant reductions in model size and computational requirements while improving accuracy."], "limitations": "The research is still nascent regarding Early Exit mechanisms and potential performance losses from binarization.", "keywords": ["Binarization", "Early Exit", "Transformer", "Selective-Learn Forget Network", "NLP"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2501.03940", "pdf": "https://arxiv.org/pdf/2501.03940.pdf", "abs": "https://arxiv.org/abs/2501.03940", "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection", "authors": ["Pablo Miralles-González", "Javier Huertas-Tato", "Alejandro Martín", "David Camacho"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.", "AI": {"tldr": "This paper presents Perplexity Attention Weighted Network (PAWN), a method for detecting AI-generated content leveraging LLM next-token distribution metrics and last hidden states to improve performance while reducing resource requirements.", "motivation": "The rise of large language models has led to concerns about AI content misuse, necessitating effective detection methods, especially in unfamiliar domains.", "method": "The proposed PAWN employs attention weights on next-token distribution metrics to aggregate predictions more effectively, caching relevant data to lower training costs.", "result": "PAWN outperforms state-of-the-art baseline models, showing better performance in-distribution with fewer trainable parameters and improved generalization to unseen domains.", "conclusion": "PAWN enhances detection capabilities for AI-generated text, maintaining robustness against adversarial attacks and showing effective multilingual generalization.", "key_contributions": ["Introduction of Perplexity Attention Weighted Network (PAWN) for detection of AI-generated content", "Improved aggregation of next-token predictions through attention weighting", "Demonstrated superior performance with reduced parameter requirements compared to fine-tuned LMs"], "limitations": "", "keywords": ["large language models", "AI content detection", "Perplexity Attention Weighted Network", "machine learning", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.12851", "pdf": "https://arxiv.org/pdf/2501.12851.pdf", "abs": "https://arxiv.org/abs/2501.12851", "title": "ACEBench: Who Wins the Match Point in Tool Usage?", "authors": ["Chen Chen", "Xinlong Hao", "Weiwen Liu", "Xu Huang", "Xingshan Zeng", "Shuai Yu", "Dexun Li", "Shuai Wang", "Weinan Gan", "Yuefeng Huang", "Wulong Liu", "Xinzhi Wang", "Defu Lian", "Baoqun Yin", "Yasheng Wang", "Wu Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes.", "AI": {"tldr": "ACEBench is a new benchmark for evaluating tool usage in Large Language Models (LLMs), addressing limitations in existing evaluation methods.", "motivation": "Current benchmarks for LLMs' tool usage are inadequate in evaluating real multi-turn dialogue contexts, assessment dimensions, and the overhead of relying on LLMs or real API executions.", "method": "ACEBench categorizes evaluation data into three types: Normal (basic scenarios), Special (ambiguous/incomplete instructions), and Agent (multi-agent interactions), to assess tool usage more comprehensively.", "result": "Extensive experiments with ACEBench allowed for in-depth analysis of various LLMs and provided insights into error causes across different evaluation scenarios.", "conclusion": "ACEBench offers a robust framework for evaluating LLMs' tool usage, allowing for better assessments of their decision-making and reasoning capabilities.", "key_contributions": ["Introduction of ACEBench framework for tool usage evaluation", "Categorization of evaluation scenarios into Normal, Special, and Agent", "Granular analysis of error causes in LLMs' tool usage"], "limitations": "", "keywords": ["Large Language Models", "tool usage", "ACEBench", "evaluation", "multi-turn dialogue"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.07776", "pdf": "https://arxiv.org/pdf/2502.07776.pdf", "abs": "https://arxiv.org/abs/2502.07776", "title": "Auditing Prompt Caching in Language Model APIs", "authors": ["Chenchen Gu", "Xiang Lisa Li", "Rohith Kuditipudi", "Percy Liang", "Tatsunori Hashimoto"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.", "AI": {"tldr": "This paper investigates the implications of prompt caching in large language models (LLMs), revealing timing variations that may lead to privacy issues, including potential side-channel attacks.", "motivation": "To address the risks posed by prompt caching in LLMs, especially in terms of privacy and information leakage.", "method": "The authors conducted statistical audits on real-world LLM API providers to detect prompt caching behavior.", "result": "The study found evidence of global cache sharing across seven API providers, including OpenAI, leading to potential privacy leakage and insights into model architecture.", "conclusion": "The findings highlight the need for increased transparency from API providers regarding their caching policies to protect user privacy.", "key_contributions": ["Detection of global prompt caching across multiple API providers", "Evidence of potential privacy leakage in LLM interactions", "Insights into model architecture obtained through timing analysis"], "limitations": "The study may not cover all API providers and caching behaviors, and results may vary over time with new updates from providers.", "keywords": ["prompt caching", "large language models", "privacy leakage", "timing attacks", "API security"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.12992", "pdf": "https://arxiv.org/pdf/2502.12992.pdf", "abs": "https://arxiv.org/abs/2502.12992", "title": "B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability", "authors": ["Yifan Wang", "Sukrut Rao", "Ji-Ung Lee", "Mayank Jobanputra", "Vera Demberg"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Post-hoc explanation methods for black-box models often struggle with\nfaithfulness and human interpretability due to the lack of explainability in\ncurrent neural architectures. Meanwhile, B-cos networks have been introduced to\nimprove model explainability by proposing an architecture that removes bias\nterms and promotes input-weight alignment. Although B-cos networks have shown\nsuccess in building explainable systems, their application has so far been\nlimited to computer vision models and their associated training pipelines. In\nthis work, we introduce B-cos LMs, i.e., B-cos language models (LMs) empowered\nfor natural language processing (NLP) tasks. Our approach directly transforms\npre-trained language models into B-cos LMs by combining B-cos conversion and\ntask fine-tuning, improving efficiency compared to previous methods. Our\nautomatic and human evaluation results demonstrate that B-cos LMs produce more\nfaithful and human interpretable explanations than post-hoc methods, while\nmaintaining task performance comparable to conventional fine-tuning. Our\nin-depth analysis explores how B-cos LMs differ from conventionally fine-tuned\nmodels in their learning processes and explanation patterns. Finally, we are\nalso the first to explore the transformation of decoder-only models to B-cos\nLMs for generation tasks.", "AI": {"tldr": "This paper presents B-cos language models (LMs) that enhance explainability in NLP tasks by transforming existing pre-trained models into B-cos structures, improving explanation fidelity and interpretability while maintaining task performance.", "motivation": "To improve explainability and human interpretability in language models, which often struggle with post-hoc explanation methods.", "method": "The authors introduce B-cos language models by applying B-cos conversion to pre-trained models and fine-tuning them for specific tasks, leading to enhanced explanation quality.", "result": "B-cos LMs yielded more faithful and interpretable explanations than traditional post-hoc methods, with task performance on par with conventional fine-tuning approaches.", "conclusion": "B-cos LMs represent a promising step towards improving model interpretability in NLP, particularly for generating explanations and enhancing user interactions.", "key_contributions": ["Introduction of B-cos LMs for natural language processing tasks", "Demonstration of improved explanation quality over traditional post-hoc methods", "Exploration of transforming decoder-only models into B-cos LMs for generation tasks."], "limitations": "", "keywords": ["B-cos networks", "explainability", "language models", "NLP", "human interpretability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.13640", "pdf": "https://arxiv.org/pdf/2502.13640.pdf", "abs": "https://arxiv.org/abs/2502.13640", "title": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts", "authors": ["Maiya Goloburda", "Nurkhan Laiyk", "Diana Turmakhan", "Yuxia Wang", "Mukhammed Togmanov", "Jonibek Mansurov", "Askhat Sametov", "Nurdaulet Mukhituly", "Minghan Wang", "Daniil Orel", "Zain Muhammad Mujahid", "Fajri Koto", "Timothy Baldwin", "Preslav Nakov"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to have the potential to generate\nharmful content, posing risks to users. While significant progress has been\nmade in developing taxonomies for LLM risks and safety evaluation prompts, most\nstudies have focused on monolingual contexts, primarily in English. However,\nlanguage- and region-specific risks in bilingual contexts are often overlooked,\nand core findings can diverge from those in monolingual settings. In this\npaper, we introduce Qorgau, a novel dataset specifically designed for safety\nevaluation in Kazakh and Russian, reflecting the unique bilingual context in\nKazakhstan, where both Kazakh (a low-resource language) and Russian (a\nhigh-resource language) are spoken. Experiments with both multilingual and\nlanguage-specific LLMs reveal notable differences in safety performance,\nemphasizing the need for tailored, region-specific datasets to ensure the\nresponsible and safe deployment of LLMs in countries like Kazakhstan. Warning:\nthis paper contains example data that may be offensive, harmful, or biased.", "AI": {"tldr": "This paper introduces Qorgau, a dataset for safety evaluation of LLMs in Kazakh and Russian, addressing bilingual risks overlooked in existing research.", "motivation": "To address the lack of bilingual context in safety evaluations of large language models, particularly in regions where multiple languages are spoken.", "method": "Development and experimentation with the Qorgau dataset, focusing on safety evaluation of language-specific and multilingual LLMs.", "result": "Findings indicate significant differences in LLM safety performance between Kazakh and Russian, highlighting the need for tailored datasets.", "conclusion": "The study advocates for the creation of region-specific datasets to ensure responsible use of LLMs in multilingual settings.", "key_contributions": ["Introduction of the Qorgau dataset for bilingual safety evaluation", "Demonstration of significant differences in LLM safety performance across languages", "Highlighting the necessity for tailored datasets in multilingual contexts."], "limitations": "The dataset may contain offensive or harmful example data.", "keywords": ["large language models", "safety evaluation", "bilingual context", "Kazakh", "Russian"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.18448", "pdf": "https://arxiv.org/pdf/2502.18448.pdf", "abs": "https://arxiv.org/abs/2502.18448", "title": "Disambiguate First, Parse Later: Generating Interpretations for Ambiguity Resolution in Semantic Parsing", "authors": ["Irina Saparina", "Mirella Lapata"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of ACL 2025", "summary": "Handling ambiguity and underspecification is an important challenge in\nnatural language interfaces, particularly for tasks like text-to-SQL semantic\nparsing. We propose a modular approach that resolves ambiguity using natural\nlanguage interpretations before mapping these to logical forms (e.g., SQL\nqueries). Although LLMs excel at parsing unambiguous utterances, they show\nstrong biases for ambiguous ones, typically predicting only preferred\ninterpretations. We constructively exploit this bias to generate an initial set\nof preferred disambiguations and then apply a specialized infilling model to\nidentify and generate missing interpretations. To train the infilling model, we\nintroduce an annotation method that uses SQL execution to validate different\nmeanings. Our approach improves interpretation coverage and generalizes across\ndatasets with different annotation styles, database structures, and ambiguity\ntypes.", "AI": {"tldr": "Proposes a modular approach for resolving ambiguity in natural language interfaces, particularly in text-to-SQL parsing, improving interpretation coverage.", "motivation": "Address the challenge of ambiguity and underspecification in natural language tasks, especially for text-to-SQL semantic parsing.", "method": "A modular approach that resolves ambiguity using natural language interpretations, followed by a mapping to logical forms. It involves generating preferred disambiguations and using a specialized infilling model for missing interpretations with an annotation method validated through SQL execution.", "result": "The approach enhances interpretation coverage and generalizes effectively across various datasets with different annotation styles, database structures, and types of ambiguity.", "conclusion": "This modular method provides a more robust framework for handling ambiguity in natural language interfaces, leading to improved performance in semantic parsing tasks.", "key_contributions": ["Modular ambiguity resolution for natural language interfaces", "Annotation method validated through SQL execution", "Improved generalization across diverse datasets"], "limitations": "", "keywords": ["Natural Language Processing", "Semantic Parsing", "Text-to-SQL"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.18746", "pdf": "https://arxiv.org/pdf/2502.18746.pdf", "abs": "https://arxiv.org/abs/2502.18746", "title": "A Survey of Automatic Prompt Optimization with Instruction-focused Heuristic-based Search Algorithm", "authors": ["Wendi Cui", "Zhuohang Li", "Hao Sun", "Damien Lopez", "Kamalika Das", "Bradley A. Malin", "Sricharan Kumar", "Jiaxin Zhang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Recent advances in Large Language Models have led to remarkable achievements\nacross a variety of Natural Language Processing tasks, making prompt\nengineering increasingly central to guiding model outputs. While manual methods\ncan be effective, they typically rely on intuition and do not automatically\nrefine prompts over time. In contrast, automatic prompt optimization employing\nheuristic-based search algorithms can systematically explore and improve\nprompts with minimal human oversight. This survey proposes a comprehensive\ntaxonomy of these methods, categorizing them by where optimization occurs, what\nis optimized, what criteria drive the optimization, which operators generate\nnew prompts, and which iterative search algorithms are applied. We further\nhighlight specialized datasets and tools that support and accelerate automated\nprompt refinement. We conclude by discussing key open challenges pointing\ntoward future opportunities for more robust and versatile LLM applications.", "AI": {"tldr": "This survey reviews automatic prompt optimization methods for Large Language Models, categorizing them and highlighting key datasets and tools.", "motivation": "The paper addresses the need for systematic prompt optimization methods to enhance the effectiveness of Large Language Models beyond manual intuition-based techniques.", "method": "The authors propose a comprehensive taxonomy of automatic prompt optimization methods, categorizing them based on optimization locations, optimization targets, driving criteria, prompt generation operators, and search algorithms employed.", "result": "The survey identifies various techniques and tools that facilitate automated prompt refinement, enhancing model performance with minimal human intervention.", "conclusion": "The paper concludes by outlining significant open challenges and future opportunities for advancing LLM applications through improved prompt optimization techniques.", "key_contributions": ["Comprehensive taxonomy of automatic prompt optimization methods", "Identification of specialized datasets and tools for automated prompt refinement", "Discussion of future challenges and opportunities in LLM applications"], "limitations": "", "keywords": ["Large Language Models", "Prompt Optimization", "Natural Language Processing", "Heuristic Algorithms", "Survey"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.02240", "pdf": "https://arxiv.org/pdf/2503.02240.pdf", "abs": "https://arxiv.org/abs/2503.02240", "title": "OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale", "authors": ["Haoyang Li", "Shang Wu", "Xiaokang Zhang", "Xinmei Huang", "Jing Zhang", "Fuxin Jiang", "Shuai Wang", "Tieying Zhang", "Jianjun Chen", "Rui Shi", "Hong Chen", "Cuiping Li"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Text-to-SQL, the task of translating natural language questions into SQL\nqueries, plays a crucial role in enabling non-experts to interact with\ndatabases. While recent advancements in large language models (LLMs) have\nsignificantly enhanced text-to-SQL performance, existing approaches face\nnotable limitations in real-world text-to-SQL applications. Prompting-based\nmethods often depend on closed-source LLMs, which are expensive, raise privacy\nconcerns, and lack customization. Fine-tuning-based methods, on the other hand,\nsuffer from poor generalizability due to the limited coverage of publicly\navailable training data. To overcome these challenges, we propose a novel and\nscalable text-to-SQL data synthesis framework for automatically synthesizing\nlarge-scale, high-quality, and diverse datasets without extensive human\nintervention. Using this framework, we introduce SynSQL-2.5M, the first\nmillion-scale text-to-SQL dataset, containing 2.5 million samples spanning over\n16,000 synthetic databases. Each sample includes a database, SQL query, natural\nlanguage question, and chain-of-thought (CoT) solution. Leveraging SynSQL-2.5M,\nwe develop OmniSQL, a powerful open-source text-to-SQL model available in three\nsizes: 7B, 14B, and 32B. Extensive evaluations across nine datasets demonstrate\nthat OmniSQL achieves state-of-the-art performance, matching or surpassing\nleading closed-source and open-source LLMs, including GPT-4o and DeepSeek-V3,\ndespite its smaller size. We release all code, datasets, and models to support\nfurther research.", "AI": {"tldr": "This paper introduces a scalable framework for synthesizing large-scale text-to-SQL datasets and presents OmniSQL, a state-of-the-art text-to-SQL model.", "motivation": "To address the limitations of existing text-to-SQL approaches, particularly regarding dependency on closed-source LLMs and poor generalizability of fine-tuning methods.", "method": "A novel data synthesis framework for automatically generating a large-scale, diverse text-to-SQL dataset, resulting in the SynSQL-2.5M dataset with 2.5 million samples from synthetic databases.", "result": "OmniSQL outperforms leading closed-source and open-source LLMs in text-to-SQL performance on multiple benchmarks, despite being smaller in size.", "conclusion": "The proposed framework and model pave the way for further research and development in the field of text-to-SQL applications.", "key_contributions": ["Introduction of a scalable text-to-SQL data synthesis framework", "Creation of SynSQL-2.5M, a million-scale text-to-SQL dataset", "Development of OmniSQL, an open-source model with state-of-the-art performance"], "limitations": "", "keywords": ["text-to-SQL", "large language models", "data synthesis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.22362", "pdf": "https://arxiv.org/pdf/2503.22362.pdf", "abs": "https://arxiv.org/abs/2503.22362", "title": "Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs", "authors": ["Yuan He", "Bailan He", "Zifeng Ding", "Alisia Lupidi", "Yuqicheng Zhu", "Shuo Chen", "Caiqi Zhang", "Jiaoyan Chen", "Yunpu Ma", "Volker Tresp", "Ian Horrocks"], "categories": ["cs.CL"], "comment": "Accepted at COLM 2025", "summary": "Understanding and mitigating hallucinations in Large Language Models (LLMs)\nis crucial for ensuring reliable content generation. While previous research\nhas primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and\ndirectly links model behaviour to the pre-training data that forms their prior\nknowledge. Specifically, we demonstrate that an asymmetry exists in the\nrecognition of logically equivalent facts, which can be attributed to frequency\ndiscrepancies of entities appearing as subjects versus objects. Given that most\npre-training datasets are inaccessible, we leverage the fully open-source OLMo\nseries by indexing its Dolma dataset to estimate entity frequencies. Using\nrelational facts (represented as triples) from Wikidata5M, we construct probing\ndatasets to isolate this effect. Our experiments reveal that facts with a\nhigh-frequency subject and a low-frequency object are better recognised than\ntheir inverse, despite their logical equivalence. The pattern reverses in\nlow-to-high frequency settings, and no statistically significant asymmetry\nemerges when both entities are high-frequency. These findings highlight the\ninfluential role of pre-training data in shaping model predictions and provide\ninsights for inferring the characteristics of pre-training data in closed or\npartially closed LLMs.", "AI": {"tldr": "This paper investigates hallucinations in Large Language Models (LLMs), linking model behavior to pre-training data and revealing how entity frequency affects recognition of logically equivalent facts.", "motivation": "Understanding the causes of hallucinations in LLMs is essential for improving reliable content generation.", "method": "The study utilizes the open-source OLMo series and Dolma dataset to estimate entity frequencies, constructing probing datasets from relational facts in Wikidata5M to examine the effect of subject and object frequencies on model performance.", "result": "Experiments show that facts with a high-frequency subject and a low-frequency object are better recognized than their inverse, and patterns change based on frequency settings.", "conclusion": "Findings underline the impact of pre-training data composition on LLM predictions, with potential implications for developing more reliable models.", "key_contributions": ["Linking LLM hallucinations to pre-training data characteristics", "Demonstrating the asymmetry in recognition of logical equivalence based on entity frequency", "Providing insights for inferring pre-training dataset characteristics in LLMs."], "limitations": "Limited to analysis of entity frequencies and their effects on recognition, does not address all aspects of hallucinations.", "keywords": ["Large Language Models", "hallucinations", "pre-training data", "entity frequency", "Wikidata"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.02882", "pdf": "https://arxiv.org/pdf/2504.02882.pdf", "abs": "https://arxiv.org/abs/2504.02882", "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models", "authors": ["Sunghee Jung", "Donghun Lee", "Shinbok Lee", "Gaeun Seo", "Daniel Lee", "Byeongil Ko", "Junrae Cho", "Kihyun Kim", "Eunggyun Kim", "Myeongcheol Shin"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to SIGDIAL 2025", "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.", "AI": {"tldr": "The paper presents DiaTool-DPO, a novel method for enhancing Tool-Augmented Large Language Models (TA-LLMs) dialogue capabilities through Direct Preference Optimization, addressing challenges like incomplete queries.", "motivation": "To improve the dialogue capabilities of TA-LLMs in real-world applications, especially in handling incomplete queries and out-of-scope requests without needing additional expert demonstrations.", "method": "The authors model TA-LLM interactions as a Markov Decision Process with 5 dialogue states and categorize user queries based on state transition trajectories. They create paired trajectory datasets of correct and incorrect dialogue flows and employ a specialized objective loss for dialogue control.", "result": "DiaTool-DPO approaches GPT-4o's performance with 94.8% in information gathering and 91% in tool call rejection, significantly outperforming baseline results of 44% and 9.6% respectively, while maintaining core functionality.", "conclusion": "The proposed DiaTool-DPO method enhances TA-LLMs’ performance in diverse scenarios, paving the way for development without requiring expert labeling.", "key_contributions": ["Introduction of DiaTool-DPO for TA-LLMs", "Modeling dialogue as a Markov Decision Process", "A specialized objective loss for dialogue control"], "limitations": "", "keywords": ["Tool-Augmented Large Language Models", "Direct Preference Optimization", "Dialogue Systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.11183", "pdf": "https://arxiv.org/pdf/2504.11183.pdf", "abs": "https://arxiv.org/abs/2504.11183", "title": "Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting", "authors": ["Ej Zhou", "Weiming Lu"], "categories": ["cs.CL"], "comment": null, "summary": "Social bias in language models can potentially exacerbate social\ninequalities. Despite it having garnered wide attention, most research focuses\non English data. In a low-resource scenario, the models often perform worse due\nto insufficient training data. This study aims to leverage high-resource\nlanguage corpora to evaluate bias and experiment with debiasing methods in\nlow-resource languages. We evaluated the performance of recent multilingual\nmodels in five languages: English, Chinese, Russian, Indonesian and Thai, and\nanalyzed four bias dimensions: gender, religion, nationality, and race-color.\nBy constructing multilingual bias evaluation datasets, this study allows fair\ncomparisons between models across languages. We have further investigated three\ndebiasing methods-CDA, Dropout, SenDeb-and demonstrated that debiasing methods\nfrom high-resource languages can be effectively transferred to low-resource\nones, providing actionable insights for fairness research in multilingual NLP.", "AI": {"tldr": "This study evaluates bias in multilingual models, particularly in low-resource languages, and explores the effectiveness of debiasing methods developed for high-resource languages.", "motivation": "To address social bias in language models and its potential to exacerbate social inequalities, especially in low-resource languages.", "method": "Evaluated recent multilingual models in five languages across four bias dimensions and constructed multilingual bias evaluation datasets for fair comparisons. Investigated three debiasing methods from high-resource languages.", "result": "Demonstrated that debiasing methods can be effectively transferred from high-resource to low-resource languages, providing insights for fairness in multilingual NLP.", "conclusion": "The findings provide actionable insights for implementing debiasing strategies in low-resource languages, helping to improve fairness in NLP applications.", "key_contributions": ["Development of multilingual bias evaluation datasets", "Evaluation of multilingual models across diverse languages", "Transferability of debiasing methods from high-resource to low-resource languages"], "limitations": "The study primarily focuses on just five languages and may not account for all cultural nuances in bias evaluation.", "keywords": ["social bias", "language models", "multilingual NLP", "debiasing methods", "low-resource languages"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.12355", "pdf": "https://arxiv.org/pdf/2504.12355.pdf", "abs": "https://arxiv.org/abs/2504.12355", "title": "Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media", "authors": ["Muhammad Ahmad", "Fida Ullah", "Ummhy Habiba", "ldar Batyrshin", "Grigori Sidorov"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies.", "AI": {"tldr": "The study presents an AI-driven NLP framework for detecting drug use and overdose symptoms through social media analysis.", "motivation": "To address the limitations of traditional research methods in understanding drug overdose issues, utilizing real-time data from social media.", "method": "An AI-driven NLP framework was developed using a hybrid annotation strategy with LLMs and human annotators, applying traditional ML models, neural networks, and transformer-based models.", "result": "The framework achieved 98% accuracy in multi-class and 97% in multi-label classification, outperforming baseline models by up to 8%.", "conclusion": "The findings indicate that AI can enhance public health surveillance and inform personalized intervention strategies for drug overdose.", "key_contributions": ["Development of an AI-driven NLP framework for drug detection", "Hybrid annotation strategy combining LLMs and human effort", "High accuracy in classification of substance use from social media data"], "limitations": "", "keywords": ["AI", "NLP", "drug overdose", "social media", "health informatics"], "importance_score": 9, "read_time_minutes": 5}}
