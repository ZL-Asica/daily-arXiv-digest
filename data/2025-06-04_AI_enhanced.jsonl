{"id": "2506.01982", "pdf": "https://arxiv.org/pdf/2506.01982.pdf", "abs": "https://arxiv.org/abs/2506.01982", "title": "Music interpretation and emotion perception: A computational and neurophysiological investigation", "authors": ["Vassilis Lyberatos", "Spyridon Kantarelis", "Ioanna Zioga", "Christina Anagnostopoulou", "Giorgos Stamou", "Anastasia Georgaki"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This study investigates emotional expression and perception in music\nperformance using computational and neurophysiological methods. The influence\nof different performance settings, such as repertoire, diatonic modal etudes,\nand improvisation, as well as levels of expressiveness, on performers'\nemotional communication and listeners' reactions is explored. Professional\nmusicians performed various tasks, and emotional annotations were provided by\nboth performers and the audience. Audio analysis revealed that expressive and\nimprovisational performances exhibited unique acoustic features, while emotion\nanalysis showed stronger emotional responses. Neurophysiological measurements\nindicated greater relaxation in improvisational performances. This multimodal\nstudy highlights the significance of expressivity in enhancing emotional\ncommunication and audience engagement.", "AI": {"tldr": "The study explores emotional expression in music performance through computational and neurophysiological methods, revealing the impact of performance settings and expressiveness on emotional communication and audience reaction.", "motivation": "To investigate how different performance settings and levels of expressiveness in music influence emotional expression and listener perception.", "method": "The study employed both computational audio analysis and neurophysiological measurements to assess emotional communication in music performances, using different repertoires and expressiveness levels.", "result": "Expressive and improvisational performances exhibited unique acoustic features, leading to stronger emotional responses from listeners, while neurophysiological data indicated greater relaxation in improvisational settings.", "conclusion": "Expressivity in music performance significantly enhances emotional communication and audience engagement, emphasizing its importance in musical interpretation.", "key_contributions": ["Introduced a multimodal approach to study emotional expression in music performance.", "Demonstrated the unique acoustic features of expressive and improvisational music performances.", "Provided insights into the neurophysiological responses elicited by varying levels of expressiveness in music."], "limitations": "", "keywords": ["emotional expression", "music performance", "audience engagement", "improvisation", "neurophysiological measurements"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2506.01998", "pdf": "https://arxiv.org/pdf/2506.01998.pdf", "abs": "https://arxiv.org/abs/2506.01998", "title": "Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents", "authors": ["Takao Fujii", "Katie Seaborn", "Madeleine Steeds", "Jun Kato"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI '25", "summary": "Conversational agents that mimic people have raised questions about the\nethics of anthropomorphizing machines with human social identity cues. Critics\nhave also questioned assumptions of identity neutrality in humanlike agents.\nRecent work has revealed that intersectional Japanese pronouns can elicit\ncomplex and sometimes evasive impressions of agent identity. Yet, the role of\nother \"neutral\" non-pronominal self-referents (NPSR) and voice as a socially\nexpressive medium remains unexplored. In a crowdsourcing study, Japanese\nparticipants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and\nEmber) using seven self-referents. We found strong evidence of voice gendering\nalongside the potential of intersectional self-referents to evade gendering,\ni.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age\nand formality intersected with gendering as per sociolinguistic theories,\nespecially boku and watakushi. This work provides a nuanced take on agent\nidentity perceptions and champions intersectional and culturally-sensitive work\non voice agents.", "AI": {"tldr": "This study investigates the impact of voice and non-pronominal self-referents in shaping perceptions of digital agents’ identities among Japanese users.", "motivation": "To explore the ethics of anthropomorphizing machines and the implications of using culturally specific self-referents in conversational agents.", "method": "A crowdsourcing study involving 204 Japanese participants evaluated three ChatGPT voices with seven different self-referents to analyze perceptions of gender, age, and formality.", "result": "The study found strong evidence of voice gendering and showed how intersectional self-referents can create ambiguity, evading conventional gendering assumptions.", "conclusion": "The findings highlight the need for culturally-sensitive design in voice agents, emphasizing the intricate relationship between self-referents and user perceptions of identity.", "key_contributions": ["Identifies the role of non-pronominal self-referents in shaping identity perceptions of conversational agents.", "Reveals the intersectionality of gender, age, and formality in the context of voice agents.", "Calls for culturally-sensitive approaches in the design of human-like agents."], "limitations": "Limited to Japanese participants; findings may not generalize across other cultures or languages.", "keywords": ["Conversational agents", "Anthropomorphism", "Voice identity", "Intersectionality", "Cultural sensitivity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.02262", "pdf": "https://arxiv.org/pdf/2506.02262.pdf", "abs": "https://arxiv.org/abs/2506.02262", "title": "Composable Building Blocks for Controllable and Transparent Interactive AI Systems", "authors": ["Sebe Vanbrabant", "Gustavo Rovelo Ruiz", "Davy Vanacken"], "categories": ["cs.HC", "cs.AI", "H.5.2; I.2.0"], "comment": "Accepted to The 3rd Workshop on Engineering Interactive Systems\n  Embedding AI Technologies, EICS 2025", "summary": "While the increased integration of AI technologies into interactive systems\nenables them to solve an equally increasing number of tasks, the black box\nproblem of AI models continues to spread throughout the interactive system as a\nwhole. Explainable AI (XAI) techniques can make AI models more accessible by\nemploying post-hoc methods or transitioning to inherently interpretable models.\nWhile this makes individual AI models clearer, the overarching system\narchitecture remains opaque. To this end, we propose an approach to represent\ninteractive systems as sequences of structural building blocks, such as AI\nmodels and control mechanisms grounded in the literature. These can then be\nexplained through accompanying visual building blocks, such as XAI techniques.\nThe flow and APIs of the structural building blocks form an explicit overview\nof the system. This serves as a communication basis for both humans and\nautomated agents like LLMs, aligning human and machine interpretability of AI\nmodels. We discuss a selection of building blocks and concretize our flow-based\napproach in an architecture and accompanying prototype interactive system.", "AI": {"tldr": "This paper proposes a structured approach to represent interactive systems using building blocks that improve the interpretability of AI models and their architectures, addressing the black box problem in AI systems.", "motivation": "The integration of AI technologies in interactive systems has led to increased complexity, where the black box nature of AI models hampers understanding. Explainable AI (XAI) techniques can clarify individual models, but the overall system architecture remains opaque.", "method": "The authors propose a method to represent interactive systems as sequences of structural building blocks which include AI models and control mechanisms. These blocks can be explained through visual aids and XAI techniques, forming a comprehensive overview of the system's structure and flow.", "result": "The proposed approach facilitates alignment between human and machine interpretability, allowing for clearer communication regarding the system's architecture and functioning. A prototype interactive system was developed to demonstrate the approach.", "conclusion": "By utilizing a flow-based architecture and structural building blocks, the proposed method enhances the understanding of interactive systems that employ AI technologies, making them more accessible to users.", "key_contributions": ["Development of a flow-based architecture for interactive systems using structural building blocks.", "Enhancement of interpretability for both individual AI models and the overall system architecture through visual explanations.", "Creation of a prototype interactive system to showcase the proposed approach."], "limitations": "The approach may require further validation in diverse scenarios and with various types of interactive systems to assess its generalizability and effectiveness.", "keywords": ["Explainable AI", "Interactive Systems", "Interpretability", "Structural Building Blocks", "Human-Machine Communication"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.02447", "pdf": "https://arxiv.org/pdf/2506.02447.pdf", "abs": "https://arxiv.org/abs/2506.02447", "title": "Visualization for interactively adjusting the de-bias effect of word embedding", "authors": ["Arisa Sugino", "Takayuki Itoh"], "categories": ["cs.HC"], "comment": null, "summary": "Word embedding, which converts words into numerical values, is an important\nnatural language processing technique and widely used. One of the serious\nproblems of word embedding is that the bias will be learned and affect the\nmodel if the dataset used for pre-training contains bias. On the other hand,\nindiscriminate removal of bias from word embeddings may result in the loss of\ninformation, even if the bias is undesirable to us. As a result, a risk of\nmodel performance degradation due to bias removal will be another problem. As a\nsolution to this problem, we focus on gender bias in Japanese and propose an\ninteractive visualization method to adjust the degree of debias for each word\ncategory. Specifically, we visualize the accuracy in a category classification\ntask after debiasing, and allow the user to adjust the parameters based on the\nvisualization results, so that the debiasing can be adjusted according to the\nuser's objectives. In addition, considering a trade-off between debiasing and\npreventing degradation of model performance, and that different people perceive\ngender bias differently, we developed a mechanism to present multiple choices\nof debiasing configurations applying an optimization scheme. This paper\npresents the results of an experiment in which we removed the gender bias for\nword embeddings learned from the Japanese version of Wikipedia. We classified\nwords into five categories based on a news corpus, and observed that the degree\nof influence of debiasing differed greatly among the categories. We then\nadjusted the degree of debiasing for each category based on the visualization\nresults.", "AI": {"tldr": "This paper addresses gender bias in Japanese word embeddings by proposing an interactive visualization method to adjust debiasing levels per word category while monitoring model performance.", "motivation": "To mitigate the impact of learned biases in word embeddings, particularly gender bias, without degrading model performance.", "method": "An interactive visualization tool allows users to adjust debiasing parameters based on the accuracy of a category classification task, alongside presenting multiple debiasing configurations using an optimization scheme.", "result": "The study shows significant variation in debiasing influence across different categories of words, emphasizing the need for tailored debiasing approaches.", "conclusion": "The proposed method allows for user-specific adjustments in debiasing processes, balancing bias removal and performance retention in word embeddings.", "key_contributions": ["Interactive visualization for debiasing word embeddings", "User-driven adjustment of debiasing parameters", "Optimization scheme for presenting debiasing options"], "limitations": "The focus is solely on gender bias in Japanese, which may not generalize to other types of biases or languages.", "keywords": ["word embeddings", "gender bias", "debiasing", "interactive visualization", "natural language processing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.01961", "pdf": "https://arxiv.org/pdf/2506.01961.pdf", "abs": "https://arxiv.org/abs/2506.01961", "title": "Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System", "authors": ["Jinzhu Yang"], "categories": ["cs.CL"], "comment": null, "summary": "This study is dedicated to exploring the application of prompt learning\nmethods to advance Named Entity Recognition (NER) within the medical domain. In\nrecent years, the emergence of large-scale models has driven significant\nprogress in NER tasks, particularly with the introduction of the BioBERT\nlanguage model, which has greatly enhanced NER capabilities in medical texts.\nOur research introduces the Prompt-bioMRC model, which integrates both hard\ntemplate and soft prompt designs aimed at refining the precision and efficiency\nof medical entity recognition. Through extensive experimentation across diverse\nmedical datasets, our findings consistently demonstrate that our approach\nsurpasses traditional models. This enhancement not only validates the efficacy\nof our methodology but also highlights its potential to provide reliable\ntechnological support for applications like intelligent diagnosis systems. By\nleveraging advanced NER techniques, this study contributes to advancing\nautomated medical data processing, facilitating more accurate medical\ninformation extraction, and supporting efficient healthcare decision-making\nprocesses.", "AI": {"tldr": "This study presents the Prompt-bioMRC model for improving Named Entity Recognition in the medical domain using prompt learning methods.", "motivation": "To enhance Named Entity Recognition (NER) within medical texts by leveraging prompt learning techniques, especially in light of advancements in large-scale models like BioBERT.", "method": "The study introduces the Prompt-bioMRC model, which employs both hard template and soft prompt designs, and conducts extensive experiments on various medical datasets.", "result": "The findings show that the Prompt-bioMRC model consistently outperforms traditional NER models across multiple medical datasets.", "conclusion": "The study validates the effectiveness of the Prompt-bioMRC model and emphasizes its potential to support intelligent diagnosis systems and improve healthcare decision-making through better automated medical data processing.", "key_contributions": ["Introduction of the Prompt-bioMRC model combining hard and soft prompts for NER", "Demonstrated superior performance over traditional NER models in medical contexts", "Potential applications in intelligent diagnosis systems and healthcare decision support."], "limitations": "", "keywords": ["Named Entity Recognition", "Prompt Learning", "Medical Informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.02514", "pdf": "https://arxiv.org/pdf/2506.02514.pdf", "abs": "https://arxiv.org/abs/2506.02514", "title": "To Embody or Not: The Effect Of Embodiment On User Perception Of LLM-based Conversational Agents", "authors": ["Kyra Wang", "Boon-Kiat Quek", "Jessica Goh", "Dorien Herremans"], "categories": ["cs.HC"], "comment": null, "summary": "Embodiment in conversational agents (CAs) refers to the physical or visual\nrepresentation of these agents, which can significantly influence user\nperception and interaction. Limited work has been done examining the effect of\nembodiment on the perception of CAs utilizing modern large language models\n(LLMs) in non-hierarchical cooperative tasks, a common use case of CAs as more\npowerful models become widely available for general use. To bridge this\nresearch gap, we conducted a mixed-methods within-subjects study on how users\nperceive LLM-based CAs in cooperative tasks when embodied and non-embodied. The\nresults show that the non-embodied agent received significantly better\nquantitative appraisals for competence than the embodied agent, and in\nqualitative feedback, many participants believed that the embodied CA was more\nsycophantic than the non-embodied CA. Building on prior work on users'\nperceptions of LLM sycophancy and anthropomorphic features, we theorize that\nthe typically-positive impact of embodiment on perception of CA credibility can\nbecome detrimental in the presence of sycophancy. The implication of such a\nphenomenon is that, contrary to intuition and existing literature, embodiment\nis not a straightforward way to improve a CA's perceived credibility if there\nexists a tendency to sycophancy.", "AI": {"tldr": "This paper investigates how the embodiment of conversational agents (CAs) using large language models affects user perception during cooperative tasks.", "motivation": "To fill the research gap regarding the impact of embodiment on user perception of LLM-based conversational agents in cooperative contexts.", "method": "A mixed-methods within-subjects study was conducted to compare user perceptions of embodied versus non-embodied CAs during cooperative tasks.", "result": "The non-embodied agent was rated significantly more competent than the embodied one; qualitative feedback indicated that the embodied agent was perceived as more sycophantic.", "conclusion": "Embodiment may negatively impact the perceived credibility of conversational agents if they are perceived to be sycophantic, challenging existing beliefs about the positive effects of embodiment.", "key_contributions": ["Investigation of embodiment effects on LLM-based conversational agents", "Findings suggesting embodiment can hinder perceived credibility", "Qualitative insights into user perceptions of sycophancy in embodied CAs"], "limitations": "The study is limited to specific tasks and settings, and may not generalize to all contexts of CA use.", "keywords": ["Conversational Agents", "Embodiment", "Large Language Models", "User Perception", "Sycophancy"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.01992", "pdf": "https://arxiv.org/pdf/2506.01992.pdf", "abs": "https://arxiv.org/abs/2506.01992", "title": "No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success", "authors": ["Lukas Rauch", "Moritz Wirth", "Denis Huseljic", "Marek Herde", "Bernhard Sick", "Matthias Aßenmacher"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "under review @NeurIPS2025", "summary": "The advent of large language models (LLMs) capable of producing\ngeneral-purpose representations lets us revisit the practicality of deep active\nlearning (AL): By leveraging frozen LLM embeddings, we can mitigate the\ncomputational costs of iteratively fine-tuning large backbones. This study\nestablishes a benchmark and systematically investigates the influence of LLM\nembedding quality on query strategies in deep AL. We employ five top-performing\nmodels from the massive text embedding benchmark (MTEB) leaderboard and two\nbaselines for ten diverse text classification tasks. Our findings reveal key\ninsights: First, initializing the labeled pool using diversity-based sampling\nsynergizes with high-quality embeddings, boosting performance in early AL\niterations. Second, the choice of the optimal query strategy is sensitive to\nembedding quality. While the computationally inexpensive Margin sampling can\nachieve performance spikes on specific datasets, we find that strategies like\nBadge exhibit greater robustness across tasks. Importantly, their effectiveness\nis often enhanced when paired with higher-quality embeddings. Our results\nemphasize the need for context-specific evaluation of AL strategies, as\nperformance heavily depends on embedding quality and the target task.", "AI": {"tldr": "This paper investigates the impact of large language model (LLM) embeddings on deep active learning strategies for text classification.", "motivation": "To explore the practicality of deep active learning (AL) using frozen LLM embeddings and evaluate their influence on query strategies.", "method": "Five leading models from the MTEB leaderboard were tested against two baselines across ten text classification tasks to assess the effectiveness of various query strategies in active learning.", "result": "The study found that diversity-based sampling for initializing labeled pools significantly boosts performance, and the effectiveness of query strategies depends heavily on the quality of embeddings used.", "conclusion": "A context-specific evaluation of AL strategies is necessary, as performance varies based on embedding quality and the task.", "key_contributions": ["Established a benchmark for LLM embedding quality in active learning.", "Demonstrated the importance of synergy between sampling methods and embedding quality.", "Provided insights into the sensitivity of query strategies to embedding quality."], "limitations": "The study is limited to ten text classification tasks and may not generalize to other domains.", "keywords": ["active learning", "large language models", "text classification", "embeddings", "machine learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.02700", "pdf": "https://arxiv.org/pdf/2506.02700.pdf", "abs": "https://arxiv.org/abs/2506.02700", "title": "Cognitive Load-Driven VR Memory Palaces: Personalizing Focus and Recall Enhancement", "authors": ["Zhengyang Li", "Hailin Deng"], "categories": ["cs.HC"], "comment": "10 pages, 5 figures, submitted to HCII 2025. Includes EEG-based VR\n  adaptation experiments, dynamic spatial modeling, and empirical evaluation of\n  memory performance", "summary": "Cognitive load, which varies across individuals, can significantly affect\nfocus and memory performance.This study explores the integration of Virtual\nReality (VR) with memory palace techniques, aiming to optimize VR environments\ntailored to individual cognitive load levels to improve focus and memory. We\nutilized EEG devices, specifically the Oculus Quest 2, to monitor Beta wave\nactivity in 10 participants.By modeling their cognitive load profiles through\npolynomial regression, we dynamically adjusted spatial variables within a VR\nenvironment using Grasshopper, creating personalized experiences. Results\nindicate that 8 participants showed a notable increase in Beta wave activity,\ndemonstrating improved focus and cognitive performance in the customized VR\nsettings.These findings underscore the potential of VR-based memory\nenvironments, driven by cognitive load considerations, and provide valuable\ninsights for advancing VR memory research", "AI": {"tldr": "This study investigates using Virtual Reality to enhance memory through personalized environments based on individual cognitive load profiles.", "motivation": "To optimize Virtual Reality environments tailored to individual cognitive load levels to improve focus and memory performance.", "method": "EEG devices were used to monitor participants' Beta wave activity, while polynomial regression modeled their cognitive load profiles, adjusting spatial variables in the VR environment dynamically.", "result": "8 out of 10 participants exhibited an increase in Beta wave activity, indicating improved focus and cognitive performance in customized VR settings.", "conclusion": "The findings highlight the potential of VR-based memory environments that consider cognitive load, offering new insights for VR memory research.", "key_contributions": ["Integration of VR with memory palace techniques", "Dynamic adjustment of VR spatial variables based on cognitive load", "Empirical evidence of improved cognitive performance using personalized VR settings"], "limitations": "", "keywords": ["Cognitive Load", "Virtual Reality", "Memory Performance", "EEG", "Personalized Experience"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.02000", "pdf": "https://arxiv.org/pdf/2506.02000.pdf", "abs": "https://arxiv.org/abs/2506.02000", "title": "NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts", "authors": ["Abhay Gupta", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) struggle to answer questions that span\ntens of thousands of tokens, especially when multi-hop reasoning is involved.\nWhile prior benchmarks explore long-context comprehension or multi-hop\nreasoning in isolation, none jointly vary context length and reasoning depth in\nnatural narrative settings. We introduce NovelHopQA, the first benchmark to\nevaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length\npublic-domain novels. A keyword-guided pipeline builds hop-separated chains\ngrounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models\nand apply oracle-context filtering to ensure all questions are genuinely\nanswerable. Human annotators validate both alignment and hop depth. We noticed\nconsistent accuracy drops with increased hops and context length, even in\nfrontier models-revealing that sheer scale does not guarantee robust reasoning.\nOur failure mode analysis highlights common breakdowns, such as missed\nfinal-hop integration and long-range drift. NovelHopQA offers a controlled\ndiagnostic setting to stress-test multi-hop reasoning at scale.", "AI": {"tldr": "NovelHopQA introduces a benchmark for evaluating multi-hop question answering in large language models over extensive text excerpts, revealing significant reasoning challenges even for state-of-the-art models.", "motivation": "To address the limitations of existing benchmarks in evaluating multi-hop reasoning in long-context environments, especially for large language models.", "method": "The paper presents NovelHopQA, a benchmark that combines context length and reasoning depth, using 64k-128k-token excerpts from 83 public-domain novels and a keyword-guided pipeline to create hop-separated chains.", "result": "Evaluation of six state-of-the-art models showed consistent accuracy drops with increased hop count and context length, revealing that larger models do not necessarily succeed in complex reasoning tasks.", "conclusion": "NovelHopQA serves as a valuable diagnostic tool to assess and enhance multi-hop reasoning capabilities in large language models, highlighting areas where these models struggle.", "key_contributions": ["Introduction of NovelHopQA benchmark for multi-hop QA over large-context narratives", "Controlled evaluation methodology with human annotation for alignment and hop depth", "Analysis of failure modes in current SOTA models regarding long-range reasoning"], "limitations": "The study focuses on specific narrative contexts and may not generalize across all types of texts or QA tasks.", "keywords": ["multi-hop reasoning", "large language models", "benchmark", "natural language processing", "health informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02714", "pdf": "https://arxiv.org/pdf/2506.02714.pdf", "abs": "https://arxiv.org/abs/2506.02714", "title": "Heatables: Effects of Infrared-LED-Induced Ear Heating on Thermal Perception, Comfort, and Cognitive Performance", "authors": ["Valeria Zitz", "Michael Küttner", "Jonas Hummel", "Michael T. Knierim", "Michael Beigl", "Tobias Röddiger"], "categories": ["cs.HC"], "comment": null, "summary": "Maintaining thermal comfort in shared indoor environments remains\nchallenging, as centralized HVAC systems are slow to adapt and standardized to\ngroup norms. Cold exposure not only reduces subjective comfort but can impair\ncognitive performance, particularly under moderate to severe cold stress.\nPersonal Comfort Systems (PCS) have shown promise by providing localized\nheating, yet many designs target distal body parts with low thermosensitivity\nand often lack portability. In this work, we investigate whether targeted\nthermal stimulation using in-ear worn devices can manipulate thermal perception\nand enhance thermal comfort. We present Heatables, a novel in-ear wearable that\nemits Near-Infrared (NIR) and Infrared (IR) radiation via integrated LEDs to\ndeliver localized optical heating. This approach leverages NIR-IR's ability to\npenetrate deeper tissues, offering advantages over traditional resistive\nheating limited to surface warming. In a placebo-controlled study with 24\nparticipants, each exposed for 150 minutes in a cool office environment\n(approximately 17.5 degrees Celsius) to simulate sustained cold stress during\ntypical sedentary office activities, Heatables significantly increased the\nperceived ambient temperature by around 1.5 degrees Celsius and delayed cold\ndiscomfort. Importantly, thermal benefits extended beyond the ear region,\nimproving both whole-body comfort and thermal acceptability. These findings\nposition in-ear NIR-IR-LED-based stimulation as a promising modality for\nunobtrusive thermal comfort enhancement in everyday contexts.", "AI": {"tldr": "This paper explores a novel in-ear wearable device, Heatables, that uses Near-Infrared (NIR) and Infrared (IR) radiation to enhance thermal comfort by providing localized heating.", "motivation": "Maintaining thermal comfort in shared indoor environments is challenging, especially with traditional HVAC systems, and cold exposure can impair cognitive performance.", "method": "The study used Heatables, an in-ear device emitting NIR and IR radiation, in a placebo-controlled experiment with 24 participants exposed to a cool office environment for 150 minutes.", "result": "Heatables significantly increased perceived ambient temperature by approximately 1.5 degrees Celsius and delayed cold discomfort; thermal benefits were observed beyond the ear region, enhancing whole-body comfort.", "conclusion": "In-ear NIR-IR-LED stimulation shows promise for enhancing thermal comfort in everyday situations unobtrusively.", "key_contributions": ["Introduction of the Heatables wearable device.", "Demonstration of effective localized heating for thermal comfort enhancement.", "Validation of benefits beyond the immediate area of application."], "limitations": "The sample size was limited to 24 participants, and longer-term effects were not assessed.", "keywords": ["thermal comfort", "wearable technology", "heat stimulation", "NIR-IR radiation", "user-centered design"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.02005", "pdf": "https://arxiv.org/pdf/2506.02005.pdf", "abs": "https://arxiv.org/abs/2506.02005", "title": "Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT", "authors": ["Timothy Do", "Pranav Saran", "Harshita Poojary", "Pranav Prabhu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL"], "comment": "9 pages, 7 figures", "summary": "In this paper, we address the persistent challenges that figurative language\nexpressions pose for natural language processing (NLP) systems, particularly in\nlow-resource languages such as Konkani. We present a hybrid model that\nintegrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM\nand a linear classifier. This architecture is fine-tuned on a newly introduced\nannotated dataset for metaphor classification, developed as part of this work.\nTo improve the model's efficiency, we implement a gradient-based attention head\npruning strategy. For metaphor classification, the pruned model achieves an\naccuracy of 78%. We also applied our pruning approach to expand on an existing\nidiom classification task, achieving 83% accuracy. These results demonstrate\nthe effectiveness of attention head pruning for building efficient NLP tools in\nunderrepresented languages.", "AI": {"tldr": "The paper presents a hybrid NLP model using mBERT and LSTM for metaphor classification in low-resource languages, achieving significant accuracy with a focus on attention head pruning.", "motivation": "To tackle challenges posed by figurative language in NLP systems, especially for low-resource languages like Konkani.", "method": "A hybrid model combining pre-trained Multilingual BERT with a bidirectional LSTM and a linear classifier, fine-tuned on a new annotated metaphor classification dataset. An attention head pruning strategy was implemented for efficiency.", "result": "The model achieved 78% accuracy in metaphor classification and 83% accuracy in idiom classification after pruning.", "conclusion": "Attention head pruning proved effective in enhancing the efficiency of NLP tools for underrepresented languages.", "key_contributions": ["Introduction of a new annotated dataset for metaphor classification in Konkani", "Development of a hybrid model integrating mBERT and LSTM", "Implementation of attention head pruning strategy for improved model efficiency"], "limitations": "", "keywords": ["Natural Language Processing", "Metaphor Classification", "Attention Head Pruning", "Low-resource Languages", "Multilingual Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.02856", "pdf": "https://arxiv.org/pdf/2506.02856.pdf", "abs": "https://arxiv.org/abs/2506.02856", "title": "Exploring listeners' perceptions of AI-generated and human-composed music for functional emotional applications", "authors": ["Kimaya Lecamwasam", "Tishya Ray Chaudhuri"], "categories": ["cs.HC"], "comment": "18 pages (includes references and appendix), 2 figures, 5 tables\n  (includes 1 in appendix), and appendix", "summary": "This work investigates how listeners perceive and evaluate AI-generated as\ncompared to human-composed music in the context of emotional resonance and\nregulation. Across a mixed-methods design, participants were exposed to both AI\nand human music under various labeling conditions (music correctly labeled as\nAI- or human-origin, music incorrectly labeled as AI- or human-origin, and\nunlabeled music) and emotion cases (Calm and Upbeat), and were asked to rate\npreference, efficacy of target emotion elicitation, and emotional impact.\nParticipants were significantly more likely to rate human-composed music,\nregardless of labeling, as more effective at eliciting target emotional states,\nthough quantitative analyses revealed no significant differences in emotional\nresponse. However, participants were significantly more likely to indicate\npreference for AI-generated music, yielding further questions regarding the\nimpact of emotional authenticity and perceived authorship on musical appraisal.\nQualitative data underscored this, with participants associating humanness with\nqualities such as imperfection, flow, and 'soul.' These findings challenge the\nassumption that preference alone signals success in generative music systems.\nRather than positioning AI tools as replacements for human creativity or\nemotional expression, they point toward a more careful design ethos that\nacknowledges the limits of replication and prioritizes human values such as\nauthenticity, individuality, and emotion regulation in wellness and affective\ntechnologies.", "AI": {"tldr": "The study explores listener perceptions of AI-generated vs human-composed music, focusing on emotional resonance and preference under various conditions.", "motivation": "To understand how AI-generated music is evaluated compared to human music, particularly in emotional contexts.", "method": "A mixed-methods design was employed where participants rated music classified as AI or human, under different labeling and emotion conditions.", "result": "Participants preferred AI-generated music despite rating human music as more effective in eliciting specific emotional states, highlighting a contradiction in emotional authenticity perceptions.", "conclusion": "The findings suggest that preference for AI music does not equate to emotional efficacy, prompting a re-evaluation of how AI in music should be designed to prioritize human values in emotional contexts.", "key_contributions": ["Found differences in preference versus emotional impact between AI and human music", "Qualitative insights linking humanness in music to emotional qualities", "Challenges the notion of AI as a replacement for human creativity in music."], "limitations": "Quantitative analyses showed no significant differences in emotional response despite preference ratings.", "keywords": ["AI-generated music", "human-composed music", "emotional resonance", "music appraisal", "emotional authenticity"], "importance_score": 4, "read_time_minutes": 18}}
{"id": "2506.02018", "pdf": "https://arxiv.org/pdf/2506.02018.pdf", "abs": "https://arxiv.org/abs/2506.02018", "title": "Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data", "authors": ["Christopher Lee Lübbers"], "categories": ["cs.CL", "I.2.7"], "comment": "21 pages, 11 figures. Master's thesis, University of Goettingen,\n  December 2025. Code: https://github.com/cluebbers/dpo-rlhf-paraphrase-types.\n  Models:\n  https://huggingface.co/collections/cluebbers/enhancing-paraphrase-type-generation-673ca8d75dfe2ce962a48ac0", "summary": "Paraphrasing re-expresses meaning to enhance applications like text\nsimplification, machine translation, and question-answering. Specific\nparaphrase types facilitate accurate semantic analysis and robust language\nmodels. However, existing paraphrase-type generation methods often misalign\nwith human preferences due to reliance on automated metrics and limited\nhuman-annotated training data, obscuring crucial aspects of semantic fidelity\nand linguistic transformations.\n  This study addresses this gap by leveraging a human-ranked paraphrase-type\ndataset and integrating Direct Preference Optimization (DPO) to align model\noutputs directly with human judgments. DPO-based training increases\nparaphrase-type generation accuracy by 3 percentage points over a supervised\nbaseline and raises human preference ratings by 7 percentage points. A newly\ncreated human-annotated dataset supports more rigorous future evaluations.\nAdditionally, a paraphrase-type detection model achieves F1 scores of 0.91 for\naddition/deletion, 0.78 for same polarity substitution, and 0.70 for\npunctuation changes.\n  These findings demonstrate that preference data and DPO training produce more\nreliable, semantically accurate paraphrases, enabling downstream applications\nsuch as improved summarization and more robust question-answering. The PTD\nmodel surpasses automated metrics and provides a more reliable framework for\nevaluating paraphrase quality, advancing paraphrase-type research toward\nricher, user-aligned language generation and establishing a stronger foundation\nfor future evaluations grounded in human-centric criteria.", "AI": {"tldr": "This study improves paraphrase-type generation using a human-ranked dataset and Direct Preference Optimization (DPO), enhancing accuracy and human preference ratings for better semantic accuracy in language models.", "motivation": "Existing paraphrase-type generation methods struggle to align with human preferences due to reliance on automated metrics and limited training data, impacting semantic fidelity.", "method": "The study utilizes a human-ranked paraphrase-type dataset and applies Direct Preference Optimization (DPO) to enhance the alignment of model outputs with human judgments.", "result": "DPO-based training improves paraphrase-type generation accuracy by 3 percentage points and human preference ratings by 7 percentage points, with the detection model achieving high F1 scores for various paraphrase types.", "conclusion": "The findings indicate that preference data and DPO training yield more reliable and semantically accurate paraphrases, supporting improved text summarization and question-answering, thus advancing the field toward user-aligned language generation.", "key_contributions": ["Developed a human-ranked paraphrase-type dataset for training", "Applied Direct Preference Optimization (DPO) for better alignment with human judgments", "Achieved high F1 scores for paraphrase-type detection models."], "limitations": "", "keywords": ["Paraphrasing", "Machine Translation", "Question-Answering", "Human-Centric Models", "Preference Optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02966", "pdf": "https://arxiv.org/pdf/2506.02966.pdf", "abs": "https://arxiv.org/abs/2506.02966", "title": "Unpacking Graduate Students' Learning Experience with Generative AI Teaching Assistant in A Quantitative Methodology Course", "authors": ["Zhanxin Hao", "Haifeng Luo", "Yongyi Chen", "Yu Zhang"], "categories": ["cs.HC", "68-11", "J.4"], "comment": "37 pages, 5 figures", "summary": "The study was conducted in an Advanced Quantitative Research Methods course\ninvolving 20 graduate students. During the course, student inquiries made to\nthe AI were recorded and coded using Bloom's taxonomy and the CLEAR framework.\nA series of independent sample t-tests and poisson regression analyses were\nemployed to analyse the characteristics of different questions asked by\nstudents with different backgrounds. Post course interviews were conducted with\n10 students to gain deeper insights into their perceptions. The findings\nrevealed a U-shaped pattern in students' use of the AI assistant, with higher\nusage at the beginning and towards the end of the course, and a decrease in\nusage during the middle weeks. Most questions posed to the AI focused on\nknowledge and comprehension levels, with fewer questions involving deeper\ncognitive thinking. Students with a weaker mathematical foundation used the AI\nassistant more frequently, though their inquiries tended to lack explicit and\nlogical structure compared to those with a strong mathematical foundation, who\nengaged less with the tool. These patterns suggest the need for targeted\nguidance to optimise the effectiveness of AI tools for students with varying\nlevels of academic proficiency.", "AI": {"tldr": "The study analyzes AI usage among graduate students in a research methods course, revealing varying patterns of inquiry based on students' backgrounds and understanding of course content.", "motivation": "To explore how students engage with AI assistants during a quantitative research methods course and to assess the impact of their academic backgrounds on AI usage.", "method": "The study involved coding student inquiries using Bloom's taxonomy and the CLEAR framework, followed by statistical analyses including t-tests and poisson regression, and post-course interviews for qualitative insights.", "result": "A U-shaped pattern was observed in AI usage, with higher engagement at both the start and end of the course, and lower usage mid-term. Students with weaker mathematical backgrounds utilized the AI more, but with less structured inquiries.", "conclusion": "Targeted guidance is necessary to better assist students with varying academic proficiency, aiming to enhance AI tool effectiveness in educational settings.", "key_contributions": ["Identified patterns in AI usage among students of different academic strengths", "Utilized Bloom's taxonomy and CLEAR framework for inquiry coding", "Provided insights for improving AI tool engagement in education"], "limitations": "Study limited to a specific course and small sample size; findings may not generalize beyond this context.", "keywords": ["AI tools", "graduate education", "Bloom's taxonomy", "CLEAR framework", "student inquiry"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.02019", "pdf": "https://arxiv.org/pdf/2506.02019.pdf", "abs": "https://arxiv.org/abs/2506.02019", "title": "ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking", "authors": ["E Fan", "Weizong Wang", "Tianhan Zhang"], "categories": ["cs.CL"], "comment": "19 pages, 8 figures", "summary": "Computational Fluid Dynamics (CFD) is essential for scientific and\nengineering advancements but is limited by operational complexity and the need\nfor extensive expertise. This paper presents ChatCFD, a large language\nmodel-driven pipeline that automates CFD workflows within the OpenFOAM\nframework. It enables users to configure and execute complex simulations from\nnatural language prompts or published literature with minimal expertise. The\ninnovation is its structured approach to database construction, configuration\nvalidation, and error reflection, integrating CFD and OpenFOAM knowledge with\ngeneral language models to improve accuracy and adaptability. Validation shows\nChatCFD can autonomously reproduce published CFD results, handling complex,\nunseen configurations beyond basic examples, a task challenging for general\nlanguage models.", "AI": {"tldr": "ChatCFD is a large language model-driven pipeline that automates CFD workflows, simplifying complex simulations for users with minimal expertise.", "motivation": "To reduce the operational complexity and expertise required for Computational Fluid Dynamics (CFD) simulations.", "method": "A structured pipeline integrating large language models with the OpenFOAM framework to automate configuration and execution of CFD workflows from natural language prompts.", "result": "ChatCFD can autonomously reproduce published CFD results and effectively manage complex configurations that are challenging for traditional language models.", "conclusion": "ChatCFD bridges the gap between language processing and CFD expertise, enabling broader accessibility and efficiency in simulation tasks.", "key_contributions": ["Automation of CFD workflows using large language models", "Integration with OpenFOAM for natural language configuration", "Validation against published CFD results with complex configurations"], "limitations": "", "keywords": ["Computational Fluid Dynamics", "ChatCFD", "OpenFOAM", "large language models", "automation"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2506.02993", "pdf": "https://arxiv.org/pdf/2506.02993.pdf", "abs": "https://arxiv.org/abs/2506.02993", "title": "Mapping Student-AI Interaction Dynamics in Multi-Agent Learning Environments: Supporting Personalised Learning and Reducing Performance Gaps", "authors": ["Zhanxin Hao", "Jie Cao", "Ruimiao Li", "Jifan Yu", "Zhiyuan Liu", "Yu Zhang"], "categories": ["cs.HC"], "comment": "27 pages, 8 figures", "summary": "Multi-agent AI systems, which simulate diverse instructional roles such as\nteachers and peers, offer new possibilities for personalized and interactive\nlearning. Yet, student-AI interaction patterns and their pedagogical\nimplications remain unclear. This study explores how university students\nengaged with multiple AI agents, and how these interactions influenced\ncognitive outcomes (learning gains) and non-cognitive factors (motivation,\ntechnology acceptance). Based on MAIC, an online learning platform with\nmulti-agent, the research involved 305 university students and 19,365 lines of\ndialogue data. Pre- and post-test scores, self-reported motivation and\ntechnology acceptance were also collected. The study identified two engagement\npatterns: co-construction of knowledge and co-regulation. Lag sequential\nanalysis revealed that students with lower prior knowledge relied more on\nco-construction of knowledge sequences, showing higher learning gains and\npost-course motivation. In contrast, students with higher prior knowledge\nengaged more in co-regulation behaviors but exhibited limited learning\nimprovement. Technology acceptance increased across all groups. These findings\nsuggest that multi-agent AI systems can adapt to students' varying needs,\nsupport differentiated engagement, and reduce performance gaps. Implications\nfor personalized system design and future research directions are discussed.", "AI": {"tldr": "This study examines the interaction patterns of university students with multi-agent AI systems and their impact on learning outcomes and motivation.", "motivation": "To explore how student engagement with multiple AI agents influences cognitive outcomes and non-cognitive factors in personalized learning environments.", "method": "The research used a dataset of 305 university students interacting with an online platform (MAIC) featuring multi-agent AI, analyzing pre- and post-test scores, self-reported motivation, and technology acceptance across 19,365 lines of dialogue.", "result": "Two engagement patterns were identified: co-construction of knowledge (beneficial for less knowledgeable students) and co-regulation (more common among students with higher knowledge), with technology acceptance increasing for all participants.", "conclusion": "Multi-agent AI can cater to diverse student needs, enhancing personalized learning and diminishing performance gaps, suggesting directions for future research.", "key_contributions": ["Identified engagement patterns in student-AI interactions", "Demonstrated varying impacts based on students' prior knowledge", "Showed increased technology acceptance across diverse student groups"], "limitations": "The study is limited by its focus on one learning platform and may not generalize to all educational contexts.", "keywords": ["multi-agent AI", "personalized learning", "student engagement", "technology acceptance", "learning outcomes"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02037", "pdf": "https://arxiv.org/pdf/2506.02037.pdf", "abs": "https://arxiv.org/abs/2506.02037", "title": "FinS-Pilot: A Benchmark for Online Financial System", "authors": ["Feng Wang", "Yiding Sun", "Jiaxin Mao", "Wei Xue", "Danqing Xu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. However, the development of financial RAG\nbenchmarks has been constrained by data confidentiality issues and the lack of\ndynamic data integration. To address this issue, we introduces FinS-Pilot, a\nnovel benchmark for evaluating RAG systems in online financial applications.\nConstructed from real-world financial assistant interactions, our benchmark\nincorporates both real-time API data and structured text sources, organized\nthrough an intent classification framework covering critical financial domains\nsuch as equity analysis and macroeconomic forecasting. The benchmark enables\ncomprehensive evaluation of financial assistants' capabilities in handling both\nstatic knowledge and time-sensitive market information. Through systematic\nexperiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's\neffectiveness in identifying models suitable for financial applications while\naddressing the current gap in specialized evaluation tools for the financial\ndomain. Our work contributes both a practical evaluation framework and a\ncurated dataset to advance research in financial NLP systems. The code and\ndataset are accessible on\nGitHub\\footnote{https://github.com/PhealenWang/financial\\_rag\\_benchmark}.", "AI": {"tldr": "The paper presents FinS-Pilot, a benchmark for evaluating retrieval-augmented generation (RAG) systems in financial applications, addressing issues of data confidentiality and dynamic data integration.", "motivation": "To fill the gap in standardized benchmarks for evaluating RAG systems in the financial domain due to challenges in data confidentiality and dynamic integration.", "method": "The authors developed FinS-Pilot using real-world financial assistant interactions, incorporating real-time API data and structured text, organized via intent classification for critical financial tasks.", "result": "FinS-Pilot was shown to effectively evaluate the capabilities of leading Chinese LLMs, identifying models that are well-suited for financial applications and addressing the deficit in specialized evaluation tools for this area.", "conclusion": "The introduction of FinS-Pilot offers both a practical framework for evaluation and a dataset designed to propel research in financial NLP systems.", "key_contributions": ["Development of FinS-Pilot, a novel benchmark for financial RAG systems", "Integration of real-time data and structured sources in evaluations", "Accessibility of the dataset and evaluation code on GitHub"], "limitations": "", "keywords": ["Large language models", "Financial RAG benchmark", "Financial applications", "NLP", "Dynamic data integration"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.03052", "pdf": "https://arxiv.org/pdf/2506.03052.pdf", "abs": "https://arxiv.org/abs/2506.03052", "title": "Feedstack: Layering Structured Representations over Unstructured Feedback to Scaffold Human AI Conversation", "authors": ["Hannah Vy Nguyen", "Yu-Chun Grace Yen", "Omar Shakir", "Hang Huynh", "Sebastian Gutierrez", "June A. Smith", "Sheila Jimenez", "Salma Abdelgelil", "Stephen MacNeil"], "categories": ["cs.HC"], "comment": "CUI '25, Proceedings of the 7th ACM Conference on Conversational User\n  Interfaces, July 8--10, 2025, Waterloo, ON, Canada", "summary": "Many conversational user interfaces facilitate linear conversations with\nturn-based dialogue, similar to face-to-face conversations between people.\nHowever, digital conversations can afford more than simple back-and-forth; they\ncan be layered with interaction techniques and structured representations that\nscaffold exploration, reflection, and shared understanding between users and AI\nsystems. We introduce Feedstack, a speculative interface that augments feedback\nconversations with layered affordances for organizing, navigating, and\nexternalizing feedback. These layered structures serve as a shared\nrepresentation of the conversation that can surface user intent and reveal\nunderlying design principles. This work represents an early exploration of this\nvision using a research-through-design approach. We describe system features\nand design rationale, and present insights from two formative (n=8, n=8)\nstudies to examine how novice designers engage with these layered supports.\nRather than presenting a conclusive evaluation, we reflect on Feedstack as a\ndesign probe that opens up new directions for conversational feedback systems.", "AI": {"tldr": "Feedstack is a speculative interface designed to enhance feedback conversations through layered interaction techniques and structured representations, promoting exploration and understanding between users and AI.", "motivation": "To move beyond simple turn-based dialogue in digital conversations to allow for richer interactions and shared understanding between users and AI systems.", "method": "A research-through-design approach was taken to develop Feedstack, and formative studies were conducted with novice designers to explore their engagement with the system's layered features.", "result": "The findings indicate how layered structures in conversations can surface user intent and reveal design principles, providing insights into potential new directions for conversational feedback systems.", "conclusion": "Feedstack acts as a design probe, reflecting on its potential to enhance conversational feedback rather than delivering final evaluations.", "key_contributions": ["Introduction of Feedstack, a novel conversational user interface.", "Exploratory insights into layered affordances for feedback conversations.", "New directions for future conversational feedback systems."], "limitations": "The studies conducted were formative and not conclusive, limiting the evaluation of Feedstack's effectiveness.", "keywords": ["Conversational User Interfaces", "Feedback Systems", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02041", "pdf": "https://arxiv.org/pdf/2506.02041.pdf", "abs": "https://arxiv.org/abs/2506.02041", "title": "Enhancing Multimodal Continual Instruction Tuning with BranchLoRA", "authors": ["Duzhen Zhang", "Yong Ren", "Zhong-Zhi Li", "Yahan Yu", "Jiahua Dong", "Chenxing Li", "Zhilong Ji", "Jinfeng Bai"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal\nLarge Language Models (MLLMs) to continually align with human intent across\nsequential tasks. Existing approaches often rely on the Mixture-of-Experts\n(MoE) LoRA framework to preserve previous instruction alignments. However,\nthese methods are prone to Catastrophic Forgetting (CF), as they aggregate all\nLoRA blocks via simple summation, which compromises performance over time. In\nthis paper, we identify a critical parameter inefficiency in the MoELoRA\nframework within the MCIT context. Based on this insight, we propose\nBranchLoRA, an asymmetric framework to enhance both efficiency and performance.\nTo mitigate CF, we introduce a flexible tuning-freezing mechanism within\nBranchLoRA, enabling branches to specialize in intra-task knowledge while\nfostering inter-task collaboration. Moreover, we incrementally incorporate\ntask-specific routers to ensure an optimal branch distribution over time,\nrather than favoring the most recent task. To streamline inference, we\nintroduce a task selector that automatically routes test inputs to the\nappropriate router without requiring task identity. Extensive experiments on\nthe latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms\nMoELoRA and maintains its superiority across various MLLM sizes.", "AI": {"tldr": "This paper presents BranchLoRA, a new framework designed to improve the efficiency and performance of Multimodal Large Language Models in continual instruction tuning, addressing the issue of Catastrophic Forgetting in existing methods.", "motivation": "To address the inefficiency in the Mixture-of-Experts LoRA framework within the context of Multimodal Continual Instruction Tuning and to mitigate the challenges posed by Catastrophic Forgetting.", "method": "The paper proposes BranchLoRA, which incorporates a flexible tuning-freezing mechanism and task-specific routers to improve task specialization and collaboration among branches.", "result": "BranchLoRA significantly outperforms the existing MoELoRA framework across various Multimodal Large Language Model sizes in terms of efficiency and performance while mitigating Catastrophic Forgetting.", "conclusion": "The proposed BranchLoRA framework effectively addresses the critical parameter inefficiency seen in existing continual instruction tuning methods and demonstrates superior performance in maintaining alignment with human intent over sequential tasks.", "key_contributions": ["Introduction of BranchLoRA framework", "Flexible tuning-freezing mechanism to reduce CF", "Task-specific routers for optimal branch distribution"], "limitations": "The framework's performance across unfamiliar tasks or domains has not been evaluated.", "keywords": ["multimodal", "instruction tuning", "large language models", "catastrophic forgetting", "machine learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.03113", "pdf": "https://arxiv.org/pdf/2506.03113.pdf", "abs": "https://arxiv.org/abs/2506.03113", "title": "Assessing Workers Neuro-physiological Stress Responses to Augmented Reality Safety Warnings in Immersive Virtual Roadway Work Zones", "authors": ["Fatemeh Banani Ardecani", "Omidreza Shoghli"], "categories": ["cs.HC"], "comment": null, "summary": "This paper presents a multi-stage experimental framework that integrates\nimmersive Virtual Reality (VR) simulations, wearable sensors, and advanced\nsignal processing to investigate construction workers neuro-physiological\nstress responses to multi-sensory AR-enabled warnings. Participants performed\nlight- and moderate-intensity roadway maintenance tasks within a high-fidelity\nVR roadway work zone, while key stress markers of electrodermal activity (EDA),\nheart rate variability (HRV), and electroencephalography (EEG) were\ncontinuously measured. Statistical analyses revealed that task intensity\nsignificantly influenced physiological and neurological stress indicators.\nModerate-intensity tasks elicited greater autonomic arousal, evidenced by\nelevated heart rate measures (mean-HR, std-HR, max-HR) and stronger\nelectrodermal responses, while EEG data indicated distinct stress-related alpha\nsuppression and beta enhancement. Feature-importance analysis further\nidentified mean EDR and short-term HR metrics as discriminative for classifying\ntask intensity. Correlation results highlighted a temporal lag between\nimmediate neural changes and subsequent physiological stress reactions,\nemphasizing the interplay between cognition and autonomic regulation during\nhazardous tasks.", "AI": {"tldr": "The paper presents a framework integrating VR simulations and sensors to study construction workers' stress responses to AR-enabled warnings during roadway tasks.", "motivation": "To investigate the neuro-physiological stress responses of construction workers under multi-sensory AR-enabled warnings within a VR environment.", "method": "Participants engaged in roadway maintenance tasks of varying intensities while physiological metrics (EDA, HRV, EEG) were monitored and analyzed.", "result": "Task intensity significantly affected stress indicators, with moderate tasks increasing heart rate and producing identifiable changes in EEG patterns.", "conclusion": "The study underscores the connection between mental processing and physical stress responses in hazardous work conditions, with implications for AR safety systems.", "key_contributions": ["Integration of VR and wearable sensors for stress research", "Identification of physiological markers correlating with task intensity", "Insights into the cognitive and autonomic interplay during stress"], "limitations": "", "keywords": ["Virtual Reality", "neuro-physiological stress", "augmented reality", "construction safety", "sensor integration"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.02058", "pdf": "https://arxiv.org/pdf/2506.02058.pdf", "abs": "https://arxiv.org/abs/2506.02058", "title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?", "authors": ["Xiang Li", "Jiayi Xin", "Qi Long", "Weijie J. Su"], "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.AP", "stat.ME"], "comment": null, "summary": "Accurate evaluation of large language models (LLMs) is crucial for\nunderstanding their capabilities and guiding their development. However,\ncurrent evaluations often inconsistently reflect the actual capacities of these\nmodels. In this paper, we demonstrate that one of many contributing factors to\nthis \\textit{evaluation crisis} is the oversight of unseen knowledge --\ninformation encoded by LLMs but not directly observed or not yet observed\nduring evaluations. We introduce KnowSum, a statistical framework designed to\nprovide a more comprehensive assessment by quantifying the unseen knowledge for\na class of evaluation tasks. KnowSum estimates the unobserved portion by\nextrapolating from the appearance frequencies of observed knowledge instances.\nWe demonstrate the effectiveness and utility of KnowSum across three critical\napplications: estimating total knowledge, evaluating information retrieval\neffectiveness, and measuring output diversity. Our experiments reveal that a\nsubstantial volume of knowledge is omitted when relying solely on observed LLM\nperformance. Importantly, KnowSum yields significantly different comparative\nrankings for several common LLMs based on their internal knowledge.", "AI": {"tldr": "The paper introduces KnowSum, a framework for better evaluating LLMs by quantifying unseen knowledge that current evaluations often overlook.", "motivation": "To address the inadequacies in current LLM evaluations which fail to fully capture model capabilities due to the oversight of unseen knowledge.", "method": "KnowSum statistically extrapolates unobserved knowledge from the frequencies of observed knowledge instances to provide a more thorough evaluation of LLMs.", "result": "The framework was tested across multiple tasks, revealing that significant amounts of knowledge were neglected when based solely on observed performance and that KnowSum led to different comparative rankings of LLMs.", "conclusion": "KnowSum enhances the evaluation of LLMs by revealing the amount of unseen knowledge, impacting how we understand model capabilities and performance ranking.", "key_contributions": ["Introduction of the KnowSum framework for evaluating unseen knowledge in LLMs.", "Demonstrated effectiveness across total knowledge assessment, retrieval effectiveness, and output diversity.", "Revised comparative rankings for common LLMs based on internal knowledge assessment."], "limitations": "The framework's effectiveness may vary based on the specific tasks and knowledge types being evaluated.", "keywords": ["large language models", "evaluation", "KnowSum", "unseen knowledge", "information retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.04332", "pdf": "https://arxiv.org/pdf/2504.04332.pdf", "abs": "https://arxiv.org/abs/2504.04332", "title": "IMPersona: Evaluating Individual Level LM Impersonation", "authors": ["Quan Shi", "Carlos E. Jimenez", "Stephen Dong", "Brian Seo", "Caden Yao", "Adam Kelch", "Karthik Narasimhan"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "25 pages, 9 pages main", "summary": "As language models achieve increasingly human-like capabilities in\nconversational text generation, a critical question emerges: to what extent can\nthese systems simulate the characteristics of specific individuals? To evaluate\nthis, we introduce IMPersona, a framework for evaluating LMs at impersonating\nspecific individuals' writing style and personal knowledge. Using supervised\nfine-tuning and a hierarchical memory-inspired retrieval system, we demonstrate\nthat even modestly sized open-source models, such as Llama-3.1-8B-Instruct, can\nachieve impersonation abilities at concerning levels. In blind conversation\nexperiments, participants (mis)identified our fine-tuned models with memory\nintegration as human in 44.44% of interactions, compared to just 25.00% for the\nbest prompting-based approach. We analyze these results to propose detection\nmethods and defense strategies against such impersonation attempts. Our\nfindings raise important questions about both the potential applications and\nrisks of personalized language models, particularly regarding privacy,\nsecurity, and the ethical deployment of such technologies in real-world\ncontexts.", "AI": {"tldr": "The paper introduces IMPersona, a framework to evaluate language models' ability to impersonate individual writing styles and knowledge, achieving notable success with modestly sized models.", "motivation": "With language models becoming increasingly capable of human-like text generation, it is crucial to understand their ability to mimic individual identities and the implications of such capabilities.", "method": "The study employs supervised fine-tuning and a hierarchical memory-inspired retrieval system to enhance language models like Llama-3.1-8B-Instruct for impersonation tasks.", "result": "In blind tests, fine-tuned models with memory integration were misidentified as human 44.44% of the time, outperforming the best existing prompting-based strategies at 25.00%.", "conclusion": "The results highlight potential risks and raise ethical concerns regarding the use of personalized language models, urging the need for detection and defense strategies against impersonation.", "key_contributions": ["Introduction of the IMPersona framework for evaluating language model impersonation", "Demonstration of effective impersonation by modestly sized language models", "Analysis of ethical implications and defense strategies against impersonation"], "limitations": "Focused on fine-tuned models; may not generalize across all language models or contexts of use.", "keywords": ["language models", "impersonation", "human-like text generation", "supervised fine-tuning", "ethical implications"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2506.02126", "pdf": "https://arxiv.org/pdf/2506.02126.pdf", "abs": "https://arxiv.org/abs/2506.02126", "title": "Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains", "authors": ["Juncheng Wu", "Sheng Liu", "Haoqin Tu", "Hang Yu", "Xiaoke Huang", "James Zou", "Cihang Xie", "Yuyin Zhou"], "categories": ["cs.CL"], "comment": "17 pages, preprint", "summary": "Recent advances in reasoning-enhanced Large Language Models such as\nOpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex\ntasks. However, the quality and transparency of their internal reasoning\nprocesses remain underexplored. This work moves beyond the final-answer\naccuracy and investigates step-by-step reasoning in the medical and\nmathematical domains by explicitly decomposing the thinking trajectories into\ntwo parts: knowledge and reasoning. Specifically, we introduce a fine-grained\nevaluation framework that judges: (1) the correctness of knowledge used\n(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured\nby Information Gain (InfoGain)). Using this framework, we study R1-distilled\nand base Qwen models trained with supervised fine-tuning (SFT) and/or\nreinforcement learning (RL) in the medical and math domains. Three intriguing\nfindings emerge: (1) The general reasoning abilities in R1-distilled models do\nnot transfer effectively to the medical domain through either SFT or RL. (2)\nSFT raises final-answer accuracy in both domains, but often at the cost of\nreasoning quality: InfoGain drops by 38.9% on average compared with untrained\nmodels; In the medical domain, however, SFT remains crucial because domain\nknowledge is indispensable. (3) RL enhances medical reasoning by pruning\ninaccurate or irrelevant knowledge from reasoning paths, thereby improving both\nreasoning accuracy and knowledge correctness.", "AI": {"tldr": "This paper investigates the internal reasoning processes of reasoning-enhanced Large Language Models (LLMs) in medical and mathematical domains, proposing a new evaluation framework to assess knowledge and reasoning quality.", "motivation": "To explore the underexplored quality and transparency of internal reasoning processes in reasoning-enhanced LLMs, especially in the medical and mathematical domains.", "method": "A fine-grained evaluation framework is introduced that assesses two aspects: the correctness of knowledge (Knowledge Index - KI) and the quality of reasoning (Information Gain - InfoGain). The study examines R1-distilled and base Qwen models with supervised fine-tuning and/or reinforcement learning.", "result": "The study finds that (1) general reasoning abilities do not transfer well to the medical domain, (2) supervised fine-tuning increases accuracy but reduces reasoning quality, and (3) reinforcement learning improves reasoning accuracy by eliminating irrelevant knowledge.", "conclusion": "The research highlights the complexity of reasoning in medical tasks and the trade-offs between accuracy and reasoning quality, suggesting the importance of refining training methodologies in LLMs.", "key_contributions": ["Introduced a new evaluation framework for assessing knowledge and reasoning in LLMs.", "Identified the limitations of model transferability to medical reasoning tasks.", "Demonstrated the benefits of reinforcement learning in improving reasoning accuracy."], "limitations": "The generalizability of findings to other domains beyond medical and mathematical has not been established.", "keywords": ["Large Language Models", "reasoning", "medical domain", "machine learning", "evaluation framework"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2506.02132", "pdf": "https://arxiv.org/pdf/2506.02132.pdf", "abs": "https://arxiv.org/abs/2506.02132", "title": "Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models", "authors": ["Michael Li", "Nishant Subramani"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large transformer-based language models dominate modern NLP, yet our\nunderstanding of how they encode linguistic information is rooted in studies of\nearly models like BERT and GPT-2. To better understand today's language models,\nwe investigate how both classical architectures (BERT, DeBERTa, GPT-2)and\ncontemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,\nLlama-3.1) represent lexical identity and inflectional morphology. We train\nlinear and nonlinear classifiers on layer-wise activations to predict word\nlemmas and inflectional features. We discover that models concentrate lexical\ninformation linearly in early layers and increasingly nonlinearly in later\nlayers, while keeping inflectional information uniformly accessible and\nlinearly separable throughout the layers. Further analysis reveals that these\nmodels encode inflectional morphology through generalizable abstractions, but\nrely predominantly on memorization to encode lexical identity. Remarkably,\nthese patterns emerge across all 16 models we test, despite differences in\narchitecture, size, and training regime (including pretrained and\ninstruction-tuned variants). This consistency suggests that, despite\nsubstantial advances in LLM technologies, transformer models organize\nlinguistic information in similar ways, indicating that these properties could\nbe fundamental for next token prediction and are learned early during\npretraining. Our code is available at\nhttps://github.com/ml5885/model_internal_sleuthing.", "AI": {"tldr": "This paper analyzes how various transformer-based language models encode lexical identity and inflectional morphology, finding consistent patterns in information representation across multiple models.", "motivation": "To understand how contemporary language models represent linguistic information beyond early models like BERT and GPT-2.", "method": "The study trains classifiers on layer-wise activations of models to predict word lemmas and inflectional features, examining both classical and contemporary architectures.", "result": "It was found that models linearly represent lexical information in early layers and nonlinearly in later layers while encoding inflectional features consistently across all layers.", "conclusion": "Despite advances in architecture and training regimes, the way transformer models represent linguistic information appears fundamentally similar.", "key_contributions": ["Analysis of lexical and inflectional information across various language models", "Identification of linear and nonlinear encoding patterns in different layers", "Demonstration of consistency in linguistic information representation across 16 models tested"], "limitations": "", "keywords": ["transformer models", "lexical identity", "inflectional morphology", "language representation", "BERT"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.02147", "pdf": "https://arxiv.org/pdf/2506.02147.pdf", "abs": "https://arxiv.org/abs/2506.02147", "title": "BabyLM's First Constructions: Causal interventions provide a signal of learning", "authors": ["Joshua Rozner", "Leonie Weissweiler", "Cory Shain"], "categories": ["cs.CL"], "comment": null, "summary": "Construction grammar posits that children acquire constructions (form-meaning\npairings) from the statistics of their environment. Recent work supports this\nhypothesis by showing sensitivity to constructions in pretrained language\nmodels (PLMs), including one recent study (Rozner et al., 2025) demonstrating\nthat constructions shape the PLM's output distribution. However, models under\nstudy have generally been trained on developmentally implausible amounts of\ndata, casting doubt on their relevance to human language learning. Here we use\nRozner et al.'s methods to evaluate constructional learning in models from the\n2024 BabyLM challenge. Our results show that even when trained on\ndevelopmentally plausible quantities of data, models represent diverse\nconstructions, even hard cases that are superficially indistinguishable. We\nfurther find correlational evidence that constructional performance may be\nfunctionally relevant: models that better represent constructions perform\nbetter on the BabyLM benchmarks.", "AI": {"tldr": "This paper evaluates construction learning in language models trained on developmentally plausible data, showing that they can represent diverse constructions and perform better on benchmarks when they do.", "motivation": "To explore how well language models trained on realistic data can represent linguistic constructions similar to human language acquisition.", "method": "The study replicates methods from Rozner et al. (2025) to assess constructional learning in models from the 2024 BabyLM challenge.", "result": "Models trained on developmentally plausible data demonstrate an ability to represent diverse constructions and correlate performance on benchmarks with constructional representation.", "conclusion": "Even with limited data, models can learn to represent complex linguistic constructions, suggesting relevance to human language learning processes.", "key_contributions": ["Demonstration that language models can learn constructions with less training data than typically used.", "Correlation between constructional performance and success on BabyLM benchmarks.", "Insight into the implications of construction representation for language learning in humans."], "limitations": "The study is limited to specific models and may not generalize across all types of language models or constructions.", "keywords": ["Construction Grammar", "Pretrained Language Models", "BabyLM Challenge", "Language Learning", "Neural Models"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.02449", "pdf": "https://arxiv.org/pdf/2506.02449.pdf", "abs": "https://arxiv.org/abs/2506.02449", "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data", "authors": ["Bo Peng", "Zhiheng Wang", "Heyang Gong", "Chaochao Lu"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In modern dialogue systems, the ability to implicitly infer user backgrounds\nfrom conversations and leverage this information for personalized assistance is\ncrucial. However, the scarcity of high-quality data remains a fundamental\nchallenge to evaluating and improving this capability. Traditional dataset\nconstruction methods are labor-intensive, resource-demanding, and raise privacy\nconcerns. To address these issues, we propose a novel approach for automatic\nsynthetic data generation and introduce the Implicit Personalized Dialogue\n(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12\nuser attribute types. Additionally, we develop a systematic evaluation\nframework with four metrics to assess both attribute awareness and reasoning\ncapabilities. We further propose five causal graphs to elucidate models'\nreasoning pathways during implicit personalization. Extensive experiments yield\ninsightful observations and prove the reliability of our dataset.", "AI": {"tldr": "A novel method for automatic synthetic data generation in dialogue systems, introducing the IP-Dialog benchmark and a systematic evaluation framework.", "motivation": "To improve personalized assistance in dialogue systems by inferring user backgrounds while addressing challenges in data scarcity and traditional dataset creation methods.", "method": "A synthetic data generation approach is proposed along with the creation of the Implicit Personalized Dialogue benchmark, which includes a training dataset for 10 tasks and 12 user attribute types, supplemented by a systematic evaluation framework with four assessment metrics.", "result": "Experiments validate the reliability of the synthetic dataset and provide insights into models' performance in user attribute awareness and reasoning.", "conclusion": "The proposed method improves the capabilities of dialogue systems to provide personalized assistance while overcoming traditional data limitations.", "key_contributions": ["Introduction of the Implicit Personalized Dialogue benchmark", "Automatic synthetic data generation for dialogue systems", "Development of a systematic evaluation framework for attribute awareness and reasoning"], "limitations": "", "keywords": ["dialogue systems", "synthetic data generation", "personalized assistance", "user background", "evaluation framework"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.02157", "pdf": "https://arxiv.org/pdf/2506.02157.pdf", "abs": "https://arxiv.org/abs/2506.02157", "title": "HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation", "authors": ["Amir Hussein", "Cihan Xiao", "Matthew Wiesner", "Dan Povey", "Leibny Paola Garcia", "Sanjeev Khudanpur"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Neural transducers (NT) provide an effective framework for speech streaming,\ndemonstrating strong performance in automatic speech recognition (ASR).\nHowever, the application of NT to speech translation (ST) remains challenging,\nas existing approaches struggle with word reordering and performance\ndegradation when jointly modeling ASR and ST, resulting in a gap with\nattention-based encoder-decoder (AED) models. Existing NT-based ST approaches\nalso suffer from high computational training costs. To address these issues, we\npropose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech\nRecognition and Translation), a novel framework that factorizes ASR and\ntranslation tasks to better handle reordering. To ensure robust ST while\npreserving ASR performance, we use self-distillation with CTC consistency\nregularization. Moreover, we improve computational efficiency by incorporating\nbest practices from ASR transducers, including a down-sampled hierarchical\nencoder, a stateless predictor, and a pruned transducer loss to reduce training\ncomplexity. Finally, we introduce a blank penalty during decoding, reducing\ndeletions and improving translation quality. Our approach is evaluated on three\nconversational datasets Arabic, Spanish, and Mandarin achieving new\nstate-of-the-art performance among NT models and substantially narrowing the\ngap with AED-based systems.", "AI": {"tldr": "HENT-SRT is a novel framework for speech recognition and translation using Neural Transducers, addressing challenges in word reordering and computational efficiency.", "motivation": "Current Neural Transducer approaches struggle with word reordering and performance when modeling ASR and ST together, leading to performance gaps with attention-based models.", "method": "HENT-SRT separates ASR and translation tasks, utilizes self-distillation with CTC consistency, and incorporates efficient ASR practices for reduced training complexity.", "result": "HENT-SRT achieved state-of-the-art performance on Arabic, Spanish, and Mandarin conversational datasets, significantly narrowing the performance gap with AED systems.", "conclusion": "The proposed framework shows substantial improvements in translation quality and efficiency while maintaining ASR performance.", "key_contributions": ["Introduction of HENT-SRT framework for improved speech translation.", "Use of self-distillation with CTC regularization for robust ST.", "Incorporation of efficient methods from ASR to reduce training complexity."], "limitations": "", "keywords": ["Neural Transducer", "Speech Recognition", "Speech Translation"], "importance_score": 4, "read_time_minutes": 6}}
{"id": "2506.02533", "pdf": "https://arxiv.org/pdf/2506.02533.pdf", "abs": "https://arxiv.org/abs/2506.02533", "title": "Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey", "authors": ["Maike Behrendt", "Stefan Sylvius Wagner", "Carina Weinmann", "Marike Bormann", "Mira Warne", "Stefan Harmeling"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Political online participation in the form of discussing political issues and\nexchanging opinions among citizens is gaining importance with more and more\nformats being held digitally. To come to a decision, a careful discussion and\nconsideration of opinions and a civil exchange of arguments, which is defined\nas the act of deliberation, is desirable. The quality of discussions and\nparticipation processes in terms of their deliberativeness highly depends on\nthe design of platforms and processes. To facilitate online communication for\nboth participants and initiators, machine learning methods offer a lot of\npotential. In this work we want to showcase which issues occur in political\nonline discussions and how machine learning can be used to counteract these\nissues and enhance deliberation.", "AI": {"tldr": "This paper examines the challenges in political online discussions and proposes machine learning solutions to improve deliberative quality.", "motivation": "The increasing importance of political online participation necessitates high-quality discussions characterized by civil argument exchange, dependent on platform design.", "method": "The paper explores the issues arising in political online discussions and discusses the application of machine learning methods to enhance deliberation.", "result": "Machine learning has the potential to address specific challenges in political online discussions, improving the overall quality of deliberation.", "conclusion": "By leveraging machine learning, platforms can facilitate better online communication among citizens, enhancing deliberative processes.", "key_contributions": ["Identification of key issues in political online discussions.", "Application of machine learning methods to enhance deliberation.", "Recommendations for designing platforms that support civil exchanges."], "limitations": "", "keywords": ["political participation", "machine learning", "deliberation", "online discussions", "platform design"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2506.02172", "pdf": "https://arxiv.org/pdf/2506.02172.pdf", "abs": "https://arxiv.org/abs/2506.02172", "title": "Different Speech Translation Models Encode and Translate Speaker Gender Differently", "authors": ["Dennis Fucci", "Marco Gaido", "Matteo Negri", "Luisa Bentivogli", "Andre Martins", "Giuseppe Attanasio"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "Recent studies on interpreting the hidden states of speech models have shown\ntheir ability to capture speaker-specific features, including gender. Does this\nfinding also hold for speech translation (ST) models? If so, what are the\nimplications for the speaker's gender assignment in translation? We address\nthese questions from an interpretability perspective, using probing methods to\nassess gender encoding across diverse ST models. Results on three language\ndirections (English-French/Italian/Spanish) indicate that while traditional\nencoder-decoder models capture gender information, newer architectures --\nintegrating a speech encoder with a machine translation system via adapters --\ndo not. We also demonstrate that low gender encoding capabilities result in\nsystems' tendency toward a masculine default, a translation bias that is more\npronounced in newer architectures.", "AI": {"tldr": "This paper examines gender encoding in speech translation models and its implications for speaker gender assignment in translations.", "motivation": "To understand how speech translation models capture and encode gender characteristics, and the bias introduced in translations based on model architecture.", "method": "Using probing methods to assess the gender encoding capabilities of various speech translation models across three language directions: English-French, English-Italian, and English-Spanish.", "result": "Traditional encoder-decoder models effectively capture gender information, whereas newer hybrid architectures tend to lack this capability, leading to a translation bias towards masculine defaults.", "conclusion": "The study highlights the need for awareness of gender biases in speech translation systems and suggests that improvements are necessary in newer architectures to avoid such biases.", "key_contributions": ["Assessment of gender encoding in speech translation models", "Comparison of traditional and newer speech translation architectures", "Identification of translation bias towards masculine default in newer models."], "limitations": "", "keywords": ["speech translation", "gender encoding", "interpretability", "translation bias", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.02175", "pdf": "https://arxiv.org/pdf/2506.02175.pdf", "abs": "https://arxiv.org/abs/2506.02175", "title": "AI Debate Aids Assessment of Controversial Claims", "authors": ["Salman Rahman", "Sheriff Issaka", "Ashima Suvarna", "Genglin Liu", "James Shiffer", "Jaeyoung Lee", "Md Rizwan Parvez", "Hamid Palangi", "Shi Feng", "Nanyun Peng", "Yejin Choi", "Julian Michael", "Liwei Jiang", "Saadia Gabriel"], "categories": ["cs.CL"], "comment": null, "summary": "As AI grows more powerful, it will increasingly shape how we understand the\nworld. But with this influence comes the risk of amplifying misinformation and\ndeepening social divides-especially on consequential topics like public health\nwhere factual accuracy directly impacts well-being. Scalable Oversight aims to\nensure AI truthfulness by enabling humans to supervise systems that may exceed\nhuman capabilities--yet humans themselves hold different beliefs and biases\nthat impair their judgment. We study whether AI debate can guide biased judges\ntoward the truth by having two AI systems debate opposing sides of\ncontroversial COVID-19 factuality claims where people hold strong prior\nbeliefs. We conduct two studies: one with human judges holding either\nmainstream or skeptical beliefs evaluating factuality claims through\nAI-assisted debate or consultancy protocols, and a second examining the same\nproblem with personalized AI judges designed to mimic these different human\nbelief systems. In our human study, we find that debate-where two AI advisor\nsystems present opposing evidence-based arguments-consistently improves\njudgment accuracy and confidence calibration, outperforming consultancy with a\nsingle-advisor system by 10% overall. The improvement is most significant for\njudges with mainstream beliefs (+15.2% accuracy), though debate also helps\nskeptical judges who initially misjudge claims move toward accurate views\n(+4.7% accuracy). In our AI judge study, we find that AI judges with human-like\npersonas achieve even higher accuracy (78.5%) than human judges (70.1%) and\ndefault AI judges without personas (69.8%), suggesting their potential for\nsupervising frontier AI models. These findings highlight AI debate as a\npromising path toward scalable, bias-resilient oversight--leveraging both\ndiverse human and AI judgments to move closer to truth in contested domains.", "AI": {"tldr": "This paper explores the use of AI debate to improve fact-checking and judgment accuracy on controversial topics like COVID-19 claims, highlighting the effectiveness of AI systems in countering biases.", "motivation": "As AI influences our understanding of the world, it risks amplifying misinformation, particularly in public health, necessitating methods to ensure AI truthfulness and effective human oversight.", "method": "The study consists of two parts: one with human judges evaluating COVID-19 claims through AI-assisted debate vs. consultancy, and another using personalized AI judges designed to mimic different human biases.", "result": "The human study shows AI debate improves judgment accuracy by 10%, especially aiding judges with mainstream beliefs (+15.2% accuracy) and slightly assisting skeptical judges (+4.7% accuracy). The AI judge study indicates AI judges with personas achieve higher accuracy (78.5%) than human judges (70.1%).", "conclusion": "AI debate frameworks can serve as effective tools for scalable oversight, enhancing judgment accuracy and potentially offering bias-resilient supervision of AI systems.", "key_contributions": ["Demonstrates that AI debate can enhance human judgment in assessing factual claims.", "Shows that personalized AI judges can outperform human judges in judgment accuracy.", "Highlights the potential of leveraging AI systems for truth-seeking in biased domains."], "limitations": "", "keywords": ["AI debate", "fact-checking", "COVID-19 misinformation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02181", "pdf": "https://arxiv.org/pdf/2506.02181.pdf", "abs": "https://arxiv.org/abs/2506.02181", "title": "Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution", "authors": ["Dennis Fucci", "Marco Gaido", "Matteo Negri", "Mauro Cettolo", "Luisa Bentivogli"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Interspeech 2025", "summary": "Despite significant advances in ASR, the specific acoustic cues models rely\non remain unclear. Prior studies have examined such cues on a limited set of\nphonemes and outdated models. In this work, we apply a feature attribution\ntechnique to identify the relevant acoustic cues for a modern Conformer-based\nASR system. By analyzing plosives, fricatives, and vowels, we assess how\nfeature attributions align with their acoustic properties in the time and\nfrequency domains, also essential for human speech perception. Our findings\nshow that the ASR model relies on vowels' full time spans, particularly their\nfirst two formants, with greater saliency in male speech. It also better\ncaptures the spectral characteristics of sibilant fricatives than non-sibilants\nand prioritizes the release phase in plosives, especially burst\ncharacteristics. These insights enhance the interpretability of ASR models and\nhighlight areas for future research to uncover potential gaps in model\nrobustness.", "AI": {"tldr": "This paper uses feature attribution to identify acoustic cues in a Conformer-based ASR system, revealing insights into the model's reliance on phonemic characteristics.", "motivation": "To clarify the specific acoustic cues that modern ASR models depend on, addressing limitations found in prior studies with outdated models and limited phoneme analysis.", "method": "Feature attribution techniques were applied to analyze acoustic properties of plosives, fricatives, and vowels in a modern Conformer-based ASR system.", "result": "The analysis revealed that the ASR model relies heavily on the full time spans of vowels, particularly their first two formants, and shows greater sensitivity to male speech. It also distinguishes better spectral characteristics of sibilant fricatives relatively to non-sibilants and emphasizes the release phase in plosives.", "conclusion": "The insights gained from this study improve the interpretability of ASR systems and point to important areas for further research on model robustness.", "key_contributions": ["Identification of specific acoustic cues used by ASR models", "Enhanced interpretability of modern ASR systems", "New insights into phoneme processing, particularly sibilants and plosives"], "limitations": "", "keywords": ["ASR", "feature attribution", "Conformer-based"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.02911", "pdf": "https://arxiv.org/pdf/2506.02911.pdf", "abs": "https://arxiv.org/abs/2506.02911", "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1", "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.", "AI": {"tldr": "The paper introduces the CellPuzzles task for assigning unique cell types in single-cell RNA sequencing data, and presents Cell-o1, a fine-tuned LLM that outperforms existing models in this context.", "motivation": "To provide a methodology for unique cell type annotation considering batch-level cellular context, addressing limitations of existing models that annotate cells independently.", "method": "Introduces the CellPuzzles benchmark for unique cell type assignment, uses a 7B LLM (Cell-o1) trained with supervised fine-tuning on reasoning traces and reinforcement learning with batch-level rewards.", "result": "Cell-o1 achieves state-of-the-art performance, surpassing OpenAI's o1 model by over 73% and generalizing well across various tissues and conditions.", "conclusion": "The findings highlight the potential of Cell-o1 in improving batch-level cell type annotation, with insights into training dynamics and reasoning behaviors contributing to understanding performance.", "key_contributions": ["Introduction of the CellPuzzles benchmark for batch-level cell type annotation", "Development of Cell-o1, which significantly improves annotation accuracy", "Insights into reasoning behaviors relevant to batch-level cellular context"], "limitations": "", "keywords": ["cell type annotation", "single-cell RNA sequencing", "large language models", "batch-level context", "CellPuzzles"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.02204", "pdf": "https://arxiv.org/pdf/2506.02204.pdf", "abs": "https://arxiv.org/abs/2506.02204", "title": "BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models", "authors": ["Lindia Tjuatja", "Graham Neubig"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Language model evaluation is a daunting task: prompts are brittle,\ncorpus-level perplexities are vague, and the choice of benchmarks are endless.\nFinding examples that show meaningful, generalizable differences between two\nLMs is crucial to understanding where one model succeeds and another fails. Can\nthis process be done automatically? In this work, we propose methodology for\nautomated comparison of language models that uses performance-aware contextual\nembeddings to find fine-grained features of text where one LM outperforms\nanother. Our method, which we name BehaviorBox, extracts coherent features that\ndemonstrate differences with respect to the ease of generation between two LMs.\nSpecifically, BehaviorBox finds features that describe groups of words in\nfine-grained contexts, such as \"conditional 'were' in the phrase 'if you were'\"\nand \"exclamation marks after emotional statements\", where one model outperforms\nanother within a particular datatset. We apply BehaviorBox to compare models\nthat vary in size, model family, and post-training, and enumerate insights into\nspecific contexts that illustrate meaningful differences in performance which\ncannot be found by measures such as corpus-level perplexity alone.", "AI": {"tldr": "This paper introduces BehaviorBox, a methodology for the automated comparison of language models that identifies fine-grained text features where one model outperforms another.", "motivation": "Understanding the performance differences between language models is challenging due to the limitations of existing evaluation methods.", "method": "BehaviorBox uses performance-aware contextual embeddings to extract coherent features of text that demonstrate performance differences between two language models.", "result": "The method successfully identifies specific contexts where one model outperforms another, providing insights that are not captured by traditional measures like corpus-level perplexity.", "conclusion": "BehaviorBox enhances the evaluation of language models by revealing meaningful performance differences in fine-grained contexts.", "key_contributions": ["Introduction of the BehaviorBox methodology for automated LM comparison", "Identification of fine-grained textual features where model performance differs", "Insights into model performance that traditional metrics fail to capture"], "limitations": "", "keywords": ["language models", "automated evaluation", "contextual embeddings", "performance comparison", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02987", "pdf": "https://arxiv.org/pdf/2506.02987.pdf", "abs": "https://arxiv.org/abs/2506.02987", "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis", "authors": ["Richard Armitage"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "12 pages, 1 Table", "summary": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.", "AI": {"tldr": "This paper evaluates the performance of leading large language models in answering primary care examination questions, showing they perform significantly better than GPs.", "motivation": "To explore the capabilities of advanced LLMs in the medical field, particularly in the context of primary care education and examination.", "method": "Four LLMs (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) were tested on 100 multiple choice questions from the Royal College of General Practitioners GP SelfTest, with responses scored against correct answers.", "result": "The models achieved scores of 99.0%, 95.0%, 95.0%, and 95.0% respectively, outperforming the average peer score of 73.0%.", "conclusion": "The results indicate that LLMs can effectively support primary care, particularly those trained on relevant clinical data, with implications for their integration in medical training and practice.", "key_contributions": ["Demonstrated the capabilities of leading LLMs in answering primary care exam questions.", "Showed significant performance improvement over average GP scores.", "Strengthened the argument for integrating reasoning LLMs in clinical practice."], "limitations": "", "keywords": ["Large language models", "Primary care", "Medical education", "Clinical performance", "LLM applications"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.02212", "pdf": "https://arxiv.org/pdf/2506.02212.pdf", "abs": "https://arxiv.org/abs/2506.02212", "title": "Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics", "authors": ["Ella Rannon", "David Burstein"], "categories": ["cs.CL", "cs.AI", "q-bio.GN"], "comment": null, "summary": "Natural Language Processing (NLP) has transformed various fields beyond\nlinguistics by applying techniques originally developed for human language to\nthe analysis of biological sequences. This review explores the application of\nNLP methods to biological sequence data, focusing on genomics, transcriptomics,\nand proteomics. We examine how various NLP methods, from classic approaches\nlike word2vec to advanced models employing transformers and hyena operators,\nare being adapted to analyze DNA, RNA, protein sequences, and entire genomes.\nThe review also examines tokenization strategies and model architectures,\nevaluating their strengths, limitations, and suitability for different\nbiological tasks. We further cover recent advances in NLP applications for\nbiological data, such as structure prediction, gene expression, and\nevolutionary analysis, highlighting the potential of these methods for\nextracting meaningful insights from large-scale genomic data. As language\nmodels continue to advance, their integration into bioinformatics holds immense\npromise for advancing our understanding of biological processes in all domains\nof life.", "AI": {"tldr": "This review discusses the application of NLP techniques to biological sequence data, including genomics and proteomics, and evaluates various models and their effectiveness in this context.", "motivation": "To explore how NLP methods, traditionally used for human language, can be beneficial in analyzing biological sequences and large-scale genomic data.", "method": "The review examines different NLP methods such as word2vec and transformer models, along with tokenization strategies and model architectures suited for biological tasks.", "result": "The paper highlights the integration of NLP methods into bioinformatics and their advancements in applications like gene expression and structure prediction.", "conclusion": "The integration of advanced NLP techniques into bioinformatics shows great potential to advance our understanding of biological processes.", "key_contributions": ["Assessment of various NLP models for biological applications", "Evaluation of tokenization strategies relevant to genomics", "Discussion of recent advancements in NLP for bioinformatics"], "limitations": "", "keywords": ["Natural Language Processing", "biological sequences", "bioinformatics", "genomics", "NLP applications"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.02239", "pdf": "https://arxiv.org/pdf/2506.02239.pdf", "abs": "https://arxiv.org/abs/2506.02239", "title": "Investigating the Impact of Word Informativeness on Speech Emotion Recognition", "authors": ["Sofoklis Kakouros"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "In emotion recognition from speech, a key challenge lies in identifying\nspeech signal segments that carry the most relevant acoustic variations for\ndiscerning specific emotions. Traditional approaches compute functionals for\nfeatures such as energy and F0 over entire sentences or longer speech portions,\npotentially missing essential fine-grained variation in the long-form\nstatistics. This research investigates the use of word informativeness, derived\nfrom a pre-trained language model, to identify semantically important segments.\nAcoustic features are then computed exclusively for these identified segments,\nenhancing emotion recognition accuracy. The methodology utilizes standard\nacoustic prosodic features, their functionals, and self-supervised\nrepresentations. Results indicate a notable improvement in recognition\nperformance when features are computed on segments selected based on word\ninformativeness, underscoring the effectiveness of this approach.", "AI": {"tldr": "This paper presents a method for enhancing emotion recognition from speech by identifying semantically important segments using word informativeness from a pre-trained language model.", "motivation": "The study addresses the challenge of effectively identifying speech segments that carry relevant acoustic variations for emotion recognition.", "method": "The approach combines standard acoustic prosodic features with word informativeness derived from a pre-trained language model to selectively compute features on important speech segments.", "result": "The method resulted in a significant improvement in emotion recognition accuracy by focusing on semantically informative segments rather than entire sentences.", "conclusion": "The use of word informativeness for segment selection enhances emotion recognition performance, demonstrating a novel approach in speech processing.", "key_contributions": ["Introduction of word informativeness for segment selection in emotion recognition", "Demonstration of improved accuracy in emotion recognition tasks", "Utilization of self-supervised representations in analysis"], "limitations": "", "keywords": ["emotion recognition", "speech processing", "word informativeness", "acoustic features", "language model"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2503.24160", "pdf": "https://arxiv.org/pdf/2503.24160.pdf", "abs": "https://arxiv.org/abs/2503.24160", "title": "A Comparative Study of Scanpath Models in Graph-Based Visualization", "authors": ["Angela Lopez-Cardona", "Parvin Emami", "Sebastian Idesis", "Saravanakumar Duraisamy", "Luis A. Leiva", "Ioannis Arapakis"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Information Visualization (InfoVis) systems utilize visual representations to\nenhance data interpretation. Understanding how visual attention is allocated is\nessential for optimizing interface design. However, collecting Eye-tracking\n(ET) data presents challenges related to cost, privacy, and scalability.\nComputational models provide alternatives for predicting gaze patterns, thereby\nadvancing InfoVis research. In our study, we conducted an ET experiment with 40\nparticipants who analyzed graphs while responding to questions of varying\ncomplexity within the context of digital forensics. We compared human scanpaths\nwith synthetic ones generated by models such as DeepGaze, UMSS, and Gazeformer.\nOur research evaluates the accuracy of these models and examines how question\ncomplexity and number of nodes influence performance. This work contributes to\nthe development of predictive modeling in visual analytics, offering insights\nthat can enhance the design and effectiveness of InfoVis systems.", "AI": {"tldr": "This study evaluates computational models for predicting gaze patterns in Information Visualization (InfoVis) systems using eye-tracking data from participants analyzing graphs.", "motivation": "The need to understand visual attention allocation to optimize interface design and the challenges of collecting eye-tracking data.", "method": "Conducted an eye-tracking experiment with 40 participants, comparing human scanpaths to synthetic ones generated by models such as DeepGaze, UMSS, and Gazeformer.", "result": "The study found that the computational models could approximate human gaze patterns, with performance influenced by question complexity and node number.", "conclusion": "This research contributes to predictive modeling in visual analytics, providing insights for better InfoVis system design.", "key_contributions": ["Evaluation of gaze prediction models in InfoVis", "Analysis of the impact of question complexity on gaze patterns", "Advancement of predictive modeling in visual analytics"], "limitations": "Potential limitations include the specificity of the context (digital forensics) and variability in individual gaze behavior.", "keywords": ["Information Visualization", "Eye-tracking", "Gaze prediction", "Model evaluation", "Visual analytics"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.02264", "pdf": "https://arxiv.org/pdf/2506.02264.pdf", "abs": "https://arxiv.org/abs/2506.02264", "title": "CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment", "authors": ["Radin Shayanfar", "Chu Fei Luo", "Rohan Bhambhoria", "Samuel Dahan", "Xiaodan Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "It is often challenging to teach specialized, unseen tasks to dialogue\nsystems due to the high cost of expert knowledge, training data, and high\ntechnical difficulty. To support domain-specific applications - such as law,\nmedicine, or finance - it is essential to build frameworks that enable\nnon-technical experts to define, test, and refine system behaviour with minimal\neffort. Achieving this requires cross-disciplinary collaboration between\ndevelopers and domain specialists. In this work, we introduce a novel\nframework, CoDial (Code for Dialogue), that converts expert knowledge,\nrepresented as a novel structured heterogeneous graph, into executable\nconversation logic. CoDial can be easily implemented in existing guardrailing\nlanguages, such as Colang, to enable interpretable, modifiable, and true\nzero-shot specification of task-oriented dialogue systems. Empirically, CoDial\nachieves state-of-the-art performance on the STAR dataset for inference-based\nmodels and is competitive with similar baselines on the well-known MultiWOZ\ndataset. We also demonstrate CoDial's iterative improvement via manual and\nLLM-aided feedback, making it a practical tool for expert-guided alignment of\nLLMs in high-stakes domains.", "AI": {"tldr": "CoDial is a framework enabling non-technical experts to define and refine dialogue systems using structured graphs and supports zero-shot task specification.", "motivation": "To facilitate the creation of dialogue systems for specialized domains like law and medicine without requiring extensive technical expertise or large amounts of training data.", "method": "CoDial converts expert knowledge into executable conversation logic using a structured heterogeneous graph and integrates with guardrailing languages for task-oriented dialogue.", "result": "CoDial achieves state-of-the-art performance on the STAR dataset and competes well on the MultiWOZ dataset while allowing for iterative improvements through expert and LLM feedback.", "conclusion": "CoDial is a practical tool that supports the expert-guided alignment of LLMs in critical domains, making dialogue systems more accessible to non-experts.", "key_contributions": ["Introduction of CoDial framework for dialogue systems", "Utilization of a structured heterogeneous graph for task specification", "Demonstration of iterative improvement through manual and LLM-aided feedback"], "limitations": "", "keywords": ["dialogue systems", "human-computer interaction", "LLM", "task-oriented dialogue", "expert feedback"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.13880", "pdf": "https://arxiv.org/pdf/2504.13880.pdf", "abs": "https://arxiv.org/abs/2504.13880", "title": "An AI-powered Public Health Automated Kiosk System for Personalized Care: An Experimental Pilot Study", "authors": ["Sonya Falahati", "Morteza Alizadeh", "Fatemeh Ghazipour", "Zhino Safahi", "Navid Khaledian", "Mohammad R. Salmanpour"], "categories": ["cs.HC", "F.2.2; I.2.7"], "comment": "16 pages, 3 figures, 2 tables", "summary": "Background: The HERMES Kiosk (Healthcare Enhanced Recommendations through\nArtificial Intelligence & Expertise System) is designed to provide personalized\nOver-the-Counter (OTC) medication recommendations, addressing the limitations\nof traditional health kiosks. It integrates an advanced GAMENet model enhanced\nwith Graph Attention Networks (GAT) and Multi-Head Cross-Attention (MHCA) while\nensuring user privacy through federated learning. This paper outlines the\nconceptual design and architecture of HERMES, with a focus on deployment in\nhigh-traffic public areas. Methods: HERMES analyzes self-reported symptoms and\nanonymized medical histories using AI algorithms to generate context-aware OTC\nmedication recommendations. The system was initially trained using Electronic\nHealth Records (EHR) from the MIMIC-III dataset (6,350 patients) and Drug-Drug\nInteraction (DDI) data from the TWOSIDES database, incorporating the top 90\nseverity DDI types. Real-time DDI checks and ATC-mapped drug codes further\nimprove safety. The kiosk is designed for accessibility, offering multilingual\nsupport, large fonts, voice commands, and Braille compatibility. A built-in\nhealth education library promotes preventive care and health literacy. A survey\nwas conducted among 10 medical professionals to evaluate its potential\napplications in medicine. Results: Preliminary results show that the enhanced\nGAMENet model achieved a Precision-Recall AUC (PRAUC) of 0.74, outperforming\nthe original model. These findings suggest a strong potential for delivering\naccurate and secure healthcare recommendations in public settings. Conclusion:\nHERMES demonstrates how AI-driven, privacy-preserving kiosks can enhance public\nhealth access, empower users, and alleviate burdens on healthcare systems.\nFuture work will focus on real-world deployment, usability testing, and\nscalability for broader adoption.", "AI": {"tldr": "The HERMES Kiosk provides personalized OTC medication recommendations using an AI model enhanced with GAT and MHCA, while ensuring user privacy through federated learning.", "motivation": "Address the limitations of traditional health kiosks by providing AI-driven personalized OTC medication recommendations in public areas.", "method": "HERMES analyzes self-reported symptoms and anonymized medical histories using AI; trained on EHR and DDI data, with emphasis on user privacy and accessibility features.", "result": "The enhanced GAMENet model achieved a PRAUC of 0.74, indicating improved accuracy in healthcare recommendations over previous models.", "conclusion": "HERMES showcases the potential of AI-powered kiosks to enhance public health access and reduce healthcare system burdens, with future efforts aimed at deployment and scalability.", "key_contributions": ["Implementation of GAMENet model with GAT and MHCA", "Privacy-preserving design using federated learning", "Multilingual support and accessibility features for public health kiosks"], "limitations": "", "keywords": ["HERMES Kiosk", "OTC medication", "Artificial Intelligence", "Federated Learning", "Public Health"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2506.02279", "pdf": "https://arxiv.org/pdf/2506.02279.pdf", "abs": "https://arxiv.org/abs/2506.02279", "title": "ImpRAG: Retrieval-Augmented Generation with Implicit Queries", "authors": ["Wenzheng Zhang", "Xi Victoria Lin", "Karl Stratos", "Wen-tau Yih", "Mingda Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval\nand generation as separate processes, requiring explicit textual queries to\nconnect them. This separation can limit the ability of models to generalize\nacross diverse tasks. In this work, we propose a query-free RAG system, named\nImpRAG, which integrates retrieval and generation into a unified model. ImpRAG\nallows models to implicitly express their information needs, eliminating the\nneed for human-specified queries. By dividing pretrained decoder-only language\nmodels into specialized layer groups, ImpRAG optimizes retrieval and generation\ntasks simultaneously. Our approach employs a two-stage inference process, using\nthe same model parameters and forward pass for both retrieval and generation,\nthereby minimizing the disparity between retrievers and language models.\nExperiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves\n3.6-11.5 improvements in exact match scores on unseen tasks with diverse\nformats, highlighting its effectiveness in enabling models to articulate their\nown information needs and generalize across tasks. Our analysis underscores the\nimportance of balancing retrieval and generation parameters and leveraging\ngeneration perplexities as retrieval training objectives for enhanced\nperformance.", "AI": {"tldr": "ImpRAG is a unified RAG system that integrates retrieval and generation without explicit queries, improving model generalization across diverse tasks.", "motivation": "To overcome limitations in traditional RAG systems that treat retrieval and generation separately, which can hinder model generalization.", "method": "ImpRAG integrates retrieval and generation into a single model, allowing implicit expression of information needs. It utilizes a two-stage inference process with shared model parameters to optimize both tasks simultaneously.", "result": "ImpRAG achieved 3.6-11.5 improvements in exact match scores on diverse knowledge-intensive tasks, demonstrating improved generalization capabilities.", "conclusion": "The approach highlights the necessity of balancing retrieval and generation parameters and using generation perplexities as training objectives to enhance model performance.", "key_contributions": ["Development of ImpRAG, a query-free RAG system", "Unified model for simultaneous retrieval and generation", "Demonstrated improvements in task generalization through empirical experiments"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "query-free system", "language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02283", "pdf": "https://arxiv.org/pdf/2506.02283.pdf", "abs": "https://arxiv.org/abs/2506.02283", "title": "Sounding Like a Winner? Prosodic Differences in Post-Match Interviews", "authors": ["Sofoklis Kakouros", "Haoyu Chen"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "This study examines the prosodic characteristics associated with winning and\nlosing in post-match tennis interviews. Additionally, this research explores\nthe potential to classify match outcomes solely based on post-match interview\nrecordings using prosodic features and self-supervised learning (SSL)\nrepresentations. By analyzing prosodic elements such as pitch and intensity,\nalongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine\nwhether an athlete has won or lost their match. Traditional acoustic features\nand deep speech representations are extracted from the data, and machine\nlearning classifiers are employed to distinguish between winning and losing\nplayers. Results indicate that SSL representations effectively differentiate\nbetween winning and losing outcomes, capturing subtle speech patterns linked to\nemotional states. At the same time, prosodic cues -- such as pitch variability\n-- remain strong indicators of victory.", "AI": {"tldr": "This study analyzes prosodic features in post-match tennis interviews to classify match outcomes using self-supervised learning models.", "motivation": "The motivation is to explore how prosodic characteristics can indicate the emotional states of athletes based on match outcomes.", "method": "The study analyzes pitch and intensity in interview recordings, applying self-supervised learning models like Wav2Vec 2.0 and HuBERT to classify winning and losing outcomes.", "result": "The results show that SSL representations successfully differentiate between winning and losing outcomes by capturing subtle speech patterns, with prosodic cues such as pitch variability being strong indicators.", "conclusion": "The research concludes that prosodic features are significant for indicating match outcomes and can be classified using advanced SSL techniques.", "key_contributions": ["Introduces the classification of tennis match outcomes based on prosodic features of post-match interviews", "Demonstrates the efficacy of self-supervised learning models in emotional recognition through speech", "Highlights the importance of pitch variability in determining winners and losers."], "limitations": "", "keywords": ["prosody", "self-supervised learning", "speech classification", "tennis", "emotional states"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.02298", "pdf": "https://arxiv.org/pdf/2506.02298.pdf", "abs": "https://arxiv.org/abs/2506.02298", "title": "LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback", "authors": ["Thai Hoang", "Kung-Hsiang Huang", "Shirley Kokane", "Jianguo Zhang", "Zuxin Liu", "Ming Zhu", "Jake Grigsby", "Tian Lan", "Michael S Ryoo", "Chien-Sheng Wu", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong", "Juan Carlos Niebles"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "LAM Simulator framework for agentic data generation", "summary": "Large Action Models (LAMs) for AI Agents offer incredible potential but face\nchallenges due to the need for high-quality training data, especially for\nmulti-steps tasks that involve planning, executing tool calls, and responding\nto feedback. To address these issues, we present LAM SIMULATOR, a comprehensive\nframework designed for online exploration of agentic tasks with high-quality\nfeedback. Our framework features a dynamic task query generator, an extensive\ncollection of tools, and an interactive environment where Large Language Model\n(LLM) Agents can call tools and receive real-time feedback. This setup enables\nLLM Agents to explore and solve tasks autonomously, facilitating the discovery\nof multiple approaches to tackle any given task. The resulting action\ntrajectory data are then used to create high-quality training datasets for\nLAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,\nhighlight the effectiveness of LAM SIMULATOR: models trained with\nself-generated datasets using our framework achieve significant performance\ngains, up to a 49.3\\% improvement over their original baselines. LAM SIMULATOR\nrequires minimal human input during dataset creation, highlighting LAM\nSIMULATOR's efficiency and effectiveness in speeding up development of AI\nagents.", "AI": {"tldr": "LAM SIMULATOR is a framework for generating high-quality training data for Large Action Models (LAMs) and AI agents through online exploration of tasks.", "motivation": "To address the challenge of generating high-quality training data for LAMs, especially for complex multi-step tasks requiring effective planning and execution.", "method": "The framework includes a dynamic task query generator, a collection of tools, and an interactive environment for LLM Agents to autonomously explore tasks and receive feedback.", "result": "Models trained with datasets generated by LAM SIMULATOR showed up to a 49.3% improvement over original performance baselines on benchmarks ToolBench and CRMArena.", "conclusion": "LAM SIMULATOR is efficient and effective for speeding up the development of AI agents, requiring minimal human input for dataset creation.", "key_contributions": ["Introduces LAM SIMULATOR for autonomous task-solving using LLM Agents.", "Demonstrates significant performance improvements in LAMs with self-generated datasets.", "Offers a minimal human input solution for AI agent training data generation."], "limitations": "", "keywords": ["Large Action Models", "AI Agents", "Training Data Generation", "Feedback Mechanism", "Interactive Environment"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.02302", "pdf": "https://arxiv.org/pdf/2506.02302.pdf", "abs": "https://arxiv.org/abs/2506.02302", "title": "Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments", "authors": ["Russell Scheinberg", "Ameeta Agrawal", "Amber Shore", "So Young Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Findings", "summary": "Large language models (LLMs) can explain grammatical rules, yet they often\nfail to apply those rules when judging sentence acceptability. We present\n\"grammar prompting\", an explain-then-process paradigm: a large LLM first\nproduces a concise explanation of the relevant syntactic phenomenon, then that\nexplanation is fed back as additional context to the target model -- either an\nLLM or a smaller language model (SLM) -- before deciding which sentence of a\nminimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian\nRuBLiMP benchmarks, this simple prompt design yields substantial improvements\nover strong baselines across many syntactic phenomena. Feeding an LLM's\nmetalinguistic explanation back to the target model bridges the gap between\nknowing a rule and using it. On SLMs, grammar prompting alone trims the average\nLLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by\n56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,\nlanguage-agnostic cue lets low-cost SLMs approach frontier-LLM performance in\nmultilingual settings.", "AI": {"tldr": "This paper introduces 'grammar prompting', a method to enhance large language models' (LLMs) ability to judge sentence acceptability by first generating explanations of syntactic rules before applying them to sentence comparison tasks.", "motivation": "LLMs can explain grammatical rules but struggle to apply them accurately when assessing sentence acceptability. This research aims to address this gap and improve grammar judgment performance.", "method": "The study employs a method called 'grammar prompting', where an LLM explains the relevant syntactic phenomenon, and this explanation is then used as context for a target model (either another LLM or a smaller language model, SLM) to make decisions about sentence acceptability.", "result": "Grammar prompting significantly enhances performance on syntactic judgment tasks across multiple language benchmarks, reducing the LLM-SLM accuracy gap by approximately 20%, and when combined with chain-of-thought reasoning, by 56%.", "conclusion": "The findings demonstrate that providing metalinguistic explanations to SLMs allows them to perform comparably to larger LLMs in multilingual environments with minimal computational cost.", "key_contributions": ["Introduction of grammar prompting as a novel technique in LLMs", "Demonstrated performance gains across multiple language benchmarks", "The lightweight method allows SLMs to achieve near state-of-the-art results"], "limitations": "", "keywords": ["Grammar prompting", "Large language models", "Syntactic phenomena"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.02321", "pdf": "https://arxiv.org/pdf/2506.02321.pdf", "abs": "https://arxiv.org/abs/2506.02321", "title": "Quantifying Misattribution Unfairness in Authorship Attribution", "authors": ["Pegah Alipoormolabashi", "Ajay Patel", "Niranjan Balasubramanian"], "categories": ["cs.CL"], "comment": null, "summary": "Authorship misattribution can have profound consequences in real life. In\nforensic settings simply being considered as one of the potential authors of an\nevidential piece of text or communication can result in undesirable scrutiny.\nThis raises a fairness question: Is every author in the candidate pool at equal\nrisk of misattribution? Standard evaluation measures for authorship attribution\nsystems do not explicitly account for this notion of fairness. We introduce a\nsimple measure, Misattribution Unfairness Index (MAUIk), which is based on how\noften authors are ranked in the top k for texts they did not write. Using this\nmeasure we quantify the unfairness of five models on two different datasets.\nAll models exhibit high levels of unfairness with increased risks for some\nauthors. Furthermore, we find that this unfairness relates to how the models\nembed the authors as vectors in the latent search space. In particular, we\nobserve that the risk of misattribution is higher for authors closer to the\ncentroid (or center) of the embedded authors in the haystack. These results\nindicate the potential for harm and the need for communicating with and\ncalibrating end users on misattribution risk when building and providing such\nmodels for downstream use.", "AI": {"tldr": "The paper introduces the Misattribution Unfairness Index (MAUIk) to measure unfairness in authorship attribution systems, highlighting unequal risks of misattribution among authors.", "motivation": "To address the fairness of authorship attribution systems and the risks of misattribution for different authors in forensic contexts.", "method": "The authors introduce the Misattribution Unfairness Index (MAUIk) to quantify unfairness by measuring how often authors are incorrectly attributed as the top writers for texts they did not write, across five models and two datasets.", "result": "All tested authorship attribution models displayed high levels of unfairness, with risks disproportionately affecting some authors, particularly those closer to the centroid of the author embedding space.", "conclusion": "The study underscores the need to communicate misattribution risks and to calibrate models for ethical usage, especially considering potential harm to authors at higher risk.", "key_contributions": ["Introduction of the Misattribution Unfairness Index (MAUIk) for evaluating authorship attribution models.", "Demonstration of unfairness in existing models across various datasets.", "Identification of the relationship between author embedding positions and misattribution risk."], "limitations": "The study is limited to five models and two datasets, which may not represent all cases of authorship attribution.", "keywords": ["authorship attribution", "fairness", "misattribution", "machine learning", "forensic analysis"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.02326", "pdf": "https://arxiv.org/pdf/2506.02326.pdf", "abs": "https://arxiv.org/abs/2506.02326", "title": "Something Just Like TRuST : Toxicity Recognition of Span and Target", "authors": ["Berk Atil", "Namrata Sureddy", "Rebecca J. Passonneau"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Toxicity in online content, including content generated by language models,\nhas become a critical concern due to its potential for negative psychological\nand social impact. This paper introduces TRuST, a comprehensive dataset\ndesigned to improve toxicity detection that merges existing datasets, and has\nlabels for toxicity, target social group, and toxic spans. It includes a\ndiverse range of target groups such as ethnicity, gender, religion, disability,\nand politics, with both human/machine-annotated and human machine-generated\ndata. We benchmark state-of-the-art large language models (LLMs) on toxicity\ndetection, target group identification, and toxic span extraction. We find that\nfine-tuned models consistently outperform zero-shot and few-shot prompting,\nthough performance remains low for certain social groups. Further, reasoning\ncapabilities do not significantly improve performance, indicating that LLMs\nhave weak social reasoning skills.", "AI": {"tldr": "This paper presents TRuST, a dataset for toxicity detection, benchmarking LLMs on various toxicity-related tasks.", "motivation": "To address the critical issue of toxicity in online content, especially that generated by language models.", "method": "A comprehensive dataset merging existing ones, labeled for toxicity, target social groups, and toxic spans; benchmarking was done with state-of-the-art LLMs on toxicity detection tasks.", "result": "Fine-tuned models outperform zero-shot and few-shot prompting, but still show low performance for certain social groups, revealing weaknesses in social reasoning skills of LLMs.", "conclusion": "Improved datasets are needed to enhance performance in toxicity detection across diverse social groups, and LLMs require better social reasoning abilities.", "key_contributions": ["Introduction of the TRuST dataset for toxicity detection.", "Benchmarking of LLMs on toxicity tasks.", "Insights into the social reasoning capabilities of LLMs."], "limitations": "Performance remains low for specific social groups and reasoning capabilities do not significantly boost outcomes.", "keywords": ["toxicity detection", "language models", "social reasoning", "dataset", "benchmarking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02338", "pdf": "https://arxiv.org/pdf/2506.02338.pdf", "abs": "https://arxiv.org/abs/2506.02338", "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL", "authors": ["Hyungjoo Chae", "Dongjin Kang", "Jihyuk Kim", "Beong-woo Kwak", "Sunghyun Park", "Haeju Park", "Jinyoung Yeo", "Moontae Lee", "Kyungjae Lee"], "categories": ["cs.CL"], "comment": "ACL 2025 Industry", "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.", "AI": {"tldr": "This paper introduces the Long CoT Collection, a dataset of 100K chain-of-thought rationales designed to support independent development of large reasoning models (LRMs) without relying on existing models like R1.", "motivation": "The reliance on existing large reasoning models (LRMs) such as R1 limits the advancement of the field; thus, developing independent LRM capabilities is essential.", "method": "The authors present a dataset consisting of long chain-of-thought (CoT) rationales annotated by existing short CoT LLMs and develop a pipeline to induce novel reasoning strategies to enhance long reasoning abilities.", "result": "The Long CoT Collection yields CoT rationales of quality comparable to R1 and demonstrates that training on this dataset significantly boosts general reasoning skills, allowing substantial gains in reinforcement learning.", "conclusion": "Training on the Long CoT Collection not only strengthens reasoning skills but also lays a strong foundation for improved reinforcement learning performance.", "key_contributions": ["Introduction of the Long CoT Collection dataset", "Induction of novel reasoning strategies into short CoT LLMs", "Demonstration of significant gains in RL models initialized on the dataset."], "limitations": "The dataset's quality is slightly below R1, indicating room for improvement.", "keywords": ["Long Chain-of-Thought", "Large Reasoning Models", "Dataset", "Reinforcement Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02347", "pdf": "https://arxiv.org/pdf/2506.02347.pdf", "abs": "https://arxiv.org/abs/2506.02347", "title": "STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation", "authors": ["Jiaming Li", "Yukun Chen", "Ziqiang Liu", "Minghuan Tan", "Lei Zhang", "Yunshui Li", "Run Luo", "Longze Chen", "Jing Luo", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Wei Zhou", "Min Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Stories are central to human culture, serving to share ideas, preserve\ntraditions, and foster connections. Automatic story generation, a key\nadvancement in artificial intelligence (AI), offers new possibilities for\ncreating personalized content, exploring creative ideas, and enhancing\ninteractive experiences. However, existing methods struggle to maintain\nnarrative coherence and logical consistency. This disconnect compromises the\noverall storytelling experience, underscoring the need for substantial\nimprovements. Inspired by human cognitive processes, we introduce Storyteller,\na novel approach that systemically improves the coherence and consistency of\nautomatically generated stories. Storyteller introduces a plot node structure\nbased on linguistically grounded subject verb object (SVO) triplets, which\ncapture essential story events and ensure a consistent logical flow. Unlike\nprevious methods, Storyteller integrates two dynamic modules, the STORYLINE and\nnarrative entity knowledge graph (NEKG),that continuously interact with the\nstory generation process. This integration produces structurally sound,\ncohesive and immersive narratives. Extensive experiments demonstrate that\nStoryteller significantly outperforms existing approaches, achieving an 84.33%\naverage win rate through human preference evaluation. At the same time, it is\nalso far ahead in other aspects including creativity, coherence, engagement,\nand relevance.", "AI": {"tldr": "Storyteller is a novel approach for automatic story generation that enhances narrative coherence and consistency by utilizing a plot node structure based on SVO triplets and integrating dynamic modules for improved interaction.", "motivation": "The need for substantial improvements in narrative coherence and logical consistency in automatic story generation, as existing methods fail to provide a satisfactory storytelling experience.", "method": "The introduction of Storyteller, which employs a plot node structure grounded in SVO triplets and integrates STORYLINE and NEKG modules that interact dynamically during the story generation process.", "result": "Storyteller significantly outperforms existing methods, achieving an 84.33% average win rate in human preference evaluations, and excels in creativity, coherence, engagement, and relevance metrics.", "conclusion": "The proposed Storyteller approach offers a substantial advancement in creating coherent and immersive narratives through innovative structural and dynamic integrations.", "key_contributions": ["Introduction of a SVO triplet-based plot node structure", "Dynamic interaction of STORYLINE and NEKG modules", "Demonstrated superior performance in creativity and coherence compared to existing methods."], "limitations": "", "keywords": ["automatic story generation", "narrative coherence", "machine learning", "human preference evaluation", "storytelling"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.02350", "pdf": "https://arxiv.org/pdf/2506.02350.pdf", "abs": "https://arxiv.org/abs/2506.02350", "title": "Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection", "authors": ["Herun Wan", "Jiaying Wu", "Minnan Luo", "Zhi Zeng", "Zhixiong Su"], "categories": ["cs.CL"], "comment": null, "summary": "Misinformation detection models often rely on superficial cues (i.e.,\n\\emph{shortcuts}) that correlate with misinformation in training data but fail\nto generalize to the diverse and evolving nature of real-world misinformation.\nThis issue is exacerbated by large language models (LLMs), which can easily\ngenerate convincing misinformation through simple prompts. We introduce\nTruthOverTricks, a unified evaluation paradigm for measuring shortcut learning\nin misinformation detection. TruthOverTricks categorizes shortcut behaviors\ninto intrinsic shortcut induction and extrinsic shortcut injection, and\nevaluates seven representative detectors across 14 popular benchmarks, along\nwith two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.\nEmpirical results reveal that existing detectors suffer severe performance\ndegradation when exposed to both naturally occurring and adversarially crafted\nshortcuts. To address this, we propose SMF, an LLM-augmented data augmentation\nframework that mitigates shortcut reliance through paraphrasing, factual\nsummarization, and sentiment normalization. SMF consistently enhances\nrobustness across 16 benchmarks, encouraging models to rely on deeper semantic\nunderstanding rather than shortcut cues. To promote the development of\nmisinformation detectors, we have published the resources publicly at\nhttps://github.com/whr000001/TruthOverTricks.", "AI": {"tldr": "The paper introduces TruthOverTricks, a framework for evaluating and addressing shortcut learning in misinformation detection models, and proposes a data augmentation method (SMF) to improve robustness against various shortcuts.", "motivation": "Existing misinformation detection models are limited by their reliance on superficial cues that do not generalize well to the real-world context of misinformation.", "method": "The paper presents TruthOverTricks for evaluating shortcut learning, categorizes shortcut behaviors, and introduces SMF for data augmentation that includes paraphrasing and factual summarization.", "result": "Empirical results show existing detectors experience significant performance drops when facing intrinsic and extrinsic shortcuts, while SMF improves robustness across 16 benchmarks.", "conclusion": "The findings emphasize the need for deep semantic understanding in misinformation detection and provide publicly available resources to aid in research.", "key_contributions": ["Introduction of TruthOverTricks evaluation paradigm", "Identification of intrinsic and extrinsic shortcut behaviors", "Development of SMF data augmentation framework"], "limitations": "The evaluation may not cover all possible types of misinformation shortcuts, and the generalizability of the proposed methods to all contexts is yet to be established.", "keywords": ["misinformation detection", "shortcut learning", "data augmentation", "large language models", "semantic understanding"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02351", "pdf": "https://arxiv.org/pdf/2506.02351.pdf", "abs": "https://arxiv.org/abs/2506.02351", "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "summary": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond.", "AI": {"tldr": "DIAMOND is an LLM-driven agent for baseball highlight summarization that combines structured sports analytics with natural language reasoning to improve play importance assessment and storytelling in sports highlights.", "motivation": "Traditional approaches to sports highlight summarization often miss strategic depth and momentum shifts. Manual curation is resource-intensive and not scalable, necessitating an automated solution that combines analytics and narrative.", "method": "DIAMOND integrates sabermetric features like Win Expectancy and Leverage Index with an LLM for context-aware play selection, ensuring both quantitative rigor and qualitative storytelling.", "result": "DIAMOND achieves an F1-score improvement from 42.9% to 84.8% compared to traditional WPA-only approaches and outperforms commercial and statistical baselines on Korean Baseball Organization League games.", "conclusion": "The results suggest that modular, interpretable agent-based frameworks have the potential for effective event-level summarization in sports and potentially other areas.", "key_contributions": ["Introduction of a hybrid LLM-driven summarization approach for sports highlights", "Integration of structured analytics with natural language reasoning", "Significant improvement in summarization performance metrics over traditional methods"], "limitations": "Limited in scale; results based on a small set of games.", "keywords": ["LLM", "highlight summarization", "sports analytics", "natural language reasoning", "baseball"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.02372", "pdf": "https://arxiv.org/pdf/2506.02372.pdf", "abs": "https://arxiv.org/abs/2506.02372", "title": "AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output", "authors": ["Hisami Suzuki", "Satoru Katsumata", "Takashi Kodama", "Tetsuro Takahashi", "Kouta Nakayama", "Satoshi Sekine"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper we present AnswerCarefully, a dataset for promoting the safety\nand appropriateness of Japanese LLM outputs. The dataset consists of 1,800\npairs of questions and reference answers, where the questions require special\nattention in answering. It covers a wide range of risk categories established\nin prior English-language datasets, but the data samples are original in that\nthey are manually created to reflect the socio-cultural context of LLM usage in\nJapan. We show that using this dataset for instruction to fine-tune a Japanese\nLLM led to improved output safety without compromising the utility of general\nresponses. We also report the results of a safety evaluation of 12 Japanese\nLLMs using this dataset as a benchmark. Finally, we describe the latest update\non the dataset which provides English translations and annotations of the\nquestions, aimed at facilitating the derivation of similar datasets in\ndifferent languages and regions.", "AI": {"tldr": "This paper introduces AnswerCarefully, a dataset designed to enhance the safety and appropriateness of Japanese LLM outputs through a set of 1,800 question-answer pairs reflecting local socio-cultural contexts.", "motivation": "To improve the safety and appropriateness of outputs from Japanese LLMs by providing a tailored dataset that reflects socio-cultural contexts.", "method": "The paper presents a dataset of 1,800 question-answer pairs specifically designed for Japanese LLMs, followed by a fine-tuning process of a Japanese LLM using this dataset and a safety evaluation of 12 Japanese LLMs.", "result": "The fine-tuned Japanese LLM demonstrated improved output safety without compromising the quality of general responses, and the dataset serves as a benchmark for evaluating LLM safety.", "conclusion": "The dataset not only improves safety for Japanese LLMs but also promotes the creation of similar datasets for other languages and regions through its English translations and annotations.", "key_contributions": ["Introduction of AnswerCarefully dataset for Japanese LLMs.", "Demonstration of improved safety in LLM outputs after fine-tuning.", "Provision of English translations to aid the development of multilingual datasets."], "limitations": "", "keywords": ["Japanese LLM", "safety evaluation", "fine-tuning", "dataset", "multilingual applications"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.02378", "pdf": "https://arxiv.org/pdf/2506.02378.pdf", "abs": "https://arxiv.org/abs/2506.02378", "title": "Exploring Explanations Improves the Robustness of In-Context Learning", "authors": ["Ukyo Honda", "Tatsushi Oka"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "In-context learning (ICL) has emerged as a successful paradigm for leveraging\nlarge language models (LLMs). However, it often struggles to generalize beyond\nthe distribution of the provided demonstrations. A recent advancement in\nenhancing robustness is ICL with explanations (X-ICL), which improves\nprediction reliability by guiding LLMs to understand and articulate the\nreasoning behind correct labels. Building on this approach, we introduce an\nadvanced framework that extends X-ICL by systematically exploring explanations\nfor all possible labels (X$^2$-ICL), thereby enabling more comprehensive and\nrobust decision-making. Experimental results on multiple natural language\nunderstanding datasets validate the effectiveness of X$^2$-ICL, demonstrating\nsignificantly improved robustness to out-of-distribution data compared to the\nexisting ICL approaches.", "AI": {"tldr": "Introduction of X²-ICL, enhancing robustness of large language models by exploring explanations for all possible labels.", "motivation": "To improve generalization and robustness of in-context learning in large language models, especially when faced with out-of-distribution data.", "method": "The paper proposes X²-ICL, which systematically explores explanations for all possible labels compared to the traditional X-ICL approach.", "result": "X²-ICL shows significantly improved robustness on various natural language understanding datasets, outperforming existing in-context learning methods.", "conclusion": "The studies validate that X²-ICL enhances decision-making capabilities in language models, making them more reliable amidst data variability.", "key_contributions": ["Introduction of the X²-ICL framework", "Demonstration of improved robustness in LLMs", "Systematic exploration of all possible label explanations"], "limitations": "", "keywords": ["In-context learning", "Large language models", "Robustness", "Natural language understanding", "Explanations"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02391", "pdf": "https://arxiv.org/pdf/2506.02391.pdf", "abs": "https://arxiv.org/abs/2506.02391", "title": "Consultant Decoding: Yet Another Synergistic Mechanism", "authors": ["Chuanghao Ding", "Jiaping Wang", "Ziqing Yang", "Xiaoliang Wang", "Dahua Lin", "Cam-Tu Nguyen", "Fei Tan"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 findings", "summary": "The synergistic mechanism based on Speculative Decoding (SD) has garnered\nconsiderable attention as a simple yet effective approach for accelerating the\ninference of large language models (LLMs). Nonetheless, the high rejection\nrates require repeated LLMs calls to validate draft tokens, undermining the\noverall efficiency gain of SD. In this work, we revisit existing verification\nmechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).\nUnlike SD, which relies on a metric derived from importance sampling for\nverification, CD verifies candidate drafts using token-level likelihoods\ncomputed solely by the LLM. CD achieves up to a 2.5-fold increase in inference\nspeed compared to the target model, while maintaining comparable generation\nquality (around 100% of the target model's performance). Interestingly, this is\nachieved by combining models whose parameter sizes differ by two orders of\nmagnitude. In addition, CD reduces the call frequency of the large target model\nto below 10%, particularly in more demanding tasks. CD's performance was even\nfound to surpass that of the large target model, which theoretically represents\nthe upper bound for speculative decoding.", "AI": {"tldr": "This paper introduces Consultant Decoding (CD), a new mechanism that improves the efficiency of inference speed for large language models while maintaining performance quality.", "motivation": "The paper addresses the inefficiency caused by high rejection rates in Speculative Decoding (SD) for large language models (LLMs) and aims to provide a more effective verification mechanism.", "method": "CD verifies candidate drafts using token-level likelihoods computed solely by the LLM, rather than using importance sampling as in SD.", "result": "CD achieves up to a 2.5-fold increase in inference speed compared to the target model, while maintaining performance quality at about 100% of the target model's performance.", "conclusion": "CD significantly enhances inference efficiency by combining models of varying parameter sizes and reducing reliance on the large model in lower than 10% of calls in demanding tasks.", "key_contributions": ["Introduction of Consultant Decoding (CD) as a novel verification mechanism.", "Demonstration of a 2.5-fold speed increase in LLM inference compared to traditional methods.", "Reduction of call frequency for large models to below 10% in challenging tasks."], "limitations": "", "keywords": ["Consultant Decoding", "Speculative Decoding", "Language Models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02404", "pdf": "https://arxiv.org/pdf/2506.02404.pdf", "abs": "https://arxiv.org/abs/2506.02404", "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qianwen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community.", "AI": {"tldr": "GraphRAG-Bench is introduced as a benchmark for evaluating Graph Retrieval Augmented Generation models in a structured way that emphasizes multi-hop reasoning and diverse task coverage.", "motivation": "To address limited evaluations of GraphRAG models that rely on traditional datasets, which do not comprehensively assess reasoning capabilities.", "method": "Developing a large-scale, domain-specific benchmark with challenging questions across 16 disciplines and a holistic evaluation framework for GraphRAG models.", "result": "The applied benchmark revealed insights into the relationship between graph-based structuring and improved reasoning capabilities across various GraphRAG methods.", "conclusion": "GraphRAG-Bench enhances the evaluation of GraphRAG models by focusing on complex reasoning tasks and providing a comprehensive assessment of the entire GraphRAG pipeline.", "key_contributions": ["Introduction of a new benchmark for GraphRAG evaluations", "Inclusion of multi-hop reasoning in question design", "Comprehensive evaluation framework covering the full GraphRAG process."], "limitations": "", "keywords": ["Graph Retrieval Augmented Generation", "GraphRAG-Bench", "benchmarking"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.02412", "pdf": "https://arxiv.org/pdf/2506.02412.pdf", "abs": "https://arxiv.org/abs/2506.02412", "title": "SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning", "authors": ["Zhengyuan Liu", "Geyu Lin", "Hui Li Tan", "Huayun Zhang", "Yanfeng Lu", "Xiaoxue Gao", "Stella Xin Yin", "He Sun", "Hock Huan Goh", "Lung Hsiang Wong", "Nancy F. Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Industry Track", "summary": "The integration of generative artificial intelligence into educational\napplications has enhanced personalized and interactive learning experiences,\nand it shows strong potential to promote young learners language acquisition.\nHowever, it is still challenging to ensure consistent and robust performance\nacross different languages and cultural contexts, and kids-friendly design\nrequires simplified instructions, engaging interactions, and age-appropriate\nscaffolding to maintain motivation and optimize learning outcomes. In this\nwork, we introduce SingaKids, a dialogic tutor designed to facilitate language\nlearning through picture description tasks. Our system integrates dense image\ncaptioning, multilingual dialogic interaction, speech understanding, and\nengaging speech generation to create an immersive learning environment in four\nlanguages: English, Mandarin, Malay, and Tamil. We further improve the system\nthrough multilingual pre-training, task-specific tuning, and scaffolding\noptimization. Empirical studies with elementary school students demonstrate\nthat SingaKids provides effective dialogic teaching, benefiting learners at\ndifferent performance levels.", "AI": {"tldr": "SingaKids is a dialogic tutor that enhances language acquisition for children through picture description tasks and multilingual interactions.", "motivation": "To improve personalized and interactive learning experiences for young learners and promote language acquisition through the integration of generative AI.", "method": "SingaKids incorporates dense image captioning, multilingual dialogic interaction, speech understanding, and engaging speech generation, and is enhanced through multilingual pre-training and task-specific tuning.", "result": "Empirical studies show that SingaKids effectively facilitates language learning for elementary school students across four languages, benefiting learners of varying performance levels.", "conclusion": "The system demonstrates promise in providing engaging and effective dialogic teaching to young learners in diverse cultural contexts.", "key_contributions": ["Development of SingaKids as a dialogic tutor for language learning", "Integration of multilingual interactions and speech generation", "Empirical validation with elementary school students for effective learning outcomes"], "limitations": "", "keywords": ["generative AI", "language acquisition", "dialogic tutor", "multilingual education", "interactive learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.02425", "pdf": "https://arxiv.org/pdf/2506.02425.pdf", "abs": "https://arxiv.org/abs/2506.02425", "title": "Gender Inequality in English Textbooks Around the World: an NLP Approach", "authors": ["Tairan Liu"], "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Textbooks play a critical role in shaping children's understanding of the\nworld. While previous studies have identified gender inequality in individual\ncountries' textbooks, few have examined the issue cross-culturally. This study\napplies natural language processing methods to quantify gender inequality in\nEnglish textbooks from 22 countries across 7 cultural spheres. Metrics include\ncharacter count, firstness (which gender is mentioned first), and TF-IDF word\nassociations by gender. The analysis also identifies gender patterns in proper\nnames appearing in TF-IDF word lists, tests whether large language models can\ndistinguish between gendered word lists, and uses GloVe embeddings to examine\nhow closely keywords associate with each gender. Results show consistent\noverrepresentation of male characters in terms of count, firstness, and named\nentities. All regions exhibit gender inequality, with the Latin cultural sphere\nshowing the least disparity.", "AI": {"tldr": "This study analyzes gender inequality in English textbooks from 22 countries using NLP techniques, revealing consistent male overrepresentation.", "motivation": "To examine cross-cultural gender inequality in textbooks and understand how it impacts children's worldviews.", "method": "Applied natural language processing methods to analyze character count, first mention, TF-IDF word associations, and GloVe embeddings for gendered keywords in English textbooks.", "result": "Findings indicate a persistent overrepresentation of male characters across all regions, with the Latin cultural sphere exhibiting the least disparity.", "conclusion": "The study highlights the need for awareness of gender biases in educational materials globally.", "key_contributions": ["Quantitative analysis of gender representation in international textbooks using NLP techniques.", "Identification of gender patterns in proper names and character mentions across cultures.", "Establishment of benchmarks for future studies on gender inequality in educational resources."], "limitations": "The study focuses solely on English textbooks and may not account for nuances in non-English contexts.", "keywords": ["gender inequality", "natural language processing", "textbooks", "cross-cultural analysis", "education"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2506.02426", "pdf": "https://arxiv.org/pdf/2506.02426.pdf", "abs": "https://arxiv.org/abs/2506.02426", "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification", "authors": ["Maryam Berijanian", "Kuldeep Singh", "Amin Sehati"], "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1"], "comment": null, "summary": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at\n\\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.", "AI": {"tldr": "This paper analyzes three AI agent architectures for entity relationship classification using large language models, demonstrating that multi-agent coordination outperforms standard prompting techniques.", "motivation": "Entity relationship classification faces challenges due to limited labeled data and complex relational structures, making it essential to improve methods for relation classification using LLMs.", "method": "The study compares three architectures: reflective self-evaluation, hierarchical task decomposition, and a multi-agent dynamic example generation mechanism, assessing their performance across multiple domains and backends.", "result": "Multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models in relation classification tasks.", "conclusion": "The findings provide practical guidance for designing LLM-based systems for structured relation extraction, with implications for improving AI agents' capabilities in handling complex relational tasks.", "key_contributions": ["Introduction of multi-agent dynamic example generation for relation classification.", "Systematic evaluation of diverse AI agent architectures targeting entity relationship classification.", "Demonstration of performance improvements over standard few-shot prompting methods."], "limitations": "", "keywords": ["entity relationship classification", "large language models", "multi-agent systems", "prompting techniques", "information extraction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.02431", "pdf": "https://arxiv.org/pdf/2506.02431.pdf", "abs": "https://arxiv.org/abs/2506.02431", "title": "From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models", "authors": ["Mahammed Kamruzzaman", "Abdullah Al Monsur", "Gene Louis Kim", "Anshuman Chhabra"], "categories": ["cs.CL"], "comment": null, "summary": "Emotions are a fundamental facet of human experience, varying across\nindividuals, cultural contexts, and nationalities. Given the recent success of\nLarge Language Models (LLMs) as role-playing agents, we examine whether LLMs\nexhibit emotional stereotypes when assigned nationality-specific personas.\nSpecifically, we investigate how different countries are represented in\npre-trained LLMs through emotion attributions and whether these attributions\nalign with cultural norms. Our analysis reveals significant nationality-based\ndifferences, with emotions such as shame, fear, and joy being\ndisproportionately assigned across regions. Furthermore, we observe notable\nmisalignment between LLM-generated and human emotional responses, particularly\nfor negative emotions, highlighting the presence of reductive and potentially\nbiased stereotypes in LLM outputs.", "AI": {"tldr": "This paper investigates emotional stereotypes in Large Language Models (LLMs) based on nationality-specific personas, revealing significant biases and misalignment with human emotional responses.", "motivation": "To examine how LLMs represent emotions across different nationalities and to identify potential biases in their outputs.", "method": "Analysis of pre-trained LLMs by assigning nationality-specific personas and evaluating emotional attributions against cultural norms.", "result": "Significant differences in emotion attribution among nationalities were found, with notable misalignments between LLM-generated emotions and human emotional responses, especially for negative emotions.", "conclusion": "LLMs exhibit reductive and biased emotional stereotypes linked to nationalities, which can misrepresent human emotional diversity.", "key_contributions": ["Identification of nationality-based emotional biases in LLMs", "Demonstration of misalignment between LLM and human emotional responses", "Analysis of emotional stereotypes in LLM outputs across cultures"], "limitations": "Focus on a limited number of countries; the study may not account for all cultural contexts.", "keywords": ["Large Language Models", "emotional stereotypes", "nationality", "HCI", "bias"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.02442", "pdf": "https://arxiv.org/pdf/2506.02442.pdf", "abs": "https://arxiv.org/abs/2506.02442", "title": "Should LLM Safety Be More Than Refusing Harmful Instructions?", "authors": ["Utsav Maskey", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "This paper presents a systematic evaluation of Large Language Models' (LLMs)\nbehavior on long-tail distributed (encrypted) texts and their safety\nimplications. We introduce a two-dimensional framework for assessing LLM\nsafety: (1) instruction refusal-the ability to reject harmful obfuscated\ninstructions, and (2) generation safety-the suppression of generating harmful\nresponses. Through comprehensive experiments, we demonstrate that models that\npossess capabilities to decrypt ciphers may be susceptible to\nmismatched-generalization attacks: their safety mechanisms fail on at least one\nsafety dimension, leading to unsafe responses or over-refusal. Based on these\nfindings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss\ntheir strengths and limitations. This work contributes to understanding the\nsafety of LLM in long-tail text scenarios and provides directions for\ndeveloping robust safety mechanisms.", "AI": {"tldr": "Systematic evaluation of LLM behavior on long-tail encrypted texts and safety implications.", "motivation": "To assess the safety of LLMs in processing long-tail distributed encrypted texts and understand potential vulnerabilities.", "method": "Introduced a two-dimensional framework assessing LLM safety on instruction refusal and generation safety, with extensive experimental evaluation.", "result": "Demonstrated that LLMs capable of decrypting ciphers can be vulnerable to mismatched-generalization attacks, leading to unsafe responses or over-refusal.", "conclusion": "The findings indicate the need for robust safety mechanisms for LLMs, particularly in long-tail text scenarios, while evaluating existing safeguards.", "key_contributions": ["Introduction of a two-dimensional safety framework for LLMs", "Identification of mismatched-generalization attacks in LLMs", "Evaluation of pre-LLM and post-LLM safeguards for model safety."], "limitations": "Focuses solely on long-tail encrypted texts and may not generalize to all text types.", "keywords": ["Large Language Models", "safety mechanisms", "long-tail distribution", "instruction refusal", "encrypted texts"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02449", "pdf": "https://arxiv.org/pdf/2506.02449.pdf", "abs": "https://arxiv.org/abs/2506.02449", "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data", "authors": ["Bo Peng", "Zhiheng Wang", "Heyang Gong", "Chaochao Lu"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In modern dialogue systems, the ability to implicitly infer user backgrounds\nfrom conversations and leverage this information for personalized assistance is\ncrucial. However, the scarcity of high-quality data remains a fundamental\nchallenge to evaluating and improving this capability. Traditional dataset\nconstruction methods are labor-intensive, resource-demanding, and raise privacy\nconcerns. To address these issues, we propose a novel approach for automatic\nsynthetic data generation and introduce the Implicit Personalized Dialogue\n(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12\nuser attribute types. Additionally, we develop a systematic evaluation\nframework with four metrics to assess both attribute awareness and reasoning\ncapabilities. We further propose five causal graphs to elucidate models'\nreasoning pathways during implicit personalization. Extensive experiments yield\ninsightful observations and prove the reliability of our dataset.", "AI": {"tldr": "This paper introduces a method for automatic synthetic data generation to enhance personalized assistance in dialogue systems, along with a new benchmark and evaluation framework.", "motivation": "The need for personalized assistance in dialogue systems, hindered by the lack of high-quality training data, and the challenges of traditional dataset construction methods.", "method": "The authors propose automatic synthetic data generation methods for creating a training dataset for the Implicit Personalized Dialogue (IP-Dialog) benchmark, which comprises 10 tasks and 12 user attribute types. They also develop a systematic evaluation framework and causal graphs to analyze reasoning pathways.", "result": "The experiments demonstrate the reliability of the synthetic dataset and provide insights into the models' performances in attribute awareness and reasoning.", "conclusion": "The proposed data generation method and IP-Dialog benchmark enable significant advancements in building more effective personalized dialogue systems.", "key_contributions": ["Introduction of automatic synthetic data generation for dialogue systems.", "Creation of the Implicit Personalized Dialogue (IP-Dialog) benchmark with 10 tasks and 12 user attributes.", "Development of a systematic evaluation framework with causal graphs for reasoning pathways."], "limitations": "The effectiveness of synthetic data may vary based on the complexity of user interactions and potential bias in generated data.", "keywords": ["dialogue systems", "synthetic data generation", "personalized assistance", "evaluation framework", "causal graphs"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.02454", "pdf": "https://arxiv.org/pdf/2506.02454.pdf", "abs": "https://arxiv.org/abs/2506.02454", "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework", "authors": ["Zhaorui Yang", "Bo Pan", "Han Wang", "Yiyao Wang", "Xingyu Liu", "Minfeng Zhu", "Bo Zhang", "Wei Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "47 pages", "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.", "AI": {"tldr": "This paper presents a framework for generating multimodal reports that integrate texts and visualizations using Large Language Models (LLMs).", "motivation": "Existing frameworks for generating comprehensive reports primarily focus on text, neglecting the integration of visualizations, which is crucial for effective communication.", "method": "The paper proposes a structured representation of charts called Formal Description of Visualization (FDV) and introduces Multimodal DeepResearcher, a framework that breaks report generation into researching, textualization, planning, and multimodal generation.", "result": "Multimodal DeepResearcher is evaluated using MultimodalReportBench, demonstrating an 82% win rate over baseline methods based on extensive experiments.", "conclusion": "The proposed framework significantly enhances the capability of LLMs in generating integrated multimodal reports, bridging the gap in automated visualization generation.", "key_contributions": ["Introduction of Formal Description of Visualization (FDV) for chart representation.", "Development of Multimodal DeepResearcher for multimodal report generation.", "Creation of MultimodalReportBench for evaluation of generated reports."], "limitations": "The paper may not address all potential design challenges for a wide range of complex visualizations.", "keywords": ["Large Language Models", "visualization generation", "multimodal reports", "HCI", "AI in health informatics"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2506.02460", "pdf": "https://arxiv.org/pdf/2506.02460.pdf", "abs": "https://arxiv.org/abs/2506.02460", "title": "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework", "authors": ["Yupeng Qi", "Ziyu Lyu", "Min Yang", "Yanlin Wang", "Lu Bai", "Lixin Cui"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly applied across various\ndomains, enhancing safety while maintaining the helpfulness of LLMs has become\na critical challenge. Recent studies solve this problem through\nsafety-constrained online preference optimization or safety-constrained offline\npreference optimization. However, the safety-constrained online methods often\nsuffer from excessive safety, which might reduce helpfulness, while the\nsafety-constrained offline methods perform poorly in adaptively balancing\nsafety and helpfulness. To address these limitations, we propose MidPO, a\n\\textbf{\\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness\n\\textbf{\\underline{d}}ual \\textbf{\\underline{P}}reference\n\\textbf{\\underline{O}}ptimization. Firstly, MidPO devises single-preference\nenhanced direct preference optimization approach to transform the base model\ninto two independent experts, termed safety and helpfulness experts, and\nfine-tunes the two independent experts for optimal safety or helpfulness\nperformance. Secondly, to achieve an effective balance between safety and\nhelpfulness, MidPO incorporates the two experts into the MoE framework and\ndesigns a dynamic routing mechanism to allocate contributions from each expert\nadaptively. We conduct quantitative and qualitative experiments on three\npopular datasets to demonstrate the proposed MidPO significantly outperforms\nstate-of-the-art approaches in both safety and helpfulness. The code and models\nwill be released.", "AI": {"tldr": "MidPO is a Mixture of Experts (MoE) framework that optimizes the balance between safety and helpfulness in large language models (LLMs) by using independent safety and helpfulness experts with dynamic routing.", "motivation": "Enhance the safety and helpfulness of LLMs, addressing limitations of existing safety-constrained optimization methods.", "method": "Develops a Mixture of Experts framework that creates two independent experts for safety and helpfulness. It employs a dynamic routing mechanism for adaptively balancing contributions from each expert.", "result": "MidPO significantly outperforms state-of-the-art approaches in both safety and helpfulness on three popular datasets.", "conclusion": "The proposed framework effectively addresses the trade-off between safety and helpfulness in LLMs, and code/models will be publicly available.", "key_contributions": ["Introduction of a Mixture of Experts framework for LLMs.", "Development of a single-preference enhanced direct preference optimization.", "Implementation of a dynamic routing mechanism for expert contribution allocation."], "limitations": "", "keywords": ["large language models", "Mixture of Experts", "safety optimization", "helpfulness optimization", "dynamic routing"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.02461", "pdf": "https://arxiv.org/pdf/2506.02461.pdf", "abs": "https://arxiv.org/abs/2506.02461", "title": "XToM: Exploring the Multilingual Theory of Mind for Large Language Models", "authors": ["Chunkit Chan", "Yauwai Yim", "Hongchuan Zeng", "Zhiying Zou", "Xinyuan Cheng", "Zhifan Sun", "Zheye Deng", "Kawai Chung", "Yuzhuo Ao", "Yixiang Fan", "Cheng Jiayang", "Ercong Nie", "Ginny Y. Wong", "Helmut Schmid", "Hinrich Schütze", "Simon See", "Yangqiu Song"], "categories": ["cs.CL"], "comment": null, "summary": "Theory of Mind (ToM), the ability to infer mental states in others, is\npivotal for human social cognition. Existing evaluations of ToM in LLMs are\nlargely limited to English, neglecting the linguistic diversity that shapes\nhuman cognition. This limitation raises a critical question: can LLMs exhibit\nMultilingual Theory of Mind, which is the capacity to reason about mental\nstates across diverse linguistic contexts? To address this gap, we present\nXToM, a rigorously validated multilingual benchmark that evaluates ToM across\nfive languages and incorporates diverse, contextually rich task scenarios.\nUsing XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a\npronounced dissonance: while models excel in multilingual language\nunderstanding, their ToM performance varies across languages. Our findings\nexpose limitations in LLMs' ability to replicate human-like mentalizing across\nlinguistic contexts.", "AI": {"tldr": "This paper presents XToM, a multilingual benchmark evaluating Theory of Mind (ToM) in LLMs across five languages, revealing performance discrepancies in ToM relative to language understanding.", "motivation": "To examine the capacity of LLMs to demonstrate Multilingual Theory of Mind, addressing the neglect of linguistic diversity in existing ToM evaluations.", "method": "The authors developed XToM, a benchmark assessing LLMs' ToM capabilities in five languages through various contextually rich tasks.", "result": "Evaluation of LLMs using XToM showed that, while they perform well in multilingual language understanding, their ToM abilities vary significantly across different languages.", "conclusion": "LLMs do not replicate human-like mentalizing across linguistic contexts, exposing their limitations in this area despite strong language performance.", "key_contributions": ["Introduction of XToM as a benchmark for multilingual ToM evaluation in LLMs.", "Demonstration of the variation in ToM performance of LLMs across languages.", "Highlighting the impact of linguistic diversity on cognitive modeling in AI."], "limitations": "The benchmark is evaluated only on five languages, which may not represent the full scope of linguistic diversity.", "keywords": ["Theory of Mind", "LLMs", "multilingual", "cognitive modeling", "language understanding"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02478", "pdf": "https://arxiv.org/pdf/2506.02478.pdf", "abs": "https://arxiv.org/abs/2506.02478", "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging", "authors": ["Zijian Li", "Xiaocheng Feng", "Huixin Liu", "Yichong Huang", "Ting Liu", "Bing Qin"], "categories": ["cs.CL"], "comment": "12 pages, 11 figures", "summary": "With the development of large language models, fine-tuning has emerged as an\neffective method to enhance performance in specific scenarios by injecting\ndomain-specific knowledge. In this context, model merging techniques provide a\nsolution for fusing knowledge from multiple fine-tuning models by combining\ntheir parameters. However, traditional methods often encounter task\ninterference when merging full fine-tuning models, and this problem becomes\neven more evident in parameter-efficient fine-tuning scenarios. In this paper,\nwe introduce an improvement to the RegMean method, which indirectly leverages\nthe training data to approximate the outputs of the linear layers before and\nafter merging. We propose an adaptive merging method called FroM, which\ndirectly measures the model parameters using the Frobenius norm, without any\ntraining data. By introducing an additional hyperparameter for control, FroM\noutperforms baseline methods across various fine-tuning scenarios, alleviating\nthe task interference problem.", "AI": {"tldr": "This paper presents FroM, an adaptive merging method that enhances model merging by directly measuring model parameters and alleviating task interference.", "motivation": "To address the challenges of task interference when merging fine-tuning models, particularly in parameter-efficient scenarios.", "method": "FroM utilizes the Frobenius norm to measure model parameters directly, without requiring training data, and introduces an additional hyperparameter for control.", "result": "FroM outperforms baseline methods in various fine-tuning scenarios, effectively mitigating task interference.", "conclusion": "The proposed FroM method offers a promising solution for parameter-efficient fine-tuning model merging, ensuring better performance without the usual drawbacks of traditional methods.", "key_contributions": ["Introduction of FroM, an adaptive merging method that requires no training data.", "Effectively alleviates task interference during model merging.", "Demonstrates superior performance over baseline methods across multiple scenarios."], "limitations": "", "keywords": ["model merging", "fine-tuning", "Frobenius norm", "task interference", "adaptive merging"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2506.02480", "pdf": "https://arxiv.org/pdf/2506.02480.pdf", "abs": "https://arxiv.org/abs/2506.02480", "title": "ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities", "authors": ["Yifan Duan", "Yihong Tang", "Kehai Chen", "Liqiang Nie", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "High-quality prompts are crucial for eliciting outstanding performance from\nlarge language models (LLMs) on complex tasks. Existing research has explored\nmodel-driven strategies for prompt optimization. However, these methods often\nsuffer from high computational overhead or require strong optimization\ncapabilities from the model itself, which limits their broad applicability.To\naddress these challenges, we propose ORPP (Optimized Role-Playing Prompt),a\nframework that enhances model performance by optimizing and generating\nrole-playing prompts. The core idea of ORPP is to confine the prompt search\nspace to role-playing scenarios, thereby fully activating the model's intrinsic\ncapabilities through carefully crafted, high-quality role-playing prompts.\nSpecifically, ORPP first performs iterative optimization on a small subset of\ntraining samples to generate high-quality role-playing prompts. Then,\nleveraging the model's few-shot learning capability, it transfers the\noptimization experience to efficiently generate suitable prompts for the\nremaining samples.Our experimental results show that ORPP not only matches but\nin most cases surpasses existing mainstream prompt optimization methods in\nterms of performance. Notably, ORPP demonstrates superior \"plug-and-play\"\ncapability. In most cases, it can be integrated with various other prompt\nmethods and further enhance their effectiveness.", "AI": {"tldr": "This paper introduces ORPP, a framework for generating optimized role-playing prompts to improve the performance of large language models (LLMs) by confining the prompt search space to role-playing scenarios.", "motivation": "The existing model-driven strategies for prompt optimization often suffer from high computational overhead and limited applicability due to their requirements for strong optimization capabilities from the model.", "method": "ORPP performs iterative optimization on a small subset of training samples to generate high-quality role-playing prompts and uses few-shot learning to apply this optimization to other samples.", "result": "ORPP matches or surpasses existing prompt optimization methods, displaying superior performance and integration with other prompt techniques.", "conclusion": "ORPP effectively enhances LLM performance through optimized role-playing prompts and offers a flexible, plug-and-play capability.", "key_contributions": ["Introduction of the ORPP framework for prompt optimization", "Effective use of role-playing scenarios to generate prompts", "Demonstration of superior performance compared to existing methods."], "limitations": "", "keywords": ["Large Language Models", "Prompt Optimization", "Role-Playing Prompts"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.02481", "pdf": "https://arxiv.org/pdf/2506.02481.pdf", "abs": "https://arxiv.org/abs/2506.02481", "title": "Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths", "authors": ["Inderjeet Nair", "Lu Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluations of LLMs' ethical risks and value inclinations often rely on\nshort-form surveys and psychometric tests, yet real-world use involves\nlong-form, open-ended responses -- leaving value-related risks and preferences\nin practical settings largely underexplored. In this work, we ask: Do value\npreferences inferred from short-form tests align with those expressed in\nlong-form outputs? To address this question, we compare value preferences\nelicited from short-form reactions and long-form responses, varying the number\nof arguments in the latter to capture users' differing verbosity preferences.\nAnalyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),\nwe find (1) a weak correlation between value preferences inferred from\nshort-form and long-form responses across varying argument counts, and (2)\nsimilarly weak correlation between preferences derived from any two distinct\nlong-form generation settings. (3) Alignment yields only modest gains in the\nconsistency of value expression. Further, we examine how long-form generation\nattributes relate to value preferences, finding that argument specificity\nnegatively correlates with preference strength, while representation across\nscenarios shows a positive correlation. Our findings underscore the need for\nmore robust methods to ensure consistent value expression across diverse\napplications.", "AI": {"tldr": "This paper investigates whether value preferences inferred from short-form assessments align with those expressed in longer, open-ended responses generated by LLMs.", "motivation": "To explore the alignment of ethical value preferences expressed in short-form tests with those articulated in long-form LLM outputs, addressing the inadequacies of current evaluation methods.", "method": "The study compares value preferences from short-form reactions with long-form responses from five LLMs, varying argument counts to examine verbosity preferences.", "result": "The analysis reveals a weak correlation between short-form and long-form value preferences, and similar weak correlations across distinct long-form settings, indicating that alignment yields only modest consistency gains in value expression.", "conclusion": "The findings highlight the necessity for improved methods to ensure consistent value expression across varied applications of LLMs.", "key_contributions": ["Identification of weak correlations between short-form and long-form value preferences", "Insights into how verbosity affects value expression consistency", "Highlighting the need for more robust evaluation methods in LLM assessments"], "limitations": "The study only examines five specific LLMs and may not generalize to others or wider contexts.", "keywords": ["LLMs", "Ethical risks", "Value preferences", "Long-form responses", "Value expression"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02483", "pdf": "https://arxiv.org/pdf/2506.02483.pdf", "abs": "https://arxiv.org/abs/2506.02483", "title": "Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks", "authors": ["Sina Bagheri Nezhad", "Ameeta Agrawal"], "categories": ["cs.CL"], "comment": "Accepted at 19th Conference on Neurosymbolic Learning and Reasoning\n  (NeSy 2025)", "summary": "Large language models (LLMs) often struggle to perform multi-target reasoning\nin long-context scenarios where relevant information is scattered across\nextensive documents. To address this challenge, we introduce NeuroSymbolic\nAugmented Reasoning (NSAR), which combines the benefits of neural and symbolic\nreasoning during inference. NSAR explicitly extracts symbolic facts from text\nand generates executable Python code to handle complex reasoning steps. Through\nextensive experiments across seven languages and diverse context lengths, we\ndemonstrate that NSAR significantly outperforms both a vanilla RAG baseline and\nadvanced prompting strategies in accurately identifying and synthesizing\nmultiple pieces of information. Our results highlight the effectiveness of\ncombining explicit symbolic operations with neural inference for robust,\ninterpretable, and scalable reasoning in multilingual settings.", "AI": {"tldr": "This paper introduces NeuroSymbolic Augmented Reasoning (NSAR), a method that enhances multi-target reasoning in long-context scenarios using a combination of neural and symbolic reasoning.", "motivation": "To improve the performance of large language models in multi-target reasoning with scattered information in extensive documents.", "method": "NSAR extracts symbolic facts from text and generates executable Python code for complex reasoning tasks, tested across multiple languages and context lengths.", "result": "NSAR significantly outperforms both a vanilla RAG baseline and advanced prompting strategies in reasoning accuracy.", "conclusion": "Combining explicit symbolic operations with neural inference leads to robust, interpretable, and scalable reasoning in multilingual contexts.", "key_contributions": ["Introduction of NeuroSymbolic Augmented Reasoning (NSAR) for improved reasoning in LLMs", "Demonstration of substantial performance gains over baseline methods", "Evaluation across diverse languages and context lengths for broader applicability"], "limitations": "", "keywords": ["NeuroSymbolic Reasoning", "Large Language Models", "Python Code Generation", "Multilingual Reasoning", "Symbolic Operations"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02494", "pdf": "https://arxiv.org/pdf/2506.02494.pdf", "abs": "https://arxiv.org/abs/2506.02494", "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks.", "AI": {"tldr": "The paper introduces Minos-Corpus, a large-scale multimodal evaluation dataset and the Minos model which outperforms existing evaluation models on text-to-image generation.", "motivation": "To address the lack of evaluation capabilities in multimodal generation tasks, particularly for text-to-image (T2I) generation, and to incorporate large-scale human evaluation data.", "method": "The authors propose a large-scale dataset, Minos-Corpus, that includes human and GPT evaluation data for I2T and T2I tasks. They develop Data Selection and Balance, Mix-SFT training methods, and the Minos evaluation model based on a 7B backbone, leveraging high-quality human evaluation data.", "result": "Minos achieves state-of-the-art performance on multimodal evaluation tasks, particularly outperforming existing models in T2I generation evaluation.", "conclusion": "Leveraging large-scale human evaluation data is crucial for developing effective multimodal evaluation systems, and Minos demonstrates significant advancements in evaluation performance across tasks.", "key_contributions": ["Introduction of Minos-Corpus dataset for multimodal evaluation", "Development of the Minos evaluation model with state-of-the-art performance", "Demonstration of the importance of human evaluation data in model training"], "limitations": "", "keywords": ["multimodal evaluation", "text-to-image generation", "human evaluation data"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02503", "pdf": "https://arxiv.org/pdf/2506.02503.pdf", "abs": "https://arxiv.org/abs/2506.02503", "title": "KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG", "authors": ["Yongjian Li", "HaoCheng Chu", "Yukun Yan", "Zhenghao Liu", "Shi Yu", "Zheni Zeng", "Ruobing Wang", "Sen Song", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess broader knowledge sources, yet factual inconsistencies persist due to\nnoise in retrieved documents-even with advanced retrieval methods. We\ndemonstrate that enhancing generative models' capacity to process noisy content\nis equally critical for robust performance. In this paper, we present KARE-RAG\n(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge\nutilization through three key innovations: (1) structured knowledge\nrepresentations that facilitate error detection during training, (2) Dense\nDirect Preference Optimization (DDPO)-a refined training objective that\nprioritizes correction of critical errors, and (3) a contrastive data\ngeneration pipeline that maintains semantic consistency while rectifying\nfactual inaccuracies. Experiments show our method significantly enhances\nstandard RAG pipelines across model scales, improving both in-domain and\nout-of-domain task performance without compromising general capabilities.\nNotably, these gains are achieved with modest training data, suggesting\ndata-efficient optimization is possible through targeted learning strategies.\nOur findings establish a new direction for RAG improvement: by improving how\nmodels learn to process retrieved content, we can enhance performance across\ndiverse inference paradigms. All data and code will be publicly available on\nGithub.", "AI": {"tldr": "This paper introduces KARE-RAG, an approach to enhance retrieval-augmented generation (RAG) by improving the processing of noisy content and correcting factual inaccuracies.", "motivation": "There are persistent factual inconsistencies in retrieval-augmented generation (RAG) due to noise in retrieved documents, highlighting the need for improved handling of such content.", "method": "KARE-RAG employs structured knowledge representations for error detection, a refined training objective called Dense Direct Preference Optimization (DDPO) that focuses on correcting critical errors, and a contrastive data generation pipeline for maintaining semantic consistency while addressing factual inaccuracies.", "result": "Experiments show that KARE-RAG significantly enhances standard RAG pipelines across various model scales, improving performance on both in-domain and out-of-domain tasks without compromising general capabilities.", "conclusion": "The study establishes a new direction in improving retrieval-augmented generation by enhancing models' ability to process retrieved knowledge, leading to better performance across different inference paradigms.", "key_contributions": ["Structured knowledge representations for error detection", "Dense Direct Preference Optimization (DDPO) for error correction", "A contrastive data generation pipeline for semantic consistency"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Knowledge-Aware Refinement", "Dense Direct Preference Optimization", "Factual Consistency", "Semantic Consistency"], "importance_score": 10, "read_time_minutes": 10}}
{"id": "2506.02510", "pdf": "https://arxiv.org/pdf/2506.02510.pdf", "abs": "https://arxiv.org/abs/2506.02510", "title": "M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset", "authors": ["Jie Zhu", "Junhui Li", "Yalong Wen", "Xiandong Li", "Lifan Guo", "Feng Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL-2025", "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called $\\texttt{M$^3$FinMeeting}$, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, $\\texttt{M$^3$FinMeeting}$ supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\n$\\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\n$\\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.", "AI": {"tldr": "Introducing the $\texttt{M$^3$FinMeeting}$ benchmark for evaluating LLMs in financial meeting comprehension, featuring multilingual support and multi-task capabilities.", "motivation": "To address the limitations of existing financial benchmarks that fail to capture real-world dynamics of financial meetings, which are crucial for evaluating LLMs in a practical context.", "method": "Developed a dataset called $\texttt{M$^3$FinMeeting}$ that supports three languages (English, Chinese, Japanese) and includes tasks like summarization, QA pair extraction, and question answering.", "result": "Experimental evaluations show that even advanced long-context LLMs have significant room for improvement when using the $\texttt{M$^3$FinMeeting}$ benchmark.", "conclusion": "The $\texttt{M$^3$FinMeeting}$ benchmark is effective for assessing LLMs' understanding of financial meetings, providing insights into their performance and areas for enhancement.", "key_contributions": ["A multilingual benchmark for financial meeting understanding in English, Chinese, and Japanese.", "Inclusion of various industry sectors to cover a wide range of financial activities.", "Three distinct evaluation tasks tailored to better assess LLM capabilities."], "limitations": "", "keywords": ["Large Language Models", "Financial Meeting Understanding", "Benchmark Datasets", "Multilingual", "AI Applications"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2506.02515", "pdf": "https://arxiv.org/pdf/2506.02515.pdf", "abs": "https://arxiv.org/abs/2506.02515", "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning", "authors": ["Zhuohan Xie", "Dhruv Sahnan", "Debopriyo Banerjee", "Georgi Georgiev", "Rushil Thareja", "Hachem Madmoun", "Jinyan Su", "Aaryamonvikram Singh", "Yuxia Wang", "Rui Xing", "Fajri Koto", "Haonan Li", "Ivan Koychev", "Tanmoy Chakraborty", "Salem Lahlou", "Veselin Stoyanov", "Preslav Nakov"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 8 figures, 2 tables", "summary": "Multi-step symbolic reasoning is critical for advancing downstream\nperformance on financial tasks. Yet, benchmarks for systematically evaluating\nthis capability are lacking. Existing datasets like FinQA and ConvFinQA\nsupervise only final numerical answers, without assessing intermediate\nreasoning steps. To address this, we introduce FinChain, the first symbolic\nbenchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.\nSpanning 54 topics across 12 financial domains, Fin- Chain offers five\nparameterized templates per topic, each varying in reasoning complexity and\ndomain expertise required. Each dataset instance includes an executable Python\ntrace, enabling automatic generation of extensive training data and easy\nadaptation to other domains. We also introduce ChainEval, a new metric for\nautomatic evaluation of both final answers and intermediate reasoning.\nBenchmarking 30 LLMs on our dataset, we find that even state-of-the-art models\nhave considerable room for improvement in multi-step financial reasoning. All\ntemplates and evaluation metrics for FinChain are available at https:\n//github.com/mbzuai-nlp/finchain.", "AI": {"tldr": "FinChain is a new benchmark for evaluating multi-step symbolic reasoning in financial tasks, addressing the lack of datasets that assess intermediate reasoning steps.", "motivation": "To systematically evaluate multi-step symbolic reasoning in financial tasks, where existing benchmarks fail to consider the reasoning processes leading to final answers.", "method": "Introduced FinChain, a benchmark that spans 54 topics across 12 financial domains with varied complexity and expertise levels; includes executable Python traces for data generation and adaptation.", "result": "30 LLMs were benchmarked on FinChain, showing a significant gap in multi-step reasoning performance, even among the best models.", "conclusion": "The FinChain benchmark and ChainEval metric provide a structured approach to enhance the evaluation of financial reasoning capabilities of LLMs.", "key_contributions": ["FinChain benchmark for multi-step financial reasoning", "ChainEval evaluation metric for assessing intermediate reasoning", "Python traces for automatic data generation and model adaptation"], "limitations": "", "keywords": ["Financial Reasoning", "Benchmarking", "Chain-of-Thought", "LLMs", "Symbolic Reasoning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.02519", "pdf": "https://arxiv.org/pdf/2506.02519.pdf", "abs": "https://arxiv.org/abs/2506.02519", "title": "Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning", "authors": ["Sohan Patnaik", "Milan Aggarwal", "Sumit Bhatia", "Balaji Krishnamurthy"], "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions\nby generating step-by-step rationales. Prior works have utilized this\ncapability to improve smaller and cheaper LMs (say, with 7B parameters).\nHowever, various practical constraints, such as copyright and legal issues,\nowing to lack of transparency in the pre-training data of large (often closed)\nmodels, prevent their use in commercial settings. Little focus has been given\nto improving the innate reasoning ability of smaller models without distilling\ninformation from larger LLMs. To address this, we propose COLLATE, a trainable\nframework that tunes a (small) LLM to generate those outputs from a pool of\ndiverse rationales that selectively improves the downstream task. COLLATE\nenforces multiple instances of the same LLM to exhibit distinct behavior and\nemploys them to generate rationales to obtain diverse outputs. The LLM is then\ntuned via preference optimization to choose the candidate rationale which\nmaximizes the likelihood of ground-truth answer. COLLATE outperforms several\ntrainable and prompting baselines on 5 datasets across 3 domains: maths problem\nsolving, natural language inference, and commonsense reasoning. We show the eff\nicacy of COLLATE on LLMs from different model families across varying parameter\nscales (1B to 8B) and demonstrate the benefit of multiple rationale providers\nguided by the end task through ablations. Code is released here\n(https://github.com/Sohanpatnaik106/collate).", "AI": {"tldr": "COLLATE is a framework that improves the reasoning abilities of small language models without relying on larger models, using diverse rationales to enhance task performance.", "motivation": "Address the limitations of applying large language models (LLMs) in commercial settings due to transparency and copyright issues, while improving smaller LLMs' reasoning abilities.", "method": "COLLATE tunes a small LLM by generating outputs from a pool of diverse rationales, utilizing preference optimization to select the rationale that maximizes the likelihood of the ground-truth answer.", "result": "COLLATE outperforms existing trainable and prompting baselines across 5 datasets in 3 domains, showing improvements in reasoning tasks.", "conclusion": "The framework demonstrates the value of using multiple rationale providers to enhance the capabilities of smaller LLMs, applicable across various parameter scales.", "key_contributions": ["Introduction of the COLLATE framework", "Improvement of reasoning capabilities in smaller LLMs without size distillation", "Demonstration of efficacy across multiple datasets and domains"], "limitations": "", "keywords": ["COLLATE", "small language models", "reasoning", "preference optimization", "natural language inference"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.02527", "pdf": "https://arxiv.org/pdf/2506.02527.pdf", "abs": "https://arxiv.org/abs/2506.02527", "title": "Multilingual Information Retrieval with a Monolingual Knowledge Base", "authors": ["Yingying Zhuang", "Aman Gupta", "Anurag Beniwal"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "6 pages, accepted at GENNEXT@SIGIR25", "summary": "Multilingual information retrieval has emerged as powerful tools for\nexpanding knowledge sharing across languages. On the other hand, resources on\nhigh quality knowledge base are often scarce and in limited languages,\ntherefore an effective embedding model to transform sentences from different\nlanguages into a feature vector space same as the knowledge base language\nbecomes the key ingredient for cross language knowledge sharing, especially to\ntransfer knowledge available in high-resource languages to low-resource ones.\nIn this paper we propose a novel strategy to fine-tune multilingual embedding\nmodels with weighted sampling for contrastive learning, enabling multilingual\ninformation retrieval with a monolingual knowledge base. We demonstrate that\nthe weighted sampling strategy produces performance gains compared to standard\nones by up to 31.03\\% in MRR and up to 33.98\\% in Recall@3. Additionally, our\nproposed methodology is language agnostic and applicable for both multilingual\nand code switching use cases.", "AI": {"tldr": "This paper presents a strategy to fine-tune multilingual embedding models for better information retrieval across languages, particularly aiding low-resource languages using a monolingual knowledge base.", "motivation": "To improve knowledge sharing across languages, especially for low-resource languages, by developing an effective embedding model for multilingual information retrieval.", "method": "The authors propose a novel strategy that employs weighted sampling for contrastive learning to fine-tune multilingual embedding models.", "result": "The proposed method shows performance improvements of up to 31.03% in Mean Reciprocal Rank (MRR) and up to 33.98% in Recall@3 compared to standard methods.", "conclusion": "The methodology is proven to be language agnostic and can be applied to both multilingual and code-switching scenarios, enhancing cross-language knowledge sharing.", "key_contributions": ["Novel strategy for fine-tuning multilingual embedding models", "Use of weighted sampling for contrastive learning", "Demonstrated performance gains in information retrieval tasks"], "limitations": "", "keywords": ["multilingual information retrieval", "embedding models", "contrastive learning"], "importance_score": 4, "read_time_minutes": 6}}
{"id": "2506.02532", "pdf": "https://arxiv.org/pdf/2506.02532.pdf", "abs": "https://arxiv.org/abs/2506.02532", "title": "ReasoningFlow: Semantic Structure of Complex Reasoning Traces", "authors": ["Jinu Lee", "Sagnik Mukherjee", "Dilek Hakkani-Tur", "Julia Hockenmaier"], "categories": ["cs.CL"], "comment": "10 pages, 6 figures. ArgMining 2025 Workshop (Non-archival) @ ACL\n  2025", "summary": "Large reasoning models (LRMs) generate complex reasoning traces with\nplanning, reflection, verification, and backtracking. In this work, we\nintroduce ReasoningFlow, a unified schema for analyzing the semantic structures\nof these complex traces. ReasoningFlow parses traces into directed acyclic\ngraphs, enabling the characterization of distinct reasoning patterns as\nsubgraph structures. This human-interpretable representation offers promising\napplications in understanding, evaluating, and enhancing the reasoning\nprocesses of LRMs.", "AI": {"tldr": "ReasoningFlow introduces a schema for analyzing reasoning structures in large reasoning models (LRMs), transforming reasoning traces into directed acyclic graphs for improved understanding and evaluation.", "motivation": "To improve the understanding, evaluation, and enhancement of reasoning processes in large reasoning models.", "method": "The paper presents ReasoningFlow, which parses reasoning traces from LRMs into directed acyclic graphs to characterize reasoning patterns as subgraph structures.", "result": "ReasoningFlow offers a human-interpretable framework that facilitates the analysis of complex reasoning traces.", "conclusion": "The framework potentially enhances the transparency and effectiveness of reasoning processes in LRMs.", "key_contributions": ["Introduction of ReasoningFlow schema for analyzing reasoning patterns.", "Transformation of reasoning traces into directed acyclic graphs.", "Improvement in understanding and evaluation of LRM reasoning processes."], "limitations": "", "keywords": ["Reasoning models", "Semantic analysis", "Graphs", "Human-computer interaction", "Machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.02533", "pdf": "https://arxiv.org/pdf/2506.02533.pdf", "abs": "https://arxiv.org/abs/2506.02533", "title": "Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey", "authors": ["Maike Behrendt", "Stefan Sylvius Wagner", "Carina Weinmann", "Marike Bormann", "Mira Warne", "Stefan Harmeling"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Political online participation in the form of discussing political issues and\nexchanging opinions among citizens is gaining importance with more and more\nformats being held digitally. To come to a decision, a careful discussion and\nconsideration of opinions and a civil exchange of arguments, which is defined\nas the act of deliberation, is desirable. The quality of discussions and\nparticipation processes in terms of their deliberativeness highly depends on\nthe design of platforms and processes. To facilitate online communication for\nboth participants and initiators, machine learning methods offer a lot of\npotential. In this work we want to showcase which issues occur in political\nonline discussions and how machine learning can be used to counteract these\nissues and enhance deliberation.", "AI": {"tldr": "This paper discusses the role of machine learning in improving the quality of online political discussions by addressing issues related to deliberation.", "motivation": "With the rise of digital formats for discussing political issues, enhancing the quality of online deliberation has become increasingly important.", "method": "The paper analyzes the challenges in political online discussions and explores machine learning techniques to improve deliberative practices.", "result": "Machine learning can help identify issues in discussions and propose solutions to facilitate better argument exchange and civil discourse.", "conclusion": "By leveraging machine learning, we can enhance the deliberative quality of online political discussions.", "key_contributions": ["Identification of key issues in online political discussions", "Application of machine learning methods to counteract these issues", "Recommendations for platform design improvements based on findings"], "limitations": "", "keywords": ["political participation", "machine learning", "online discourse", "deliberation", "platform design"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2506.02536", "pdf": "https://arxiv.org/pdf/2506.02536.pdf", "abs": "https://arxiv.org/abs/2506.02536", "title": "Answer Convergence as a Signal for Early Stopping in Reasoning", "authors": ["Xin Liu", "Lu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications.", "AI": {"tldr": "This paper investigates the minimum reasoning steps necessary for large language models (LLMs) to produce accurate answers and proposes strategies to enhance inference efficiency by reducing unnecessary reasoning.", "motivation": "The paper addresses the problem of verbose and redundant outputs in large language models when using chain-of-thought prompting, which increases inference costs.", "method": "The authors conduct a systematic study to determine the minimal reasoning required for stable decision-making in math tasks and propose three strategies: early stopping via answer consistency, boosting end-of-reasoning signals, and a supervised method for stopping based on internal activations.", "result": "Experiments on five benchmarks and five open-weight LLMs show a significant reduction in token usage with minimal accuracy impact, particularly with Answer Consistency reducing tokens by over 40% while improving accuracy on NaturalQuestions.", "conclusion": "The findings highlight the value of efficient reasoning methods at inference time, with practical advantages for applying LLMs in real-world scenarios.", "key_contributions": ["Identify redundant reasoning steps in LLMs during math tasks.", "Propose novel inference-time strategies to enhance efficiency in LLMs.", "Demonstrate significant reductions in token usage without accuracy loss."], "limitations": "The study primarily focuses on math reasoning tasks, and results may not generalize across all domains of reasoning.", "keywords": ["chain-of-thought prompting", "large language models", "inference efficiency", "early stopping", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02544", "pdf": "https://arxiv.org/pdf/2506.02544.pdf", "abs": "https://arxiv.org/abs/2506.02544", "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG", "authors": ["Yang Tian", "Fan Liu", "Jingyuan Zhang", "Victoria W.", "Yupeng Hu", "Liqiang Nie"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main", "summary": "Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to\nenhance Multimodal Large Language Models by incorporating externally retrieved\nmultimodal knowledge, but it introduces two challenges: Parametric-Retrieved\nKnowledge Inconsistency (PRKI), where discrepancies between parametric and\nretrieved knowledge create uncertainty in determining reliability, and\nVisual-Textual Knowledge Inconsistency (VTKI), where misalignment between\nvisual and textual sources disrupts entity representation. To address these\nchallenges, we propose \\textbf{C}r\\textbf{o}ss-source knowledge\n\\textbf{Re}conciliation for \\textbf{M}ulti\\textbf{M}odal \\textbf{RAG}\n(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles\ninconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage\npipeline: it first generates an internal response from parametric knowledge,\nthen selects the most relevant multimodal evidence via joint similarity\nassessment, generates an external response, and finally integrates both to\nproduce a reliable answer. Additionally, a specialized training paradigm\nenhances knowledge source discrimination, multimodal integration, and unified\nanswer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG\nachieves substantial improvements over baseline methods, achieving 5.6\\% and\n9.3\\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We\nrelease code and data at\n\\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.", "AI": {"tldr": "This paper presents CoRe-MMRAG, a framework to reconcile inconsistencies in Multimodal Retrieval-Augmented Generation, improving the reliability of multimodal large language models.", "motivation": "To address the challenges of knowledge inconsistency in multimodal retrieval-augmented generation, especially the discrepancies between parametric and retrieved knowledge and misalignments in visual-textual sources.", "method": "CoRe-MMRAG employs a four-stage pipeline: generating an internal response, selecting relevant multimodal evidence, generating an external response, and integrating both for a reliable answer.", "result": "The proposed framework achieves significant performance improvements on KB-VQA benchmarks, with 5.6% and 9.3% gains on InfoSeek and Encyclopedic-VQA respectively.", "conclusion": "CoRe-MMRAG enhances the robustness of multimodal large language models by reconciling source inconsistencies and improves overall performance in multimodal tasks.", "key_contributions": ["Introduction of CoRe-MMRAG framework for knowledge reconciliation in MMRAG.", "Improvements in multimodal integration and unified answer generation.", "Substantial performance gains on relevant benchmarks."], "limitations": "", "keywords": ["Multimodal Retrieval-Augmented Generation", "CoRe-MMRAG", "Knowledge Inconsistency", "Multimodal Integration", "Large Language Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.02561", "pdf": "https://arxiv.org/pdf/2506.02561.pdf", "abs": "https://arxiv.org/abs/2506.02561", "title": "Pruning General Large Language Models into Customized Expert Models", "authors": ["Yirao Zhao", "Guizhen Chen", "Kenji Kawaguchi", "Lidong Bing", "Wenxuan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their substantial model sizes often require substantial computational\nresources. To preserve computing resources and accelerate inference speed, it\nis crucial to prune redundant parameters, especially for experienced users who\noften need compact expert models tailored to specific downstream scenarios.\nHowever, most existing pruning methods focus on preserving the model's general\ncapabilities, often requiring extensive post-training or suffering from\ndegraded performance due to coarse-grained pruning. In this work, we design a\n$\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to\nprune a large general model into a smaller lightweight expert model, which is\npositioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying\nand pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates\nexpert models without any post-training. Our experiments demonstrate that\n$\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal\nloss in both expert and general capabilities across various models from\ndifferent model families and sizes.", "AI": {"tldr": "This paper presents a new pruning method for large language models that focuses on creating compact expert models without extensive post-training.", "motivation": "To reduce computational resource requirements and improve inference speed for large language models by developing a method that prunes redundant parameters effectively.", "method": "The paper introduces a custom pruning method called Cus-Prun, which identifies and prunes irrelevant neurons based on language, domain, and task dimensions, creating lightweight expert models without the need for post-training.", "result": "Cus-Prun consistently outperforms existing pruning techniques, demonstrating minimal loss in capabilities while transitioning from large general models to smaller expert models.", "conclusion": "The proposed Cus-Prun method allows for efficient pruning of large language models, making it possible to tailor them to specific tasks without degradation in performance.", "key_contributions": ["Introduction of Cus-Prun for efficient pruning of large language models", "Demonstrated performance improvements over existing pruning methods", "Facilitates the creation of expert models without requiring post-training adjustments."], "limitations": "", "keywords": ["large language models", "pruning", "expert models", "machine learning", "natural language processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.02573", "pdf": "https://arxiv.org/pdf/2506.02573.pdf", "abs": "https://arxiv.org/abs/2506.02573", "title": "IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages", "authors": ["Muhammad Falensi Azmi", "Muhammad Dehan Al Kautsar", "Alfan Farizki Wicaksono", "Fajri Koto"], "categories": ["cs.CL"], "comment": "25 pages", "summary": "Although region-specific large language models (LLMs) are increasingly\ndeveloped, their safety remains underexplored, particularly in culturally\ndiverse settings like Indonesia, where sensitivity to local norms is essential\nand highly valued by the community. In this work, we present IndoSafety, the\nfirst high-quality, human-verified safety evaluation dataset tailored for the\nIndonesian context, covering five language varieties: formal and colloquial\nIndonesian, along with three major local languages: Javanese, Sundanese, and\nMinangkabau. IndoSafety is constructed by extending prior safety frameworks to\ndevelop a taxonomy that captures Indonesia's sociocultural context. We find\nthat existing Indonesian-centric LLMs often generate unsafe outputs,\nparticularly in colloquial and local language settings, while fine-tuning on\nIndoSafety significantly improves safety while preserving task performance. Our\nwork highlights the critical need for culturally grounded safety evaluation and\nprovides a concrete step toward responsible LLM deployment in multilingual\nsettings. Warning: This paper contains example data that may be offensive,\nharmful, or biased.", "AI": {"tldr": "The paper introduces IndoSafety, a safety evaluation dataset for Indonesian LLMs, emphasizing culturally sensitive safety measures.", "motivation": "To address the lack of safety evaluations for region-specific LLMs, particularly in culturally diverse environments like Indonesia, where understanding local norms is critical.", "method": "Developed a high-quality, human-verified dataset that evaluates the safety of Indonesian LLMs across various language varieties and established a taxonomy for safety evaluation that reflects Indonesia's sociocultural context.", "result": "IndoSafety demonstrated that existing LLMs for Indonesian often output unsafe content, especially in colloquial and local languages, but showed improvement in safety with fine-tuning on this new dataset without sacrificing performance.", "conclusion": "The study emphasizes the importance of culturally grounded safety measures for LLMs and the potential of IndoSafety to enhance responsible LLM deployment in multilingual contexts.", "key_contributions": ["Introduction of IndoSafety dataset tailored for Indonesian LLMs", "Cultural context incorporated into safety evaluation framework", "Demonstrated effectiveness of the dataset in improving LLM safety"], "limitations": "Focus limited to Indonesian languages and may not be generalizable to other cultural contexts; examples included may be offensive or biased.", "keywords": ["safety evaluation", "large language models", "cultural sensitivity", "Indonesian languages", "multilingual LLMs"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2506.02584", "pdf": "https://arxiv.org/pdf/2506.02584.pdf", "abs": "https://arxiv.org/abs/2506.02584", "title": "Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning", "authors": ["Sarenne Wallbridge", "Christoph Minixhofer", "Catherine Lai", "Peter Bell"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "People exploit the predictability of lexical structures during text\ncomprehension. Though predictable structure is also present in speech, the\ndegree to which prosody, e.g. intonation, tempo, and loudness, contributes to\nsuch structure independently of the lexical content is unclear. This study\nleverages self-supervised learning (SSL) to examine the temporal granularity of\nstructures in the acoustic correlates of prosody. Representations from our\nproposed Masked Prosody Model can predict perceptual labels dependent on local\ninformation, such as word boundaries, but provide the most value for labels\ninvolving longer-term structures, like emotion recognition. Probing experiments\nacross various perceptual labels show strong relative gains over untransformed\npitch, energy, and voice activity features. Our results reveal the importance\nof SSL training objective timescale and highlight the value of complex\nSSL-encoded structures compared to more constrained classical structures.", "AI": {"tldr": "This study investigates the role of prosody in speech comprehension using self-supervised learning to analyze acoustic structures.", "motivation": "Understanding how prosodic features like intonation, tempo, and loudness contribute to speech comprehension beyond lexical content.", "method": "The study employs a self-supervised learning approach with the Masked Prosody Model to analyze and predict perceptual labels from prosodic features.", "result": "The Masked Prosody Model shows improved prediction capabilities for longer-term structures, such as emotion recognition, over traditional acoustic features.", "conclusion": "The research highlights the significance of training objectives in self-supervised learning and the advantages of complex SSL-encoded structures.", "key_contributions": ["Introduces the Masked Prosody Model for analyzing prosody in speech.", "Demonstrates relative gains in predicting perceptual labels using SSL compared to traditional methods.", "Highlights the importance of timescale in SSL training objectives for prosodic analysis."], "limitations": "", "keywords": ["self-supervised learning", "prosody", "speech comprehension", "acoustic features", "emotion recognition"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.02589", "pdf": "https://arxiv.org/pdf/2506.02589.pdf", "abs": "https://arxiv.org/abs/2506.02589", "title": "Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM", "authors": ["Maria Levchenko"], "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7; H.3.3"], "comment": null, "summary": "This paper addresses the challenge of Named Entity Recognition (NER) for\nperson names within the specialized domain of Russian news texts concerning\ncultural events. The study utilizes the unique SPbLitGuide dataset, a\ncollection of event announcements from Saint Petersburg spanning 1999 to 2019.\nA comparative evaluation of diverse NER models is presented, encompassing\nestablished transformer-based architectures such as DeepPavlov, RoBERTa, and\nSpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,\nand GPT-4o. Key findings highlight the superior performance of GPT-4o when\nprovided with specific prompting for JSON output, achieving an F1 score of\n0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The\nresearch contributes to a deeper understanding of current NER model\ncapabilities and limitations when applied to morphologically rich languages\nlike Russian within the cultural heritage domain, offering insights for\nresearchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)\nachieves F1=0.94 for both simple and structured prompts, demonstrating rapid\nprogress across model families and simplified deployment requirements.", "AI": {"tldr": "This paper explores Named Entity Recognition (NER) for person names in Russian news regarding cultural events, using a dataset from Saint Petersburg. It evaluates various models, finding GPT-4o performs best with specific prompts.", "motivation": "To address NER challenges in Russian cultural event texts and evaluate the effectiveness of various NER models.", "method": "Comparison of multiple NER models, including transformer-based architectures (DeepPavlov, RoBERTa, SpaCy) and recent LLMs (GPT-3.5, GPT-4, GPT-4o), on the unique SPbLitGuide dataset.", "result": "GPT-4o achieved an F1 score of 0.93 with specific JSON prompting, while GPT-4 had the highest precision at 0.99.", "conclusion": "The study enhances understanding of NER models for morphologically rich languages like Russian and shows advancements in model capabilities and deployment.", "key_contributions": ["Introduction of the SPbLitGuide dataset for cultural event NER", "Demonstration of superior performance in recent LLMs for NER tasks", "Insights into NER model limitations and capabilities in Russian."], "limitations": "", "keywords": ["Named Entity Recognition", "Russian language", "cultural events", "Large Language Models", "NER models"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.02591", "pdf": "https://arxiv.org/pdf/2506.02591.pdf", "abs": "https://arxiv.org/abs/2506.02591", "title": "On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures", "authors": ["Minh Duc Bui", "Kyung Eun Park", "Goran Glavaš", "Fabian David Schmidt", "Katharina von der Wense"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main (Camera-Ready Version)", "summary": "Measurement systems (e.g., currencies) differ across cultures, but the\nconversions between them are well defined so that humans can state facts using\nany measurement system of their choice. Being available to users from diverse\ncultural backgrounds, large language models (LLMs) should also be able to\nprovide accurate information irrespective of the measurement system at hand.\nUsing newly compiled datasets we test if this is the case for seven open-source\nLLMs, addressing three key research questions: (RQ1) What is the default system\nused by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their\naccuracy vary across different measurement systems? (RQ3) Can LLMs mitigate\npotential challenges w.r.t. underrepresented systems via reasoning? Our\nfindings show that LLMs default to the measurement system predominantly used in\nthe data. Additionally, we observe considerable instability and variance in\nperformance across different measurement systems. While this instability can in\npart be mitigated by employing reasoning methods such as chain-of-thought\n(CoT), this implies longer responses and thereby significantly increases\ntest-time compute (and inference costs), marginalizing users from cultural\nbackgrounds that use underrepresented measurement systems.", "AI": {"tldr": "This paper investigates the ability of large language models (LLMs) to provide accurate conversions across different measurement systems, focusing on their default responses and accuracy variations.", "motivation": "As measurement systems vary across cultures, it's crucial for LLMs to effectively handle these differences in order to provide accurate and relevant information to diverse users.", "method": "The study tests seven open-source LLMs using newly compiled datasets, addressing three research questions related to default measurement systems, accuracy variations across systems, and the effectiveness of reasoning methods.", "result": "LLMs default to the measurement system predominantly present in their training data, show significant performance instability across systems, and can mitigate some challenges through chain-of-thought reasoning, though at a cost of increased response length and compute resources.", "conclusion": "While reasoning methods can help stabilize responses across underrepresented measurement systems, the increased costs may limit usability for affected cultural groups.", "key_contributions": ["Identification of default measurement systems used by LLMs", "Analysis of accuracy variance across different measurement systems", "Evaluation of reasoning methods to improve performance for underrepresented systems."], "limitations": "The study primarily focuses on open-source LLMs and may not generalize to all models; increased compute cost can discourage use in practical applications.", "keywords": ["Large Language Models", "Measurement Systems", "Cultural Differences", "Chain-of-Thought", "Accuracy Variance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02592", "pdf": "https://arxiv.org/pdf/2506.02592.pdf", "abs": "https://arxiv.org/abs/2506.02592", "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments", "authors": ["Zhi-Yuan Chen", "Hao Wang", "Xinyu Zhang", "Enrui Hu", "Yankai Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.", "AI": {"tldr": "This paper introduces the DBG score to measure self-preference bias in large language models (LLMs) using gold judgments to account for response quality, and explores the impact of response style and training data on bias.", "motivation": "To accurately measure self-preference bias in LLMs without conflating it with response quality.", "method": "The DBG score is introduced, which measures self-preference bias by comparing model scores to gold judgments, followed by extensive experiments across various LLMs and analysis of contributing factors to bias.", "result": "The DBG score effectively isolates self-preference bias from response quality influences, and findings indicate that response style and post-training data can mitigate this bias.", "conclusion": "Using the DBG score allows for a clearer understanding of self-preference bias in LLMs, with important insights on how to alleviate it through specific intervention strategies.", "key_contributions": ["Introduction of the DBG score for measuring self-preference bias", "Comprehensive experiments across various LLMs", "Insights into factors influencing self-preference bias"], "limitations": "", "keywords": ["self-preference bias", "large language models", "DBG score"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02596", "pdf": "https://arxiv.org/pdf/2506.02596.pdf", "abs": "https://arxiv.org/abs/2506.02596", "title": "EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing", "authors": ["Fan Gao", "Dongyuan Li", "Ding Xia", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Baojun Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chinese essay writing and its evaluation are critical in educational\ncontexts, yet the capabilities of Large Language Models (LLMs) in this domain\nremain largely underexplored. Existing benchmarks often rely on coarse-grained\ntext quality metrics, largely overlooking the structural and rhetorical\ncomplexities of Chinese essays, particularly across diverse genres. To address\nthis gap, we propose \\benchName, a multi-genre benchmark specifically designed\nfor Chinese essay writing across four major genres: Argumentative, Narrative,\nDescriptive, and Expository. We curate and refine a total of 728 real-world\nprompts to ensure authenticity and meticulously categorize them into the\n\\textit{Open-Ended} and \\textit{Constrained} sets to capture diverse writing\nscenarios. To reliably evaluate generated essays, we develop a fine-grained,\ngenre-specific scoring framework that hierarchically aggregates scores. We\nfurther validate our evaluation protocol through a comprehensive human\nagreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their\nstrengths and limitations across genres and instruction types. With \\benchName,\nwe aim to advance LLM-based Chinese essay evaluation and inspire future\nresearch on improving essay generation in educational settings.", "AI": {"tldr": "This paper introduces a multi-genre benchmark for evaluating Chinese essay writing using Large Language Models (LLMs), addressing structural and rhetorical complexities.", "motivation": "To explore the capabilities of LLMs in generating and evaluating Chinese essays, which are critical in educational contexts but underexplored in existing benchmarks.", "method": "The authors propose \benchName, a multi-genre benchmark for Chinese essays across four genres: Argumentative, Narrative, Descriptive, and Expository, including 728 real-world prompts and a genre-specific scoring framework.", "result": "The evaluation framework allows for fine-grained assessment of essays, and the benchmarking of 15 large LLMs provides an analysis of their performance and limitations across genres.", "conclusion": "The study aims to enhance LLM-based evaluation of Chinese essays and encourages further research to improve essay generation in educational settings.", "key_contributions": ["Development of a multi-genre benchmark for Chinese essay writing", "Creation of a fine-grained, genre-specific scoring framework", "Comprehensive benchmarking of 15 LLMs across various essay genres."], "limitations": "", "keywords": ["Chinese essay writing", "Large Language Models", "benchmark", "education", "evaluation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.02627", "pdf": "https://arxiv.org/pdf/2506.02627.pdf", "abs": "https://arxiv.org/abs/2506.02627", "title": "Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning", "authors": ["Ömer Tarik Özyilmaz", "Matt Coler", "Matias Valdenegro-Toro"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Although commercial Arabic automatic speech recognition (ASR) systems support\nModern Standard Arabic (MSA), they struggle with dialectal speech. We\ninvestigate the effect of fine-tuning OpenAI's Whisper on five major Arabic\ndialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common\nVoice for MSA and the MASC dataset for dialectal speech. We evaluate MSA\ntraining size effects, benefits of pre-training on MSA data, and\ndialect-specific versus dialect-pooled models. We find that small amounts of\nMSA fine-tuning data yield substantial improvements for smaller models,\nmatching larger non-fine-tuned models. While MSA pre-training shows minimal\nbenefit, suggesting limited shared features between MSA and dialects, our\ndialect-pooled models perform comparably to dialect-specific ones. This\nindicates that pooling dialectal data, when properly balanced, can help address\ndata scarcity in low-resource ASR without significant performance loss.", "AI": {"tldr": "Fine-tuning OpenAI's Whisper on Arabic dialects improves ASR performance, finding that dialect-pooled models can match dialect-specific ones without significant performance loss.", "motivation": "To address the limitations of commercial Arabic ASR systems in understanding dialectal Arabic and improve their performance.", "method": "Investigated the effect of fine-tuning Whisper on five Arabic dialects using Mozilla Common Voice and MASC dataset, assessing MSA training size and model types.", "result": "Small MSA fine-tuning data substantially improves smaller models, and dialect-pooled models perform comparably to dialect-specific ones.", "conclusion": "Pooling dialectal data can effectively mitigate data scarcity issues in low-resource ASR systems with minimal performance loss.", "key_contributions": ["Fine-tuning Whisper for dialectal Arabic ASR", "Insights on MSA pre-training benefits", "Comparison of dialect-pooled and dialect-specific models"], "limitations": "Limited benefits from MSA pre-training suggest minimal shared features between MSA and dialects.", "keywords": ["Arabic ASR", "Dialectal Speech", "Whisper", "Machine Learning", "Speech Recognition"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.02659", "pdf": "https://arxiv.org/pdf/2506.02659.pdf", "abs": "https://arxiv.org/abs/2506.02659", "title": "Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs", "authors": ["Manon Reusens", "Bart Baesens", "David Jurgens"], "categories": ["cs.CL"], "comment": null, "summary": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub.", "AI": {"tldr": "This paper introduces a standardized framework to analyze the consistency of personalized Large Language Models (LLMs) when assigned specific personas across various tasks.", "motivation": "To address the gap in understanding how consistently LLMs adhere to predefined personas in different writing styles and tasks.", "method": "A new framework is developed to evaluate persona consistency across four categories—happiness, occupation, personality, and political stance—through various task dimensions.", "result": "Findings indicate that consistency in responses is influenced by factors such as the assigned persona, stereotypes, and model design, with consistency varying across tasks and increasing with more structured tasks.", "conclusion": "The study highlights the importance of understanding persona consistency in LLMs, providing insights into model behavior across different contexts.", "key_contributions": ["Development of a standardized framework for evaluating persona consistency in LLMs", "Identification of factors influencing persona consistency", "Insights into how task structure impacts LLM responses"], "limitations": "", "keywords": ["Large Language Models", "persona consistency", "human-computer interaction", "natural language processing", "task evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02672", "pdf": "https://arxiv.org/pdf/2506.02672.pdf", "abs": "https://arxiv.org/abs/2506.02672", "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "authors": ["Shihan Dou", "Ming Zhang", "Chenhao Huang", "Jiayi Chen", "Feng Chen", "Shichun Liu", "Yan Liu", "Chenxiao Liu", "Cheng Zhong", "Zongzhang Zhang", "Tao Gui", "Chao Xin", "Wei Chengzhi", "Lin Yan", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "47 pages, 24 figures", "summary": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.", "AI": {"tldr": "EvaLearn is a benchmark for assessing the learning capability and efficiency of large language models (LLMs) using 648 challenging problems across six task types, with performance evaluation based on sequential problem-solving.", "motivation": "To evaluate LLMs not just on static performance but on their ability to learn and improve through experience, addressing an underexplored aspect of model potential.", "method": "EvaLearn requires models to solve problems sequentially rather than in parallel, using five comprehensive automated metrics to assess learning capability and efficiency.", "result": "Benchmarking nine models showed diverse performance profiles, highlighting that models with strong static abilities do not necessarily excel in learning capability.", "conclusion": "EvaLearn provides a new evaluation perspective that may help bridge the gap between model performance and human capabilities, encouraging improved evaluation methodologies in LLM research.", "key_contributions": ["Introduction of a novel benchmark (EvaLearn) for evaluating LLM learning capabilities.", "In-depth analysis of model performance based on sequential problem-solving.", "Availability of datasets and evaluation framework to promote future research."], "limitations": "", "keywords": ["large language models", "learning capability", "benchmark", "evaluation metrics", "AI research"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02678", "pdf": "https://arxiv.org/pdf/2506.02678.pdf", "abs": "https://arxiv.org/abs/2506.02678", "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "comment": null, "summary": "Large Language Models (LLMs) have recently achieved remarkable progress by\nleveraging Reinforcement Learning and extended Chain-of-Thought (CoT)\ntechniques. However, the challenge of performing efficient language\nreasoning--especially during inference with extremely long outputs--has drawn\nincreasing attention from the research community. In this work, we propose a\ndynamic ratio-based training pipeline that does not rely on sophisticated data\nannotations or interpolation between multiple models. We continuously balance\nthe weights between the model's System-1 and System-2 data to eliminate\nredundant reasoning processes while preserving the model's reasoning\ncapability. We validate our approach across models on DeepSeek-R1-Distill-7B\nand DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying\ndifficulty levels. Our method significantly reduces the number of output tokens\nby nearly 40% while maintaining the accuracy of the reasoning. Our code and\ndata will be available soon.", "AI": {"tldr": "This paper presents a dynamic ratio-based training pipeline for improving language reasoning in LLMs without extensive data annotations, achieving significant output reduction while maintaining accuracy.", "motivation": "To address the challenge of efficient language reasoning with long outputs in Large Language Models (LLMs).", "method": "A dynamic ratio-based training pipeline that balances weights between System-1 and System-2 data to eliminate redundant reasoning processes.", "result": "The proposed method reduces the number of output tokens by nearly 40% while preserving reasoning accuracy across various models and benchmarks.", "conclusion": "The dynamic training pipeline effectively enhances language reasoning efficiency in LLMs without needing complex data annotations.", "key_contributions": ["Dynamic ratio-based training pipeline for LLMs", "Reduction of output tokens by nearly 40%", "Validation across multiple models and benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Chain-of-Thought", "language reasoning", "dynamic training"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.02683", "pdf": "https://arxiv.org/pdf/2506.02683.pdf", "abs": "https://arxiv.org/abs/2506.02683", "title": "Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints", "authors": ["Zhengdong Lu", "Weikai Lu", "Yiling Tao", "Yun Dai", "ZiXuan Chen", "Huiping Zhuang", "Cen Chen", "Hao Peng", "Ziqian Zeng"], "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advances in Large Language Models (LLMs), planning tasks\nstill present challenges for LLM-based agents. Existing planning methods face\ntwo key limitations: heavy constraints and cascading errors. To address these\nlimitations, we propose a novel parallel planning paradigm, which Decomposes,\nPlans for subtasks in Parallel, and Merges subplans into a final plan (DPPM).\nSpecifically, DPPM decomposes the complex task based on constraints into\nsubtasks, generates the subplan for each subtask in parallel, and merges them\ninto a global plan. In addition, our approach incorporates a verification and\nrefinement module, enabling error correction and conflict resolution.\nExperimental results demonstrate that DPPM significantly outperforms existing\nmethods in travel planning tasks.", "AI": {"tldr": "This paper introduces a novel parallel planning paradigm (DPPM) for LLM-based agents to improve performance in planning tasks.", "motivation": "Existing LLM planning methods struggle with heavy constraints and cascading errors.", "method": "The DPPM approach decomposes complex tasks into subtasks, plans for each subtask in parallel, and merges them into a final plan. It also includes a verification and refinement module.", "result": "DPPM significantly outperforms existing methods in travel planning tasks.", "conclusion": "The DPPM paradigm effectively addresses the limitations of current planning methods in LLMs, enhancing their utility in complex planning scenarios.", "key_contributions": ["Introduction of the DPPM paradigm", "Verification and refinement module for error correction", "Improvement of LLM planning tasks in travel planning"], "limitations": "", "keywords": ["Large Language Models", "planning", "subtasks", "parallel processing", "travel planning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02689", "pdf": "https://arxiv.org/pdf/2506.02689.pdf", "abs": "https://arxiv.org/abs/2506.02689", "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching", "authors": ["Liang Yue", "Yihong Tang", "Kehai Chen", "Jie Liu", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'\ninstruction-following capabilities and task-specific performance. However,\nobtaining high-quality fine-tuning data for large models is challenging due to\ndata collection difficulties and high production costs. To address this, we\npropose MASTER, a novel data augmentation method that enriches original data\nthrough interactions among multiple agents with varying cognitive levels. We\nsimulate three pedagogically grounded teaching scenarios, leveraging\nmulti-agent conversations to generate high-quality teacher-student interaction\ndata. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented\nfrom existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.\nExperiments show that models fine-tuned with BOOST-QA perform excellently\nacross multiple benchmarks, demonstrating strong multitask generalization.\nNotably, MASTER significantly improves models' reasoning abilities in complex\ntasks, providing valuable insights for future research.", "AI": {"tldr": "MASTER is a novel data augmentation method for enhancing instruction-following capabilities in NLP models through simulated multi-agent interactions, creating the BOOST-QA dataset for fine-tuning.", "motivation": "Improving instruction-following capabilities and task-specific performance of NLP models by generating high-quality fine-tuning data.", "method": "Introducing MASTER, a data augmentation approach using multiple agents with varied cognitive levels to create teacher-student interaction data.", "result": "Models fine-tuned with BOOST-QA demonstrate excellent performance across benchmarks and significant enhancement in reasoning abilities for complex tasks.", "conclusion": "MASTER provides a promising avenue for generating high-quality training data, improving multitask generalization in NLP models.", "key_contributions": ["Development of MASTER for data augmentation using multi-agent interactions.", "Creation of BOOST-QA dataset from existing datasets for fine-tuning.", "Improvement in reasoning abilities of models on complex tasks."], "limitations": "", "keywords": ["data augmentation", "instruction fine-tuning", "multi-agent interactions", "NLP", "reasoning abilities"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.02701", "pdf": "https://arxiv.org/pdf/2506.02701.pdf", "abs": "https://arxiv.org/abs/2506.02701", "title": "On Entity Identification in Language Models", "authors": ["Masaki Sakata", "Sho Yokoi", "Benjamin Heinzerling", "Takumi Ito", "Kentaro Inui"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings; 26 pages, 13 figures, 9 tables", "summary": "We analyze the extent to which internal representations of language models\n(LMs) identify and distinguish mentions of named entities, focusing on the\nmany-to-many correspondence between entities and their mentions. We first\nformulate two problems of entity mentions -- ambiguity and variability -- and\npropose a framework analogous to clustering quality metrics. Specifically, we\nquantify through cluster analysis of LM internal representations the extent to\nwhich mentions of the same entity cluster together and mentions of different\nentities remain separated. Our experiments examine five Transformer-based\nautoregressive models, showing that they effectively identify and distinguish\nentities with metrics analogous to precision and recall ranging from 0.66 to\n0.9. Further analysis reveals that entity-related information is compactly\nrepresented in a low-dimensional linear subspace at early LM layers.\nAdditionally, we clarify how the characteristics of entity representations\ninfluence word prediction performance. These findings are interpreted through\nthe lens of isomorphism between LM representations and entity-centric knowledge\nstructures in the real world, providing insights into how LMs internally\norganize and use entity information.", "AI": {"tldr": "This paper analyzes how language models represent and distinguish named entities through cluster analysis of their internal representations, providing insights into their effectiveness and organization of entity-related information.", "motivation": "The study aims to explore how language models identify and manage named entities within their internal representations, focusing on the challenges of ambiguity and variability in entity mentions.", "method": "The authors introduce a framework that applies cluster analysis to evaluate the internal representations of five Transformer-based autoregressive models, measuring their ability to cluster identical entities and separate different entities using precision and recall metrics.", "result": "The experiments demonstrate that the models achieve precision and recall scores between 0.66 and 0.9, indicating effective differentiation between entity mentions, with significant findings around low-dimensional linear subspace representation in early layers of LMs.", "conclusion": "The research reveals that internal representations in LMs effectively manage entity information, influencing predictive performance and paralleling real-world entity-centric knowledge structures.", "key_contributions": ["Development of a framework for analyzing entity mention representations in language models", "Empirical assessment of Transformer-based models' effectiveness in distinguishing entities", "Insights into the dimensionality and structure of entity-related information in language models."], "limitations": "", "keywords": ["language models", "named entities", "cluster analysis", "Transformer", "entity representation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.02726", "pdf": "https://arxiv.org/pdf/2506.02726.pdf", "abs": "https://arxiv.org/abs/2506.02726", "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models", "authors": ["Qihang Yan", "Xinyu Zhang", "Luming Guo", "Qi Zhang", "Feifan Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "Large Language Models (LLMs) struggle with accuracy, domain-specific\nreasoning, and interpretability in vertical domains. Traditional preference\nalignment methods like Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) often overlook the underlying knowledge\nsources and reasoning logic. This paper introduces RACE-Align\n(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel\nframework designed to address these limitations. RACE-Align systematically\nconstructs a binary preference dataset incorporating external knowledge support\nand explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO\nalgorithm. The core innovation lies in its preference data construction\nstrategy: it integrates AI-driven retrieval for factual grounding, enhancing\nknowledgeability and accuracy, and emphasizes the optimization of\ndomain-specific CoT, treating the reasoning process itself as a key preference\ndimension. A multi-stage, AI-driven refinement pipeline cost-effectively\ngenerates these preference pairs. Experimental validation in Traditional\nChinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that\nRACE-Align significantly outperforms the original base model and a model\nfine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed\nacross multiple dimensions, including answer accuracy, information richness,\napplication of TCM thinking patterns, logicality and depth of reasoning, and\ninterpretability. These findings suggest RACE-Align offers an effective pathway\nto enhance LLMs' knowledge application, reasoning reliability, and process\ntransparency in complex vertical domains.", "AI": {"tldr": "This paper presents RACE-Align, a framework that enhances Large Language Models in vertical domains by integrating retrieval-augmented knowledge and Chain-of-Thought reasoning for improved accuracy and interpretability.", "motivation": "LLMs face challenges with accuracy, reasoning, and interpretability in specific domains, necessitating new alignment strategies that leverage knowledge and reasoning.", "method": "RACE-Align utilizes a binary preference dataset with external knowledge and Chain-of-Thought reasoning to align LLMs through Direct Preference Optimization, supported by an AI-driven refinement pipeline.", "result": "Experimental results demonstrate that RACE-Align outperforms the base model and a fine-tuned model (SFT) in Traditional Chinese Medicine, showing improvements in accuracy, reasoning depth, and interpretability.", "conclusion": "RACE-Align provides a viable solution to enhance the knowledge application and reasoning of LLMs in complex domains, addressing key limitations of traditional alignment methods.", "key_contributions": ["Introduction of RACE-Align framework", "Integration of retrieval mechanisms and Chain-of-Thought reasoning", "Demonstrated effectiveness in Traditional Chinese Medicine domain"], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Chain-of-Thought", "Traditional Chinese Medicine", "Knowledge Retrieval"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02740", "pdf": "https://arxiv.org/pdf/2506.02740.pdf", "abs": "https://arxiv.org/abs/2506.02740", "title": "Stereotypical gender actions can be extracted from Web text", "authors": ["Amaç Herdağdelen", "Marco Baroni"], "categories": ["cs.CL"], "comment": null, "summary": "We extracted gender-specific actions from text corpora and Twitter, and\ncompared them to stereotypical expectations of people. We used Open Mind Common\nSense (OMCS), a commonsense knowledge repository, to focus on actions that are\npertinent to common sense and daily life of humans. We use the gender\ninformation of Twitter users and Web-corpus-based pronoun/name gender\nheuristics to compute the gender bias of the actions. With high recall, we\nobtained a Spearman correlation of 0.47 between corpus-based predictions and a\nhuman gold standard, and an area under the ROC curve of 0.76 when predicting\nthe polarity of the gold standard. We conclude that it is feasible to use\nnatural text (and a Twitter-derived corpus in particular) in order to augment\ncommonsense repositories with the stereotypical gender expectations of actions.\nWe also present a dataset of 441 commonsense actions with human judges' ratings\non whether the action is typically/slightly masculine/feminine (or neutral),\nand another larger dataset of 21,442 actions automatically rated by the methods\nwe investigate in this study.", "AI": {"tldr": "This paper explores gender biases in actions derived from text corpora and social media, leveraging commonsense knowledge to evaluate stereotypes.", "motivation": "To understand how gender stereotypes manifest in actions and to evaluate their alignment with common sense using natural language sources.", "method": "A comparison of gender-specific actions extracted from Twitter and other text corpora against stereotypical expectations, employing gender heuristics derived from Web corpora.", "result": "Achieved a Spearman correlation of 0.47 with a human gold standard and an ROC curve of 0.76 for predicting action polarity, indicating a good fit between corpus data and human judgment.", "conclusion": "Natural text data, particularly from Twitter, can effectively augment commonsense understanding of gender expectations in actions; datasets of both manually and automatically rated actions were developed.", "key_contributions": ["Developed a dataset of 441 commonsense actions with gender ratings", "Created a larger dataset of 21,442 actions rated automatically for gender bias", "Demonstrated feasibility of using social media data for augmenting commonsense knowledge with gender stereotypes."], "limitations": "The study relies on Twitter data which may not fully represent broader societal views; potential biases in user demographics are acknowledged.", "keywords": ["gender bias", "commonsense knowledge", "natural language processing", "Twitter", "stereotypes"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.02753", "pdf": "https://arxiv.org/pdf/2506.02753.pdf", "abs": "https://arxiv.org/abs/2506.02753", "title": "Multi-task Learning with Active Learning for Arabic Offensive Speech Detection", "authors": ["Aisha Alansari", "Hamzah Luqman"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth of social media has amplified the spread of offensive,\nviolent, and vulgar speech, which poses serious societal and cybersecurity\nconcerns. Detecting such content in Arabic text is particularly complex due to\nlimited labeled data, dialectal variations, and the language's inherent\ncomplexity. This paper proposes a novel framework that integrates multi-task\nlearning (MTL) with active learning to enhance offensive speech detection in\nArabic social media text. By jointly training on two auxiliary tasks, violent\nand vulgar speech, the model leverages shared representations to improve the\ndetection accuracy of the offensive speech. Our approach dynamically adjusts\ntask weights during training to balance the contribution of each task and\noptimize performance. To address the scarcity of labeled data, we employ an\nactive learning strategy through several uncertainty sampling techniques to\niteratively select the most informative samples for model training. We also\nintroduce weighted emoji handling to better capture semantic cues. Experimental\nresults on the OSACT2022 dataset show that the proposed framework achieves a\nstate-of-the-art macro F1-score of 85.42%, outperforming existing methods while\nusing significantly fewer fine-tuning samples. The findings of this study\nhighlight the potential of integrating MTL with active learning for efficient\nand accurate offensive language detection in resource-constrained settings.", "AI": {"tldr": "A novel framework combining multi-task learning and active learning to improve offensive speech detection in Arabic social media text.", "motivation": "To address the increasing issues of offensive speech on social media, particularly in Arabic, where complexities arise from dialectal variations and limited labeled data.", "method": "The framework integrates multi-task learning with active learning, employing several uncertainty sampling techniques to select the most informative samples for model training, and dynamically adjusts task weights during training.", "result": "Achieved a state-of-the-art macro F1-score of 85.42% on the OSACT2022 dataset, outperforming existing methods while requiring fewer fine-tuning samples.", "conclusion": "The study demonstrates the effectiveness of combining MTL and active learning for accurate offensive language detection in Arabic, particularly in resource-constrained environments.", "key_contributions": ["Integration of multi-task learning with active learning for Arabic offensive speech detection", "Dynamic task weight adjustment for improved model performance", "Novel handling of emojis to enhance semantic understanding"], "limitations": "", "keywords": ["offensive speech detection", "multi-task learning", "active learning", "Arabic text", "natural language processing"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2506.02758", "pdf": "https://arxiv.org/pdf/2506.02758.pdf", "abs": "https://arxiv.org/abs/2506.02758", "title": "Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs", "authors": ["Stefano Bannò", "Kate Knill", "Mark Gales"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications", "summary": "Vocabulary use is a fundamental aspect of second language (L2) proficiency.\nTo date, its assessment by automated systems has typically examined the\ncontext-independent, or part-of-speech (PoS) related use of words. This paper\nintroduces a novel approach to enable fine-grained vocabulary evaluation\nexploiting the precise use of words within a sentence. The scheme combines\nlarge language models (LLMs) with the English Vocabulary Profile (EVP). The EVP\nis a standard lexical resource that enables in-context vocabulary use to be\nlinked with proficiency level. We evaluate the ability of LLMs to assign\nproficiency levels to individual words as they appear in L2 learner writing,\naddressing key challenges such as polysemy, contextual variation, and\nmulti-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to\nexploit additional semantic information that yields improved performance. We\nalso explore correlations between word-level proficiency and essay-level\nproficiency. Finally, the approach is applied to examine the consistency of the\nEVP proficiency levels. Results show that LLMs are well-suited for the task of\nvocabulary assessment.", "AI": {"tldr": "This paper presents a novel method for assessing second language vocabulary proficiency using large language models (LLMs) and the English Vocabulary Profile (EVP).", "motivation": "The research aims to improve the assessment of vocabulary use in second language proficiency by utilizing contextual information rather than relying solely on part-of-speech tagging.", "method": "The method combines LLMs with the EVP to evaluate vocabulary use in context, focusing on challenges like polysemy and contextual variation. The performance of LLMs is compared to a baseline that uses part-of-speech-based assessments.", "result": "LLMs demonstrate improved performance in assigning proficiency levels to words based on contextual use in L2 learner writing compared to traditional PoS-based methods.", "conclusion": "LLMs are effective tools for vocabulary assessment, providing better insights into both word-level and essay-level proficiency in second language writing.", "key_contributions": ["Introduces a fine-grained vocabulary assessment method using LLMs", "Links in-context vocabulary use with proficiency levels using EVP", "Demonstrates the advantages of LLMs over traditional PoS-based approaches in language assessment."], "limitations": "", "keywords": ["vocabulary assessment", "language proficiency", "large language models", "second language learning", "contextual evaluation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.02803", "pdf": "https://arxiv.org/pdf/2506.02803.pdf", "abs": "https://arxiv.org/abs/2506.02803", "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking", "authors": ["Sifan Li", "Yujun Cai", "Yiwei Wang"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.", "AI": {"tldr": "This paper introduces HC-Bench, a benchmark to evaluate vision-language models (VLMs) on their ability to detect hidden content in images, highlighting their reliance on high-level semantics and proposing a solution that significantly improves accuracy.", "motivation": "To assess the ability of vision-language models in detecting hidden content in various images, revealing their limitations compared to human perception.", "method": "The authors created HC-Bench, a benchmark consisting of 112 images with hidden elements, and tested leading VLMs, finding they performed poorly. They then proposed SemVink, which involves scaling images to low resolutions to improve accuracy.", "result": "Leading VLMs achieved near-zero accuracy on the benchmark, but using SemVink resulted in over 99% accuracy by reducing visual noise.", "conclusion": "The study highlights a critical limitation in VLMs, suggesting a need for hybrid models that integrate multi-scale processing to enhance robustness in real-world applications.", "key_contributions": ["Development of HC-Bench for evaluating VLMs on hidden content detection", "Introduction of SemVink for improved image processing", "Identification of a critical flaw in VLMs related to semantic overreliance"], "limitations": "The study primarily focuses on a specific type of visual task and may not generalize across all visual perception challenges.", "keywords": ["vision-language models", "hidden content detection", "semantic visual thinking"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.02818", "pdf": "https://arxiv.org/pdf/2506.02818.pdf", "abs": "https://arxiv.org/abs/2506.02818", "title": "ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations", "authors": ["Ekaterina Grishina", "Mikhail Gorbunov", "Maxim Rakhuba"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by ACL Findings", "summary": "Large language models (LLMs) demonstrate impressive results in natural\nlanguage processing tasks but require a significant amount of computational and\nmemory resources. Structured matrix representations are a promising way for\nreducing the number of parameters of these models. However, it seems\nunrealistic to expect that weight matrices of pretrained models can be\naccurately represented by structured matrices without any fine-tuning. To\novercome this issue, we utilize the fact that LLM output is invariant under\ncertain orthogonal transformations of weight matrices. This insight can be\nleveraged to identify transformations that significantly improve the\ncompressibility of weights within structured classes. The proposed approach is\napplicable to various types of structured matrices that support efficient\nprojection operations. Code is available at\nhttps://github.com/GrishKate/ProcrustesGPT", "AI": {"tldr": "The paper discusses a method for improving the compressibility of large language models (LLMs) by applying orthogonal transformations to weight matrices.", "motivation": "To address the challenge of high computational and memory demands of LLMs while maintaining performance.", "method": "The authors explore leveraging orthogonal transformations of weight matrices to enhance the compressibility of the parameters in structured matrix representations, facilitating effective model size reduction without fine-tuning.", "result": "The proposed approach shows that certain orthogonal transformations can significantly improve the compressibility of weights in structured matrices, making LLMs more efficient.", "conclusion": "This work presents a feasible strategy for reducing parameter counts in LLMs, which could lead to more resource-efficient NLP models.", "key_contributions": ["Introduction of orthogonal transformations to enhance weight compressibility in LLMs.", "Demonstration of the method's applicability across various structured matrix types.", "Release of code for practical implementation."], "limitations": "The approach relies on the assumption that pretrained weights can be transformed without fine-tuning, which may not hold in all cases.", "keywords": ["large language models", "compression", "orthogonal transformations", "structured matrices", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.02827", "pdf": "https://arxiv.org/pdf/2506.02827.pdf", "abs": "https://arxiv.org/abs/2506.02827", "title": "TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference", "authors": ["Yulin Dou", "Jiangming Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can effectively elicit human preferences through\nmulti-turn dialogue. Complex tasks can be accomplished through iterative\nclarifying questions and final responses generated by an LLM acting as a\nquestioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches\nbased on self-taught reasoning struggle to identify optimal dialogue\ntrajectories and avoid irrelevant questions to the tasks. To address this\nlimitation, we propose TO-GATE, a novel framework that enhances question\ngeneration through trajectory optimization, which consists of two key\ncomponents: a clarification resolver that generates optimal questioning\ntrajectories, and a summarizer that ensures task-aligned final responses. The\ntrajectory optimization enables the model to produce effective elicitation\nquestions and summary responses tailored to specific tasks. Experimental\nresults demonstrate that TO-GATE significantly outperforms baseline methods,\nachieving a 9.32% improvement on standard preference elicitation tasks.", "AI": {"tldr": "TO-GATE enhances question generation for LLMs by optimizing questioning trajectories, improving preference elicitation tasks.", "motivation": "Existing approaches struggle with optimal dialogue trajectories and may generate irrelevant questions.", "method": "The paper presents TO-GATE, which consists of a clarification resolver to generate optimal questioning trajectories and a summarizer for task-aligned final responses.", "result": "TO-GATE significantly outperformed baseline methods with a 9.32% improvement in efficacy on preference elicitation tasks.", "conclusion": "The trajectory optimization improves the ability of models to produce effective questions and aligned summary responses.", "key_contributions": ["Introduction of TO-GATE framework for trajectory optimization in question generation", "Enhanced task-alignment of final responses through a summarizer", "Demonstrated significant improvement over existing baseline methods in preference elicitation tasks"], "limitations": "", "keywords": ["Large Language Models", "Preference Elicitation", "Question Generation", "Trajectory Optimization", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.02872", "pdf": "https://arxiv.org/pdf/2506.02872.pdf", "abs": "https://arxiv.org/abs/2506.02872", "title": "Token and Span Classification for Entity Recognition in French Historical Encyclopedias", "authors": ["Ludovic Moncla", "Hédi Zeghidi"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Named Entity Recognition (NER) in historical texts presents unique challenges\ndue to non-standardized language, archaic orthography, and nested or\noverlapping entities. This study benchmarks a diverse set of NER approaches,\nranging from classical Conditional Random Fields (CRFs) and spaCy-based models\nto transformer-based architectures such as CamemBERT and sequence-labeling\nmodels like Flair. Experiments are conducted on the GeoEDdA dataset, a richly\nannotated corpus derived from 18th-century French encyclopedias. We propose\nframing NER as both token-level and span-level classification to accommodate\ncomplex nested entity structures typical of historical documents. Additionally,\nwe evaluate the emerging potential of few-shot prompting with generative\nlanguage models for low-resource scenarios. Our results demonstrate that while\ntransformer-based models achieve state-of-the-art performance, especially on\nnested entities, generative models offer promising alternatives when labeled\ndata are scarce. The study highlights ongoing challenges in historical NER and\nsuggests avenues for hybrid approaches combining symbolic and neural methods to\nbetter capture the intricacies of early modern French text.", "AI": {"tldr": "This paper benchmarks various Named Entity Recognition (NER) approaches on historical texts, addressing challenges of non-standard language and nested entities.", "motivation": "To tackle the unique challenges of Named Entity Recognition in historical texts due to non-standard language and archaic orthography, alongside complex nested and overlapping entities.", "method": "The study benchmarks classical CRFs, spaCy models, transformer architectures like CamemBERT, and sequence-labeling models such as Flair on the GeoEDdA dataset, proposing both token-level and span-level classification.", "result": "Transformer-based models deliver state-of-the-art performance on nested entities, while few-shot prompting with generative models shows promise in low-resource scenarios.", "conclusion": "The research underscores the need for hybrid approaches that integrate symbolic and neural methods to address the complexities of early modern French texts and historical NER challenges.", "key_contributions": ["Benchmarking diverse NER approaches on a historical dataset", "Proposing a dual framing of NER as token-level and span-level classification", "Exploring few-shot prompting with generative models for low-resource NER"], "limitations": "Challenges in representing the complexities of historical texts persist, and the effectiveness of generative models in various contexts needs further exploration.", "keywords": ["Named Entity Recognition", "historical texts", "generative models", "transformer-based architectures", "Natural Language Processing"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.02878", "pdf": "https://arxiv.org/pdf/2506.02878.pdf", "abs": "https://arxiv.org/abs/2506.02878", "title": "CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective", "authors": ["Jintian Shao", "Yiming Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.\nChain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.", "AI": {"tldr": "This paper critiques Chain-of-Thought (CoT) prompting in Large Language Models (LLMs), arguing it enhances performance by imitating reasoning rather than showcasing genuine reasoning capability.", "motivation": "To challenge the widespread belief that Chain-of-Thought prompting leads to real abstract reasoning in Large Language Models.", "method": "The authors theoretically analyze the function of Chain-of-Thought prompting, suggesting it serves as a structural constraint rather than a true reasoning enhancer.", "result": "The analysis indicates that CoT prompting primarily helps models to generate sequences that mimic reasoning by leveraging their capacity for pattern recognition, rather than fostering abstract reasoning.", "conclusion": "The findings question the narrative of emergent reasoning in models prompted with Chain-of-Thought and highlight the importance of understanding the mechanisms behind these model outputs.", "key_contributions": ["Theoretical critique of Chain-of-Thought prompting", "Reinterpretation of LLM performance enhancements as imitation of reasoning", "Insight into model behavior regarding sequence generation and pattern matching"], "limitations": "", "keywords": ["Chain-of-Thought", "Large Language Models", "reasoning", "sequence prediction", "pattern matching"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02894", "pdf": "https://arxiv.org/pdf/2506.02894.pdf", "abs": "https://arxiv.org/abs/2506.02894", "title": "A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation", "authors": ["Verena Blaschke", "Miriam Winkler", "Constantin Förster", "Gabriele Wenger-Glemser", "Barbara Plank"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Although Germany has a diverse landscape of dialects, they are\nunderrepresented in current automatic speech recognition (ASR) research. To\nenable studies of how robust models are towards dialectal variation, we present\nBetthupferl, an evaluation dataset containing four hours of read speech in\nthree dialect groups spoken in Southeast Germany (Franconian, Bavarian,\nAlemannic), and half an hour of Standard German speech. We provide both\ndialectal and Standard German transcriptions, and analyze the linguistic\ndifferences between them. We benchmark several multilingual state-of-the-art\nASR models on speech translation into Standard German, and find differences\nbetween how much the output resembles the dialectal vs. standardized\ntranscriptions. Qualitative error analyses of the best ASR model reveal that it\nsometimes normalizes grammatical differences, but often stays closer to the\ndialectal constructions.", "AI": {"tldr": "A dataset named Betthupferl is introduced to improve ASR models' performance on dialects in Germany, containing read speech samples and transcriptions from different dialects and Standard German.", "motivation": "To address the underrepresentation of diverse German dialects in automatic speech recognition (ASR) research and to study model robustness towards dialectal variation.", "method": "The study presents the Betthupferl dataset comprising four hours of speech data across three dialects and Standard German. It benchmarks several ASR models for their ability to translate speech into Standard German and analyzes linguistic differences in transcriptions.", "result": "The analysis reveals that ASR outputs exhibit variations in resemblance to dialectal versus standardized transcriptions, with some models normalizing grammatical differences while others retain dialectal constructions.", "conclusion": "The findings highlight the need to incorporate dialectal variations in ASR research to improve model robustness and accuracy across diverse linguistic features.", "key_contributions": ["Introduction of the Betthupferl dataset for dialectal ASR research", "Benchmarking multilingual ASR models on dialects", "Qualitative analysis of ASR output differences between dialectal and Standard German."], "limitations": "The dataset is limited to dialects spoken in Southeast Germany and may not encompass all German dialectical variations.", "keywords": ["automatic speech recognition", "German dialects", "ASR models", "speech translation", "linguistic analysis"], "importance_score": 6, "read_time_minutes": 7}}
{"id": "2506.02899", "pdf": "https://arxiv.org/pdf/2506.02899.pdf", "abs": "https://arxiv.org/abs/2506.02899", "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator", "authors": ["Yusuke Sakai", "Takumi Goto", "Taro Watanabe"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "We propose IMPARA-GED, a novel reference-free automatic grammatical error\ncorrection (GEC) evaluation method with grammatical error detection (GED)\ncapabilities. We focus on the quality estimator of IMPARA, an existing\nautomatic GEC evaluation method, and construct that of IMPARA-GED using a\npre-trained language model with enhanced GED capabilities. Experimental results\non SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,\ndemonstrate that IMPARA-GED achieves the highest correlation with human\nsentence-level evaluations.", "AI": {"tldr": "IMPARA-GED is a reference-free model for automatic grammatical error correction evaluation that outperforms existing methods by effectively integrating grammatical error detection capabilities.", "motivation": "To enhance the evaluation of automatic grammatical error correction systems by providing a method that does not depend on reference sentences.", "method": "IMPARA-GED leverages a pre-trained language model to assess the quality of grammatical error corrections while detecting errors simultaneously.", "result": "IMPARA-GED demonstrates the highest correlation with human evaluations on the SEEDA dataset, indicating its effectiveness in estimating correction quality.", "conclusion": "The findings suggest that IMPARA-GED offers a superior approach for evaluating GEC systems, potentially improving the development of such technologies.", "key_contributions": ["Introduction of a reference-free evaluation method for GEC", "Enhancement of error detection through the use of a pre-trained language model", "High correlation with human evaluations, validating the proposed approach"], "limitations": "", "keywords": ["grammatical error correction", "error detection", "language model", "evaluation method", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.02911", "pdf": "https://arxiv.org/pdf/2506.02911.pdf", "abs": "https://arxiv.org/abs/2506.02911", "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1", "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.", "AI": {"tldr": "The paper presents CellPuzzles, a task for cell type annotation in single-cell RNA sequencing data, and introduces Cell-o1, a large language model that outperforms previous models in this task by leveraging batch-level reasoning.", "motivation": "To improve cell type annotation in single-cell RNA sequencing data by mimicking human expert workflows and addressing limitations of existing large language models.", "method": "Introduction of the CellPuzzles benchmark for batch-level cell type assignment, using a state-of-the-art 7B LLM (Cell-o1) trained on specialized reasoning techniques and reinforced with batch-level rewards.", "result": "Cell-o1 significantly outperforms the best baseline model (OpenAI's o1), achieving over 73% improvement in batch-level accuracy, with a final accuracy surpassing 19.0%.", "conclusion": "Cell-o1 demonstrates enhanced performance in cell type annotation by incorporating batch-level reasoning, which is crucial for accurate and contextualized cell type assignments.", "key_contributions": ["Introduction of the CellPuzzles task for batch-level cell annotation.", "Development of Cell-o1, an LLM specifically fine-tuned for this task.", "Demonstration of improved accuracy in cell type prediction through novel training methods."], "limitations": "The model's performance is still limited compared to human experts, indicating room for further improvements and research.", "keywords": ["cell type annotation", "single-cell RNA sequencing", "large language models", "CellPuzzles", "batch-level reasoning"], "importance_score": 8, "read_time_minutes": 28}}
{"id": "2506.02921", "pdf": "https://arxiv.org/pdf/2506.02921.pdf", "abs": "https://arxiv.org/abs/2506.02921", "title": "A Controllable Examination for Long-Context Language Models", "authors": ["Yijun Yang", "Zeyu Huang", "Wenhao Zhu", "Zihan Qiu", "Fei Yuan", "Jeff Z. Pan", "Ivan Titov"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: $\\textit{seamless context}$, $\\textit{controllable setting}$, and\n$\\textit{sound evaluation}$. This study introduces $\\textbf{LongBioBench}$, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\n$\\textit{understanding}$, $\\textit{reasoning}$, and $\\textit{trustworthiness}$.\nOur experimental evaluation, which includes $\\textbf{18}$ LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.", "AI": {"tldr": "This paper introduces LongBioBench, a benchmark for evaluating long-context language models using artificially generated biographies to address limitations in existing evaluation frameworks.", "motivation": "To address the limitations of current frameworks for evaluating long-context language models (LCLM), which suffer from complexities and contamination in real-world tasks and lack of coherence in synthetic tasks.", "method": "The study introduces LongBioBench, which uses artificially generated biographies in a controlled environment to assess LCLMs across understanding, reasoning, and trustworthiness.", "result": "Experimental evaluation of 18 LCLMs shows deficiencies in semantic understanding and reasoning, with models becoming less trustworthy as context length increases.", "conclusion": "LongBioBench offers a better trade-off between mirroring authentic language tasks and maintaining controllability, being more interpretable and configurable than previous synthetic benchmarks.", "key_contributions": ["Introduction of LongBioBench as a novel evaluation framework for LCLMs", "Demonstration of LCLMs' deficiencies in understanding, reasoning, and trustworthiness", "Analysis of design choices in existing benchmarks that undermine model evaluations"], "limitations": "", "keywords": ["long-context models", "evaluation framework", "machine learning", "natural language processing", "bioinformatics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.02924", "pdf": "https://arxiv.org/pdf/2506.02924.pdf", "abs": "https://arxiv.org/abs/2506.02924", "title": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification", "authors": ["Diogo A. P. Nunes", "Eugénio Ribeiro"], "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; J.3; H.3.3"], "comment": "12 pages, 1 figure, 6 tables", "summary": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams.", "AI": {"tldr": "A team's approach to classifying depression symptoms using Information Retrieval metrics and foundation model fine-tuning, which led to successful outcomes in an eRisk competition.", "motivation": "To effectively identify symptoms of depression through the lens of Information Retrieval, leveraging a structured approach to categorize sentences based on relevance to specific symptoms in the Beck's Depression Inventory.", "method": "The study framed the task as a binary classification problem, using labeled data to train and validate models. It involved techniques such as fine-tuning foundation models, sentence similarity analysis, LLM prompting, and ensemble methods, with a focus on minimizing class imbalance with synthetic data.", "result": "The approach demonstrated that fine-tuning foundation models provided the best results, particularly when synthetic data was utilized to address class imbalance. The team achieved the highest scores in the official evaluation against 16 other teams.", "conclusion": "The results indicate that different symptoms may require tailored approaches, highlighting the importance of model adaptability and the effectiveness of ensemble techniques in achieving optimal performance.", "key_contributions": ["Framework for sentence classification related to depression symptoms", "Use of foundation model fine-tuning with synthetic data", "Competitive performance in an official evaluation setting"], "limitations": "The reliance on a labeled dataset may restrict generalizability; optimal methods varied by symptom, indicating a need for further exploration.", "keywords": ["Depression", "Information Retrieval", "Foundation Models", "Classification", "Health Informatics"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.02945", "pdf": "https://arxiv.org/pdf/2506.02945.pdf", "abs": "https://arxiv.org/abs/2506.02945", "title": "Quantitative LLM Judges", "authors": ["Aishwarya Sahoo", "Jeevana Kruthi Karnuthala", "Tushar Parmanand Budhwani", "Pranchal Agarwal", "Sankaran Vaidyanathan", "Alexa Siu", "Franck Dernoncourt", "Jennifer Healey", "Nedim Lipka", "Ryan Rossi", "Uttaran Bhattacharya", "Branislav Kveton"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.", "AI": {"tldr": "This paper introduces a framework for LLM-as-a-judge, where a large language model evaluates another LLM's output through quantitative regression models that align LLM evaluation scores with human assessments.", "motivation": "There is a need for efficient evaluation methods for LLM outputs, especially when human feedback is scarce.", "method": "The authors train regression models that use judgments from existing LLM judges to improve their alignment with human evaluators, demonstrating the method's effectiveness across different feedback types.", "result": "Empirical validation on four datasets indicates that the proposed quantitative judges enhance the predictive capabilities of existing judges more efficiently than traditional supervised learning methods.", "conclusion": "Quantitative judges provide a scalable and statistically efficient solution for LLM evaluations, improving output assessments in situations with limited human feedback.", "key_contributions": ["Introduction of the LLM-as-a-judge framework", "Development of regression models for aligning LLM scores with human assessments", "Demonstration of effectiveness across various feedback types"], "limitations": "The empirical validation relies on specific datasets and may not generalize universally across all LLM applications.", "keywords": ["LLM", "evaluation", "regression models", "human feedback", "quantitative judges"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.02951", "pdf": "https://arxiv.org/pdf/2506.02951.pdf", "abs": "https://arxiv.org/abs/2506.02951", "title": "Adaptive Graph Pruning for Multi-Agent Communication", "authors": ["Boyi Li", "Zhonghan Zhao", "Der-Horng Lee", "Gaoang Wang"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks.", "AI": {"tldr": "This paper presents Adaptive Graph Pruning (AGP), a novel framework that optimizes the number of agents and communication structures in multi-agent systems using Large Language Models (LLMs).", "motivation": "Current multi-agent systems are limited by fixed agent numbers and static communication structures, which hinder adaptability to complex tasks.", "method": "The framework employs a two-stage training strategy to determine optimal agent quantities and communication topologies, optimizing both hard-pruning and soft-pruning in a dynamic manner.", "result": "AGP achieves state-of-the-art results with a performance increase of 2.58% to 9.84% across six benchmarks, adapts efficiently to specific tasks, and reduces token consumption by over 90%.", "conclusion": "The AGP framework outperforms existing methods across multiple benchmarks with fewer training requirements, demonstrating its effectiveness in adaptive multi-agent collaboration.", "key_contributions": ["Proposed a task-adaptive multi-agent collaboration framework.", "Achieved state-of-the-art performance across multiple benchmarks.", "Reduced token consumption significantly while maintaining high performance."], "limitations": "", "keywords": ["Multi-Agent Systems", "Large Language Models", "Adaptive Graph Pruning", "Collaboration Framework", "Token Efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02959", "pdf": "https://arxiv.org/pdf/2506.02959.pdf", "abs": "https://arxiv.org/abs/2506.02959", "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring", "authors": ["Zhixiong Su", "Yichen Wang", "Herun Wan", "Zhaohan Zhang", "Minnan Luo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement.", "AI": {"tldr": "The paper addresses the gap in fine-grained machine-generated text (MGT) detection in human-AI coauthored works, proposing a new dataset and evaluating existing detection methods.", "motivation": "Existing MGT detection primarily focuses on binary, document-level scenarios, ignoring the complexities of texts jointly written by humans and LLMs.", "method": "The authors introduce the HACo-Det dataset for human-AI coauthored texts and retrofit seven existing document-level detectors to perform word-level detection. They evaluate these on the proposed dataset for both word- and sentence-level detection.", "result": "Detectors exhibit difficulty with fine-grained detection, achieving an average F1 score of 0.462, with fine-tuned models demonstrating superior performance.", "conclusion": "Fine-grained co-authored text detection remains a challenging problem, requiring further analysis of performance factors and improvements to existing methods.", "key_contributions": ["Introduction of the HACo-Det dataset for coauthored text detection", "Evaluation of existing document-level detectors in a fine-grained context", "Analysis of performance influences and identification of limitations in current methods."], "limitations": "Current methods struggle with fine-grained detection and have notable performance issues, necessitating further research and development.", "keywords": ["machine-generated text detection", "human-AI coauthorship", "fine-grained detection", "HACo-Det dataset", "NLP"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.02961", "pdf": "https://arxiv.org/pdf/2506.02961.pdf", "abs": "https://arxiv.org/abs/2506.02961", "title": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models", "authors": ["Yan Gao", "Massimo Roberto Scamarcia", "Javier Fernandez-Marques", "Mohammad Naseri", "Chong Shen Ng", "Dimitris Stripelis", "Zexi Li", "Tao Shen", "Jiamu Bai", "Daoyuan Chen", "Zikai Zhang", "Rui Hu", "InSeo Song", "Lee KangYoon", "Hong Jia", "Ting Dang", "Junyan Wang", "Zheyuan Liu", "Daniel Janes Beutel", "Lingjuan Lyu", "Nicholas D. Lane"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved state-of-the-art results across\ndiverse domains, yet their development remains reliant on vast amounts of\npublicly available data, raising concerns about data scarcity and the lack of\naccess to domain-specific, sensitive information. Federated Learning (FL)\npresents a compelling framework to address these challenges by enabling\ndecentralized fine-tuning on pre-trained LLMs without sharing raw data.\nHowever, the compatibility and performance of pre-trained LLMs in FL settings\nremain largely under explored. We introduce the FlowerTune LLM Leaderboard, a\nfirst-of-its-kind benchmarking suite designed to evaluate federated fine-tuning\nof LLMs across four diverse domains: general NLP, finance, medical, and coding.\nEach domain includes federated instruction-tuning datasets and domain-specific\nevaluation metrics. Our results, obtained through a collaborative, open-source\nand community-driven approach, provide the first comprehensive comparison\nacross 26 pre-trained LLMs with different aggregation and fine-tuning\nstrategies under federated settings, offering actionable insights into model\nperformance, resource constraints, and domain adaptation. This work lays the\nfoundation for developing privacy-preserving, domain-specialized LLMs for\nreal-world applications.", "AI": {"tldr": "Introduction of the FlowerTune LLM Leaderboard for benchmarking federated fine-tuning of LLMs across multiple domains.", "motivation": "To address data scarcity and the challenge of developing domain-specific models by utilizing Federated Learning for fine-tuning LLMs without sharing sensitive data.", "method": "A benchmarking suite called FlowerTune LLM Leaderboard is introduced, evaluating federated fine-tuning of LLMs in four domains: NLP, finance, medical, and coding, using community-driven datasets and specific evaluation metrics.", "result": "The study compares 26 pre-trained LLMs using various aggregation and fine-tuning strategies, offering insights into their performance and domain adaptability under federated settings.", "conclusion": "The findings establish a foundational framework for the creation of privacy-preserving, domain-specialized LLMs suitable for practical applications.", "key_contributions": ["First comprehensive benchmarking suite for federated fine-tuning of LLMs", "Comparison across diverse domains and LLMs", "Insights into model performance and domain adaptation strategies"], "limitations": "", "keywords": ["Federated Learning", "Large Language Models", "Benchmarking", "Domain-specific LLMs", "Privacy-preserving AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.02973", "pdf": "https://arxiv.org/pdf/2506.02973.pdf", "abs": "https://arxiv.org/abs/2506.02973", "title": "Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation", "authors": ["Dingwei Chen", "Ziqiang Liu", "Feiteng Fang", "Chak Tou Leong", "Shiwen Ni", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang", "Chengming Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance.", "AI": {"tldr": "This paper presents PLI, a method to reduce hallucinations in Large Language Models by enhancing factuality through premature layers interpolation, improving coherence without training overhead.", "motivation": "The paper addresses the critical issue of factual inconsistency in Large Language Models, known as 'hallucinations', which existing methods fail to effectively mitigate.", "method": "The proposed method, PLI (Premature Layers Interpolation), is a training-free technique that enhances the output of LLMs by interpolating mathematical formations of premature layers with adjacent layers to extend information processing.", "result": "Experiments on four publicly available datasets show that PLI significantly reduces hallucinations and often outperforms existing baseline methods.", "conclusion": "The findings indicate that the success of PLI is linked to the internal mechanisms of LLMs, enhancing factual coherence without the resource demands of traditional fine-tuning methods.", "key_contributions": ["Introduction of PLI as a training-free method to improve LLM factuality", "Demonstration of improved performance over existing methods in reducing hallucinations", "Insights into the relationship between layer interpolation and LLM internal mechanisms"], "limitations": "", "keywords": ["Large Language Models", "factual consistency", "hallucinations", "training-free method", "interpolation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.02979", "pdf": "https://arxiv.org/pdf/2506.02979.pdf", "abs": "https://arxiv.org/abs/2506.02979", "title": "Towards a Japanese Full-duplex Spoken Dialogue System", "authors": ["Atsumoto Ohashi", "Shinya Iizuka", "Jingjing Jiang", "Ryuichiro Higashinaka"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Full-duplex spoken dialogue systems, which can model simultaneous\nbidirectional features of human conversations such as speech overlaps and\nbackchannels, have attracted significant attention recently. However, the study\nof full-duplex spoken dialogue systems for the Japanese language has been\nlimited, and the research on their development in Japanese remains scarce. In\nthis paper, we present the first publicly available full-duplex spoken dialogue\nmodel in Japanese, which is built upon Moshi, a full-duplex dialogue model in\nEnglish. Our model is trained through a two-stage process: pre-training on a\nlarge-scale spoken dialogue data in Japanese, followed by fine-tuning on\nhigh-quality stereo spoken dialogue data. We further enhance the model's\nperformance by incorporating synthetic dialogue data generated by a\nmulti-stream text-to-speech system. Evaluation experiments demonstrate that the\ntrained model outperforms Japanese baseline models in both naturalness and\nmeaningfulness.", "AI": {"tldr": "This paper presents the first publicly available full-duplex spoken dialogue model in Japanese, demonstrating improved naturalness and meaningfulness over baseline models.", "motivation": "Full-duplex spoken dialogue systems can replicate the bidirectional nature of human conversations, but research in Japanese dialogue systems has been limited.", "method": "The model is built using a two-stage training process: pre-training on large-scale Japanese spoken dialogue data and fine-tuning on high-quality stereo spoken dialogue data, supplemented with synthetic data from a multi-stream text-to-speech system.", "result": "The model shows superior performance compared to existing Japanese baseline models, in terms of both naturalness and meaningfulness of conversation.", "conclusion": "The development of this full-duplex spoken dialogue model is a significant step forward for conversational AI in the Japanese language.", "key_contributions": ["First publicly available full-duplex spoken dialogue model in Japanese.", "Two-stage training process utilizing both real and synthetic data.", "Demonstrated performance improvement over existing Japanese models."], "limitations": "", "keywords": ["full-duplex dialogue", "spoken dialogue systems", "Japanese language"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.02987", "pdf": "https://arxiv.org/pdf/2506.02987.pdf", "abs": "https://arxiv.org/abs/2506.02987", "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis", "authors": ["Richard Armitage"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "12 pages, 1 Table", "summary": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.", "AI": {"tldr": "This paper evaluates the performance of leading large language models (LLMs) in answering primary care examination questions to support clinical practice.", "motivation": "To explore the capabilities of advanced LLMs in the context of primary care education and their potential role in clinical practice.", "method": "Four leading LLMs (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) were assessed on 100 multiple choice questions from the MRCGP exam context, simulating responses expected from UK general practitioners.", "result": "o3 achieved a score of 99%, while Claude Opus 4, Grok3, and Gemini 2.5 Pro scored 95.0% each, all far exceeding the average peer score of 73.0%.", "conclusion": "The study indicates that advanced LLMs, particularly o3, can significantly support primary care delivery by outperforming average practitioners in clinical assessments.", "key_contributions": ["Demonstrated high efficacy of LLMs in answering primary care exam questions", "Establishes performance benchmarks for LLMs against human practitioners", "Advocated for the integration of reasoning models in clinical education and practice"], "limitations": "The assessment covered a specific range of examination questions and may not generalize across all aspects of primary care education.", "keywords": ["Large language models", "Primary care", "Clinical practice", "Artificial intelligence", "Medical education"], "importance_score": 10, "read_time_minutes": 12}}
{"id": "2506.02995", "pdf": "https://arxiv.org/pdf/2506.02995.pdf", "abs": "https://arxiv.org/abs/2506.02995", "title": "It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems", "authors": ["Iuliia Zaitova", "Badr M. Abdullah", "Wei Xue", "Dietrich Klakow", "Bernd Möbius", "Tania Avgustinova"], "categories": ["cs.CL"], "comment": "13 pages, 3 figures, ACL 2025", "summary": "Idioms are defined as a group of words with a figurative meaning not\ndeducible from their individual components. Although modern machine translation\nsystems have made remarkable progress, translating idioms remains a major\nchallenge, especially for speech-to-text systems, where research on this topic\nis notably sparse. In this paper, we systematically evaluate idiom translation\nas compared to conventional news translation in both text-to-text machine\ntranslation (MT) and speech-to-text translation (SLT) systems across two\nlanguage pairs (German to English, Russian to English). We compare\nstate-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large\nv3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large\nLanguage Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal\nthat SLT systems experience a pronounced performance drop on idiomatic data,\noften reverting to literal translations even in higher layers, whereas MT\nsystems and Large Language Models demonstrate better handling of idioms. These\nfindings underscore the need for idiom-specific strategies and improved\ninternal representations in SLT architectures.", "AI": {"tldr": "This paper investigates the challenges of translating idioms in speech-to-text (SLT) and text-to-text (MT) systems, highlighting significant performance drops in SLT systems compared to MT and large language models.", "motivation": "To address the major challenge of idiom translation in modern machine translation systems, particularly in the context of speech-to-text applications where research is lacking.", "method": "The study systematically evaluates idiom translation performance versus conventional news translation in both text-to-text and speech-to-text systems across German-English and Russian-English language pairs, comparing various state-of-the-art SLT and MT systems.", "result": "The evaluation shows that SLT systems exhibit a marked performance decline on idiomatic data, often resulting in literal translations, while MT systems and large language models perform comparatively better in managing idioms.", "conclusion": "The findings emphasize the necessity for idiom-specific strategies and enhancements in the internal representations of SLT architectures to improve translation accuracy.", "key_contributions": ["Systematic evaluation of idiom translation in SLT vs MT systems", "Comparison of state-of-the-art SLT and MT systems across multiple language pairs", "Highlighting the necessity for improved idiomatic handling strategies in SLT architectures"], "limitations": "", "keywords": ["idiom translation", "speech-to-text", "machine translation", "large language models", "semantic understanding"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.02998", "pdf": "https://arxiv.org/pdf/2506.02998.pdf", "abs": "https://arxiv.org/abs/2506.02998", "title": "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems", "authors": ["Đorđe Klisura", "Astrid R Bernaga Torres", "Anna Karen Gárate-Escamilla", "Rajesh Roshan Biswal", "Ke Yang", "Hilal Pataci", "Anthony Rios"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information.", "AI": {"tldr": "A multi-agent framework is proposed to reduce dialectal biases in privacy policy question answering systems, enhancing accessibility for diverse populations.", "motivation": "To address the complexities of privacy policies that limit accessibility and reduce performance disparities for speakers of non-standard English dialects in Privacy Policy QA systems.", "method": "The proposed framework includes a Dialect Agent for translating queries to Standard American English while preserving dialectal intent and a Privacy Policy Agent for improving predictions with domain expertise, avoiding the need for retraining or dialect-specific fine-tuning.", "result": "The framework improved GPT-4o-mini's zero-shot accuracy significantly on PrivacyQA (from 0.394 to 0.601) and PolicyQA (from 0.352 to 0.464), outperforming or matching few-shot baselines without extra training data.", "conclusion": "The results indicate that structured collaboration in agent systems can effectively reduce dialectal biases, emphasizing the urgency for NLP systems to consider linguistic diversity for fair access to privacy information.", "key_contributions": ["Introduction of a multi-agent framework to address dialect biases in QA systems", "Integration of a Dialect Agent and a Privacy Policy Agent for improved query handling", "Demonstration of substantial accuracy improvements without additional training"], "limitations": "", "keywords": ["privacy policies", "dialectal biases", "NLP", "machine learning", "human-centered design"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2506.03009", "pdf": "https://arxiv.org/pdf/2506.03009.pdf", "abs": "https://arxiv.org/abs/2506.03009", "title": "Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech", "authors": ["Florian Ludwig", "Torsten Zesch", "Frederike Zufall"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts.", "AI": {"tldr": "The paper investigates conditioning Large Language Models (LLMs) at different levels of abstraction to assess legal problems, particularly hate speech classification in social media posts according to the German Criminal Code.", "motivation": "To explore how well LLMs can internalize various levels of legal abstraction and their ability to classify hate speech under legal definitions.", "method": "The study involves conditioning LLMs using different levels of legal abstraction and testing their performance on classifying social media posts related to incitement to hatred as defined by German law.", "result": "The findings indicate a significant performance gap between LLMs and legal experts, with models lacking depth in understanding legal tasks and often producing contradictory answers, despite reasonable performance in identifying target groups.", "conclusion": "While some models showed potential in applying concrete legal knowledge, significant improvements are needed for LLMs to effectively assess hate speech and align with expert legal reasoning.", "key_contributions": ["Investigation of LLMs' ability to handle legal abstraction levels", "Evaluation of hate speech classification based on German Criminal Law", "Insights into the limitations of LLMs in legal contexts"], "limitations": "LLMs struggle with deep understanding of legal tasks, often leading to contradictions and hallucinations in their responses.", "keywords": ["Large Language Models", "Hate Speech", "Legal Abstraction", "German Criminal Code", "Natural Language Processing"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2506.03011", "pdf": "https://arxiv.org/pdf/2506.03011.pdf", "abs": "https://arxiv.org/abs/2506.03011", "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers", "authors": ["Aditya Bharat Soni", "Boxuan Li", "Xingyao Wang", "Valerie Chen", "Graham Neubig"], "categories": ["cs.CL"], "comment": null, "summary": "Modern human labor is characterized by specialization; we train for years and\ndevelop particular tools that allow us to perform well across a variety of\ntasks. In addition, AI agents have been specialized for domains such as\nsoftware engineering, web navigation, and workflow automation. However, this\nresults in agents that are good for one thing but fail to generalize beyond\ntheir intended scope. One reason for this is that agent developers provide a\nhighly specialized set of tools or make architectural decisions optimized for a\nspecific use case or benchmark. In this work, we ask the question: what is the\nminimal set of general tools that can be used to achieve high performance\nacross a diverse set of tasks? Our answer is OpenHands-Versa, a generalist\nagent built with a modest number of general tools: code editing and execution,\nweb search, as well as multimodal web browsing and file access. Importantly,\nOpenHands-Versa demonstrates superior or competitive performance over leading\nspecialized agents across three diverse and challenging benchmarks: SWE-Bench\nMultimodal, GAIA, and The Agent Company, outperforming the best-performing\npreviously published results with absolute improvements in success rate of 9.1,\n1.3, and 9.1 points respectively. Further, we show how existing\nstate-of-the-art multi-agent systems fail to generalize beyond their target\ndomains. These results demonstrate the feasibility of developing a generalist\nagent to solve diverse tasks and establish OpenHands-Versa as a strong baseline\nfor future research.", "AI": {"tldr": "OpenHands-Versa is a generalist AI agent that utilizes a minimal set of tools to achieve high performance across various tasks, outperforming specialized agents.", "motivation": "To explore the minimal set of general tools required for high performance across diverse tasks, addressing limitations of specialized AI agents.", "method": "The research involves the development and evaluation of OpenHands-Versa, a generalist agent, using benchmarks such as SWE-Bench Multimodal, GAIA, and The Agent Company.", "result": "OpenHands-Versa outperformed specialized agents, showing absolute improvements in success rates of 9.1, 1.3, and 9.1 points across the three benchmarks.", "conclusion": "The study demonstrates the feasibility of creating a generalist agent like OpenHands-Versa, which provides a strong baseline for future research in AI agents.", "key_contributions": ["Introduction of OpenHands-Versa as a generalist agent", "Demonstration of superior performance over specialized agents", "Establishment of new performance benchmarks for generalist agents"], "limitations": "", "keywords": ["generalist agent", "AI tools", "performance benchmarking"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.03035", "pdf": "https://arxiv.org/pdf/2506.03035.pdf", "abs": "https://arxiv.org/abs/2506.03035", "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning", "authors": ["Pierre Lepagnol", "Sahar Ghannay", "Thomas Gerald", "Christophe Servan", "Sophie Rosset"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Conference paper accepted to INTERSPEECH 2025", "summary": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.", "AI": {"tldr": "This paper explores the enhancement of Spoken Language Understanding (SLU) tasks through example selection using Information Retrieval (IR) techniques, demonstrating improved performance with existing prompt lengths.", "motivation": "The motivation behind this work is to improve the reliability of SLU systems, which are critical in many applications and typically rely on large amounts of training data that may not be available for all tasks or languages.", "method": "The authors propose leveraging Information Retrieval methods to select examples that enhance prompts used in SLU tasks, thus adapting instruction-tuned large language models for better performance in few-shot scenarios.", "result": "Experimental results indicate that the incorporation of lexical IR methods significantly improves SLU performance on various benchmarks without the need to increase the prompt length.", "conclusion": "The findings suggest that IR techniques can be effectively utilized for prompt enhancement in SLU tasks, providing a valuable tool for improving model performance in scenarios with limited data.", "key_contributions": ["Introduction of example selection using IR for SLU tasks", "Demonstration of improved SLU performance with enhanced prompts", "Validation of methods across various SLU benchmarks"], "limitations": "", "keywords": ["Spoken Language Understanding", "Information Retrieval", "Large Language Models", "Prompt Engineering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.03038", "pdf": "https://arxiv.org/pdf/2506.03038.pdf", "abs": "https://arxiv.org/abs/2506.03038", "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "authors": ["Jintian Shao", "Yiming Cheng"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents.", "AI": {"tldr": "This paper analyzes the limitations of the VAPO framework in reinforcement learning for long-chain reasoning in large language models.", "motivation": "To explore fundamental limitations in the modeling of deep, long-term value and step-by-step policy guidance in reinforcement learning for complex reasoning tasks.", "method": "The paper employs a theoretical analysis to examine issues related to credit assignment, value function capacity with abstracted goals, and the translation of global value signals into local policy improvements.", "result": "The analysis identifies inherent difficulties in utilizing VAPO for long-term reasoning tasks, emphasizing the challenges of dealing with sparse rewards.", "conclusion": "Understanding VAPO's limitations can inform future research directions for enhancing the robustness of LLM agents in reinforcement learning contexts.", "key_contributions": ["Theoretical exploration of VAPO's limitations in modeling long-term value.", "Identification of challenges in credit assignment and policy guidance in RL for LLMs.", "Suggestions for future research directions to improve LLM agents."], "limitations": "Focused on theoretical aspects and might not include empirical validation of the findings.", "keywords": ["Reinforcement Learning", "Large Language Models", "Long-Term Value Modeling", "Policy Guidance", "Theoretical Analysis"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.03051", "pdf": "https://arxiv.org/pdf/2506.03051.pdf", "abs": "https://arxiv.org/abs/2506.03051", "title": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs", "authors": ["Yuval Kansal", "Shmuel Berman", "Lydia Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages.", "AI": {"tldr": "This paper evaluates the factual accuracy of the Llama3.1 family of Large Language Models (LLMs) in answering educational questions in multiple languages, highlighting issues of correctness and biases in language performance.", "motivation": "To address the need for factuality in educational tools that utilize LLMs, particularly as their adoption in education increases.", "method": "The paper assesses the ability of the Llama3.1 models to answer factual questions relevant to middle and high school curricula across various languages.", "result": "The evaluation revealed that the Llama3.1 models frequently produce inaccurate responses and show an increase in biases against less commonly spoken languages.", "conclusion": "Ensuring the correctness of LLMs in educational contexts is crucial, as performance varies significantly across languages and is prone to biases.", "key_contributions": ["Evaluation of Llama3.1 models' factual accuracy in education-related contexts.", "Identification of language bias in LLMs against rare languages.", "Recommendations for improving LLM performance in educational applications."], "limitations": "The study does not cover all LLM models and focuses specifically on the Llama3.1 family, which may not represent all educational LLMs.", "keywords": ["Large Language Models", "factual accuracy", "education", "language bias", "Llama3.1"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.03090", "pdf": "https://arxiv.org/pdf/2506.03090.pdf", "abs": "https://arxiv.org/abs/2506.03090", "title": "Literary Evidence Retrieval via Long-Context Language Models", "authors": ["Katherine Thai", "Mohit Iyyer"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction.", "AI": {"tldr": "The paper examines how well long-context language models can understand literary fiction through literary evidence retrieval tasks, revealing gaps in performance between different model types.", "motivation": "To investigate the capabilities of long-context language models in understanding and analyzing literary texts, specifically in literary evidence retrieval.", "method": "The authors repurposed the RELiC dataset and created a benchmark requiring LLMs to generate missing quotations from literary works, assessing their ability to perform narrative reasoning and textual examination.", "result": "Experiments indicate that Gemini Pro 2.5 outperforms human experts in accuracy (62.5% vs. 50%), while the best open-weight model only achieves 29.1% accuracy, revealing significant gaps in interpretive reasoning capabilities between model types.", "conclusion": "The study highlights the proficiency of advanced models in literary tasks while emphasizing their ongoing struggles with nuanced signals and the issue of overgeneration, posing challenges for future literary analysis applications.", "key_contributions": ["Introduction of a benchmark for literary evidence retrieval with LLMs", "Demonstration of model performance exceeding human experts", "Release of a curated literary dataset for further research"], "limitations": "The paper notes that even strong models have limitations in understanding nuanced literary signals and face issues with overgeneration, suggesting areas for improvement.", "keywords": ["long-context language models", "literary evidence retrieval", "narrative reasoning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.03101", "pdf": "https://arxiv.org/pdf/2506.03101.pdf", "abs": "https://arxiv.org/abs/2506.03101", "title": "Beyond Text Compression: Evaluating Tokenizers Across Scales", "authors": ["Jonas F. Lotz", "António V. Lopes", "Stephan Peitz", "Hendra Setiawan", "Leonardo Emili"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The choice of tokenizer can profoundly impact language model performance, yet\naccessible and reliable evaluations of tokenizer quality remain an open\nchallenge. Inspired by scaling consistency, we show that smaller models can\naccurately predict significant differences in tokenizer impact on larger models\nat a fraction of the compute cost. By systematically evaluating both\nEnglish-centric and multilingual tokenizers, we find that tokenizer choice has\nnegligible effects on tasks in English but results in consistent performance\ndifferences in multilingual settings. We propose new intrinsic tokenizer\nmetrics inspired by Zipf's law that correlate more strongly with downstream\nperformance than text compression when modeling unseen languages. By combining\nseveral metrics to capture multiple aspects of tokenizer behavior, we develop a\nreliable framework for intrinsic tokenizer evaluations. Our work offers a more\nefficient path to informed tokenizer selection in future language model\ndevelopment.", "AI": {"tldr": "This paper addresses the impact of tokenizer choice on language model performance, especially in multilingual settings, and proposes a framework for evaluating tokenizer quality without requiring extensive computational resources.", "motivation": "To improve the evaluations of tokenizer quality and its effect on language model performance, particularly for multilingual tasks.", "method": "The authors use smaller models to predict the impact of different tokenizers on larger models, developing new intrinsic metrics based on Zipf's law for evaluation.", "result": "Tokenizer choice is shown to have a minor effect on English tasks, but significant performance variations occur in multilingual contexts. The proposed metrics correlate more with downstream performance than traditional methods.", "conclusion": "The framework presented allows for informed tokenizer selection, simplifying the process of tokenizer evaluation in the development of language models.", "key_contributions": ["Demonstrated the use of smaller models to evaluate tokenizer impact on larger models", "Introduced new intrinsic metrics inspired by Zipf's law for tokenizer evaluation", "Developed a reliable framework for intrinsic tokenizer evaluations"], "limitations": "The evaluations primarily focus on multilingual settings and may not encompass all aspects of tokenizer performance across various tasks.", "keywords": ["tokenizer", "language model", "evaluation", "multilingual", "Zipf's law"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.03106", "pdf": "https://arxiv.org/pdf/2506.03106.pdf", "abs": "https://arxiv.org/abs/2506.03106", "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback", "authors": ["Xiaoying Zhang", "Hao Sun", "Yipeng Zhang", "Kaituo Feng", "Chao Yang", "Helen Meng"], "categories": ["cs.CL", "cs.AI"], "comment": "38 pages", "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.", "AI": {"tldr": "This paper presents Critique-GRPO, an online reinforcement learning framework that integrates numerical and natural language feedback to improve the reasoning capabilities of large language models (LLMs), demonstrating superior performance on various reasoning tasks compared to existing methods.", "motivation": "To address the challenges faced by reinforcement learning with solely numerical feedback, including performance plateaus and failures, by integrating natural language critiques.", "method": "Critique-GRPO combines natural language feedback with numerical rewards in a reinforcement learning framework, enabling simultaneous learning from initial responses and critiques while maintaining exploration.", "result": "Critique-GRPO outperforms both supervised and RL-based fine-tuning methods across multiple reasoning tasks, improving pass@1 scores by approximately 4.5% and 5% on Qwen2.5 and Qwen3 models, respectively.", "conclusion": "The proposed framework deepens our understanding of policy exploration in RL, emphasizing that higher entropy and longer responses do not always lead to more efficient learning.", "key_contributions": ["Introduction of Critique-GRPO framework that integrates natural language feedback into RL processes.", "Demonstration of enhanced performance of LLMs in reasoning tasks using critique-guided refinements.", "Insights that challenge assumptions about exploration efficiency in RL."], "limitations": "The paper does not address the scalability of the Critique-GRPO framework in larger, more complex settings or its applicability to other domains beyond reasoning tasks.", "keywords": ["Reinforcement Learning", "Natural Language Feedback", "Large Language Models", "Critique-Guided Learning", "Policy Optimization"], "importance_score": 9, "read_time_minutes": 38}}
{"id": "2506.03122", "pdf": "https://arxiv.org/pdf/2506.03122.pdf", "abs": "https://arxiv.org/abs/2506.03122", "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation", "authors": ["Prashanth Vijayaraghavan", "Luyao Shi", "Ehsan Degan", "Vandana Mukherjee", "Xin Zhang"], "categories": ["cs.CL"], "comment": "9 Pages (Content), 4 Pages (Appendix), 7 figures, ICML'2025", "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design.", "AI": {"tldr": "Proposes AUTOCIRCUIT-RL, a reinforcement learning framework for automated analog circuit synthesis using Large Language Models.", "motivation": "Addresses the challenges in analog circuit topology synthesis due to vast design search space and strict constraints.", "method": "Utilizes a two-phase approach combining instruction tuning with a large language model and reinforcement learning refinement.", "result": "AUTOCIRCUIT-RL generates ~12% more valid circuits, improves efficiency by ~14%, and reduces duplicate generation rates by ~38%.", "conclusion": "Demonstrates the effectiveness of the framework in scaling to complex circuits while maintaining efficiency and adherence to constraints.", "key_contributions": ["Introduction of AUTOCIRCUIT-RL framework", "Improvement in valid circuit generation", "Significant reduction in duplicate generation rates"], "limitations": "", "keywords": ["Analog Circuit Synthesis", "Reinforcement Learning", "Large Language Models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.03136", "pdf": "https://arxiv.org/pdf/2506.03136.pdf", "abs": "https://arxiv.org/abs/2506.03136", "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning", "authors": ["Yinjie Wang", "Ling Yang", "Ye Tian", "Ke Shen", "Mengdi Wang"], "categories": ["cs.CL"], "comment": "Project: https://github.com/Gen-Verse/CURE", "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE", "AI": {"tldr": "CURE is a reinforcement learning framework that co-evolves code and test generation, improving code generation accuracy through interaction-based learning.", "motivation": "To develop a reinforcement learning framework that enhances coding and unit test generation capabilities without relying on ground-truth supervision.", "method": "CURE employs a unique reward design, allowing the unit tester to learn directly from the coder's mistakes, thus enabling scalable training.", "result": "The ReasonFlux-Coder models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0%, outperforming existing models.", "conclusion": "CURE demonstrates effective improvements in code generation and unit testing, and can also serve as a reward model for reinforcement learning.", "key_contributions": ["Introduction of the CURE framework for co-evolving coding and testing capabilities", "Development of ReasonFlux-Coder models with improved accuracy", "Extensibility of the model to downstream tasks and agentic coding."], "limitations": "", "keywords": ["reinforcement learning", "code generation", "unit testing"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2506.03143", "pdf": "https://arxiv.org/pdf/2506.03143.pdf", "abs": "https://arxiv.org/abs/2506.03143", "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents", "authors": ["Qianhui Wu", "Kanzhi Cheng", "Rui Yang", "Chaoyun Zhang", "Jianwei Yang", "Huiqiang Jiang", "Jian Mu", "Baolin Peng", "Bo Qiao", "Reuben Tan", "Si Qin", "Lars Liden", "Qingwei Lin", "Huan Zhang", "Tong Zhang", "Jianbing Zhang", "Dongmei Zhang", "Jianfeng Gao"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.", "AI": {"tldr": "The paper presents GUI-Actor, a method for coordinate-free visual grounding in VLM-powered GUI agents, demonstrating improved action region localization and performance on multiple benchmarks.", "motivation": "The challenges of localizing screen regions for action execution in VLM-powered GUI agents due to weak spatial-semantic alignment and limitations of existing text-based approaches raise the need for more effective grounding methods.", "method": "GUI-Actor introduces an attention-based action head that aligns a <ACTOR> token with relevant visual patch tokens, allowing the model to propose action regions in one pass, supplemented by a grounding verifier to select the best candidates for action.", "result": "GUI-Actor outperforms previous state-of-the-art methods on various benchmarks, achieving higher scores and better generalization to new screen layouts compared to models like UI-TARS.", "conclusion": "GUI-Actor effectively enhances VLM capabilities for GUI grounding without sacrificing general-purpose performance, demonstrating that fine-tuning a small number of parameters can yield state-of-the-art results.", "key_contributions": ["Introduction of a coordinate-free method for GUI grounding", "An attention-based action head that enhances spatial alignment", "Demonstrated performance improvements over existing models."], "limitations": "", "keywords": ["VLM", "GUI agents", "visual grounding", "action execution", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.03145", "pdf": "https://arxiv.org/pdf/2506.03145.pdf", "abs": "https://arxiv.org/abs/2506.03145", "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM", "authors": ["Pralaypati Ta", "Sriram Venkatesaperumal", "Keerthi Ram", "Mohanasankar Sivaprakasam"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions.", "AI": {"tldr": "This work presents novel methods for constructing a knowledge graph from unlabeled neuroscience literature using large language models.", "motivation": "To accurately retrieve information and discover insights from the extensive and dispersed neuroscience literature, addressing the limitations of current retrieval methods.", "method": "The proposed methods involve constructing a knowledge graph using LLM, neuroscience ontology, and text embeddings from a large-scale unlabeled neuroscience research corpus.", "result": "The methods achieved an F1 score of 0.84 for entity extraction and improved answers to over 54% of questions posed using the knowledge extracted from the KG.", "conclusion": "The proposed approaches significantly enhance knowledge discovery from the neuroscience corpus, demonstrating the effectiveness of using LLMs for knowledge graph construction.", "key_contributions": ["Novel methods for knowledge graph construction from unlabeled data", "Integration of LLMs and neuroscience ontology", "Enhanced information retrieval through an entity-augmented algorithm"], "limitations": "", "keywords": ["knowledge graph", "large language models", "neuroscience", "information retrieval", "entity extraction"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.03149", "pdf": "https://arxiv.org/pdf/2506.03149.pdf", "abs": "https://arxiv.org/abs/2506.03149", "title": "Causal Estimation of Tokenisation Bias", "authors": ["Pietro Lesci", "Clara Meister", "Thomas Hofmann", "Andreas Vlachos", "Tiago Pimentel"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published as a conference paper at ACL 2025", "summary": "Modern language models are typically trained over subword sequences, but\nultimately define probabilities over character-strings. Ideally, the choice of\nthe tokeniser -- which maps character-strings to subwords -- should not affect\nthe probability assigned to the underlying character-string; in practice, it\ndoes. We define this mismatch as tokenisation bias. In this work, we quantify\none particular type of tokenisation bias: the effect of including or not a\nsubword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the\nprobability a trained model assigns to the corresponding characters (i.e.,\n\\textit{``hello''}). Estimating this effect is challenging because each model\nis trained with only one tokeniser. We address this by framing tokenisation\nbias as a causal effect and estimating it using the regression discontinuity\ndesign. Specifically, we exploit the fact that tokenisation algorithms rank\nsubwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an\narbitrary cutoff point. As such, we can estimate a causal effect by comparing\nsimilar subwords around this cutoff. Experimentally, we find that tokenisation\nconsistently affects models' outputs across scales, vocabularies, and\ntokenisers. Notably, a subword's presence in a small model's vocabulary may\nincrease its characters' probability by up to 17 times, highlighting\ntokenisation as a key design choice in language modelling.", "AI": {"tldr": "This study quantifies tokenisation bias in language models by analyzing how the inclusion of subwords in tokenisers affects the probability assigned to character-strings.", "motivation": "To address the mismatch in probabilities assigned to character-strings by language models due to variations in tokenisation methods.", "method": "The authors frame tokenisation bias as a causal effect and estimate it using regression discontinuity design, comparing similar subwords around a cutoff point in tokeniser vocabulary.", "result": "Tokenisation significantly affects model outputs, with the inclusion of a subword in a small model's vocabulary increasing the probability of its characters by up to 17 times, indicating the importance of tokeniser design in language modelling.", "conclusion": "The findings reveal that tokenisation is a critical choice in language model design due to its substantial impact on probability assignments.", "key_contributions": ["Quantification of tokenisation bias in language models.", "Methodology using regression discontinuity design to estimate causal effects of tokenisation.", "Demonstration of significant output variations due to tokenisation choices."], "limitations": "", "keywords": ["tokenisation bias", "language models", "causal effect", "subwords", "ACL 2025"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01998", "pdf": "https://arxiv.org/pdf/2506.01998.pdf", "abs": "https://arxiv.org/abs/2506.01998", "title": "Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents", "authors": ["Takao Fujii", "Katie Seaborn", "Madeleine Steeds", "Jun Kato"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI '25", "summary": "Conversational agents that mimic people have raised questions about the\nethics of anthropomorphizing machines with human social identity cues. Critics\nhave also questioned assumptions of identity neutrality in humanlike agents.\nRecent work has revealed that intersectional Japanese pronouns can elicit\ncomplex and sometimes evasive impressions of agent identity. Yet, the role of\nother \"neutral\" non-pronominal self-referents (NPSR) and voice as a socially\nexpressive medium remains unexplored. In a crowdsourcing study, Japanese\nparticipants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and\nEmber) using seven self-referents. We found strong evidence of voice gendering\nalongside the potential of intersectional self-referents to evade gendering,\ni.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age\nand formality intersected with gendering as per sociolinguistic theories,\nespecially boku and watakushi. This work provides a nuanced take on agent\nidentity perceptions and champions intersectional and culturally-sensitive work\non voice agents.", "AI": {"tldr": "The paper explores how voices of conversational agents can evoke gender perceptions and discusses the impact of Japanese self-referents on agent identity perception.", "motivation": "To investigate the ethics and implications of using human social identity cues in conversational agents, highlighting the role of cultural aspects in shaping user perceptions.", "method": "A crowdsourcing study with 204 Japanese participants evaluated three ChatGPT voices while using seven different non-pronominal self-referents.", "result": "The study found significant evidence of voice gendering and highlighted that intersectional self-referents can create ambiguity in gender perception, enriching the understanding of identity neutrality in voice agents.", "conclusion": "The findings underline the necessity for culturally sensitive approaches in designing conversational agents, advocating for consideration of intersectionality in agent identity.", "key_contributions": ["Demonstrates the impact of voice on gender perception in conversational agents.", "Introduces the concept of non-pronominal self-referents in understanding agent identity.", "Reveals the intersectionality of age, formality, and gendering in agent perception."], "limitations": "Limited to Japanese cultural context and voices; findings may not generalize to other languages or cultures.", "keywords": ["Conversational Agents", "Identity", "Voice Gendering", "Intersectionality", "Cultural Sensitivity"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2110.13658", "pdf": "https://arxiv.org/pdf/2110.13658.pdf", "abs": "https://arxiv.org/abs/2110.13658", "title": "Can Character-based Language Models Improve Downstream Task Performance in Low-Resource and Noisy Language Scenarios?", "authors": ["Arij Riabi", "Benoît Sagot", "Djamé Seddah"], "categories": ["cs.CL", "cs.LG"], "comment": "updated version with new results", "summary": "Recent impressive improvements in NLP, largely based on the success of\ncontextual neural language models, have been mostly demonstrated on at most a\ncouple dozen high-resource languages. Building language models and, more\ngenerally, NLP systems for non-standardized and low-resource languages remains\na challenging task. In this work, we focus on North-African colloquial\ndialectal Arabic written using an extension of the Latin script, called\nNArabizi, found mostly on social media and messaging communication. In this\nlow-resource scenario with data displaying a high level of variability, we\ncompare the downstream performance of a character-based language model on\npart-of-speech tagging and dependency parsing to that of monolingual and\nmultilingual models. We show that a character-based model trained on only 99k\nsentences of NArabizi and fined-tuned on a small treebank of this language\nleads to performance close to those obtained with the same architecture\npre-trained on large multilingual and monolingual models. Confirming these\nresults a on much larger data set of noisy French user-generated content, we\nargue that such character-based language models can be an asset for NLP in\nlow-resource and high language variability set-tings.", "AI": {"tldr": "This paper examines the effectiveness of character-based language models for NLP tasks in low-resource North-African dialectal Arabic (NArabizi), finding comparable performance to multilingual models.", "motivation": "The study aims to address the challenges of building NLP systems for low-resource languages, focusing specifically on North-African colloquial Arabic written in NArabizi.", "method": "The authors compare the performance of a character-based language model on tasks like part-of-speech tagging and dependency parsing against monolingual and multilingual models, using 99k sentences of NArabizi and additional data from noisy French content.", "result": "The character-based model demonstrates performance close to that of larger multilingual and monolingual models, suggesting its viability for low-resource languages", "conclusion": "Character-based language models can provide effective solutions for NLP in low-resource and high variability language scenarios.", "key_contributions": ["Analysis of character-based models in low-resource NLP", "Performance comparison with multilingual models", "Insights on NArabizi as a low-resource language"], "limitations": "", "keywords": ["NLP", "low-resource languages", "character-based models", "NArabizi", "language variability"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2111.00157", "pdf": "https://arxiv.org/pdf/2111.00157.pdf", "abs": "https://arxiv.org/abs/2111.00157", "title": "TransAug: Translate as Augmentation for Sentence Embeddings", "authors": ["Jue Wang"], "categories": ["cs.CL"], "comment": null, "summary": "While contrastive learning greatly advances the representation of sentence\nembeddings, it is still limited by the size of the existing sentence datasets.\nIn this paper, we present TransAug (Translate as Augmentation), which provide\nthe first exploration of utilizing translated sentence pairs as data\naugmentation for text, and introduce a two-stage paradigm to advances the\nstate-of-the-art sentence embeddings. Instead of adopting an encoder trained in\nother languages setting, we first distill a Chinese encoder from a SimCSE\nencoder (pretrained in English), so that their embeddings are close in semantic\nspace, which can be regraded as implicit data augmentation. Then, we only\nupdate the English encoder via cross-lingual contrastive learning and frozen\nthe distilled Chinese encoder. Our approach achieves a new state-of-art on\nstandard semantic textual similarity (STS), outperforming both SimCSE and\nSentence-T5, and the best performance in corresponding tracks on transfer tasks\nevaluated by SentEval.", "AI": {"tldr": "This paper introduces TransAug, a method that utilizes translated sentence pairs for data augmentation in sentence embeddings, successfully achieving state-of-the-art results in semantic textual similarity tasks.", "motivation": "The paper addresses limitations in current contrastive learning techniques for sentence embeddings, particularly the scarcity of large sentence datasets.", "method": "TransAug involves distilling a Chinese encoder from a pre-trained English SimCSE encoder, followed by a cross-lingual contrastive learning approach where the Chinese encoder remains frozen while the English one is updated.", "result": "The approach achieves state-of-the-art performance on standard semantic textual similarity tasks and excels in transfer tasks evaluated with SentEval.", "conclusion": "TransAug demonstrates that translated sentence pairs can effectively improve sentence embedding representations, suggesting new avenues for data augmentation in NLP.", "key_contributions": ["Introduction of Translate as Augmentation (TransAug) for data augmentation in NLP.", "The novel two-stage paradigm for improving sentence embeddings.", "Setting new state-of-the-art results in semantic textual similarity and transfer tasks."], "limitations": "", "keywords": ["Contrastive Learning", "Data Augmentation", "Sentence Embeddings", "Natural Language Processing", "Semantic Textual Similarity"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2303.12892", "pdf": "https://arxiv.org/pdf/2303.12892.pdf", "abs": "https://arxiv.org/abs/2303.12892", "title": "Improving Transformer Performance for French Clinical Notes Classification Using Mixture of Experts on a Limited Dataset", "authors": ["Thanh-Dung Le", "Philippe Jouvet", "Rita Noumeir"], "categories": ["cs.CL", "eess.SP"], "comment": "Accepted for publication in the IEEE Journal of Translational\n  Engineering in Health and Medicine", "summary": "Transformer-based models have shown outstanding results in natural language\nprocessing but face challenges in applications like classifying small-scale\nclinical texts, especially with constrained computational resources. This study\npresents a customized Mixture of Expert (MoE) Transformer models for\nclassifying small-scale French clinical texts at CHU Sainte-Justine Hospital.\nThe MoE-Transformer addresses the dual challenges of effective training with\nlimited data and low-resource computation suitable for in-house hospital use.\nDespite the success of biomedical pre-trained models such as CamemBERT-bio,\nDrBERT, and AliBERT, their high computational demands make them impractical for\nmany clinical settings. Our MoE-Transformer model not only outperforms\nDistillBERT, CamemBERT, FlauBERT, and Transformer models on the same dataset\nbut also achieves impressive results: an accuracy of 87\\%, precision of 87\\%,\nrecall of 85\\%, and F1-score of 86\\%. While the MoE-Transformer does not\nsurpass the performance of biomedical pre-trained BERT models, it can be\ntrained at least 190 times faster, offering a viable alternative for settings\nwith limited data and computational resources. Although the MoE-Transformer\naddresses challenges of generalization gaps and sharp minima, demonstrating\nsome limitations for efficient and accurate clinical text classification, this\nmodel still represents a significant advancement in the field. It is\nparticularly valuable for classifying small French clinical narratives within\nthe privacy and constraints of hospital-based computational resources.", "AI": {"tldr": "The study introduces a Mixture of Expert Transformer model tailored for classifying small-scale French clinical texts, optimized for low-resource settings, achieving competitive performance.", "motivation": "To address the challenges faced by existing Transformer models in classifying small clinical texts with limited computational resources.", "method": "A customized Mixture of Expert (MoE) Transformer model was developed and tested on small-scale clinical texts at CHU Sainte-Justine Hospital.", "result": "The MoE-Transformer achieved 87% accuracy, 87% precision, 85% recall, and 86% F1-score, outperforming DistillBERT and others, while being trainable at 190 times the speed of biomedical pre-trained models.", "conclusion": "The MoE-Transformer provides a viable alternative for classifying clinical texts in settings with limited data and computational resources, despite some limitations in performance compared to advanced biomedical models.", "key_contributions": ["Customized MoE-Transformer model for clinical text classification", "Significant speed advantage in training (190x faster)", "Demonstrated effectiveness with limited resources and data in clinical settings"], "limitations": "Limited performance compared to advanced biomedical pre-trained BERT models; some challenges in generalization and accuracy remain.", "keywords": ["Mixture of Experts", "Transformer", "clinical text classification", "low-resource computation", "French clinical narratives"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2403.04247", "pdf": "https://arxiv.org/pdf/2403.04247.pdf", "abs": "https://arxiv.org/abs/2403.04247", "title": "UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities", "authors": ["Yangning Li", "Qingsong Lv", "Tianyu Yu", "Yinghui Li", "Xuming Hu", "Wenhao Jiang", "Hai-Tao Zheng", "Hui Wang"], "categories": ["cs.CL"], "comment": "Accepted by ICDE 2025", "summary": "Entity Set Expansion (ESE) aims to identify new entities belonging to the\nsame semantic class as the given set of seed entities. Traditional methods\nsolely relied on positive seed entities to represent the target fine-grained\nsemantic class, rendering them tough to represent ultra-fine-grained semantic\nclasses. Specifically, merely relying on positive seed entities leads to two\ninherent shortcomings: (i) Ambiguity among ultra-fine-grained semantic classes.\n(ii) Inability to define ``unwanted'' semantics. Hence, previous ESE methods\nstruggle to address the ultra-fine-grained ESE (Ultra-ESE) task. To solve this\nissue, we first introduce negative seed entities in the inputs, which jointly\ndescribe the ultra-fine-grained semantic class with positive seed entities.\nNegative seed entities eliminate the semantic ambiguity by providing a contrast\nbetween positive and negative attributes. Meanwhile, it provides a\nstraightforward way to express ``unwanted''. To assess model performance in\nUltra-ESE and facilitate further research, we also constructed UltraWiki, the\nfirst large-scale dataset tailored for Ultra-ESE. UltraWiki encompasses 50,973\nentities and 394,097 sentences, alongside 236 ultra-fine-grained semantic\nclasses, where each class is represented with 3-5 positive and negative seed\nentities. Moreover, a retrieval-based framework RetExpan and a generation-based\nframework GenExpan are proposed to provide powerful baselines for Ultra-ESE.\nAdditionally, we devised two strategies to enhance models' comprehension of\nultra-fine-grained entities' semantics: contrastive learning and\nchain-of-thought reasoning. Extensive experiments confirm the effectiveness of\nour proposed strategies and also reveal that there remains a large space for\nimprovement in Ultra-ESE.", "AI": {"tldr": "The paper introduces a novel approach to Entity Set Expansion (ESE) by incorporating negative seed entities, addressing challenges in ultra-fine-grained ESE, and presenting a large-scale dataset called UltraWiki.", "motivation": "Traditional ESE methods struggle with ultra-fine-grained classes due to reliance solely on positive seed entities, causing ambiguity and insufficient unwanted semantics representation.", "method": "The authors propose two frameworks, RetExpan (retrieval-based) and GenExpan (generation-based), and introduce contrastive learning and chain-of-thought reasoning to enhance understanding of ultra-fine-grained entities' semantics while utilizing a new dataset, UltraWiki.", "result": "The proposed methods significantly improve model performance in Ultra-ESE and validate the importance of negative seed entities in eliminating ambiguity and defining unwanted semantics.", "conclusion": "The paper presents effective strategies for Ultra-ESE but acknowledges that there is still considerable potential for further improvements.", "key_contributions": ["Introduction of negative seed entities for ESE", "Creation of UltraWiki dataset for Ultra-ESE", "Development of RetExpan and GenExpan frameworks for ESE tasks"], "limitations": "While the proposed methods show improvement, the authors note that there remain opportunities for enhancing Ultra-ESE models further.", "keywords": ["Entity Set Expansion", "Ultra-fine-grained Entities", "Negative Seed Entities", "Contrastive Learning", "Chain-of-Thought Reasoning"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2403.09073", "pdf": "https://arxiv.org/pdf/2403.09073.pdf", "abs": "https://arxiv.org/abs/2403.09073", "title": "Revealing the Parallel Multilingual Learning within Large Language Models", "authors": ["Yongyu Mu", "Peinan Feng", "Zhiquan Cao", "Yuzhang Wu", "Bei Li", "Chenglong Wang", "Tong Xiao", "Kai Song", "Tongran Liu", "Chunliang Zhang", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2024", "summary": "In this study, we reveal an in-context learning (ICL) capability of\nmultilingual large language models (LLMs): by translating the input to several\nlanguages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which\nsignificantly enhances their comprehension abilities. To test this capability,\nwe design extensive experiments encompassing 8 typical datasets, 7 languages\nand 8 state-of-the-art multilingual LLMs. Experimental results show that (1)\nincorporating more languages help PiM surpass the conventional ICL further; (2)\neven combining with the translations that are inferior to baseline performance\ncan also help. Moreover, by examining the activated neurons in LLMs, we\ndiscover a counterintuitive but interesting phenomenon. Contrary to the common\nthought that PiM would activate more neurons than monolingual input to leverage\nknowledge learned from diverse languages, PiM actually inhibits neurons and\npromotes more precise neuron activation especially when more languages are\nadded. This phenomenon aligns with the neuroscience insight about synaptic\npruning, which removes less used neural connections, strengthens remainders,\nand then enhances brain intelligence.", "AI": {"tldr": "This study investigates the in-context learning capabilities of multilingual large language models (LLMs) using parallel input in multiple languages, revealing significant enhancements in comprehension and neuron activation patterns.", "motivation": "To explore how multilingual input affects the comprehension abilities of LLMs and to demonstrate the potential of in-context learning (ICL) using multiple languages.", "method": "The study involves designing extensive experiments that utilize 8 typical datasets, 7 languages, and 8 state-of-the-art multilingual LLMs to test the effectiveness of Parallel Input in Multiple Languages (PiM).", "result": "Experiments show that using PiM surpasses conventional ICL, with the incorporation of more languages improving performance even when combined with less effective translations. The research also uncovers a phenomenon where PiM activates fewer neurons for more precise usage, akin to synaptic pruning in neuroscience.", "conclusion": "The findings suggest that multilingual inputs not only enhance comprehension in LLMs but also refine their neural activation patterns, indicating a more efficient processing mechanism.", "key_contributions": ["Introduces the concept of Parallel Input in Multiple Languages (PiM) for enhancing LLM comprehension.", "Demonstrates the counterintuitive finding that PiM can lead to more precise neuron activation rather than increased activation.", "Aligns findings with neuroscience principles, providing insight into neural efficiency in LLMs."], "limitations": "", "keywords": ["Multilingual LLMs", "In-context learning", "Neural activation", "Language processing", "Synaptic pruning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2403.19390", "pdf": "https://arxiv.org/pdf/2403.19390.pdf", "abs": "https://arxiv.org/abs/2403.19390", "title": "Checkpoint Merging via Bayesian Optimization in LLM Pretraining", "authors": ["Deyuan Liu", "Zecheng Wang", "Bingning Wang", "Weipeng Chen", "Chunshan Li", "Zhiying Tu", "Dianhui Chu", "Bo Li", "Dianbo Sui"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid proliferation of large language models (LLMs) such as GPT-4 and\nGemini underscores the intense demand for resources during their training\nprocesses, posing significant challenges due to substantial computational and\nenvironmental costs. To alleviate this issue, we propose checkpoint merging in\npretraining LLM. This method utilizes LLM checkpoints with shared training\ntrajectories, and is rooted in an extensive search space exploration for the\nbest merging weight via Bayesian optimization. Through various experiments, we\ndemonstrate that: (1) Our proposed methodology exhibits the capacity to augment\npretraining, presenting an opportunity akin to obtaining substantial benefits\nat minimal cost; (2) Our proposed methodology, despite requiring a given\nheld-out dataset, still demonstrates robust generalization capabilities across\ndiverse domains, a pivotal aspect in pretraining.", "AI": {"tldr": "This paper proposes a method for checkpoint merging in pretraining large language models to reduce computational costs while improving performance.", "motivation": "The increasing demand for resources in training large language models like GPT-4 and Gemini presents challenges related to computational and environmental costs.", "method": "The proposed method leverages LLM checkpoints with shared training trajectories and utilizes Bayesian optimization to explore merging weights effectively.", "result": "Experiments show that the methodology not only enhances pretraining efficiency but also maintains strong generalization across different domains.", "conclusion": "Checkpoint merging presents an opportunity to gain substantial benefits in pretraining without significant computational overhead.", "key_contributions": ["Proposed checkpoint merging method for pretraining LLMs", "Effective exploration of merging weights using Bayesian optimization", "Demonstrated improved generalization capabilities across domains"], "limitations": "", "keywords": ["large language models", "checkpoint merging", "Bayesian optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2405.06705", "pdf": "https://arxiv.org/pdf/2405.06705.pdf", "abs": "https://arxiv.org/abs/2405.06705", "title": "LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought", "authors": ["Zhuoxuan Jiang", "Haoyuan Peng", "Shanshan Feng", "Fan Li", "Dongsheng Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCAI 2024", "summary": "Self-correction is emerging as a promising approach to mitigate the issue of\nhallucination in Large Language Models (LLMs). To facilitate effective\nself-correction, recent research has proposed mistake detection as its initial\nstep. However, current literature suggests that LLMs often struggle with\nreliably identifying reasoning mistakes when using simplistic prompting\nstrategies. To address this challenge, we introduce a unique prompting\nstrategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is\nspecifically designed to guide the identification of reasoning mistakes,\nparticularly mathematical reasoning mistakes. PedCoT consists of pedagogical\nprinciples for prompts (PPP) design, two-stage interaction process (TIP) and\ngrounded PedCoT prompts, all inspired by the educational theory of the Bloom\nCognitive Model (BCM). We evaluate our approach on two public datasets\nfeaturing math problems of varying difficulty levels. The experiments\ndemonstrate that our zero-shot prompting strategy significantly outperforms\nstrong baselines. The proposed method can achieve the goal of reliable\nmathematical mistake identification and provide a foundation for automatic math\nanswer grading. The results underscore the significance of educational theory,\nserving as domain knowledge, in guiding prompting strategy design for\naddressing challenging tasks with LLMs effectively.", "AI": {"tldr": "This paper introduces a novel prompting strategy called Pedagogical Chain-of-Thought (PedCoT) to enhance self-correction in LLMs by improving the identification of reasoning mistakes, especially in mathematics.", "motivation": "Current LLMs struggle to identify reasoning mistakes reliably using simplistic prompting strategies, leading to hallucinations in responses.", "method": "The PedCoT approach incorporates pedagogical principles for prompt design, a two-stage interaction process, and ground prompts based on educational theories to guide LLMs in identifying mistakes.", "result": "The experiments conducted on two public datasets show that PedCoT significantly outperforms existing baselines in identifying mathematical reasoning mistakes.", "conclusion": "The study highlights the critical role of educational theory in enhancing prompting strategies for LLMs to tackle complex tasks effectively.", "key_contributions": ["Introduction of the Pedagogical Chain-of-Thought (PedCoT) prompting strategy.", "Demonstration of improved mistake detection performance on math problems through a novel prompting approach.", "Theoretical foundation based on educational principles to inform prompting strategies."], "limitations": "", "keywords": ["self-correction", "large language models", "prompting strategies", "mathematical reasoning", "educational theory"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2406.06907", "pdf": "https://arxiv.org/pdf/2406.06907.pdf", "abs": "https://arxiv.org/abs/2406.06907", "title": "SignMusketeers: An Efficient Multi-Stream Approach for Sign Language Translation at Scale", "authors": ["Shester Gueuwou", "Xiaodan Du", "Greg Shakhnarovich", "Karen Livescu"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to ACL (Findings) 2025", "summary": "A persistent challenge in sign language video processing, including the task\nof sign to written language translation, is how we learn representations of\nsign language in an effective and efficient way that preserves the important\nattributes of these languages, while remaining invariant to irrelevant visual\ndifferences. Informed by the nature and linguistics of signed languages, our\nproposed method focuses on just the most relevant parts in a signing video: the\nface, hands and body pose of the signer. However, instead of fully relying on\npose estimation from off-the-shelf pose tracking models, which have\ninconsistent performance for hands and faces, we propose to learn a\nrepresentation of the complex handshapes and facial expressions of sign\nlanguages in a self-supervised fashion. Our approach is based on learning from\nindividual frames (rather than video sequences) and is therefore much more\nefficient than prior work on sign language pre-training. Compared to a recent\nmodel that established a new state of the art in sign language translation on\nthe How2Sign dataset, our approach yields similar translation performance,\nusing less than 3\\% of the compute.", "AI": {"tldr": "This paper proposes a self-supervised approach to learning representations of sign language that focuses on relevant visual attributes like the face, hands, and body pose, achieving competitive performance with significantly lower compute resources.", "motivation": "To improve representation learning in sign language video processing while focusing on the most relevant visual features, avoiding reliance on inconsistent pose estimation models.", "method": "The proposed method processes individual frames from signing videos to learn handshapes and facial expressions in a self-supervised manner, improving efficiency over prior techniques.", "result": "The approach reaches translation performance comparable to state-of-the-art methods on the How2Sign dataset while using less than 3% of the compute resources.", "conclusion": "This self-supervised method provides an efficient alternative for sign language to written language translation by focusing on critical aspects of the signer's video.", "key_contributions": ["Self-supervised learning approach for handshapes and expressions", "Focus on relevant visual components (face, hands, body pose)", "High efficiency with competitive translation performance"], "limitations": "", "keywords": ["Sign Language", "Self-supervised Learning", "Video Processing", "Translation", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2406.14545", "pdf": "https://arxiv.org/pdf/2406.14545.pdf", "abs": "https://arxiv.org/abs/2406.14545", "title": "Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems", "authors": ["Đorđe Klisura", "Anthony Rios"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Findings", "summary": "Text-to-SQL systems empower users to interact with databases using natural\nlanguage, automatically translating queries into executable SQL code. However,\ntheir reliance on database schema information for SQL generation exposes them\nto significant security vulnerabilities, particularly schema inference attacks\nthat can lead to unauthorized data access or manipulation. In this paper, we\nintroduce a novel zero-knowledge framework for reconstructing the underlying\ndatabase schema of text-to-SQL models without any prior knowledge of the\ndatabase. Our approach systematically probes text-to-SQL models with specially\ncrafted questions and leverages a surrogate GPT-4 model to interpret the\noutputs, effectively uncovering hidden schema elements -- including tables,\ncolumns, and data types. We demonstrate that our method achieves high accuracy\nin reconstructing table names, with F1 scores of up to .99 for generative\nmodels and .78 for fine-tuned models, underscoring the severity of schema\nleakage risks. We also show that our attack can steal prompt information in\nnon-text-to-SQL models. Furthermore, we propose a simple protection mechanism\nfor generative models and empirically show its limitations in mitigating these\nattacks.", "AI": {"tldr": "This paper presents a zero-knowledge framework to uncover database schema from text-to-SQL models, demonstrating significant vulnerabilities in such systems.", "motivation": "Text-to-SQL systems face vulnerabilities due to their dependence on database schema information, leading to schema inference attacks.", "method": "We systematically probe text-to-SQL models with specially crafted questions and utilize a surrogate GPT-4 model to interpret outputs, uncovering hidden schema elements.", "result": "Our method achieves high accuracy in reconstructing table names with F1 scores of up to .99 for generative models and .78 for fine-tuned models, revealing serious risks of schema leakage.", "conclusion": "While we propose a protection mechanism for generative models, our analysis shows its limitations in effectively mitigating these attacks.", "key_contributions": ["Introduced a novel zero-knowledge framework for schema reconstruction", "Achieved high accuracy in identifying hidden schema elements", "Outlined a protection mechanism with noted limitations"], "limitations": "The proposed protection mechanism has limitations in effectively mitigating schema inference attacks.", "keywords": ["text-to-SQL", "schema inference attacks", "GPT-4", "database security", "natural language processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2407.01384", "pdf": "https://arxiv.org/pdf/2407.01384.pdf", "abs": "https://arxiv.org/abs/2407.01384", "title": "Free-text Rationale Generation under Readability Level Control", "authors": ["Yi-Sheng Hsu", "Nils Feldhus", "Sherzod Hakimov"], "categories": ["cs.CL"], "comment": "ACL 2025 Workshop on Generation, Evaluation, and Metrics (GEM^2)", "summary": "Free-text rationales justify model decisions in natural language and thus\nbecome likable and accessible among approaches to explanation across many\ntasks. However, their effectiveness can be hindered by misinterpretation and\nhallucination. As a perturbation test, we investigate how large language models\n(LLMs) perform rationale generation under the effects of readability level\ncontrol, i.e., being prompted for an explanation targeting a specific expertise\nlevel, such as sixth grade or college. We find that explanations are adaptable\nto such instruction, though the observed distinction between readability levels\ndoes not fully match the defined complexity scores according to traditional\nreadability metrics. Furthermore, the generated rationales tend to feature\nmedium level complexity, which correlates with the measured quality using\nautomatic metrics. Finally, our human annotators confirm a generally\nsatisfactory impression on rationales at all readability levels, with\nhigh-school-level readability being most commonly perceived and favored.", "AI": {"tldr": "This study examines how large language models generate explanations at different readability levels and their effectiveness.", "motivation": "To understand how the readability level of generated rationales affects their effectiveness and user perception.", "method": "A perturbation test was conducted where large language models were instructed to generate rationales aimed at specific readability levels, with a focus on their adaptability and quality correlation.", "result": "The study found that explanations are adaptable to readability prompts, but the differentiation in complexity levels does not entirely align with traditional readability metrics. Generated rationales exhibited medium complexity, correlating with quality metrics, and received favorable feedback from human annotators, particularly at high-school readability levels.", "conclusion": "Rationales generated by LLMs are versatile across different readability levels, yet the expected complexity does not fully comply with traditional metrics; high-school readability is often favored.", "key_contributions": ["Investigates LLM rationale generation across varying readability levels.", "Identifies misalignment between generated rationales' complexity and traditional readability scores.", "Highlights human perceptions of the effectiveness of different readability levels in rationales."], "limitations": "Results may be limited by the subjective nature of human annotation and the specific contexts of readability assessment.", "keywords": ["large language models", "rationale generation", "readability levels", "explanation", "human-annotated quality"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2407.03525", "pdf": "https://arxiv.org/pdf/2407.03525.pdf", "abs": "https://arxiv.org/abs/2407.03525", "title": "UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization", "authors": ["Md Nayem Uddin", "Amir Saeidi", "Divij Handa", "Agastya Seth", "Tran Cao Son", "Eduardo Blanco", "Steven R. Corman", "Chitta Baral"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Main)", "summary": "This paper introduces UnSeenTimeQA, a novel data contamination-free\ntime-sensitive question-answering (TSQA) benchmark. It differs from existing\nTSQA benchmarks by avoiding web-searchable queries grounded in the real world.\nWe present a series of time-sensitive event scenarios based on synthetically\ngenerated facts. It requires large language models (LLMs) to engage in genuine\ntemporal reasoning without depending on the factual knowledge acquired during\nthe pre-training phase. Our data generation framework enables on-demand\ngeneration of new samples, mitigating the risk of data leakage. We designed\nthree types of time-sensitive questions to test LLMs' temporal reasoning\nabilities over sequential and parallel event occurrences. Our evaluation of\nfive LLMs on synthetic fact-based TSQA reveals mixed results: while they\nperform well on simpler subsets, their overall performance remains inferior as\ncompared to real world fact-based TSQA. Error analysis indicates that LLMs face\ndifficulties in reasoning over long-range event dependencies and parallel\nevents.", "AI": {"tldr": "UnSeenTimeQA is a new benchmark for time-sensitive question-answering designed to enhance LLMs' temporal reasoning without relying on real-world factual knowledge.", "motivation": "The paper seeks to address the limitations of existing time-sensitive question-answering benchmarks by introducing a contamination-free approach that avoids the use of real-world, web-searchable queries.", "method": "The authors created a data generation framework that synthetically generates event scenarios and designs three types of time-sensitive questions for evaluation.", "result": "Evaluation of five large language models shows mixed performance, excelling on simpler tasks but struggling with complex temporal reasoning over long-range dependencies and parallel events.", "conclusion": "The findings indicate that while there are capabilities in handling simpler temporal reasoning tasks, LLMs still struggle with complex scenarios in the TSQA domain.", "key_contributions": ["Introduction of UnSeenTimeQA benchmark for time-sensitive question-answering", "Data generation framework for creating synthetic event scenarios", "Evaluation highlighting the limitations of LLMs in temporal reasoning tasks"], "limitations": "LLMs perform inadequately on complex temporal reasoning tasks compared to real-world benchmarks.", "keywords": ["time-sensitive question-answering", "temporal reasoning", "large language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2407.11930", "pdf": "https://arxiv.org/pdf/2407.11930.pdf", "abs": "https://arxiv.org/abs/2407.11930", "title": "Localizing and Mitigating Errors in Long-form Question Answering", "authors": ["Rachneet Sachdeva", "Yixiao Song", "Mohit Iyyer", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings; Code and data are available:\n  https://github.com/UKPLab/acl2025-lfqa-hallucination", "summary": "Long-form question answering (LFQA) aims to provide thorough and in-depth\nanswers to complex questions, enhancing comprehension. However, such detailed\nresponses are prone to hallucinations and factual inconsistencies, challenging\ntheir faithful evaluation. This work introduces HaluQuestQA, the first\nhallucination dataset with localized error annotations for human-written and\nmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k\nspan-level error annotations for five different error types by expert\nannotators, along with preference judgments. Using our collected data, we\nthoroughly analyze the shortcomings of long-form answers and find that they\nlack comprehensiveness and provide unhelpful references. We train an automatic\nfeedback model on this dataset that predicts error spans with incomplete\ninformation and provides associated explanations. Finally, we propose a\nprompt-based approach, Error-informed refinement, that uses signals from the\nlearned feedback model to refine generated answers, which we show reduces\nerrors and improves answer quality across multiple models. Furthermore, humans\nfind answers generated by our approach comprehensive and highly prefer them\n(84%) over the baseline answers.", "AI": {"tldr": "HaluQuestQA introduces a dataset for analyzing hallucinations in long-form question answering and proposes a method to improve answer quality by modeling error feedback.", "motivation": "Long-form question answering can lead to hallucinations and factual inaccuracies, making evaluation difficult and impacting user comprehension.", "method": "The authors developed HaluQuestQA, a dataset with annotated error spans, and trained a model to predict these error spans. They also proposed a prompt-based refinement approach to enhance answer quality.", "result": "The analysis revealed that long-form answers often lack comprehensiveness and relevancy. The refined answers showed a preference of 84% among humans over baseline answers, indicating significant quality improvement.", "conclusion": "The proposed Error-informed refinement approach successfully reduces errors in LFQA responses, leading to better quality answers preferred by human evaluators.", "key_contributions": ["Introduction of HaluQuestQA, the first hallucination dataset for LFQA with localized error annotations.", "Development of an automatic feedback model for predicting error spans.", "Proposal of a prompt-based refinement method that improves answer quality in LFQA."], "limitations": "The dataset is limited to specific error types and may not cover all hallucination forms encountered in LFQA.", "keywords": ["long-form question answering", "hallucinations", "feedback model", "dataset", "error refinement"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.15186", "pdf": "https://arxiv.org/pdf/2407.15186.pdf", "abs": "https://arxiv.org/abs/2407.15186", "title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks", "authors": ["Liang Shi", "Zhengju Tang", "Nan Zhang", "Xiaotong Zhang", "Zhi Yang"], "categories": ["cs.CL"], "comment": "Accepted by ACM Computing Surveys (CSUR)", "summary": "With the development of the Large Language Models (LLMs), a large range of\nLLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a\ncomprehensive review of LLM-based Text2SQL studies. We first enumerate classic\nbenchmarks and evaluation metrics. For the two mainstream methods, prompt\nengineering and finetuning, we introduce a comprehensive taxonomy and offer\npractical insights into each subcategory. We present an overall analysis of the\nabove methods and various models evaluated on well-known datasets and extract\nsome characteristics. Finally, we discuss the challenges and future directions\nin this field.", "AI": {"tldr": "This paper reviews the development of LLM-based Text-to-SQL methods, outlining benchmarks, methodologies, and future challenges.", "motivation": "To provide a comprehensive overview of the advancements and methods in LLM-based Text-to-SQL techniques, highlighting their applications and assessment metrics.", "method": "The paper utilizes a comparative analysis of the two main approaches in Text-to-SQL: prompt engineering and finetuning, supplemented by a taxonomy of methods and evaluation on various datasets.", "result": "An analysis of classic benchmarks, evaluation metrics, and characteristics of different models in the Text-to-SQL space, informed by practical insights.", "conclusion": "The paper concludes by outlining existing challenges in LLM-based Text-to-SQL and proposing future research directions in the area.", "key_contributions": ["Comprehensive taxonomy of prompt engineering and finetuning methods in Text-to-SQL", "Analysis of benchmark datasets and evaluation metrics", "Discussion of challenges and future research directions in LLM-based Text-to-SQL"], "limitations": "", "keywords": ["Large Language Models", "Text-to-SQL", "prompt engineering", "finetuning", "benchmark analysis"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2407.21050", "pdf": "https://arxiv.org/pdf/2407.21050.pdf", "abs": "https://arxiv.org/abs/2407.21050", "title": "Cross-Institutional Dental EHR Entity Extraction via Generative AI and Synthetic Notes", "authors": ["Yao-Shun Chuang", "Chun-Teh Lee", "Oluwabunmi Tokede", "Guo-Hao Lin", "Ryan Brandon", "Trung Duong Tran", "Xiaoqian Jiang", "Muhammad F. Walji"], "categories": ["cs.CL"], "comment": "11 pages, 2 tables, 3 figures, under review", "summary": "This research addresses the issue of missing structured data in dental\nrecords by extracting diagnostic information from unstructured text. The\nupdated periodontology classification system's complexity has increased\nincomplete or missing structured diagnoses. To tackle this, we use advanced AI\nand NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a\nRoBERTa model. This significantly enhances the model's ability to understand\nmedical and dental language. We evaluated the model using 120 randomly selected\nclinical notes from two datasets, demonstrating its improved diagnostic\nextraction accuracy. The results showed high accuracy in diagnosing periodontal\nstatus, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In\nthe subtype category, Site 2 achieved perfect scores, outperforming Site 1.\nThis method enhances extraction accuracy and broadens its use across dental\ncontexts. The study underscores AI and NLP's transformative impact on\nhealthcare delivery and management. Integrating AI and NLP technologies\nenhances documentation and simplifies administrative tasks by precisely\nextracting complex clinical information. This approach effectively addresses\nchallenges in dental diagnostics. Using synthetic training data from LLMs\noptimizes the training process, improving accuracy and efficiency in\nidentifying periodontal diagnoses from clinical notes. This innovative method\nholds promise for broader healthcare applications, potentially improving\npatient care quality.", "AI": {"tldr": "This study explores using AI and NLP to improve the extraction of diagnostic information from unstructured dental texts, demonstrating high accuracy with a fine-tuned RoBERTa model.", "motivation": "The research aims to address the problem of incomplete or missing structured data in dental records due to the complexity of modern diagnostic classifications.", "method": "Advanced AI and NLP techniques were employed, utilizing GPT-4 to generate synthetic notes for fine-tuning a RoBERTa model, which was evaluated on clinical notes for extracting diagnostic accuracy.", "result": "The model achieved diagnostic extraction accuracy of 0.99 for Site 1 and 0.98 for Site 2, with perfect scores in subtype categories in Site 2, thus confirming the method's effectiveness.", "conclusion": "The innovative AI and NLP methods improved the extraction of complex dental diagnostics and have the potential for wider applications across healthcare, enhancing patient care quality.", "key_contributions": ["Utilization of GPT-4 for generating synthetic training data", "Improved diagnostic extraction accuracy in dentistry", "Potential broader healthcare applications for AI and NLP methods"], "limitations": "", "keywords": ["AI", "NLP", "Dental Diagnostics", "RoBERTa", "Healthcare"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2408.08769", "pdf": "https://arxiv.org/pdf/2408.08769.pdf", "abs": "https://arxiv.org/abs/2408.08769", "title": "Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused", "authors": ["Dingwei Chen", "Feiteng Fang", "Shiwen Ni", "Feng Liang", "Xiping Hu", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang", "Chengming Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, they occasionally generate\ninaccurate and counterfactual outputs, a phenomenon commonly referred to as\n\"hallucinations''. To tackle this issue, recent studies have explored\ncontrastive decoding between the original model and an amateur model with\ninduced hallucination, showing promising results. Nevertheless, this approach\ncan disrupt the original LLM's output distribution due to coarse contrast and\nsimple subtraction operations, potentially leading to errors. In this paper, we\nintroduce a novel contrastive decoding framework, termed LOL (LOwer Layer\nMatters). Unlike prior methods that focus solely on the final layer, our\napproach integrates contrastive information from lower layers to enable\nmulti-layer fusion during contrastive decoding. Additionally, we incorporate a\ntruthfulness refocused module that leverages instruction guidance to further\nimprove truthfulness in contrastive decoding. Extensive experiments on four\npublicly available datasets demonstrate that the LOL framework significantly\nmitigates hallucination while outperforming existing baselines in most cases.\nFor reproducibility, we will release our code and data upon acceptance.", "AI": {"tldr": "The paper introduces LOL, a novel contrastive decoding framework that reduces hallucinations in Large Language Models by integrating information from lower layers and adding a truthfulness module.", "motivation": "To address the issue of hallucinations in LLMs, which lead to inaccurate outputs and disrupt the original model's performance.", "method": "The LOL framework employs multi-layer fusion during contrastive decoding by incorporating information from lower layers along with a truthfulness refocused module.", "result": "Extensive experiments show that LOL significantly reduces hallucinations and outperforms traditional baselines across various datasets.", "conclusion": "The LOL framework presents a more effective approach to contrastive decoding that enhances the reliability of LLM outputs.", "key_contributions": ["Introduction of the LOL framework for multi-layer fusion in contrastive decoding.", "Integration of a truthfulness refocused module to enhance output validity.", "Demonstration of superior performance on four publicly available datasets."], "limitations": "", "keywords": ["Large Language Models", "contrastive decoding", "hallucinations", "truthfulness", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.15762", "pdf": "https://arxiv.org/pdf/2409.15762.pdf", "abs": "https://arxiv.org/abs/2409.15762", "title": "XTRUST: On the Multilingual Trustworthiness of Large Language Models", "authors": ["Yahan Li", "Yi Wang", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL"], "comment": "21 pages", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing (NLP) tasks, capturing the attention of\nboth practitioners and the broader public. A key question that now preoccupies\nthe AI community concerns the capabilities and limitations of these models,\nwith trustworthiness emerging as a central issue, particularly as LLMs are\nincreasingly applied in sensitive fields like healthcare and finance, where\nerrors can have serious consequences. However, most previous studies on the\ntrustworthiness of LLMs have been limited to a single language, typically the\npredominant one in the dataset, such as English. In response to the growing\nglobal deployment of LLMs, we introduce XTRUST, the first comprehensive\nmultilingual trustworthiness benchmark. XTRUST encompasses a diverse range of\ntopics, including illegal activities, hallucination, out-of-distribution (OOD)\nrobustness, physical and mental health, toxicity, fairness, misinformation,\nprivacy, and machine ethics, across 10 different languages. Using XTRUST, we\nconduct an empirical evaluation of the multilingual trustworthiness of five\nwidely used LLMs, offering an in-depth analysis of their performance across\nlanguages and tasks. Our results indicate that many LLMs struggle with certain\nlow-resource languages, such as Arabic and Russian, highlighting the\nconsiderable room for improvement in the multilingual trustworthiness of\ncurrent language models. The code is available at\nhttps://github.com/LluckyYH/XTRUST.", "AI": {"tldr": "This paper introduces XTRUST, a multilingual benchmark for evaluating the trustworthiness of large language models (LLMs) in various sensitive domains.", "motivation": "The paper addresses the growing concern over the trustworthiness of LLMs, especially when applied in critical sectors such as healthcare and finance, where errors can have significant repercussions.", "method": "The authors developed XTRUST, a comprehensive multilingual benchmark that assesses the trustworthiness of LLMs across diverse topics and languages. The empirical evaluation included five popular LLMs and covered their performance on various tasks.", "result": "The evaluation revealed that many LLMs exhibit poor performance in low-resource languages like Arabic and Russian, indicating a significant need for improvement in their multilingual capabilities.", "conclusion": "The study emphasizes the necessity for enhanced trustworthiness in multilingual LLMs and suggests improvements to address the deficiencies identified in the evaluation.", "key_contributions": ["Introduction of the first comprehensive multilingual trustworthiness benchmark, XTRUST.", "Empirical evaluation of five widely used LLMs across multiple languages and topics.", "Identification of performance gaps in LLMs for low-resource languages."], "limitations": "The study primarily focuses on five LLMs and may not represent the entire spectrum of available models; additionally, the benchmark may need to be expanded for broader evaluation.", "keywords": ["trustworthiness", "large language models", "multilingual", "healthcare", "benchmark"], "importance_score": 9, "read_time_minutes": 21}}
{"id": "2409.17044", "pdf": "https://arxiv.org/pdf/2409.17044.pdf", "abs": "https://arxiv.org/abs/2409.17044", "title": "How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not", "authors": ["Francesco Verdini", "Pierfrancesco Melucci", "Stefano Perna", "Francesco Cariaggi", "Marco Gaido", "Sara Papi", "Szymon Mazurek", "Marek Kasztelnik", "Luisa Bentivogli", "Sébastien Bratières", "Paolo Merialdo", "Simone Scardapane"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to Interspeech 2025", "summary": "The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM.", "AI": {"tldr": "This paper investigates the impact of various components (Speech Foundational Models, adapters, and Large Language Models) on the performance of speech-to-text tasks.", "motivation": "To address the lack of understanding regarding the influence of individual components on speech-to-text downstream task performance.", "method": "Evaluate combinations of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on Automatic Speech Recognition and Speech Translation tasks.", "result": "The evaluation shows that the Speech Foundational Model significantly affects downstream performance, while the adapter choice has a moderate impact that varies based on the SFM and LLM used.", "conclusion": "The findings highlight the importance of the SFM in achieving high performance in speech-to-text tasks, suggesting that careful selection of components is crucial for optimizing results.", "key_contributions": ["Evaluates the performance impact of different components in speech-to-text tasks", "Demonstrates the pivotal role of Speech Foundational Models", "Analyzes the dependency of adapter design on selected SFM and LLM"], "limitations": "", "keywords": ["Large Language Models", "Speech-to-Text", "Speech Foundational Models"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2410.00382", "pdf": "https://arxiv.org/pdf/2410.00382.pdf", "abs": "https://arxiv.org/abs/2410.00382", "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning", "authors": ["Shota Takashiro", "Takeshi Kojima", "Andrew Gambardella", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "As large language models (LLMs) are applied across diverse domains, the\nability to selectively unlearn specific information is becoming increasingly\nessential. For instance, LLMs are expected to selectively provide confidential\ninformation to authorized internal users, such as employees or trusted\npartners, while withholding it from external users, including the general\npublic and unauthorized entities. Therefore, we propose a novel method termed\n``in-context knowledge unlearning'', which enables the model to selectively\nforget information in test-time based on the query context. Our method\nfine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge\nwithin the context, while preserving unrelated information. Experiments on\nTOFU, AGE and RWKU datasets using Llama2-7B/13B and Mistral-7B models\ndemonstrate that our method achieves up to 95% forget accuracy while retaining\n80% of unrelated knowledge, significantly outperforming baselines in both\nin-domain and out-of-domain scenarios. Further investigation of the model's\ninternal behavior revealed that while fine-tuned LLMs generate correct\npredictions in the middle layers and preserve them up to the final layer.\nHowever, the decision to forget is made only at the last layer, i.e. ``LLMs\npretend to forget''. Our findings offer valuable insight into the improvement\nof the robustness of the unlearning mechanisms in LLMs, laying a foundation for\nfuture research in the field.", "AI": {"tldr": "Proposes in-context knowledge unlearning for LLMs, allowing selective forgetting of information during inference while maintaining unrelated data.", "motivation": "As LLMs are used in various fields, the need to unlearn specific information while preserving privacy is crucial.", "method": "The proposed method fine-tunes pre-trained LLMs to enable prompt unlearning of knowledge based on query context, focusing on test-time adjustments.", "result": "Achieves up to 95% forget accuracy and retains 80% of unrelated knowledge, outperforming existing methods in both in-domain and out-of-domain tests on common datasets.", "conclusion": "The study highlights insights into LLMs' internal functioning, indicating that forgetting decisions occur at the last layer, which suggests room for enhancing unlearning mechanisms.", "key_contributions": ["Introduction of in-context knowledge unlearning for LLMs", "Demonstrated significant improvements in forget accuracy", "Insights into LLM internal behavior regarding knowledge retention and forgetting."], "limitations": "", "keywords": ["Large Language Models", "Knowledge Unlearning", "In-context Learning", "AI Ethics", "Information Privacy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.02677", "pdf": "https://arxiv.org/pdf/2410.02677.pdf", "abs": "https://arxiv.org/abs/2410.02677", "title": "CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark by Human-AI CulturalTeaming", "authors": ["Yu Ying Chiu", "Liwei Jiang", "Bill Yuchen Lin", "Chan Young Park", "Shuyue Stella Li", "Sahithya Ravi", "Mehar Bhatia", "Maria Antoniak", "Yulia Tsvetkov", "Vered Shwartz", "Yejin Choi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Main, 39 pages, 16 figures. arXiv admin note: text overlap\n  with arXiv:2404.06664", "summary": "Robust, diverse, and challenging cultural knowledge benchmarks are essential\nfor measuring our progress towards making LMs that are helpful across diverse\ncultures. We introduce CulturalBench: a set of 1,696 human-written and\nhuman-verified questions to assess LMs' cultural knowledge, covering 45 global\nregions including underrepresented ones like Bangladesh, Zimbabwe, and Peru.\nQuestions are each verified by five independent annotators and span 17 diverse\ntopics ranging from food preferences to greeting etiquette. We construct\nCulturalBench using methods inspired by Human-AI Red-Teaming. Compared to human\nperformance (92.4% accuracy), the hard version of CulturalBench is challenging\neven for the best-performing frontier LMs, ranging from 28.7% to 61.5% in\naccuracy. We find that LMs often struggle with tricky questions that have\nmultiple correct answers (e.g., What utensils do the Chinese usually use?),\nrevealing a tendency to overfit to a single answer. Our results indicate that\nGPT-4o substantially outperform other models across cultures, besting local\nproviders (e.g., Mistral on European culture and DeepSeek on Chinese culture).\nAcross the board, models under-perform on questions related to North Africa,\nSouth America and Middle East.", "AI": {"tldr": "CulturalBench is a benchmark of 1,696 questions designed to assess cultural knowledge of language models (LMs), with findings that highlight the challenges faced by LMs in accurately answering culturally diverse questions.", "motivation": "To evaluate progress in developing language models that are culturally knowledgeable and useful across diverse global contexts, particularly for underrepresented cultures.", "method": "A set of 1,696 human-written and verified questions spanning 45 regions and 17 topics, constructed using Human-AI Red-Teaming methods, with multiple annotators assessing each question's validity.", "result": "Top-performing LMs struggle with cultural knowledge, achieving accuracy between 28.7% to 61.5% on the hard version of the benchmark, and showing significant variance in performance across different cultures.", "conclusion": "CulturalBench reveals substantial gaps in LMs' abilities to address culturally nuanced questions, suggesting that models like GPT-4o perform best but still struggle significantly with certain regions and topics.", "key_contributions": ["Introduction of a comprehensive cultural knowledge benchmark for LMs.", "Human verification of questions ensuring quality and reliability.", "Insights into LM performance across a diverse set of cultural contexts."], "limitations": "The performance of LMs varies significantly across different global regions, highlighting limitations in their cultural understanding and applicability.", "keywords": ["Cultural knowledge", "Language models", "Benchmarking", "Human-AI Red-Teaming", "Diversity"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2410.03869", "pdf": "https://arxiv.org/pdf/2410.03869.pdf", "abs": "https://arxiv.org/abs/2410.03869", "title": "Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step", "authors": ["Wenxuan Wang", "Kuiyi Gao", "Youliang Yuan", "Jen-tse Huang", "Qiuzhi Liu", "Shuai Wang", "Wenxiang Jiao", "Zhaopeng Tu"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV", "cs.MM"], "comment": "Accepted by ACL 2025 Findings", "summary": "Text-based image generation models, such as Stable Diffusion and DALL-E 3,\nhold significant potential in content creation and publishing workflows, making\nthem the focus in recent years. Despite their remarkable capability to generate\ndiverse and vivid images, considerable efforts are being made to prevent the\ngeneration of harmful content, such as abusive, violent, or pornographic\nmaterial. To assess the safety of existing models, we introduce a novel\njailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises\nimage generation models through a step-by-step editing process. Specifically,\nfor malicious queries that cannot bypass the safeguards with a single prompt,\nwe intentionally decompose the query into multiple sub-queries. The image\ngeneration models are then prompted to generate and iteratively edit images\nbased on these sub-queries. To evaluate the effectiveness of our CoJ attack\nmethod, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine\nsafety scenarios, three types of editing operations, and three editing\nelements. Experiments on four widely-used image generation services provided by\nGPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack\nmethod can successfully bypass the safeguards of models for over 60% cases,\nwhich significantly outperforms other jailbreaking methods (i.e., 14%).\nFurther, to enhance these models' safety against our CoJ attack method, we also\npropose an effective prompting-based method, Think Twice Prompting, that can\nsuccessfully defend over 95% of CoJ attack. We release our dataset and code to\nfacilitate the AI safety research.", "AI": {"tldr": "The paper presents a novel jailbreaking method, Chain-of-Jailbreak (CoJ) attack, which compromises image generation models through a step-by-step editing process; it also proposes a defense method to enhance model safety.", "motivation": "To address the significant potential for abuse in text-based image generation models by assessing and improving their safety mechanisms.", "method": "The CoJ attack decomposes harmful queries into sub-queries, enabling iterative image generation and editing, while a new dataset called CoJ-Bench is constructed for evaluation.", "result": "The CoJ attack successfully bypassed safeguards in over 60% of cases across four popular image generation models, significantly outperforming traditional jailbreaking methods.", "conclusion": "The research highlights vulnerabilities in image generation models and proposes a robust defense method, Think Twice Prompting, that can defend against over 95% of CoJ attacks.", "key_contributions": ["Introduction of the Chain-of-Jailbreak (CoJ) attack method", "Creation of the CoJ-Bench dataset for evaluating model safety", "Development of Think Twice Prompting as a defense strategy against CoJ attacks."], "limitations": "", "keywords": ["image generation", "safety assessment", "jailbreaking", "machine learning", "AI safety"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2410.10672", "pdf": "https://arxiv.org/pdf/2410.10672.pdf", "abs": "https://arxiv.org/abs/2410.10672", "title": "Large Language Model Evaluation via Matrix Nuclear-Norm", "authors": ["Yahan Li", "Tingyu Xia", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL"], "comment": "21 pages", "summary": "As large language models (LLMs) continue to evolve, efficient evaluation\nmetrics are vital for assessing their ability to compress information and\nreduce redundancy. While traditional metrics like Matrix Entropy offer valuable\ninsights, they are computationally intensive for large-scale models due to\ntheir \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To\nmitigate this issue, we introduce the Matrix Nuclear-Norm, which not only\nserves as a metric to quantify the data compression proficiency of LLM but also\nprovides a convex approximation of matrix rank to capture both predictive\ndiscriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to\nfurther approximate the nuclear norm, we can effectively assess the model's\ninformation compression capabilities. This approach reduces the time complexity\nto \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the\nMatrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy\nfor the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This\nperformance gap becomes more pronounced with larger models, as validated in\ntests with other models like Pythia. Additionally, evaluations on benchmarks\nand model responses confirm that our proposed Matrix Nuclear-Norm is a\nreliable, scalable, and efficient tool for assessing LLMs' performance,\nstriking a balance between accuracy and computational efficiency. The code is\navailable at https://github.com/MLGroupJLU/MatrixNuclearNorm.", "AI": {"tldr": "The paper introduces the Matrix Nuclear-Norm as an efficient evaluation metric for large language models, reducing the computational complexity of assessing data compression capabilities from O(n^3) to O(n^2).", "motivation": "To address the limitations of traditional metrics like Matrix Entropy, which are computationally intensive for large language models (LLMs).", "method": "The Matrix Nuclear-Norm is introduced as a metric to quantify data compression proficiency and provide a convex approximation of matrix rank, utilizing the L_{1,2}-norm to approximate the nuclear norm for efficient evaluation.", "result": "The Matrix Nuclear-Norm achieves evaluation speeds that are 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model, with performance improvements becoming more pronounced for larger models, as validated by tests with other models and benchmarks.", "conclusion": "The proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLM performance, balancing accuracy with computational efficiency.", "key_contributions": ["Introduction of the Matrix Nuclear-Norm as an efficient evaluation metric for LLMs.", "Reduction of time complexity from O(n^3) to O(n^2) for model evaluations.", "Demonstrated performance speed improvements for large models compared to traditional metrics."], "limitations": "", "keywords": ["Large Language Models", "Matrix Nuclear-Norm", "Data Compression", "Computational Efficiency", "Evaluation Metrics"], "importance_score": 8, "read_time_minutes": 21}}
{"id": "2410.10995", "pdf": "https://arxiv.org/pdf/2410.10995.pdf", "abs": "https://arxiv.org/abs/2410.10995", "title": "Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation", "authors": ["Emmanouil Zaranis", "Giuseppe Attanasio", "Sweta Agrawal", "André F. T. Martins"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Quality estimation (QE)-the automatic assessment of translation quality-has\nrecently become crucial across several stages of the translation pipeline, from\ndata curation to training and decoding. While QE metrics have been optimized to\nalign with human judgments, whether they encode social biases has been largely\noverlooked. Biased QE risks favoring certain demographic groups over others,\ne.g., by exacerbating gaps in visibility and usability. This paper defines and\ninvestigates gender bias of QE metrics and discusses its downstream\nimplications for machine translation (MT). Experiments with state-of-the-art QE\nmetrics across multiple domains, datasets, and languages reveal significant\nbias. When a human entity's gender in the source is undisclosed,\nmasculine-inflected translations score higher than feminine-inflected ones, and\ngender-neutral translations are penalized. Even when contextual cues\ndisambiguate gender, using context-aware QE metrics leads to more errors in\nselecting the correct translation inflection for feminine referents than for\nmasculine ones. Moreover, a biased QE metric affects data filtering and\nquality-aware decoding. Our findings underscore the need for a renewed focus on\ndeveloping and evaluating QE metrics centered on gender.", "AI": {"tldr": "This paper investigates the gender bias in Quality Estimation (QE) metrics for machine translation, revealing that these metrics favor masculine translations and penalize gender-neutral ones.", "motivation": "The study addresses the overlooked issue of social biases in QE metrics that can favor certain demographic groups and impact translation quality.", "method": "Experiments were conducted with state-of-the-art QE metrics across various domains, datasets, and languages to assess their bias regarding gender.", "result": "Significant gender bias was found in QE metrics, with masculine-inflected translations receiving higher scores and gender-neutral translations penalized in scenarios of undisclosed gender.", "conclusion": "The findings highlight the critical need for the development and evaluation of gender-neutral QE metrics to ensure fairer machine translation outcomes.", "key_contributions": ["Identification of gender bias in QE metrics", "Analysis of impact on machine translation quality", "Call for gender-focused evaluation of QE metrics"], "limitations": "The study primarily focuses on gender bias, leaving other possible biases unexamined.", "keywords": ["Quality Estimation", "gender bias", "machine translation", "translation quality", "bias in metrics"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2410.11020", "pdf": "https://arxiv.org/pdf/2410.11020.pdf", "abs": "https://arxiv.org/abs/2410.11020", "title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning", "authors": ["Bokai Hu", "Sai Ashish Somayajula", "Xin Pan", "Pengtao Xie"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction-fine-tuned large language models (LLMs) under 14B parameters\ncontinue to underperform on natural language understanding (NLU) tasks, often\ntrailing smaller models like BERT-base on benchmarks such as GLUE and\nSuperGLUE. Motivated by the success of reinforcement learning in reasoning\ntasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a\nframework to improve the NLU capabilities of LLMs. We frame NLU as a\nreinforcement learning environment, treating token generation as a sequence of\nactions and optimizing for reward signals based on alignment with ground-truth\nlabels. PPO consistently outperforms supervised fine-tuning, yielding an\naverage improvement of 6.3 points on GLUE, and surpasses zero-shot and few-shot\nprompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models\noutperform GPT-4o by over 4\\% on average across sentiment and natural language\ninference tasks, including gains of 7.3\\% on the Mental Health dataset and\n10.9\\% on SIGA-nli. This work highlights a promising direction for adapting\nLLMs to new tasks by reframing them as reinforcement learning problems,\nenabling learning through simple end-task rewards rather than extensive data\ncuration.", "AI": {"tldr": "Exploring Proximal Policy Optimization (PPO) to enhance NLU capabilities of instruction-fine-tuned large language models (LLMs), yielding significant performance improvements over traditional supervised fine-tuning.", "motivation": "LLMs under 14B parameters underperform on NLU tasks compared to smaller models like BERT-base; this research seeks to improve their performance using reinforcement learning techniques.", "method": "The study uses Proximal Policy Optimization (PPO) to frame NLU as a reinforcement learning environment, treating token generation as a series of actions optimized for reward signals based on ground-truth labels.", "result": "PPO outperforms supervised fine-tuning by an average of 6.3 points on GLUE and surpasses zero-shot and few-shot prompting by 38.7 and 26.1 points, respectively. PPO-tuned models also outperformed GPT-4o by over 4% on average across tasks.", "conclusion": "Reframing LLM training as reinforcement learning problems allows for effective adaptation to new tasks using simple rewards instead of extensive data curation, highlighting a new approach to improving NLU abilities in LLMs.", "key_contributions": ["Introduces PPO for enhancing NLU in LLMs", "Demonstrates significant performance gains on established benchmarks", "Highlights the benefits of using reinforcement learning for task adaptation"], "limitations": "", "keywords": ["Large Language Models", "Natural Language Understanding", "Proximal Policy Optimization", "Reinforcement Learning", "Machine Learning"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2410.12974", "pdf": "https://arxiv.org/pdf/2410.12974.pdf", "abs": "https://arxiv.org/abs/2410.12974", "title": "BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks", "authors": ["Anna Sokol", "Elizabeth Daly", "Michael Hind", "David Piorkowski", "Xiangliang Zhang", "Nuno Moniz", "Nitesh Chawla"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are powerful tools capable of handling diverse\ntasks. Comparing and selecting appropriate LLMs for specific tasks requires\nsystematic evaluation methods, as models exhibit varying capabilities across\ndifferent domains. However, finding suitable benchmarks is difficult given the\nmany available options. This complexity not only increases the risk of\nbenchmark misuse and misinterpretation but also demands substantial effort from\nLLM users, seeking the most suitable benchmarks for their specific needs. To\naddress these issues, we introduce \\texttt{BenchmarkCards}, an intuitive and\nvalidated documentation framework that standardizes critical benchmark\nattributes such as objectives, methodologies, data sources, and limitations.\nThrough user studies involving benchmark creators and users, we show that\n\\texttt{BenchmarkCards} can simplify benchmark selection and enhance\ntransparency, facilitating informed decision-making in evaluating LLMs. Data &\nCode: https://github.com/SokolAnn/BenchmarkCards", "AI": {"tldr": "This paper introduces BenchmarkCards, a framework to standardize and simplify the evaluation of benchmarks for large language models (LLMs).", "motivation": "The need for systematic evaluation methods for selecting appropriate LLMs for tasks due to varying model capabilities and the difficulty in finding suitable benchmarks.", "method": "The authors developed BenchmarkCards, which documents critical benchmark attributes and conducted user studies with benchmark creators and users.", "result": "User studies indicated that BenchmarkCards facilitate easier benchmark selection and enhance transparency in LLM evaluation.", "conclusion": "BenchmarkCards simplifies the process of benchmark selection, leading to more informed decisions when evaluating LLM performance.", "key_contributions": ["Introduction of BenchmarkCards for documenting benchmark attributes", "Evidence from user studies indicating improved selection transparency", "Facilitates informed decision-making for LLM evaluations"], "limitations": "No significant limitations mentioned; focuses on the utility of BenchmarkCards.", "keywords": ["large language models", "benchmarking", "HCI", "transparency", "evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.14817", "pdf": "https://arxiv.org/pdf/2410.14817.pdf", "abs": "https://arxiv.org/abs/2410.14817", "title": "A Complexity-Based Theory of Compositionality", "authors": ["Eric Elmoznino", "Thomas Jiralerspong", "Yoshua Bengio", "Guillaume Lajoie"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Compositionality is believed to be fundamental to intelligence. In humans, it\nunderlies the structure of thought, language, and higher-level reasoning. In\nAI, compositional representations can enable a powerful form of\nout-of-distribution generalization, in which a model systematically adapts to\nnovel combinations of known concepts. However, while we have strong intuitions\nabout what compositionality is, we lack satisfying formal definitions for it\nthat are measurable and mathematical. Here, we propose such a definition, which\nwe call representational compositionality, that accounts for and extends our\nintuitions about compositionality. The definition is conceptually simple,\nquantitative, grounded in algorithmic information theory, and applicable to any\nrepresentation. Intuitively, representational compositionality states that a\ncompositional representation satisfies three properties. First, it must be\nexpressive. Second, it must be possible to re-describe the representation as a\nfunction of discrete symbolic sequences with re-combinable parts, analogous to\nsentences in natural language. Third, the function that relates these symbolic\nsequences to the representation, analogous to semantics in natural language,\nmust be simple. Through experiments on both synthetic and real world data, we\nvalidate our definition of compositionality and show how it unifies disparate\nintuitions from across the literature in both AI and cognitive science. We also\nshow that representational compositionality, while theoretically intractable,\ncan be readily estimated using standard deep learning tools. We hope that our\ndefinition can inspire the design of novel, theoretically-driven models that\nbetter capture the mechanisms of compositional thought. We make our code\navailable at https://github.com/EricElmoznino/complexity_compositionality.", "AI": {"tldr": "This paper proposes a formal definition of representational compositionality, providing a quantifiable framework for understanding compositionality in AI and cognitive science.", "motivation": "The goal is to establish a measurable and mathematical definition of compositionality, which is fundamental to intelligence and crucial for effective AI generalization.", "method": "The authors introduce 'representational compositionality' defined by three properties: expressiveness, the ability to describe the representation using combinable symbolic sequences, and the simplicity of the function relating these sequences to the representation.", "result": "Experiments validate the proposed definition and show how it can unify various intuitions from AI and cognitive science, demonstrating that it can be estimated using existing deep learning tools.", "conclusion": "The definition could inspire the creation of new models that better reflect the mechanisms underlying compositional thought.", "key_contributions": ["Proposes a novel formal definition of representational compositionality.", "Validates the definition through both synthetic and real-world data experiments.", "Demonstrates the application of standard deep learning tools to estimate compositionality."], "limitations": "The theoretical aspects of representational compositionality are intractable, which may hinder practical applications.", "keywords": ["compositionality", "AI", "cognitive science", "deep learning", "algorithmic information theory"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2410.21271", "pdf": "https://arxiv.org/pdf/2410.21271.pdf", "abs": "https://arxiv.org/abs/2410.21271", "title": "EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation", "authors": ["Shih-Yang Liu", "Maksim Khadkevich", "Nai Chit Fung", "Charbel Sakr", "Chao-Han Huck Yang", "Chien-Yi Wang", "Saurav Muralidharan", "Hongxu Yin", "Kwang-Ting Cheng", "Jan Kautz", "Yu-Chiang Frank Wang", "Pavlo Molchanov", "Min-Hung Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While post-training compression techniques effectively reduce the memory\nfootprint, latency, and power consumption of Large Language Models (LLMs), they\noften result in noticeable accuracy degradation and remain limited by hardware\nand kernel constraints that restrict supported compression formats ultimately\nreducing flexibility across a wide range of deployment scenarios. In this work,\nwe propose EoRA, a novel fine-tuning-free method that augments compressed LLMs\nwith low-rank matrices, allowing users to rapidly enhance task-specific\nperformance and freely balance the trade-off between accuracy and computational\noverhead beyond the constraints of compression formats. EoRA consistently\noutperforms prior training-free low rank methods in recovering the accuracy of\ncompressed LLMs, achieving notable accuracy improvements (e.g.,\n$\\mathbf{10.84\\%}$ on ARC-Challenge, $\\mathbf{6.74\\%}$ on MathQA, and\n$\\mathbf{6.74\\%}$ on GSM8K) for LLaMA3-8B compressed to 3-bit. We also\nintroduce an optimized CUDA kernel, accelerating inference by up to 1.4x and\nreducing memory overhead through quantizing EoRA. Overall, EoRA offers a prompt\nsolution for improving the accuracy of compressed models under varying user\nrequirements, enabling more efficient and flexible deployment of LLMs. Code is\navailable at https://github.com/NVlabs/EoRA.", "AI": {"tldr": "EoRA is a fine-tuning-free method that enhances the performance of compressed Large Language Models (LLMs) using low-rank matrices, improving accuracy while maintaining computational efficiency.", "motivation": "To improve the performance of compressed LLMs without fine-tuning, addressing the issues of accuracy degradation and deployment constraints.", "method": "EoRA augments compressed LLMs with low-rank matrices, allowing for a flexible trade-off between accuracy and computational overhead without additional fine-tuning.", "result": "EoRA outperforms previous training-free low-rank methods, achieving significant accuracy improvements on benchmark tasks such as ARC-Challenge, MathQA, and GSM8K, alongside enhancements in inference speed and reduced memory usage.", "conclusion": "EoRA provides an efficient solution for optimizing compressed LLM performance, facilitating their deployment across diverse user requirements without sacrificing accuracy or computational efficiency.", "key_contributions": ["Development of the EoRA method for augmenting compressed LLMs", "Demonstration of significant accuracy recovery on benchmark datasets", "Introduction of an optimized CUDA kernel for faster inference"], "limitations": "", "keywords": ["Compression", "Large Language Models", "Low-rank Matrices", "Fine-tuning-free", "CUDA"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2411.00418", "pdf": "https://arxiv.org/pdf/2411.00418.pdf", "abs": "https://arxiv.org/abs/2411.00418", "title": "Self-Evolved Reward Learning for LLMs", "authors": ["Chenghua Huang", "Zhizhen Fan", "Lu Wang", "Fangkai Yang", "Pu Zhao", "Zeqi Lin", "Qingwei Lin", "Dongmei Zhang", "Saravan Rajmohan", "Qi Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages,6 figures,Accepted to ICLR 2025", "summary": "Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for\naligning language models with human preferences, playing a pivotal role in the\nsuccess of conversational models like GPT-4, ChatGPT, and Llama 2. A core\nchallenge in employing RLHF lies in training a reliable reward model (RM),\nwhich relies on high-quality labels typically provided by human experts or\nadvanced AI system. These methods can be costly and may introduce biases that\naffect the language model's responses. As language models improve, human input\nmay become less effective in further enhancing their performance. In this\npaper, we propose Self-Evolved Reward Learning (SER), a novel approach where\nthe RM generates additional training data to iteratively improve itself. We\nconducted extensive experiments on multiple datasets such as HH-RLHF and\nUltraFeedback, using models like Mistral and Llama 3, and compare SER against\nvarious baselines. Our results demonstrate that even with limited\nhuman-annotated data, learning from self-feedback can robustly enhance RM\nperformance, thereby boosting the capabilities of large language models (LLMs).\nResources of this paper can be found at https://aka.ms/ser", "AI": {"tldr": "This paper introduces Self-Evolved Reward Learning (SER), a method that enhances the training of reward models in reinforcement learning by generating additional data from self-feedback, improving LLM performance with less reliance on human annotations.", "motivation": "To address the limitations of traditional reward models that rely on human feedback, which can be costly and introduce biases, this work explores a self-improving approach to training reward models.", "method": "The authors propose Self-Evolved Reward Learning (SER), where the reward model autonomously generates training data to iteratively refine its performance, tested on datasets like HH-RLHF and UltraFeedback, with models including Mistral and Llama 3.", "result": "Experiments show that SER can significantly improve reward model performance even with limited human-annotated data, enhancing the efficacy of large language models like GPT-4 and ChatGPT.", "conclusion": "SER presents a promising alternative to traditional human-centric approaches for training reward models, demonstrating robustness in LLM performance improvement through self-feedback mechanisms.", "key_contributions": ["Introduction of Self-Evolved Reward Learning (SER) for reward model improvement.", "Demonstration of enhanced performance of LLMs with limited human feedback.", "Extensive experimental validation on multiple datasets and comparisons against existing baselines."], "limitations": "The effectiveness of SER may depend on the initial quality of the reward model and the datasets used for training.", "keywords": ["Reinforcement Learning", "Human Feedback", "Reward Model", "Self-Evolved Learning", "Large Language Models"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2411.02430", "pdf": "https://arxiv.org/pdf/2411.02430.pdf", "abs": "https://arxiv.org/abs/2411.02430", "title": "Generative Emotion Cause Explanation in Multimodal Conversations", "authors": ["Lin Wang", "Xiaocui Yang", "Shi Feng", "Daling Wang", "Yifei Zhang", "Zhitao Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal conversation, a crucial form of human communication, carries rich\nemotional content, making the exploration of the causes of emotions within it a\nresearch endeavor of significant importance. However, existing research on the\ncauses of emotions typically employs an utterance selection method within a\nsingle textual modality to locate causal utterances. This approach remains\nlimited to coarse-grained assessments, lacks nuanced explanations of emotional\ncausation, and demonstrates inadequate capability in identifying multimodal\nemotional triggers. Therefore, we introduce a task-\\textbf{Multimodal Emotion\nCause Explanation in Conversation (MECEC)}. This task aims to generate a\nsummary based on the multimodal context of conversations, clearly and\nintuitively describing the reasons that trigger a given emotion. To adapt to\nthis task, we develop a new dataset (ECEM) based on the MELD dataset. ECEM\ncombines video clips with detailed explanations of character emotions, helping\nto explore the causal factors behind emotional expression in multimodal\nconversations. A novel approach, FAME-Net, is further proposed, that harnesses\nthe power of Large Language Models (LLMs) to analyze visual data and accurately\ninterpret the emotions conveyed through facial expressions in videos. By\nexploiting the contagion effect of facial emotions, FAME-Net effectively\ncaptures the emotional causes of individuals engaged in conversations. Our\nexperimental results on the newly constructed dataset show that FAME-Net\noutperforms several excellent baselines. Code and dataset are available at\nhttps://github.com/3222345200/FAME-Net.", "AI": {"tldr": "This paper addresses the multimodal emotion cause explanation in conversations by introducing a novel task and dataset, ECEM, and proposing FAME-Net for analyzing emotional cues via Large Language Models.", "motivation": "To improve understanding of emotional causation in multimodal conversations, which is under-explored in existing research focused on single modalities.", "method": "Developed a new dataset (ECEM) leveraging video clips and emotional explanations, and proposed FAME-Net, a model utilizing LLMs to analyze visual data for interpreting emotions from facial expressions in conversations.", "result": "FAME-Net demonstrates superior performance in identifying emotional causes in conversations compared to existing methods on the ECEM dataset.", "conclusion": "The introduction of the MECEC task and the innovative approach of FAME-Net significantly advances the field of multimodal emotion recognition in conversations.", "key_contributions": ["Introduction of the MECEC task for multimodal emotion cause exploration.", "Development of the ECEM dataset that includes video clips and emotional responses.", "Proposal of FAME-Net, leveraging LLMs for facial emotion analysis."], "limitations": "", "keywords": ["Multimodal conversation", "Emotional causation", "Large Language Models", "Emotion recognition", "Dataset ECEM"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2411.02528", "pdf": "https://arxiv.org/pdf/2411.02528.pdf", "abs": "https://arxiv.org/abs/2411.02528", "title": "What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length", "authors": ["Lindia Tjuatja", "Graham Neubig", "Tal Linzen", "Sophie Hao"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 (Main Conference)", "summary": "When comparing the linguistic capabilities of language models (LMs) with\nhumans using LM probabilities, factors such as the length of the sequence and\nthe unigram frequency of lexical items have a significant effect on LM\nprobabilities in ways that humans are largely robust to. Prior works in\ncomparing LM and human acceptability judgments treat these effects uniformly\nacross models, making a strong assumption that models require the same degree\nof adjustment to control for length and unigram frequency effects. We propose\nMORCELA, a new linking theory between LM scores and acceptability judgments\nwhere the optimal level of adjustment for these effects is estimated from data\nvia learned parameters for length and unigram frequency. We first show that\nMORCELA outperforms a commonly used linking theory for acceptability - SLOR\n(Pauls and Klein, 2012; Lau et al. 2017) - across two families of transformer\nLMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of\nadjustment in SLOR for length and unigram frequency overcorrect for these\nconfounds, and that larger models require a lower relative degree of adjustment\nfor unigram frequency, though a significant amount of adjustment is still\nnecessary for all models. Finally, our subsequent analysis shows that larger\nLMs' lower susceptibility to frequency effects can be explained by an ability\nto better predict rarer words in context.", "AI": {"tldr": "This paper introduces MORCELA, a new linking theory for comparing language model probabilities to human acceptability judgments, proposing data-driven adjustments for factors like length and unigram frequency.", "motivation": "To improve the comparison between language models and human linguistic judgments by addressing the limitations of existing linking theories that treat adjustment factors uniformly across models.", "method": "MORCELA applies learned parameters to estimate optimal adjustments for length and unigram frequency based on data, and its performance is compared to the SLOR linking theory.", "result": "MORCELA outperforms SLOR across two families of transformer language models (Pythia and OPT), highlighting that larger models require less adjustment for unigram frequency.", "conclusion": "The study concludes that larger language models exhibit lower susceptibility to frequency effects due to their improved ability to predict rarer words in context, indicating the necessity of adjusting for these confounds.", "key_contributions": ["Introduction of MORCELA as a new linking theory for LM probabilities to human judgments.", "Demonstration of MORCELA's superiority over SLOR in performance across different models.", "Insights into the relationship between model size and adjustment needs for unigram frequency."], "limitations": "The paper primarily focuses on transformer models and may not generalize to other architectures.", "keywords": ["language models", "human acceptability", "MORCELA", "machine learning", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.04975", "pdf": "https://arxiv.org/pdf/2411.04975.pdf", "abs": "https://arxiv.org/abs/2411.04975", "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications", "authors": ["Gabriele Oliaro", "Zhihao Jia", "Daniel Campos", "Aurick Qiao"], "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference.", "AI": {"tldr": "SuffixDecoding is a new method using suffix trees to enhance the efficiency of speculative decoding in LLMs, achieving significant speedups for repetitive inference requests common in LLM-based agentic frameworks.", "motivation": "To improve latency in large language model inference by addressing workload characteristics of LLM-based agents that require repetitive inference requests.", "method": "Introduces SuffixDecoding, which leverages efficient suffix trees to cache token sequences and adaptively speculates based on acceptance likelihood.", "result": "SuffixDecoding achieves speedups of up to 5.3x on benchmarks like SWE-Bench and Text-to-SQL, outperforming state-of-the-art methods by a significant margin.", "conclusion": "SuffixDecoding effectively optimizes performance for agentic frameworks where predictability of tasks allows for more efficient speculative decoding.", "key_contributions": ["Introduction of SuffixDecoding method", "Utilization of suffix trees for caching sequences", "Demonstrated speedups over existing methods in agentic benchmarks"], "limitations": "", "keywords": ["Speculative Decoding", "Large Language Models", "Suffix Trees"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.11479", "pdf": "https://arxiv.org/pdf/2411.11479.pdf", "abs": "https://arxiv.org/abs/2411.11479", "title": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts", "authors": ["Jingxuan Li", "Yuning Yang", "Shengqi Yang", "Linfan Zhang", "Ying Nian Wu"], "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "The recent progress in Vision-Language Models (VLMs) has broadened the scope\nof multimodal applications. However, evaluations often remain limited to\nfunctional tasks, neglecting abstract dimensions such as personality traits and\nhuman values. To address this gap, we introduce Value-Spectrum, a novel Visual\nQuestion Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz's\nvalue dimensions that capture core human values guiding people's preferences\nand actions. We design a VLM agent pipeline to simulate video browsing and\nconstruct a vector database comprising over 50,000 short videos from TikTok,\nYouTube Shorts, and Instagram Reels. These videos span multiple months and\ncover diverse topics, including family, health, hobbies, society, technology,\netc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs\nhandle value-oriented content. Beyond identifying VLMs' intrinsic preferences,\nwe also explore the ability of VLM agents to adopt specific personas when\nexplicitly prompted, revealing insights into the adaptability of the model in\nrole-playing scenarios. These findings highlight the potential of\nValue-Spectrum as a comprehensive evaluation set for tracking VLM preferences\nin value-based tasks and abilities to simulate diverse personas. The complete\ncode and data are available at: https://github.com/Jeremyyny/Value-Spectrum.", "AI": {"tldr": "The paper introduces Value-Spectrum, a benchmark for evaluating Vision-Language Models (VLMs) based on personality traits and human values, using a dataset of over 50,000 videos for enhanced VQA assessments.", "motivation": "There is a need to evaluate VLMs not just on functional tasks but also on abstract dimensions like personality traits and human values, which are often overlooked.", "method": "The study develops a VLM agent pipeline for video browsing and creates a vector database with over 50,000 videos from platforms like TikTok and Instagram, aimed at assessing VLMs on value-oriented content.", "result": "Benchmarking reveals significant variations in how VLMs interpret value-oriented content and their adaptability in role-playing personas when prompted.", "conclusion": "Value-Spectrum provides a comprehensive evaluation framework for assessing VLM preferences in value-based tasks, offering insights into model adaptability and persona simulation.", "key_contributions": ["Introduction of the Value-Spectrum benchmark for VLM evaluation", "Creation of a large-scale video dataset for multimodal assessment", "Insights into VLM adaptability and persona simulation capabilities."], "limitations": "", "keywords": ["Vision-Language Models", "Value-Spectrum", "Visual Question Answering", "human values", "multimodal applications"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.16765", "pdf": "https://arxiv.org/pdf/2411.16765.pdf", "abs": "https://arxiv.org/abs/2411.16765", "title": "SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction", "authors": ["Shester Gueuwou", "Xiaodan Du", "Greg Shakhnarovich", "Karen Livescu", "Alexander H. Liu"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted to ACL 2025", "summary": "Sign language processing has traditionally relied on task-specific models,\nlimiting the potential for transfer learning across tasks. Pre-training methods\nfor sign language have typically focused on either supervised pre-training,\nwhich cannot take advantage of unlabeled data, or context-independent (frame or\nvideo segment) representations, which ignore the effects of relationships\nacross time in sign language. We introduce SHuBERT (Sign Hidden-Unit BERT), a\nself-supervised contextual representation model learned from approximately\n1,000 hours of American Sign Language video. SHuBERT adapts masked token\nprediction objectives to multi-stream visual sign language input, learning to\npredict multiple targets corresponding to clustered hand, face, and body pose\nstreams. SHuBERT achieves state-of-the-art performance across multiple tasks\nincluding sign language translation, isolated sign language recognition, and\nfingerspelling detection.", "AI": {"tldr": "SHuBERT is a self-supervised model for processing sign language, achieving state-of-the-art results across several tasks.", "motivation": "To overcome limitations of task-specific models in sign language processing and enhance the transfer learning potential across different tasks.", "method": "SHuBERT implements self-supervised masked token prediction applied to multi-stream visual inputs from American Sign Language videos.", "result": "The model has achieved state-of-the-art performance in sign language translation, isolated sign language recognition, and fingerspelling detection.", "conclusion": "SHuBERT demonstrates significant advancements in sign language processing through contextual representation and self-supervised learning techniques.", "key_contributions": ["Introduction of SHuBERT for sign language processing.", "Utilization of self-supervised learning with multiple visual streams.", "State-of-the-art performance across various sign language tasks."], "limitations": "", "keywords": ["sign language", "self-supervised learning", "contextual representation", "machine learning", "transfer learning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2412.12797", "pdf": "https://arxiv.org/pdf/2412.12797.pdf", "abs": "https://arxiv.org/abs/2412.12797", "title": "Is it the end of (generative) linguistics as we know it?", "authors": ["Cristiano Chesi"], "categories": ["cs.CL"], "comment": null, "summary": "A significant debate has emerged in response to a paper written by Steven\nPiantadosi (Piantadosi, 2023) and uploaded to the LingBuzz platform, the open\narchive for generative linguistics. Piantadosi's dismissal of Chomsky's\napproach is ruthless, but generative linguists deserve it. In this paper, I\nwill adopt three idealized perspectives -- computational, theoretical, and\nexperimental -- to focus on two fundamental issues that lend partial support to\nPiantadosi's critique: (a) the evidence challenging the Poverty of Stimulus\n(PoS) hypothesis and (b) the notion of simplicity as conceived within\nmainstream Minimalism. In conclusion, I argue that, to reclaim a central role\nin language studies, generative linguistics -- representing a prototypical\ntheoretical perspective on language -- needs a serious update leading to (i)\nmore precise, consistent, and complete formalizations of foundational\nintuitions and (ii) the establishment and utilization of a standardized dataset\nof crucial empirical evidence to evaluate the theory's adequacy. On the other\nhand, ignoring the formal perspective leads to major drawbacks in both\ncomputational and experimental approaches. Neither descriptive nor explanatory\nadequacy can be easily achieved without the precise formulation of general\nprinciples that can be challenged empirically.", "AI": {"tldr": "This paper critiques generative linguistics based on computational, theoretical, and experimental perspectives, arguing for a necessary update to the field.", "motivation": "To address criticisms of generative linguistics and its adherence to the Poverty of Stimulus hypothesis, and to advocate for improved empirical standards.", "method": "Adopts three idealized perspectives (computational, theoretical, and experimental) to analyze critiques of generative linguistics.", "result": "The paper finds that generative linguistics needs improvement in formalization and empirical evidence to maintain its relevance in language studies.", "conclusion": "An update of generative linguistics is necessary for improved precision and empirical adequacy to ensure its continued relevance in theoretical language studies.", "key_contributions": ["Critically analyzes Piantadosi's critique of generative linguistics.", "Suggests a need for precise formalizations in generative linguistics.", "Advocates for the creation of standardized datasets for empirical evaluation."], "limitations": "", "keywords": ["generative linguistics", "Poverty of Stimulus", "theoretical perspectives", "empirical adequacy", "formalization"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2412.15628", "pdf": "https://arxiv.org/pdf/2412.15628.pdf", "abs": "https://arxiv.org/abs/2412.15628", "title": "Can Input Attributions Explain Inductive Reasoning in In-Context Learning?", "authors": ["Mengyu Ye", "Tatsuki Kuribayashi", "Goro Kobayashi", "Jun Suzuki"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Interpreting the internal process of neural models has long been a challenge.\nThis challenge remains relevant in the era of large language models (LLMs) and\nin-context learning (ICL); for example, ICL poses a new issue of interpreting\nwhich example in the few-shot examples contributed to identifying/solving the\ntask. To this end, in this paper, we design synthetic diagnostic tasks of\ninductive reasoning, inspired by the generalization tests typically adopted in\npsycholinguistics. Here, most in-context examples are ambiguous w.r.t. their\nunderlying rule, and one critical example disambiguates it. The question is\nwhether conventional input attribution (IA) methods can track such a reasoning\nprocess, i.e., identify the influential example, in ICL. Our experiments\nprovide several practical findings; for example, a certain simple IA method\nworks the best, and the larger the model, the generally harder it is to\ninterpret the ICL with gradient-based IA methods.", "AI": {"tldr": "This paper investigates the interpretability of in-context learning in large language models through synthetic diagnostic tasks that test inductive reasoning, evaluating the effectiveness of input attribution methods in identifying influential examples.", "motivation": "To understand the internal processes of neural models, particularly in the context of in-context learning where determining the influence of few-shot examples is crucial.", "method": "The authors design synthetic diagnostic tasks that contain ambiguous examples, assessing various input attribution methods to see if they can accurately identify the critical example that disambiguates the task.", "result": "Experiments show that a particular simple input attribution method outperforms others, and larger models generally present greater challenges for interpretation using gradient-based methods.", "conclusion": "Interpretability remains a complex issue in in-context learning, with findings suggesting that simpler methods may be more effective than expected for understanding model reasoning.", "key_contributions": ["Creation of synthetic diagnostic tasks inspired by psycholinguistic generalization tests.", "Evaluation of input attribution methods in identifying critical examples for inductive reasoning.", "Insights into the challenges of interpreting larger language models."], "limitations": "Focuses on synthetic tasks which may not fully represent real-world scenarios; results may vary with different model architectures.", "keywords": ["neural models", "in-context learning", "input attribution", "interpretability", "inductive reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.17063", "pdf": "https://arxiv.org/pdf/2412.17063.pdf", "abs": "https://arxiv.org/abs/2412.17063", "title": "Computational Analysis of Character Development in Holocaust Testimonies", "authors": ["Esther Shizgal", "Eitan Wagner", "Renana Keydar", "Omri Abend"], "categories": ["cs.CL"], "comment": null, "summary": "This work presents a computational approach to analyze character development\nalong the narrative timeline. The analysis characterizes the inner and outer\nchanges the protagonist undergoes within a narrative, and the interplay between\nthem. We consider transcripts of Holocaust survivor testimonies as a test case,\neach telling the story of an individual in first-person terms. We focus on the\nsurvivor's religious trajectory, examining the evolution of their disposition\ntoward religious belief and practice along the testimony. Clustering the\nresulting trajectories in the dataset, we identify common sequences in the\ndata. Our findings highlight multiple common structures of religiosity across\nthe narratives: in terms of belief, most present a constant disposition, while\nfor practice, most present an oscillating structure, serving as valuable\nmaterial for historical and sociological research. This work demonstrates the\npotential of natural language processing techniques for analyzing character\nevolution through thematic trajectories in narratives.", "AI": {"tldr": "This paper presents a computational method for analyzing character development, focusing on Holocaust survivor testimonies to explore changes in religious belief and practice.", "motivation": "The research aims to understand character evolution within narratives, particularly in the context of Holocaust survivor testimonies, providing insights into their religious trajectories.", "method": "The study employs natural language processing techniques to analyze transcripts of survivor testimonies, focusing on the evolution of their religious beliefs and practices.", "result": "Clustering the trajectories reveals common narratives in belief, which tend to remain constant, while practices often oscillate, indicating different structures of religiosity among survivors.", "conclusion": "The findings show the effectiveness of NLP methods in analyzing character development, contributing to both historical and sociological understanding of individual narratives.", "key_contributions": ["Introduction of a computational approach to character analysis in narratives.", "Identification of common religiosity structures in Holocaust survivor testimonies.", "Demonstration of NLP techniques' applicability in historical narrative analysis."], "limitations": "", "keywords": ["character development", "narrative analysis", "Holocaust testimonies", "religious trajectory", "natural language processing"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2412.17451", "pdf": "https://arxiv.org/pdf/2412.17451.pdf", "abs": "https://arxiv.org/abs/2412.17451", "title": "Diving into Self-Evolving Training for Multimodal Reasoning", "authors": ["Wei Liu", "Junlong Li", "Xiwen Zhang", "Fan Zhou", "Yu Cheng", "Junxian He"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "ICML 2025, Project Page: https://mstar-lmm.github.io", "summary": "Self-evolving trainin--where models iteratively learn from their own\noutputs--has emerged as a key approach for complex reasoning tasks, addressing\nthe scarcity of high-quality chain-of-thought data. However, its effectiveness\nin multimodal reasoning, a domain more intricate than text-only reasoning,\nremains underexplored, and the understanding of critical factors in this\ntraining paradigm remains limited. Furthermore, a central challenge for this\ntraining method is performance saturation, which impedes further improvements\nand scalability. Inspired by reinforcement learning (RL), in this paper, we\nreframe self-evolving training for multimodal reasoning through the lens of RL,\nidentifying three pivotal factors: Training Method, Reward Model, and Prompt\nVariation. Through systematic analysis, we establish relatively optimal design\nprinciples that significantly enhance multimodal reasoning capabilities.\nMoreover, delving deeper into training dynamics, we uncover the roots of\nsaturation and propose a new automatic balancing mechanism to mitigate this\nlimitation. Building on these insights, we propose M-STAR (Multimodal\nSelf-evolving Training for Reasoning), a framework that achieves consistent\nperformance gains across models of varying sizes and diverse benchmarks. All\nresources are made publicly available at https://mstar-lmm.github.io.", "AI": {"tldr": "This paper explores self-evolving training for multimodal reasoning, identifying critical factors for its effectiveness and proposing a framework (M-STAR) that improves performance while addressing saturation issues.", "motivation": "To address the scarcity of high-quality chain-of-thought data for complex reasoning tasks, particularly in multimodal reasoning where traditional methods struggle.", "method": "The paper reframes self-evolving training using principles from reinforcement learning, analyzing Training Method, Reward Model, and Prompt Variation to improve efficacy.", "result": "Establishes optimal design principles for multimodal reasoning and introduces a new balancing mechanism to overcome the saturation challenge.", "conclusion": "The proposed M-STAR framework demonstrates consistent performance improvements across various models and benchmarks, enhancing multimodal reasoning capabilities.", "key_contributions": ["Introduction of M-STAR framework for multimodal reasoning", "Identification of key factors affecting self-evolving training", "Development of an automatic balancing mechanism to mitigate saturation."], "limitations": "", "keywords": ["multimodal reasoning", "self-evolving training", "reinforcement learning", "M-STAR", "performance saturation"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2412.17533", "pdf": "https://arxiv.org/pdf/2412.17533.pdf", "abs": "https://arxiv.org/abs/2412.17533", "title": "Behind Closed Words: Creating and Investigating the forePLay Annotated Dataset for Polish Erotic Discourse", "authors": ["Anna Kołos", "Katarzyna Lorenc", "Emilia Wiśnios", "Agnieszka Karlińska"], "categories": ["cs.CL"], "comment": "Accepted for ACL 2025 Main Conference", "summary": "The surge in online content has created an urgent demand for robust detection\nsystems, especially in non-English contexts where current tools demonstrate\nsignificant limitations. We present forePLay, a novel Polish language dataset\nfor erotic content detection, featuring over 24k annotated sentences with a\nmultidimensional taxonomy encompassing ambiguity, violence, and social\nunacceptability dimensions. Our comprehensive evaluation demonstrates that\nspecialized Polish language models achieve superior performance compared to\nmultilingual alternatives, with transformer-based architectures showing\nparticular strength in handling imbalanced categories. The dataset and\naccompanying analysis establish essential frameworks for developing\nlinguistically-aware content moderation systems, while highlighting critical\nconsiderations for extending such capabilities to morphologically complex\nlanguages.", "AI": {"tldr": "Development of forePLay, a Polish language dataset for detecting erotic content, showcasing superior performance of specialized models over multilingual ones.", "motivation": "Addressing the urgent demand for effective detection systems in non-English contexts, specifically for erotic content in Polish, where existing tools are lacking.", "method": "Creation of a dataset containing over 24,000 annotated Polish sentences based on a multidimensional taxonomy that includes ambiguity, violence, and social unacceptability.", "result": "Specialized Polish language models outperform multilingual alternatives in detecting erotic content, with transformer architectures excelling in handling imbalanced categories.", "conclusion": "The forePLay dataset and analysis provide frameworks for linguistically-aware content moderation systems while drawing attention to the complexities of morphologically rich languages.", "key_contributions": ["Introduction of forePLay dataset with rich annotations for Polish language", "Demonstration of superior model performance for specialized Polish models", "Establishment of frameworks for developing content moderation systems in linguistically complex environments."], "limitations": "", "keywords": ["content detection", "Polish language", "dataset", "transformer models", "content moderation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2501.02086", "pdf": "https://arxiv.org/pdf/2501.02086.pdf", "abs": "https://arxiv.org/abs/2501.02086", "title": "Instruction-Following Pruning for Large Language Models", "authors": ["Bairu Hou", "Qibin Chen", "Jianyu Wang", "Guoli Yin", "Chong Wang", "Nan Du", "Ruoming Pang", "Shiyu Chang", "Tao Lei"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "With the rapid scaling of large language models (LLMs), structured pruning\nhas become a widely used technique to learn efficient, smaller models from\nlarger ones, delivering superior performance compared to training similarly\nsized models from scratch. In this paper, we move beyond the traditional static\npruning approach of determining a fixed pruning mask for a model, and propose a\ndynamic approach to structured pruning. In our method, the pruning mask is\ninput-dependent and adapts dynamically based on the information described in a\nuser instruction. Our approach, termed \"instruction-following pruning\",\nintroduces a sparse mask predictor that takes the user instruction as input and\ndynamically selects the most relevant model parameters for the given task. To\nidentify and activate effective parameters, we jointly optimize the sparse mask\npredictor and the LLM, leveraging both instruction-following data and the\npre-training corpus. Experimental results demonstrate the effectiveness of our\napproach on a wide range of evaluation benchmarks. For example, our 3B\nactivated model improves over the 3B dense model by 5-8 points of absolute\nmargin on domains such as math and coding, and rivals the performance of a 9B\nmodel.", "AI": {"tldr": "This paper introduces a dynamic pruning approach for large language models (LLMs) that selects relevant parameters based on user instructions, improving performance over static pruning.", "motivation": "To enhance the efficiency and performance of large language models beyond traditional static pruning methods.", "method": "The authors propose 'instruction-following pruning', where a sparse mask predictor adapts the pruning mask based on input user instructions, optimizing both the predictor and the LLM together.", "result": "The dynamic pruning approach yields a 5-8 point improvement on benchmarks in math and coding tasks compared to a 3B dense model and matches the performance of a 9B model.", "conclusion": "The proposed method demonstrates significant advantages in adapting LLMs to specific tasks without losing efficiency or performance.", "key_contributions": ["Introduction of instruction-following pruning for dynamic parameter selection", "Joint optimization of sparse mask predictor and LLM", "Demonstrated improvements on evaluation benchmarks compared to static pruning"], "limitations": "", "keywords": ["large language models", "dynamic pruning", "instruction-following pruning"], "importance_score": 10, "read_time_minutes": 5}}
{"id": "2501.06645", "pdf": "https://arxiv.org/pdf/2501.06645.pdf", "abs": "https://arxiv.org/abs/2501.06645", "title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings", "authors": ["Tong Liu", "Xiao Yu", "Wenxuan Zhou", "Jindong Gu", "Volker Tresp"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness.", "AI": {"tldr": "FocalPO, a variant of Direct Preference Optimization, improves preference optimization for LLMs by down-weighting misranked pairs.", "motivation": "To address the limitation of Direct Preference Optimization (DPO) where it rarely improves misranked preference pairs despite its focus on them, leading to ineffective training.", "method": "FocalPO modifies the DPO approach by adding a modulating factor to its loss function, allowing it to down-weight misranked preference pairs and focus on enhancing the model's understanding of correctly ranked pairs.", "result": "FocalPO outperforms DPO and its variants on Alpaca Eval 2.0 benchmarks when using Mistral-Base-7B and Llama-3-Instruct-8B models.", "conclusion": "FocalPO is more effective than DPO for training LLMs in preference optimization, particularly in distinguishing between correct and incorrect sample groups.", "key_contributions": ["Introduction of FocalPO as a DPO variant", "Demonstrated superiority of FocalPO over traditional DPO", "Detailed empirical results showing FocalPO's effect on training dynamics"], "limitations": "", "keywords": ["Direct Preference Optimization", "Focal Loss", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.15781", "pdf": "https://arxiv.org/pdf/2501.15781.pdf", "abs": "https://arxiv.org/abs/2501.15781", "title": "Large Language Models to Diffusion Finetuning", "authors": ["Edoardo Cetin", "Tianyu Zhao", "Yujin Tang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Camera-ready version, presented at ICML 2025. Code available at:\n  https://github.com/SakanaAI/L2D", "summary": "We propose a new finetuning method to provide pre-trained large language\nmodels (LMs) the ability to scale test-time compute through the diffusion\nframework. By increasing the number of diffusion steps, we show our finetuned\nmodels achieve monotonically increasing accuracy, directly translating to\nimproved performance across downstream tasks. Furthermore, our finetuned models\ncan expertly answer questions on specific topics by integrating powerful\nguidance techniques, and autonomously determine the compute required for a\ngiven problem by leveraging adaptive ODE solvers. Our method is universally\napplicable to any foundation model pre-trained with a cross-entropy loss and\ndoes not modify any of its original weights, fully preserving its strong\nsingle-step generation capabilities. We show our method is more effective and\nfully compatible with traditional finetuning approaches, introducing an\northogonal new direction to unify the strengths of the autoregressive and\ndiffusion frameworks.", "AI": {"tldr": "A new finetuning method for large language models enhances performance through a diffusion framework, enabling adaptive compute requirements and improved accuracy on downstream tasks.", "motivation": "To improve the accuracy and performance of pre-trained large language models by integrating diffusion methods into their finetuning process.", "method": "The proposed method involves increasing the number of diffusion steps during finetuning, allowing models to autonomously adjust compute requirements based on specific tasks using adaptive ODE solvers without altering original model weights.", "result": "Models finetuned with this method demonstrated monotonically increasing accuracy across various downstream tasks and improved question-answering capabilities on specified topics with the integration of guidance techniques.", "conclusion": "The method effectively combines strengths of autoregressive and diffusion frameworks for better performance in large language models, remaining compatible with traditional finetuning approaches.", "key_contributions": ["A novel finetuning technique leveraging adaptive ODE solvers with diffusion steps.", "Demonstrated consistency in accuracy improvements related to increased compute at test-time.", "Compatibility with existing autoregressive frameworks while maintaining original model performance."], "limitations": "", "keywords": ["large language models", "finetuning", "diffusion framework", "autoregressive models", "adaptive ODE solvers"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.08246", "pdf": "https://arxiv.org/pdf/2502.08246.pdf", "abs": "https://arxiv.org/abs/2502.08246", "title": "Inference-time sparse attention with asymmetric indexing", "authors": ["Pierre-Emmanuel Mazaré", "Gergely Szilvasy", "Maria Lomeli", "Francisco Massa", "Naila Murray", "Hervé Jégou", "Matthijs Douze"], "categories": ["cs.CL"], "comment": null, "summary": "Self-attention in transformer models is an incremental associative memory\nthat maps key vectors to value vectors. One way to speed up self-attention is\nto employ GPU-compatible vector search algorithms based on standard\npartitioning methods such as k-means. However, such partitioning methods yield\npoor results in this context because (1) the keys and queries follow different\ndistributions, and (2) the RoPE positional encoding hinders the bucket\nassignment.\n  This paper introduces Saap (Self-Attention with Asymmetric Partitions), which\novercomes these problems. It is an asymmetrical indexing technique that employs\ndistinct partitions for keys and queries, thereby approximating self-attention\nwith a data-adaptive sparsity pattern. It works on pretrained language models\nand only requires to train (offline) a small query classifier. On a long\ncontext Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens,\nSaap typically reduces by a factor of 20 the fraction of memory that needs to\nbe looked-up, which translates to a time saving of 60\\% when compared to\nFlashAttention-v2.", "AI": {"tldr": "The paper presents Saap, an asymmetric indexing technique for self-attention in transformer models, significantly improving efficiency by optimizing memory usage and processing time.", "motivation": "To enhance the speed of self-attention in transformer models by addressing the limitations of existing GPU-compatible vector search algorithms that use standard partitioning methods.", "method": "Saap introduces asymmetrical partitions for keys and queries, approximating self-attention with a data-adaptive sparsity pattern, requiring only an offline trained small query classifier.", "result": "Saap reduces the memory lookup fraction by a factor of 20 and provides a 60% time saving compared to FlashAttention-v2, particularly effective on models like Llama 3.1-8b with long context sequences.", "conclusion": "Saap effectively improves self-attention efficiency in pretrained language models without extensive retraining, making it a promising method for large-scale applications.", "key_contributions": ["Introduction of asymmetric partitions for keys and queries", "Significant reduction in memory lookup and processing time", "Application to pretrained language models with minimal retraining requirements"], "limitations": "", "keywords": ["self-attention", "transformer models", "asymmetric partitions", "memory efficiency", "Llama 3.1"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2502.09416", "pdf": "https://arxiv.org/pdf/2502.09416.pdf", "abs": "https://arxiv.org/abs/2502.09416", "title": "Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human?", "authors": ["Takumi Goto", "Yusuke Sakai", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "ACL 2025 (Main), 5 pages, 2 figures", "summary": "One of the goals of automatic evaluation metrics in grammatical error\ncorrection (GEC) is to rank GEC systems such that it matches human preferences.\nHowever, current automatic evaluations are based on procedures that diverge\nfrom human evaluation. Specifically, human evaluation derives rankings by\naggregating sentence-level relative evaluation results, e.g., pairwise\ncomparisons, using a rating algorithm, whereas automatic evaluation averages\nsentence-level absolute scores to obtain corpus-level scores, which are then\nsorted to determine rankings. In this study, we propose an aggregation method\nfor existing automatic evaluation metrics which aligns with human evaluation\nmethods to bridge this gap. We conducted experiments using various metrics,\nincluding edit-based metrics, n-gram based metrics, and sentence-level metrics,\nand show that resolving the gap improves results for the most of metrics on the\nSEEDA benchmark. We also found that even BERT-based metrics sometimes\noutperform the metrics of GPT-4. The proposed ranking method is integrated\ngec-metrics.", "AI": {"tldr": "This paper proposes a new aggregation method for automatic evaluation metrics in grammatical error correction (GEC) to align better with human preferences.", "motivation": "Current automatic evaluations in GEC do not match human evaluation methods, leading to discrepancies in system rankings.", "method": "The study introduces an aggregation method that aligns automatic evaluation metrics with human evaluation rankings, using experimental validation on various metrics including edit-based and n-gram based metrics.", "result": "The new method improves results for most existing metrics on the SEEDA benchmark, with BERT-based metrics outperforming GPT-4 metrics in some cases.", "conclusion": "The proposed method enhances the correlation between automatic and human evaluations in GEC, contributing to more accurate system rankings.", "key_contributions": ["Introduces a novel aggregation method for GEC evaluation metrics.", "Demonstrates improved alignment of automatic metrics with human evaluations.", "Provides empirical evidence using the SEEDA benchmark."], "limitations": "The study may not cover all potential GEC metrics and their interactions with human evaluators.", "keywords": ["grammatical error correction", "automatic evaluation", "human evaluation", "ranking method", "SEEDA benchmark"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.10973", "pdf": "https://arxiv.org/pdf/2502.10973.pdf", "abs": "https://arxiv.org/abs/2502.10973", "title": "Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues", "authors": ["David Sasu", "Zehui Wu", "Ziwei Gong", "Run Chen", "Pengyuan Shi", "Lin Ai", "Julia Hirschberg", "Natalie Schluter"], "categories": ["cs.CL"], "comment": "Accepted to Findings at ACL 2025", "summary": "In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the\nfirst multimodal emotion dialogue dataset for an African language, addressing\nthe significant lack of resources for low-resource languages in emotion\nrecognition research. ACE, developed for the Akan language, contains 385\nemotion-labeled dialogues and 6,162 utterances across audio, visual, and\ntextual modalities, along with word-level prosodic prominence annotations. The\npresence of prosodic labels in this dataset also makes it the first\nprosodically annotated African language dataset. We demonstrate the quality and\nutility of ACE through experiments using state-of-the-art emotion recognition\nmethods, establishing solid baselines for future research. We hope ACE inspires\nfurther work on inclusive, linguistically and culturally diverse NLP resources.", "AI": {"tldr": "Introducing the Akan Conversation Emotion (ACE) dataset, a multimodal resource for emotion recognition in the Akan language, addressing the underrepresentation of low-resource languages.", "motivation": "To fill the gap in emotion recognition resources for low-resource African languages.", "method": "Development of a multimodal dataset containing 385 emotion-labeled dialogues and annotations in audio, visual, and textual modalities, with added prosodic prominence labels.", "result": "Demonstrated the quality and utility of the ACE dataset through experiments, establishing solid baselines for emotion recognition methods.", "conclusion": "ACE serves as a foundational resource for future research in inclusive NLP focusing on linguistically and culturally diverse data.", "key_contributions": ["First multimodal emotion dialogue dataset for the Akan language.", "Includes word-level prosodic prominence annotations.", "Establishes baselines for emotion recognition using state-of-the-art methods."], "limitations": "", "keywords": ["Akan language", "emotion recognition", "multimodal dataset", "prosodic annotations", "low-resource languages"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2502.11075", "pdf": "https://arxiv.org/pdf/2502.11075.pdf", "abs": "https://arxiv.org/abs/2502.11075", "title": "Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models", "authors": ["Haoyang Li", "Xuejia Chen", "Zhanchao XU", "Darian Li", "Nicole Hu", "Fei Teng", "Yiming Li", "Luyu Qiu", "Chen Jason Zhang", "Qing Li", "Lei Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nnatural language processing tasks, such as text generation and semantic\nunderstanding. However, their performance on numerical reasoning tasks, such as\nbasic arithmetic, numerical retrieval, and magnitude comparison, remains\nsurprisingly poor. This gap arises from their reliance on surface-level\nstatistical patterns rather than understanding numbers as continuous\nmagnitudes. Existing benchmarks primarily focus on either linguistic competence\nor structured mathematical problem-solving, neglecting fundamental numerical\nreasoning required in real-world scenarios. To bridge this gap, we propose\nNumericBench, a comprehensive benchmark to evaluate six fundamental numerical\ncapabilities: number recognition, arithmetic operations, contextual retrieval,\ncomparison, summary, and logical reasoning. NumericBench includes datasets\nranging from synthetic number lists to the crawled real-world data, addressing\nchallenges like long contexts, noise, and multi-step reasoning. Extensive\nexperiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal\npersistent weaknesses in numerical reasoning, highlighting the urgent need to\nimprove numerically-aware language modeling. The benchmark is released in:\nhttps://github.com/TreeAI-Lab/NumericBench.", "AI": {"tldr": "Proposal of a benchmark called NumericBench aimed at evaluating numerical reasoning capabilities of Large Language Models (LLMs).", "motivation": "To address the poor performance of LLMs on numerical reasoning tasks by providing a comprehensive evaluation framework.", "method": "Introduction of NumericBench, which evaluates six fundamental numerical capabilities through diverse datasets.", "result": "Numerous state-of-the-art LLMs demonstrate persistent weaknesses in tasks involving numerical reasoning, underscoring the need for improvements in numerically-aware language modeling.", "conclusion": "The NumericBench benchmark aims to enhance the evaluation and development of LLMs' numerical reasoning abilities.", "key_contributions": ["Introduction of the NumericBench benchmark for numerical reasoning evaluation", "Coverage of various numerical tasks and real-world context challenges", "Release of extensive datasets for robust evaluation"], "limitations": "", "keywords": ["Large Language Models", "Numerical Reasoning", "Benchmarking", "Machine Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11184", "pdf": "https://arxiv.org/pdf/2502.11184.pdf", "abs": "https://arxiv.org/abs/2502.11184", "title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs", "authors": ["Wenxuan Wang", "Xiaoyuan Liu", "Kuiyi Gao", "Jen-tse Huang", "Youliang Yuan", "Pinjia He", "Shuai Wang", "Zhaopeng Tu"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "comment": "Accepted by ACL 2025", "summary": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research.", "AI": {"tldr": "The paper presents MMSafeAware, a benchmark for evaluating safety awareness in Multimodal Large Language Models (MLLMs) across various safety scenarios, revealing significant deficiencies in current models' safety performance.", "motivation": "The need for improved safety awareness in MLLMs, specifically in correctly identifying unsafe content and reducing over-sensitivity.", "method": "Introduces MMSafeAware benchmark with 1500 image-prompt pairs to evaluate safety across 29 scenarios; evaluates nine widely used MLLMs.", "result": "Current MLLMs exhibit poor safety performance, with high misclassification rates of both unsafe and benign inputs; the methods proposed for improvement were ineffective.", "conclusion": "Current MLLMs struggle with safety awareness, indicating a critical need for better solutions and further research.", "key_contributions": ["Introduction of the MMSafeAware benchmark for MLLM safety evaluation.", "Evaluation of nine popular MLLMs demonstrating their deficiencies in safety awareness.", "Exploration of methods to enhance safety awareness that proved largely ineffective."], "limitations": "Limited success of proposed methods to improve safety awareness; further research needed to address safety challenges in MLLMs.", "keywords": ["Multimodal Large Language Models", "safety awareness", "benchmark", "MMSafeAware", "image-prompt pairs"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.12665", "pdf": "https://arxiv.org/pdf/2502.12665.pdf", "abs": "https://arxiv.org/abs/2502.12665", "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization", "authors": ["Junhui He", "Junna Xing", "Nan Wang", "Rui Xu", "Shangyu Wu", "Peng Zhou", "Qiang Liu", "Chun Jason Xue", "Qingan Li"], "categories": ["cs.CL"], "comment": null, "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.", "AI": {"tldr": "This paper introduces A$^2$ATS, a novel method for efficient retrieval-based KV cache reduction in long context LLMs, achieving improved performance with reduced overhead.", "motivation": "Long context LLMs face challenges related to memory footprint and KV cache access overhead during inference, necessitating effective reduction methods.", "method": "The paper proposes A$^2$ATS, which utilizes vector quantization on key states for efficient top-K token retrieval, with techniques like Windowed Rotary Position Embedding and query-aware vector quantization to enhance attention score approximation.", "result": "A$^2$ATS reduces performance degradation and overhead compared to existing approaches, improving long context serving throughput by up to 2.7 times.", "conclusion": "A$^2$ATS presents a promising solution for enhancing the efficiency of long context LLMs, facilitating better performance with larger batch sizes.", "key_contributions": ["Introduction of A$^2$ATS for KV cache reduction", "Windowed Rotary Position Embedding for decoupling positional dependencies", "Query-aware vector quantization for optimizing attention score approximation"], "limitations": "The study might still face challenges in real-world application performance and scalability beyond current benchmarks.", "keywords": ["long context", "KV cache", "vector quantization", "attention scores", "retrieval-based"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.12921", "pdf": "https://arxiv.org/pdf/2502.12921.pdf", "abs": "https://arxiv.org/abs/2502.12921", "title": "Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison", "authors": ["George-Kirollos Saad", "Scott Sanner"], "categories": ["cs.CL"], "comment": null, "summary": "Query-driven recommendation with unknown items poses a challenge for users to\nunderstand why certain items are appropriate for their needs. Query-driven\nContrastive Summarization (QCS) is a methodology designed to address this issue\nby leveraging language-based item descriptions to clarify contrasts between\nthem. However, existing state-of-the-art contrastive summarization methods such\nas STRUM-LLM fall short of this goal. To overcome these limitations, we\nintroduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs\ndebate-style prompting to generate focused and contrastive summarizations of\nitem aspects relevant to a query. Leveraging modern large language models\n(LLMs) as powerful tools for generating debates, Q-STRUM Debate provides\nenhanced contrastive summaries. Experiments across three datasets demonstrate\nthat Q-STRUM Debate yields significant performance improvements over existing\nmethods on key contrastive summarization criteria, thus introducing a novel and\nperformant debate prompting methodology for QCS.", "AI": {"tldr": "Q-STRUM Debate is a new method for contrastive summarization in query-driven recommendations, improving upon existing methods by utilizing debate-style prompting to generate clearer contrasts between items.", "motivation": "To enhance user understanding of why certain items are suitable for their needs in query-driven recommendations, addressing shortcomings of existing contrastive summarization methods.", "method": "Q-STRUM Debate employs debate-style prompting in large language models to create focused and effective contrastive summaries of item aspects relevant to user queries.", "result": "Experiments show that Q-STRUM Debate significantly outperforms existing contrastive summarization methods on key criteria across three datasets.", "conclusion": "The Q-STRUM Debate methodology introduces an innovative way to generate clearer contrastive summaries, demonstrating enhanced performance in query-driven recommendation scenarios.", "key_contributions": ["Introduction of a novel debate prompting methodology for contrastive summarization.", "Significant performance improvements over existing methods.", "Enhanced user understanding of item relevance through improved summarization techniques."], "limitations": "", "keywords": ["contrastive summarization", "query-driven recommendation", "large language models", "debate prompting", "user understanding"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.14376", "pdf": "https://arxiv.org/pdf/2502.14376.pdf", "abs": "https://arxiv.org/abs/2502.14376", "title": "A Similarity Paradigm Through Textual Regularization Without Forgetting", "authors": ["Fangming Cui", "Jan Fong", "Rongfei Zeng", "Xinmei Tian", "Jun Yu"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Prompt learning has emerged as a promising method for adapting pre-trained\nvisual-language models (VLMs) to a range of downstream tasks. While optimizing\nthe context can be effective for improving performance on specific tasks, it\ncan often lead to poor generalization performance on unseen classes or datasets\nsampled from different distributions. It may be attributed to the fact that\ntextual prompts tend to overfit downstream data distributions, leading to the\nforgetting of generalized knowledge derived from hand-crafted prompts. In this\npaper, we propose a novel method called Similarity Paradigm with Textual\nRegularization (SPTR) for prompt learning without forgetting. SPTR is a\ntwo-pronged design based on hand-crafted prompts that is an inseparable\nframework. 1) To avoid forgetting general textual knowledge, we introduce the\noptimal transport as a textual regularization to finely ensure approximation\nwith hand-crafted features and tuning textual features. 2) In order to\ncontinuously unleash the general ability of multiple hand-crafted prompts, we\npropose a similarity paradigm for natural alignment score and adversarial\nalignment score to improve model robustness for generalization. Both modules\nshare a common objective in addressing generalization issues, aiming to\nmaximize the generalization capability derived from multiple hand-crafted\nprompts. Four representative tasks (i.e., non-generalization few-shot learning,\nbase-to-novel generalization, cross-dataset generalization, domain\ngeneralization) across 11 datasets demonstrate that SPTR outperforms existing\nprompt learning methods.", "AI": {"tldr": "A novel method named Similarity Paradigm with Textual Regularization (SPTR) for improving prompt learning in visual-language models, aimed at enhancing generalization without losing essential knowledge.", "motivation": "To improve the generalization performance of visual-language models in prompt learning while avoiding overfitting on specific downstream data distributions.", "method": "SPTR utilizes optimal transport for textual regularization to retain hand-crafted features while tuning textual features, alongside a similarity paradigm to enhance model robustness through alignment scores.", "result": "SPTR outperformed existing prompt learning methods in four key tasks across 11 datasets, showcasing improved generalization capabilities.", "conclusion": "The proposed method addresses generalization issues in prompt learning by effectively combining textual regularization and alignment through multi-hand-crafted prompts.", "key_contributions": ["Introduction of Similarity Paradigm with Textual Regularization (SPTR)", "Optimal transport for retaining hand-crafted features", "Improved generalization capabilities across multiple tasks"], "limitations": "", "keywords": ["prompt learning", "visual-language models", "generalization", "textual regularization", "machine learning"], "importance_score": 6, "read_time_minutes": 7}}
{"id": "2502.15109", "pdf": "https://arxiv.org/pdf/2502.15109.pdf", "abs": "https://arxiv.org/abs/2502.15109", "title": "Social Genome: Grounded Social Reasoning Abilities of Multimodal Models", "authors": ["Leena Mathur", "Marian Qian", "Paul Pu Liang", "Louis-Philippe Morency"], "categories": ["cs.CL", "cs.LG"], "comment": "Under Review, 24 pages", "summary": "Social reasoning abilities are crucial for AI systems to effectively\ninterpret and respond to multimodal human communication and interaction within\nsocial contexts. We introduce SOCIAL GENOME, the first benchmark for\nfine-grained, grounded social reasoning abilities of multimodal models. SOCIAL\nGENOME contains 272 videos of interactions and 1,486 human-annotated reasoning\ntraces related to inferences about these interactions. These traces contain\n5,777 reasoning steps that reference evidence from visual cues, verbal cues,\nvocal cues, and external knowledge (contextual knowledge external to videos).\nSOCIAL GENOME is also the first modeling challenge to study external knowledge\nin social reasoning. SOCIAL GENOME computes metrics to holistically evaluate\nsemantic and structural qualities of model-generated social reasoning traces.\nWe demonstrate the utility of SOCIAL GENOME through experiments with\nstate-of-the-art models, identifying performance gaps and opportunities for\nfuture research to improve the grounded social reasoning abilities of\nmultimodal models.", "AI": {"tldr": "SOCIAL GENOME is a benchmark for evaluating social reasoning abilities in multimodal AI models, featuring videos and human-annotated reasoning traces.", "motivation": "To enable AI systems to better interpret and respond to human communication and interactions in social contexts.", "method": "Introduced SOCIAL GENOME benchmark with 272 videos and 1,486 human-annotated reasoning traces, which include 5,777 reasoning steps referencing multiple cue types and external knowledge.", "result": "Experiments with state-of-the-art models revealed performance gaps in social reasoning abilities, highlighting areas for improvement.", "conclusion": "SOCIAL GENOME provides a comprehensive framework for evaluating and improving grounded social reasoning in multimodal models.", "key_contributions": ["First benchmark for fine-grained social reasoning in AI", "Modeling challenge focusing on external knowledge in social reasoning", "Holistic evaluation metrics for social reasoning traces"], "limitations": "", "keywords": ["social reasoning", "multimodal models", "benchmark", "external knowledge", "AI communication"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.17110", "pdf": "https://arxiv.org/pdf/2502.17110.pdf", "abs": "https://arxiv.org/abs/2502.17110", "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation", "authors": ["Junyang Wang", "Haiyang Xu", "Xi Zhang", "Ming Yan", "Ji Zhang", "Fei Huang", "Jitao Sang"], "categories": ["cs.CL", "cs.CV"], "comment": "17 pages, 7 figures, 9 tables", "summary": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation.", "AI": {"tldr": "Mobile-Agent-V is a novel framework that improves mobile automation by using video to inject operational knowledge, reducing manual effort and increasing performance by 36%.", "motivation": "The rise in mobile device usage creates a need for efficient task management automation, which is currently lacking in many AI frameworks due to limited operational expertise.", "method": "The framework utilizes video as a knowledge source to streamline the process of acquiring operational knowledge for mobile automation, eliminating the burden of manual knowledge writing.", "result": "Mobile-Agent-V demonstrates a 36% performance improvement over existing mobile automation methods when assessed using the new Mobile-Knowledge benchmark.", "conclusion": "This framework significantly enhances mobile automation by effortlessly integrating operational knowledge, showcasing advantages in efficiency and performance.", "key_contributions": ["Introduction of the Mobile-Agent-V framework for mobile automation using video knowledge.", "Development of the Mobile-Knowledge benchmark for evaluating external knowledge impact on mobile agents.", "Demonstrated 36% performance improvement in mobile automation tasks."], "limitations": "", "keywords": ["Mobile automation", "operational knowledge", "video-based learning", "AI frameworks", "benchmarking"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.17214", "pdf": "https://arxiv.org/pdf/2502.17214.pdf", "abs": "https://arxiv.org/abs/2502.17214", "title": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought", "authors": ["Boxuan Zhang", "Ruqi Zhang"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Accepted by ACL 2025 Findings", "summary": "Large language models (LLMs) excel in many tasks but struggle to accurately\nquantify uncertainty in their generated responses. This limitation makes it\nchallenging to detect misinformation and ensure reliable decision-making.\nExisting uncertainty quantification (UQ) methods for LLMs are primarily\nprompt-wise rather than response-wise, often requiring multiple response\nsamples, which incurs high computational costs. Moreover, LLMs have been shown\nto be overconfident, particularly when using reasoning steps to derive their\nanswers. In this work, we propose CoT-UQ, a response-wise UQ framework that\nintegrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT)\ninto the UQ process. CoT-UQ captures critical information during inference by\nextracting keywords from each reasoning step and assessing their importance to\nthe final answer. This key reasoning information is then aggregated to produce\na final uncertainty estimate. We conduct extensive experiments based on Llama\nFamily with model sizes varying from 8B to 13B across logical and mathematical\nreasoning tasks. Experimental results demonstrate that CoT-UQ significantly\noutperforms existing UQ methods, achieving an average improvement of 5.9% AUROC\ncompared to current UQ methods. The code is available at:\nhttps://github.com/ZBox1005/CoT-UQ.", "AI": {"tldr": "This paper introduces CoT-UQ, a response-wise uncertainty quantification framework for large language models that integrates reasoning capabilities to enhance decision-making reliability.", "motivation": "Large language models often fail to accurately quantify uncertainty in their responses, leading to challenges in detecting misinformation and making reliable decisions.", "method": "We propose CoT-UQ, a framework that utilizes Chain-of-Thought reasoning from LLMs to extract and assess the importance of keywords during inference, yielding a response-wise uncertainty estimate.", "result": "CoT-UQ demonstrates a significant performance increase, with an average improvement of 5.9% AUROC when compared to existing uncertainty quantification methods across various reasoning tasks using the Llama Family models.", "conclusion": "The new framework improves reliability in LLM output by providing a more precise uncertainty quantification, making LLMs more effective in critical applications.", "key_contributions": ["Development of CoT-UQ for response-wise uncertainty quantification", "Integration of reasoning through Chain-of-Thought into the UQ process", "Demonstrated significant performance improvements over existing UQ methods"], "limitations": "", "keywords": ["Uncertainty Quantification", "Large Language Models", "Chain-of-Thought", "Reasoning", "Misinformation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.17878", "pdf": "https://arxiv.org/pdf/2502.17878.pdf", "abs": "https://arxiv.org/abs/2502.17878", "title": "Towards Enhanced Immersion and Agency for LLM-based Interactive Drama", "authors": ["Hongqiu Wu", "Weiqi Wu", "Tianyang Xu", "Jiameng Zhang", "Hai Zhao"], "categories": ["cs.CL"], "comment": "Accepted by ACL'2025", "summary": "LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the\nuser (i.e. the player) plays the role of a character in the story, has\nconversations with characters played by LLM agents, and experiences an\nunfolding story. This paper begins with understanding interactive drama from\ntwo aspects: Immersion, the player's feeling of being present in the story, and\nAgency, the player's ability to influence the story world. Both are crucial to\ncreating an enjoyable interactive experience, while they have been\nunderexplored in previous work. To enhance these two aspects, we first propose\nPlaywriting-guided Generation, a novel method that helps LLMs craft dramatic\nstories with substantially improved structures and narrative quality.\nAdditionally, we introduce Plot-based Reflection for LLM agents to refine their\nreactions to align with the player's intentions. Our evaluation relies on human\njudgment to assess the gains of our methods in terms of immersion and agency.", "AI": {"tldr": "This paper presents LLM-based Interactive Drama, improving immersion and agency in AI-driven storytelling through novel methods.", "motivation": "The paper aims to enhance the user experience in interactive drama by focusing on immersion and agency, which have previously been underexplored.", "method": "The authors propose Playwriting-guided Generation to create structured stories, and Plot-based Reflection for LLM agents to improve their responses to player intentions.", "result": "The evaluation shows significant improvements in immersion and agency through the proposed methods as assessed by human judges.", "conclusion": "The proposed methods advance the capabilities of LLMs in interactive storytelling, enhancing player engagement and experience.", "key_contributions": ["Introduction of Playwriting-guided Generation for better story structure.", "Development of Plot-based Reflection for LLM agents to improve player interaction.", "Empirical evaluation demonstrating enhanced immersion and agency."], "limitations": "The evaluation is based on human judgment which may introduce subjectivity; further quantitative metrics could strengthen the findings.", "keywords": ["Interactive Drama", "LLM", "Immersion", "Agency", "Storytelling"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.18460", "pdf": "https://arxiv.org/pdf/2502.18460.pdf", "abs": "https://arxiv.org/abs/2502.18460", "title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers", "authors": ["Xueguang Ma", "Xi Victoria Lin", "Barlas Oguz", "Jimmy Lin", "Wen-tau Yih", "Xilun Chen"], "categories": ["cs.CL", "cs.IR"], "comment": "ACL 2025", "summary": "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.", "AI": {"tldr": "DRAMA is a training framework that effectively combines large language models with smaller dense retrievers to enhance performance and efficiency.", "motivation": "To overcome the computational challenges associated with large language models in retrieval tasks, while improving generalization with smaller models.", "method": "The approach trains smaller dense retrievers using LLMs as a backbone, employing a single-stage contrastive learning on LLM-augmented data.", "result": "DRAMA outperforms traditional retrieval models in multilingual scenarios and long-context applications, demonstrating enhanced efficiency and generalization.", "conclusion": "The framework bridges the gap between model performance and resource efficiency, making dense retrieval more viable in practical applications.", "key_contributions": ["Introduction of the DRAMA training framework", "Utilization of pruned LLMs to enhance smaller retrievers", "Demonstrated superior performance across tasks and languages compared to traditional methods."], "limitations": "", "keywords": ["dense retrievers", "large language models", "contrastive learning", "multilingual retrieval", "long-context capabilities"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.19756", "pdf": "https://arxiv.org/pdf/2502.19756.pdf", "abs": "https://arxiv.org/abs/2502.19756", "title": "PolyPrompt: Automating Knowledge Extraction from Multilingual Language Models with Dynamic Prompt Generation", "authors": ["Nathan Roll"], "categories": ["cs.CL", "cs.LG"], "comment": "6 pages, 2 figures", "summary": "Large language models (LLMs) showcase increasingly impressive English\nbenchmark scores, however their performance profiles remain inconsistent across\nmultilingual settings. To address this gap, we introduce PolyPrompt, a novel,\nparameter-efficient framework for enhancing the multilingual capabilities of\nLLMs. Our method learns a set of trigger tokens for each language through a\ngradient-based search, identifying the input query's language and selecting the\ncorresponding trigger tokens which are prepended to the prompt during\ninference. We perform experiments on two ~1 billion parameter models, with\nevaluations on the global MMLU benchmark across fifteen typologically and\nresource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared\nto naive and translation-pipeline baselines.", "AI": {"tldr": "PolyPrompt enhances multilingual capabilities of LLMs using a parameter-efficient framework.", "motivation": "LLMs show inconsistent performance in multilingual contexts, prompting the need for improved methods.", "method": "PolyPrompt learns language-specific trigger tokens through a gradient-based search, appending them to prompts based on the identified language during inference.", "result": "Experiments show accuracy improvements of 3.7%-19.9% on the global MMLU benchmark across fifteen languages compared to naive and translation-pipeline methods.", "conclusion": "PolyPrompt effectively increases the multilingual accuracy of LLMs, addressing significant performance gaps.", "key_contributions": ["Introduction of PolyPrompt framework for LLMs", "Use of gradient-based search for trigger token learning", "Demonstrated multilingual accuracy gains on a diverse language benchmark"], "limitations": "", "keywords": ["Large Language Models", "Multilingual Learning", "Gradient-Based Search", "PolyPrompt", "MMLU Benchmark"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.20129", "pdf": "https://arxiv.org/pdf/2502.20129.pdf", "abs": "https://arxiv.org/abs/2502.20129", "title": "Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking", "authors": ["Yifan Zhang", "Wenyu Du", "Dongming Jin", "Jie Fu", "Zhi Jin"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Chain-of-thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. Our\nkey contributions are: (1) We evaluate the state tracking capabilities of\nTransformer+CoT and its variants, confirming the effectiveness of CoT. (2)\nNext, we identify the circuit (a subset of model components, responsible for\ntracking the world state), indicating that late-layer MLP neurons play a key\nrole. We propose two metrics, compression and distinction, and show that the\nneuron sets for each state achieve nearly 100% accuracy, providing evidence of\nan implicit finite state automaton (FSA) embedded within the model. (3)\nAdditionally, we explore three challenging settings: skipping intermediate\nsteps, introducing data noises, and testing length generalization. Our results\ndemonstrate that Transformer+CoT learns robust algorithms (FSAs), highlighting\nits resilience in challenging scenarios. Our code is available at\nhttps://github.com/IvanChangPKU/FSA.", "AI": {"tldr": "The paper evaluates the effectiveness of Chain-of-Thought (CoT) in Transformer models, highlighting its state tracking capabilities and discovering the role of late-layer MLP neurons.", "motivation": "To understand the mechanisms behind Transformer+CoT and its effectiveness in learning state tracking algorithms.", "method": "Evaluated Transformer+CoT's state tracking capabilities, identified key circuits in the model, and proposed metrics to analyze neuron performance.", "result": "Demonstrated that late-layer MLP neurons play a crucial role in state tracking, achieving nearly 100% accuracy in implicit finite state automata (FSAs) identification.", "conclusion": "Transformer+CoT can learn robust algorithms that are resilient in challenging scenarios, suggesting a strong mechanistic understanding of its capabilities.", "key_contributions": ["Evaluation of state tracking in Transformer+CoT", "Identification of late-layer MLP neuron role", "Proposing metrics for analyzing model circuits"], "limitations": "", "keywords": ["Chain-of-Thought", "Transformer models", "state tracking"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.01926", "pdf": "https://arxiv.org/pdf/2503.01926.pdf", "abs": "https://arxiv.org/abs/2503.01926", "title": "Unnatural Languages Are Not Bugs but Features for LLMs", "authors": ["Keyu Duan", "Yiran Zhao", "Zhili Feng", "Jinjie Ni", "Tianyu Pang", "Qian Liu", "Tianle Cai", "Longxu Dou", "Kenji Kawaguchi", "Anirudh Goyal", "J. Zico Kolter", "Michael Qizhe Shieh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been observed to process non-human-readable\ntext sequences, such as jailbreak prompts, often viewed as a bug for aligned\nLLMs. In this work, we present a systematic investigation challenging this\nperception, demonstrating that unnatural languages - strings that appear\nincomprehensible to humans but maintain semantic meanings for LLMs - contain\nlatent features usable by models. Notably, unnatural languages possess latent\nfeatures that can be generalized across different models and tasks during\ninference. Furthermore, models fine-tuned on unnatural versions of instruction\ndatasets perform on-par with those trained on natural language, achieving 49.71\nwin rates in Length-controlled AlpacaEval 2.0 in average across various base\nmodels. In addition, through comprehensive analysis, we demonstrate that LLMs\nprocess unnatural languages by filtering noise and inferring contextual meaning\nfrom filtered words.", "AI": {"tldr": "This paper investigates the utility of unnatural languages in Large Language Models (LLMs), demonstrating that they have latent features useful for various tasks and can match the performance of models trained on natural languages.", "motivation": "The study challenges the notion that LLMs should not process non-human-readable text sequences, seeking to understand the latent features of unnatural languages.", "method": "A systematic investigation comparing the performance of LLMs trained on unnatural languages to those trained on natural languages, with evaluations on Length-controlled AlpacaEval 2.0.", "result": "Models fine-tuned on unnatural versions of instruction datasets achieved performance comparable to those trained on natural language, with 49.71 win rates on average across different models.", "conclusion": "Unnatural languages contain interpretable features for LLMs, and models can effectively process these languages while filtering out noise to extract contextual meanings.", "key_contributions": ["Demonstrated that unnatural languages have latent features usable by LLMs.", "Showed that models trained on unnatural instruction datasets can perform on par with those trained on natural language.", "Provided comprehensive analysis on how LLMs process and infer meaning from unnatural text."], "limitations": "Limited to specific LLM architectures and datasets; further research needed to generalize findings across all model types.", "keywords": ["Large Language Models", "unnatural languages", "human-computer interaction", "text processing", "semantic meaning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.02519", "pdf": "https://arxiv.org/pdf/2503.02519.pdf", "abs": "https://arxiv.org/abs/2503.02519", "title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent", "authors": ["Xingzuo Li", "Kehai Chen", "Yunfei Long", "Xuefeng Bai", "Yong Xu", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) agents typically adopt a step-by-step reasoning\nframework, in which they interleave the processes of thinking and acting to\naccomplish the given task. However, this paradigm faces a deep-rooted one-pass\nissue whereby each generated intermediate thought is plugged into the\ntrajectory regardless of its correctness, which can cause irreversible error\npropagation. To address the issue, this paper proposes a novel framework called\nGenerator-Assistant Stepwise Rollback (GA-Rollback) to induce better\ndecision-making for LLM agents. Particularly, GA-Rollback utilizes a generator\nto interact with the environment and an assistant to examine each action\nproduced by the generator, where the assistant triggers a rollback operation\nupon detection of incorrect actions. Moreover, we introduce two additional\nstrategies tailored for the rollback scenario to further improve its\neffectiveness. Extensive experiments show that GA-Rollback achieves significant\nimprovements over several strong baselines on three widely used benchmarks. Our\nanalysis further reveals that GA-Rollback can function as a robust\nplug-and-play module, integrating seamlessly with other methods.", "AI": {"tldr": "This paper presents GA-Rollback, a new framework for improving decision-making in LLM agents by allowing for rollback on incorrect actions, thus mitigating error propagation.", "motivation": "To mitigate the issue of error propagation in LLM agents that arises from the one-pass decision-making process where incorrect intermediate thoughts affect outcomes.", "method": "The proposed GA-Rollback framework utilizes a generator to take actions and an assistant to evaluate those actions, allowing rollbacks on detected mistakes to improve overall decision-making.", "result": "GA-Rollback shows significant performance enhancements over strong baselines across three standard benchmarks in LLM decision-making tasks.", "conclusion": "GA-Rollback is positioned as an effective, robust, and modular addition that can enhance existing LLM methodologies without extensive modifications.", "key_contributions": ["Introduction of the GA-Rollback framework for LLM decision-making", "Implementation of a rollback mechanism to correct errors in real-time", "Demonstration of improved performance on benchmark tasks compared to existing methods."], "limitations": "", "keywords": ["Large Language Models", "Rollback Mechanism", "Decision-Making"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.05037", "pdf": "https://arxiv.org/pdf/2503.05037.pdf", "abs": "https://arxiv.org/abs/2503.05037", "title": "Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence", "authors": ["Mohsen Fayyaz", "Ali Modarressi", "Hinrich Schuetze", "Nanyun Peng"], "categories": ["cs.CL", "cs.IR"], "comment": "ACL 2025 Main Conference", "summary": "Dense retrieval models are commonly used in Information Retrieval (IR)\napplications, such as Retrieval-Augmented Generation (RAG). Since they often\nserve as the first step in these systems, their robustness is critical to avoid\ndownstream failures. In this work, we repurpose a relation extraction dataset\n(e.g., Re-DocRED) to design controlled experiments that quantify the impact of\nheuristic biases, such as a preference for shorter documents, on retrievers\nlike Dragon+ and Contriever. We uncover major vulnerabilities, showing\nretrievers favor shorter documents, early positions, repeated entities, and\nliteral matches, all while ignoring the answer's presence! Notably, when\nmultiple biases combine, models exhibit catastrophic performance degradation,\nselecting the answer-containing document in less than 10% of cases over a\nsynthetic biased document without the answer. Furthermore, we show that these\nbiases have direct consequences for downstream applications like RAG, where\nretrieval-preferred documents can mislead LLMs, resulting in a 34% performance\ndrop than providing no documents at all.\nhttps://huggingface.co/datasets/mohsenfayyaz/ColDeR", "AI": {"tldr": "This paper investigates the impact of heuristic biases in dense retrieval models used in Information Retrieval applications, particularly Retrieval-Augmented Generation, revealing significant vulnerabilities in the selection of documents by these models.", "motivation": "To understand and quantify the impact of heuristic biases on retrieval models to prevent downstream failures in applications like RAG.", "method": "The authors repurpose a relation extraction dataset to conduct controlled experiments and analyze the performance of the retrievers Dragon+ and Contriever under various biases.", "result": "The experiments reveal that retrievers show a strong preference for shorter documents and biases that cause catastrophic performance degradation, with the ability to select the correct answer-containing document dropping to less than 10% under combined biases.", "conclusion": "Biases in retrieval models not only affect their performance but also have a detrimental impact on downstream applications, leading to significant performance drops in LLM outcomes when biased documents are prioritized.", "key_contributions": ["Quantified the impact of heuristic biases in dense retrieval models.", "Demonstrated significant performance degradation due to the combination of biases.", "Highlighted the implications of biases on downstream AI applications like RAG."], "limitations": "The study focuses on specific heuristic biases and may not cover all possible biases in retrieval models.", "keywords": ["Dense Retrieval", "Information Retrieval", "Retrieval-Augmented Generation", "Heuristic Biases", "Performance Analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.10927", "pdf": "https://arxiv.org/pdf/2503.10927.pdf", "abs": "https://arxiv.org/abs/2503.10927", "title": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM Responses", "authors": ["Angela Lopez-Cardona", "Sebastian Idesis", "Miguel Barreda-Ángeles", "Sergi Abadal", "Ioannis Arapakis"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper has been accepted to ACM ETRA 2025 and published on\n  PACMHCI", "summary": "While Large Language Models (LLMs) have significantly advanced natural\nlanguage processing, aligning them with human preferences remains an open\nchallenge. Although current alignment methods rely primarily on explicit\nfeedback, eye-tracking (ET) data offers insights into real-time cognitive\nprocessing during reading. In this paper, we present OASST-ETC, a novel\neye-tracking corpus capturing reading patterns from 24 participants, while\nevaluating LLM-generated responses from the OASST1 dataset. Our analysis\nreveals distinct reading patterns between preferred and non-preferred\nresponses, which we compare with synthetic eye-tracking data. Furthermore, we\nexamine the correlation between human reading measures and attention patterns\nfrom various transformer-based models, discovering stronger correlations in\npreferred responses. This work introduces a unique resource for studying human\ncognitive processing in LLM evaluation and suggests promising directions for\nincorporating eye-tracking data into alignment methods. The dataset and\nanalysis code are publicly available.", "AI": {"tldr": "This paper introduces OASST-ETC, a novel eye-tracking corpus that captures reading patterns from participants evaluating LLM-generated responses, revealing distinct cognitive processing differences and correlations between human preferences and transformer model attention patterns.", "motivation": "The paper addresses the challenge of aligning Large Language Models (LLMs) with human preferences by utilizing eye-tracking data to better understand cognitive processing during reading.", "method": "The study involves the collection of eye-tracking data from 24 participants while they evaluate LLM-generated responses from the OASST1 dataset, analyzing patterns and correlations with synthetic eye-tracking data.", "result": "Distinct reading patterns were observed between preferred and non-preferred LLM responses, with stronger correlations found between human reading measures and attention from transformer models for preferred responses.", "conclusion": "The work provides a unique dataset for human cognitive processing study in LLM evaluation and proposes future directions for integrating eye-tracking data into LLM alignment methods.", "key_contributions": ["Introduction of the OASST-ETC eye-tracking corpus", "Analysis of reading patterns related to LLM responses", "Correlation findings between human preferences and model attention patterns"], "limitations": "", "keywords": ["eye-tracking", "Large Language Models", "cognitive processing", "alignment methods", "human preferences"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.11630", "pdf": "https://arxiv.org/pdf/2503.11630.pdf", "abs": "https://arxiv.org/abs/2503.11630", "title": "The time scale of redundancy between prosody and linguistic context", "authors": ["Tamar I. Regev", "Chiebuka Ohams", "Shaylee Xie", "Lukas Wolf", "Evelina Fedorenko", "Alex Warstadt", "Ethan G. Wilcox", "Tiago Pimentel"], "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": "13 pages, 4 figures, accepted to ACL. Updated following ACL reviewers\n  comments", "summary": "In spoken communication, information is transmitted not only via words, but\nalso through a rich array of non-verbal signals, including prosody--the\nnon-segmental auditory features of speech. Do these different communication\nchannels carry distinct information? Prior work has shown that the information\ncarried by prosodic features is substantially redundant with that carried by\nthe surrounding words. Here, we systematically examine the time scale of this\nrelationship, studying how it varies with the length of past and future\ncontexts. We find that a word's prosodic features require an extended past\ncontext (3-8 words across different features) to be reliably predicted. Given\nthat long-scale contextual information decays in memory, prosody may facilitate\ncommunication by adding information that is locally unique. We also find that a\nword's prosodic features show some redundancy with future words, but only with\na short scale of 1-2 words, consistent with reports of incremental short-term\nplanning in language production. Thus, prosody may facilitate communication by\nhelping listeners predict upcoming material. In tandem, our results highlight\npotentially distinct roles that prosody plays in facilitating integration of\nwords into past contexts and in helping predict upcoming words.", "AI": {"tldr": "The paper investigates the role of prosody in spoken communication, focusing on how prosodic features are influenced by past and future context lengths during speech.", "motivation": "Understanding the interaction between words and prosodic features can enhance insights into how non-verbal signals contribute to effective communication.", "method": "The study systematically examines the relationship between prosodic features of words and their preceding and following contexts, analyzing varying time scales of 3-8 words for past context and 1-2 words for future context.", "result": "The research finds that prosodic features require extended past contexts for reliable prediction and show redundancy with short future contexts, indicating distinct roles of prosody in facilitating communication.", "conclusion": "Prosody adds unique information locally and aids in predicting upcoming speech, suggesting its importance in language processing.", "key_contributions": ["Demonstrates the need for extended past contexts to predict prosody reliably.", "Identifies distinct roles of prosody in integrating past contexts and in predicting future words.", "Highlights the redundancy between prosody and future words only within short contexts."], "limitations": "The study focuses on specific time scales and does not consider all possible aspects of communication signaling.", "keywords": ["prosody", "spoken communication", "context integration", "language production", "predictive processing"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2503.14433", "pdf": "https://arxiv.org/pdf/2503.14433.pdf", "abs": "https://arxiv.org/abs/2503.14433", "title": "Splintering Nonconcatenative Languages for Better Tokenization", "authors": ["Bar Gazit", "Shaltiel Shmidman", "Avi Shmidman", "Yuval Pinter"], "categories": ["cs.CL"], "comment": "Findings of the ACL 2025", "summary": "Common subword tokenization algorithms like BPE and UnigramLM assume that\ntext can be split into meaningful units by concatenative measures alone. This\nis not true for languages such as Hebrew and Arabic, where morphology is\nencoded in root-template patterns, or Malay and Georgian, where split affixes\nare common. We present SPLINTER, a pre-processing step which rearranges text\ninto a linear form that better represents such nonconcatenative morphologies,\nenabling meaningful contiguous segments to be found by the tokenizer. We\ndemonstrate SPLINTER's merit using both intrinsic measures evaluating token\nvocabularies in Hebrew, Arabic, and Malay; as well as on downstream tasks using\nBERT-architecture models trained for Hebrew.", "AI": {"tldr": "SPLINTER is introduced as a pre-processing technique for better tokenization in languages with nonconcatenative morphologies.", "motivation": "Common tokenization algorithms fail to adequately process languages with unique morphological structures, leading to suboptimal performance.", "method": "SPLINTER rearranges text into a linear form to enhance the tokenization process for languages like Hebrew, Arabic, and Malay.", "result": "SPLINTER shows improved token vocabularies and effective performance in downstream tasks related to Hebrew when evaluated against BERT models.", "conclusion": "By utilizing SPLINTER, tokenization becomes more effective for languages with complex morphological patterns, enhancing NLP model performance.", "key_contributions": ["Introduction of SPLINTER for nonconcatenative morphologies", "Demonstration of intrinsic measures for token vocabularies", "Evaluation on downstream BERT tasks in Hebrew"], "limitations": "", "keywords": ["tokenization", "morphology", "SPLINTER", "BERT", "NLP"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2503.16048", "pdf": "https://arxiv.org/pdf/2503.16048.pdf", "abs": "https://arxiv.org/abs/2503.16048", "title": "Meta-Learning Neural Mechanisms rather than Bayesian Priors", "authors": ["Michael Goodale", "Salvador Mascarenhas", "Yair Lakretz"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main", "summary": "Children acquire language despite being exposed to several orders of\nmagnitude less data than large language models require. Meta-learning has been\nproposed as a way to integrate human-like learning biases into neural-network\narchitectures, combining both the structured generalizations of symbolic models\nwith the scalability of neural-network models. But what does meta-learning\nexactly imbue the model with? We investigate the meta-learning of formal\nlanguages and find that, contrary to previous claims, meta-trained models are\nnot learning simplicity-based priors when meta-trained on datasets organised\naround simplicity. Rather, we find evidence that meta-training imprints neural\nmechanisms (such as counters) into the model, which function like cognitive\nprimitives for the network on downstream tasks. Most surprisingly, we find that\nmeta-training on a single formal language can provide as much improvement to a\nmodel as meta-training on 5000 different formal languages, provided that the\nformal language incentivizes the learning of useful neural mechanisms. Taken\ntogether, our findings provide practical implications for efficient\nmeta-learning paradigms and new theoretical insights into linking symbolic\ntheories and neural mechanisms.", "AI": {"tldr": "The paper investigates meta-learning in neural networks and formal languages, revealing that it teaches neural mechanisms rather than simplicity-based priors, with implications for efficient learning paradigms.", "motivation": "To understand what meta-learning imparts to neural network models and how they learn from formal languages.", "method": "The authors explored meta-training on datasets organized by simplicity and assessed the resulting models' capabilities on downstream tasks.", "result": "Meta-trained models were found to learn cognitive primitives like counters, achieving significant performance improvements even with training on a single formal language.", "conclusion": "The findings challenge previous assumptions about meta-learning, highlighting its potential for linking symbolic theories with neural mechanisms and improving meta-learning strategies.", "key_contributions": ["Demonstrated that meta-training imparts neural mechanisms to models.", "Showed that training on one formal language can be as beneficial as training on many.", "Provided insights linking symbolic theories with neural mechanisms."], "limitations": "", "keywords": ["meta-learning", "neural networks", "formal languages", "cognitive mechanics", "symbolic theories"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.17579", "pdf": "https://arxiv.org/pdf/2503.17579.pdf", "abs": "https://arxiv.org/abs/2503.17579", "title": "Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility", "authors": ["Suet-Ying Lam", "Qingcheng Zeng", "Jingyi Wu", "Rob Voigt"], "categories": ["cs.CL"], "comment": "ACL 2025 Camera-ready", "summary": "Whether large language models (LLMs) process language similarly to humans has\nbeen the subject of much theoretical and practical debate. We examine this\nquestion through the lens of the production-interpretation distinction found in\nhuman sentence processing and evaluate the extent to which instruction-tuned\nLLMs replicate this distinction. Using an empirically documented asymmetry\nbetween pronoun production and interpretation in humans for implicit causality\nverbs as a testbed, we find that some LLMs do quantitatively and qualitatively\nreflect human-like asymmetries between production and interpretation. We\ndemonstrate that whether this behavior holds depends upon both model size-with\nlarger models more likely to reflect human-like patterns and the choice of\nmeta-linguistic prompts used to elicit the behavior. Our codes and results are\navailable at\nhttps://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025.", "AI": {"tldr": "This paper investigates whether large language models (LLMs) reflect human-like patterns in language processing, specifically focusing on the production-interpretation distinction in pronoun usage.", "motivation": "The study aims to understand the similarities and differences between LLMs and human language processing, particularly regarding how pronouns are produced and interpreted.", "method": "The researchers examine the asymmetry in pronoun production and interpretation related to implicit causality verbs in both humans and instruction-tuned LLMs, assessing their performance through various meta-linguistic prompts.", "result": "The findings indicate that certain LLMs demonstrate human-like asymmetries in pronoun usage, with larger models being more likely to replicate these patterns.", "conclusion": "Model size and the type of prompts significantly influence the LLMs' ability to mimic human-like language processing behavior.", "key_contributions": ["Demonstrates that LLMs can reflect human-like production-interpretation asymmetries.", "Establishes a relationship between model size and the reflection of human language patterns.", "Provides empirical data on LLM performance in language processing tasks."], "limitations": "The analysis is limited to specific models and types of prompts, and further research is needed across a broader range of models and scenarios.", "keywords": ["large language models", " language processing", " human-like behavior"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.22395", "pdf": "https://arxiv.org/pdf/2503.22395.pdf", "abs": "https://arxiv.org/abs/2503.22395", "title": "Negation: A Pink Elephant in the Large Language Models' Room?", "authors": ["Tereza Vrabcová", "Marek Kadlčík", "Petr Sojka", "Michal Štefánik", "Michal Spiegel"], "categories": ["cs.CL"], "comment": null, "summary": "Negations are key to determining sentence meaning, making them essential for\nlogical reasoning. Despite their importance, negations pose a substantial\nchallenge for large language models (LLMs) and remain underexplored.\n  We constructed and published two new textual entailment datasets NoFEVER-ML\nand NoSNLI-ML in four languages (English, Czech, German, and Ukrainian) with\n  examples differing in negation. It allows investigation of the root causes of\nthe negation problem and its exemplification: how popular LLM model properties\nand language impact their inability to handle negation correctly.\n  Contrary to previous work, we show that increasing the model size may improve\nthe models' ability to handle negations. Furthermore, we find that both the\nmodels' reasoning accuracy and robustness to negation are language-dependent\nand that the length and explicitness of the premise have an impact on\nrobustness. There is better accuracy in projective language with fixed order,\nsuch as English, than in non-projective ones, such as German or Czech.\n  Our entailment datasets pave the way to further research for explanation and\nexemplification of the negation problem, minimization of LLM hallucinations,\nand improvement of LLM reasoning in multilingual settings.", "AI": {"tldr": "This paper introduces two datasets for studying the challenges of negation in language models, revealing key insights about the linguistic properties affecting LLM performance.", "motivation": "Negations are crucial for logical reasoning but pose significant challenges for large language models (LLMs), which necessitates further investigation.", "method": "Two new textual entailment datasets, NoFEVER-ML and NoSNLI-ML, were constructed in four languages (English, Czech, German, Ukrainian) to analyze how LLMs handle negation.", "result": "Larger model sizes may improve handling of negations; model accuracy and robustness are language-dependent, with projective languages outperforming non-projective ones.", "conclusion": "The datasets will aid in understanding and improving LLM reasoning about negation and reducing hallucinations in multilingual contexts.", "key_contributions": ["Introduction of new datasets for negation challenges in LLMs", "Demonstration of the impact of model size on negation handling", "Insights on language-dependent performance in LLMs"], "limitations": "Focus is primarily on negation, which may not address other reasoning challenges in LLMs.", "keywords": ["negation", "language models", "textual entailment", "multilingual", "LLM robustness"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2504.00589", "pdf": "https://arxiv.org/pdf/2504.00589.pdf", "abs": "https://arxiv.org/abs/2504.00589", "title": "Efficient Annotator Reliability Assessment with EffiARA", "authors": ["Owen Cook", "Jake Vasilakes", "Ian Roberts", "Xingyi Song"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Data annotation is an essential component of the machine learning pipeline;\nit is also a costly and time-consuming process. With the introduction of\ntransformer-based models, annotation at the document level is increasingly\npopular; however, there is no standard framework for structuring such tasks.\nThe EffiARA annotation framework is, to our knowledge, the first project to\nsupport the whole annotation pipeline, from understanding the resources\nrequired for an annotation task to compiling the annotated dataset and gaining\ninsights into the reliability of individual annotators as well as the dataset\nas a whole. The framework's efficacy is supported by two previous studies: one\nimproving classification performance through annotator-reliability-based\nsoft-label aggregation and sample weighting, and the other increasing the\noverall agreement among annotators through removing identifying and replacing\nan unreliable annotator. This work introduces the EffiARA Python package and\nits accompanying webtool, which provides an accessible graphical user interface\nfor the system. We open-source the EffiARA Python package at\nhttps://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at\nhttps://effiara.gate.ac.uk.", "AI": {"tldr": "EffiARA is a novel annotation framework designed to enhance the machine learning annotation process by supporting the entire pipeline and ensuring evaluative insights on annotators and datasets.", "motivation": "The need for a standardized framework for document-level annotation in machine learning, addressing high costs and time consumption associated with data annotation.", "method": "Introduction of the EffiARA annotation framework, which encompasses resources understanding, annotation dataset compilation, and insights into annotator reliability, plus the release of a Python package and webtool.", "result": "The efficacy of the EffiARA framework is evidenced by improvements in classification performance through reliable labeling and increased agreement among annotators.", "conclusion": "EffiARA offers an efficient and structured approach to data annotation, with tools that enhance reliability and quality of annotated datasets.", "key_contributions": ["First comprehensive annotation framework supporting all stages of the annotation process", "Includes a graphical user interface for easier access", "Open-source availability of both Python package and webtool to promote community use."], "limitations": "", "keywords": ["data annotation", "machine learning", "transformer models", "annotator reliability", "EffiARA"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.08961", "pdf": "https://arxiv.org/pdf/2504.08961.pdf", "abs": "https://arxiv.org/abs/2504.08961", "title": "A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models", "authors": ["Kseniia Petukhova", "Ekaterina Kochmar"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have shown promise in\nautomating discourse annotation for conversations. While manually designing\ntree annotation schemes significantly improves annotation quality for humans\nand models, their creation remains time-consuming and requires expert\nknowledge. We propose a fully automated pipeline that uses LLMs to construct\nsuch schemes and perform annotation. We evaluate our approach on speech\nfunctions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our\nexperiments compare various design choices, and we show that frequency-guided\ndecision trees, paired with an advanced LLM for annotation, can outperform\npreviously manually designed trees and even match or surpass human annotators\nwhile significantly reducing the time required for annotation. We release all\ncode and resultant schemes and annotations to facilitate future research on\ndiscourse annotation.", "AI": {"tldr": "The paper presents a fully automated pipeline utilizing Large Language Models (LLMs) to create and perform discourse annotation, aiming to improve efficiency and quality in comparison to traditional methods.", "motivation": "The manual design of tree annotation schemes is time-consuming and necessitates expert knowledge, prompting the need for an automated solution.", "method": "We developed an automated pipeline that constructs annotation schemes using LLMs and evaluates it against established taxonomies like SWBD-DAMSL, analyzing different design choices to optimize annotation quality.", "result": "The automated pipeline, particularly with frequency-guided decision trees and advanced LLMs, outperforms traditionally designed trees and can match or exceed human annotators while significantly reducing annotation time.", "conclusion": "This approach not only enhances the efficiency of discourse annotation but also maintains or improves quality, with all related resources made publicly available for further research.", "key_contributions": ["Introduction of a fully automated LLM-based discourse annotation pipeline", "Demonstration of superior performance over manually designed annotation schemes", "Public release of code and data to support ongoing research"], "limitations": "", "keywords": ["Large Language Models", "discourse annotation", "automated pipelines"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.12216", "pdf": "https://arxiv.org/pdf/2504.12216.pdf", "abs": "https://arxiv.org/abs/2504.12216", "title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning", "authors": ["Siyan Zhao", "Devaansh Gupta", "Qinqing Zheng", "Aditya Grover"], "categories": ["cs.CL", "cs.LG"], "comment": "27 pages, project page at https://dllm-reasoning.github.io/", "summary": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO, the first integration of policy gradient methods to masked dLLMs.\nThrough empirical studies, we investigate the performance of different\npost-training recipes on multiple mathematical and planning benchmarks. We find\nthat d1 yields the best performance and significantly improves performance of a\nstate-of-the-art dLLM. Our code is released at\nhttps://dllm-reasoning.github.io/.", "AI": {"tldr": "This paper proposes d1, a framework for enhancing reasoning capabilities in non-autoregressive large language models (dLLMs) through a combination of supervised finetuning and reinforcement learning.", "motivation": "To explore if diffusion-based large language models can leverage recent advances in reasoning observed in autoregressive models, thereby enhancing their performance in reasoning tasks.", "method": "The authors adapt pre-trained masked dLLMs using a masked supervised finetuning technique for distillation of knowledge and introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO.", "result": "Empirical studies demonstrate that the d1 framework significantly improves the reasoning performance of a state-of-the-art dLLM on various mathematical and planning benchmarks.", "conclusion": "The proposed d1 framework effectively enhances reasoning in masked dLLMs, outperforming previous models in key benchmarks.", "key_contributions": ["Introduction of the d1 framework for masked dLLMs", "Development of the diffu-GRPO RL algorithm", "Demonstration of improved performance on reasoning benchmarks"], "limitations": "", "keywords": ["large language models", "reinforcement learning", "diffusion models", "reasoning", "natural language processing"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.09825", "pdf": "https://arxiv.org/pdf/2505.09825.pdf", "abs": "https://arxiv.org/abs/2505.09825", "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning", "authors": ["Peiqi Sui", "Juan Diego Rodriguez", "Philippe Laban", "Dean Murphy", "Joseph P. Dexter", "Richard Jean So", "Samuel Baker", "Pramit Chaudhuri"], "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks.", "AI": {"tldr": "The paper presents KRISTEVA, a benchmark for evaluating close reading skills in LLMs, uncovering their strengths and weaknesses in interpretive reasoning.", "motivation": "To assess the performance of large language models (LLMs) in close reading tasks amidst the lack of suitable evaluation benchmarks.", "method": "The study introduces KRISTEVA, a close reading benchmark comprising 1331 multiple-choice questions designed to evaluate LLMs on tasks related to literary analysis.", "result": "LLMs show college-level close reading competency with an accuracy of 49.7% - 69.7%, but human evaluators outperform them in 10 of 11 tasks.", "conclusion": "LLMs have some understanding of literary works, but their performance in close reading falls short of that of experienced human evaluators.", "key_contributions": ["Introduction of KRISTEVA, the first close reading benchmark for LLM evaluation", "Development of three task sets reflecting stages of the close reading process", "Empirical results showing LLM accuracy compared to human evaluators"], "limitations": "The benchmark may not cover all aspects of close reading and relies on specific classroom data for questions.", "keywords": ["close reading", "benchmarking", "large language models", "interpretive reasoning", "literary analysis"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.22830", "pdf": "https://arxiv.org/pdf/2505.22830.pdf", "abs": "https://arxiv.org/abs/2505.22830", "title": "What Has Been Lost with Synthetic Evaluation?", "authors": ["Alexander Gill", "Abhilasha Ravichander", "Ana Marasović"], "categories": ["cs.CL", "cs.AI"], "comment": "v2: Fixed low resolution figures", "summary": "Large language models (LLMs) are increasingly used for data generation.\nHowever, creating evaluation benchmarks raises the bar for this emerging\nparadigm. Benchmarks must target specific phenomena, penalize exploiting\nshortcuts, and be challenging. Through two case studies, we investigate whether\nLLMs can meet these demands by generating reasoning over-text benchmarks and\ncomparing them to those created through careful crowdsourcing. Specifically, we\nevaluate both the validity and difficulty of LLM-generated versions of two\nhigh-quality reading comprehension datasets: CondaQA, which evaluates reasoning\nabout negation, and DROP, which targets reasoning about quantities. We find\nthat prompting LLMs can produce variants of these datasets that are often valid\naccording to the annotation guidelines, at a fraction of the cost of the\noriginal crowdsourcing effort. However, we show that they are less challenging\nfor LLMs than their human-authored counterparts. This finding sheds light on\nwhat may have been lost by generating evaluation data with LLMs, and calls for\ncritically reassessing the immediate use of this increasingly prevalent\napproach to benchmark creation.", "AI": {"tldr": "This paper investigates the effectiveness of large language models (LLMs) in generating evaluation benchmarks for reasoning tasks, finding that while LLMs can produce valid datasets at lower costs, these datasets are less challenging than human-authored ones.", "motivation": "The increasing use of LLMs for data generation has created a need for rigorous evaluation benchmarks that effectively test reasoning capabilities and discourage shortcut exploitation.", "method": "The study involves two case studies that compare LLM-generated versions of high-quality reading comprehension datasets (CondaQA and DROP) to their human-authored counterparts, assessing validity and difficulty.", "result": "LLM-generated datasets often meet annotation guidelines for validity and are cheaper to produce, but they are found to be less challenging for LLMs than human-generated datasets.", "conclusion": "While LLMs can generate valid evaluation data, the resulting datasets may be insufficiently challenging, prompting a reassessment of LLM benchmarks in evaluation tasks.", "key_contributions": ["Demonstration of LLMs generating valid reasoning benchmarks at lower costs.", "Comparison of LLM-generated and human-authored datasets for reading comprehension tasks.", "Call for critical reassessment of using LLMs for benchmark creation."], "limitations": "LLM-generated datasets are less challenging than their human-created equivalents, suggesting potential shortcomings in their effectiveness.", "keywords": ["large language models", "benchmark creation", "reading comprehension", "reasoning", "evaluation datasets"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.22848", "pdf": "https://arxiv.org/pdf/2505.22848.pdf", "abs": "https://arxiv.org/abs/2505.22848", "title": "LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference", "authors": ["Pingjun Hong", "Beiduo Chen", "Siyao Peng", "Marie-Catherine de Marneffe", "Barbara Plank"], "categories": ["cs.CL"], "comment": "21 pages, 6 figures", "summary": "There is increasing evidence of Human Label Variation (HLV) in Natural\nLanguage Inference (NLI), where annotators assign different labels to the same\npremise-hypothesis pair. However, within-label variation--cases where\nannotators agree on the same label but provide divergent reasoning--poses an\nadditional and mostly overlooked challenge. Several NLI datasets contain\nhighlighted words in the NLI item as explanations, but the same spans on the\nNLI item can be highlighted for different reasons, as evidenced by free-text\nexplanations, which offer a window into annotators' reasoning. To\nsystematically understand this problem and gain insight into the rationales\nbehind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for\ncategorizing free-text explanations. Using this taxonomy, we annotate a subset\nof the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it\naligns with NLI labels, highlights, and explanations. We further assess the\ntaxonomy's usefulness in explanation generation, demonstrating that\nconditioning generation on LITEX yields explanations that are linguistically\ncloser to human explanations than those generated using only labels or\nhighlights. Our approach thus not only captures within-label variation but also\nshows how taxonomy-guided generation for reasoning can bridge the gap between\nhuman and model explanations more effectively than existing strategies.", "AI": {"tldr": "The paper introduces LITEX, a taxonomy for categorizing free-text explanations in Natural Language Inference (NLI) to address within-label variation, and demonstrates its effectiveness in generating linguistically closer explanations to humans.", "motivation": "To address the challenge of within-label variation in Natural Language Inference (NLI), where annotators may agree on labels but diverge in reasoning.", "method": "A new taxonomy, LITEX, was introduced to systematically categorize free-text explanations, followed by annotation of the e-SNLI dataset, validation of the taxonomy's reliability, and analysis of its alignment with NLI labels, highlights, and explanations.", "result": "LITEX annotations showed how different rationales align with the same NLI labels and highlighted the effectiveness of using LITEX for generating explanations that resemble human reasoning more closely than traditional methods.", "conclusion": "The LITEX taxonomy captures within-label variation in NLI and enhances explanation generation, leading to better alignment between human and model-generated explanations.", "key_contributions": ["Introduction of LITEX taxonomy for free-text explanation categorization in NLI", "Systematic annotation and validation of e-SNLI dataset based on the taxonomy", "Demonstration of improved explanatory generation by conditioning on LITEX."], "limitations": "", "keywords": ["Natural Language Inference", "Human Label Variation", "Taxonomy", "Explanation Generation", "e-SNLI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.23001", "pdf": "https://arxiv.org/pdf/2505.23001.pdf", "abs": "https://arxiv.org/abs/2505.23001", "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors", "authors": ["Yize Cheng", "Wenxiao Wang", "Mazda Moayeri", "Soheil Feizi"], "categories": ["cs.CL"], "comment": null, "summary": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors.", "AI": {"tldr": "DyePack is a framework designed to detect if models have been contaminated by benchmark test sets during training using backdoor attacks without needing model internals.", "motivation": "To address the issue of test set contamination in large language models which compromises reproducibility and transparency in evaluations.", "method": "DyePack mixes backdoor samples with test data and employs multiple backdoors with stochastic targets to compute false positive rates (FPR) for flagged models.", "result": "DyePack successfully detects all contaminated models in multiple-choice and open-ended tasks with extremely low false positive rates, ensuring reliable identification of contamination.", "conclusion": "DyePack provides a principled method for identifying test set contamination, maintaining low false positive rates and offering strong evidence for flagged models.", "key_contributions": ["Introduction of the DyePack framework for test set contamination detection.", "Use of backdoor attacks to flag models without internal access.", "Evaluation across multiple models and datasets ensuring low false positive rates."], "limitations": "The framework's effectiveness may vary with the complexity of the models used or the nature of the datasets.", "keywords": ["backdoor attacks", "test set contamination", "large language models", "reproducibility", "transparency"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.23114", "pdf": "https://arxiv.org/pdf/2505.23114.pdf", "abs": "https://arxiv.org/abs/2505.23114", "title": "Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data", "authors": ["Seohyeong Lee", "Eunwon Kim", "Hwaran Lee", "Buru Chang"], "categories": ["cs.CL"], "comment": null, "summary": "Human preference data plays a critical role in aligning large language models\n(LLMs) with human values. However, collecting such data is often expensive and\ninefficient, posing a significant scalability challenge. To address this, we\nintroduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and\ndiagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we\ncompute alignment scores for LLM-generated responses to instructions from\nexisting preference datasets. These scores are then used to construct an\nAlignment Data Map based on their mean and variance. Our experiments show that\nusing only 33 percent of the data, specifically samples in the high-mean,\nlow-variance region, achieves performance comparable to or better than using\nthe entire dataset. This finding suggests that the Alignment Data Map can\nsignificantly improve data collection efficiency by identifying high-quality\nsamples for LLM alignment without requiring explicit annotations. Moreover, the\nAlignment Data Map can diagnose existing preference datasets. Our analysis\nshows that it effectively detects low-impact or potentially misannotated\nsamples. Source code is available online.", "AI": {"tldr": "The paper introduces the Alignment Data Map, a tool to efficiently analyze preference data for LLM alignment, significantly improving data collection by identifying high-quality samples without explicit annotations.", "motivation": "To tackle the high costs and inefficiencies in collecting human preference data necessary for aligning LLMs with human values.", "method": "Utilizing GPT-4o to compute alignment scores for LLM responses based on existing preference datasets, leading to the formation of an Alignment Data Map which shows data's mean and variance.", "result": "With only 33% of the data, specifically from high-mean, low-variance regions, the tool achieves comparable or superior performance to using the entire dataset.", "conclusion": "The Alignment Data Map enhances data collection efficiency and can also diagnose existing preference datasets, identifying low-impact or misannotated samples.", "key_contributions": ["Introduction of the Alignment Data Map for LLM preference analysis", "Demonstration of improved efficiency in data collection using reduced datasets", "Capability to diagnose preference datasets for quality."], "limitations": "", "keywords": ["LLM alignment", "preference data", "GPT-4o", "data efficiency", "diagnosis tools"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.23368", "pdf": "https://arxiv.org/pdf/2505.23368.pdf", "abs": "https://arxiv.org/abs/2505.23368", "title": "Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation", "authors": ["Beiduo Chen", "Yang Janet Liu", "Anna Korhonen", "Barbara Plank"], "categories": ["cs.CL"], "comment": "22 pages, 7 figures", "summary": "The recent rise of reasoning-tuned Large Language Models (LLMs)--which\ngenerate chains of thought (CoTs) before giving the final answer--has attracted\nsignificant attention and offers new opportunities for gaining insights into\nhuman label variation, which refers to plausible differences in how multiple\nannotators label the same data instance. Prior work has shown that\nLLM-generated explanations can help align model predictions with human label\ndistributions, but typically adopt a reverse paradigm: producing explanations\nbased on given answers. In contrast, CoTs provide a forward reasoning path that\nmay implicitly embed rationales for each answer option, before generating the\nanswers. We thus propose a novel LLM-based pipeline enriched with\nlinguistically-grounded discourse segmenters to extract supporting and opposing\nstatements for each answer option from CoTs with improved accuracy. We also\npropose a rank-based HLV evaluation framework that prioritizes the ranking of\nanswers over exact scores, which instead favor direct comparison of label\ndistributions. Our method outperforms a direct generation method as well as\nbaselines on three datasets, and shows better alignment of ranking methods with\nhumans, highlighting the effectiveness of our approach.", "AI": {"tldr": "This paper introduces a novel LLM-based pipeline for enhancing answer option evaluation through forward reasoning with chains of thought (CoTs), improving alignment with human label distributions.", "motivation": "To address human label variation in multiple annotators and improve model predictions alignment with human labeling through better reasoning processes.", "method": "The proposed pipeline uses reasoning-tuned LLMs to generate chains of thought for each answer option and employs discourse segmenters to extract supporting and opposing statements, followed by a rank-based evaluation framework.", "result": "The method outperforms previous direct generation techniques and baselines across three datasets, leading to better alignment of ranked answers with human judgments.", "conclusion": "The proposed approach demonstrates significant improvements in reasoning accuracy and human alignment for labeling tasks.", "key_contributions": ["Development of an LLM-based pipeline utilizing chains of thought for answer evaluation.", "Introduction of a rank-based evaluation framework that emphasizes ranking over exact scores.", "Demonstration of improved accuracy and alignment with human label distributions across multiple datasets."], "limitations": "", "keywords": ["Large Language Models", "Reasoning", "Human Label Variation", "Evaluation Framework", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2505.23754", "pdf": "https://arxiv.org/pdf/2505.23754.pdf", "abs": "https://arxiv.org/abs/2505.23754", "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning", "authors": ["Ziyin Zhang", "Jiahao Xu", "Zhiwei He", "Tian Liang", "Qiuzhi Liu", "Yansi Li", "Linfeng Song", "Zhenwen Liang", "Zhuosheng Zhang", "Rui Wang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.", "AI": {"tldr": "DeepTheorem is a novel informal theorem-proving framework that enhances LLM mathematical reasoning through natural language, featuring a large benchmark dataset and a reinforcement learning strategy.", "motivation": "Traditional automated theorem proving approaches do not align well with the informal knowledge that large language models (LLMs) acquire during pre-training, limiting their mathematical reasoning capabilities.", "method": "The framework includes a comprehensive dataset with 121K informal theorems and proofs, and employs a novel reinforcement learning strategy (RL-Zero) tailored for informal theorem proving.", "result": "DeepTheorem significantly improves LLM theorem-proving performance, achieving state-of-the-art accuracy and reasoning quality after extensive experiments.", "conclusion": "DeepTheorem has the potential to advance automated informal theorem proving and facilitate deeper mathematical exploration.", "key_contributions": ["Introduction of a large-scale benchmark dataset for informal theorem proving.", "Development of a novel reinforcement learning strategy for enhancing mathematical reasoning in LLMs.", "Comprehensive evaluation metrics for assessing proof correctness and reasoning quality."], "limitations": "", "keywords": ["theorem proving", "large language models", "reinforcement learning", "mathematical reasoning", "benchmark datasets"], "importance_score": 9, "read_time_minutes": 15}}
