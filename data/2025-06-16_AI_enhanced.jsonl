{"id": "2506.11276", "pdf": "https://arxiv.org/pdf/2506.11276.pdf", "abs": "https://arxiv.org/abs/2506.11276", "title": "Needling Through the Threads: A Visualization Tool for Navigating Threaded Online Discussions", "authors": ["Yijun Liu", "Frederick Choi", "Eshwar Chandrasekharan"], "categories": ["cs.HC"], "comment": null, "summary": "Navigating large-scale online discussions is difficult due to the rapid pace\nand large volume of user-generated content. Prior work in CSCW has shown that\nmoderators often struggle to follow multiple simultaneous discussions, track\nevolving conversations, and maintain contextual understanding--all of which\nhinder timely and effective moderation. While platforms like Reddit use\nthreaded structures to organize discourse, deeply nested threads can still\nobscure discussions and make it difficult to grasp the overall trajectory of\nconversations. In this paper, we present an interactive system called Needle to\nsupport better navigation and comprehension of complex discourse within\nthreaded discussions. Needle uses visual analytics to summarize key\nconversational metrics--such as activity, toxicity levels, and voting\ntrends--over time, offering both high-level insights and detailed breakdowns of\ndiscussion threads. Through a user study with ten Reddit moderators, we find\nthat Needle supports moderation by reducing cognitive load in making sense of\nlarge discussion, helping prioritize areas that need attention, and providing\ndecision-making supports. Based on our findings, we provide a set of design\nguidelines to inform future visualization-driven moderation tools and\nsociotechnical systems. To the best of our knowledge, Needle is one of the\nfirst systems to combine interactive visual analytics with human-in-the-loop\nmoderation for threaded online discussions.", "AI": {"tldr": "Needle is an interactive system designed to improve navigation and comprehension in large-scale threaded online discussions by utilizing visual analytics.", "motivation": "Moderators often find it challenging to effectively manage multiple simultaneous discussions due to high volumes of user-generated content, which leads to difficulties in maintaining context and timely moderation.", "method": "Needle leverages visual analytics to summarize key metrics such as activity, toxicity levels, and voting trends over time, providing both high-level insights and detailed breakdowns of discussions. A user study was conducted with ten Reddit moderators to evaluate its effectiveness.", "result": "The user study found that Needle helps reduce cognitive load for moderators, allows them to prioritize important areas, and aids in decision-making during moderation.", "conclusion": "Needle not only improves understanding of complex threaded discussions but also sets design guidelines for future visualization-driven moderation tools and sociotechnical systems.", "key_contributions": ["Introduction of Needle, an interactive system for moderating threaded discussions", "Use of visual analytics to summarize discussion metrics", "Provision of design guidelines for future moderation tools"], "limitations": "", "keywords": ["visual analytics", "moderation", "threaded discussions", "cognitive load", "online discourse"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2506.11326", "pdf": "https://arxiv.org/pdf/2506.11326.pdf", "abs": "https://arxiv.org/abs/2506.11326", "title": "Combining Log Data and Collaborative Dialogue Features to Predict Project Quality in Middle School AI Education", "authors": ["Conrad Borchers", "Xiaoyi Tian", "Kristy Elizabeth Boyer", "Maya Israel"], "categories": ["cs.HC"], "comment": "Research paper accepted to the 9th Educational Data Mining in\n  Computer Science Education (CSEDM) Workshop", "summary": "Project-based learning plays a crucial role in computing education. However,\nits open-ended nature makes tracking project development and assessing success\nchallenging. We investigate how dialogue and system interaction logs predict\nproject quality during collaborative, project-based AI learning of 94 middle\nschool students working in pairs. We used linguistic features from dialogue\ntranscripts and behavioral features from system logs to predict three project\nquality outcomes: productivity (number of training phrases), content richness\n(word density), and lexical variation (word diversity) of chatbot training\nphrases. We compared the predictive accuracy of each modality and a fusion of\nthe modalities. Results indicate log data better predicts productivity, while\ndialogue data is more effective for content richness. Both modalities modestly\npredict lexical variation. Multimodal fusion improved predictions for\nproductivity and lexical variation of training phrases but not content\nrichness. These findings suggest that the value of multimodal fusion depends on\nthe specific learning outcome. The study contributes to multimodal learning\nanalytics by demonstrating the nuanced interplay between behavioral and\nlinguistic data in assessing student learning progress in open-ended AI\nlearning environments.", "AI": {"tldr": "This study examines how dialogue and system interaction logs can predict the quality of collaborative AI learning projects among middle school students, with a focus on productivity, content richness, and lexical variation in their chatbot training phrases.", "motivation": "To address the challenges of tracking project development and assessing success in open-ended project-based learning in computing education.", "method": "The study analyzed linguistic features from dialogue transcripts and behavioral features from system logs of 94 middle school students working in pairs on AI projects to predict three project quality outcomes.", "result": "Log data was found to be a better predictor of productivity, while dialogue data was more effective for content richness. Multimodal fusion improved predictions for productivity and lexical variation but not for content richness.", "conclusion": "The effectiveness of multimodal fusion in predicting learning outcomes varies depending on the specific aspect being assessed, contributing to multimodal learning analytics.", "key_contributions": ["Demonstrates the interplay between dialogue and system log data in project-based learning assessments.", "Shows the varying predictive power of different data modalities regarding specific learning outcomes.", "Highlights the impact of multimodal fusion on prediction effectiveness in educational contexts."], "limitations": "", "keywords": ["project-based learning", "AI education", "multimodal learning analytics", "dialogue analysis", "system interaction logs"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.11366", "pdf": "https://arxiv.org/pdf/2506.11366.pdf", "abs": "https://arxiv.org/abs/2506.11366", "title": "Meeting Patients Where They're At: Toward the Expansion of Chaplaincy Care into Online Spiritual Care Communities", "authors": ["Alemitu Bezabih", "Shadi Nourriz", "Anne-Marie Snider", "Rosalie Rauenzahn", "C. Estelle Smith"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Despite a growing need for spiritual care in the US, it is often\nunder-served, inaccessible, or misunderstood, while almost no prior work in\nCSCW/HCI research has engaged with professional chaplains and spiritual care\nproviders. This interdisciplinary study aims to develop a foundational\nunderstanding of how spiritual care may (or may not) be expanded into online\nspaces -- especially focusing on anonymous, asynchronous, and text-based online\ncommunities. We conducted an exploratory mixed-methods study with chaplains\n(N=22) involving interviews and user testing sessions centered around Reddit\nsupport communities to understand participants' perspectives on technology and\ntheir ideations about the role of chaplaincy in prospective Online Spiritual\nCare Communities (OSCCs). Our Grounded Theory Method analysis highlighted\nbenefits of OSCCs including: meeting patients where they are at; accessibility\nand scalability; and facilitating patient-initiated care. Chaplains highlighted\nhow their presence in OSCCs could help with shaping peer interactions,\nmoderation, synchronous chats for group care, and redirecting to external\nresources, while also raising important feasibility concerns, risks, and needs\nfor future design and research. We used an existing taxonomy of chaplaincy\ntechniques to show that some spiritual care strategies may be amenable to\nonline spaces, yet we also exposed the limitations of technology to fully\nmediate spiritual care and the need to develop new online chaplaincy\ninterventions. Based on these findings, we contribute the model of a ``Care\nLoop'' between institutionally-based formal care and platform-based community\ncare to expand access and drive greater awareness and utilization of spiritual\ncare. We also contribute design implications to guide future work in online\nspiritual care.", "AI": {"tldr": "This study explores expanding spiritual care into online spaces, focusing on community roles and implications for design.", "motivation": "To address the growing need for spiritual care that is currently under-served and not well integrated into technology and online communities.", "method": "A mixed-methods approach involving interviews and user testing sessions with 22 chaplains, centered around Reddit support communities.", "result": "The study identified benefits of online spiritual care communities (OSCCs) such as accessibility, scalability, and patient-initiated care, while also highlighting the limitations of technology in mediating spiritual care.", "conclusion": "Developing a 'Care Loop' model can enhance access to spiritual care and raise awareness of its utilization, alongside providing design implications for online spiritual care interventions.", "key_contributions": ["Model of a 'Care Loop' for integrating formal care and community care in spirituality", "Identification of challenges and opportunities for chaplains in online settings", "Design implications for future online spiritual care projects"], "limitations": "The study highlights that technology cannot fully mediate spiritual care and indicates additional research is needed for new online interventions.", "keywords": ["spiritual care", "online communities", "chaplaincy", "HCI", "CSCW"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.11393", "pdf": "https://arxiv.org/pdf/2506.11393.pdf", "abs": "https://arxiv.org/abs/2506.11393", "title": "Co-Designing a Chatbot for Culturally Competent Clinical Communication: Experience and Reflections", "authors": ["Sandro Radovanović", "Shuangyu Li"], "categories": ["cs.HC", "cs.CY"], "comment": "19 pages, 7 figures", "summary": "Clinical communication skills are essential for preparing healthcare\nprofessionals to provide equitable care across cultures. However, traditional\ntraining with simulated patients can be resource intensive and difficult to\nscale, especially in under-resourced settings. In this project, we explore the\nuse of an AI-driven chatbot to support culturally competent communication\ntraining for medical students. The chatbot was designed to simulate realistic\npatient conversations and provide structured feedback based on the ACT Cultural\nCompetence model. We piloted the chatbot with a small group of third-year\nmedical students at a UK medical school in 2024. Although we did not follow a\nformal experimental design, our experience suggests that the chatbot offered\nuseful opportunities for students to reflect on their communication,\nparticularly around empathy and interpersonal understanding. More challenging\nareas included addressing systemic issues and historical context. Although this\nearly version of the chatbot helped surface some interesting patterns,\nlimitations were also clear, such as the absence of nonverbal cues and the\ntendency for virtual patients to be overly agreeable. In general, this\nreflection highlights both the potential and the current limitations of AI\ntools in communication training. More work is needed to better understand their\nimpact and improve the learning experience.", "AI": {"tldr": "This paper explores the use of an AI-driven chatbot to enhance communication training for medical students, focusing on cultural competence.", "motivation": "To address the resource challenges of traditional simulated patient training in medical education, particularly in under-resourced settings.", "method": "Development and pilot testing of an AI chatbot designed to simulate patient conversations and provide structured feedback based on the ACT Cultural Competence model.", "result": "Initial feedback from third-year medical students indicated that the chatbot facilitated reflection on communication skills, particularly in empathy, but also revealed challenges such as lack of nonverbal cues.", "conclusion": "The use of AI tools like chatbots shows promise for communication training, yet considerable limitations remain that need to be addressed through further research.", "key_contributions": ["Exploration of AI in medical communication training", "Pilot testing of a novel chatbot for cultural competence", "Identification of both benefits and limitations in training using AI tools"], "limitations": "Absence of nonverbal cues and tendency for virtual patients to be overly agreeable.", "keywords": ["AI chatbot", "cultural competence", "communication training", "medical education", "healthcare"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2506.11017", "pdf": "https://arxiv.org/pdf/2506.11017.pdf", "abs": "https://arxiv.org/abs/2506.11017", "title": "TeleEval-OS: Performance evaluations of large language models for operations scheduling", "authors": ["Yanyan Wang", "Yingying Wang", "Junli Liang", "Yin Xu", "Yunlong Liu", "Yiming Xu", "Zhengwang Jiang", "Zhehe Li", "Fei Li", "Long Zhao", "Kuang Xu", "Qi Song", "Xiangyang Li"], "categories": ["cs.CL", "cs.AI", "cs.PF"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has significantly\npropelled progress in artificial intelligence, demonstrating substantial\napplication potential across multiple specialized domains. Telecommunications\noperation scheduling (OS) is a critical aspect of the telecommunications\nindustry, involving the coordinated management of networks, services, risks,\nand human resources to optimize production scheduling and ensure unified\nservice control. However, the inherent complexity and domain-specific nature of\nOS tasks, coupled with the absence of comprehensive evaluation benchmarks, have\nhindered thorough exploration of LLMs' application potential in this critical\nfield. To address this research gap, we propose the first Telecommunications\nOperation Scheduling Evaluation Benchmark (TeleEval-OS). Specifically, this\nbenchmark comprises 15 datasets across 13 subtasks, comprehensively simulating\nfour key operational stages: intelligent ticket creation, intelligent ticket\nhandling, intelligent ticket closure, and intelligent evaluation. To\nsystematically assess the performance of LLMs on tasks of varying complexity,\nwe categorize their capabilities in telecommunications operation scheduling\ninto four hierarchical levels, arranged in ascending order of difficulty: basic\nNLP, knowledge Q&A, report generation, and report analysis. On TeleEval-OS, we\nleverage zero-shot and few-shot evaluation methods to comprehensively assess 10\nopen-source LLMs (e.g., DeepSeek-V3) and 4 closed-source LLMs (e.g., GPT-4o)\nacross diverse scenarios. Experimental results demonstrate that open-source\nLLMs can outperform closed-source LLMs in specific scenarios, highlighting\ntheir significant potential and value in the field of telecommunications\noperation scheduling.", "AI": {"tldr": "This paper introduces TeleEval-OS, a benchmark for evaluating large language models in telecommunications operation scheduling tasks.", "motivation": "To address the research gap in evaluating large language models' application potential in telecommunications operation scheduling due to complexities and lack of benchmarks.", "method": "The authors propose the Telecommunications Operation Scheduling Evaluation Benchmark (TeleEval-OS) comprising 15 datasets across 13 subtasks to assess LLM performance using zero-shot and few-shot evaluation methods.", "result": "Experimental results indicate that open-source LLMs can outperform closed-source models in specific scenarios within telecommunications operation scheduling.", "conclusion": "The proposed benchmark enables a systematic evaluation of LLMs, revealing the potential of open-source models in effective telecommunications operation scheduling.", "key_contributions": ["Introduction of the first benchmark for telecommunications operation scheduling (TeleEval-OS).", "Categorization of task complexities for LLM evaluations in this domain.", "Demonstration of the performance advantage of open-source LLMs over closed-source ones in specific scenarios."], "limitations": "", "keywords": ["large language models", "telecommunications operation scheduling", "evaluation benchmark", "TeleEval-OS", "NLP"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.11536", "pdf": "https://arxiv.org/pdf/2506.11536.pdf", "abs": "https://arxiv.org/abs/2506.11536", "title": "Do Not Immerse and Drive? Prolonged Effects of Cybersickness on Physiological Stress Markers And Cognitive Performance", "authors": ["Daniel Zielasko", "Ben Rehling", "Bernadette von Dawans", "Gregor Domes"], "categories": ["cs.HC"], "comment": null, "summary": "Extended exposure to virtual reality environments can induce motion sickness,\noften referred to as cybersickness, which may lead to physiological stress\nresponses and impaired cognitive performance. This study investigates the\naftereffects of VR-induced motion sickness with a focus on physiological stress\nmarkers and working memory performance. Using a carousel simulation to elicit\ncybersickness, we assessed subjective discomfort (SSQ, FMS), physiological\nstress (salivary cortisol, alpha-amylase, electrodermal activity, heart rate),\nand cognitive performance (n-Back task) over a 90-minute post-exposure period.\nOur findings demonstrate a significant increase in both subjective and\nphysiological stress indicators following VR exposure, accompanied by a decline\nin working memory performance. Notably, delayed symptom progression was\nobserved in a substantial proportion of participants, with some reporting peak\nsymptoms up to 90 minutes post-stimulation. Salivary cortisol levels remained\nelevated throughout the observation period, indicating prolonged stress\nrecovery. These results highlight the need for longer washout phases in XR\nresearch and raise safety concerns for professional applications involving\npost-exposure task performance.", "AI": {"tldr": "This study investigates the aftereffects of VR-induced motion sickness, focusing on physiological stress markers and working memory performance, revealing significant discomfort and cognitive decline post-exposure.", "motivation": "To explore the impact of prolonged virtual reality exposure on physiological stress and cognitive performance, particularly aftereffects of motion sickness.", "method": "Participants were subjected to a carousel simulation to induce cybersickness while assessing subjective discomfort, physiological stress indicators, and cognitive performance through a 90-minute post-exposure analysis.", "result": "Significant increases in subjective and physiological stress markers were found, along with a notable decline in working memory performance, with peak symptoms reported up to 90 minutes after VR exposure.", "conclusion": "The findings suggest a need for longer recovery periods after VR exposure to mitigate the effects on cognitive function and highlight safety risks in professional settings.", "key_contributions": ["Identified prolonged physiological stress response after VR exposure.", "Demonstrated decline in working memory performance following cybersickness.", "Emphasized the need for extended washout periods in XR research."], "limitations": "", "keywords": ["cybersickness", "virtual reality", "physiological stress", "working memory", "cognitive performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11063", "pdf": "https://arxiv.org/pdf/2506.11063.pdf", "abs": "https://arxiv.org/abs/2506.11063", "title": "Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation", "authors": ["Jiayu Yao", "Shenghua Liu", "Yiwei Wang", "Lingrui Mei", "Baolong Bi", "Yuyao Ge", "Zhecheng Li", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (RAG) systems have become essential\nin knowledge-intensive and open-domain tasks. As retrieval complexity\nincreases, ensuring the robustness of these systems is critical. However,\ncurrent RAG models are highly sensitive to the order in which evidence is\npresented, often resulting in unstable performance and biased reasoning,\nparticularly as the number of retrieved items or modality diversity grows. This\nraises a central question: How does the position of retrieved evidence affect\nmultimodal RAG performance? To answer this, we present the first comprehensive\nstudy of position bias in multimodal RAG systems. Through controlled\nexperiments across text-only, image-only, and mixed-modality tasks, we observe\na consistent U-shaped accuracy curve with respect to evidence position. To\nquantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and\ndevelop a visualization framework to trace attention allocation patterns across\ndecoder layers. Our results reveal that multimodal interactions intensify\nposition bias compared to unimodal settings, and that this bias increases\nlogarithmically with retrieval range. These findings offer both theoretical and\nempirical foundations for position-aware analysis in RAG, highlighting the need\nfor evidence reordering or debiasing strategies to build more reliable and\nequitable generation systems.", "AI": {"tldr": "This paper investigates the impact of the position of retrieved evidence on the performance of Multimodal Retrieval-Augmented Generation (RAG) systems, revealing significant position bias and introducing the Position Sensitivity Index (PSI_p).", "motivation": "The paper addresses the growing need for robust multimodal RAG systems in knowledge-intensive tasks, highlighting the issues of sensitivity to evidence order and performance instability as evidence complexity increases.", "method": "The study employs controlled experiments across text-only, image-only, and mixed-modality tasks to analyze the effects of evidence position on performance, introducing the Position Sensitivity Index to quantify bias.", "result": "The findings indicate a U-shaped accuracy curve concerning evidence position, showing that position bias is more pronounced in multimodal interactions and increases logarithmically with the retrieval range.", "conclusion": "The results emphasize the necessity for developing strategies to reorder evidence or mitigate bias in RAG systems to enhance their reliability and fairness.", "key_contributions": ["First comprehensive study of position bias in multimodal RAG systems", "Introduction of Position Sensitivity Index (PSI_p)", "Development of a visualization framework for attention patterns across decoder layers"], "limitations": "The study primarily focuses on experimental settings and may require further validation in real-world applications.", "keywords": ["Multimodal RAG", "Position Bias", "Position Sensitivity Index", "Evidence Position", "Debiasing Strategies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11610", "pdf": "https://arxiv.org/pdf/2506.11610.pdf", "abs": "https://arxiv.org/abs/2506.11610", "title": "\"If we misunderstand the client, we misspend 100 hours\": Exploring conversational AI and response types for information elicitation", "authors": ["Daniel Hove Paludan", "Julie Fredsgård", "Kasper Patrick Bährentz", "Ilhan Aslan"], "categories": ["cs.HC"], "comment": "27 pages, 8 figures", "summary": "Client-designer alignment is crucial to the success of design projects, yet\nlittle research has explored how digital technologies might influence this\nalignment. To address this gap, this paper presents a three-phase study\ninvestigating how digital systems can support requirements elicitation in\nprofessional design practice. Specifically, it examines how integrating a\nconversational agent and choice-based response formats into a digital\nelicitation tool affects early-stage client-designer collaboration. The first\nphase of the study inquired into the current practices of 10 design companies\nthrough semi-structured interviews, informing the system's design. The second\nphase evaluated the system using a 2x2 factorial design with 50 mock clients,\nquantifying the effects of conversational AI and response type on user\nexperience and perceived preparedness. In phase three, the system was presented\nto seven of the original 10 companies to gather reflections on its value,\nlimitations, and potential integration into practice. Findings show that both\nconversational AI and choice-based responses lead to lower dependability scores\non the User Experience Questionnaire, yet result in client input with greater\nclarity. We contribute design implications for integrating conversational AI\nand choice-based responses into elicitation tools to support mutual\nunderstanding in early-stage client-designer collaboration.", "AI": {"tldr": "This paper investigates how digital technologies, specifically conversational agents and choice-based response formats, can improve client-designer alignment during requirements elicitation in design projects.", "motivation": "Client-designer alignment is critical for the success of design projects, yet the impact of digital technologies on this alignment is underexplored.", "method": "A three-phase study involving semi-structured interviews, factorial design experiments with mock clients, and reflections from design companies on a developed digital elicitation tool was conducted.", "result": "The integration of conversational AI and choice-based response formats in the elicitation tool led to lower dependability scores on the User Experience Questionnaire but resulted in clearer client input.", "conclusion": "The paper outlines design implications for using conversational AI and choice-based responses in tools that support mutual understanding in early client-designer collaboration.", "key_contributions": ["Investigates the use of digital technologies in client-designer alignment", "Demonstrates the effects of AI and response formats on client input clarity", "Provides implications for integrating conversational AI into design practices"], "limitations": "The findings were based on a limited sample of design companies and mock clients, which may not fully represent real-world applications.", "keywords": ["Client-Designer Alignment", "Conversational Agent", "Choice-Based Responses", "Design Practice", "User Experience"], "importance_score": 6, "read_time_minutes": 27}}
{"id": "2506.11065", "pdf": "https://arxiv.org/pdf/2506.11065.pdf", "abs": "https://arxiv.org/abs/2506.11065", "title": "Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study", "authors": ["Alexey Tikhonov", "Sergei Shteiner", "Anna Bykova", "Ivan P. Yamshchikov"], "categories": ["cs.CL", "Primary 68T50, Secondary 68T05, 91F20", "I.2.7; I.2.6; I.5.4"], "comment": "ACL Findings 2025", "summary": "Russenorsk, a pidgin language historically used in trade interactions between\nRussian and Norwegian speakers, represents a unique linguistic phenomenon. In\nthis paper, we attempt to analyze its lexicon using modern large language\nmodels (LLMs), based on surviving literary sources. We construct a structured\ndictionary of the language, grouped by synonyms and word origins. Subsequently,\nwe use this dictionary to formulate hypotheses about the core principles of\nword formation and grammatical structure in Russenorsk and show which\nhypotheses generated by large language models correspond to the hypotheses\npreviously proposed ones in the academic literature. We also develop a\n\"reconstruction\" translation agent that generates hypothetical Russenorsk\nrenderings of contemporary Russian and Norwegian texts.", "AI": {"tldr": "This paper analyzes the lexicon of the pidgin language Russenorsk using large language models, creating a structured dictionary and formulating hypotheses about its word formation and grammatical structure.", "motivation": "To understand the linguistic structure and principles of word formation in the pidgin language Russenorsk through contemporary NLP techniques.", "method": "The authors constructed a structured dictionary of Russenorsk based on historical sources and applied LLMs to formulate and test hypotheses regarding its linguistic characteristics.", "result": "The analysis revealed correspondences between the hypotheses generated by LLMs and those proposed in existing literature, providing insights into the language's structure.", "conclusion": "The study enhances understanding of Russenorsk and demonstrates the effectiveness of LLMs in analyzing historical and lesser-known languages.", "key_contributions": ["Creation of a structured dictionary of Russenorsk", "Insights into word formation and grammatical structure using LLMs", "Development of a translation agent for Russenorsk renderings."], "limitations": "The analysis is based on surviving literary sources, which may not fully represent the language's historical usage.", "keywords": ["Russenorsk", "pidgin language", "large language models", "lexicon analysis", "translation agent"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2506.11665", "pdf": "https://arxiv.org/pdf/2506.11665.pdf", "abs": "https://arxiv.org/abs/2506.11665", "title": "Perspectives on Explanation Formats From Two Stakeholder Groups in Germany: Software Providers and Dairy Farmers", "authors": ["Mengisti Berihu Girmay", "Felix Möhrle"], "categories": ["cs.HC"], "comment": "Accepted at IJCAI 2024, Explainable AI Workshop", "summary": "This paper examines the views of software providers in the German dairy\nindustry with regard to dairy farmers' needs for explanation of digital\ndecision support systems. The study is based on mastitis detection in dairy\ncows using a hypothetical herd management system. We designed four exemplary\nexplanation formats for mastitis assessments with different types of\npresentation (textual, rule-based, herd comparison, and time series). In our\nprevious study, 14 dairy farmers in Germany had rated these formats in terms of\ncomprehensibility and the trust they would have in a system providing each\nformat. In this study, we repeat the survey with 13 software providers active\nin the German dairy industry. We ask them how well they think the formats would\nbe received by farmers. We hypothesized that there may be discrepancies between\nthe views of both groups that are worth investigating, partly to find reasons\nfor the reluctance to adopt digital systems. A comparison of the feedback from\nboth groups supports the hypothesis and calls for further investigation. The\nresults show that software providers tend to make assumptions about farmers'\npreferences that are not necessarily accurate. Our study, although not\nrepresentative due to the small sample size, highlights the potential benefits\nof a thorough user requirements analysis (farmers' needs) to improve software\nadaptation and user acceptance.", "AI": {"tldr": "The paper explores the perceptions of software providers in the German dairy industry regarding the explanation needs of dairy farmers for digital decision support systems, specifically focusing on mastitis detection in cows.", "motivation": "The study aims to understand the gap between software providers' assumptions and dairy farmers' actual needs for explanations of digital decision support systems to enhance user acceptance.", "method": "The research involved creating four explanation formats for mastitis detection and surveying both dairy farmers and software providers to assess their perceptions and preferences regarding these formats.", "result": "The comparison of feedback indicated significant discrepancies between software providers’ assumptions and the preferences of dairy farmers, suggesting the need for better user requirement analyses.", "conclusion": "The findings emphasize the importance of understanding farmer needs to improve the design and acceptance of digital systems in agriculture.", "key_contributions": ["Development of explanation formats for mastitis detection", "Comparison of views between farmers and software providers", "Highlighting the need for user requirements analysis"], "limitations": "The study's findings are not representative due to a small sample size of participants.", "keywords": ["Human-Computer Interaction", "Explainable AI", "Agricultural informatics", "User acceptance", "Dairy farming"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.11067", "pdf": "https://arxiv.org/pdf/2506.11067.pdf", "abs": "https://arxiv.org/abs/2506.11067", "title": "A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes", "authors": ["Hieu Nghiem", "Hemanth Reddy Singareddy", "Zhuqi Miao", "Jivan Lamichhane", "Abdulaziz Ahmed", "Johnson Thomas", "Dursun Delen", "William Paiva"], "categories": ["cs.CL"], "comment": null, "summary": "Objective: Develop a cost-effective, large language model (LLM)-based\npipeline for automatically extracting Review of Systems (ROS) entities from\nclinical notes. Materials and Methods: The pipeline extracts ROS sections using\nSecTag, followed by few-shot LLMs to identify ROS entity spans, their\npositive/negative status, and associated body systems. We implemented the\npipeline using open-source LLMs (Mistral, Llama, Gemma) and ChatGPT. The\nevaluation was conducted on 36 general medicine notes containing 341 annotated\nROS entities. Results: When integrating ChatGPT, the pipeline achieved the\nlowest error rates in detecting ROS entity spans and their corresponding\nstatuses/systems (28.2% and 14.5%, respectively). Open-source LLMs enable\nlocal, cost-efficient execution of the pipeline while delivering promising\nperformance with similarly low error rates (span: 30.5-36.7%; status/system:\n24.3-27.3%). Discussion and Conclusion: Our pipeline offers a scalable and\nlocally deployable solution to reduce ROS documentation burden. Open-source\nLLMs present a viable alternative to commercial models in resource-limited\nhealthcare environments.", "AI": {"tldr": "Development of an LLM-based pipeline to extract Review of Systems entities from clinical notes to reduce documentation burden in healthcare.", "motivation": "To create a cost-effective solution for extracting Review of Systems entities from clinical notes, addressing the documentation burden in healthcare.", "method": "The pipeline uses SecTag to extract ROS sections, followed by few-shot LLMs for identifying entity spans, statuses, and associated body systems. Open-source LLMs (Mistral, Llama, Gemma) and ChatGPT were utilized and evaluated on 36 general medicine notes with 341 annotated entities.", "result": "The integration of ChatGPT led to the lowest error rates in detecting ROS entities and their statuses, with open-source LLMs achieving promising performance.", "conclusion": "The proposed pipeline is scalable and can be locally deployed in resource-limited environments, providing a viable alternative to commercial models.", "key_contributions": ["Cost-effective LLM-based pipeline for ROS extraction", "Demonstrated high performance with open-source LLMs", "Reduced documentation burden in healthcare settings"], "limitations": "Limited to evaluation on 36 general medicine notes; results may vary with different clinical contexts.", "keywords": ["Human-Computer Interaction", "Language Models", "Healthcare", "Clinical NLP", "Cost-effective Solutions"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.11718", "pdf": "https://arxiv.org/pdf/2506.11718.pdf", "abs": "https://arxiv.org/abs/2506.11718", "title": "Interaction, Process, Infrastructure: A Unified Architecture for Human-Agent Collaboration", "authors": ["Yun Wang", "Yan Lu"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As AI tools proliferate across domains, from chatbots and copilots to\nemerging agents, they increasingly support professional knowledge work. Yet\ndespite their growing capabilities, these systems remain fragmented: they\nassist with isolated tasks but lack the architectural scaffolding for\nsustained, adaptive collaboration. We propose a layered framework for\nhuman-agent systems that integrates three interdependent dimensions:\ninteraction, process, and infrastructure. Crucially, our architecture elevates\nprocess to a primary focus by making it explicit, inspectable, and adaptable,\nenabling humans and agents to align with evolving goals and coordinate over\ntime. This model clarifies limitations of current tools, unifies emerging\nsystem design approaches, and reveals new opportunities for researchers and AI\nsystem builders. By grounding intelligent behavior in structured collaboration,\nwe reimagine human-agent collaboration not as task-specific augmentation, but\nas a form of coherent and aligned system for real-world work.", "AI": {"tldr": "The paper proposes a layered framework for human-agent systems that focuses on creating a sustainable and adaptive collaboration between humans and AI tools.", "motivation": "To address the fragmentation of current AI tools which assist with isolated tasks but lack the ability for sustained collaboration in professional knowledge work.", "method": "The authors introduce a layered framework integrating interaction, process, and infrastructure dimensions, emphasizing process as a primary focus.", "result": "The model allows for explicit, inspectable, and adaptable processes enabling alignment of evolving goals between humans and agents.", "conclusion": "This framework clarifies limitations of current AI tools and highlights opportunities for AI system design with an emphasis on structured collaboration.", "key_contributions": ["Introduction of a layered framework for human-agent collaboration", "Emphasis on process as a pivotal element", "Clarification of current AI tools' limitations and unified design approaches."], "limitations": "", "keywords": ["human-agent systems", "collaboration", "AI tools", "interaction", "process"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.11068", "pdf": "https://arxiv.org/pdf/2506.11068.pdf", "abs": "https://arxiv.org/abs/2506.11068", "title": "Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models", "authors": ["Bumjin Park", "Jinsil Lee", "Jaesik Choi"], "categories": ["cs.CL"], "comment": "20 pages including references and appendix; To appear in ACL 2025\n  main conference", "summary": "Large language models (LLMs) are increasingly engaging in moral and ethical\nreasoning, where criteria for judgment are often unclear, even for humans.\nWhile LLM alignment studies cover many areas, one important yet underexplored\narea is how LLMs make judgments about obligations. This work reveals a strong\ntendency in LLMs to judge non-obligatory contexts as obligations when prompts\nare augmented with modal expressions such as must or ought to. We introduce\nthis phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge\nover 90\\% of commonsense scenarios as obligations when modal expressions are\npresent. This tendency is consist across various LLM families, question types,\nand answer formats. To mitigate DKB, we propose a judgment strategy that\nintegrates few-shot examples with reasoning prompts. This study sheds light on\nhow modal expressions, as a form of linguistic framing, influence the normative\ndecisions of LLMs and underscores the importance of addressing such biases to\nensure judgment alignment.", "AI": {"tldr": "This paper explores how large language models (LLMs) misjudge non-obligatory contexts as obligations due to the influence of modal expressions, introducing the concept of Deontological Keyword Bias (DKB) and proposing a strategy to mitigate it.", "motivation": "To understand how large language models (LLMs) make judgments about obligations and to address the underexplored area of moral and ethical reasoning in AI.", "method": "Empirical analysis of LLM responses to various prompts augmented with modal expressions; introduction of a judgment strategy using few-shot examples and reasoning prompts.", "result": "Over 90% of commonsense scenarios were judged as obligations when modal expressions like 'must' or 'ought to' were present, consistent across different models and question formats.", "conclusion": "The study highlights the substantial impact of modal expressions on LLMs' judgment processes, emphasizing the need to mitigate biases for better alignment in normative decision-making.", "key_contributions": ["Introduction of Deontological Keyword Bias (DKB) in LLMs.", "Empirical evidence showing LLMs' tendency to misjudge obligations based on prompt wording.", "Development of a judgment strategy to mitigate DKB."], "limitations": "", "keywords": ["large language models", "moral reasoning", "deontological keyword bias", "AI alignment", "linguistic framing"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.11781", "pdf": "https://arxiv.org/pdf/2506.11781.pdf", "abs": "https://arxiv.org/abs/2506.11781", "title": "GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant", "authors": ["Gaspard Merten", "Gilles Dejaegere", "Mahmoud Sakr"], "categories": ["cs.HC", "cs.SE"], "comment": "Submitted to ACM SIGSPATIAL 2025", "summary": "Geospatial data analysis plays a crucial role in tackling intricate societal\nchallenges such as urban planning and climate modeling. However, employing\ntools like GeoPandas, a prominent Python library for geospatial data\nmanipulation, necessitates expertise in complex domain-specific syntax and\nworkflows. GeoPandas-AI addresses this gap by integrating LLMs directly into\nthe GeoPandas workflow, transforming the GeoDataFrame class into an\nintelligent, stateful class for both data analysis and geospatial code\ndevelopment. This paper formalizes the design of such a smart class and\nprovides an open-source implementation of GeoPandas-AI in PyPI package manager.\nThrough its innovative combination of conversational interfaces and stateful\nexploitation of LLMs for code generation and data analysis, GeoPandas-AI\nintroduces a new paradigm for code-copilots and instantiates it for geospatial\ndevelopment.", "AI": {"tldr": "GeoPandas-AI integrates LLMs into GeoPandas to simplify geospatial data analysis.", "motivation": "To address the expertise gap in using complex geospatial data manipulation tools like GeoPandas.", "method": "The paper presents a design for a smart GeoDataFrame class that incorporates LLMs for intelligent data analysis and code generation.", "result": "An open-source implementation of GeoPandas-AI is provided via the PyPI package manager, showing effective integration of conversational interfaces and stateful LLMs.", "conclusion": "GeoPandas-AI represents a new paradigm in geospatial development, enhancing code-copiloting capabilities.", "key_contributions": ["Integration of LLMs in GeoPandas for geospatial analysis", "Development of a smart GeoDataFrame class", "Open-source implementation available on PyPI"], "limitations": "", "keywords": ["geospatial data analysis", "GeoPandas", "LLM integration"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.11070", "pdf": "https://arxiv.org/pdf/2506.11070.pdf", "abs": "https://arxiv.org/abs/2506.11070", "title": "Targeted control of fast prototyping through domain-specific interface", "authors": ["Yu-Zhe Shi", "Mingchen Liu", "Hanlu Ma", "Qiao Xu", "Huamin Qu", "Kun He", "Lecheng Ruan", "Qining Wang"], "categories": ["cs.CL"], "comment": "In International Conference on Machine Learning (ICML'25)", "summary": "Industrial designers have long sought a natural and intuitive way to achieve\nthe targeted control of prototype models -- using simple natural language\ninstructions to configure and adjust the models seamlessly according to their\nintentions, without relying on complex modeling commands. While Large Language\nModels have shown promise in this area, their potential for controlling\nprototype models through language remains partially underutilized. This\nlimitation stems from gaps between designers' languages and modeling languages,\nincluding mismatch in abstraction levels, fluctuation in semantic precision,\nand divergence in lexical scopes. To bridge these gaps, we propose an interface\narchitecture that serves as a medium between the two languages. Grounded in\ndesign principles derived from a systematic investigation of fast prototyping\npractices, we devise the interface's operational mechanism and develop an\nalgorithm for its automated domain specification. Both machine-based\nevaluations and human studies on fast prototyping across various product design\ndomains demonstrate the interface's potential to function as an auxiliary\nmodule for Large Language Models, enabling precise and effective targeted\ncontrol of prototype models.", "AI": {"tldr": "The paper presents an interface architecture that allows industrial designers to control prototype models using natural language, mitigating the current gaps between designers' and modeling languages.", "motivation": "To achieve intuitive control of prototype models using natural language instructions, addressing limitations in current modeling languages and practices.", "method": "The authors propose an interface architecture that serves as a bridge between natural language and modeling languages, and develop an algorithm for automated domain specification based on design principles from fast prototyping practices.", "result": "Machine-based evaluations and human studies indicate that the proposed interface effectively allows targeted control of prototype models through Large Language Models, enhancing their usability in design contexts.", "conclusion": "The interface can act as an auxiliary module for LLMs, facilitating precise adjustments to prototypes as intended by designers.", "key_contributions": ["Proposed interface architecture for bridging language gaps in design and modeling.", "Developed an algorithm for automated domain specification.", "Demonstrated effective integration with Large Language Models through evaluations."], "limitations": "The study may need further validation across more design domains and contexts.", "keywords": ["Human-Computer Interaction", "Large Language Models", "Prototype Control", "Design Interfaces", "Fast Prototyping"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.11788", "pdf": "https://arxiv.org/pdf/2506.11788.pdf", "abs": "https://arxiv.org/abs/2506.11788", "title": "Digital Labor: Challenges, Ethical Insights, and Implications", "authors": ["ATM Mizanur Rahman", "Sharifa Sultana"], "categories": ["cs.HC"], "comment": null, "summary": "Digital workers on crowdsourcing platforms (e.g., Amazon Mechanical Turk,\nAppen, Clickworker, Prolific) play a crucial role in training and improving AI\nsystems, yet they often face low pay, unfair conditions, and a lack of\nrecognition for their contributions. To map these issues in the existing\nliterature of computer science, AI, and related scholarship, we selected over\n300 research papers on digital labor published between 2015 and 2024, narrowing\nthem down to 143 on digital gig-labor for a detailed analysis. This analysis\nprovides a broad overview of the key challenges, concerns, and trends in the\nfield. Our synthesis reveals how the persistent patterns of representation and\nvoices of gig workers in digital labor are structured and governed. We offer\nnew insights for researchers, platform designers, and policymakers, helping\nthem better understand the experiences of digital workers and pointing to key\nareas where interventions and future investigations are promptly needed. By\nmapping the findings from the past ten years' growth of the domain and possible\nimplications, this paper contributes to a more coherent and critical\nunderstanding of digital labor in contemporary and future AI ecosystems.", "AI": {"tldr": "This paper analyzes over 300 research papers on digital labor, focusing on the challenges and trends affecting digital workers on crowdsourcing platforms.", "motivation": "To address the low pay, unfair conditions, and lack of recognition for digital workers on crowdsourcing platforms and to synthesize findings in digital labor research.", "method": "A systematic literature review was conducted, selecting and analyzing 143 relevant papers on digital gig-labor from a database of over 300 papers published from 2015 to 2024.", "result": "The analysis reveals key challenges and trends in the experiences of digital workers, highlighting persistent issues in representation and governance.", "conclusion": "The paper offers critical insights for researchers, platform designers, and policymakers to improve the conditions and recognition of digital workers and identifies areas for future research.", "key_contributions": ["Synthesis of literature on digital labor issues", "Identification of key challenges faced by gig workers", "Insights for designing better platforms and policies"], "limitations": "", "keywords": ["digital labor", "crowdsourcing", "gig economy", "AI", "platform design"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.11073", "pdf": "https://arxiv.org/pdf/2506.11073.pdf", "abs": "https://arxiv.org/abs/2506.11073", "title": "CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention", "authors": ["Zekai Ye", "Qiming Li", "Xiaocheng Feng", "Libo Qin", "Yichong Huang", "Baohang Li", "Kui Jiang", "Yang Xiang", "Zhirui Zhang", "Yunfei Lu", "Duyu Tang", "Dandan Tu", "Bing Qin"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL2025 Main", "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal\nabilities but remain prone to multilingual object hallucination, with a higher\nlikelihood of generating responses inconsistent with the visual input when\nutilizing queries in non-English languages compared to English. Most existing\napproaches to address these rely on pretraining or fine-tuning, which are\nresource-intensive. In this paper, inspired by observing the disparities in\ncross-modal attention patterns across languages, we propose Cross-Lingual\nAttention Intervention for Mitigating multilingual object hallucination (CLAIM)\nin LVLMs, a novel near training-free method by aligning attention patterns.\nCLAIM first identifies language-specific cross-modal attention heads, then\nestimates language shift vectors from English to the target language, and\nfinally intervenes in the attention outputs during inference to facilitate\ncross-lingual visual perception capability alignment. Extensive experiments\ndemonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in\nSpanish) on the POPE and 21.75% on the hallucination subsets of the MME\nbenchmark across various languages. Further analysis reveals that multilingual\nattention divergence is most prominent in intermediate layers, highlighting\ntheir critical role in multilingual scenarios.", "AI": {"tldr": "The paper introduces CLAIM, a new method to reduce multilingual object hallucination in LVLMs by aligning cross-modal attention patterns, achieving significant performance improvements with minimal resource usage.", "motivation": "To address the issue of multilingual object hallucination in Large Vision-Language Models, especially with non-English queries, without the resource demands of pretraining or fine-tuning.", "method": "CLAIM identifies language-specific cross-modal attention heads and estimates language shift vectors from English to the target language, intervening during inference to align attention outputs.", "result": "CLAIM achieved an average improvement of 13.56% on the POPE and 21.75% on the hallucination subsets of the MME benchmark across various languages, with peak improvements of up to 30% in Spanish.", "conclusion": "The study underscores the importance of multilingual attention divergence in intermediate layers and presents CLAIM as an effective near training-free solution for improved cross-lingual visual perception.", "key_contributions": ["Introduces CLAIM for addressing multilingual object hallucination in LVLMs.", "Demonstrates performance improvements without extensive resource expenditure.", "Highlights the critical role of intermediate layers in multilingual attention divergence."], "limitations": "The approach may still be influenced by the inherent limitations of the existing LVLMs and the extent of language diversity.", "keywords": ["Large Vision-Language Models", "multilingual object hallucination", "cross-modal attention", "CLAIM", "attention intervention"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2506.11789", "pdf": "https://arxiv.org/pdf/2506.11789.pdf", "abs": "https://arxiv.org/abs/2506.11789", "title": "Conversational AI as a Catalyst for Informal Learning: An Empirical Large-Scale Study on LLM Use in Everyday Learning", "authors": ["Nađa Terzimehić", "Babette Bühler", "Enkelejda Kasneci"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models have not only captivated the public imagination but\nhave also sparked a profound rethinking of how we learn. In the third year\nfollowing the breakthrough launch of ChatGPT, everyday informal learning has\nbeen transformed as diverse user groups explore these novel tools. Who is\nembracing LLMs for self-directed learning, and who remains hesitant? What are\ntheir reasons for adoption or avoidance? What learning patterns emerge with\nthis novel technological landscape? We present an in-depth analysis from a\nlarge-scale survey of 776 participants, showcasing that 88% of our respondents\nalready incorporate LLMs into their everyday learning routines for a wide\nvariety of (learning) tasks. Young adults are at the forefront of adopting\nLLMs, primarily to enhance their learning experiences independently of time and\nspace. Four types of learners emerge across learning contexts, depending on the\ntasks they perform with LLMs and the devices they use to access them.\nInterestingly, our respondents exhibit paradoxical behaviours regarding their\ntrust in LLMs' accuracy and privacy protection measures. Our implications\nemphasize the importance of including different media types for learning,\nenabling collaborative learning, providing sources and meeting the needs of\ndifferent types of learners and learning by design.", "AI": {"tldr": "The study analyzes the adoption of large language models (LLMs) in self-directed learning, revealing that a majority of users, particularly young adults, integrate LLMs into their learning routines, with emerging patterns and trust issues in their use.", "motivation": "To understand the adoption and impact of large language models on informal learning practices.", "method": "A large-scale survey of 776 participants examining the use of LLMs in self-directed learning and identifying learner types based on their usage patterns.", "result": "88% of respondents utilize LLMs for various learning tasks, with young adults leading this trend, revealing four distinct types of learners and paradoxical trust behaviors regarding LLMs' accuracy and privacy.", "conclusion": "The findings highlight the need for adaptive learning designs that cater to diverse learner types and incorporate collaborative learning and media variety.", "key_contributions": ["Identified four types of learners using LLMs for self-directed learning.", "Provided insights into the trust issues users have with LLMs' accuracy and privacy.", "Emphasized the importance of inclusive learning design that adapts to different user needs."], "limitations": "", "keywords": ["large language models", "self-directed learning", "user adoption", "learning patterns", "trust behaviors"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.11077", "pdf": "https://arxiv.org/pdf/2506.11077.pdf", "abs": "https://arxiv.org/abs/2506.11077", "title": "CyclicReflex: Improving Large Reasoning Models via Cyclical Reflection Token Scheduling", "authors": ["Chongyu Fan", "Yihua Zhang", "Jinghan Jia", "Alfred Hero", "Sijia Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, harness\ntest-time scaling to perform multi-step reasoning for complex problem-solving.\nThis reasoning process, executed before producing final answers, is often\nguided by special juncture tokens or textual segments that prompt\nself-evaluative reflection. We refer to these transition markers and reflective\ncues as \"reflection tokens\" (e.g., \"wait\", \"but\", \"alternatively\"). In this\nwork, we treat reflection tokens as a \"resource\" and introduce the problem of\nresource allocation, aimed at improving the test-time compute performance of\nLRMs by adaptively regulating the frequency and placement of reflection tokens.\nThrough empirical analysis, we show that both excessive and insufficient use of\nreflection tokens, referred to as over-reflection and under-reflection, can\ndegrade model performance. To better understand and manage this trade-off, we\ndraw an analogy between reflection token usage and learning rate scheduling in\noptimization. Building on this insight, we propose cyclical reflection token\nscheduling (termed CyclicReflex), a decoding strategy that dynamically\nmodulates reflection token logits using a position-dependent triangular\nwaveform. Experiments on MATH500, AIME2024/2025, and AMC2023 demonstrate that\nCyclicReflex consistently improves performance across model sizes (1.5B-8B),\noutperforming standard decoding and more recent approaches such as TIP (thought\nswitching penalty) and S1. Codes are available at\nhttps://github.com/OPTML-Group/CyclicReflex.", "AI": {"tldr": "Introduction of CyclicReflex, a decoding strategy that optimally modulates reflection tokens to improve performance in large reasoning models during complex problem-solving tasks.", "motivation": "To enhance test-time compute performance of large reasoning models (LRMs) by managing the use of reflection tokens, which are crucial for multi-step reasoning and self-evaluation.", "method": "The paper proposes cyclical reflection token scheduling (CyclicReflex) that dynamically adjusts the frequency and position of reflection tokens using a triangular waveform to prevent over-reflection and under-reflection.", "result": "CyclicReflex consistently outperforms standard decoding methods and recent approaches like TIP and S1 on benchmarks MATH500, AIME2024/2025, and AMC2023, leading to improved model performance across varying model sizes.", "conclusion": "CyclicReflex allows for better allocation of reflection tokens, resulting in enhanced reasoning capabilities of large models. The methodology can be beneficial for future research in improving reasoning models.", "key_contributions": ["Introduction of reflection tokens as a resource in reasoning models", "Proposed CyclicReflex for dynamic modulation of reflection tokens", "Empirical evidence demonstrating performance improvements across multiple benchmarks"], "limitations": "", "keywords": ["Large reasoning models", "Reflection tokens", "Cyclical scheduling", "Machine learning", "Performance improvement"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.11890", "pdf": "https://arxiv.org/pdf/2506.11890.pdf", "abs": "https://arxiv.org/abs/2506.11890", "title": "Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars in Virtual Reality Teacher Training", "authors": ["Judson Leroy Dean Haynes IV"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Virtual Reality simulators offer a powerful tool for teacher training, yet\nthe integration of AI-powered student avatars presents a critical challenge:\ndetermining the optimal level of avatar realism for effective pedagogy. This\nliterature review examines the evolution of avatar realism in VR teacher\ntraining, synthesizes its theoretical implications, and proposes a new\npedagogical framework to guide future design. Through a systematic review, this\npaper traces the progression from human-controlled avatars to generative AI\nprototypes. Applying learning theories like Cognitive Load Theory, we argue\nthat hyper-realism is not always optimal, as high-fidelity avatars can impose\nexcessive extraneous cognitive load on novices, a stance supported by recent\nempirical findings. A significant gap exists between the technological drive\nfor photorealism and the pedagogical need for scaffolded learning. To address\nthis gap, we propose Graduated Realism, a framework advocating for starting\ntrainees with lower-fidelity avatars and progressively increasing behavioral\ncomplexity as skills develop. To make this computationally feasible, we outline\na novel single-call architecture, Crazy Slots, which uses a probabilistic\nengine and a Retrieval-Augmented Generation database to generate authentic,\nreal-time responses without the latency and cost of multi-step reasoning\nmodels. This review provides evidence-based principles for designing the next\ngeneration of AI simulators, arguing that a pedagogically grounded approach to\nrealism is essential for creating scalable and effective teacher education\ntools.", "AI": {"tldr": "This literature review explores the integration of AI avatars in VR teacher training, proposing a framework called Graduated Realism to optimize avatar realism for effective pedagogy.", "motivation": "To determine the optimal level of avatar realism in AI-powered VR simulators for teacher training and address the gap between technological photorealism and pedagogical needs.", "method": "Systematic review of literature examining the evolution of avatar realism and application of learning theories like Cognitive Load Theory.", "result": "High-fidelity avatars can impose excessive cognitive load on novices; evidence supports the Graduated Realism framework which starts with lower-fidelity avatars and increases complexity as skills develop.", "conclusion": "A pedagogically grounded approach to realism in VR teacher training is essential for scalability and effectiveness in simulation design.", "key_contributions": ["Proposes the Graduated Realism framework for VR training.", "Introduces a novel architecture, Crazy Slots, for efficient avatar response generation.", "Synthesize evidence-based principles for AI simulator design."], "limitations": "", "keywords": ["Virtual Reality", "AI Avatars", "Teacher Training", "Pedagogical Framework", "Cognitive Load"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.11078", "pdf": "https://arxiv.org/pdf/2506.11078.pdf", "abs": "https://arxiv.org/abs/2506.11078", "title": "RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs", "authors": ["Yuzhou Yang", "Yangming Zhou", "Zhiying Zhu", "Zhenxing Qian", "Xinpeng Zhang", "Sheng Li"], "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of deceptive content online necessitates robust Fake News\nDetection (FND) systems. While evidence-based approaches leverage external\nknowledge to verify claims, existing methods face critical limitations: noisy\nevidence selection, generalization bottlenecks, and unclear decision-making\nprocesses. Recent efforts to harness Large Language Models (LLMs) for FND\nintroduce new challenges, including hallucinated rationales and conclusion\nbias. To address these issues, we propose \\textbf{RoE-FND}\n(\\textbf{\\underline{R}}eason \\textbf{\\underline{o}}n\n\\textbf{\\underline{E}}xperiences FND), a framework that reframes evidence-based\nFND as a logical deduction task by synergizing LLMs with experiential learning.\nRoE-FND encompasses two stages: (1) \\textit{self-reflective knowledge\nbuilding}, where a knowledge base is curated by analyzing past reasoning\nerrors, namely the exploration stage, and (2) \\textit{dynamic criterion\nretrieval}, which synthesizes task-specific reasoning guidelines from\nhistorical cases as experiences during deployment. It further cross-checks\nrationales against internal experience through a devised dual-channel\nprocedure. Key contributions include: a case-based reasoning framework for FND\nthat addresses multiple existing challenges, a training-free approach enabling\nadaptation to evolving situations, and empirical validation of the framework's\nsuperior generalization and effectiveness over state-of-the-art methods across\nthree datasets.", "AI": {"tldr": "RoE-FND is a framework for Fake News Detection that reinterprets it as a logical deduction task by combining LLMs with experiential learning.", "motivation": "The rise of deceptive online content requires effective Fake News Detection (FND) systems, but current methods struggle with evidence selection, generalization, and decision-making clarity.", "method": "RoE-FND includes two stages: self-reflective knowledge building for curating a knowledge base from past errors and dynamic criterion retrieval for synthesizing reasoning guidelines from historical cases during deployment.", "result": "The framework exhibits superior generalization and effectiveness compared to state-of-the-art methods across three datasets.", "conclusion": "RoE-FND offers a case-based reasoning approach for FND that adapts to new situations and provides empirical validation for its methodologies.", "key_contributions": ["A case-based reasoning framework that addresses existing challenges in FND", "A training-free approach for adapting to evolving contexts", "Empirical validation showing superior performance over existing methods"], "limitations": "", "keywords": ["Fake News Detection", "Large Language Models", "Experiential Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11080", "pdf": "https://arxiv.org/pdf/2506.11080.pdf", "abs": "https://arxiv.org/abs/2506.11080", "title": "MANBench: Is Your Multimodal Model Smarter than Human?", "authors": ["Han Zhou", "Qitong Xu", "Yiheng Dong", "Xin Yang"], "categories": ["cs.CL"], "comment": "Multimodal Benchmark, Project Url: https://github.com/micdz/MANBench,\n  ACL2025 Findings", "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited\ndiscussions regarding their potential to surpass human performance in\nmultimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms\nBenchmark), a bilingual benchmark (English and Chinese) comprising 1,314\nquestions across nine tasks, spanning knowledge-based and non-knowledge-based\ndomains. MANBench emphasizes intuitive reasoning, seamless cross-modal\nintegration, and real-world complexity, providing a rigorous evaluation\nframework.\n  Through extensive human experiments involving diverse participants, we\ncompared human performance against state-of-the-art MLLMs. The results indicate\nthat while MLLMs excel in tasks like Knowledge and Text-Image Understanding,\nthey struggle with deeper cross-modal reasoning tasks such as Transmorphic\nUnderstanding, Image Consistency, and Multi-image Understanding. Moreover, both\nhumans and MLLMs face challenges in highly complex tasks like Puzzles and\nSpatial Imagination.\n  MANBench highlights the strengths and limitations of MLLMs, revealing that\neven advanced models fall short of achieving human-level performance across\nmany domains. We hope MANBench will inspire efforts to bridge the gap between\nMLLMs and human multimodal capabilities. The code and dataset are available at\nhttps://github.com/micdz/MANBench.", "AI": {"tldr": "MANBench introduces a bilingual benchmark for evaluating Multimodal Large Language Models (MLLMs) across various tasks, revealing their strengths and limitations compared to human performance.", "motivation": "To evaluate the potential of MLLMs against human performance in multimodal tasks using a comprehensive benchmark that emphasizes intuitive reasoning and real-world complexities.", "method": "A bilingual benchmark, MANBench, was created with 1,314 questions across nine tasks, involving extensive human experiments to compare human and MLLM performance on knowledge-based and non-knowledge-based domains.", "result": "MLLMs performed well in tasks like Knowledge and Text-Image Understanding but struggled with complex cross-modal reasoning tasks, showing gaps compared to human abilities.", "conclusion": "MANBench highlights that while MLLMs excel in certain areas, they do not yet reach human-level performance in many complex tasks, encouraging further research to improve MLLM capabilities.", "key_contributions": ["Introduction of MANBench for evaluating MLLMs", "Empirical findings on the strengths and weaknesses of MLLMs", "Availability of the code and dataset for community use"], "limitations": "N/A", "keywords": ["Multimodal Learning", "Large Language Models", "Benchmarking", "Human-Computer Interaction", "Cross-modal reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.11081", "pdf": "https://arxiv.org/pdf/2506.11081.pdf", "abs": "https://arxiv.org/abs/2506.11081", "title": "SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs", "authors": ["Aditi", "Hyunwoo Park", "Sicheol Sung", "Yo-Sub Han", "Sang-Ki Ko"], "categories": ["cs.CL"], "comment": null, "summary": "Grammar-based test case generation has proven effective for competitive\nprogramming problems, but generating valid and general grammars from natural\nlanguage specifications remains a key challenge, especially under limited\nsupervision. Context-Free Grammars with Counters (CCFGs) have recently been\nintroduced as a formalism to represent such specifications with logical\nconstraints by storing and reusing counter values during derivation. In this\nwork, we explore the use of open-source large language models (LLMs) to induce\nCCFGs from specifications using a small number of labeled examples and\nverifiable reward-guided reinforcement learning. Our approach first fine-tunes\nan open-source LLM to perform specification-to-grammar translation, and further\napplies Group Relative Policy Optimization (GRPO) to enhance grammar validity\nand generality. We also examine the effectiveness of iterative feedback for\nopen and closed-source LLMs in correcting syntactic and semantic errors in\ngenerated grammars.\n  Experimental results show that our approach SAGE achieves stronger\ngeneralization and outperforms 17 open and closed-source LLMs in both grammar\nquality and test effectiveness, improving over the state-of-the-art by 15.92%p\nin grammar validity and 12.34%p in test effectiveness. We provide our\nimplementation and dataset at the following anonymous\nrepository:https://anonymous.4open.science/r/SAGE-5714", "AI": {"tldr": "This paper presents an approach to generate Context-Free Grammars with Counters (CCFGs) from natural language specifications using large language models (LLMs) and reinforcement learning.", "motivation": "Generating valid and general grammars from natural language specifications is challenging, particularly with limited supervision.", "method": "The proposed method fine-tunes an open-source LLM for specification-to-grammar translation and applies Group Relative Policy Optimization (GRPO) to enhance grammar validity and generality.", "result": "SAGE, the proposed approach, outperforms 17 other LLMs in grammar quality and test effectiveness, achieving improvements of 15.92% in grammar validity and 12.34% in test effectiveness over the state-of-the-art.", "conclusion": "The experimental results demonstrate that the SAGE approach effectively induces CCFGs with improved performance metrics.", "key_contributions": ["Induction of CCFGs from natural language specifications using LLMs", "Use of reinforcement learning for grammar validation", "Improved generalization and performance compared to existing methods"], "limitations": "", "keywords": ["Grammar-based test case generation", "Context-Free Grammars", "Large Language Models", "Reinforcement Learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.11082", "pdf": "https://arxiv.org/pdf/2506.11082.pdf", "abs": "https://arxiv.org/abs/2506.11082", "title": "PRISM: A Transformer-based Language Model of Structured Clinical Event Data", "authors": ["Lionel Levine", "John Santerre", "Alex S. Young", "T. Barry Levine", "Francis Campion", "Majid Sarrafzadeh"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 Figures, 1 Table", "summary": "We introduce PRISM (Predictive Reasoning in Sequential Medicine), a\ntransformer-based architecture designed to model the sequential progression of\nclinical decision-making processes. Unlike traditional approaches that rely on\nisolated diagnostic classification, PRISM frames clinical trajectories as\ntokenized sequences of events - including diagnostic tests, laboratory results,\nand diagnoses - and learns to predict the most probable next steps in the\npatient diagnostic journey. Leveraging a large custom clinical vocabulary and\nan autoregressive training objective, PRISM demonstrates the ability to capture\ncomplex dependencies across longitudinal patient timelines. Experimental\nresults show substantial improvements over random baselines in next-token\nprediction tasks, with generated sequences reflecting realistic diagnostic\npathways, laboratory result progressions, and clinician ordering behaviors.\nThese findings highlight the feasibility of applying generative language\nmodeling techniques to structured medical event data, enabling applications in\nclinical decision support, simulation, and education. PRISM establishes a\nfoundation for future advancements in sequence-based healthcare modeling,\nbridging the gap between machine learning architectures and real-world\ndiagnostic reasoning.", "AI": {"tldr": "PRISM is a transformer-based architecture for modeling clinical decision-making by predicting sequences of medical events.", "motivation": "To improve clinical decision-making processes by predicting the next steps in patient diagnostics using a sequence-based approach.", "method": "PRISM uses a transformer architecture to model tokenized clinical event sequences and applies autoregressive training to forecast next diagnostic actions.", "result": "PRISM shows significant improvements in next-token prediction tasks, accurately reflecting real-world diagnostic pathways and clinician behaviors.", "conclusion": "The framework paves the way for integrating generative modeling techniques into healthcare, enhancing clinical decision support and education.", "key_contributions": ["Introduction of PRISM architecture for sequential clinical reasoning", "Application of autoregressive training on medical event data", "Demonstrated improvements in modeling clinical decision pathways."], "limitations": "The model may require extensive customization for different clinical contexts and interactions with real-world data.", "keywords": ["clinical decision-making", "transformer architecture", "predictive modeling", "healthcare applications", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.11092", "pdf": "https://arxiv.org/pdf/2506.11092.pdf", "abs": "https://arxiv.org/abs/2506.11092", "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation", "authors": ["Jubin Abhishek Soni", "Amit Anand", "Rajesh Kumar Pandey", "Aniket Abhishek Soni"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "6 pages, 5 figures, 3 tables. This manuscript has been submitted to\n  IEEE conference. Researchers are welcome to read and build upon this work;\n  please cite it appropriately. For questions or clarifications, feel free to\n  contact me", "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.", "AI": {"tldr": "Dynamic Context Tuning (DCT) enhances Retrieval-Augmented Generation (RAG) to support multi-turn dialogue and evolving tool environments, showing improved accuracy and reduced hallucinations in AI applications.", "motivation": "Existing RAG systems are limited to static interactions, making them unsuitable for dynamic domains like healthcare and smart homes where user intent and tool availability change over time.", "method": "DCT integrates an attention-based context cache for tracking relevant past information, employs LoRA-based retrieval for dynamic tool selection, and uses efficient context compression to stay within LLM limits.", "result": "DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while achieving performance that matches GPT-4 at a lower cost.", "conclusion": "DCT generalizes to unseen tools, enabling scalable and adaptable AI assistants across various dynamic environments.", "key_contributions": ["Introduction of Dynamic Context Tuning (DCT) for evolving contexts in RAG.", "Demonstration of improved plan accuracy and reduced hallucinations.", "Capability to generalize to previously unseen tools."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Dynamic Context Tuning", "Machine Learning", "Human-Computer Interaction", "AI in Healthcare"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.11083", "pdf": "https://arxiv.org/pdf/2506.11083.pdf", "abs": "https://arxiv.org/abs/2506.11083", "title": "RedDebate: Safer Responses through Multi-Agent Red Teaming Debates", "authors": ["Ali Asad", "Stephen Obadinma", "Radin Shayanfar", "Xiaodan Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "We propose RedDebate, a novel multi-agent debate framework that leverages\nadversarial argumentation among Large Language Models (LLMs) to proactively\nidentify and mitigate their own unsafe behaviours. Existing AI safety methods\noften depend heavily on costly human evaluations or isolated single-model\nassessment, both subject to scalability constraints and oversight risks.\nRedDebate instead embraces collaborative disagreement, enabling multiple LLMs\nto critically examine one another's reasoning, and systematically uncovering\nunsafe blind spots through automated red-teaming, and iteratively improve their\nresponses. We further integrate distinct types of long-term memory that retain\nlearned safety insights from debate interactions. Evaluating on established\nsafety benchmarks such as HarmBench, we demonstrate the proposed method's\neffectiveness. Debate alone can reduce unsafe behaviours by 17.7%, and when\ncombined with long-term memory modules, achieves reductions exceeding 23.5%. To\nour knowledge, RedDebate constitutes the first fully automated framework that\ncombines multi-agent debates with red-teaming to progressively enhance AI\nsafety without direct human intervention.(Github Repository:\nhttps://github.com/aliasad059/RedDebate)", "AI": {"tldr": "RedDebate is a multi-agent debate framework using LLMs to enhance AI safety by identifying unsafe behaviors through adversarial argumentation.", "motivation": "To address the limitations of existing AI safety methods that rely on human evaluations and single-model assessments, which are not scalable and are prone to oversight.", "method": "The proposed framework uses collaborative disagreement among multiple LLMs to critically examine each other's reasoning, uncover blind spots, and improve their responses. Long-term memory is integrated to retain safety insights from these debates.", "result": "RedDebate effectively reduces unsafe behaviors in AI models, achieving a 17.7% reduction through debate alone, and exceeding 23.5% when combined with long-term memory modules on safety benchmarks like HarmBench.", "conclusion": "RedDebate is the first fully automated framework combining multi-agent debates and red-teaming to enhance AI safety without human intervention.", "key_contributions": ["Novel multi-agent debate framework for LLMs", "First automated method combining debates with red-teaming", "Integration of long-term memory for improved safety insights"], "limitations": "", "keywords": ["AI safety", "Large Language Models", "multi-agent systems", "adversarial argumentation", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.11112", "pdf": "https://arxiv.org/pdf/2506.11112.pdf", "abs": "https://arxiv.org/abs/2506.11112", "title": "Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)", "authors": ["Christine Bauer", "Li Chen", "Nicola Ferro", "Norbert Fuhr", "Avishek Anand", "Timo Breuer", "Guglielmo Faggioli", "Ophir Frieder", "Hideo Joho", "Jussi Karlgren", "Johannes Kiesel", "Bart P. Knijnenburg", "Aldo Lipani", "Lien Michiels", "Andrea Papenmeier", "Maria Soledad Pera", "Mark Sanderson", "Scott Sanner", "Benno Stein", "Johanne R. Trippas", "Karin Verspoor", "Martijn C Willemsen"], "categories": ["cs.CL", "cs.HC", "cs.IR"], "comment": "43 pages; 10 figures; Dagstuhl manifesto", "summary": "During the workshop, we deeply discussed what CONversational Information\nACcess (CONIAC) is and its unique features, proposing a world model abstracting\nit, and defined the Conversational Agents Framework for Evaluation (CAFE) for\nthe evaluation of CONIAC systems, consisting of six major components: 1) goals\nof the system's stakeholders, 2) user tasks to be studied in the evaluation, 3)\naspects of the users carrying out the tasks, 4) evaluation criteria to be\nconsidered, 5) evaluation methodology to be applied, and 6) measures for the\nquantitative criteria chosen.", "AI": {"tldr": "This paper defines and evaluates the concept of Conversational Information Access (CONIAC) through a proposed framework.", "motivation": "The need for a structured evaluation framework to assess the effectiveness of CONIAC systems.", "method": "The authors developed the Conversational Agents Framework for Evaluation (CAFE), which focuses on six components essential for evaluating CONIAC systems.", "result": "CAFE provides a comprehensive guideline for evaluating user interactions in CONIAC, addressing stakeholders' goals and user tasks.", "conclusion": "Effective evaluation of CONIAC systems can be achieved through the CAFE framework, aiding in the design of better conversational agents.", "key_contributions": ["Introduction of the CAFE framework for CONIAC evaluation", "Identification of key evaluation components", "Discussion on stakeholder goals and user interactions"], "limitations": "", "keywords": ["CONIAC", "Conversational Agents", "Evaluation Framework", "User Tasks", "Human-Computer Interaction"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2506.11088", "pdf": "https://arxiv.org/pdf/2506.11088.pdf", "abs": "https://arxiv.org/abs/2506.11088", "title": "Two Birds with One Stone: Improving Factuality and Faithfulness of LLMs via Dynamic Interactive Subspace Editing", "authors": ["Pengbo Wang", "Chaozhuo Li", "Chenxu Wang", "Liwen Zheng", "Litian Zhang", "Xi Zhang"], "categories": ["cs.CL", "cs.AI", "68T50"], "comment": null, "summary": "LLMs have demonstrated unprecedented capabilities in natural language\nprocessing, yet their practical deployment remains hindered by persistent\nfactuality and faithfulness hallucinations. While existing methods address\nthese hallucination types independently, they inadvertently induce performance\ntrade-offs, as interventions targeting one type often exacerbate the other.\nThrough empirical and theoretical analysis of activation space dynamics in\nLLMs, we reveal that these hallucination categories share overlapping subspaces\nwithin neural representations, presenting an opportunity for concurrent\nmitigation. To harness this insight, we propose SPACE, a unified framework that\njointly enhances factuality and faithfulness by editing shared activation\nsubspaces. SPACE establishes a geometric foundation for shared subspace\nexistence through dual-task feature modeling, then identifies and edits these\nsubspaces via a hybrid probe strategy combining spectral clustering and\nattention head saliency scoring. Experimental results across multiple benchmark\ndatasets demonstrate the superiority of our approach.", "AI": {"tldr": "This paper introduces SPACE, a unified framework for mitigating factuality and faithfulness hallucinations in LLMs by editing shared activation subspaces.", "motivation": "The practical deployment of LLMs is hindered by hallucinations related to factuality and faithfulness, which current methods address independently, leading to performance trade-offs.", "method": "SPACE employs a dual-task feature modeling approach to establish a geometric foundation for shared subspace existence and uses a hybrid probe strategy that combines spectral clustering and attention head saliency scoring to edit these subspaces.", "result": "Experimental results show that SPACE enhances both factuality and faithfulness compared to existing methods across multiple benchmark datasets.", "conclusion": "SPACE offers a novel method of improving LLM performance by concurrently addressing factuality and faithfulness through a shared subspace editing approach.", "key_contributions": ["Introduces the SPACE framework for joint enhancement of factuality and faithfulness.", "Identifies overlapping subspaces in LLM activation dynamics.", "Employs a unique hybrid probe strategy for subspace editing."], "limitations": "", "keywords": ["LLMs", "factuality", "faithfulness", "hallucinations", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.11091", "pdf": "https://arxiv.org/pdf/2506.11091.pdf", "abs": "https://arxiv.org/abs/2506.11091", "title": "Customizing Speech Recognition Model with Large Language Model Feedback", "authors": ["Shaoshi Ling", "Guoli Ye"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic speech recognition (ASR) systems have achieved strong performance\non general transcription tasks. However, they continue to struggle with\nrecognizing rare named entities and adapting to domain mismatches. In contrast,\nlarge language models (LLMs), trained on massive internet-scale datasets, are\noften more effective across a wide range of domains. In this work, we propose a\nreinforcement learning based approach for unsupervised domain adaptation,\nleveraging unlabeled data to enhance transcription quality, particularly the\nnamed entities affected by domain mismatch, through feedback from a LLM. Given\ncontextual information, our framework employs a LLM as the reward model to\nscore the hypotheses from the ASR model. These scores serve as reward signals\nto fine-tune the ASR model via reinforcement learning. Our method achieves a\n21\\% improvement on entity word error rate over conventional self-training\nmethods.", "AI": {"tldr": "This paper proposes a reinforcement learning approach for unsupervised domain adaptation in ASR systems, improving transcription quality of rare named entities using feedback from large language models.", "motivation": "To address the difficulties ASR systems face with rare named entities and domain mismatches, and improve their transcription performance.", "method": "The proposed method uses a large language model (LLM) as a reward model to score entity hypotheses from an ASR system, applying reinforcement learning to fine-tune the ASR model based on these scores.", "result": "The method achieves a 21% improvement in entity word error rate compared to conventional self-training methods.", "conclusion": "The integration of LLM feedback significantly enhances the transcription quality of ASR systems regarding domain-specific challenges.", "key_contributions": ["Introduction of a reinforcement learning framework for ASR domain adaptation.", "Use of LLM as a reward model for scoring ASR hypotheses.", "Demonstrated improvement in entity recognition performance through unsupervised learning."], "limitations": "", "keywords": ["automatic speech recognition", "domain adaptation", "reinforcement learning", "large language models", "named entities"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11092", "pdf": "https://arxiv.org/pdf/2506.11092.pdf", "abs": "https://arxiv.org/abs/2506.11092", "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation", "authors": ["Jubin Abhishek Soni", "Amit Anand", "Rajesh Kumar Pandey", "Aniket Abhishek Soni"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "6 pages, 5 figures, 3 tables. This manuscript has been submitted to\n  IEEE conference. Researchers are welcome to read and build upon this work;\n  please cite it appropriately. For questions or clarifications, feel free to\n  contact me", "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.", "AI": {"tldr": "Dynamic Context Tuning (DCT) enhances Retrieval-Augmented Generation (RAG) for multi-turn dialogues in dynamic environments like healthcare, improving performance metrics significantly.", "motivation": "To overcome limitations of existing RAG systems that are static and single-turn, particularly in dynamic domains.", "method": "DCT integrates an attention-based context cache, LoRA-based retrieval for dynamic tool selection, and context compression to adapt to evolving dialogue and tool environments without retraining.", "result": "DCT improves plan accuracy by 14% and reduces hallucinations by 37%, matching GPT-4 performance at a lower cost.", "conclusion": "DCT enables scalable and adaptable AI assistants by generalizing to new tools in dynamic contexts.", "key_contributions": ["Introduction of Dynamic Context Tuning (DCT) framework for RAG.", "Improvement in dialogue accuracy and reduction in hallucinations.", "Generalization to unseen tools in dynamic environments."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Dynamic Context Tuning", "Large Language Models", "Healthcare", "Multi-turn Dialogue"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2506.11094", "pdf": "https://arxiv.org/pdf/2506.11094.pdf", "abs": "https://arxiv.org/abs/2506.11094", "title": "The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs", "authors": ["Songyang Liu", "Chaozhuo Li", "Jiameng Qiu", "Xi Zhang", "Feiran Huang", "Litian Zhang", "Yiming Hei", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "21 pages, preprint", "summary": "With the rapid advancement of artificial intelligence technology, Large\nLanguage Models (LLMs) have demonstrated remarkable potential in the field of\nNatural Language Processing (NLP), including areas such as content generation,\nhuman-computer interaction, machine translation, and code generation, among\nothers. However, their widespread deployment has also raised significant safety\nconcerns. In recent years, LLM-generated content has occasionally exhibited\nunsafe elements like toxicity and bias, particularly in adversarial scenarios,\nwhich has garnered extensive attention from both academia and industry. While\nnumerous efforts have been made to evaluate the safety risks associated with\nLLMs, there remains a lack of systematic reviews summarizing these research\nendeavors. This survey aims to provide a comprehensive and systematic overview\nof recent advancements in LLMs safety evaluation, focusing on several key\naspects: (1) \"Why evaluate\" that explores the background of LLMs safety\nevaluation, how they differ from general LLMs evaluation, and the significance\nof such evaluation; (2) \"What to evaluate\" that examines and categorizes\nexisting safety evaluation tasks based on key capabilities, including\ndimensions such as toxicity, robustness, ethics, bias and fairness,\ntruthfulness, and so on; (3) \"Where to evaluate\" that summarizes the evaluation\nmetrics, datasets and benchmarks currently used in safety evaluations; (4) \"How\nto evaluate\" that reviews existing evaluation toolkit, and categorizing\nmainstream evaluation methods based on the roles of the evaluators. Finally, we\nidentify the challenges in LLMs safety evaluation and propose potential\nresearch directions to promote further advancement in this field. We emphasize\nthe importance of prioritizing LLMs safety evaluation to ensure the safe\ndeployment of these models in real-world applications.", "AI": {"tldr": "This survey provides a systematic overview of recent advancements in the safety evaluation of Large Language Models (LLMs), highlighting evaluation motivations, tasks, metrics, methods, and future research directions.", "motivation": "To address significant safety concerns associated with LLM-generated content, including toxicity and bias, and to highlight the importance of evaluating their safety before real-world deployment.", "method": "The paper categorizes safety evaluation tasks based on capabilities like toxicity, ethics, robustness, and bias; reviews existing evaluation metrics, datasets, and evaluation toolkits; and discusses the roles of evaluators in the process.", "result": "The survey outlines the current landscape of LLM safety evaluation, identifies existing challenges in the field, and suggests avenues for future research to enhance the evaluation process.", "conclusion": "Prioritizing safety evaluation is crucial for ensuring the responsible deployment of LLMs in various applications.", "key_contributions": ["Comprehensive categorization of safety evaluation tasks for LLMs", "Review of existing metrics and benchmarks used for LLM safety", "Identification of challenges and research directions for advancing LLM safety evaluation"], "limitations": "The survey may not cover all recent advancements and might focus primarily on mainstream evaluation approaches.", "keywords": ["Large Language Models", "safety evaluation", "Natural Language Processing", "toxicity", "bias"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2506.11095", "pdf": "https://arxiv.org/pdf/2506.11095.pdf", "abs": "https://arxiv.org/abs/2506.11095", "title": "Persistent Homology of Topic Networks for the Prediction of Reader Curiosity", "authors": ["Manuel D. S. Hopp", "Vincent Labatut", "Arthur Amalvy", "Richard Dufour", "Hannah Stone", "Hayley Jach", "Kou Murayama"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reader curiosity, the drive to seek information, is crucial for textual\nengagement, yet remains relatively underexplored in NLP. Building on\nLoewenstein's Information Gap Theory, we introduce a framework that models\nreader curiosity by quantifying semantic information gaps within a text's\nsemantic structure. Our approach leverages BERTopic-inspired topic modeling and\npersistent homology to analyze the evolving topology (connected components,\ncycles, voids) of a dynamic semantic network derived from text segments,\ntreating these features as proxies for information gaps. To empirically\nevaluate this pipeline, we collect reader curiosity ratings from participants\n(n = 49) as they read S. Collins's ''The Hunger Games'' novel. We then use the\ntopological features from our pipeline as independent variables to predict\nthese ratings, and experimentally show that they significantly improve\ncuriosity prediction compared to a baseline model (73% vs. 30% explained\ndeviance), validating our approach. This pipeline offers a new computational\nmethod for analyzing text structure and its relation to reader engagement.", "AI": {"tldr": "The paper introduces a framework to model reader curiosity in text by quantifying semantic information gaps using topic modeling and topological analysis.", "motivation": "Reader curiosity is crucial for textual engagement but is underexplored in NLP.", "method": "The framework uses BERTopic-inspired topic modeling and persistent homology to analyze a dynamic semantic network derived from text segments.", "result": "The proposed method significantly improves curiosity prediction, achieving 73% explained deviance compared to a baseline of 30%.", "conclusion": "This approach provides a new computational method for analyzing text structure in relation to reader engagement.", "key_contributions": ["Introduction of a novel framework to model reader curiosity", "Use of topological features to quantify semantic information gaps", "Empirical validation of the approach through reader curiosity ratings"], "limitations": "", "keywords": ["reader curiosity", "information gap theory", "topic modeling"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.11097", "pdf": "https://arxiv.org/pdf/2506.11097.pdf", "abs": "https://arxiv.org/abs/2506.11097", "title": "C-SEO Bench: Does Conversational SEO Work?", "authors": ["Haritz Puerto", "Martin Gubri", "Tommaso Green", "Seong Joon Oh", "Sangdoo Yun"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) are transforming search engines into\nConversational Search Engines (CSE). Consequently, Search Engine Optimization\n(SEO) is being shifted into Conversational Search Engine Optimization (C-SEO).\nWe are beginning to see dedicated C-SEO methods for modifying web documents to\nincrease their visibility in CSE responses. However, they are often tested only\nfor a limited breadth of application domains; we do not understand whether\ncertain C-SEO methods would be effective for a broad range of domains.\nMoreover, existing evaluations consider only a single-actor scenario where only\none web document adopts a C-SEO method; in reality, multiple players are likely\nto competitively adopt the cutting-edge C-SEO techniques, drawing an analogy\nfrom the dynamics we have seen in SEO. We present C-SEO Bench, the first\nbenchmark designed to evaluate C-SEO methods across multiple tasks, domains,\nand number of actors. We consider two search tasks, question answering and\nproduct recommendation, with three domains each. We also formalize a new\nevaluation protocol with varying adoption rates among involved actors. Our\nexperiments reveal that most current C-SEO methods are largely ineffective,\ncontrary to reported results in the literature. Instead, traditional SEO\nstrategies, those aiming to improve the ranking of the source in the LLM\ncontext, are significantly more effective. We also observe that as we increase\nthe number of C-SEO adopters, the overall gains decrease, depicting a congested\nand zero-sum nature of the problem. Our code and data are available at\nhttps://github.com/parameterlab/c-seo-bench and\nhttps://huggingface.co/datasets/parameterlab/c-seo-bench.", "AI": {"tldr": "The paper introduces C-SEO Bench, a benchmark to evaluate Conversational Search Engine Optimization methods, revealing traditional SEO strategies are more effective.", "motivation": "To evaluate the effectiveness of Conversational Search Engine Optimization (C-SEO) methods across multiple domains and tasks, as existing methods are limited in scope.", "method": "C-SEO Bench benchmark was developed, which assesses C-SEO methods involving multiple search tasks (question answering and product recommendation) and domains, with a formal evaluation protocol considering various adoption rates.", "result": "Experiments showed that most current C-SEO methods are largely ineffective, and traditional SEO techniques provided significantly better outcomes.", "conclusion": "The study highlights the competitive nature of C-SEO adoption and suggests traditional SEO strategies outperform new C-SEO methods; code and data are made publicly available.", "key_contributions": ["Introduction of C-SEO Bench as a new benchmark for evaluating C-SEO methods.", "Formalization of a new evaluation protocol for C-SEO.", "Demonstration of traditional SEO effectiveness over C-SEO methods."], "limitations": "Current evaluations do not consider long-term dynamics of C-SEO; only limited domains and tasks were assessed.", "keywords": ["Conversational Search", "C-SEO", "SEO", "Benchmark", "Search Engine Optimization"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.11102", "pdf": "https://arxiv.org/pdf/2506.11102.pdf", "abs": "https://arxiv.org/abs/2506.11102", "title": "Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey", "authors": ["Jiachen Zhu", "Menghui Zhu", "Renting Rui", "Rong Shan", "Congmin Zheng", "Bo Chen", "Yunjia Xi", "Jianghao Lin", "Weiwen Liu", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of large language models (LLMs), such as GPT, Gemini, and\nDeepSeek, has significantly advanced natural language processing, giving rise\nto sophisticated chatbots capable of diverse language-related tasks. The\ntransition from these traditional LLM chatbots to more advanced AI agents\nrepresents a pivotal evolutionary step. However, existing evaluation frameworks\noften blur the distinctions between LLM chatbots and AI agents, leading to\nconfusion among researchers selecting appropriate benchmarks. To bridge this\ngap, this paper introduces a systematic analysis of current evaluation\napproaches, grounded in an evolutionary perspective. We provide a detailed\nanalytical framework that clearly differentiates AI agents from LLM chatbots\nalong five key aspects: complex environment, multi-source instructor, dynamic\nfeedback, multi-modal perception, and advanced capability. Further, we\ncategorize existing evaluation benchmarks based on external environments\ndriving forces, and resulting advanced internal capabilities. For each\ncategory, we delineate relevant evaluation attributes, presented\ncomprehensively in practical reference tables. Finally, we synthesize current\ntrends and outline future evaluation methodologies through four critical\nlenses: environment, agent, evaluator, and metrics. Our findings offer\nactionable guidance for researchers, facilitating the informed selection and\napplication of benchmarks in AI agent evaluation, thus fostering continued\nadvancement in this rapidly evolving research domain.", "AI": {"tldr": "The paper analyzes evaluation frameworks for AI agents and LLM chatbots, introducing a detailed framework that clarifies their distinctions and categorizes existing benchmarks.", "motivation": "To address the confusion among researchers regarding evaluation frameworks for LLM chatbots and AI agents.", "method": "Systematic analysis of current evaluation approaches, differentiating AI agents from LLM chatbots based on five key aspects and categorizing benchmarks by internal capabilities.", "result": "Introduced a comprehensive analytical framework and categorized existing evaluation benchmarks, providing practical reference tables and guidance for researchers.", "conclusion": "The study offers actionable insights for the informed selection and application of evaluation benchmarks in AI agent research.", "key_contributions": ["Systematic analysis of evaluation approaches for AI agents and LLM chatbots", "Comprehensive framework differentiating AI agents and LLM chatbots", "Categorization of evaluation benchmarks with relevant attributes"], "limitations": "", "keywords": ["evaluation frameworks", "AI agents", "LLM chatbots", "benchmarking", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.11103", "pdf": "https://arxiv.org/pdf/2506.11103.pdf", "abs": "https://arxiv.org/abs/2506.11103", "title": "You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model", "authors": ["Wenchong He", "Liqian Peng", "Zhe Jiang", "Alex Go"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 6 figures", "summary": "Large language models (LLMs) possess a remarkable ability to perform\nin-context learning (ICL), which enables them to handle multiple downstream\ntasks simultaneously without requiring task-specific fine-tuning. Recent\nstudies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma\n7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of\nall tasks at once. However, this approach still lags behind dedicated\nfine-tuning, where a separate model is trained for each individual task.\n  In this paper, we propose a novel approach, Many-Shot In-Context Fine-tuning\n(ManyICL), which significantly narrows this performance gap by extending the\nprinciples of ICL to a many-shot setting. To unlock the full potential of\nManyICL and address the inherent inefficiency of processing long sequences with\nnumerous in-context examples, we propose a novel training objective. Instead of\nsolely predicting the final answer, our approach treats every answer within the\ncontext as a supervised training target. This effectively shifts the role of\nmany-shot examples from prompts to targets for autoregressive learning. Through\nextensive experiments on diverse downstream tasks, including classification,\nsummarization, question answering, natural language inference, and math, we\ndemonstrate that ManyICL substantially outperforms zero/few-shot fine-tuning\nand approaches the performance of dedicated fine-tuning. Furthermore, ManyICL\nsignificantly mitigates catastrophic forgetting issues observed in\nzero/few-shot fine-tuning. The code will be made publicly available upon\npublication.", "AI": {"tldr": "This paper introduces Many-Shot In-Context Fine-tuning (ManyICL), a novel approach that leverages in-context learning to enhance model performance across multiple tasks simultaneously.", "motivation": "The paper addresses the performance gap between in-context learning and dedicated fine-tuning of language models, proposing a more efficient method to improve model generalization.", "method": "ManyICL extends in-context learning principles to a many-shot framework, treating multiple examples as supervised training targets rather than just prompts.", "result": "ManyICL outperforms traditional zero/few-shot fine-tuning and approaches dedicated fine-tuning performance across diverse tasks such as classification, summarization, and math, while reducing catastrophic forgetting.", "conclusion": "The proposed ManyICL method significantly boosts the efficiency and effectiveness of language models in handling multiple tasks simultaneously.", "key_contributions": ["Introduction of Many-Shot In-Context Fine-tuning (ManyICL) for LLMs.", "Novel training objective that utilizes in-context answers as supervised targets.", "Demonstration of improved performance compared to zero/few-shot strategies, approaching dedicated fine-tuning results."], "limitations": "", "keywords": ["In-Context Learning", "Fine-tuning", "Language Models", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2506.11104", "pdf": "https://arxiv.org/pdf/2506.11104.pdf", "abs": "https://arxiv.org/abs/2506.11104", "title": "DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration", "authors": ["Hanzhi Zhang", "Heng Fan", "Kewei Sha", "Yan Huang", "Yunhe Feng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding is crucial for many NLP applications, yet\ntransformers struggle with efficiency due to the quadratic complexity of\nself-attention. Sparse attention methods alleviate this cost but often impose\nstatic, predefined masks, failing to capture heterogeneous attention patterns.\nThis results in suboptimal token interactions, limiting adaptability and\nretrieval accuracy in long-sequence tasks. This work introduces a dynamic\nsparse attention mechanism that assigns adaptive masks at the attention-map\nlevel, preserving heterogeneous patterns across layers and heads. Unlike\nexisting approaches, our method eliminates the need for fine-tuning and\npredefined mask structures while maintaining computational efficiency. By\nlearning context-aware attention structures, it achieves high alignment with\nfull-attention models, ensuring minimal performance degradation while reducing\nmemory and compute overhead. This approach provides a scalable alternative to\nfull attention, enabling the practical deployment of large-scale Large Language\nModels (LLMs) without sacrificing retrieval performance. DAM is available at:\nhttps://github.com/HanzhiZhang-Ulrica/DAM.", "AI": {"tldr": "This paper presents a dynamic sparse attention mechanism that improves efficiency in NLP tasks while maintaining adaptability and performance without predefined masks.", "motivation": "Long-context understanding in NLP requires efficient self-attention mechanisms due to the limitations of transformers in handling long sequences effectively.", "method": "The proposed method introduces dynamic masks at the attention-map level, which adaptively choose attention patterns depending on the context, unlike static masks used in traditional sparse attention.", "result": "The dynamic sparse attention mechanism achieves alignment with full-attention models, showing minimal performance degradation and significantly reducing memory and compute overhead in long-sequence NLP tasks.", "conclusion": "The proposed approach provides a scalable solution for deploying large-scale LLMs while ensuring effective retrieval performance, eliminating the need for fine-tuning or static mask definitions.", "key_contributions": ["Dynamic sparse attention mechanism", "No need for fine-tuning or predefined masks", "Scalable alternative to full attention"], "limitations": "", "keywords": ["sparse attention", "NLP", "dynamic masking", "large language models", "efficiency"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2506.11105", "pdf": "https://arxiv.org/pdf/2506.11105.pdf", "abs": "https://arxiv.org/abs/2506.11105", "title": "Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation", "authors": ["Uttej Kallakurik", "Edward Humes", "Rithvik Jonna", "Xiaomin Lin", "Tinoosh Mohsenin"], "categories": ["cs.CL", "cs.AI", "cs.AR", "cs.SY", "eess.SY"], "comment": null, "summary": "Large Language Models (LLMs) have significant impact on the healthcare\nscenarios but remain prohibitively large for deployment in real-time,\nresource-constrained environments such as edge devices. In this work, we\nintroduce a novel medical assistant system, optimized through our\ngeneral-purpose compression framework, which tailors Large Language Models\n(LLMs) for deployment in specialized domains. By measuring neuron saliency on\ndomain-specific data, our method can aggressively prune irrelevant neurons,\nreducing model size while preserving performance. Following pruning, we apply\npost-training quantization to further reduce the memory footprint, and evaluate\nthe compressed model across medical benchmarks including MedMCQA, MedQA, and\nPubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed\nLLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),\nachieving real-time, energy-efficient inference under hardware constraints.", "AI": {"tldr": "This paper introduces a novel, resource-efficient medical assistant system using compressed LLMs tailored for real-time deployment in edge devices.", "motivation": "To enable the deployment of Large Language Models in real-time, resource-constrained environments in healthcare scenarios.", "method": "A general-purpose compression framework that measures neuron saliency on domain-specific data to prune irrelevant neurons, followed by post-training quantization.", "result": "The compressed models were evaluated against medical benchmarks, demonstrating the ability to efficiently deploy significantly reduced models while maintaining performance.", "conclusion": "Optimized LLMs can achieve real-time, energy-efficient inference for medical applications on edge hardware.", "key_contributions": ["Introduction of a general-purpose compression framework for LLMs in health informatics.", "Demonstration of effective neuron pruning to reduce model size while maintaining performance.", "Successful deployment of compressed models on resource-constrained devices."], "limitations": "", "keywords": ["Large Language Models", "medical assistant system", "model compression", "healthcare", "edge devices"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2506.11106", "pdf": "https://arxiv.org/pdf/2506.11106.pdf", "abs": "https://arxiv.org/abs/2506.11106", "title": "Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking", "authors": ["Ningyuan Li", "Junrui Liu", "Yi Shan", "Minghui Huang", "Tong Li"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Contemporary graph-based retrieval-augmented generation (RAG) methods\ntypically begin by extracting entities from user queries and then leverage\npre-constructed knowledge graphs to retrieve related relationships and\nmetadata. However, this pipeline's exclusive reliance on entity-level\nextraction can lead to the misinterpretation or omission of latent yet critical\ninformation and relations. As a result, retrieved content may be irrelevant or\ncontradictory, and essential knowledge may be excluded, exacerbating\nhallucination risks and degrading the fidelity of generated responses. To\naddress these limitations, we introduce PankRAG, a framework that combines a\nglobally aware, hierarchical query-resolution strategy with a novel\ndependency-aware reranking mechanism. PankRAG first constructs a multi-level\nresolution path that captures both parallel and sequential interdependencies\nwithin a query, guiding large language models (LLMs) through structured\nreasoning. It then applies its dependency-aware reranker to exploit the\ndependency structure among resolved sub-questions, enriching and validating\nretrieval results for subsequent sub-questions. Empirical evaluations\ndemonstrate that PankRAG consistently outperforms state-of-the-art approaches\nacross multiple benchmarks, underscoring its robustness and generalizability.", "AI": {"tldr": "PankRAG enhances graph-based retrieval-augmented generation by addressing the limitations of entity-level extraction through a hierarchical query-resolution strategy and dependency-aware reranking.", "motivation": "To overcome the misinterpretation and omission of critical information in graph-based retrieval methods that rely solely on entity extraction, thus improving the fidelity of generated responses.", "method": "PankRAG uses a multi-level resolution path for structured reasoning, capturing both parallel and sequential interdependencies, followed by a dependency-aware reranking to validate retrieval results.", "result": "PankRAG outperforms state-of-the-art approaches across multiple benchmarks in terms of retrieval and generation fidelity.", "conclusion": "The robust performance of PankRAG demonstrates its effectiveness in enhancing retrieval-augmented generation tasks.", "key_contributions": ["Introduces a hierarchical query-resolution strategy that captures interdependencies in queries.", "Develops a dependency-aware reranking mechanism to improve the quality of retrieved information.", "Demonstrates consistent performance improvements over existing methods across various benchmarks."], "limitations": "", "keywords": ["graph-based retrieval", "retrieval-augmented generation", "dependency-aware", "query-resolution", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.11108", "pdf": "https://arxiv.org/pdf/2506.11108.pdf", "abs": "https://arxiv.org/abs/2506.11108", "title": "History-Aware Cross-Attention Reinforcement: Self-Supervised Multi Turn and Chain-of-Thought Fine-Tuning with vLLM", "authors": ["Andrew Kiruluta", "Andreas Lemos", "Priscilla Burity"], "categories": ["cs.CL"], "comment": null, "summary": "We present CAGSR-vLLM-MTC, an extension of our Self-Supervised\nCross-Attention-Guided Reinforcement (CAGSR) framework, now implemented on the\nhigh-performance vLLM runtime, to address both multi-turn dialogue and\nchain-of-thought reasoning. Building upon our original single-turn approach, we\nfirst instrumented vLLM's C++/CUDA kernels to asynchronously capture per-layer,\nper-head cross-attention weights during generation. We then generalized our\nself-supervised reward function to accumulate attention signals over entire\nconversation histories and intermediate chain-of-thought steps. We discuss\npractical trade-offs, including an entropy-based clamping mechanism to prevent\nattention collapse on early context, and outline future directions for\nmulti-party dialogues and hierarchical reasoning.", "AI": {"tldr": "CAGSR-vLLM-MTC enhances the CAGSR framework to support multi-turn dialogue and chain-of-thought reasoning through novel methods of capturing attention signals.", "motivation": "To improve dialogue systems and reasoning capabilities in AI through an advanced reinforcement learning framework.", "method": "The approach extends the CAGSR framework with a high-performance vLLM runtime, capturing attention weights during generation and accumulating signals over conversation histories and reasoning steps.", "result": "The extension shows improvements in handling multi-turn dialogues and reasoning processes, addressing practical trade-offs in attention mechanisms.", "conclusion": "CAGSR-vLLM-MTC can facilitate future advancements in multi-party dialogues and hierarchical reasoning in AI.", "key_contributions": ["Implementation of an enhanced vLLM runtime for better performance in dialogue.", "A self-supervised reward function that aggregates attention signals.", "Novel entropy-based clamping mechanism to maintain attention integrity."], "limitations": "", "keywords": ["CAGSR", "vLLM", "multi-turn dialogue", "chain-of-thought reasoning", "reinforcement learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.07406", "pdf": "https://arxiv.org/pdf/2409.07406.pdf", "abs": "https://arxiv.org/abs/2409.07406", "title": "Predicting Trust Dynamics Type Using Seven Personal Characteristics", "authors": ["Hyesun Chung", "X. Jessie Yang"], "categories": ["cs.HC"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This study aims to explore the associations between individuals' trust\ndynamics in automated/autonomous technologies and their personal\ncharacteristics, and to further examine whether personal characteristics can be\nused to predict a user's trust dynamics type. We conducted a human-subject\nexperiment (N=130) in which participants performed a simulated surveillance\ntask assisted by an automated threat detector. Using a pre-experimental survey\ncovering 12 constructs and 28 dimensions, we collected data on participants'\npersonal characteristics. Based on the experimental data, we performed k-means\nclustering and identified three trust dynamics types. Subsequently, we\nconducted one-way Analyses of Variance to evaluate differences among the three\ntrust dynamics types in terms of personal characteristics, behaviors,\nperformance, and post-experimental ratings. Participants were clustered into\nthree groups, namely Bayesian decision makers, disbelievers, and oscillators.\nResults showed that the clusters differ significantly in seven personal\ncharacteristics: masculinity, positive affect, extraversion, neuroticism,\nintellect, performance expectancy, and high expectations. The disbelievers tend\nto have high neuroticism and low performance expectancy. The oscillators tend\nto have higher scores in masculinity, positive affect, extraversion, and\nintellect. We also found significant differences in behaviors, performance, and\npost-experimental ratings across the three groups. The disbelievers are the\nleast likely to blindly follow the recommendations made by the automated threat\ndetector. Based on the significant personal characteristics, we developed a\ndecision tree model to predict the trust dynamics type with an accuracy of 70%.\nThis model offers promising implications for identifying individuals whose\ntrust dynamics may deviate from a Bayesian pattern.", "AI": {"tldr": "The study investigates the relationship between personal characteristics and trust dynamics in automated technologies, identifying three trust dynamics types through clustering and analyzing differences in behaviors and ratings.", "motivation": "To explore the associations between trust dynamics in automated technologies and personal characteristics, and to predict trust dynamics types.", "method": "Human-subject experiment with 130 participants performing a surveillance task, collecting data through a survey and using k-means clustering to identify trust dynamics types.", "result": "Identified three trust dynamics types: Bayesian decision makers, disbelievers, and oscillators; found significant differences in personal characteristics, behaviors, and performance among these types.", "conclusion": "Developed a decision tree model with 70% accuracy to predict trust dynamics types based on personal characteristics.", "key_contributions": ["Identification of three distinct trust dynamics types related to personal characteristics.", "Significant findings on how personal traits influence trust in automated technologies.", "Prediction model for trust dynamics with practical implications."], "limitations": "", "keywords": ["trust dynamics", "automated technologies", "personal characteristics", "decision tree", "humans-in-the-loop"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.11109", "pdf": "https://arxiv.org/pdf/2506.11109.pdf", "abs": "https://arxiv.org/abs/2506.11109", "title": "Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization", "authors": ["Yile Chen", "Yicheng Tao", "Yue Jiang", "Shuai Liu", "Han Yu", "Gao Cong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD'25", "summary": "The widespread adoption of location-based services has led to the generation\nof vast amounts of mobility data, providing significant opportunities to model\nuser movement dynamics within urban environments. Recent advancements have\nfocused on adapting Large Language Models (LLMs) for mobility analytics.\nHowever, existing methods face two primary limitations: inadequate semantic\nrepresentation of locations (i.e., discrete IDs) and insufficient modeling of\nmobility signals within LLMs (i.e., single templated instruction fine-tuning).\nTo address these issues, we propose QT-Mob, a novel framework that\nsignificantly enhances LLMs for mobility analytics. QT-Mob introduces a\nlocation tokenization module that learns compact, semantically rich tokens to\nrepresent locations, preserving contextual information while ensuring\ncompatibility with LLMs. Furthermore, QT-Mob incorporates a series of\ncomplementary fine-tuning objectives that align the learned tokens with the\ninternal representations in LLMs, improving the model's comprehension of\nsequential movement patterns and location semantics. The proposed QT-Mob\nframework not only enhances LLMs' ability to interpret mobility data but also\nprovides a more generalizable approach for various mobility analytics tasks.\nExperiments on three real-world dataset demonstrate the superior performance in\nboth next-location prediction and mobility recovery tasks, outperforming\nexisting deep learning and LLM-based methods.", "AI": {"tldr": "QT-Mob enhances LLMs for mobility analytics by introducing location tokenization and improved fine-tuning objectives, outperforming existing models in mobility tasks.", "motivation": "To address limitations in semantic representation of locations and mobility signal modeling in current LLMs used for mobility analytics.", "method": "QT-Mob includes a location tokenization module for compact and semantically rich location representation and a series of fine-tuning objectives to improve comprehension of movement patterns.", "result": "Experiments show QT-Mob significantly improves next-location prediction and mobility recovery, outperforming traditional deep learning and LLM-based approaches on real-world datasets.", "conclusion": "QT-Mob provides a more generalizable framework for mobility analytics with enhanced comprehension of mobility data.", "key_contributions": ["Introduction of a location tokenization module for better semantic representation of locations.", "Development of complementary fine-tuning objectives for aligning tokens with LLM representations.", "Demonstration of superior performance on real-world mobility tasks compared to existing methods."], "limitations": "", "keywords": ["location-based services", "large language models", "mobility analytics", "tokenization", "fine-tuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.03295", "pdf": "https://arxiv.org/pdf/2411.03295.pdf", "abs": "https://arxiv.org/abs/2411.03295", "title": "Examining Human-AI Collaboration for Co-Writing Constructive Comments Online", "authors": ["Farhana Shahid", "Maximilian Dittgen", "Mor Naaman", "Aditya Vashistha"], "categories": ["cs.HC"], "comment": null, "summary": "This paper examines if large language models (LLMs) can help people write\nconstructive comments on divisive social issues due to the difficulty of\nexpressing constructive disagreement online. Through controlled experiments\nwith 600 participants from India and the US, who reviewed and wrote\nconstructive comments on threads related to Islamophobia and homophobia, we\nobserved potential misalignment between how LLMs and humans perceive\nconstructiveness in online comments. While the LLM was more likely to\nprioritize politeness and balance among contrasting viewpoints when evaluating\nconstructiveness, participants emphasized logic and facts more than the LLM\ndid. Despite these differences, participants rated both LLM-generated and\nhuman-AI co-written comments as significantly more constructive than those\nwritten independently by humans. Our analysis also revealed that LLM-generated\ncomments integrated significantly more linguistic features of constructiveness\ncompared to human-written comments. When participants used LLMs to refine their\ncomments, the resulting comments were more constructive, more positive, less\ntoxic, and retained the original intent. However, occasionally LLMs distorted\npeople's original views -- especially when their stances were not outright\npolarizing. Based on these findings, we discuss ethical and design\nconsiderations in using LLMs to facilitate constructive discourse online.", "AI": {"tldr": "This paper evaluates the effectiveness of large language models (LLMs) in aiding the writing of constructive comments on divisive social issues and reveals misalignment between LLM and human perceptions of constructiveness.", "motivation": "To address the challenge of expressing constructive disagreement online, especially on divisive social issues like Islamophobia and homophobia.", "method": "Controlled experiments with 600 participants from India and the US who reviewed and wrote comments, examining their interactions with LLM-generated suggestions.", "result": "Participants found that LLM-generated comments were more constructive and positively framed compared to independent human comments, although there were misalignments in the understanding of constructiveness between LLMs and humans.", "conclusion": "While LLMs can enhance the constructiveness of online comments, their tendency to distort original views needs consideration for ethical and design purposes.", "key_contributions": ["Identification of misalignment between LLM and human constructs of constructiveness.", "Demonstration that LLMs can significantly improve comment constructiveness and reduce toxicity.", "Ethical considerations for using LLMs in online discourse."], "limitations": "LLMs occasionally distorted participants' original views, particularly on non-polarizing stances.", "keywords": ["Large Language Models", "Constructive Comments", "Online Discourse", "Ethical Considerations", "Social Issues"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.11110", "pdf": "https://arxiv.org/pdf/2506.11110.pdf", "abs": "https://arxiv.org/abs/2506.11110", "title": "AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models", "authors": ["Jaeho Lee", "Atharv Chowdhary"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 4 figures, appendix contains 2 additional figures and 2\n  tables", "summary": "Recent benchmarks have probed factual consistency and rhetorical robustness\nin Large Language Models (LLMs). However, a knowledge gap exists regarding how\ndirectional framing of factually true statements influences model agreement, a\ncommon scenario for LLM users. AssertBench addresses this by sampling\nevidence-supported facts from FEVEROUS, a fact verification dataset. For each\n(evidence-backed) fact, we construct two framing prompts: one where the user\nclaims the statement is factually correct, and another where the user claims it\nis incorrect. We then record the model's agreement and reasoning. The desired\noutcome is that the model asserts itself, maintaining consistent truth\nevaluation across both framings, rather than switching its evaluation to agree\nwith the user. AssertBench isolates framing-induced variability from the\nmodel's underlying factual knowledge by stratifying results based on the\nmodel's accuracy on the same claims when presented neutrally. In doing so, this\nbenchmark aims to measure an LLM's ability to \"stick to its guns\" when\npresented with contradictory user assertions about the same fact. The complete\nsource code is available at https://github.com/achowd32/assert-bench.", "AI": {"tldr": "This paper introduces AssertBench, a benchmark to study how the framing of factually true statements affects the agreement of Large Language Models (LLMs).", "motivation": "To investigate how directional framing of true statements influences model agreement in LLMs, filling the gap in understanding this interaction.", "method": "AssertBench utilizes evidence-supported facts from the FEVEROUS dataset to create two framing prompts for each fact: one asserting correctness and another asserting incorrectness, measuring model responses.", "result": "The study indicates how LLMs respond differently based on the framing of factual statements, assessing their consistency.", "conclusion": "LLMs should ideally maintain consistent evaluations of facts regardless of user-framed prompts, showcasing their factual reliability.", "key_contributions": ["Introduction of AssertBench for evaluating LLMs' agreement based on framing.", "Methodology for isolating framing effects from factual accuracy.", "Insights into the LLMs' consistency in truth evaluation amidst contradictory user statements."], "limitations": "The study is focused on LLMs and may not generalize to other types of models or tasks; the results are dependent on the training data used.", "keywords": ["Large Language Models", "framing effects", "factual consistency", "AssertBench", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.07598", "pdf": "https://arxiv.org/pdf/2502.07598.pdf", "abs": "https://arxiv.org/abs/2502.07598", "title": "Towards spatial computing: recent advances in multimodal natural interaction for XR headsets", "authors": ["Zhimin Wang", "Maohang Rao", "Shanghua Ye", "Weitao Song", "Feng Lu"], "categories": ["cs.HC"], "comment": "28 pages, 10 figures", "summary": "With the widespread adoption of Extended Reality (XR) headsets, spatial\ncomputing technologies are gaining increasing attention. Spatial computing\nenables interaction with virtual elements through natural input methods such as\neye tracking, hand gestures, and voice commands, thus placing natural\nhuman-computer interaction at its core. While previous surveys have reviewed\nconventional XR interaction techniques, recent advancements in natural\ninteraction, particularly driven by artificial intelligence (AI) and large\nlanguage models (LLMs), have introduced new paradigms and technologies. In this\npaper, we review research on multimodal natural interaction for wearable XR,\nfocusing on papers published between 2022 and 2024 in six top venues: ACM CHI,\nUIST, IMWUT (Ubicomp), IEEE VR, ISMAR, and TVCG. We classify and analyze these\nstudies based on application scenarios, operation types, and interaction\nmodalities. This analysis provides a structured framework for understanding how\nresearchers are designing advanced natural interaction techniques in XR. Based\non these findings, we discuss the challenges in natural interaction techniques\nand suggest potential directions for future research. This review provides\nvaluable insights for researchers aiming to design natural and efficient\ninteraction systems for XR, ultimately contributing to the advancement of\nspatial computing.", "AI": {"tldr": "This paper reviews recent advancements in multimodal natural interaction techniques for wearable Extended Reality (XR) technology, highlighting AI and LLM's role in enhancing human-computer interaction.", "motivation": "The paper aims to understand how recent advancements in natural interaction, particularly involving AI and LLMs, influence the design of interaction systems in XR.", "method": "The authors reviewed papers published between 2022 and 2024 across six top venues, classifying the studies based on application scenarios, operation types, and interaction modalities.", "result": "The review identifies key trends in natural interaction techniques and presents a structured framework for future research directions in XR.", "conclusion": "The findings underscore the importance of addressing challenges in natural interaction techniques, with implications for the advancement of spatial computing.", "key_contributions": ["Comprehensive review of multimodal interaction techniques in XR focused on recent AI advancements", "Classification framework for understanding interaction modalities and application scenarios", "Identification of future research challenges and directions in natural interaction for XR"], "limitations": "", "keywords": ["Extended Reality", "natural interaction", "AI", "multimodal interaction", "spatial computing"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2506.11111", "pdf": "https://arxiv.org/pdf/2506.11111.pdf", "abs": "https://arxiv.org/abs/2506.11111", "title": "Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions", "authors": ["Kun Zhang", "Le Wu", "Kui Yu", "Guangyi Lv", "Dacao Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "33 pages, 5 figures", "summary": "Large Language Models (LLMs) have gained enormous attention in recent years\ndue to their capability of understanding and generating natural languages. With\nthe rapid development and wild-range applications (e.g., Agents, Embodied\nIntelligence), the robustness of LLMs has received increased attention. As the\ncore brain of many AI applications, the robustness of LLMs requires that models\nshould not only generate consistent contents, but also ensure the correctness\nand stability of generated content when dealing with unexpeted application\nscenarios (e.g., toxic prompts, limited noise domain data, outof-distribution\n(OOD) applications, etc). In this survey paper, we conduct a thorough review of\nthe robustness of LLMs, aiming to provide a comprehensive terminology of\nconcepts and methods around this field and facilitate the community.\nSpecifically, we first give a formal definition of LLM robustness and present\nthe collection protocol of this survey paper. Then, based on the types of\nperturbated inputs, we organize this survey from the following perspectives: 1)\nAdversarial Robustness: tackling the problem that prompts are manipulated\nintentionally, such as noise prompts, long context, data attack, etc; 2) OOD\nRobustness: dealing with the unexpected real-world application scenarios, such\nas OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of\nRobustness: summarizing the new evaluation datasets, metrics, and tools for\nverifying the robustness of LLMs. After reviewing the representative work from\neach perspective, we discuss and highlight future opportunities and research\ndirections in this field. Meanwhile, we also organize related works and provide\nan easy-to-search project\n(https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the\ncommunity.", "AI": {"tldr": "This survey reviews the robustness of Large Language Models (LLMs), outlining definitions, evaluation methods, and future research directions.", "motivation": "To address the increasing attention on the robustness of LLMs amidst their wide-ranging applications like Agents and Embodied Intelligence.", "method": "The survey is organized based on the types of perturbated inputs affecting LLM robustness, covering Adversarial Robustness, OOD Robustness, and Evaluation of Robustness frameworks.", "result": "The paper reviews various approaches to LLM robustness, highlighting challenges posed by adversarial prompts, out-of-distribution inputs, and evaluating robustness through new datasets and metrics.", "conclusion": "The authors propose future research opportunities and provide a collection of relevant work to facilitate community efforts in improving LLM robustness.", "key_contributions": ["Formal definition of LLM robustness.", "Comprehensive classification of robustness issues related to LLMs.", "Compilation of evaluation datasets and metrics for LLM robustness."], "limitations": "Limited to the literature reviewed, may not cover all emerging techniques and methods.", "keywords": ["Large Language Models", "robustness", "adversarial prompts", "out-of-distribution", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 60}}
{"id": "2504.09346", "pdf": "https://arxiv.org/pdf/2504.09346.pdf", "abs": "https://arxiv.org/abs/2504.09346", "title": "\"It's not a representation of me\": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services", "authors": ["Shira Michel", "Sufi Kaur", "Sarah Elizabeth Gillespie", "Jeffrey Gleason", "Christo Wilson", "Avijit Ghosh"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "This paper has been accepted to FAccT 2025", "summary": "Recent advances in artificial intelligence (AI) speech generation and voice\ncloning technologies have produced naturalistic speech and accurate voice\nreplication, yet their influence on sociotechnical systems across diverse\naccents and linguistic traits is not fully understood. This study evaluates two\nsynthetic AI voice services (Speechify and ElevenLabs) through a mixed methods\napproach using surveys and interviews to assess technical performance and\nuncover how users' lived experiences influence their perceptions of accent\nvariations in these speech technologies. Our findings reveal technical\nperformance disparities across five regional, English-language accents and\ndemonstrate how current speech generation technologies may inadvertently\nreinforce linguistic privilege and accent-based discrimination, potentially\ncreating new forms of digital exclusion. Overall, our study highlights the need\nfor inclusive design and regulation by providing actionable insights for\ndevelopers, policymakers, and organizations to ensure equitable and socially\nresponsible AI speech technologies.", "AI": {"tldr": "This study evaluates AI speech generation technologies and their socio-technical effects on users' perceptions of accent variations, revealing technical disparities and advocating for inclusive design.", "motivation": "To understand the impact of AI speech generation on sociotechnical systems and address issues of linguistic privilege and accent discrimination.", "method": "A mixed methods approach using surveys and interviews to assess the technical performance of two AI voice services and user experiences.", "result": "Findings indicate disparities in the technical performance of AI speech technologies across different English accents and highlight issues of digital exclusion related to accent variations.", "conclusion": "The study underscores the importance of inclusive design in AI speech technologies to avoid exacerbating linguistic privilege and discrimination.", "key_contributions": ["Evaluation of technical performance of AI speech technologies across various accents.", "Insights into user perceptions of accent variations and their impacts on digital inclusion.", "Recommendations for inclusive design and regulation of AI speech technologies."], "limitations": "The focus is limited to only two AI voice services and may not fully capture all accents or technologies available in the market.", "keywords": ["AI speech generation", "voice cloning", "linguistic privilege", "accent discrimination", "inclusive design"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.11112", "pdf": "https://arxiv.org/pdf/2506.11112.pdf", "abs": "https://arxiv.org/abs/2506.11112", "title": "Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)", "authors": ["Christine Bauer", "Li Chen", "Nicola Ferro", "Norbert Fuhr", "Avishek Anand", "Timo Breuer", "Guglielmo Faggioli", "Ophir Frieder", "Hideo Joho", "Jussi Karlgren", "Johannes Kiesel", "Bart P. Knijnenburg", "Aldo Lipani", "Lien Michiels", "Andrea Papenmeier", "Maria Soledad Pera", "Mark Sanderson", "Scott Sanner", "Benno Stein", "Johanne R. Trippas", "Karin Verspoor", "Martijn C Willemsen"], "categories": ["cs.CL", "cs.HC", "cs.IR"], "comment": "43 pages; 10 figures; Dagstuhl manifesto", "summary": "During the workshop, we deeply discussed what CONversational Information\nACcess (CONIAC) is and its unique features, proposing a world model abstracting\nit, and defined the Conversational Agents Framework for Evaluation (CAFE) for\nthe evaluation of CONIAC systems, consisting of six major components: 1) goals\nof the system's stakeholders, 2) user tasks to be studied in the evaluation, 3)\naspects of the users carrying out the tasks, 4) evaluation criteria to be\nconsidered, 5) evaluation methodology to be applied, and 6) measures for the\nquantitative criteria chosen.", "AI": {"tldr": "This paper discusses the definition and framework for evaluating Conversational Information Access (CONIAC) systems through the Conversational Agents Framework for Evaluation (CAFE).", "motivation": "To define and evaluate the unique features of Conversational Information Access (CONIAC) systems.", "method": "The paper proposes a world model for CONIAC and outlines the CAFE framework, consisting of six components for evaluation: stakeholder goals, user tasks, user aspects, evaluation criteria, evaluation methodology, and quantitative measures.", "result": "The CAFE framework provides a structured approach for evaluating CONIAC systems, focusing on several critical components that influence their effectiveness.", "conclusion": "The structured evaluation framework aims to enhance understanding and improve the design of CONIAC systems.", "key_contributions": ["Definition of Conversational Information Access (CONIAC)", "Introduction of the CAFE framework for evaluation", "Identification of six components essential for evaluating CONIAC systems."], "limitations": "", "keywords": ["Conversational Information Access", "evaluation framework", "Conversational Agents", "HCI", "user tasks"], "importance_score": 5, "read_time_minutes": 30}}
{"id": "2506.11113", "pdf": "https://arxiv.org/pdf/2506.11113.pdf", "abs": "https://arxiv.org/abs/2506.11113", "title": "Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks", "authors": ["Tzu-Ling Lin", "Wei-Chih Chen", "Teng-Fang Hsiao", "Hou-I Liu", "Ya-Hsin Yeh", "Yu Kai Chan", "Wen-Sheng Lien", "Po-Yen Kuo", "Philip S. Yu", "Hong-Han Shuai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Peer review is essential for maintaining academic quality, but the increasing\nvolume of submissions places a significant burden on reviewers. Large language\nmodels (LLMs) offer potential assistance in this process, yet their\nsusceptibility to textual adversarial attacks raises reliability concerns. This\npaper investigates the robustness of LLMs used as automated reviewers in the\npresence of such attacks. We focus on three key questions: (1) The\neffectiveness of LLMs in generating reviews compared to human reviewers. (2)\nThe impact of adversarial attacks on the reliability of LLM-generated reviews.\n(3) Challenges and potential mitigation strategies for LLM-based review. Our\nevaluation reveals significant vulnerabilities, as text manipulations can\ndistort LLM assessments. We offer a comprehensive evaluation of LLM performance\nin automated peer reviewing and analyze its robustness against adversarial\nattacks. Our findings emphasize the importance of addressing adversarial risks\nto ensure AI strengthens, rather than compromises, the integrity of scholarly\ncommunication.", "AI": {"tldr": "This paper evaluates the robustness of LLMs in automated peer review against adversarial attacks, highlighting vulnerabilities that affect their reliability.", "motivation": "To address the burden on human reviewers and explore the potential of LLMs for automated peer review amidst concerns regarding their reliability under adversarial conditions.", "method": "The study compares the effectiveness of LLM-generated reviews to those of human reviewers and assesses the impact of adversarial attacks on these reviews, identifying mitigation strategies.", "result": "The evaluation shows significant vulnerabilities of LLMs, indicating that adversarial text manipulations can distort their assessments, risking the integrity of the review process.", "conclusion": "Addressing adversarial risks is crucial for ensuring that AI enhances rather than undermines the quality of academic peer review.", "key_contributions": ["Evaluation of LLMs as automated reviewers", "Identification of vulnerabilities to adversarial attacks", "Proposition of challenges and mitigation strategies for LLM-based reviews"], "limitations": "", "keywords": ["Large Language Models", "Peer Review", "Adversarial Attacks", "Automated Review", "Academic Integrity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.11114", "pdf": "https://arxiv.org/pdf/2506.11114.pdf", "abs": "https://arxiv.org/abs/2506.11114", "title": "KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations", "authors": ["Junyu Liu", "Kaiqi Yan", "Tianyang Wang", "Qian Niu", "Momoko Nagai-Tanima", "Tomoki Aoyama"], "categories": ["cs.CL", "cs.AI"], "comment": "9pages, 3 figures", "summary": "Recent advances in large language models (LLMs) have demonstrated notable\nperformance in medical licensing exams. However, comprehensive evaluation of\nLLMs across various healthcare roles, particularly in high-stakes clinical\nscenarios, remains a challenge. Existing benchmarks are typically text-based,\nEnglish-centric, and focus primarily on medicines, which limits their ability\nto assess broader healthcare knowledge and multimodal reasoning. To address\nthese gaps, we introduce KokushiMD-10, the first multimodal benchmark\nconstructed from ten Japanese national healthcare licensing exams. This\nbenchmark spans multiple fields, including Medicine, Dentistry, Nursing,\nPharmacy, and allied health professions. It contains over 11588 real exam\nquestions, incorporating clinical images and expert-annotated rationales to\nevaluate both textual and visual reasoning. We benchmark over 30\nstate-of-the-art LLMs, including GPT-4o, Claude 3.5, and Gemini, across both\ntext and image-based settings. Despite promising results, no model consistently\nmeets passing thresholds across domains, highlighting the ongoing challenges in\nmedical AI. KokushiMD-10 provides a comprehensive and linguistically grounded\nresource for evaluating and advancing reasoning-centric medical AI across\nmultilingual and multimodal clinical tasks.", "AI": {"tldr": "KokushiMD-10 is a multimodal benchmark for evaluating large language models in healthcare, constructed from Japanese national healthcare licensing exams, offering over 11588 questions and highlighting ongoing challenges in medical AI.", "motivation": "To comprehensively evaluate large language models across various healthcare roles in clinical scenarios, addressing limitations of existing benchmarks that are text-based and English-centric.", "method": "Introduction of KokushiMD-10, a benchmark sourced from ten Japanese national healthcare licensing exams, spanning multiple medical fields and incorporating clinical images and rationales.", "result": "Benchmarking of over 30 state-of-the-art LLMs revealed that no model consistently meets passing thresholds across the assessed domains, indicating challenges in medical AI performance.", "conclusion": "KokushiMD-10 serves as a resource for evaluating and improving reasoning-centric medical AI in multilingual and multimodal contexts.", "key_contributions": ["Introduction of a comprehensive multimodal benchmark in healthcare.", "Inclusion of real exam questions, clinical images, and expert annotations for evaluation.", "Assessment of multiple LLMs across text and image modalities."], "limitations": "No model consistently passing across domains, indicating ongoing challenges in LLM performance in healthcare.", "keywords": ["Multimodal benchmark", "Healthcare licensing exams", "Large language models", "Multilingual AI", "Clinical reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.11115", "pdf": "https://arxiv.org/pdf/2506.11115.pdf", "abs": "https://arxiv.org/abs/2506.11115", "title": "Incorporating Domain Knowledge into Materials Tokenization", "authors": ["Yerim Oh", "Jun-Hyung Park", "Junho Kim", "SungHo Kim", "SangKeun Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While language models are increasingly utilized in materials science, typical\nmodels rely on frequency-centric tokenization methods originally developed for\nnatural language processing. However, these methods frequently produce\nexcessive fragmentation and semantic loss, failing to maintain the structural\nand semantic integrity of material concepts. To address this issue, we propose\nMATTER, a novel tokenization approach that integrates material knowledge into\ntokenization. Based on MatDetector trained on our materials knowledge base and\na re-ranking method prioritizing material concepts in token merging, MATTER\nmaintains the structural integrity of identified material concepts and prevents\nfragmentation during tokenization, ensuring their semantic meaning remains\nintact. The experimental results demonstrate that MATTER outperforms existing\ntokenization methods, achieving an average performance gain of $4\\%$ and $2\\%$\nin the generation and classification tasks, respectively. These results\nunderscore the importance of domain knowledge for tokenization strategies in\nscientific text processing. Our code is available at\nhttps://github.com/yerimoh/MATTER", "AI": {"tldr": "This paper presents MATTER, a novel tokenization approach that integrates material knowledge to enhance semantic integrity and reduce fragmentation in materials science text processing.", "motivation": "Traditional tokenization methods used in NLP do not adequately preserve the structural and semantic integrity of material concepts in scientific texts, leading to issues in downstream tasks.", "method": "MATTER combines material knowledge from a trained model named MatDetector with a re-ranking technique that focuses on material concepts during token merging.", "result": "Experiments show MATTER outperforms existing tokenization methods, achieving a 4% improvement in generation tasks and 2% in classification tasks.", "conclusion": "The findings emphasize the significance of incorporating domain knowledge in tokenization for effective scientific text processing.", "key_contributions": ["Introduction of MATTER, a novel tokenization method for materials science.", "Integration of domain-specific knowledge into the tokenization process.", "Demonstrated performance improvements over traditional methods in scientific tasks."], "limitations": "The study is focused on materials science, which may limit the generalizability of the approach to other domains.", "keywords": ["tokenization", "materials science", "domain knowledge", "semantic integrity", "NLP"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.11116", "pdf": "https://arxiv.org/pdf/2506.11116.pdf", "abs": "https://arxiv.org/abs/2506.11116", "title": "Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models", "authors": ["Jijie Li", "Li Du", "Hanyu Zhao", "Bo-wen Zhang", "Liangdong Wang", "Boyan Gao", "Guang Liu", "Yonghua Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong performance in real-world\napplications, yet existing open-source instruction datasets often concentrate\non narrow domains, such as mathematics or coding, limiting generalization and\nwidening the gap with proprietary models. To bridge this gap, we introduce\nInfinity-Instruct, a high-quality instruction dataset designed to enhance both\nfoundational and chat capabilities of LLMs through a two-phase pipeline. In\nPhase 1, we curate 7.4M high-quality foundational instructions\n(InfInstruct-F-7.4M) from over 100M samples using hybrid data selection\ntechniques. In Phase 2, we synthesize 1.5M high-quality chat instructions\n(InfInstruct-G-1.5M) through a two-stage process involving instruction\nselection, evolution, and diagnostic filtering. We empirically evaluate\nInfinity-Instruct by fine-tuning several open-source models, including Mistral,\nLLaMA, Qwen, and Yi, and observe substantial performance gains across both\nfoundational and instruction following benchmarks, consistently surpassing\nofficial instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B\noutperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving\ncomparable foundational performance. These results underscore the synergy\nbetween foundational and chat training and offer new insights into holistic LLM\ndevelopment. Our\ndataset\\footnote{https://huggingface.co/datasets/BAAI/Infinity-Instruct} and\ncodes\\footnote{https://gitee.com/li-touch/infinity-instruct} have been publicly\nreleased.", "AI": {"tldr": "Infinity-Instruct is a high-quality instruction dataset aimed at improving LLMs' foundational and chat capabilities, curated through a two-phase pipeline.", "motivation": "To address the limitations of existing open-source instruction datasets that focus on narrow domains and to improve the performance of LLMs in real-world applications.", "method": "The paper introduces a two-phase pipeline: Phase 1 curates 7.4M foundational instructions from 100M samples using hybrid data selection, and Phase 2 synthesizes 1.5M chat instructions through selection, evolution, and filtering.", "result": "Empirical evaluations show substantial performance gains for models fine-tuned with Infinity-Instruct, surpassing official instruction-tuned models, with InfInstruct-LLaMA3.1-70B outperforming GPT-4-0314 by 8.6% on instruction following tasks.", "conclusion": "The results suggest a beneficial synergy between foundational and chat training in LLM development, providing new insights into creating effective instruction datasets.", "key_contributions": ["Introduction of a novel instruction dataset, Infinity-Instruct, enhancing LLM performance.", "Demonstration of significant performance improvements when fine-tuning with the dataset.", "Insights into the relationship between foundational and chat model training."], "limitations": "", "keywords": ["Large Language Models", "Instruction Datasets", "Machine Learning", "Human-Computer Interaction", "Chat Models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.11117", "pdf": "https://arxiv.org/pdf/2506.11117.pdf", "abs": "https://arxiv.org/abs/2506.11117", "title": "ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research", "authors": ["Junyong Lin", "Lu Dai", "Ruiqian Han", "Yijie Sui", "Ruilin Wang", "Xingliang Sun", "Qinglin Wu", "Min Feng", "Hao Liu", "Hui Xiong"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "KDD 2025 Accepted", "summary": "Scientific researchers need intensive information about datasets to\neffectively evaluate and develop theories and methodologies. The information\nneeds regarding datasets are implicitly embedded in particular research tasks,\nrather than explicitly expressed in search queries. However, existing\nscientific retrieval and question-answering (QA) datasets typically address\nstraightforward questions, which do not align with the distribution of\nreal-world research inquiries. To bridge this gap, we developed ScIRGen, a\ndataset generation framework for scientific QA \\& retrieval that more\naccurately reflects the information needs of professional science researchers,\nand uses it to create a large-scale scientific retrieval-augmented generation\n(RAG) dataset with realistic queries, datasets and papers. Technically, we\ndesigned a dataset-oriented information extraction method that leverages\nacademic papers to augment the dataset representation. We then proposed a\nquestion generation framework by employing cognitive taxonomy to ensure the\nquality of synthesized questions. We also design a method to automatically\nfilter synthetic answers based on the perplexity shift of LLMs, which is highly\naligned with human judgment of answers' validity. Collectively, these\nmethodologies culminated in the creation of the 61k QA dataset, ScIRGen-Geo. We\nbenchmarked representative methods on the ScIRGen-Geo dataset for their\nquestion-answering and retrieval capabilities, finding out that current methods\nstill suffer from reasoning from complex questions. This work advances the\ndevelopment of more sophisticated tools to support the intricate information\nneeds of the scientific community.", "AI": {"tldr": "ScIRGen is a framework for generating scientific QA and retrieval datasets that better reflect the actual information needs of researchers.", "motivation": "To address the misalignment between existing scientific retrieval datasets and the complex information needs of researchers, often reflected in their research tasks rather than explicit queries.", "method": "ScIRGen employs a dataset-oriented information extraction method, utilizes cognitive taxonomy for question generation, and filters synthetic answers using LLM perplexity shifts to create a large-scale scientific retrieval-augmented generation dataset.", "result": "Created the ScIRGen-Geo dataset, containing 61,000 QA pairs, and benchmarked existing methods on this dataset, revealing significant challenges in reasoning complex questions.", "conclusion": "The ScIRGen framework enhances the development of tools for supporting scientific research by generating datasets that closely reflect real-world information needs.", "key_contributions": ["Development of ScIRGen framework for scientific QA and retrieval", "Creation of ScIRGen-Geo, a large-scale dataset with 61k QA pairs", "Methodology for filtering answers based on LLM perplexity shifts"], "limitations": "Current methods still struggle with reasoning from complex questions, indicating areas for further improvement.", "keywords": ["scientific QA", "dataset generation", "information retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11119", "pdf": "https://arxiv.org/pdf/2506.11119.pdf", "abs": "https://arxiv.org/abs/2506.11119", "title": "Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech", "authors": ["Jingyu Li", "Lingchao Mao", "Hairong Wang", "Zhendong Wang", "Xi Mao", "Xuelei Sherry Ni"], "categories": ["cs.CL", "cs.SD", "eess.AS", "68T10 (Primary), 68U99 (Secondary)", "I.2.1; J.3"], "comment": null, "summary": "Background: Alzheimer's disease and related dementias (ADRD) are progressive\nneurodegenerative conditions where early detection is vital for timely\nintervention and care. Spontaneous speech contains rich acoustic and linguistic\nmarkers that may serve as non-invasive biomarkers for cognitive decline.\nFoundation models, pre-trained on large-scale audio or text data, produce\nhigh-dimensional embeddings encoding contextual and acoustic features.\n  Methods: We used the PREPARE Challenge dataset, which includes audio\nrecordings from over 1,600 participants with three cognitive statuses: healthy\ncontrol (HC), mild cognitive impairment (MCI), and Alzheimer's Disease (AD). We\nexcluded non-English, non-spontaneous, or poor-quality recordings. The final\ndataset included 703 (59.13%) HC, 81 (6.81%) MCI, and 405 (34.06%) AD cases. We\nbenchmarked a range of open-source foundation speech and language models to\nclassify cognitive status into the three categories.\n  Results: The Whisper-medium model achieved the highest performance among\nspeech models (accuracy = 0.731, AUC = 0.802). Among language models, BERT with\npause annotation performed best (accuracy = 0.662, AUC = 0.744). ADRD detection\nusing state-of-the-art automatic speech recognition (ASR) model-generated audio\nembeddings outperformed others. Including non-semantic features like pause\npatterns consistently improved text-based classification.\n  Conclusion: This study introduces a benchmarking framework using foundation\nmodels and a clinically relevant dataset. Acoustic-based approaches --\nparticularly ASR-derived embeddings -- demonstrate strong potential for\nscalable, non-invasive, and cost-effective early detection of ADRD.", "AI": {"tldr": "This paper benchmarks foundation speech and language models for early detection of Alzheimer's disease and related dementias (ADRD) using audio data from a large dataset.", "motivation": "The importance of early detection of Alzheimer's disease and related dementias (ADRD) to facilitate timely intervention and care, leveraging spontaneous speech as potential biomarkers.", "method": "Used the PREPARE Challenge dataset comprising audio recordings from over 1,600 participants with varying cognitive statuses (HC, MCI, AD), benchmarked multiple open-source speech and language models for classification of cognitive status.", "result": "The Whisper-medium model achieved the highest accuracy at 0.731 and AUC of 0.802, while BERT with pause annotation showed an accuracy of 0.662 and AUC of 0.744. ASR-generated audio embeddings outperformed other methods, and incorporating pause patterns improved classification results.", "conclusion": "The study demonstrates the potential of acoustic-based approaches for scalable, non-invasive early detection of ADRD, establishing a benchmarking framework with a clinically relevant dataset.", "key_contributions": ["Benchmarking framework for foundation models in ADRD detection", "Demonstration of ASR-generated embeddings for cognitive status classification", "Inclusion of non-semantic features like pause patterns to enhance model performance"], "limitations": "", "keywords": ["Alzheimer's Disease", "Cognitive Decline", "Speech Recognition", "Foundation Models", "Early Detection"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.11120", "pdf": "https://arxiv.org/pdf/2506.11120.pdf", "abs": "https://arxiv.org/abs/2506.11120", "title": "SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models", "authors": ["Hourun Zhu", "Chengchao Shen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In spite of strong performance achieved by LLMs, the costs of their\ndeployment are unaffordable. For the compression of LLMs, gradient-based\npruning methods present promising effectiveness. However, in these methods, the\ngradient computation with one-hot labels ignore the potential predictions on\nother words, thus missing key information for generative capability of the\noriginal model. To address this issue, we introduce a self-distillation loss\nduring the pruning phase (rather than post-training) to fully exploit the\npredictions of the original model, thereby obtaining more accurate gradient\ninformation for pruning. Moreover, we find that, compared to attention modules,\nthe predictions of LLM are less sensitive to multilayer perceptron (MLP)\nmodules, which take up more than $5 \\times$ parameters (LLaMA3.2-1.2B). To this\nend, we focus on the pruning of MLP modules, to significantly compress LLM\nwithout obvious performance degradation. Experimental results on extensive\nzero-shot benchmarks demonstrate that our method significantly outperforms\nexisting pruning methods. Furthermore, our method achieves very competitive\nperformance among 1B-scale open source LLMs. The source code and trained\nweights are available at https://github.com/visresearch/SDMPrune.", "AI": {"tldr": "This paper proposes a self-distillation loss during the pruning phase of LLMs to improve gradient information for pruning MLP modules, leading to significant compression without performance loss.", "motivation": "To reduce the high costs of deploying large language models (LLMs) while maintaining their performance, particularly focusing on the effectiveness of gradient-based pruning methods.", "method": "Introducing a self-distillation loss during the pruning phase to leverage the original model's predictions, particularly targeting the pruning of multilayer perceptron (MLP) modules in LLMs.", "result": "The proposed method outperforms existing pruning methods and achieves competitive performance on benchmarks among 1B-scale open source LLMs.", "conclusion": "Utilizing self-distillation in the pruning phase allows for significant model compression with minimal impact on performance.", "key_contributions": ["Self-distillation loss during pruning to enhance gradient information", "Focus on MLP module pruning for effective compression", "Outperformance of existing pruning methods on zero-shot benchmarks"], "limitations": "", "keywords": ["LLMs", "pruning", "self-distillation", "MLP", "compression"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.21015", "pdf": "https://arxiv.org/pdf/2412.21015.pdf", "abs": "https://arxiv.org/abs/2412.21015", "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based QA Datasets", "authors": ["Mahir Labib Dihan", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025 (Demo)", "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.", "AI": {"tldr": "MapQaTor is an open-source framework for creating geospatial question answering datasets, improving reliability and speed of the data annotation process.", "motivation": "To improve the handling of natural language geospatial queries by integrating Large Language Models with mapping services.", "method": "MapQaTor allows seamless integration with maps APIs, centralizes data retrieval and visualization, and caches API responses for consistent data.", "result": "The evaluation shows that MapQaTor accelerates the annotation process by at least 30 times compared to manual methods, facilitating the development of complex geospatial datasets.", "conclusion": "MapQaTor presents a significant advancement in evaluating LLM-based geospatial reasoning and improving their capabilities for geospatial understanding.", "key_contributions": ["Introduction of an open-source framework for geospatial QA datasets", "Integration with various maps APIs for data gathering", "Significant speedup in the annotation process by 30 times compared to manual methods"], "limitations": "", "keywords": ["Geospatial QA", "Large Language Models", "Data Annotation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.11121", "pdf": "https://arxiv.org/pdf/2506.11121.pdf", "abs": "https://arxiv.org/abs/2506.11121", "title": "SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR", "authors": ["Wei-Ping Huang", "Guan-Ting Lin", "Hung-yi Lee"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Despite progress in end-to-end ASR, real-world domain mismatches still cause\nperformance drops, which Test-Time Adaptation (TTA) aims to mitigate by\nadjusting models during inference. Recent work explores combining TTA with\nexternal language models, using techniques like beam search rescoring or\ngenerative error correction. In this work, we identify a previously overlooked\nchallenge: TTA can interfere with language model rescoring, revealing the\nnontrivial nature of effectively combining the two methods. Based on this\ninsight, we propose SUTA-LM, a simple yet effective extension of SUTA, an\nentropy-minimization-based TTA approach, with language model rescoring. SUTA-LM\nfirst applies a controlled adaptation process guided by an auto-step selection\nmechanism leveraging both acoustic and linguistic information, followed by\nlanguage model rescoring to refine the outputs. Experiments on 18 diverse ASR\ndatasets show that SUTA-LM achieves robust results across a wide range of\ndomains.", "AI": {"tldr": "This paper introduces SUTA-LM, a novel Test-Time Adaptation method that effectively integrates entropy-minimization with language model rescoring to improve ASR performance across different domains.", "motivation": "To address performance drops in ASR due to real-world domain mismatches and to explore the interaction between Test-Time Adaptation and language model rescoring.", "method": "SUTA-LM combines a controlled adaptation process using entropy minimization and an auto-step selection mechanism, followed by language model rescoring to optimize outputs.", "result": "Experiments demonstrate that SUTA-LM consistently achieves better performance across 18 diverse ASR datasets compared to previous methods.", "conclusion": "SUTA-LM effectively mitigates the challenges of combining TTA with language model rescoring in ASR applications.", "key_contributions": ["Introduction of SUTA-LM for effective ASR improvement", "Demonstration of the challenges between TTA and language model rescoring", "Robust experimental results across multiple ASR datasets"], "limitations": "Potential limitations of SUTA-LM were not discussed in the abstract.", "keywords": ["ASR", "Test-Time Adaptation", "language model rescoring", "entropy-minimization", "domain adaptation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.11125", "pdf": "https://arxiv.org/pdf/2506.11125.pdf", "abs": "https://arxiv.org/abs/2506.11125", "title": "ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams", "authors": ["Freddie Grabovski", "Gilad Gressel", "Yisroel Mirsky"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), combined with Text-to-Speech (TTS) and\nAutomatic Speech Recognition (ASR), are increasingly used to automate voice\nphishing (vishing) scams. These systems are scalable and convincing, posing a\nsignificant security threat. We identify the ASR transcription step as the most\nvulnerable link in the scam pipeline and introduce ASRJam, a proactive defence\nframework that injects adversarial perturbations into the victim's audio to\ndisrupt the attacker's ASR. This breaks the scam's feedback loop without\naffecting human callers, who can still understand the conversation. While prior\nadversarial audio techniques are often unpleasant and impractical for real-time\nuse, we also propose EchoGuard, a novel jammer that leverages natural\ndistortions, such as reverberation and echo, that are disruptive to ASR but\ntolerable to humans. To evaluate EchoGuard's effectiveness and usability, we\nconducted a 39-person user study comparing it with three state-of-the-art\nattacks. Results show that EchoGuard achieved the highest overall utility,\noffering the best combination of ASR disruption and human listening experience.", "AI": {"tldr": "The paper presents ASRJam, a defense framework against voice phishing scams that disrupts attacker ASR without affecting human callers, utilizing EchoGuard, a novel jammer leveraging natural distortions to ensure usability.", "motivation": "Voice phishing scams using LLMs, TTS, and ASR pose a significant security threat; the ASR transcription step is identified as the most vulnerable link.", "method": "The authors propose ASRJam, which injects adversarial perturbations into victims' audio, and introduce EchoGuard, a jammer that utilizes natural distortions (reverberation and echo) to disrupt ASR while remaining tolerable for humans.", "result": "EchoGuard achieved the highest overall utility in a user study, providing an effective balance of ASR disruption and human listening experience compared to three state-of-the-art attacks.", "conclusion": "The ASRJam framework and EchoGuard can effectively defend against voice phishing without compromising user experience, marking a significant step in audio security.", "key_contributions": ["Introduction of ASRJam as a proactive defense against vishing", "Development of EchoGuard utilizing natural audio distortions", "User study demonstrating superior effectiveness and usability of EchoGuard"], "limitations": "The effectiveness of the approach may vary depending on the specific attack method used and the audio environment.", "keywords": ["Voice Phishing", "Adversarial Perturbations", "ASR", "TTS", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.11127", "pdf": "https://arxiv.org/pdf/2506.11127.pdf", "abs": "https://arxiv.org/abs/2506.11127", "title": "GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions", "authors": ["Wenkang Han", "Zhixiong Zeng", "Jing Huang", "Shu Jiang", "Liming Zheng", "Longrong Yang", "Haibo Qiu", "Chang Yao", "Jingyuan Chen", "Lin Ma"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing\nhuman-computer interaction, yet their reliance on text-based instructions\nimposes limitations on accessibility and convenience, particularly in\nhands-free scenarios. To address this gap, we propose GUIRoboTron-Speech, the\nfirst end-to-end autonomous GUI agent that directly accepts speech instructions\nand on-device screenshots to predict actions. Confronted with the scarcity of\nspeech-based GUI agent datasets, we initially generated high-quality speech\ninstructions for training by leveraging a random timbre text-to-speech (TTS)\nmodel to convert existing text instructions. We then develop\nGUIRoboTron-Speech's capabilities through progressive grounding and planning\ntraining stages. A key contribution is a heuristic mixed-instruction training\nstrategy designed to mitigate the modality imbalance inherent in pre-trained\nfoundation models. Comprehensive experiments on several benchmark datasets\nvalidate the robust and superior performance of GUIRoboTron-Speech,\ndemonstrating the significant potential and widespread applicability of speech\nas an effective instruction modality for driving GUI agents. Our code and\ndatasets are available at https://github.com/GUIRoboTron/GUIRoboTron-Speech.", "AI": {"tldr": "GUIRoboTron-Speech is the first end-to-end autonomous GUI agent that accepts speech instructions and screenshots, addressing limitations in accessibility and convenience in hands-free interfaces.", "motivation": "To improve accessibility and convenience in human-computer interaction by allowing users to control GUIs via speech, especially in hands-free scenarios.", "method": "Generated high-quality speech instructions using a text-to-speech model and developed GUIRoboTron-Speech through progressive training stages and a mixed-instruction training strategy to address modality imbalances.", "result": "Experimental validation on benchmark datasets shows that GUIRoboTron-Speech outperforms existing methods in effectively interpreting speech instructions for GUI interactions.", "conclusion": "The findings indicate the significant potential for using speech as an effective instruction modality for GUI agents, making them more accessible and user-friendly.", "key_contributions": ["First end-to-end autonomous GUI agent that uses speech instructions directly", "Novel mixed-instruction training strategy to address modality imbalance", "Validation on benchmark datasets demonstrating superior performance"], "limitations": "", "keywords": ["Autonomous agents", "Graphical User Interfaces", "Speech instructions", "Human-computer interaction", "Mixed-instruction training"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.11128", "pdf": "https://arxiv.org/pdf/2506.11128.pdf", "abs": "https://arxiv.org/abs/2506.11128", "title": "Stronger Language Models Produce More Human-Like Errors", "authors": ["Andrew Keenan Richardson", "Ryan Othniel Kearns", "Sean Moss", "Vincent Wang-Mascianica", "Philipp Koralus"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Do language models converge toward human-like reasoning patterns as they\nimprove? We provide surprising evidence that while overall reasoning\ncapabilities increase with model sophistication, the nature of errors\nincreasingly mirrors predictable human reasoning fallacies: a previously\nunobserved inverse scaling phenomenon. To investigate this question, we apply\nthe Erotetic Theory of Reasoning (ETR), a formal cognitive framework with\nempirical support for predicting human reasoning outcomes. Using the\nopen-source package PyETR, we generate logical reasoning problems where humans\npredictably err, evaluating responses from 38 language models across 383\nreasoning tasks. Our analysis indicates that as models advance in general\ncapability (as measured by Chatbot Arena scores), the proportion of their\nincorrect answers that align with ETR-predicted human fallacies tends to\nincrease ($\\rho = 0.360, p = 0.0265$). Notably, as we observe no correlation\nbetween model sophistication and logical correctness on these tasks, this shift\nin error patterns toward human-likeness occurs independently of error rate.\nThese findings challenge the prevailing view that scaling language models\nnaturally obtains normative rationality, suggesting instead a convergence\ntoward human-like cognition inclusive of our characteristic biases and\nlimitations, as we further confirm by demonstrating order-effects in language\nmodel reasoning.", "AI": {"tldr": "The paper investigates whether language models exhibit human-like reasoning patterns as they improve, revealing an increase in error types that align with human reasoning fallacies.", "motivation": "To explore the relationship between the sophistication of language models and their reasoning patterns, particularly if they emulate human-like reasoning fallacies.", "method": "The study employs the Erotetic Theory of Reasoning (ETR) to generate logical reasoning tasks, analyzing responses from 38 language models on 383 problems to assess error patterns.", "result": "As language models become more capable, their errors increasingly reflect predicted human reasoning fallacies without a corresponding improvement in logical correctness.", "conclusion": "The findings suggest that advancements in language models may lead them toward mimicking human cognitive biases rather than achieving normative rationality.", "key_contributions": ["Introduces the concept of inverse scaling in language model reasoning errors.", "Applies the Erotetic Theory of Reasoning to language models in a novel context.", "Demonstrates that increased sophistication in models correlates with greater alignment to human-like reasoning fallacies."], "limitations": "The analysis may be constrained by the specific types of reasoning tasks selected and the particular language models evaluated.", "keywords": ["language models", "human reasoning", "cognitive bias", "Erotetic Theory of Reasoning", "inverse scaling"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11129", "pdf": "https://arxiv.org/pdf/2506.11129.pdf", "abs": "https://arxiv.org/abs/2506.11129", "title": "Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK", "authors": ["Carlos Garcia-Fernandez", "Luis Felipe", "Monique Shotande", "Muntasir Zitu", "Aakash Tripathi", "Ghulam Rasool", "Issam El Naqa", "Vivek Rudrapatna", "Gilmer Valdes"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) show promise in healthcare, but hallucinations\nremain a major barrier to clinical use. We present CHECK, a continuous-learning\nframework that integrates structured clinical databases with a classifier\ngrounded in information theory to detect both factual and reasoning-based\nhallucinations. Evaluated on 1500 questions from 100 pivotal clinical trials,\nCHECK reduced LLama3.3-70B-Instruct hallucination rates from 31% to 0.3% -\nmaking an open source model state of the art. Its classifier generalized across\nmedical benchmarks, achieving AUCs of 0.95-0.96, including on the MedQA (USMLE)\nbenchmark and HealthBench realistic multi-turn medical questioning. By\nleveraging hallucination probabilities to guide GPT-4o's refinement and\njudiciously escalate compute, CHECK boosted its USMLE passing rate by 5\npercentage points, achieving a state-of-the-art 92.1%. By suppressing\nhallucinations below accepted clinical error thresholds, CHECK offers a\nscalable foundation for safe LLM deployment in medicine and other high-stakes\ndomains.", "AI": {"tldr": "CHECK is a framework that reduces hallucinations in LLMs for healthcare applications, achieving a state-of-the-art performance in clinical question answering.", "motivation": "Hallucinations in large language models pose significant challenges to their reliability and safety in clinical settings.", "method": "CHECK integrates structured clinical databases with an information theory-based classifier to detect hallucinations, evaluated on clinical trial questions.", "result": "CHECK reduced the hallucination rate of LLama3.3-70B-Instruct from 31% to 0.3% and improved GPT-4o's USMLE passing rate by 5 percentage points to 92.1%.", "conclusion": "By effectively managing hallucinations, CHECK provides a scalable solution for deploying LLMs in high-stakes medical applications.", "key_contributions": ["Introduced a continuous-learning framework for LLMs in healthcare.", "Achieved state-of-the-art hallucination reduction rates.", "Improved performance on clinical benchmarks and USMLE exam."], "limitations": "", "keywords": ["large language models", "hallucinations", "healthcare", "clinical trials", "machine learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.11130", "pdf": "https://arxiv.org/pdf/2506.11130.pdf", "abs": "https://arxiv.org/abs/2506.11130", "title": "A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data", "authors": ["Cheng Kang Chou", "Chan-Jan Hsu", "Ho-Lam Chung", "Liang-Hsuan Tseng", "Hsi-Chun Cheng", "Yu-Kuan Fu", "Kuan Po Huang", "Hung-Yi Lee"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "We propose a self-refining framework that enhances ASR performance with only\nunlabeled datasets. The process starts with an existing ASR model generating\npseudo-labels on unannotated speech, which are then used to train a\nhigh-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs\nare bootstrapped into the original ASR system, completing the closed-loop\nself-improvement cycle. We demonstrated the effectiveness of the framework on\nTaiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a\nmoderate amount of text data, and synthetic content from the AI models, we\nadapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error\nrates by up to 20% on Mandarin and 50% on Mandarin-English code-switching\nbenchmarks compared to Whisper. Results highlight the framework as a compelling\nalternative to pseudo-labeling self-distillation approaches and provides a\npractical pathway for improving ASR performance in low-resource or\ndomain-specific settings.", "AI": {"tldr": "A framework to enhance ASR performance using unlabeled datasets by integrating pseudo-labeling and high-fidelity TTS systems.", "motivation": "To improve Automatic Speech Recognition (ASR) performance in low-resource or domain-specific settings without the need for labeled data.", "method": "The proposed method involves using an existing ASR model to generate pseudo-labels from unannotated speech, which are then employed to train a text-to-speech system. The synthesized speech text pairs are fed back into the ASR system for self-improvement.", "result": "The framework enhanced performance on Taiwanese Mandarin speech, achieving a reduction in error rates of up to 20% for Mandarin and 50% for Mandarin-English code-switching compared to the baseline model, Whisper.", "conclusion": "The self-refining framework provides an effective alternative to traditional pseudo-labeling and self-distillation methods, particularly advantageous in low-resource environments.", "key_contributions": ["Development of Twister, a specialized ASR model for Mandarin", "Demonstration of a closed-loop self-improvement cycle using unlabeled datasets", "Significant reduction in error rates for specific language contexts"], "limitations": "", "keywords": ["Automatic Speech Recognition", "self-refining framework", "pseudo-labels", "text-to-speech", "low-resource settings"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.11135", "pdf": "https://arxiv.org/pdf/2506.11135.pdf", "abs": "https://arxiv.org/abs/2506.11135", "title": "Large Language Models and Emergence: A Complex Systems Perspective", "authors": ["David C. Krakauer", "John W. Krakauer", "Melanie Mitchell"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "Emergence is a concept in complexity science that describes how many-body\nsystems manifest novel higher-level properties, properties that can be\ndescribed by replacing high-dimensional mechanisms with lower-dimensional\neffective variables and theories. This is captured by the idea \"more is\ndifferent\". Intelligence is a consummate emergent property manifesting\nincreasingly efficient -- cheaper and faster -- uses of emergent capabilities\nto solve problems. This is captured by the idea \"less is more\". In this paper,\nwe first examine claims that Large Language Models exhibit emergent\ncapabilities, reviewing several approaches to quantifying emergence, and\nsecondly ask whether LLMs possess emergent intelligence.", "AI": {"tldr": "This paper explores the notion of emergence in complexity science as it relates to intelligence, particularly in Large Language Models (LLMs), examining claims of emergent capabilities and intelligence in LLMs.", "motivation": "To investigate whether Large Language Models demonstrate emergent capabilities and intelligence, as described in complexity science.", "method": "The paper reviews various approaches to quantifying emergence in many-body systems and applies these concepts to analyze LLMs.", "result": "The analysis reveals complexities in defining and measuring emergent capabilities in LLMs, with varied findings regarding their intelligence.", "conclusion": "Emergent capabilities in LLMs may not necessarily equate to true emergent intelligence, warranting further investigation into the implications of these findings.", "key_contributions": ["Investigates the relationship between emergence and intelligence in LLMs.", "Reviews methods of quantifying emergence relevant to LLMs.", "Challenges existing claims of emergent intelligence in LLMs."], "limitations": "The study highlights the difficulties in defining and measuring emergence in complex systems, especially in the context of LLMs.", "keywords": ["emergence", "Large Language Models", "intelligence", "complexity science", "emergent capabilities"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.11137", "pdf": "https://arxiv.org/pdf/2506.11137.pdf", "abs": "https://arxiv.org/abs/2506.11137", "title": "Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models", "authors": ["Chong Shao", "Douglas Snyder", "Chiran Li", "Bowen Gu", "Kerry Ngan", "Chun-Ting Yang", "Jiageng Wu", "Richard Wyss", "Kueiyu Joshua Lin", "Jie Yang"], "categories": ["cs.CL"], "comment": "preprint, under review", "summary": "Identifying medication discontinuations in electronic health records (EHRs)\nis vital for patient safety but is often hindered by information being buried\nin unstructured notes. This study aims to evaluate the capabilities of advanced\nopen-sourced and proprietary large language models (LLMs) in extracting\nmedications and classifying their medication status from EHR notes, focusing on\ntheir scalability on medication information extraction without human\nannotation. We collected three EHR datasets from diverse sources to build the\nevaluation benchmark. We evaluated 12 advanced LLMs and explored multiple LLM\nprompting strategies. Performance on medication extraction, medication status\nclassification, and their joint task (extraction then classification) was\nsystematically compared across all experiments. We found that LLMs showed\npromising performance on the medication extraction and discontinuation\nclassification from EHR notes. GPT-4o consistently achieved the highest average\nF1 scores in all tasks under zero-shot setting - 94.0% for medication\nextraction, 78.1% for discontinuation classification, and 72.7% for the joint\ntask. Open-sourced models followed closely, Llama-3.1-70B-Instruct achieved the\nhighest performance in medication status classification on the MIV-Med dataset\n(68.7%) and in the joint task on both the Re-CASI (76.2%) and MIV-Med (60.2%)\ndatasets. Medical-specific LLMs demonstrated lower performance compared to\nadvanced general-domain LLMs. Few-shot learning generally improved performance,\nwhile CoT reasoning showed inconsistent gains. LLMs demonstrate strong\npotential for medication extraction and discontinuation identification on EHR\nnotes, with open-sourced models offering scalable alternatives to proprietary\nsystems and few-shot can further improve LLMs' capability.", "AI": {"tldr": "This paper evaluates the effectiveness of advanced LLMs in extracting medication information from EHR notes, finding strong performance, particularly from GPT-4o and open-sourced models.", "motivation": "To improve patient safety by accurately identifying medication discontinuations in electronic health records (EHRs), which is often complicated by unstructured note formats.", "method": "The study involved an evaluation of 12 advanced LLMs on three EHR datasets, using various prompting strategies and comparing performance in medication extraction and status classification.", "result": "GPT-4o achieved the highest average F1 scores in medication extraction (94.0%) and classification tasks under zero-shot settings, while open-sourced models like Llama-3.1-70B-Instruct also performed competitively in specific categories.", "conclusion": "Advanced LLMs exhibit strong potential for accurately extracting medication information and identifying discontinuations in EHR notes, with open-sourced models presenting scalable alternatives to proprietary solutions.", "key_contributions": ["Evaluation of 12 advanced LLMs for medication extraction from EHRs", "Identification of effective prompting strategies for LLMs", "Demonstration of the potential of few-shot learning to enhance LLM performance"], "limitations": "", "keywords": ["medication extraction", "EHR", "large language models", "machine learning", "health informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.11243", "pdf": "https://arxiv.org/pdf/2506.11243.pdf", "abs": "https://arxiv.org/abs/2506.11243", "title": "RETUYT-INCO at BEA 2025 Shared Task: How Far Can Lightweight Models Go in AI-powered Tutor Evaluation?", "authors": ["Santiago Góngora", "Ignacio Sastre", "Santiago Robaina", "Ignacio Remersaro", "Luis Chiruzzo", "Aiala Rosá"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper will be presented at the 20th BEA Workshop (Innovative Use\n  of NLP for Building Educational Applications) at ACL 2025", "summary": "In this paper, we present the RETUYT-INCO participation at the BEA 2025\nshared task. Our participation was characterized by the decision of using\nrelatively small models, with fewer than 1B parameters. This self-imposed\nrestriction tries to represent the conditions in which many research labs or\ninstitutions are in the Global South, where computational power is not easily\naccessible due to its prohibitive cost. Even under this restrictive\nself-imposed setting, our models managed to stay competitive with the rest of\nteams that participated in the shared task. According to the $exact\\ F_1$\nscores published by the organizers, the performance gaps between our models and\nthe winners were as follows: $6.46$ in Track 1; $10.24$ in Track 2; $7.85$ in\nTrack 3; $9.56$ in Track 4; and $13.13$ in Track 5. Considering that the\nminimum difference with a winner team is $6.46$ points -- and the maximum\ndifference is $13.13$ -- according to the $exact\\ F_1$ score, we find that\nmodels with a size smaller than 1B parameters are competitive for these tasks,\nall of which can be run on computers with a low-budget GPU or even without a\nGPU.", "AI": {"tldr": "The RETUYT-INCO team achieved competitive results in the BEA 2025 shared task using models under 1B parameters, demonstrating the viability of smaller models in low-resource settings.", "motivation": "To explore the effectiveness of small models in natural language processing tasks, particularly in contexts with limited computational resources, typical of institutions in the Global South.", "method": "The team utilized models with fewer than 1 billion parameters to participate in various tracks of the BEA 2025 shared task, benchmarking against teams with larger models.", "result": "The team's models were competitive, with performance gaps of 6.46 to 13.13 points in F1 scores compared to the winning teams across five tracks.", "conclusion": "Models under 1 billion parameters can effectively compete in specific NLP tasks, proving beneficial for low-resource environments.", "key_contributions": ["Demonstrated the competitiveness of small models in NLP tasks", "Provided benchmarks for models under 1B parameters in shared tasks", "Highlighted opportunities for low-resource institutions in NLP research"], "limitations": "The study focuses solely on models under 1B parameters, which may not generalize to other model sizes or settings.", "keywords": ["NLP", "Low-resource models", "F1 score", "BEA 2025", "Educational applications"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.11244", "pdf": "https://arxiv.org/pdf/2506.11244.pdf", "abs": "https://arxiv.org/abs/2506.11244", "title": "Iterative Multilingual Spectral Attribute Erasure", "authors": ["Shun Shao", "Yftah Ziser", "Zheng Zhao", "Yifu Qiu", "Shay B. Cohen", "Anna Korhonen"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures", "summary": "Multilingual representations embed words with similar meanings to share a\ncommon semantic space across languages, creating opportunities to transfer\ndebiasing effects between languages. However, existing methods for debiasing\nare unable to exploit this opportunity because they operate on individual\nlanguages. We present Iterative Multilingual Spectral Attribute Erasure\n(IMSAE), which identifies and mitigates joint bias subspaces across multiple\nlanguages through iterative SVD-based truncation. Evaluating IMSAE across eight\nlanguages and five demographic dimensions, we demonstrate its effectiveness in\nboth standard and zero-shot settings, where target language data is\nunavailable, but linguistically similar languages can be used for debiasing.\nOur comprehensive experiments across diverse language models (BERT, LLaMA,\nMistral) show that IMSAE outperforms traditional monolingual and cross-lingual\napproaches while maintaining model utility.", "AI": {"tldr": "This paper introduces IMSAE, a method for debiasing multilingual representations using iterative SVD-based truncation to mitigate bias across languages.", "motivation": "The need to transfer debiasing effects between languages to enhance multilingual representations and improve model fairness.", "method": "IMSAE utilizes iterative SVD-based truncation to identify and mitigate joint bias subspaces across multiple languages, evaluated in both standard and zero-shot settings.", "result": "IMSAE demonstrates effectiveness across eight languages and five demographic dimensions, outperforming traditional monolingual and cross-lingual approaches.", "conclusion": "IMSAE effectively reduces biases in multilingual embeddings while preserving model utility, offering a significant improvement over existing methods.", "key_contributions": ["Introduction of IMSAE for joint bias mitigation in multilingual representations.", "Demonstration of effectiveness in standard and zero-shot settings across multiple languages.", "Comparative evaluation showing outperforming of traditional approaches."], "limitations": "", "keywords": ["multilingual representation", "debiasing", "iterative SVD", "bias mitigation", "language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11246", "pdf": "https://arxiv.org/pdf/2506.11246.pdf", "abs": "https://arxiv.org/abs/2506.11246", "title": "No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning", "authors": ["Kushagra Dixit", "Abhishek Rajgaria", "Harshavardhan Kalalbandi", "Dan Roth", "Vivek Gupta"], "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 19 Tables, 9 Figures", "summary": "Temporal Table Reasoning is a critical challenge for Large Language Models\n(LLMs), requiring effective prompting techniques to extract relevant insights.\nDespite existence of multiple prompting methods, their impact on table\nreasoning remains largely unexplored. Furthermore, the performance of these\nmodels varies drastically across different table and context structures, making\nit difficult to determine an optimal approach. This work investigates multiple\nprompting technique across diverse table types to determine optimal approaches\nfor different scenarios. We find that performance varies based on entity type,\ntable structure, requirement of additional context and question complexity,\nwith NO single method consistently outperforming others. To mitigate these\nchallenges, we introduce SEAR, an adaptive prompting framework inspired by\nhuman reasoning that dynamically adjusts based on context characteristics and\nintegrates a structured reasoning. Our results demonstrate that SEAR achieves\nsuperior performance across all table types compared to other baseline\nprompting techniques. Additionally, we explore the impact of table structure\nrefactoring, finding that a unified representation enhances model's reasoning.", "AI": {"tldr": "This paper investigates the effectiveness of various prompting techniques for table reasoning in Large Language Models (LLMs) and introduces SEAR, an adaptive framework that improves performance across diverse table types.", "motivation": "Temporal Table Reasoning poses a challenge for LLMs, necessitating effective prompting techniques to derive relevant insights from tables, yet existing methods' impacts are largely unexplored.", "method": "The paper explores multiple prompting techniques across different table types and structures, introducing SEAR as a context-sensitive adaptive framework for improving table reasoning.", "result": "SEAR outperforms baseline prompting techniques across all table types, demonstrating that performance is influenced by entity type, table structure, context requirements, and question complexity.", "conclusion": "An adaptive, structured reasoning approach like SEAR can significantly enhance LLMs' reasoning capabilities with tables, suggesting that there is no one-size-fits-all method, and context plays a crucial role.", "key_contributions": ["Introduction of SEAR, an adaptive prompting framework for LLMs.", "Comprehensive analysis of prompting methods across various table types.", "Discovery that a unified table representation improves reasoning performance."], "limitations": "No single method consistently outperformed others across all tests, highlighting the complexity of table reasoning tasks.", "keywords": ["Large Language Models", "prompting techniques", "table reasoning", "adaptive frameworks", "human reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.11274", "pdf": "https://arxiv.org/pdf/2506.11274.pdf", "abs": "https://arxiv.org/abs/2506.11274", "title": "Learning a Continue-Thinking Token for Enhanced Test-Time Scaling", "authors": ["Liran Ringel", "Elad Tolochinsky", "Yaniv Romano"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time scaling has emerged as an effective approach for improving language\nmodel performance by utilizing additional compute at inference time. Recent\nstudies have shown that overriding end-of-thinking tokens (e.g., replacing\n\"</think>\" with \"Wait\") can extend reasoning steps and improve accuracy. In\nthis work, we explore whether a dedicated continue-thinking token can be\nlearned to trigger extended reasoning. We augment a distilled version of\nDeepSeek-R1 with a single learned \"<|continue-thinking|>\" token, training only\nits embedding via reinforcement learning while keeping the model weights\nfrozen. Our experiments show that this learned token achieves improved accuracy\non standard math benchmarks compared to both the baseline model and a test-time\nscaling approach that uses a fixed token (e.g., \"Wait\") for budget forcing. In\nparticular, we observe that in cases where the fixed-token approach enhances\nthe base model's accuracy, our method achieves a markedly greater improvement.\nFor example, on the GSM8K benchmark, the fixed-token approach yields a 1.3%\nabsolute improvement in accuracy, whereas our learned-token method achieves a\n4.2% improvement over the base model that does not use budget forcing.", "AI": {"tldr": "This paper explores a learned continue-thinking token for extending reasoning in language models, showing improved accuracy over fixed-token approaches.", "motivation": "To improve language model performance at inference time by extending reasoning steps using tokens.", "method": "The authors use a distilled version of DeepSeek-R1, training a learned '<|continue-thinking|>' token's embedding via reinforcement learning with frozen model weights.", "result": "The learned token shows greater improvement in accuracy on math benchmarks compared to a baseline and a fixed-token approach. For example, a 4.2% improvement on the GSM8K benchmark versus a 1.3% improvement from the fixed token.", "conclusion": "A dedicated learned continue-thinking token effectively enhances reasoning capabilities and leads to significant gains in model accuracy during inference.", "key_contributions": ["Introduction of a learned continue-thinking token", "Comparison with fixed-token strategies", "Demonstrated improvements on math benchmarks"], "limitations": "", "keywords": ["language models", "inference", "reasoning", "reinforcement learning", "accuracy"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2506.11300", "pdf": "https://arxiv.org/pdf/2506.11300.pdf", "abs": "https://arxiv.org/abs/2506.11300", "title": "Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning", "authors": ["Yang Zhang", "Amr Mohamed", "Hadi Abdine", "Guokan Shang", "Michalis Vazirgiannis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Curriculum learning has shown promise in improving training efficiency and\ngeneralization in various machine learning domains, yet its potential in\npretraining language models remains underexplored, prompting our work as the\nfirst systematic investigation in this area. We experimented with different\nsettings, including vanilla curriculum learning, pacing-based sampling, and\ninterleaved curricula-guided by six difficulty metrics spanning linguistic and\ninformation-theoretic perspectives. We train models under these settings and\nevaluate their performance on eight diverse benchmarks. Our experiments reveal\nthat curriculum learning consistently improves convergence in early and\nmid-training phases, and can yield lasting gains when used as a warmup strategy\nwith up to $3.5\\%$ improvement. Notably, we identify compression ratio, lexical\ndiversity, and readability as effective difficulty signals across settings. Our\nfindings highlight the importance of data ordering in large-scale pretraining\nand provide actionable insights for scalable, data-efficient model development\nunder realistic training scenarios.", "AI": {"tldr": "This paper investigates the benefits of curriculum learning in pretraining language models, demonstrating improved training efficiency and generalization across multiple settings and benchmarks.", "motivation": "To explore the underutilized potential of curriculum learning in pretraining language models, given its success in other machine learning domains.", "method": "The study experimented with different forms of curriculum learning, including vanilla curriculum learning, pacing-based sampling, and interleaved curricula, using various difficulty metrics—linguistic and information-theoretic—to train models.", "result": "Curriculum learning was found to improve convergence during the early and mid-training phases of language models, leading to substantial gains in performance, with up to a 3.5% improvement when used as a warmup strategy.", "conclusion": "The research emphasizes the significance of data ordering in large-scale pretraining, revealing effective difficulty signals and providing guidance for more efficient model training.", "key_contributions": ["First systematic investigation of curriculum learning in language model pretraining.", "Identification of effective difficulty metrics such as compression ratio, lexical diversity, and readability.", "Demonstration of improved training efficiency and generalization through curriculum learning."], "limitations": "", "keywords": ["curriculum learning", "language models", "training efficiency", "pretraining", "data ordering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11305", "pdf": "https://arxiv.org/pdf/2506.11305.pdf", "abs": "https://arxiv.org/abs/2506.11305", "title": "Don't Pay Attention", "authors": ["Mohammad Hammoud", "Devang Acharya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Transformer has become the de facto standard for large language models\nand a wide range of downstream tasks across various domains. Despite its\nnumerous advantages like inherent training parallelism, the Transformer still\nfaces key challenges due to its inability to effectively process sequences\nbeyond a fixed context window and the quadratic complexity of its attention\nmechanism. These challenges have renewed interest in RNN-like architectures,\nwhich offer linear scaling with sequence length and improved handling of\nlong-range dependencies, albeit with limited parallelism due to their\ninherently recurrent nature. In this paper, we propose Avey, a new neural\nfoundational architecture that breaks away from both attention and recurrence.\nAvey comprises a ranker and an autoregressive neural processor, which\ncollaboratively identify and contextualize only the most relevant tokens for\nany given token, regardless of their positions in the sequence. Specifically,\nAvey decouples sequence length from context width, thus enabling effective\nprocessing of arbitrarily long sequences. Experimental results show that Avey\ncompares favorably to the Transformer across a variety of standard short-range\nNLP benchmarks, while notably excelling at capturing long-range dependencies.", "AI": {"tldr": "Avey is a new neural architecture that improves upon Transformers and RNNs by decoupling sequence length from context width, enabling effective processing of long sequences and capturing long-range dependencies.", "motivation": "The Transformer is widely used in NLP but struggles with long sequences due to fixed context windows and quadratic attention complexity, leading to a resurgence of interest in RNN-like architectures.", "method": "Avey introduces a ranker and an autoregressive neural processor that focus on the most relevant tokens for each token in a sequence, independent of their positions.", "result": "Avey demonstrates superior performance to Transformers on various short-range NLP benchmarks and shows significant improvement in handling long-range dependencies.", "conclusion": "The proposed Avey architecture provides an effective alternative to both Transformers and RNNs for processing long sequences in NLP tasks.", "key_contributions": ["Introduction of Avey, a novel neural architecture that circumvents traditional attention and recurrence mechanisms.", "Ability to process arbitrarily long sequences by decoupling sequence length from context width.", "Experimental validation showing Avey's effectiveness in both short-range and long-range NLP tasks."], "limitations": "", "keywords": ["Transformer", "RNN", "long-range dependencies", "NLP benchmarks", "neural architecture"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11338", "pdf": "https://arxiv.org/pdf/2506.11338.pdf", "abs": "https://arxiv.org/abs/2506.11338", "title": "Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly", "authors": ["Yi-Chien Lin", "William Schuler"], "categories": ["cs.CL"], "comment": null, "summary": "As Transformers become more widely incorporated into natural language\nprocessing tasks, there has been considerable interest in using surprisal from\nthese models as predictors of human sentence processing difficulty. Recent work\nhas observed a positive relationship between Transformer-based models'\nperplexity and the predictive power of their surprisal estimates on reading\ntimes, showing that language models with more parameters and trained on more\ndata are less predictive of human reading times. However, these studies focus\non predicting latency-based measures (i.e., self-paced reading times and\neye-gaze durations) with surprisal estimates from Transformer-based language\nmodels. This trend has not been tested on brain imaging data. This study\ntherefore evaluates the predictive power of surprisal estimates from 17\npre-trained Transformer-based models across three different language families\non two functional magnetic resonance imaging datasets. Results show that the\npositive relationship between model perplexity and model fit still obtains,\nsuggesting that this trend is not specific to latency-based measures and can be\ngeneralized to neural measures.", "AI": {"tldr": "This study evaluates the predictive power of surprisal estimates from Transformer-based language models on brain imaging data, finding a continued positive relationship between model perplexity and model fit.", "motivation": "The research investigates the predictive power of surprisal from Transformer models beyond latency-based measures in the context of human sentence processing, focusing on brain imaging data.", "method": "The study assesses 17 pre-trained Transformer-based models across three language families and evaluates their surprisal estimates against two functional magnetic resonance imaging datasets.", "result": "Results indicate that the positive relationship between model perplexity and predictive power holds true for neural measures as well, suggesting generalizability beyond previous latency measures.", "conclusion": "Surprisal estimates from larger models are less predictive of human processing difficulty, confirming that trends observed in latency measures apply to brain imaging data.", "key_contributions": ["Evaluates Transformer model surprisal in the context of brain imaging data.", "Demonstrates generalizability of model perplexity trends beyond reading times.", "Explores the relationship between language model parameters and human processing difficulty in a novel way."], "limitations": "", "keywords": ["Transformers", "surprisal", "natural language processing", "neural measures", "functional MRI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11343", "pdf": "https://arxiv.org/pdf/2506.11343.pdf", "abs": "https://arxiv.org/abs/2506.11343", "title": "From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review", "authors": ["Yaohui Zhang", "Haijing Zhang", "Wenlong Ji", "Tianyu Hua", "Nick Haber", "Hancheng Cao", "Weixin Liang"], "categories": ["cs.CL"], "comment": null, "summary": "The advent of large language models (LLMs) offers unprecedented opportunities\nto reimagine peer review beyond the constraints of traditional workflows.\nDespite these opportunities, prior efforts have largely focused on replicating\ntraditional review workflows with LLMs serving as direct substitutes for human\nreviewers, while limited attention has been given to exploring new paradigms\nthat fundamentally rethink how LLMs can participate in the academic review\nprocess. In this paper, we introduce and explore a novel mechanism that employs\nLLM agents to perform pairwise comparisons among manuscripts instead of\nindividual scoring. By aggregating outcomes from substantial pairwise\nevaluations, this approach enables a more accurate and robust measure of\nrelative manuscript quality. Our experiments demonstrate that this comparative\napproach significantly outperforms traditional rating-based methods in\nidentifying high-impact papers. However, our analysis also reveals emergent\nbiases in the selection process, notably a reduced novelty in research topics\nand an increased institutional imbalance. These findings highlight both the\ntransformative potential of rethinking peer review with LLMs and critical\nchallenges that future systems must address to ensure equity and diversity.", "AI": {"tldr": "This paper explores a novel approach to peer review using LLM agents for pairwise comparisons of manuscripts, demonstrating improved accuracy over traditional methods but highlighting issues of bias.", "motivation": "To rethink the academic peer review process leveraging the capabilities of large language models (LLMs) beyond traditional workflows.", "method": "Utilizes LLM agents to perform pairwise comparisons among manuscripts, aggregating outcomes to assess relative quality instead of individual scoring.", "result": "This pairwise evaluation approach shows significant improvements in identifying high-impact papers compared to traditional rating systems.", "conclusion": "The study notes both the potential for transformation in peer review through LLMs and the need to address biases that may emerge, such as reduced novelty in topics and institutional imbalances.", "key_contributions": ["Introduces pairwise comparisons using LLMs in academic peer review.", "Demonstrates significant performance improvements over traditional methods.", "Identifies emergent biases in the peer review selection process."], "limitations": "Emergent biases related to novelty of research topics and institutional imbalance in selection.", "keywords": ["Large Language Models", "Peer Review", "Pairwise Comparison", "Academic Publishing", "Bias"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.11344", "pdf": "https://arxiv.org/pdf/2506.11344.pdf", "abs": "https://arxiv.org/abs/2506.11344", "title": "Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models", "authors": ["Peilin Wu", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": null, "summary": "We present a novel approach to Speaker Diarization (SD) by leveraging\ntext-based methods focused on Sentence-level Speaker Change Detection within\ndialogues. Unlike audio-based SD systems, which are often challenged by audio\nquality and speaker similarity, our approach utilizes the dialogue transcript\nalone. Two models are developed: the Single Prediction Model (SPM) and the\nMultiple Prediction Model (MPM), both of which demonstrate significant\nimprovements in identifying speaker changes, particularly in short\nconversations. Our findings, based on a curated dataset encompassing diverse\nconversational scenarios, reveal that the text-based SD approach, especially\nthe MPM, performs competitively against state-of-the-art audio-based SD\nsystems, with superior performance in short conversational contexts. This paper\nnot only showcases the potential of leveraging linguistic features for SD but\nalso highlights the importance of integrating semantic understanding into SD\nsystems, opening avenues for future research in multimodal and semantic\nfeature-based diarization.", "AI": {"tldr": "A novel text-based approach to Speaker Diarization (SD) that focuses on Sentence-level Speaker Change Detection, using dialogue transcripts instead of audio.", "motivation": "To address the limitations of audio-based speaker diarization systems, which struggle with audio quality and speaker similarity.", "method": "Development of two models: Single Prediction Model (SPM) and Multiple Prediction Model (MPM) that utilize dialogue transcripts for identifying speaker changes.", "result": "The text-based approach, particularly the MPM model, shows significant improvements in detecting speaker changes, especially in short conversations, and competes well with traditional audio-based systems.", "conclusion": "This work demonstrates the effectiveness of linguistic features in speaker diarization and emphasizes the integration of semantic understanding, suggesting new research directions for multimodal diarization.", "key_contributions": ["Introduction of a text-based diarization approach", "Development of SPM and MPM models", "Validation of the MPM's superior performance in short conversations"], "limitations": "", "keywords": ["Speaker Diarization", "Text-based model", "Semantic understanding"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.11361", "pdf": "https://arxiv.org/pdf/2506.11361.pdf", "abs": "https://arxiv.org/abs/2506.11361", "title": "The Biased Samaritan: LLM biases in Perceived Kindness", "authors": ["Jack H Fagan", "Ruhaan Juyaal", "Amy Yue-Ming Yu", "Siya Pun"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "While Large Language Models (LLMs) have become ubiquitous in many fields,\nunderstanding and mitigating LLM biases is an ongoing issue. This paper\nprovides a novel method for evaluating the demographic biases of various\ngenerative AI models. By prompting models to assess a moral patient's\nwillingness to intervene constructively, we aim to quantitatively evaluate\ndifferent LLMs' biases towards various genders, races, and ages. Our work\ndiffers from existing work by aiming to determine the baseline demographic\nidentities for various commercial models and the relationship between the\nbaseline and other demographics. We strive to understand if these biases are\npositive, neutral, or negative, and the strength of these biases. This paper\ncan contribute to the objective assessment of bias in Large Language Models and\ngive the user or developer the power to account for these biases in LLM output\nor in training future LLMs. Our analysis suggested two key findings: that\nmodels view the baseline demographic as a white middle-aged or young adult\nmale; however, a general trend across models suggested that non-baseline\ndemographics are more willing to help than the baseline. These methodologies\nallowed us to distinguish these two biases that are often tangled together.", "AI": {"tldr": "This paper presents a method for evaluating demographic biases in Large Language Models (LLMs) by analyzing their responses to moral interventions across various demographics.", "motivation": "Understanding and mitigating biases in LLMs is crucial for responsible AI development, impacting fairness and equity in AI outputs.", "method": "The authors evaluate LLM biases by prompting models to assess a moral patient's willingness to intervene constructively across different demographics (genders, races, ages).", "result": "The analysis revealed that LLMs perceive the baseline demographic as predominantly white, male, and middle-aged, while non-baseline demographics were generally more altruistic.", "conclusion": "The findings provide insights into LLM biases and suggest methodological approaches for future evaluations, emphasizing the need to address demographic biases in AI training and outputs.", "key_contributions": ["Novel method for evaluating LLM demographic biases", "Identification of baseline demographic perceptions", "Insight into altruism across different demographics"], "limitations": "The study primarily focuses on moral intervention scenarios and may not encompass all bias types or contexts.", "keywords": ["Large Language Models", "bias assessment", "demographic evaluation", "generative AI", "moral intervention"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.11381", "pdf": "https://arxiv.org/pdf/2506.11381.pdf", "abs": "https://arxiv.org/abs/2506.11381", "title": "A Variational Approach for Mitigating Entity Bias in Relation Extraction", "authors": ["Samuel Mensah", "Elena Kochkina", "Jabez Magomere", "Joy Prakash Sain", "Simerjot Kaur", "Charese Smiley"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main", "summary": "Mitigating entity bias is a critical challenge in Relation Extraction (RE),\nwhere models often rely excessively on entities, resulting in poor\ngeneralization. This paper presents a novel approach to address this issue by\nadapting a Variational Information Bottleneck (VIB) framework. Our method\ncompresses entity-specific information while preserving task-relevant features.\nIt achieves state-of-the-art performance on relation extraction datasets across\ngeneral, financial, and biomedical domains, in both indomain (original test\nsets) and out-of-domain (modified test sets with type-constrained entity\nreplacements) settings. Our approach offers a robust, interpretable, and\ntheoretically grounded methodology.", "AI": {"tldr": "This paper introduces a Variational Information Bottleneck framework for mitigating entity bias in Relation Extraction, leading to improved generalization and state-of-the-art performance across various domains.", "motivation": "To address the challenge of entity bias in Relation Extraction, which hampers model generalization.", "method": "The paper adapts a Variational Information Bottleneck framework that compresses entity-specific information while retaining task-relevant features.", "result": "The proposed method demonstrates state-of-the-art performance on relation extraction datasets in general, financial, and biomedical domains, across both in-domain and out-of-domain settings.", "conclusion": "The approach provides a robust and interpretable solution to mitigate entity bias in Relation Extraction.", "key_contributions": ["Introduction of a Variational Information Bottleneck framework for Relation Extraction", "Demonstrated state-of-the-art performance in various domains", "Robustness and theoretical grounding of the proposed methodology."], "limitations": "", "keywords": ["Relation Extraction", "Entity Bias", "Variational Information Bottleneck", "Generalization", "Biomedical"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.11389", "pdf": "https://arxiv.org/pdf/2506.11389.pdf", "abs": "https://arxiv.org/abs/2506.11389", "title": "Curriculum-Guided Layer Scaling for Language Model Pretraining", "authors": ["Karanpartap Singh", "Neil Band", "Ehsan Adeli"], "categories": ["cs.CL"], "comment": null, "summary": "As the cost of pretraining large language models grows, there is continued\ninterest in strategies to improve learning efficiency during this core training\nstage. Motivated by cognitive development, where humans gradually build\nknowledge as their brains mature, we propose Curriculum-Guided Layer Scaling\n(CGLS), a framework for compute-efficient pretraining that synchronizes\nincreasing data difficulty with model growth through progressive layer stacking\n(i.e. gradually adding layers during training). At the 100M parameter scale,\nusing a curriculum transitioning from synthetic short stories to general web\ndata, CGLS outperforms baseline methods on the question-answering benchmarks\nPIQA and ARC. Pretraining at the 1.2B scale, we stratify the DataComp-LM corpus\nwith a DistilBERT-based classifier and progress from general text to highly\ntechnical or specialized content. Our results show that progressively\nincreasing model depth alongside sample difficulty leads to better\ngeneralization and zero-shot performance on various downstream benchmarks.\nAltogether, our findings demonstrate that CGLS unlocks the potential of\nprogressive stacking, offering a simple yet effective strategy for improving\ngeneralization on knowledge-intensive and reasoning tasks.", "AI": {"tldr": "Curriculum-Guided Layer Scaling (CGLS) enhances the efficiency of large language model pretraining by synchronizing data difficulty with model growth through progressive layer addition, resulting in improved generalization and performance on downstream benchmarks.", "motivation": "To improve learning efficiency in pretraining large language models, inspired by cognitive development in humans.", "method": "Curriculum-Guided Layer Scaling (CGLS) that progressively adds layers and transitions from simple to complex data during training.", "result": "CGLS outperforms baseline methods on benchmarks like PIQA and ARC, demonstrating improved generalization and zero-shot performance.", "conclusion": "The findings support CGLS as an effective strategy for enhancing performance on knowledge-intensive tasks through progressive stacking of model layers.", "key_contributions": ["Introduced Curriculum-Guided Layer Scaling (CGLS) for pretraining efficiency.", "Demonstrated improved zero-shot performance using progressively complex datasets.", "Showed that model depth can be increased in correlation with data difficulty for better generalization."], "limitations": "", "keywords": ["Curriculum learning", "Layer scaling", "Model pretraining", "Generalization", "Zero-shot learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.11410", "pdf": "https://arxiv.org/pdf/2506.11410.pdf", "abs": "https://arxiv.org/abs/2506.11410", "title": "Predicting Early-Onset Colorectal Cancer with Large Language Models", "authors": ["Wilson Lau", "Youngwon Kim", "Sravanthi Parasa", "Md Enamul Haque", "Anand Oka", "Jay Nanduri"], "categories": ["cs.CL"], "comment": "Paper accepted for the proceedings of the 2025 American Medical\n  Informatics Association Annual Symposium (AMIA)", "summary": "The incidence rate of early-onset colorectal cancer (EoCRC, age < 45) has\nincreased every year, but this population is younger than the recommended age\nestablished by national guidelines for cancer screening. In this paper, we\napplied 10 different machine learning models to predict EoCRC, and compared\ntheir performance with advanced large language models (LLM), using patient\nconditions, lab results, and observations within 6 months of patient journey\nprior to the CRC diagnoses. We retrospectively identified 1,953 CRC patients\nfrom multiple health systems across the United States. The results demonstrated\nthat the fine-tuned LLM achieved an average of 73% sensitivity and 91%\nspecificity.", "AI": {"tldr": "This paper presents a study on using machine learning models, including fine-tuned large language models, to predict early-onset colorectal cancer (EoCRC) in a population younger than typical screening ages.", "motivation": "The increasing incidence of early-onset colorectal cancer (EoCRC) in younger populations necessitates improving prediction methods for early detection and intervention.", "method": "Applied 10 different machine learning models to predict EoCRC using patient conditions, lab results, and observations within 6 months prior to CRC diagnosis, comparing their performance against advanced large language models (LLM).", "result": "The fine-tuned LLM achieved an average sensitivity of 73% and specificity of 91% in predicting EoCRC from data collected prior to CRC diagnosis.", "conclusion": "The study highlights the potential of using advanced LLMs for improving EoCRC prediction, suggesting their utility in clinical settings for early diagnosis.", "key_contributions": ["Demonstration of LLM effectiveness in predicting EoCRC.", "Comparison of traditional ML models with LLMs in a healthcare context.", "Application of machine learning to a significant, emerging public health issue."], "limitations": "The study was retrospective, and results may vary across diverse populations and healthcare systems.", "keywords": ["Colorectal Cancer", "Early-Onset Cancer", "Machine Learning", "Health Informatics", "Large Language Models"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.11418", "pdf": "https://arxiv.org/pdf/2506.11418.pdf", "abs": "https://arxiv.org/abs/2506.11418", "title": "Efficient Long-Context LLM Inference via KV Cache Clustering", "authors": ["Jie Hu", "Shengnan Wang", "Yutong He", "Ping Gong", "Jiawei Yi", "Juncheng Zhang", "Youhui Bai", "Renhai Chen", "Gong Zhang", "Cheng Li", "Kun Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.", "AI": {"tldr": "Chelsea introduces a framework for efficient online KV cache clustering in long-context LLMs, achieving significant memory savings and improved inference speed.", "motivation": "To address deployment challenges posed by the substantial KV cache required for long-context LLMs, while retaining important information for future generations.", "method": "The Chelsea framework uses a Chunked Soft Matching approach that segments the sequence into chunks and identifies clusters based on similarity, then merges KV cache within each cluster into a single centroid.", "result": "Chelsea significantly reduces KV cache memory usage by up to 80% and accelerates decoding speed by up to 3.19x with minimal computational overhead.", "conclusion": "Chelsea provides an efficient solution for optimizing KV cache in long-context LLMs while preserving model performance and reducing latency.", "key_contributions": ["Introduction of Chelsea framework for KV cache clustering", "Chunked Soft Matching for efficient clustering", "Theoretical analysis of computational complexity and optimality"], "limitations": "", "keywords": ["large language models", "KV cache", "online clustering", "machine learning", "inference acceleration"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11425", "pdf": "https://arxiv.org/pdf/2506.11425.pdf", "abs": "https://arxiv.org/abs/2506.11425", "title": "Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards", "authors": ["Jeff Da", "Clinton Wang", "Xiang Deng", "Yuntao Ma", "Nikhil Barhate", "Sean Hendryx"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted\nas the de facto method for enhancing the reasoning capabilities of large\nlanguage models and has demonstrated notable success in verifiable domains like\nmath and competitive programming tasks. However, the efficacy of RLVR\ndiminishes significantly when applied to agentic environments. These settings,\ncharacterized by multi-step, complex problem solving, lead to high failure\nrates even for frontier LLMs, as the reward landscape is too sparse for\neffective model training via conventional RLVR. In this work, we introduce\nAgent-RLVR, a framework that makes RLVR effective in challenging agentic\nsettings, with an initial focus on software engineering tasks. Inspired by\nhuman pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively\nsteers the agent towards successful trajectories by leveraging diverse\ninformational cues. These cues, ranging from high-level strategic plans to\ndynamic feedback on the agent's errors and environmental interactions, emulate\na teacher's guidance, enabling the agent to navigate difficult solution spaces\nand promotes active self-improvement via additional environment exploration. In\nthe Agent-RLVR training loop, agents first attempt to solve tasks to produce\ninitial trajectories, which are then validated by unit tests and supplemented\nwith agent guidance. Agents then reattempt with guidance, and the agent policy\nis updated with RLVR based on the rewards of these guided trajectories.\nAgent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%\nto 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data\nis additionally useful for test-time reward model training, shown by further\nboosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents\nwith RLVR in complex, real-world environments where conventional RL methods\nstruggle.", "AI": {"tldr": "Agent-RLVR introduces a novel framework to enhance Reinforcement Learning from Verifiable Rewards, making it effective in agentic environments with multi-step problem solving, focusing on software engineering tasks.", "motivation": "Conventional RLVR methods struggle in complex settings due to sparse reward landscapes. Agent-RLVR aims to improve agent performance in these environments by introducing a guidance mechanism.", "method": "The framework employs agent guidance, leveraging diverse informational cues to help steer agents toward successful trajectories and actively enhance their problem-solving abilities through a structured training loop.", "result": "Agent-RLVR significantly improved the performance of the Qwen-2.5-72B-Instruct model, increasing pass rates from 9.4% to 22.4% on the SWE-Bench Verified and further to 27.8% with additional training.", "conclusion": "Agent-RLVR sets the foundation for more effective training of agents in complex real-world scenarios where traditional RL methods face challenges.", "key_contributions": ["Introduces agent guidance for effective reinforcement learning in complex environments.", "Demonstrates significant improvement in agent performance on software engineering tasks.", "Lays groundwork for future research in training agents with RL in challenging domains."], "limitations": "", "keywords": ["Reinforcement Learning", "Human-Computer Interaction", "Large Language Models", "Software Engineering", "Agent Guidance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.11432", "pdf": "https://arxiv.org/pdf/2506.11432.pdf", "abs": "https://arxiv.org/abs/2506.11432", "title": "KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models", "authors": ["Taeeun Kim", "Semin Jeong", "Youngsook Song"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 2 figures", "summary": "This research introduces KoGEC, a Korean Grammatical Error Correction system\nusing pre\\--trained translation models. We fine-tuned NLLB (No Language Left\nBehind) models for Korean GEC, comparing their performance against large\nlanguage models like GPT-4 and HCX-3. The study used two social media\nconversation datasets for training and testing. The NLLB models were fine-tuned\nusing special language tokens to distinguish between original and corrected\nKorean sentences. Evaluation was done using BLEU scores and an \"LLM as judge\"\nmethod to classify error types. Results showed that the fine-tuned NLLB (KoGEC)\nmodels outperformed GPT-4o and HCX-3 in Korean GEC tasks. KoGEC demonstrated a\nmore balanced error correction profile across various error types, whereas the\nlarger LLMs tended to focus less on punctuation errors. We also developed a\nChrome extension to make the KoGEC system accessible to users. Finally, we\nexplored token vocabulary expansion to further improve the model but found it\nto decrease model performance. This research contributes to the field of NLP by\nproviding an efficient, specialized Korean GEC system and a new evaluation\nmethod. It also highlights the potential of compact, task-specific models to\ncompete with larger, general-purpose language models in specialized NLP tasks.", "AI": {"tldr": "KoGEC is a Korean Grammatical Error Correction system that outperforms larger LLMs like GPT-4 in correcting Korean sentences.", "motivation": "To develop a specialized error correction system for Korean that outperforms general-purpose language models.", "method": "Fine-tuning of NLLB models for Korean GEC using two social media datasets, evaluated via BLEU scores and LLM-based error classification.", "result": "KoGEC models demonstrated superior performance in Korean grammatical error correction, particularly in balancing different types of errors, compared to GPT-4 and HCX-3.", "conclusion": "KoGEC provides an efficient and specialized GEC solution, showing that compact models can outperform larger LLMs in specific tasks.", "key_contributions": ["Introduction of KoGEC as a Korean GEC system", "Demonstration of fine-tuned NLLB models outperforming other LLMs in GEC", "Development of a Chrome extension for accessibility"], "limitations": "Exploration of token vocabulary expansion decreased model performance.", "keywords": ["Korean GEC", "NLLB", "grammatical error correction", "NLP", "language models"], "importance_score": 4, "read_time_minutes": 11}}
{"id": "2506.11440", "pdf": "https://arxiv.org/pdf/2506.11440.pdf", "abs": "https://arxiv.org/abs/2506.11440", "title": "AbsenceBench: Language Models Can't Tell What's Missing", "authors": ["Harvey Yiyun Fu", "Aryan Shrivastava", "Jared Moore", "Peter West", "Chenhao Tan", "Ari Holtzman"], "categories": ["cs.CL"], "comment": "23 pages, 8 figures. Code and data are publicly available at\n  https://github.com/harvey-fin/absence-bench", "summary": "Large language models (LLMs) are increasingly capable of processing long\ninputs and locating specific information within them, as evidenced by their\nperformance on the Needle in a Haystack (NIAH) test. However, while models\nexcel at recalling surprising information, they still struggle to identify\nclearly omitted information. We introduce AbsenceBench to assesses LLMs'\ncapacity to detect missing information across three domains: numerical\nsequences, poetry, and GitHub pull requests. AbsenceBench asks models to\nidentify which pieces of a document were deliberately removed, given access to\nboth the original and edited contexts. Despite the apparent straightforwardness\nof these tasks, our experiments reveal that even state-of-the-art models like\nClaude-3.7-Sonnet achieve only 69.6% F1-score with a modest average context\nlength of 5K tokens. Our analysis suggests this poor performance stems from a\nfundamental limitation: Transformer attention mechanisms cannot easily attend\nto \"gaps\" in documents since these absences don't correspond to any specific\nkeys that can be attended to. Overall, our results and analysis provide a case\nstudy of the close proximity of tasks where models are already superhuman\n(NIAH) and tasks where models breakdown unexpectedly (AbsenceBench).", "AI": {"tldr": "This paper introduces AbsenceBench, a benchmark for assessing LLMs' ability to detect missing information in various contexts.", "motivation": "To analyze the limitations of large language models in identifying deliberately omitted information, despite their success in recalling surprising data.", "method": "AbsenceBench evaluates models by asking them to determine which pieces of text have been removed from documents across three domains: numerical sequences, poetry, and GitHub pull requests, using original and edited contexts.", "result": "State-of-the-art models achieved only a 69.6% F1-score, indicating a significant shortcoming in their ability to detect absent information.", "conclusion": "The study highlights a fundamental limitation of Transformer mechanisms, suggesting that they struggle to attend to 'gaps' in information due to the nature of their attention mechanism.", "key_contributions": ["Introduction of AbsenceBench for missing information detection", "Analysis of LLM performance in identifying absences", "Discussion of Transformer model limitations in attention mechanisms"], "limitations": "The benchmark may not cover all potential types of missing information and is limited to the specified domains.", "keywords": ["large language models", "missing information", "AbsenceBench", "Transformer attention", "benchmark"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2506.11467", "pdf": "https://arxiv.org/pdf/2506.11467.pdf", "abs": "https://arxiv.org/abs/2506.11467", "title": "A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems", "authors": ["Carlos Rafael Catalan"], "categories": ["cs.CL", "cs.SI", "F.2.2, I.2.7"], "comment": "7 pages, 7 figures, presented at the HEAL Workshop at CHI", "summary": "Human evaluators provide necessary contributions in evaluating large language\nmodels. In the context of Machine Translation (MT) systems for low-resource\nlanguages (LRLs), this is made even more apparent since popular automated\nmetrics tend to be string-based, and therefore do not provide a full picture of\nthe nuances of the behavior of the system. Human evaluators, when equipped with\nthe necessary expertise of the language, will be able to test for adequacy,\nfluency, and other important metrics. However, the low resource nature of the\nlanguage means that both datasets and evaluators are in short supply. This\npresents the following conundrum: How can developers of MT systems for these\nLRLs find adequate human evaluators and datasets? This paper first presents a\ncomprehensive review of existing evaluation procedures, with the objective of\nproducing a design proposal for a platform that addresses the resource gap in\nterms of datasets and evaluators in developing MT systems. The result is a\ndesign for a recruitment and gamified evaluation platform for developers of MT\nsystems. Challenges are also discussed in terms of evaluating this platform, as\nwell as its possible applications in the wider scope of Natural Language\nProcessing (NLP) research.", "AI": {"tldr": "This paper proposes a gamified evaluation platform for Machine Translation systems in low-resource languages, addressing the shortage of human evaluators and datasets.", "motivation": "To address the inadequacies of automated metrics in evaluating Machine Translation systems for low-resource languages, which require expert human evaluators.", "method": "The paper reviews existing evaluation procedures and proposes a design for a platform aimed at recruiting evaluators and providing datasets for MT systems.", "result": "The proposed design is a recruitment and gamified evaluation platform that could help fill the resource gap for MT systems targeting low-resource languages.", "conclusion": "The platform has potential applications in Natural Language Processing research, but challenges in evaluating its effectiveness are noted.", "key_contributions": ["Comprehensive review of evaluation procedures for MT systems in low-resource contexts", "Design proposal for a gamified evaluation platform", "Discussion of challenges in its evaluation and applications in NLP"], "limitations": "Challenges in evaluating the effectiveness of the proposed platform.", "keywords": ["Machine Translation", "Low-Resource Languages", "Human Evaluation", "Natural Language Processing", "Gamified Platform"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.11474", "pdf": "https://arxiv.org/pdf/2506.11474.pdf", "abs": "https://arxiv.org/abs/2506.11474", "title": "Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards", "authors": ["Jaehoon Yun", "Jiwoong Sohn", "Jungwoo Park", "Hyunjae Kim", "Xiangru Tang", "Yanjun Shao", "Yonghoe Koo", "Minhyeok Ko", "Qingyu Chen", "Mark Gerstein", "Michael Moor", "Jaewoo Kang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have shown promise in clinical decision making, but\ncurrent approaches struggle to localize and correct errors at specific steps of\nthe reasoning process. This limitation is critical in medicine, where\nidentifying and addressing reasoning errors is essential for accurate diagnosis\nand effective patient care. We introduce Med-PRM, a process reward modeling\nframework that leverages retrieval-augmented generation to verify each\nreasoning step against established medical knowledge bases. By verifying\nintermediate reasoning steps with evidence retrieved from clinical guidelines\nand literature, our model can precisely assess the reasoning quality in a\nfine-grained manner. Evaluations on five medical QA benchmarks and two\nopen-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art\nperformance, with improving the performance of base models by up to 13.50%\nusing Med-PRM. Moreover, we demonstrate the generality of Med-PRM by\nintegrating it in a plug-and-play fashion with strong policy models such as\nMeerkat, achieving over 80\\% accuracy on MedQA for the first time using\nsmall-scale models of 8 billion parameters. Our code and data are available at:\nhttps://med-prm.github.io/", "AI": {"tldr": "Med-PRM is a framework that improves clinical decision-making by verifying reasoning steps of language models against medical knowledge, achieving state-of-the-art performance in QA and diagnostics.", "motivation": "Current large language models face challenges in localizing and correcting reasoning errors in clinical decision-making, which is vital for accurate diagnoses and effective patient care.", "method": "Med-PRM employs retrieval-augmented generation to validate each reasoning step against established medical knowledge bases.", "result": "Evaluations demonstrate that Med-PRM outperforms baseline models by up to 13.50% across five medical QA benchmarks and two open-ended diagnostic tasks, achieving over 80% accuracy on MedQA with small models.", "conclusion": "Med-PRM enhances the reliability of clinical decision-making processes using language models and can be integrated easily with existing policy models.", "key_contributions": ["Introduction of Med-PRM for process reward modeling in clinical decision-making.", "Demonstrated state-of-the-art performance on medical QA benchmarks.", "Ability to integrate with existing models like Meerkat for improved accuracy."], "limitations": "", "keywords": ["large language models", "clinical decision-making", "retrieval-augmented generation", "reasoning errors", "medical QA"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.11478", "pdf": "https://arxiv.org/pdf/2506.11478.pdf", "abs": "https://arxiv.org/abs/2506.11478", "title": "ImmunoFOMO: Are Language Models missing what oncologists see?", "authors": ["Aman Sinha", "Bogdan-Valentin Popescu", "Xavier Coubez", "Marianne Clausel", "Mathieu Constant"], "categories": ["cs.CL"], "comment": null, "summary": "Language models (LMs) capabilities have grown with a fast pace over the past\ndecade leading researchers in various disciplines, such as biomedical research,\nto increasingly explore the utility of LMs in their day-to-day applications.\nDomain specific language models have already been in use for biomedical natural\nlanguage processing (NLP) applications. Recently however, the interest has\ngrown towards medical language models and their understanding capabilities. In\nthis paper, we investigate the medical conceptual grounding of various language\nmodels against expert clinicians for identification of hallmarks of\nimmunotherapy in breast cancer abstracts. Our results show that pre-trained\nlanguage models have potential to outperform large language models in\nidentifying very specific (low-level) concepts.", "AI": {"tldr": "This paper investigates the capabilities of medical language models in identifying specific concepts related to immunotherapy in breast cancer by comparing them with expert clinicians.", "motivation": "There is growing interest in the utility of language models (LMs), particularly in biomedical applications, including understanding specific medical concepts.", "method": "The study compares various language models' performance in identifying hallmarks of immunotherapy in breast cancer abstracts against expert clinical knowledge.", "result": "Pre-trained language models demonstrated the potential to outperform larger language models in identifying low-level medical concepts.", "conclusion": "The findings suggest that pre-trained LMs can be highly effective in specific biomedical NLP tasks, indicating a need for further exploration in this area.", "key_contributions": ["Comparative analysis of medical language models against clinical expertise.", "Highlighting the effectiveness of pre-trained models in identifying specific medical concepts.", "Insight into the application of LMs in biomedical NLP tasks."], "limitations": "The study is limited to the specific context of immunotherapy in breast cancer and may not generalize to other areas of medicine or broader NLP tasks.", "keywords": ["medical language models", "immunotherapy", "breast cancer", "natural language processing", "biomedical research"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11485", "pdf": "https://arxiv.org/pdf/2506.11485.pdf", "abs": "https://arxiv.org/abs/2506.11485", "title": "Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models", "authors": ["Cole Gawin"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures, 3 tables", "summary": "While large language models like BERT demonstrate strong empirical\nperformance on semantic tasks, whether this reflects true conceptual competence\nor surface-level statistical association remains unclear. I investigate whether\nBERT encodes abstract relational schemata by examining internal representations\nof concept pairs across taxonomic, mereological, and functional relations. I\ncompare BERT's relational classification performance with representational\nstructure in [CLS] token embeddings. Results reveal that pretrained BERT\nenables high classification accuracy, indicating latent relational signals.\nHowever, concept pairs organize by relation type in high-dimensional embedding\nspace only after fine-tuning on supervised relation classification tasks. This\nindicates relational schemata are not emergent from pretraining alone but can\nbe induced via task scaffolding. These findings demonstrate that behavioral\nperformance does not necessarily imply structured conceptual understanding,\nthough models can acquire inductive biases for grounded relational abstraction\nthrough appropriate training.", "AI": {"tldr": "This paper investigates BERT's ability to encode abstract relational schemata and its classification performance on relation types across various embeddings, highlighting that fine-tuning is necessary for structured understanding of relations.", "motivation": "The research explores whether BERT's performance in semantic tasks demonstrates genuine conceptual competence or is merely a result of statistical correlations.", "method": "The study examines internal representations of concept pairs in BERT across different relation types, comparing relational classification performance with the structure of [CLS] token embeddings and assessing the effects of fine-tuning.", "result": "Pretrained BERT shows high classification accuracy, but structured organization of concept pairs by relation type occurs only after fine-tuning on supervised tasks, indicating that the relational understanding is task-induced rather than inherent.", "conclusion": "Behavioral performance of BERT does not guarantee a structured conceptual understanding; nevertheless, with suitable training, BERT can develop biases that promote grounded relational abstraction.", "key_contributions": ["Investigates encoding of relational schemata in BERT", "Demonstrates necessity of fine-tuning for relational understanding", "Clarifies distinction between empirical performance and structured competence"], "limitations": "The paper focuses on BERT and may not generalize to other models; fine-tuning requirements might limit practical applications.", "keywords": ["BERT", "relational schemata", "semantic tasks", "fine-tuning", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.11498", "pdf": "https://arxiv.org/pdf/2506.11498.pdf", "abs": "https://arxiv.org/abs/2506.11498", "title": "Lag-Relative Sparse Attention In Long Context Training", "authors": ["Manlai Liang", "Wanyi Huang", "Mandi Liu", "Huaijun Li", "Jinlong Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.", "AI": {"tldr": "The paper presents Lag-Relative Sparse Attention (LRSA) for improving long-context handling in LLMs without added parameters, enhancing efficiency and performance in question-answer tasks.", "motivation": "To address the limitations of LLMs in handling long-context input due to the quadratic complexity of attention and the issues with key-value cache compression techniques.", "method": "The proposed Lag-Relative Sparse Attention (LRSA) utilizes LagKV compression for long context post-training, focusing on selecting the top K relevant key-value pairs in a lagging window, processed chunk-by-chunk.", "result": "Experimental results demonstrate that LRSA significantly improves LLM robustness with key-value compression and achieves superior performance in fine-tuning for question-answer tasks.", "conclusion": "LRSA fills a critical gap in LLMs' ability to effectively use long-context inputs while maintaining computational efficiency.", "key_contributions": ["Introduces Lag-Relative Sparse Attention (LRSA) for long-context post-training.", "Demonstrates improved efficiency with top K key-value pair selection.", "Achieves better fine-tuned performance for question-answer tasks."], "limitations": "", "keywords": ["Long-context input", "Key-value cache compression", "Lag-Relative Sparse Attention"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.11499", "pdf": "https://arxiv.org/pdf/2506.11499.pdf", "abs": "https://arxiv.org/abs/2506.11499", "title": "On the Effectiveness of Integration Methods for Multimodal Dialogue Response Retrieval", "authors": ["Seongbo Jang", "Seonghyeon Lee", "Dongha Lee", "Hwanjo Yu"], "categories": ["cs.CL"], "comment": "9 pages, 1 figure", "summary": "Multimodal chatbots have become one of the major topics for dialogue systems\nin both research community and industry. Recently, researchers have shed light\non the multimodality of responses as well as dialogue contexts. This work\nexplores how a dialogue system can output responses in various modalities such\nas text and image. To this end, we first formulate a multimodal dialogue\nresponse retrieval task for retrieval-based systems as the combination of three\nsubtasks. We then propose three integration methods based on a two-step\napproach and an end-to-end approach, and compare the merits and demerits of\neach method. Experimental results on two datasets demonstrate that the\nend-to-end approach achieves comparable performance without an intermediate\nstep in the two-step approach. In addition, a parameter sharing strategy not\nonly reduces the number of parameters but also boosts performance by\ntransferring knowledge across the subtasks and the modalities.", "AI": {"tldr": "This paper investigates multimodal chatbots, focusing on how to design systems that can respond using both text and images, comparing various integration methods for response retrieval.", "motivation": "The growing importance of multimodal dialogue systems in both academia and industry necessitates an exploration of effective response generation using diverse modalities.", "method": "The authors formulate a multimodal dialogue response retrieval task as a combination of three subtasks and propose both a two-step and an end-to-end integration method for multimodal response generation.", "result": "Experimental results demonstrate that the end-to-end method matches the performance of the two-step approach while reducing complexity and improving parameter efficiency through knowledge transfer.", "conclusion": "The study shows the potential of a streamlined end-to-end approach for multimodal dialogue systems, highlighting the benefits of parameter sharing and improved performance.", "key_contributions": ["Development of a multimodal dialogue response retrieval task", "Introduction of two integration methods (two-step and end-to-end)", "Demonstration of performance benefits through parameter sharing across modalities"], "limitations": "", "keywords": ["multimodal chatbots", "dialogue systems", "response retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11557", "pdf": "https://arxiv.org/pdf/2506.11557.pdf", "abs": "https://arxiv.org/abs/2506.11557", "title": "From Persona to Person: Enhancing the Naturalness with Multiple Discourse Relations Graph Learning in Personalized Dialogue Generation", "authors": ["Chih-Hao Hsu", "Ying-Jia Lin", "Hung-Yu Kao"], "categories": ["cs.CL"], "comment": "Accepted by PAKDD 2025", "summary": "In dialogue generation, the naturalness of responses is crucial for effective\nhuman-machine interaction. Personalized response generation poses even greater\nchallenges, as the responses must remain coherent and consistent with the\nuser's personal traits or persona descriptions. We propose MUDI\n($\\textbf{Mu}$ltiple $\\textbf{Di}$scourse Relations Graph Learning) for\npersonalized dialogue generation. We utilize a Large Language Model to assist\nin annotating discourse relations and to transform dialogue data into\nstructured dialogue graphs. Our graph encoder, the proposed DialogueGAT model,\nthen captures implicit discourse relations within this structure, along with\npersona descriptions. During the personalized response generation phase, novel\ncoherence-aware attention strategies are implemented to enhance the decoder's\nconsideration of discourse relations. Our experiments demonstrate significant\nimprovements in the quality of personalized responses, thus resembling\nhuman-like dialogue exchanges.", "AI": {"tldr": "The paper introduces MUDI, a new method for generating personalized dialogue responses that maintain coherence and consistency with user traits, using discourse relations graph learning and a specialized graph encoder.", "motivation": "To improve the naturalness and personalization of responses in dialogue generation, addressing the challenges of coherence and user persona alignment.", "method": "MUDI employs a Large Language Model for discourse relation annotation and transforms dialogue data into structured graphs. The DialogueGAT model captures discourse relations and persona descriptions, with coherence-aware attention strategies for response generation.", "result": "Experiments show significant improvements in the quality of personalized dialogue, closely resembling human-like interactions.", "conclusion": "The MUDI approach enhances personalized dialogue generation, effectively capturing and utilizing discourse relations and user personas.", "key_contributions": ["Introduction of MUDI for personalized dialogue generation", "Development of the DialogueGAT model for discourse relation graph encoding", "Implementation of coherence-aware attention strategies in response generation"], "limitations": "", "keywords": ["personalized dialogue generation", "discourse relations", "graph learning", "Large Language Model", "coherence"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.11602", "pdf": "https://arxiv.org/pdf/2506.11602.pdf", "abs": "https://arxiv.org/abs/2506.11602", "title": "Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study", "authors": ["Hawau Olamide Toyin", "Samar M. Magdy", "Hanan Aldarmaki"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We investigate the effectiveness of large language models (LLMs) for text\ndiacritization in two typologically distinct languages: Arabic and Yoruba. To\nenable a rigorous evaluation, we introduce a novel multilingual dataset\nMultiDiac, with diverse samples that capture a range of diacritic ambiguities.\nWe evaluate 14 LLMs varying in size, accessibility, and language coverage, and\nbenchmark them against 6 specialized diacritization models. Additionally, we\nfine-tune four small open-source models using LoRA for Yoruba. Our results show\nthat many off-the-shelf LLMs outperform specialized diacritization models for\nboth Arabic and Yoruba, but smaller models suffer from hallucinations.\nFine-tuning on a small dataset can help improve diacritization performance and\nreduce hallucination rates.", "AI": {"tldr": "This paper evaluates the effectiveness of large language models (LLMs) for text diacritization in Arabic and Yoruba, introducing a multilingual dataset and comparing LLM performance against specialized models.", "motivation": "To assess the performance of LLMs in diacritization tasks and improve upon specialized diacritization models for Arabic and Yoruba using a novel dataset.", "method": "The study introduces the MultiDiac dataset and evaluates 14 LLMs against 6 specialized models, including fine-tuning of 4 small models using LoRA for Yoruba.", "result": "Many off-the-shelf LLMs outperform specialized diacritization models for both languages, but smaller models exhibit hallucinations; fine-tuning on a small dataset improves performance and reduces hallucinations.", "conclusion": "Fine-tuning smaller models can enhance diacritization performance, suggesting that LLMs can be effectively utilized for this task despite some limitations.", "key_contributions": ["Introduction of the MultiDiac dataset for evaluating diacritization in Arabic and Yoruba", "Comparative analysis between LLMs and specialized models", "Demonstration of fine-tuning techniques to improve performance"], "limitations": "Smaller models still suffer from hallucinations despite fine-tuning approaches.", "keywords": ["large language models", "LLMs", "diacritization", "Arabic", "Yoruba"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.11631", "pdf": "https://arxiv.org/pdf/2506.11631.pdf", "abs": "https://arxiv.org/abs/2506.11631", "title": "SceneGram: Conceptualizing and Describing Tangrams in Scene Context", "authors": ["Simeon Junker", "Sina Zarrieß"], "categories": ["cs.CL"], "comment": "To appear in ACL Findings 2025", "summary": "Research on reference and naming suggests that humans can come up with very\ndifferent ways of conceptualizing and referring to the same object, e.g. the\nsame abstract tangram shape can be a \"crab\", \"sink\" or \"space ship\". Another\ncommon assumption in cognitive science is that scene context fundamentally\nshapes our visual perception of objects and conceptual expectations. This paper\ncontributes SceneGram, a dataset of human references to tangram shapes placed\nin different scene contexts, allowing for systematic analyses of the effect of\nscene context on conceptualization. Based on this data, we analyze references\nto tangram shapes generated by multimodal LLMs, showing that these models do\nnot account for the richness and variability of conceptualizations found in\nhuman references.", "AI": {"tldr": "The paper introduces SceneGram, a dataset analyzing how scene context influences human references to tangram shapes, and examines the limitations of multimodal LLMs in capturing these variations.", "motivation": "To investigate how scene context influences conceptualization and naming of objects, and to highlight shortcomings in current multimodal LLMs.", "method": "The authors created the SceneGram dataset of human references to tangram shapes in various scene contexts and performed analysis on references generated by multimodal LLMs.", "result": "Analysis revealed that multimodal LLMs do not adequately capture the diversity of human conceptualizations based on scene context.", "conclusion": "The findings suggest a gap in LLM performance regarding the richness of human-like reference and naming influenced by contextual factors.", "key_contributions": ["Introduction of SceneGram dataset for studying the effects of scene context on conceptualization.", "Analysis highlighting the limitations of multimodal LLMs in reflecting human reference diversity.", "Ins from the dataset can inform improvements in LLM design for better human-like interaction."], "limitations": "The dataset is limited to tangram shapes and may not generalize to other object types or contexts.", "keywords": ["SceneGram", "multimodal LLMs", "human conceptualization", "tangram shapes", "contextual reference"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.11638", "pdf": "https://arxiv.org/pdf/2506.11638.pdf", "abs": "https://arxiv.org/abs/2506.11638", "title": "LoRA-Gen: Specializing Large Language Model via Online LoRA Generation", "authors": ["Yicheng Xiao", "Lin Song", "Rui Yang", "Cheng Cheng", "Yixiao Ge", "Xiu Li", "Ying Shan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances have highlighted the benefits of scaling language models to\nenhance performance across a wide range of NLP tasks. However, these approaches\nstill face limitations in effectiveness and efficiency when applied to\ndomain-specific tasks, particularly for small edge-side models. We propose the\nLoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA\nparameters for edge-side models based on task descriptions. By employing the\nreparameterization technique, we merge the LoRA parameters into the edge-side\nmodel to achieve flexible specialization. Our method facilitates knowledge\ntransfer between models while significantly improving the inference efficiency\nof the specialized model by reducing the input context length. Without\nspecialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which\nachieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in\nreasoning tasks. Besides, our method delivers a compression ratio of 10.1x with\nGemma-2B on intelligent agent tasks.", "AI": {"tldr": "LoRA-Gen leverages a large cloud-side model to generate efficient LoRA parameters for edge-side models, enhancing performance in domain-specific tasks without requiring specialized training.", "motivation": "To address the limitations of effectiveness and efficiency in applying language models to domain-specific tasks, especially for constrained edge-side models.", "method": "The LoRA-Gen framework generates LoRA parameters from a cloud model based on task descriptions, employing reparameterization to merge parameters into edge models for improved specialization and efficiency.", "result": "LoRA-Gen outperforms conventional LoRA fine-tuning, achieving competitive accuracy and significantly increasing inference speed (2.1x speedup) with TinyLLaMA-1.1B, alongside a compression ratio of 10.1x on intelligent agent tasks with Gemma-2B.", "conclusion": "The proposed method allows for effective knowledge transfer and improved inference efficiency, showcasing the potential of cloud-assisted optimization for edge-side language models.", "key_contributions": ["Introduction of LoRA-Gen framework for cloud-to-edge model optimization", "Demonstrated improvements in inference efficiency and speed", "Achieved significant compression ratios on specific tasks"], "limitations": "Limited to the effectiveness of the cloud-side model used for generating parameters; may not generalize across all domain-specific applications.", "keywords": ["LoRA", "edge-side models", "NLP", "cloud model", "inference efficiency"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2506.11666", "pdf": "https://arxiv.org/pdf/2506.11666.pdf", "abs": "https://arxiv.org/abs/2506.11666", "title": "Converting Annotated Clinical Cases into Structured Case Report Forms", "authors": ["Pietro Ferrazzi", "Alberto Lavelli", "Bernardo Magnini"], "categories": ["cs.CL", "cs.AI"], "comment": "to be published in BioNLP 2025", "summary": "Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166", "AI": {"tldr": "This paper presents a methodology for converting datasets annotated for information extraction into structured Case Report Forms (CRFs) to address the shortage of well-annotated CRF data. The methodology was tested on the E3C dataset in English and Italian resulting in a new dataset that enables CRF slot filling with reported accuracy metrics.", "motivation": "The scarcity of publicly available, well-annotated Case Report Form (CRF) datasets limits advancements in CRF slot filling systems that extract information from clinical notes.", "method": "A semi-automatic conversion methodology was developed to transform existing information extraction datasets into structured CRFs, specifically applied to the E3C dataset in English and Italian.", "result": "The slot filling achieved performance of 67.3% on English and 59.7% on Italian using a closed Large Language Model in a zero-shot manner, while performance on three families of open-source models was lower, indicating the complexity of the task.", "conclusion": "The study highlights the challenges of CRF slot filling, even with state-of-the-art LLMs, and provides a new high-quality dataset for future research.", "key_contributions": ["Proposed a semi-automatic conversion methodology for CRFs", "Created a new dataset for CRF slot filling from the E3C dataset", "Demonstrated challenges in filling CRFs using current LLMs"], "limitations": "The performance results indicate difficulty in achieving high accuracy for CRF slot filling using recent language models, suggesting limitations in current model capabilities.", "keywords": ["Case Report Forms", "CRF slot filling", "information extraction", "large language models", "health informatics"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.11673", "pdf": "https://arxiv.org/pdf/2506.11673.pdf", "abs": "https://arxiv.org/abs/2506.11673", "title": "Improving Causal Interventions in Amnesic Probing with Mean Projection or LEACE", "authors": ["Alicja Dobrzeniecka", "Antske Fokkens", "Pia Sommerauer"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Amnesic probing is a technique used to examine the influence of specific\nlinguistic information on the behaviour of a model. This involves identifying\nand removing the relevant information and then assessing whether the model's\nperformance on the main task changes. If the removed information is relevant,\nthe model's performance should decline. The difficulty with this approach lies\nin removing only the target information while leaving other information\nunchanged. It has been shown that Iterative Nullspace Projection (INLP), a\nwidely used removal technique, introduces random modifications to\nrepresentations when eliminating target information. We demonstrate that Mean\nProjection (MP) and LEACE, two proposed alternatives, remove information in a\nmore targeted manner, thereby enhancing the potential for obtaining behavioural\nexplanations through Amnesic Probing.", "AI": {"tldr": "This paper explores new techniques for amnesic probing in model behavior analysis, focusing on targeted information removal to improve behavioral explanations.", "motivation": "The need to understand the impact of specific linguistic information on model performance and limitations in existing removal techniques.", "method": "The study introduces Mean Projection (MP) and LEACE as alternatives to Iterative Nullspace Projection (INLP) for more precise information removal in amnesic probing.", "result": "Mean Projection and LEACE effectively eliminate target information without compromising other linguistic data, leading to clearer behavioral explanations than INLP.", "conclusion": "The proposed methods improve the robustness and clarity of the amnesic probing technique, allowing for better insights into model behavior.", "key_contributions": ["Introduction of Mean Projection (MP) and LEACE for amnesic probing", "Demonstration of improved targeted information removal", "Enhanced potential for behavioral explanations in language models"], "limitations": "", "keywords": ["amnesic probing", "Iterative Nullspace Projection", "Mean Projection", "behavioral explanations", "language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.11681", "pdf": "https://arxiv.org/pdf/2506.11681.pdf", "abs": "https://arxiv.org/abs/2506.11681", "title": "LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting Approach", "authors": ["Pratibha Zunjare", "Michael Hsiao"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task.", "AI": {"tldr": "The paper presents a hybrid method using Large Language Models for simplifying complex sentences, achieving a 70% success rate in sentence simplification for video game design applications.", "motivation": "To address the difficulty of simplifying complex sentences while maintaining semantic and logical accuracy.", "method": "A hybrid approach combining advanced prompting techniques with multi-agent architectures to facilitate the sentence simplification process.", "result": "The proposed method successfully simplified 70% of complex sentences in the context of video game design, significantly outperforming a single-agent approach, which achieved a 48% success rate.", "conclusion": "The hybrid multi-agent system offers a significant improvement in simplifying complex sentences, with potential implications for various applications.", "key_contributions": ["Hybrid method using multi-agent architectures for sentence simplification", "Significant performance improvement over single-agent approaches", "Potential applications in fields such as game design and beyond."], "limitations": "", "keywords": ["sentence simplification", "large language models", "multi-agent systems"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.11702", "pdf": "https://arxiv.org/pdf/2506.11702.pdf", "abs": "https://arxiv.org/abs/2506.11702", "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data", "authors": ["Víctor Gallego"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025 Workshop on Models of Human Feedback for AI\n  Alignment", "summary": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning", "AI": {"tldr": "Introduction of Configurable Preference Tuning (CPT) for dynamic adjustment of language model behavior based on human-interpretable directives.", "motivation": "Challenge the static nature of traditional human feedback models in AI alignment by introducing a framework that supports dynamic preference adjustment.", "method": "CPT utilizes synthetically generated preference data based on structured rubrics, allowing prompt-based modulation of LLM outputs without retraining.", "result": "Demonstrated that fine-tuning with rubric-guided preferences enables LLMs to adapt outputs in response to varying prompts, providing nuanced control over language generation.", "conclusion": "CPT enhances LLM adaptability and provides context-sensitive modeling of human feedback, marking a significant advancement in AI alignment methodologies.", "key_contributions": ["Introduction of a dynamic feedback model for LLMs", "Release of experimental artifacts including training code and datasets", "Establishment of a structured approach to preference tuning"], "limitations": "", "keywords": ["human feedback", "AI alignment", "language models", "preference tuning", "dynamic adjustment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.11728", "pdf": "https://arxiv.org/pdf/2506.11728.pdf", "abs": "https://arxiv.org/abs/2506.11728", "title": "The Cambrian Explosion of Mixed-Precision Matrix Multiplication for Quantized Deep Learning Inference", "authors": ["Héctor Martínez", "Adrián Castelló", "Francisco D. Igual", "Enrique S. Quintana-Ortí"], "categories": ["cs.CL"], "comment": "16 pages, 7 tables, 7 figures", "summary": "Recent advances in deep learning (DL) have led to a shift from traditional\n64-bit floating point (FP64) computations toward reduced-precision formats,\nsuch as FP16, BF16, and 8- or 16-bit integers, combined with mixed-precision\narithmetic. This transition enhances computational throughput, reduces memory\nand bandwidth usage, and improves energy efficiency, offering significant\nadvantages for resource-constrained edge devices. To support this shift,\nhardware architectures have evolved accordingly, now including adapted ISAs\n(Instruction Set Architectures) that expose mixed-precision vector units and\nmatrix engines tailored for DL workloads. At the heart of many DL and\nscientific computing tasks is the general matrix-matrix multiplication gemm, a\nfundamental kernel historically optimized using axpy vector instructions on\nSIMD (single instruction, multiple data) units. However, as hardware moves\ntoward mixed-precision dot-product-centric operations optimized for quantized\ninference, these legacy approaches are being phased out. In response to this,\nour paper revisits traditional high-performance gemm and describes strategies\nfor adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs,\nincluding x86_64, ARM, and RISC-V. Concretely, we illustrate novel micro-kernel\ndesigns and data layouts that better exploit today's specialized hardware and\ndemonstrate significant performance gains from MIP arithmetic over\nfloating-point implementations across three representative CPU architectures.\nThese contributions highlight a new era of gemm optimization-driven by the\ndemands of DL inference on heterogeneous architectures, marking what we term as\nthe \"Cambrian period\" for matrix multiplication.", "AI": {"tldr": "The paper discusses the transition to mixed-precision arithmetic in deep learning, focusing on optimizing general matrix-matrix multiplication (gemm) for various architectures.", "motivation": "With the shift towards reduced-precision formats in deep learning, optimizing matrix multiplication for modern hardware becomes essential to improve efficiency in resource-constrained environments.", "method": "The paper reviews traditional gemm implementations and proposes strategies for adapting these to mixed-precision integer arithmetic, showcasing new micro-kernel designs and data layouts optimized for x86_64, ARM, and RISC-V architectures.", "result": "The proposed mixed-precision implementations demonstrate significant performance improvements compared to traditional floating-point methods across the evaluated CPU architectures.", "conclusion": "The findings indicate a new era of optimization for matrix multiplication in deep learning, driven by hardware advancements and the demands of mixed-precision computations.", "key_contributions": ["Strategies for adapting gemm to mixed-precision integer arithmetic", "Novel micro-kernel designs for enhanced performance", "Demonstrated significant performance gains on various architectures"], "limitations": "", "keywords": ["deep learning", "mixed-precision", "matrix multiplication", "gemm", "hardware architecture"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.11752", "pdf": "https://arxiv.org/pdf/2506.11752.pdf", "abs": "https://arxiv.org/abs/2506.11752", "title": "DART: Distilling Autoregressive Reasoning to Silent Thought", "authors": ["Nan Jiang", "Ziming Wu", "De-Chuan Zhan", "Fuming Lai", "Shaobing Lian"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART achieves comparable\nreasoning performance to existing baselines while offering significant\nefficiency gains, serving as a feasible alternative for efficient reasoning.", "AI": {"tldr": "DART distills autoregressive reasoning in LLMs to a more efficient non-autoregressive framework, improving computational efficiency while maintaining performance.", "motivation": "To address the computational overhead of autoregressive reasoning in LLMs for latency-sensitive applications.", "method": "DART utilizes a self-distillation framework with two training pathways: CoT for traditional reasoning and ST for direct answer generation using lightweight reasoning evolvement.", "result": "DART achieves comparable reasoning performance to existing models while providing significant efficiency improvements in computation.", "conclusion": "DART serves as a viable alternative for efficient reasoning in LLMs, facilitating quicker inference without sacrificing performance.", "key_contributions": ["Introduction of DART framework for LLMs", "Implementation of self-distillation with two pathways", "Demonstration of efficiency gains in reasoning tasks"], "limitations": "The experimental setup may not cover all possible applications or demonstrate long-term performance.", "keywords": ["Large Language Models", "Chain-of-Thought", "Non-autoregressive reasoning", "Efficiency", "Self-distillation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.11763", "pdf": "https://arxiv.org/pdf/2506.11763.pdf", "abs": "https://arxiv.org/abs/2506.11763", "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents", "authors": ["Mingxuan Du", "Benfeng Xu", "Chiwei Zhu", "Xiaorui Wang", "Zhendong Mao"], "categories": ["cs.CL", "cs.IR"], "comment": "31 pages, 5 figures", "summary": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents.", "AI": {"tldr": "Introduction of DeepResearch Bench, a benchmark for evaluating LLM-based Deep Research Agents (DRAs) with 100 PhD-level research tasks.", "motivation": "To provide a systematic method for evaluating the performance of LLM-based agents in conducting research tasks, as existing solutions are absent.", "method": "Development of two methodologies: a reference-based method for assessing report quality and a framework for evaluating information retrieval and citation effectiveness.", "result": "DeepResearch Bench provides a standardized way to evaluate DRAs, achieving strong alignment with human judgment.", "conclusion": "The open-sourcing of DeepResearch Bench aims to foster advancement in LLM-based agents by offering a comprehensive evaluation framework.", "key_contributions": ["Introduction of a comprehensive benchmark for LLM-based agents", "Two novel methodologies aligning DRA evaluation with human judgment", "Open-source availability to encourage further research"], "limitations": "", "keywords": ["Deep Research Agents", "LLM-based agents", "benchmarking", "information retrieval", "citation accuracy"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.11769", "pdf": "https://arxiv.org/pdf/2506.11769.pdf", "abs": "https://arxiv.org/abs/2506.11769", "title": "Long-Short Alignment for Effective Long-Context Modeling in LLMs", "authors": ["Tianqi Du", "Haotian Huang", "Yifei Wang", "Yisen Wang"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment.", "AI": {"tldr": "This paper addresses the limitations of long-context modeling in large language models (LLMs) by proposing a focus on the output distribution and introducing a new metric to improve length generalization.", "motivation": "To improve effective long-context modeling in LLMs, which is hindered by their fixed context window and challenges with length generalization.", "method": "The authors conduct case studies on synthetic tasks to analyze long-short alignment in output distributions and propose a metric called Long-Short Misalignment to quantify this relationship, followed by introducing a regularization term during training that encourages long-short alignment.", "result": "Experiments demonstrate the effectiveness of the proposed approach, highlighting a strong correlation between Long-Short Misalignment and length generalization performance in LLMs.", "conclusion": "The findings offer new insights into achieving better long-context modeling, potentially enhancing the capabilities of LLMs in processing longer sequences.", "key_contributions": ["Proposes the concept of long-short alignment in LLM output distributions.", "Introduces the Long-Short Misalignment metric to measure length generalization.", "Develops a regularization term for promoting long-short alignment during model training."], "limitations": "", "keywords": ["Large Language Models", "Length Generalization", "Long-Short Alignment", "Regularization", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.11798", "pdf": "https://arxiv.org/pdf/2506.11798.pdf", "abs": "https://arxiv.org/abs/2506.11798", "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models", "authors": ["Maximilian Kreutner", "Marlene Lutz", "Markus Strohmaier"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation.", "AI": {"tldr": "This paper analyzes the use of zero-shot persona prompting in Large Language Models to predict individual voting decisions and the stances of European groups on various policies.", "motivation": "To address the progressive bias in LLMs and explore the effectiveness of persona prompts in simulating voting behaviors.", "method": "The study uses zero-shot persona prompting with minimal information to predict voting decisions and aggregates these predictions for European groups.", "result": "The model successfully simulates the voting behavior of Members of the European Parliament with a weighted F1 score of approximately 0.793.", "conclusion": "Zero-shot persona prompting can be effective in predicting individual and group voting decisions in a political context.", "key_contributions": ["Analysis of zero-shot persona prompting in LLMs", "Development of a persona dataset for European politicians", "Simulation of voting behavior in European Parliament with high accuracy"], "limitations": "", "keywords": ["Large Language Models", "persona prompting", "voting behavior", "European Parliament", "political discourse"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.11807", "pdf": "https://arxiv.org/pdf/2506.11807.pdf", "abs": "https://arxiv.org/abs/2506.11807", "title": "Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?", "authors": ["Simeon Junker", "Manar Ali", "Larissa Koch", "Sina Zarrieß", "Hendrik Buschmeier"], "categories": ["cs.CL"], "comment": "To appear in ACL Findings 2025", "summary": "We investigate the linguistic abilities of multimodal large language models\nin reference resolution tasks featuring simple yet abstract visual stimuli,\nsuch as color patches and color grids. Although the task may not seem\nchallenging for today's language models, being straightforward for human dyads,\nwe consider it to be a highly relevant probe of the pragmatic capabilities of\nMLLMs. Our results and analyses indeed suggest that basic pragmatic\ncapabilities, such as context-dependent interpretation of color descriptions,\nstill constitute major challenges for state-of-the-art MLLMs.", "AI": {"tldr": "This paper explores the capabilities of multimodal large language models (MLLMs) in reference resolution tasks using abstract visual stimuli and finds significant challenges in pragmatic interpretation.", "motivation": "To probe the pragmatic capabilities of multimodal large language models in interpreting context-dependent color descriptions.", "method": "The authors conduct experiments involving reference resolution tasks with visual stimuli such as color patches and grids to evaluate the performance of MLLMs.", "result": "The MLLMs demonstrated substantial difficulties in handling context-dependent interpretations of color descriptions, indicating limitations in their pragmatic abilities.", "conclusion": "The findings reveal that despite being straightforward for humans, reference resolution tasks pose significant challenges for current MLLMs, highlighting the need for improved pragmatic understanding.", "key_contributions": ["New insights into the pragmatic limits of MLLMs", "Empirical data from reference resolution tasks with visual stimuli", "Identification of context-dependent interpretation challenges"], "limitations": "The study focuses on specific tasks with abstract visual stimuli, which may not capture the full range of linguistic abilities or real-world applications.", "keywords": ["multimodal large language models", "reference resolution", "pragmatics", "visual stimuli", "context-dependent interpretation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.11857", "pdf": "https://arxiv.org/pdf/2506.11857.pdf", "abs": "https://arxiv.org/abs/2506.11857", "title": "Post Persona Alignment for Multi-Session Dialogue Generation", "authors": ["Yi-Pei Chen", "Noriki Nishida", "Hideki Nakayama", "Yuji Matsumoto"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation.", "AI": {"tldr": "Proposes a two-stage framework called Post Persona Alignment (PPA) to improve multi-session persona-based dialogue generation by first generating a response based on context and then aligning it with the speaker's persona.", "motivation": "To address challenges in maintaining long-term consistency and generating diverse, personalized responses in multi-session dialogue with large language models (LLMs).", "method": "PPA generates an initial response based on dialogue context and subsequently retrieves relevant persona memories to refine the response for better alignment with the user's persona.", "result": "PPA outperforms existing methods in consistency, diversity, and persona relevance on multi-session LLM-generated dialogue data.", "conclusion": "The post-hoc alignment strategy of PPA offers a more flexible and effective approach to long-term personalized dialogue generation, promoting naturalness and diversity.", "key_contributions": ["Introduction of Post Persona Alignment (PPA) framework", "Improvement in consistency and diversity of responses", "Effective persona relevance in dialogue generation"], "limitations": "", "keywords": ["dialogue generation", "multi-session dialogues", "persona alignment"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.11886", "pdf": "https://arxiv.org/pdf/2506.11886.pdf", "abs": "https://arxiv.org/abs/2506.11886", "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache", "authors": ["Xiaoran Liu", "Siyang He", "Qiqi Wang", "Ruixiao Li", "Yuerong Song", "Zhigeng Liu", "Linlin Li", "Qun Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "categories": ["cs.CL"], "comment": "10 pages, 7 figures, work in progress", "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.", "AI": {"tldr": "FourierAttention is a novel, training-free framework for optimizing long-context handling in Large Language Models by using Fourier bases to enhance memory efficiency.", "motivation": "Large Language Models face increasing memory demands due to growing Key-Value caches and existing methods often compromise accuracy or increase computational overhead.", "method": "FourierAttention utilizes the varying roles of transformer head dimensions to project long-context-insensitive dimensions onto orthogonal Fourier bases, approximating their evolution with fixed-length spectral coefficients.", "result": "Evaluations on LLaMA models demonstrate that FourierAttention achieves superior long-context accuracy compared to existing models on benchmarks like LongBench and NIAH.", "conclusion": "The framework, combined with a custom Triton kernel, offers an efficient deployment strategy without performance loss, addressing memory usage effectively.", "key_contributions": ["Introduces FourierAttention as a training-free method for improving long-context performance in LLMs.", "Utilizes Fourier bases for efficient memory management in transformer architecture.", "Demonstrates superior accuracy on long-context tasks using LLaMA models."], "limitations": "", "keywords": ["Large Language Models", "Fourier transform", "memory efficiency", "transformer architecture", "long-context handling"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11903", "pdf": "https://arxiv.org/pdf/2506.11903.pdf", "abs": "https://arxiv.org/abs/2506.11903", "title": "GeistBERT: Breathing Life into German NLP", "authors": ["Raphael Scheible-Schmitt", "Johann Frei"], "categories": ["cs.CL"], "comment": null, "summary": "Advances in transformer-based language models have highlighted the benefits\nof language-specific pre-training on high-quality corpora. In this context,\nGerman NLP stands to gain from updated architectures and modern datasets\ntailored to the linguistic characteristics of the German language. GeistBERT\nseeks to improve German language processing by incrementally training on a\ndiverse corpus and optimizing model performance across various NLP tasks. It\nwas pre-trained using fairseq with standard hyperparameters, initialized from\nGottBERT weights, and trained on a large-scale German corpus using Whole Word\nMasking (WWM). Based on the pre-trained model, we derived extended-input\nvariants using Nystr\\\"omformer and Longformer architectures with support for\nsequences up to 8k tokens. While these long-context models were not evaluated\non dedicated long-context benchmarks, they are included in our release. We\nassessed all models on NER (CoNLL 2003, GermEval 2014) and text classification\n(GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The\nGeistBERT models achieved strong performance, leading all tasks among the base\nmodels and setting a new state-of-the-art (SOTA). Notably, the base models\noutperformed larger models in several tasks. To support the German NLP research\ncommunity, we are releasing GeistBERT under the MIT license.", "AI": {"tldr": "GeistBERT enhances German NLP through improved language models and datasets, achieving state-of-the-art performance in various tasks.", "motivation": "To address the need for advanced architectures and datasets suited for the linguistic characteristics of the German language in NLP.", "method": "GeistBERT was pre-trained using fairseq with standard hyperparameters, initialized from GottBERT weights, and trained on a large-scale German corpus with Whole Word Masking (WWM). Extended-input variants were developed with Nyströmformer and Longformer architectures.", "result": "GeistBERT achieved strong performance across NER and text classification tasks, leading among base models and setting a new state-of-the-art, outperforming larger models in some cases.", "conclusion": "The GeistBERT models are made available under the MIT license to benefit the German NLP research community.", "key_contributions": ["Development of GeistBERT tailored for German NLP", "Enhanced performance in various NLP tasks", "Release of models under MIT license for community use"], "limitations": "", "keywords": ["GeistBERT", "German NLP", "language models", "transformer", "machine learning"], "importance_score": 3, "read_time_minutes": 8}}
{"id": "2506.11919", "pdf": "https://arxiv.org/pdf/2506.11919.pdf", "abs": "https://arxiv.org/abs/2506.11919", "title": "Effectiveness of Counter-Speech against Abusive Content: A Multidimensional Annotation and Classification Study", "authors": ["Greta Damo", "Elena Cabrio", "Serena Villata"], "categories": ["cs.CL"], "comment": null, "summary": "Counter-speech (CS) is a key strategy for mitigating online Hate Speech (HS),\nyet defining the criteria to assess its effectiveness remains an open\nchallenge. We propose a novel computational framework for CS effectiveness\nclassification, grounded in social science concepts. Our framework defines six\ncore dimensions - Clarity, Evidence, Emotional Appeal, Rebuttal, Audience\nAdaptation, and Fairness - which we use to annotate 4,214 CS instances from two\nbenchmark datasets, resulting in a novel linguistic resource released to the\ncommunity. In addition, we propose two classification strategies, multi-task\nand dependency-based, achieving strong results (0.94 and 0.96 average F1\nrespectively on both expert- and user-written CS), outperforming standard\nbaselines, and revealing strong interdependence among dimensions.", "AI": {"tldr": "A framework for assessing the effectiveness of counter-speech in combating online hate speech, utilizing six core dimensions and advanced classification methods.", "motivation": "The need for effective counter-speech strategies to address the rise of online hate speech and the challenge of evaluating their effectiveness.", "method": "A computational framework was developed for classifying counter-speech effectiveness, defining six dimensions, and annotating 4,214 instances from benchmark datasets. Two classification strategies were implemented—multi-task and dependency-based.", "result": "Achieved strong classification results with average F1 scores of 0.94 and 0.96, surpassing standard baselines and demonstrating interdependence among dimensions.", "conclusion": "The proposed framework and classification strategies provide a valuable resource for evaluating counter-speech effectiveness and can guide future research in online discourse.", "key_contributions": ["Introduction of a novel computational framework for counter-speech effectiveness assessment", "Definition of six core dimensions for evaluating counter-speech", "Release of a linguistic resource based on annotated counter-speech instances"], "limitations": "", "keywords": ["Counter-speech", "Hate Speech", "Effectiveness", "Computational Framework", "Classification"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.11930", "pdf": "https://arxiv.org/pdf/2506.11930.pdf", "abs": "https://arxiv.org/abs/2506.11930", "title": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback", "authors": ["Dongwei Jiang", "Alvin Zhang", "Andrew Wang", "Nicholas Andrews", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement.", "AI": {"tldr": "This paper investigates the ability of LLMs to incorporate external feedback, identifying a limitation termed FEEDBACK FRICTION, and explores strategies to mitigate this issue.", "motivation": "To understand how effectively LLMs can integrate external feedback in order to improve their responses in various reasoning tasks.", "method": "The study employs a controlled experimental setup where a solver model generates answers, receives targeted feedback from a feedback generator, and then attempts to improve its responses.", "result": "Despite the near-ideal conditions, solver models showed consistent resistance to the feedback provided, referred to as FEEDBACK FRICTION.", "conclusion": "The paper highlights FEEDBACK FRICTION as a significant limitation in LLMs and suggests that future research should focus on overcoming this obstacle for better self-improvement.", "key_contributions": ["Identification of FEEDBACK FRICTION in LLMs", "Evaluation across diverse reasoning tasks", "Exploration of strategies to mitigate feedback resistance"], "limitations": "Resistance to feedback appears persistent even under ideal conditions and common causes like model overconfidence were ruled out.", "keywords": ["LLM", "feedback", "machine learning", "self-improvement", "reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.11938", "pdf": "https://arxiv.org/pdf/2506.11938.pdf", "abs": "https://arxiv.org/abs/2506.11938", "title": "Improving Large Language Model Safety with Contrastive Representation Learning", "authors": ["Samuel Simko", "Mrinmaya Sachan", "Bernhard Schölkopf", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense", "AI": {"tldr": "The paper presents a novel defense framework for Large Language Models (LLMs) against adversarial attacks using contrastive representation learning.", "motivation": "To address the vulnerability of LLMs to adversarial attacks and the limitations of existing defense mechanisms.", "method": "The proposed method involves fine-tuning a model with a triplet-based loss and adversarial hard negative mining to enhance the separation between benign and harmful representations.", "result": "The experimental results indicate that this approach significantly outperforms previous defenses based on representation engineering, yielding improved robustness against various attack types.", "conclusion": "The study demonstrates the effectiveness of using contrastive representation learning for enhancing the defense mechanisms of LLMs without sacrificing performance.", "key_contributions": ["Introduction of a novel defense framework using contrastive representation learning.", "Demonstration of improved robustness against input-level and embedding-space attacks.", "Provision of code for the proposed method to enable further research."], "limitations": "", "keywords": ["Large Language Models", "adversarial attacks", "contrastive representation learning", "defense mechanisms", "robustness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.12014", "pdf": "https://arxiv.org/pdf/2506.12014.pdf", "abs": "https://arxiv.org/abs/2506.12014", "title": "code_transformed: The Influence of Large Language Models on Code", "authors": ["Yuliang Xu", "Siming Huang", "Mingmeng Geng", "Yao Wan", "Xuanhua Shi", "Dongping Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "We release all the experimental dataset and source code at:\n  https://github.com/ignorancex/LLM_code", "summary": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.", "AI": {"tldr": "This paper investigates how Large Language Models (LLMs) have influenced programming code style, focusing on naming conventions and maintainability, using data from over 19,000 GitHub repositories.", "motivation": "To explore the transformation of code style due to the advances in LLM capabilities and understand its implications for programming practices.", "method": "The study involves analyzing coding trends in over 19,000 GitHub repositories linked to arXiv papers from 2020-2025, focusing on measurable aspects like naming conventions and complexity.", "result": "The study found that the usage of snake_case variable names in Python increased from 47% in Q1 2023 to 51% in Q1 2025, indicating a clear trend influenced by LLMs.", "conclusion": "The findings provide substantial empirical evidence that LLMs are reshaping real-world coding styles, although estimating the exact amount of LLM-generated code remains challenging.", "key_contributions": ["First large-scale empirical evidence on the impact of LLMs on coding style", "Identification of measurable trends in coding practices associated with LLM influence", "Analysis of LLM reasoning processes on algorithmic problems"], "limitations": "Difficulty in estimating the exact proportion of code generated or assisted by LLMs due to their diversity and various usage scenarios.", "keywords": ["Large Language Models", "code style", "programming practices", "GitHub", "empirical study"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2405.04065", "pdf": "https://arxiv.org/pdf/2405.04065.pdf", "abs": "https://arxiv.org/abs/2405.04065", "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference", "authors": ["Runheng Liu", "Xingchen Xiao", "Heyan Huang", "Zewen Chi", "Zhijing Wu"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings, 14 pages", "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.", "AI": {"tldr": "FlashBack is a modular RALM that improves inference efficiency by appending retrieved documents to the context for better KV cache usage, resulting in faster LLM performance without compromising generation quality.", "motivation": "To address the high runtime issues and inefficiencies associated with traditional RALM methods that prepend retrieved content to inputs, affecting inference speed.", "method": "FlashBack integrates retrieved documents at the end of the context and introduces Marking Tokens for efficient fine-tuning, enhancing KV cache utilization.", "result": "FlashBack achieves up to 4x faster inference speed compared to traditional prepending methods on a 7B LLM while maintaining decent generation quality as measured by perplexity.", "conclusion": "The proposed method significantly reduces inference costs by improving LLM efficiency during retrieval-augmented generation tasks.", "key_contributions": ["Introduction of FlashBack for efficient RALM", "Utilization of appending context pattern for better KV cache performance", "Development of Marking Tokens for improved fine-tuning"], "limitations": "", "keywords": ["Retrieval-Augmented Language Modeling", "inference efficiency", "Marking Tokens"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2406.02050", "pdf": "https://arxiv.org/pdf/2406.02050.pdf", "abs": "https://arxiv.org/abs/2406.02050", "title": "JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language Models", "authors": ["Hitomi Yanaka", "Namgi Han", "Ryoma Kumon", "Jie Lu", "Masashi Takeshita", "Ryo Sekizawa", "Taisei Kato", "Hiromi Arai"], "categories": ["cs.CL"], "comment": "Accepted to the 6th Workshop on Gender Bias in Natural Language\n  Processing (GeBNLP2025) at ACL2025", "summary": "With the development of large language models (LLMs), social biases in these\nLLMs have become a pressing issue. Although there are various benchmarks for\nsocial biases across languages, the extent to which Japanese LLMs exhibit\nsocial biases has not been fully investigated. In this study, we construct the\nJapanese Bias Benchmark dataset for Question Answering (JBBQ) based on the\nEnglish bias benchmark BBQ, with analysis of social biases in Japanese LLMs.\nThe results show that while current open Japanese LLMs with more parameters\nshow improved accuracies on JBBQ, their bias scores increase. In addition,\nprompts with a warning about social biases and chain-of-thought prompting\nreduce the effect of biases in model outputs, but there is room for improvement\nin extracting the correct evidence from contexts in Japanese. Our dataset is\navailable at https://github.com/ynklab/JBBQ_data.", "AI": {"tldr": "This study constructs the Japanese Bias Benchmark dataset (JBBQ) for analyzing social biases in Japanese large language models, revealing a trade-off between model accuracy and bias scores.", "motivation": "To investigate the extent of social biases in Japanese large language models, which has not been fully studied despite the presence of benchmarks in other languages.", "method": "The study constructs the JBBQ dataset based on the English BBQ benchmark and analyzes the performance of Japanese LLMs on this dataset.", "result": "The analysis indicates that while larger Japanese LLMs achieve better accuracy on the JBBQ, their bias scores are higher; prompts designed to mitigate bias effects show some effectiveness, though further improvement is required for evidence extraction.", "conclusion": "The findings suggest that addressing social biases in Japanese LLMs remains a challenge, and further enhancements in model training and prompting techniques are necessary.", "key_contributions": ["Development of the Japanese Bias Benchmark dataset (JBBQ) for Question Answering.", "Comparison of bias scores in Japanese LLMs with varying parameters.", "Evaluation of prompting techniques to reduce social bias effects."], "limitations": "The ability to extract correct evidence from contexts in Japanese remains an area for improvement.", "keywords": ["Japanese Bias Benchmark", "large language models", "social biases", "natural language processing", "bias mitigation"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2411.15694", "pdf": "https://arxiv.org/pdf/2411.15694.pdf", "abs": "https://arxiv.org/abs/2411.15694", "title": "Deep Sparse Latent Feature Models for Knowledge Graph Completion", "authors": ["Haotian Li", "Rui Zhang", "Lingzhi Wang", "Bin Yu", "Youwei Wang", "Yuliang Wei", "Kai Wang", "Richard Yi Da Xu", "Bailing Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in knowledge graph completion (KGC) have emphasized\ntext-based approaches to navigate the inherent complexities of large-scale\nknowledge graphs (KGs). While these methods have achieved notable progress,\nthey frequently struggle to fully incorporate the global structural properties\nof the graph. Stochastic blockmodels (SBMs), especially the latent feature\nrelational model (LFRM), offer robust probabilistic frameworks for identifying\nlatent community structures and improving link prediction. This paper presents\na novel probabilistic KGC framework utilizing sparse latent feature models,\noptimized via a deep variational autoencoder (VAE). Our proposed method\ndynamically integrates global clustering information with local textual\nfeatures to effectively complete missing triples, while also providing enhanced\ninterpretability of the underlying latent structures. Extensive experiments on\nfour benchmark datasets with varying scales demonstrate the significant\nperformance gains achieved by our method.", "AI": {"tldr": "This paper introduces a new probabilistic framework for knowledge graph completion that combines global community structures with local textual features using a deep variational autoencoder.", "motivation": "To address the challenges faced by text-based approaches in fully incorporating global structural properties of knowledge graphs during completion tasks.", "method": "A novel KGC framework utilizing sparse latent feature models optimized via a deep variational autoencoder (VAE) that integrates global clustering with local textual features.", "result": "The proposed method shows significant performance gains in link prediction tasks across four benchmark datasets compared to existing methods.", "conclusion": "The framework not only improves missing triples completion but also enhances the interpretability of the latent structures within knowledge graphs.", "key_contributions": ["Development of a probabilistic KGC framework using sparse latent feature models.", "Integration of global clustering information with local textual features for improved performance.", "Enhanced interpretability of underlying latent structures in KGs."], "limitations": "", "keywords": ["knowledge graph completion", "stochastic blockmodels", "variational autoencoder", "link prediction", "latent feature models"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2412.21015", "pdf": "https://arxiv.org/pdf/2412.21015.pdf", "abs": "https://arxiv.org/abs/2412.21015", "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based QA Datasets", "authors": ["Mahir Labib Dihan", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025 (Demo)", "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.", "AI": {"tldr": "MapQaTor is an open-source framework designed to create reproducible geospatial question answering datasets by integrating with various maps APIs.", "motivation": "Existing mapping and navigation services struggle with natural language geospatial queries; thus, there's a need for better datasets for LLMs to improve understanding in this area.", "method": "MapQaTor allows users to collect and visualize data from different maps APIs with minimal setup and caches API responses for consistent ground truth.", "result": "The framework speeds up annotation processes by at least 30 times compared to manual methods, enhancing the creation of geospatial resources like complex datasets.", "conclusion": "MapQaTor offers a new opportunity for evaluating and advancing LLM-based geospatial reasoning capabilities.", "key_contributions": ["Extensible open-source framework for geospatial QA datasets creation", "Integration with any maps API for data collection", "30 times faster annotation process compared to manual methods"], "limitations": "", "keywords": ["geospatial", "Large Language Models", "question answering", "mapping services", "dataset creation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2501.03479", "pdf": "https://arxiv.org/pdf/2501.03479.pdf", "abs": "https://arxiv.org/abs/2501.03479", "title": "Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia Reflect on the Cross-Cultural Sociolinguistic Norms?", "authors": ["Sourabrata Mukherjee", "Atharva Mehta", "Soumya Teotia", "Sougata Saha", "Akhil Arora", "Monojit Choudhury"], "categories": ["cs.CL"], "comment": "Accepted at 2nd WikiNLP: Advancing Natural Language Process for\n  Wikipedia, Co-located with ACL 2025 (non-archival)", "summary": "Wikipedia, as a massively multilingual, community-driven platform, is a\nvaluable resource for Natural Language Processing (NLP), yet the consistency of\nhonorific usage in honorific-rich languages remains underexplored. Honorifics,\nsubtle yet profound linguistic markers, encode social hierarchies, politeness\nnorms, and cultural values, but Wikipedia's editorial guidelines lack clear\nstandards for their usage in languages where such forms are grammatically and\nsocially prevalent. This paper addresses this gap through a large-scale\nanalysis of third-person honorific pronouns and verb forms in Hindi and Bengali\nWikipedia articles. Using Large Language Models (LLM), we automatically\nannotate 10,000 articles per language for honorific usage and socio-demographic\nfeatures such as gender, age, fame, and cultural origin. We investigate: (i)\nthe consistency of honorific usage across articles, (ii) how inconsistencies\ncorrelate with socio-cultural factors, and (iii) the presence of explicit or\nimplicit biases across languages. We find that honorific usage is consistently\nmore common in Bengali than Hindi, while non-honorific forms are more frequent\nfor infamous, juvenile, and exotic entities in both. Notably, gender bias\nemerges in both languages, particularly in Hindi, where men are more likely to\nreceive honorifics than women. Our analysis highlights the need for Wikipedia\nto develop language-specific editorial guidelines for honorific usage.", "AI": {"tldr": "This paper analyzes honorific usage in Hindi and Bengali Wikipedia, revealing biases and inconsistencies using LLM for annotation.", "motivation": "To address the lack of clear standards for honorific usage in linguistically rich languages on Wikipedia.", "method": "Conducted large-scale analysis of 10,000 articles per language using LLM to annotate honorifics and socio-demographic features.", "result": "Found significant inconsistencies in honorific usage, with Bengali exhibiting more honorifics than Hindi, and identified gender bias, particularly in Hindi.", "conclusion": "The study underscores the necessity for Wikipedia to establish language-specific guidelines for honorific usage due to identified biases and inconsistencies.", "key_contributions": ["Large-scale analysis of honorifics in Hindi and Bengali Wikipedia", "Demonstrated gender bias in honorifics usage", "Proposed the need for language-specific editorial guidelines"], "limitations": "", "keywords": ["Honorifics", "Natural Language Processing", "Wikipedia", "Bias", "Multilingual"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.01925", "pdf": "https://arxiv.org/pdf/2502.01925.pdf", "abs": "https://arxiv.org/abs/2502.01925", "title": "PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling", "authors": ["Avery Ma", "Yangchen Pan", "Amir-massoud Farahmand"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": "Accepted at ICML 2025 (Spotlight). Code:\n  https://github.com/averyma/pandas", "summary": "Many-shot jailbreaking circumvents the safety alignment of LLMs by exploiting\ntheir ability to process long input sequences. To achieve this, the malicious\ntarget prompt is prefixed with hundreds of fabricated conversational exchanges\nbetween the user and the model. These exchanges are randomly sampled from a\npool of unsafe question-answer pairs, making it appear as though the model has\nalready complied with harmful instructions. In this paper, we present PANDAS: a\nhybrid technique that improves many-shot jailbreaking by modifying these\nfabricated dialogues with Positive Affirmations, Negative Demonstrations, and\nan optimized Adaptive Sampling method tailored to the target prompt's topic. We\nalso introduce ManyHarm, a dataset of harmful question-answer pairs, and\ndemonstrate through extensive experiments that PANDAS significantly outperforms\nbaseline methods in long-context scenarios. Through attention analysis, we\nprovide insights into how long-context vulnerabilities are exploited and show\nhow PANDAS further improves upon many-shot jailbreaking.", "AI": {"tldr": "This paper describes PANDAS, a technique that enhances many-shot jailbreaking of LLMs by modifying input dialogues and introducing a dataset called ManyHarm.", "motivation": "The motivation for this research is to address the vulnerability of large language models (LLMs) to many-shot jailbreaking, which can circumvent safety measures by leveraging the ability of LLMs to handle long input sequences.", "method": "PANDAS employs a hybrid technique that modifies fabricated dialogues using Positive Affirmations and Negative Demonstrations, combined with an Adaptive Sampling method specific to the target prompt's topic.", "result": "Experimental results show that PANDAS significantly outperforms baseline methods in scenarios that require long context handling, with detailed attention analysis highlighting how vulnerabilities are exploited.", "conclusion": "PANDAS represents a substantial improvement in many-shot jailbreaking, providing insights and methodologies to better understand and mitigate such vulnerabilities in LLMs.", "key_contributions": ["Introduction of PANDAS, a robust technique for enhancing many-shot jailbreaking.", "Development of ManyHarm, a dataset for harmful question-answer pairs.", "Attention analysis that sheds light on long-context vulnerabilities in LLMs."], "limitations": "The paper may not address the implications of adversarial robustness in real-world applications of LLMs.", "keywords": ["many-shot jailbreaking", "large language models", "adaptive sampling", "harmful question-answer pairs", "attention analysis"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.11020", "pdf": "https://arxiv.org/pdf/2502.11020.pdf", "abs": "https://arxiv.org/abs/2502.11020", "title": "TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages", "authors": ["Jafar Isbarov", "Arofat Akhundjanova", "Mammad Hajili", "Kavsar Huseynova", "Dmitry Gaynullin", "Anar Rzayev", "Osman Tursun", "Aizirek Turdubaeva", "Ilshat Saetov", "Rinat Kharisov", "Saule Belginova", "Ariana Kenbayeva", "Amina Alisheva", "Abdullatif Köksal", "Samir Rustamov", "Duygu Ataman"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025, Main Conference", "summary": "Being able to thoroughly assess massive multi-task language understanding\n(MMLU) capabilities is essential for advancing the applicability of\nmultilingual language models. However, preparing such benchmarks in high\nquality native language is often costly and therefore limits the\nrepresentativeness of evaluation datasets. While recent efforts focused on\nbuilding more inclusive MMLU benchmarks, these are conventionally built using\nmachine translation from high-resource languages, which may introduce errors\nand fail to account for the linguistic and cultural intricacies of the target\nlanguages. In this paper, we address the lack of native language MMLU benchmark\nespecially in the under-represented Turkic language family with distinct\nmorphosyntactic and cultural characteristics. We propose two benchmarks for\nTurkic language MMLU: TUMLU is a comprehensive, multilingual, and natively\ndeveloped language understanding benchmark specifically designed for Turkic\nlanguages. It consists of middle- and high-school level questions spanning 11\nacademic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar,\nTurkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise,\nbalanced, and manually verified subset of the dataset. Using this dataset, we\nsystematically evaluate a diverse range of open and proprietary multilingual\nlarge language models (LLMs), including Claude, Gemini, GPT, and LLaMA,\noffering an in-depth analysis of their performance across different languages,\nsubjects, and alphabets. To promote further research and development in\nmultilingual language understanding, we release TUMLU-mini and all\ncorresponding evaluation scripts.", "AI": {"tldr": "This paper presents TUMLU, a native language MMLU benchmark for Turkic languages, addressing the limitations of current multilingual models by evaluating their performance using this new dataset.", "motivation": "There is a need for high-quality native language benchmarks for multilingual language models, specifically for the under-represented Turkic languages, to enhance the assessment accuracy of their language understanding capabilities.", "method": "The authors developed two benchmarks: TUMLU for comprehensive evaluation across 11 academic subjects in various Turkic languages, and TUMLU-mini as a concise verified subset. They conducted systematic evaluations of multiple multilingual language models on these benchmarks.", "result": "The evaluation results provided insights into the performance of different LLMs—including Claude, Gemini, GPT, and LLaMA—across languages and subjects, highlighting their strengths and weaknesses in understanding Turkic languages.", "conclusion": "The introduction of TUMLU and TUMLU-mini facilitates more accurate multilingual language understanding research and supports further development in the field by offering accessible and reliable datasets.", "key_contributions": ["Development of TUMLU, a comprehensive MMLU benchmark for Turkic languages", "Creation of TUMLU-mini, a balanced and manually verified subset", "Systematic evaluation of LLMs on newly created benchmarks"], "limitations": "", "keywords": ["MMLU", "multilingual language models", "Turkic languages", "language understanding benchmarks", "TUMLU"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.11812", "pdf": "https://arxiv.org/pdf/2502.11812.pdf", "abs": "https://arxiv.org/abs/2502.11812", "title": "Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis", "authors": ["Xu Wang", "Yan Hu", "Wenyu Du", "Reynold Cheng", "Benyou Wang", "Difan Zou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages", "summary": "Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks\nwhere pre-trained models already perform well, we develop a set of mathematical\ntasks where fine-tuning yields substantial performance gains, which are closer\nto the practical setting. In our experiments, we identify circuits at various\ncheckpoints during fine-tuning and examine the interplay between circuit\nanalysis, fine-tuning methods, and task complexities. First, we find that while\ncircuits maintain high node similarity before and after fine-tuning, their\nedges undergo significant changes, in contrast to prior work that shows\ncircuits only add some additional components after fine-tuning. Based on these\nobservations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method,\nwhich assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.", "AI": {"tldr": "This paper explores the fine-tuning of Large Language Models (LLMs) using circuit analysis to understand its mechanisms, developing a circuit-aware Low-Rank Adaptation (LoRA) method that improves performance.", "motivation": "Understanding the mechanisms behind fine-tuning LLMs can enhance their performance in practical applications, which is currently unclear.", "method": "Circuit analysis is used to study changes in circuits during fine-tuning, and a new circuit-aware Low-Rank Adaptation (LoRA) method is proposed based on findings about circuit dynamics.", "result": "The circuit-based LoRA algorithm shows an average performance improvement of 2.46% over standard LoRA with similar parameter sizes, highlighting significant edge changes in circuits during fine-tuning.", "conclusion": "This study provides new insights into fine-tuning mechanisms and offers a method to improve LLM performance by understanding and leveraging circuit dynamics.", "key_contributions": ["Developed a circuit-aware LoRA method that utilizes circuit changes during fine-tuning", "Identified significant changes in circuit edges during fine-tuning", "Provided a framework for enhancing fine-tuning in compositional tasks by combining circuits from subtasks."], "limitations": "", "keywords": ["fine-tuning", "Large Language Models", "circuit analysis", "Low-Rank Adaptation", "Mechanistic Interpretability"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2502.19110", "pdf": "https://arxiv.org/pdf/2502.19110.pdf", "abs": "https://arxiv.org/abs/2502.19110", "title": "Conformal Linguistic Calibration: Trading-off between Factuality and Specificity", "authors": ["Zhengping Jiang", "Anqi Liu", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language model outputs are not always reliable, thus prompting research into\nhow to adapt model responses based on uncertainty. Common approaches include:\n\\emph{abstention}, where models refrain from generating responses when\nuncertain; and \\emph{linguistic calibration}, where models hedge their\nstatements using uncertainty quantifiers. However, abstention can withhold\nvaluable information, while linguistically calibrated responses are often\nchallenging to leverage in downstream tasks. We propose a unified view,\nConformal Linguistic Calibration (CLC), which reinterprets linguistic\ncalibration as \\emph{answer set prediction}. First we present a framework\nconnecting abstention and linguistic calibration through the lens of linguistic\npragmatics. We then describe an implementation of CLC that allows for\ncontrolling the level of imprecision in model responses. Results demonstrate\nour method produces calibrated outputs with conformal guarantees on factual\naccuracy. Further, our approach enables fine-tuning models to perform\nuncertainty-aware adaptive claim rewriting, offering a controllable balance\nbetween factuality and specificity.", "AI": {"tldr": "The paper proposes a method called Conformal Linguistic Calibration (CLC) that connects abstention and linguistic calibration in language models, allowing for controllable imprecision in responses.", "motivation": "Language model outputs can be unreliable, necessitating methods to adapt responses based on uncertainty without losing valuable information or complicating downstream tasks.", "method": "The proposed unified approach, CLC, integrates linguistic calibration and abstention through a framework of linguistic pragmatics and facilitates controlled imprecision in model outputs.", "result": "The CLC method produces calibrated outputs that maintain factual accuracy and supports uncertainty-aware adaptive claim rewriting, balancing factuality and specificity.", "conclusion": "The findings suggest that CLC provides an effective way to improve the reliability and usability of model outputs in uncertain situations.", "key_contributions": ["Introduction of Conformal Linguistic Calibration (CLC) as a unified approach to linguistic calibration and abstention.", "Demonstration of a framework connecting linguistic pragmatics with adaptive language model outputs.", "Results show effectiveness in producing reliable outputs while allowing control over response specificity."], "limitations": "The exploration of CLC is still preliminary, requiring further study to fully evaluate its effectiveness across various applications.", "keywords": ["Conformal Linguistic Calibration", "uncertainty quantification", "abstention", "linguistic pragmatics", "language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.19175", "pdf": "https://arxiv.org/pdf/2502.19175.pdf", "abs": "https://arxiv.org/abs/2502.19175", "title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis", "authors": ["Daniel Rose", "Chia-Chien Hung", "Marco Lepri", "Israa Alqassem", "Kiril Gashteovski", "Carolin Lawrence"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (main)", "summary": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models (LLMs) have shown promise in\nsupporting DDx, existing approaches face key limitations, including\nsingle-dataset evaluations, isolated optimization of components, unrealistic\nassumptions about complete patient profiles, and single-attempt diagnosis. We\nintroduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for\ninteractive DDx, where diagnostic reasoning evolves through iterative learning,\nrather than assuming a complete patient profile is accessible. MEDDxAgent\nintegrates three modular components: (1) an orchestrator (DDxDriver), (2) a\nhistory taking simulator, and (3) two specialized agents for knowledge\nretrieval and diagnosis strategy. To ensure robust evaluation, we introduce a\ncomprehensive DDx benchmark covering respiratory, skin, and rare diseases. We\nanalyze single-turn diagnostic approaches and demonstrate the importance of\niterative refinement when patient profiles are not available at the outset. Our\nbroad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy\nimprovements in interactive DDx across both large and small LLMs, while\noffering critical explainability into its diagnostic reasoning process.", "AI": {"tldr": "MEDDxAgent enhances differential diagnosis by enabling iterative learning and integrating modular components for better explainability and accuracy in diagnostics.", "motivation": "Existing approaches to differential diagnosis using LLMs have significant limitations, such as reliance on complete patient profiles and single dataset evaluations.", "method": "The MEDDxAgent framework includes an orchestrator, a history taking simulator, and specialized agents for knowledge retrieval and diagnostic strategy, allowing for interactive and iterative diagnostic processes.", "result": "MEDDxAgent shows over 10% accuracy improvements in interactive differential diagnosis across various disease categories compared to traditional methods.", "conclusion": "The framework provides not only improved accuracy in diagnostic recommendations but also critical explainability regarding the reasoning behind those recommendations, addressing major gaps in current DDx practices.", "key_contributions": ["Introduction of a Modular Explainable DDx Agent (MEDDxAgent) framework for iterative DDx.", "Comprehensive benchmark for evaluating DDx across new disease categories.", "Demonstration of over 10% accuracy improvement in interactive DDx compared to existing approaches."], "limitations": "", "keywords": ["Differential Diagnosis", "Large Language Models", "Health Informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.09347", "pdf": "https://arxiv.org/pdf/2503.09347.pdf", "abs": "https://arxiv.org/abs/2503.09347", "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts", "authors": ["Hongyu Chen", "Seraphina Goldfarb-Tarrant"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, ACL 2025", "summary": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.", "AI": {"tldr": "This study evaluates the reliability of Large Language Models (LLMs) as automated evaluators for content safety, highlighting biases and the influence of input artifacts on judgments.", "motivation": "The increasing use of LLMs as automated evaluators raises concerns about their reliability in assessing content safety.", "method": "The study evaluates 11 LLM judge models on self-consistency, alignment with human judgments, and susceptibility to input artifacts, particularly examining how artifacts influence judgments.", "result": "Biases in LLM judges notably distort safety evaluations, with apologetic language artifacts skewing preferences by up to 98%. Larger models do not always show greater robustness, while some smaller models are more resistant to specific artifacts.", "conclusion": "The findings indicate a need for diversified evaluation methodologies that can resist input artifacts to improve safety assessments.", "key_contributions": ["Evaluation of 11 LLM judge models across critical safety dimensions.", "Demonstration of significant biases affecting LLM evaluations based on input artifacts.", "Exploration of jury-based evaluations to enhance robustness and human alignment."], "limitations": "Artifact sensitivity persists even in the best jury configurations, indicating ongoing challenges in LLM evaluations.", "keywords": ["Large Language Models", "safety evaluation", "artifact sensitivity", "automated judging", "human alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.21227", "pdf": "https://arxiv.org/pdf/2503.21227.pdf", "abs": "https://arxiv.org/abs/2503.21227", "title": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models", "authors": ["Hengyuan Zhao", "Ziqin Wang", "Qixin Sun", "Kaiyou Song", "Yilin Li", "Xiaolin Hu", "Qingpei Guo", "Si Liu"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon.", "AI": {"tldr": "LLaVA-CMoE is a continual learning framework for LLMs that enables parameter-efficient model expansion and robust knowledge retention without replay data.", "motivation": "To tackle the challenges of parameter growth and catastrophic forgetting in large language models (LLMs) during continual multimodal learning.", "method": "The paper introduces a Probe-Guided Knowledge Extension mechanism for determining new experts' addition and a Probabilistic Task Locator that assigns lightweight routers for tasks and uses VAE for router identification during inference.", "result": "LLaVA-CMoE demonstrates strong performance on the CoIN benchmark, covering eight VQA tasks, achieving reduced forgetting and parameter overhead compared to prior methods.", "conclusion": "The approach effectively scales LLMs for continual learning while maintaining parameter efficiency and knowledge retention.", "key_contributions": ["Introduction of the Probe-Guided Knowledge Extension mechanism", "Development of the Probabilistic Task Locator for dynamic expert allocation", "Demonstration of effective continual learning performance with reduced forgetting and model size"], "limitations": "", "keywords": ["Mixture of Experts", "continual learning", "large language models", "robust knowledge retention", "parameter efficiency"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2504.11673", "pdf": "https://arxiv.org/pdf/2504.11673.pdf", "abs": "https://arxiv.org/abs/2504.11673", "title": "Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions", "authors": ["Minwoo Kang", "Suhong Moon", "Seung Hyeong Lee", "Ayush Raj", "Joseph Suh", "David M. Chan"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly capable of simulating human\nbehavior, offering cost-effective ways to estimate user responses to various\nsurveys and polls. However, the questions in these surveys usually reflect\nsocially understood attitudes: the patterns of attitudes of old/young,\nliberal/conservative, as understood by both members and non-members of those\ngroups. It is not clear whether the LLM binding is \\emph{deep}, meaning the LLM\nanswers as a member of a particular in-group would, or \\emph{shallow}, meaning\nthe LLM responds as an out-group member believes an in-group member would. To\nexplore this difference, we use questions that expose known in-group/out-group\nbiases. This level of fidelity is critical for applying LLMs to various\npolitical science studies, including timely topics on polarization dynamics,\ninter-group conflict, and democratic backsliding. To this end, we propose a\nnovel methodology for constructing virtual personas with synthetic user\n``backstories\" generated as extended, multi-turn interview transcripts. Our\ngenerated backstories are longer, rich in detail, and consistent in\nauthentically describing a singular individual, compared to previous methods.\nWe show that virtual personas conditioned on our backstories closely replicate\nhuman response distributions (up to an 87\\% improvement as measured by\nWasserstein Distance) and produce effect sizes that closely match those\nobserved in the original studies of in-group/out-group biases. Altogether, our\nwork extends the applicability of LLMs beyond estimating socially understood\nresponses, enabling their use in a broader range of human studies.", "AI": {"tldr": "This paper explores the depth of Large Language Models' (LLMs) ability to simulate human responses in surveys through the development of synthetic user backstories, enhancing the fidelity and application of LLMs in political science.", "motivation": "To investigate whether LLMs' responses to surveys are deeply representative of in-group members or merely reflect out-group perceptions, which is crucial for studies on polarization and conflict.", "method": "We developed a novel methodology that constructs virtual personas with rich synthetic backstories via extensive multi-turn interviews, allowing for a more nuanced simulation of human-like responses.", "result": "Our method achieved up to an 87% improvement in replicating human response distributions, demonstrating high fidelity in virtual persona responses compared to original studies of in-group/out-group biases.", "conclusion": "This work allows for broader applications of LLMs in social science research, moving beyond basic estimations of socially understood attitudes.", "key_contributions": ["Novel methodology for creating detailed virtual personas", "Significant improvement in replicating human responses", "Broader applicability of LLMs in political and social sciences"], "limitations": "", "keywords": ["Large Language Models", "Human Behavior Simulation", "Political Science", "In-group/Out-group Bias", "Synthetic Backstories"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.13439", "pdf": "https://arxiv.org/pdf/2504.13439.pdf", "abs": "https://arxiv.org/abs/2504.13439", "title": "D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model", "authors": ["Grace Byun", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Evaluating generative models with open-ended generation is challenging due to\ninconsistencies in response formats. Multiple-choice (MC) evaluation mitigates\nthis issue, but generating high-quality distractors is time-consuming and\nlabor-intensive. We introduce D-GEN, the first open-source distractor generator\nmodel that transforms open-ended data into an MC format. To evaluate distractor\nquality, we propose two novel methods: (1) ranking alignment, ensuring\ngenerated distractors retain the discriminatory power of ground-truth\ndistractors, and (2) entropy analysis, comparing model confidence\ndistributions. Our results show that D-GEN preserves ranking consistency\n(Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy\ndistribution of ground-truth distractors. Human evaluation further confirms the\nfluency, coherence, distractiveness, and incorrectness. Our work advances\nrobust and efficient distractor generation with automated evaluation, setting a\nnew standard for MC evaluation.", "AI": {"tldr": "D-GEN is an open-source distractor generator that transforms open-ended data into multiple-choice formats and evaluates the quality of distractors using novel methods.", "motivation": "There is a challenge in evaluating generative models due to inconsistencies in response formats, and existing methods for generating distractors are labor-intensive.", "method": "Introduction of D-GEN, which generates distractors from open-ended data and evaluates them using ranking alignment and entropy analysis.", "result": "D-GEN achieves high preservation of ranking consistency and matches entropy distributions of ground-truth distractors, supported by positive human evaluations.", "conclusion": "D-GEN sets a new standard for robust and efficient distractor generation and automated evaluation in multiple-choice settings.", "key_contributions": ["D-GEN as the first open-source distractor generator model", "Proposed novel methods for evaluating distractor quality", "High ranking consistency and matching entropy distribution with ground-truth distractors"], "limitations": "", "keywords": ["Generative Models", "Distractor Generation", "Multiple-Choice Evaluation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2504.13615", "pdf": "https://arxiv.org/pdf/2504.13615.pdf", "abs": "https://arxiv.org/abs/2504.13615", "title": "Long-context Non-factoid Question Answering in Indic Languages", "authors": ["Ritwik Mishra", "Rajiv Ratn Shah", "Ponnurangam Kumaraguru"], "categories": ["cs.CL"], "comment": "Short version of this manuscript accepted at\n  https://bda2025.iiitb.net/", "summary": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA.", "AI": {"tldr": "This study investigates context-shortening techniques for improving Question Answering (QA) performance in low-resource Indic languages, demonstrating notable gains in semantic and token-level scores across LLMs.", "motivation": "Long contexts in QA tasks are challenging for LLMs, particularly in low-resource Indic languages, necessitating exploration of context-shortening methods to improve effectiveness and efficiency.", "method": "The study evaluates several context-shortening techniques, including Open Information Extraction (OIE), coreference resolution, and Answer Paragraph Selection (APS), testing their impact on QA performance using standard metrics on four Indic languages.", "result": "The context-shortening techniques result in a 4% average improvement in semantic scores and a 47% improvement in token-level scores without fine-tuning, with additional gains observed when fine-tuning the models.", "conclusion": "Context-shortening improves the performance and efficiency of LLM-based QA systems for Indic languages, though limitations in handling non-factoid questions remain.", "key_contributions": ["Demonstrated effectiveness of context-shortening techniques in improving QA for low-resource Indic languages.", "Highlighted the reduction in computational overhead from context-shortening.", "Provided insights into the limitations of LLM-based QA systems regarding reasoning and non-factoid questions."], "limitations": "LLM-based QA systems struggle with non-factoid questions that require reasoning or debate; verbalizing OIE-generated triples does not improve performance.", "keywords": ["Question Answering", "Large Language Models", "Context-shortening", "Indic languages", "Open Information Extraction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.23252", "pdf": "https://arxiv.org/pdf/2505.23252.pdf", "abs": "https://arxiv.org/abs/2505.23252", "title": "Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers", "authors": ["Bing Ma", "Hai Zhuge"], "categories": ["cs.CL"], "comment": "26 pages, 9 figures", "summary": "Approaches form the foundation for conducting scientific research. Querying\napproaches from a vast body of scientific papers is extremely time-consuming,\nand without a well-organized management framework, researchers may face\nsignificant challenges in querying and utilizing relevant approaches.\nConstructing multiple dimensions on approaches and managing them from these\ndimensions can provide an efficient solution. Firstly, this paper identifies\napproach patterns using a top-down way, refining the patterns through four\ndistinct linguistic levels: semantic level, discourse level, syntactic level,\nand lexical level. Approaches in scientific papers are extracted based on\napproach patterns. Additionally, five dimensions for categorizing approaches\nare identified using these patterns. This paper proposes using tree structure\nto represent step and measuring the similarity between different steps with a\ntree-structure-based similarity measure that focuses on syntactic-level\nsimilarities. A collection similarity measure is proposed to compute the\nsimilarity between approaches. A bottom-up clustering algorithm is proposed to\nconstruct class trees for approach components within each dimension by merging\neach approach component or class with its most similar approach component or\nclass in each iteration. The class labels generated during the clustering\nprocess indicate the common semantics of the step components within the\napproach components in each class and are used to manage the approaches within\nthe class. The class trees of the five dimensions collectively form a\nmulti-dimensional approach space. The application of approach queries on the\nmulti-dimensional approach space demonstrates that querying within this space\nensures strong relevance between user queries and results and rapidly reduces\nsearch space through a class-based query mechanism.", "AI": {"tldr": "This paper proposes a multi-dimensional framework for querying scientific approaches by identifying patterns and similarities among them, improving the efficiency of retrieval and management.", "motivation": "Researchers struggle with managing and querying vast numbers of approaches from scientific papers, necessitating a systematic method for efficient retrieval.", "method": "The paper employs a top-down identification of approach patterns across four linguistic levels, proposes a tree-structure-based similarity measure for approaches, and introduces a bottom-up clustering algorithm to form class trees for better organization.", "result": "A multi-dimensional approach space is created, enabling efficient querying that maintains strong relevance between user queries and retrieved approaches, significantly reducing the search space.", "conclusion": "The proposed structure and methodologies provide a scalable solution for querying approaches in scientific research, enhancing usability and relevance.", "key_contributions": ["Identification of approach patterns at multiple linguistic levels", "Development of a tree structure for representing and measuring similarities between approaches", "Introduction of a bottom-up clustering algorithm for class trees in a multi-dimensional approach space"], "limitations": "", "keywords": ["approach patterns", "multi-dimensional framework", "clustering algorithm"], "importance_score": 4, "read_time_minutes": 10}}
