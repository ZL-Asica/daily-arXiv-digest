{"id": "2509.07126", "pdf": "https://arxiv.org/pdf/2509.07126.pdf", "abs": "https://arxiv.org/abs/2509.07126", "title": "Short-Term Gaze Prediction: Analysis of Individual Differences, Typical and Extreme-Case Errors", "authors": ["Kateryna Melnyk", "Lee Friedman", "Oleg Komogortsev"], "categories": ["cs.HC"], "comment": "12 pages, 14 figures", "summary": "Gaze prediction is a diverse field of study with multiple research focuses\nand practical applications. This article investigates how recurrent neural\nnetworks and transformers perform short-term gaze prediction. We used three\nmodels: a three-layer long-short-term memory (LSTM) network, a simple\ntransformer-encoder model (TF), and a classification-predictor network (ClPr),\nwhich simultaneously classifies the signal into eye movement events and\npredicts the positions of gaze. The performance of the models was evaluated for\nocular fixations and saccades of various amplitudes and as a function of\nindividual differences in both typical and extreme cases. On average, LSTM\nperformed better on fixations and saccades, whereas TF and ClPr demonstrated\nmore precise results for post-saccadic periods. In extreme cases, the\nbest-performing models vary depending on the type of eye movement. We reviewed\nthe difference between the median $P_{50}$ and high-percentile $P_{95}$ error\nprofiles across subjects. The subjects for which the models perform the best\noverall do not necessarily exhibit the lowest $P_{95}$ values, which supports\nthe idea of analyzing extreme cases separately in future work. We explore the\ntrade-offs between the proposed solutions and provide practical insights into\nmodel selection for gaze prediction.", "AI": {"tldr": "This article investigates short-term gaze prediction using recurrent neural networks and transformers, comparing the performance of LSTM, transformer-encoder, and a classification-predictor model.", "motivation": "To explore the effectiveness of different models for gaze prediction and analyze performance variations across subjects and eye movement types.", "method": "A three-layer LSTM, a simple transformer-encoder model, and a classification-predictor network were employed, evaluated on ocular fixations and saccades across various amplitudes and individual differences.", "result": "LSTM generally outperformed others on fixations and saccades, while TF and ClPr had better precision in post-saccadic periods. Performance varied significantly in extreme cases based on eye movement types.", "conclusion": "Model performance varies based on eye movement types and individual differences, suggesting future work should focus on extreme case analysis and practical model selection insights.", "key_contributions": ["Comparison of LSTM, transformer-encoder, and classification-predictor models for gaze prediction.", "Insights into the performance trade-offs of various models.", "Discussion on the importance of analyzing extreme cases in gaze prediction."], "limitations": "Does not provide exhaustive insights on all gaze prediction scenarios; focused on specific eye movement types and subjects.", "keywords": ["gaze prediction", "recurrent neural networks", "transformers", "LSTM", "eye movement"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2509.07187", "pdf": "https://arxiv.org/pdf/2509.07187.pdf", "abs": "https://arxiv.org/abs/2509.07187", "title": "Wellbeing-Centered UX: Supporting Content Moderators", "authors": ["Diana Mihalache", "Dalila Szostak"], "categories": ["cs.HC", "cs.CY", "J.4; K.4.1"], "comment": "In M. L. Daniel, A. Menking, M. T. Savio, & J. Claffey (Eds.) (In\n  Press, upcoming), Trust, Safety, and the Internet We Share: Multistakeholder\n  Insights. Taylor & Francis", "summary": "This chapter focuses on the intersection of user experience (UX) and\nwellbeing in the context of content moderation. Human content moderators play a\nkey role in protecting end users from harm by detecting, evaluating, and\naddressing content that may violate laws or product policies. They face\nnumerous challenges, including exposure to sensitive content, monotonous tasks,\nand complex decisions, which are often exacerbated by inadequate tools. This\nchapter explains the importance of incorporating wellbeing considerations\nthroughout the product development lifecycle, offering a framework and\npractical strategies for implementation across key UX disciplines: research,\nwriting, and design. By examining these considerations, this chapter provides a\nroadmap for creating user experiences that support content moderators,\nbenefiting both the user and the business.", "AI": {"tldr": "This chapter addresses the impact of user experience (UX) on the wellbeing of human content moderators in content moderation processes.", "motivation": "To highlight how content moderators face challenges due to sensitive content, monotonous tasks, and inadequate tools, and to emphasize the need for wellbeing in UX design.", "method": "The chapter offers a framework and strategies for integrating wellbeing considerations into UX disciplines such as research, writing, and design.", "result": "Provides a roadmap for improving user experiences for content moderators, which can enhance both user satisfaction and business outcomes.", "conclusion": "Incorporating wellbeing in the product development lifecycle can lead to better support for content moderators, ultimately benefiting users and businesses.", "key_contributions": ["Framework for wellbeing in UX", "Practical strategies for implementation in UX disciplines", "Roadmap for supporting content moderators"], "limitations": "", "keywords": ["User Experience", "Wellbeing", "Content Moderation", "UX Design", "Product Development"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.07202", "pdf": "https://arxiv.org/pdf/2509.07202.pdf", "abs": "https://arxiv.org/abs/2509.07202", "title": "Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data", "authors": ["Khushiyant"], "categories": ["cs.HC", "cs.CL", "I.2.7; I.2.6; J.3"], "comment": "15 pages, 10 figures, 5 tables", "summary": "Text generating capabilities have undergone a substantial transformation with\nthe introduction of large language models (LLMs). Electroencephalography\n(EEG)-based text production is still difficult, though, because it requires a\nlot of data and processing power. This paper introduces a new method that\ncombines the use of the Gemma 2B LLM with a classifier-LLM architecture to\nincorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically\nlowers the amount of data and compute power needed while achieving performance\nclose to that of cutting-edge methods. Notably, compared to current\nmethodologies, our methodology delivers an overall performance improvement of\n10%. The suggested architecture demonstrates the possibility of effective\ntransfer learning for EEG-based text production, remaining strong and\nfunctional even in the face of data limits. This work highlights the potential\nof integrating LLMs with EEG decoding to improve assistive technologies and\nimprove independence and communication for those with severe motor limitations.\nOur method pushes the limits of present capabilities and opens new paths for\nresearch and application in brain-computer interfaces by efficiently using the\nstrengths of pre-trained language models. This makes EEG-based text production\nmore accessible and efficient.", "AI": {"tldr": "This paper presents a novel method for EEG-based text production that integrates LLMs and classifiers to enhance efficiency and performance.", "motivation": "To overcome the challenges in EEG-based text production that require extensive data and processing power.", "method": "A classifier-LLM architecture combined with a Recurrent Neural Network (RNN) encoder is proposed to lower data and compute requirements while enhancing performance.", "result": "The proposed method improves overall performance by 10% compared to current methodologies, making EEG-based text production more accessible and efficient.", "conclusion": "Integrating LLMs with EEG decoding offers potential for better assistive technologies, improving communication for those with motor limitations.", "key_contributions": ["Introduction of a novel classifier-LLM architecture with RNN encoder for EEG-based text production", "Achieves comparable performance to state-of-the-art methods with significantly reduced data requirements", "Demonstrates effective transfer learning potential in brain-computer interfaces."], "limitations": "", "keywords": ["EEG", "large language models", "assistive technology", "brain-computer interfaces", "transfer learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.07314", "pdf": "https://arxiv.org/pdf/2509.07314.pdf", "abs": "https://arxiv.org/abs/2509.07314", "title": "In the Queue: Understanding How Reddit Moderators Use the Modqueue", "authors": ["Tanvi Bajpai", "Eshwar Chandrasekharan"], "categories": ["cs.HC"], "comment": "26 pages, 7 figures", "summary": "On Reddit, the moderation queue (modqueue) is a primary interface for\nmoderators to review reported content. Despite its central role in Reddit's\ncommunity-reliant moderation model, little is known about how moderators\nactually use it in practice. To address this gap, we surveyed 110 moderators,\nwho collectively oversee more than 400 unique subreddits, and asked them about\ntheir usage of the modqueue. Modqueue practices vary widely: some moderators\napproach it as a daily checklist, others as a hub to infer community-wide\npatterns, and many still find the queue insufficient to inform their moderation\ndecisions. We also identify persistent challenges around review coordination,\ninconsistent interface signals, and reliance on third-party tools. Taken\ntogether, we show the modqueue is neither a one-size-fits-all solution nor\nsufficient on its own for supporting moderator review. Our work highlights\ndesign opportunities for more modular, integrated, and customizable platform\ninfrastructures that better support the diversity of moderator workflows.", "AI": {"tldr": "This paper surveys Reddit moderators to understand their use of the moderation queue, revealing diverse practices and identifying challenges faced in moderation.", "motivation": "To explore how moderators use the moderation queue on Reddit and identify common challenges they face.", "method": "A survey of 110 moderators overseeing over 400 subreddits was conducted to gather data on modqueue practices and issues.", "result": "The study found varied practices in moderation queue usage among moderators, with persistent challenges in review coordination and reliance on external tools, indicating the need for a more customizable solution.", "conclusion": "The modqueue is not a universal tool for moderation, highlighting the need for improved infrastructure to better support diverse moderator workflows.", "key_contributions": ["Insight into moderator practices and challenges in using the modqueue on Reddit", "Identification of the need for customizable moderation tools", "Recommendations for improving platform infrastructure to support diverse workflows."], "limitations": "The study is based on self-reported data from a subset of Reddit moderators, which may not fully represent all users.", "keywords": ["moderation", "Reddit", "human-computer interaction", "surveys", "community management"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.07135", "pdf": "https://arxiv.org/pdf/2509.07135.pdf", "abs": "https://arxiv.org/abs/2509.07135", "title": "MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations", "authors": ["Ruggero Marino Lazzaroni", "Alessandro Angioi", "Michelangelo Puliga", "Davide Sanna", "Roberto Marras"], "categories": ["cs.CL"], "comment": "Accepted as an oral presentation at CLiC-it 2025", "summary": "Large language models (LLMs) show increasing potential in education, yet\nbenchmarks for non-English languages in specialized domains remain scarce. We\nintroduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on\nItalian medical university entrance examinations. Sourced from Edizioni Simone,\na leading preparatory materials publisher, MedBench-IT comprises 17,410\nexpert-written multiple-choice questions across six subjects (Biology,\nChemistry, Logic, General Culture, Mathematics, Physics) and three difficulty\nlevels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude\nseries) and resource-efficient open-source alternatives (<30B parameters)\nfocusing on practical deployability.\n  Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response\nconsistency, varying by subject), ordering bias analysis (minimal impact), and\nreasoning prompt evaluation. We also examined correlations between question\nreadability and model performance, finding a statistically significant but\nsmall inverse relationship. MedBench-IT provides a crucial resource for Italian\nNLP community, EdTech developers, and practitioners, offering insights into\ncurrent capabilities and standardized evaluation methodology for this critical\ndomain.", "AI": {"tldr": "MedBench-IT is a benchmark for evaluating LLMs on Italian medical entrance exams, comprising 17,410 expert-written questions across six subjects.", "motivation": "The scarcity of benchmarks for non-English LLMs in specialized domains, particularly in education, necessitates the creation of resources like MedBench-IT.", "method": "The benchmark includes multiple-choice questions and evaluates various LLMs for accuracy and practicality, alongside reproducibility and bias analyses.", "result": "The study found an 88.86% response consistency across subjects and a minimal impact from ordering bias, highlighting the challenges in LLM performance evaluation.", "conclusion": "MedBench-IT serves as a key resource for the Italian NLP community and EdTech developers, contributing to the understanding of LLM capabilities in specialized educational contexts.", "key_contributions": ["Introduction of the first benchmark for Italian medical exams (MedBench-IT)", "Evaluation of various LLMs including proprietary and open-source models", "Analysis of reasoning prompt effectiveness and question readability correlations."], "limitations": "Focus is on Italian language; results may not generalize to other languages or domains.", "keywords": ["Large Language Models", "MedBench-IT", "Italian Medical Exams", "NLP", "Benchmarking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.07334", "pdf": "https://arxiv.org/pdf/2509.07334.pdf", "abs": "https://arxiv.org/abs/2509.07334", "title": "SpecifyUI: Supporting Iterative UI Design Intent Expression through Structured Specifications and Generative AI", "authors": ["Yunnong Chen", "Chengwei Shi", "Liuqing Chen"], "categories": ["cs.HC"], "comment": "27 pages, 12 figures", "summary": "Large language models (LLMs) promise to accelerate UI design, yet current\ntools struggle with two fundamentals: externalizing designers' intent and\ncontrolling iterative change. We introduce SPEC, a structured, parameterized,\nhierarchical intermediate representation that exposes UI elements as\ncontrollable parameters. Building on SPEC, we present SpecifyUI, an interactive\nsystem that extracts SPEC from UI references via region segmentation and\nvision-language models, composes UIs across multiple sources, and supports\ntargeted edits at global, regional, and component levels. A multi-agent\ngenerator renders SPEC into high-fidelity designs, closing the loop between\nintent expression and controllable generation. Quantitative experiments show\nSPEC-based generation more faithfully captures reference intent than\nprompt-based baselines. In a user study with 16 professional designers,\nSpecifyUI significantly outperformed Stitch on intent alignment, design\nquality, controllability, and overall experience in human-AI co-creation. Our\nresults position SPEC as a specification-driven paradigm that shifts\nLLM-assisted design from one-shot prompting to iterative, collaborative\nworkflows.", "AI": {"tldr": "This paper introduces SPEC, a structured representation for UI design with LLMs, and SpecifyUI, an interactive system that enhances designer intent capture and controllability in UI generation.", "motivation": "To improve UI design workflows by addressing challenges in externalizing designer intent and controlling iterative changes in LLM-assisted design.", "method": "The authors present SPEC, a hierarchical intermediate representation, and SpecifyUI, which employs region segmentation and vision-language models to generate high-fidelity UI designs.", "result": "SPEC-based generation outperformed traditional prompt-based methods in capturing designer intent, with user studies indicating better intention alignment and user experience with SpecifyUI compared to existing tools.", "conclusion": "The study establishes SPEC as a novel approach to facilitate iterative and collaborative design processes using LLMs, enhancing the co-creation experience for designers.", "key_contributions": ["Introduction of SPEC as a structured representation for UI design.", "Development of SpecifyUI which utilizes SPEC to enhance controllability in UI generation.", "Demonstrated improvements in design quality and user experience compared to existing systems."], "limitations": "", "keywords": ["Large Language Models", "Human-Computer Interaction", "UI Design"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2509.07139", "pdf": "https://arxiv.org/pdf/2509.07139.pdf", "abs": "https://arxiv.org/abs/2509.07139", "title": "The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties", "authors": ["William Chen", "Chutong Meng", "Jiatong Shi", "Martijn Bartelds", "Shih-Heng Wang", "Hsiu-Hsuan Wang", "Rafael Mosquera", "Sara Hincapie", "Dan Jurafsky", "Antonis Anastasopoulos", "Hung-yi Lee", "Karen Livescu", "Shinji Watanabe"], "categories": ["cs.CL", "eess.AS"], "comment": "Interspeech 2025", "summary": "Recent improvements in multilingual ASR have not been equally distributed\nacross languages and language varieties. To advance state-of-the-art (SOTA) ASR\nmodels, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a\nnew test suite that consists of data from 200+ languages, accents, and dialects\nto evaluate SOTA multilingual speech models. The challenge also introduces an\nonline evaluation server based on DynaBench, allowing for flexibility in model\ndesign and architecture for participants. The challenge received 5 submissions\nfrom 3 teams, all of which outperformed our baselines. The best-performing\nsubmission achieved an absolute improvement in LID accuracy of 23% and a\nreduction in CER of 18% when compared to the best baseline on a general\nmultilingual test set. On accented and dialectal data, the best submission\nobtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance\nof community challenges in making speech technologies more inclusive.", "AI": {"tldr": "The Interspeech 2025 ML-SUPERB 2.0 Challenge evaluates state-of-the-art multilingual ASR models across 200+ languages, accents, and dialects, demonstrating significant improvements in speech technology inclusivity.", "motivation": "To address the unequal distribution of advancements in multilingual automatic speech recognition (ASR) across various languages and variations.", "method": "A new test suite was constructed with data from over 200 languages, accents, and dialects, and an online evaluation server was introduced for flexible model designs in the challenge.", "result": "Five submissions from three teams surpassed baseline performances; the top submission improved language identification (LID) accuracy by 23% and reduced character error rate (CER) by 18% on a general test set, with even greater improvements on accented and dialectal data.", "conclusion": "Community challenges play a crucial role in enhancing the inclusivity of speech technologies by fostering innovation and performance improvements among diverse model submissions.", "key_contributions": ["Creation of a comprehensive test suite for multilingual ASR", "Successful improvements over baseline ASR models", "Emphasis on community-driven solutions for inclusive speech technology"], "limitations": "", "keywords": ["speech recognition", "multilingual", "ASR", "community challenge", "DynaBench"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.07424", "pdf": "https://arxiv.org/pdf/2509.07424.pdf", "abs": "https://arxiv.org/abs/2509.07424", "title": "Feed-O-Meter: Fostering Design Feedback Skills through Role-playing Interactions with AI Mentee", "authors": ["Hyunseung Lim", "Dasom Choi", "DaEun Choi", "Sooyohn Nam", "Hwajung Hong"], "categories": ["cs.HC"], "comment": null, "summary": "Effective feedback, including critique and evaluation, helps designers\ndevelop design concepts and refine their ideas, supporting informed\ndecision-making throughout the iterative design process. However, in\nstudio-based design courses, students often struggle to provide feedback due to\na lack of confidence and fear of being judged, which limits their ability to\ndevelop essential feedback-giving skills. Recent advances in large language\nmodels (LLMs) suggest that role-playing with AI agents can let learners engage\nin multi-turn feedback without the anxiety of external judgment or the time\nconstraints of real-world settings. Yet prior studies have raised concerns that\nLLMs struggle to behave like real people in role-play scenarios, diminishing\nthe educational benefits of these interactions. Therefore, designing AI-based\nagents that effectively support learners in practicing and developing\nintellectual reasoning skills requires more than merely assigning the target\npersona's personality and role to the agent. By addressing these issues, we\npresent Feed-O-Meter, a novel system that employs carefully designed LLM-based\nagents to create an environment in which students can practice giving design\nfeedback. The system enables users to role-play as mentors, providing feedback\nto an AI mentee and allowing them to reflect on how that feedback impacts the\nAI mentee's idea development process. A user study (N=24) indicated that\nFeed-O-Meter increased participants' engagement and motivation through\nrole-switching and helped them adjust feedback to be more comprehensible for an\nAI mentee. Based on these findings, we discuss future directions for designing\nsystems to foster feedback skills in design education.", "AI": {"tldr": "Feed-O-Meter is a system utilizing LLM-based agents to enhance feedback skills in design education by allowing students to role-play providing feedback to an AI mentee.", "motivation": "To address the challenges students face in providing feedback due to a lack of confidence and fear of judgment, limiting their ability to develop essential feedback skills.", "method": "The system leverages LLM-based agents to create an interactive feedback environment where learners can practice giving feedback and role-play as mentors.", "result": "A user study showed that Feed-O-Meter increased engagement and motivation among participants and improved their feedback adjustment for AI mentees.", "conclusion": "The study highlights the potential for AI-powered systems to enhance feedback skills in design education and outlines future directions for development.", "key_contributions": ["Introduction of Feed-O-Meter system for feedback practice using LLMs", "Empirical findings on engagement and feedback comprehension improvement", "Insights into designing AI agents for educational purposes"], "limitations": "The study is limited by its small sample size (N=24) and the reliance on AI agents that may not fully replicate human interactions.", "keywords": ["feedback", "design education", "large language models", "role-playing", "AI agents"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.07142", "pdf": "https://arxiv.org/pdf/2509.07142.pdf", "abs": "https://arxiv.org/abs/2509.07142", "title": "Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models", "authors": ["Zhiyin Tan", "Jennifer D'Souza"], "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": "Accepted for publication in International Journal on Digital\n  Libraries (IJDL)", "summary": "This study presents a framework for automated evaluation of dynamically\nevolving topic models using Large Language Models (LLMs). Topic modeling is\nessential for organizing and retrieving scholarly content in digital library\nsystems, helping users navigate complex and evolving knowledge domains.\nHowever, widely used automated metrics, such as coherence and diversity, often\ncapture only narrow statistical patterns and fail to explain semantic failures\nin practice. We introduce a purpose-oriented evaluation framework that employs\nnine LLM-based metrics spanning four key dimensions of topic quality: lexical\nvalidity, intra-topic semantic soundness, inter-topic structural soundness, and\ndocument-topic alignment soundness. The framework is validated through\nadversarial and sampling-based protocols, and is applied across datasets\nspanning news articles, scholarly publications, and social media posts, as well\nas multiple topic modeling methods and open-source LLMs. Our analysis shows\nthat LLM-based metrics provide interpretable, robust, and task-relevant\nassessments, uncovering critical weaknesses in topic models such as redundancy\nand semantic drift, which are often missed by traditional metrics. These\nresults support the development of scalable, fine-grained evaluation tools for\nmaintaining topic relevance in dynamic datasets. All code and data supporting\nthis work are accessible at\nhttps://github.com/zhiyintan/topic-model-LLMjudgment.", "AI": {"tldr": "This paper proposes a framework using Large Language Models for evaluating topic models, addressing limitations of traditional metrics.", "motivation": "To improve the evaluation of topic models in digital libraries by addressing the shortcomings of conventional metrics.", "method": "The framework employs nine LLM-based metrics across four dimensions of topic quality, validated through various protocols on diverse datasets.", "result": "The LLM-based metrics revealed critical weaknesses in topic models, such as redundancy and semantic drift, which traditional metrics missed.", "conclusion": "The framework offers robust, interpretable assessments and encourages the development of more effective evaluation tools for dynamic datasets.", "key_contributions": ["Introduction of a purpose-oriented evaluation framework for topic models using LLMs", "Identification of critical weaknesses in traditional topic model evaluations", "Accessibility of all code and data for further research"], "limitations": "", "keywords": ["Topic Modeling", "Large Language Models", "Evaluation Framework", "Digital Libraries", "Topic Quality"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.07502", "pdf": "https://arxiv.org/pdf/2509.07502.pdf", "abs": "https://arxiv.org/abs/2509.07502", "title": "Social Media Clones: Exploring the Impact of Social Delegation with AI Clones through a Design Workbook Study", "authors": ["Jackie Liu", "Mehrnoosh Sadat Shirvani", "Hwajung Hong", "Ig-Jae Kim", "Dongwook Yoon"], "categories": ["cs.HC"], "comment": null, "summary": "Social media clones are AI-powered social delegates of ourselves created\nusing our personal data. As our identities and online personas intertwine,\nthese technologies have the potential to greatly enhance our social media\nexperience. If mismanaged, however, these clones may also pose new risks to our\nsocial reputation and online relationships. To set the foundation for a\nproductive and responsible integration, we set out to understand how social\nmedia clones will impact our online behavior and interactions. We conducted a\nseries of semi-structured interviews introducing eight speculative clone\nconcepts to 32 social media users through a design workbook. Applying existing\nwork in AI-mediated communication in the context of social media, we found that\nalthough clones can offer convenience and comfort, they can also threaten the\nuser's authenticity and increase skepticism within the online community. As a\nresult, users tend to behave more like their clones to mitigate discrepancies\nand interaction breakdowns. These findings are discussed through the lens of\npast literature in identity and impression management to highlight challenges\nin the adoption of social media clones by the general public, and propose\ndesign considerations for their successful integration into social media\nplatforms.", "AI": {"tldr": "The paper explores the impact of AI-powered social media clones on user behavior and online interactions, highlighting both potential benefits and risks.", "motivation": "To understand the effects of social media clones on online behavior and relationships due to their integration with user data and personal identity.", "method": "Conducted semi-structured interviews with 32 social media users, presenting eight speculative clone concepts through a design workbook.", "result": "Clones can provide convenience but may threaten authenticity and increase skepticism within online communities, leading users to mimic their clones to avoid interaction issues.", "conclusion": "The paper discusses the challenges of adopting social media clones and provides design considerations for their effective integration into platforms.", "key_contributions": ["Identified user behavior changes due to clones' influence", "Highlighted risks associated with authenticity in social media", "Proposed design considerations for better integration of clones"], "limitations": "Limited to user perceptions and speculative concepts without quantitative analysis.", "keywords": ["social media", "AI clones", "user behavior", "identity management", "interaction design"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.07177", "pdf": "https://arxiv.org/pdf/2509.07177.pdf", "abs": "https://arxiv.org/abs/2509.07177", "title": "Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector", "authors": ["Amal Chebbi", "Babajide Kolade"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have demonstrated impressive capabilities across\nvarious domains. However, their general-purpose nature often limits their\neffectiveness in specialized fields such as energy, where deep technical\nexpertise and precise domain knowledge are essential. In this paper, we\nintroduce EnergyGPT, a domain-specialized language model tailored for the\nenergy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised\nFine-Tuning on a high-quality, curated corpus of energy-related texts. We\npresent a complete development pipeline, including data collection and\ncuration, model fine-tuning, benchmark design and LLM-judge choice, evaluation\nand deployment. Through this work, we demonstrate that our training strategy\nenables improvements in domain relevance and performance without the need for\nlarge-scale infrastructure. By evaluating the performance of the model using\ndomain-specific question-answering benchmarks, our results demonstrate that\nEnergyGPT outperforms the base model in most of the energy-related language\nunderstanding and generation tasks.", "AI": {"tldr": "EnergyGPT is a specialized LLM for the energy sector, outperforming its base model in energy-related tasks.", "motivation": "To enhance the effectiveness of language models in specialized fields like energy that require deep technical expertise.", "method": "Fine-tuning LLaMA 3.1-8B model with a curated corpus of energy-related texts through a comprehensive development pipeline.", "result": "EnergyGPT shows improved performance on domain-specific question-answering benchmarks compared to the base model.", "conclusion": "The training strategy allows for improved domain relevance and performance without extensive infrastructure.", "key_contributions": ["Introduction of EnergyGPT, a specialized model for the energy sector", "Development pipeline from data curation to deployment", "Benchmarking results demonstrating superior performance over base model"], "limitations": "", "keywords": ["EnergyGPT", "domain-specific language model", "machine learning", "energy sector", "LLM"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.07740", "pdf": "https://arxiv.org/pdf/2509.07740.pdf", "abs": "https://arxiv.org/abs/2509.07740", "title": "Digital Twins for Extended Reality Tourism: User Experience Evaluation Across User Groups", "authors": ["Maximilian Warsinke", "Francesco Vona", "Tanja Kojić", "Jan-Niklas Voigt-Antons", "Sebastian Möller"], "categories": ["cs.HC"], "comment": "Submitted manuscript. The final version was presented at XR Salento\n  2025", "summary": "This study evaluates the user experience (UX) in extended reality (XR)\ntourism of two digital twin-based applications: an Augmented Reality Virtual\nTour (AR-VT) for enhanced on-site visits and a Virtual Reality Virtual Tour\n(VR-VT) for remote exploration. Using a quantitative exploratory approach, 84\nparticipants from Spain and Germany, divided into three sample groups, assessed\nUX, task load, presence, cybersickness, and emotional response through\nstandardized questionnaires. Findings indicate that both applications provided\na low task load and high enjoyment. The VR-based tour enhanced presence but\nposed usability and cybersickness challenges, while the AR-based tour achieved\nhigh UX ratings, with qualitative feedback suggesting areas for refinement.\nCorrelation analysis revealed significant relationships between age, prior XR\nexperience, and technological affinity with the measured metrics for both\napplications. These results highlight the importance of well-designed\nexperiences tailored to XR novices, reinforcing the critical role of UX in\ndigital twin-based XR tourism.", "AI": {"tldr": "This study evaluates user experience in XR tourism using AR and VR applications, finding high enjoyment but usability challenges in VR.", "motivation": "To assess user experience in extended reality tourism and understand how digital twin-based applications perform in this context.", "method": "A quantitative exploratory approach involving 84 participants from Spain and Germany, assessing UX, task load, presence, cybersickness, and emotional response using standardized questionnaires.", "result": "Both AR and VR applications yielded low task load and high enjoyment, though VR faced usability and cybersickness issues.", "conclusion": "Well-designed XR experiences are essential, especially for novices, underlining the importance of user experience in digital twin-based XR tourism.", "key_contributions": ["Evaluation of user experience in XR tourism", "Comparison of AR and VR applications", "Insights into the impact of demographics on UX metrics"], "limitations": "The study's sample size is limited to Spain and Germany, which may affect generalizability.", "keywords": ["extended reality", "user experience", "digital twin", "augmented reality", "virtual reality"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.07188", "pdf": "https://arxiv.org/pdf/2509.07188.pdf", "abs": "https://arxiv.org/abs/2509.07188", "title": "DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge", "authors": ["Zonghai Yao", "Michael Sun", "Won Seok Jang", "Sunjae Kwon", "Soie Kwon", "Hong Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Equal contribution for the first two authors. To appear in the\n  proceedings of the Main Conference on Empirical Methods in Natural Language\n  Processing (EMNLP) 2025", "summary": "Discharge communication is a critical yet underexplored component of patient\ncare, where the goal shifts from diagnosis to education. While recent large\nlanguage model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they\nfail to evaluate models' ability to support patients after the visit. We\nintroduce DischargeSim, a novel benchmark that evaluates LLMs on their ability\nto act as personalized discharge educators. DischargeSim simulates post-visit,\nmulti-turn conversations between LLM-driven DoctorAgents and PatientAgents with\ndiverse psychosocial profiles (e.g., health literacy, education, emotion).\nInteractions are structured across six clinically grounded discharge topics and\nassessed along three axes: (1) dialogue quality via automatic and LLM-as-judge\nevaluation, (2) personalized document generation including free-text summaries\nand structured AHRQ checklists, and (3) patient comprehension through a\ndownstream multiple-choice exam. Experiments across 18 LLMs reveal significant\ngaps in discharge education capability, with performance varying widely across\npatient profiles. Notably, model size does not always yield better education\noutcomes, highlighting trade-offs in strategy use and content prioritization.\nDischargeSim offers a first step toward benchmarking LLMs in post-visit\nclinical education and promoting equitable, personalized patient support.", "AI": {"tldr": "DischargeSim benchmarks LLMs for post-visit patient education by simulating conversations between DoctorAgents and PatientAgents with various profiles.", "motivation": "To address the lack of assessment of LLMs in supporting patients after medical visits and improve discharge communication.", "method": "DischargeSim simulates multi-turn dialogues between LLM-driven DoctorAgents and PatientAgents, structured around six discharge topics and evaluated on dialogue quality, document generation, and patient comprehension.", "result": "Experiments show significant gaps in LLM performance for discharge education, varying with patient profiles; larger models do not guarantee better outcomes.", "conclusion": "DischargeSim is a first step towards better evaluating LLMs for personalized post-visit clinical education.", "key_contributions": ["Introduction of DischargeSim, a benchmark for LLMs", "Evaluation of patient education across diverse psychosocial profiles", "Highlighting performance variations unrelated to model size"], "limitations": "", "keywords": ["discharge communication", "large language models", "patient education"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.07742", "pdf": "https://arxiv.org/pdf/2509.07742.pdf", "abs": "https://arxiv.org/abs/2509.07742", "title": "Enhancing Online Learning by Integrating Biosensors and Multimodal Learning Analytics for Detecting and Predicting Student Behavior: A Review", "authors": ["Alvaro Becerra", "Ruth Cobos", "Charles Lang"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "Accepted for publication in Behaviour & Information Technology\n  (Taylor & Francis). Final published version will be available soon at\n  https://www.tandfonline.com/journals/tbit20", "summary": "In modern online learning, understanding and predicting student behavior is\ncrucial for enhancing engagement and optimizing educational outcomes. This\nsystematic review explores the integration of biosensors and Multimodal\nLearning Analytics (MmLA) to analyze and predict student behavior during\ncomputer-based learning sessions. We examine key challenges, including emotion\nand attention detection, behavioral analysis, experimental design, and\ndemographic considerations in data collection. Our study highlights the growing\nrole of physiological signals, such as heart rate, brain activity, and\neye-tracking, combined with traditional interaction data and self-reports to\ngain deeper insights into cognitive states and engagement levels. We synthesize\nfindings from 54 key studies, analyzing commonly used methodologies such as\nadvanced machine learning algorithms and multimodal data pre-processing\ntechniques. The review identifies current research trends, limitations, and\nemerging directions in the field, emphasizing the transformative potential of\nbiosensor-driven adaptive learning systems. Our findings suggest that\nintegrating multimodal data can facilitate personalized learning experiences,\nreal-time feedback, and intelligent educational interventions, ultimately\nadvancing toward a more customized and adaptive online learning experience.", "AI": {"tldr": "This systematic review investigates the use of biosensors and Multimodal Learning Analytics (MmLA) to understand and predict student behavior in online learning environments.", "motivation": "To enhance engagement and optimize educational outcomes by understanding and predicting student behavior.", "method": "A systematic review of 54 studies analyzing the integration of biosensors, emotion and attention detection techniques, and advanced machine learning algorithms.", "result": "The synthesis of findings reveals the potential of combining physiological signals with traditional interaction data to gain insights into cognitive states and improve personalized learning experiences.", "conclusion": "Integrating multimodal data can lead to adaptive learning systems that provide real-time feedback and personalized educational interventions.", "key_contributions": ["Integration of biosensors with traditional learning analytics", "Identification of current trends and limitations in the field", "Emphasis on the potential for personalized and adaptive online learning experiences"], "limitations": "Challenges in emotion detection, experimental design, and demographic factors in data collection.", "keywords": ["Multimodal Learning Analytics", "biosensors", "student behavior prediction", "adaptive learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.07190", "pdf": "https://arxiv.org/pdf/2509.07190.pdf", "abs": "https://arxiv.org/abs/2509.07190", "title": "Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation", "authors": ["Zahra Atf", "Peter R Lewis"], "categories": ["cs.CL", "cs.HC"], "comment": "This paper was accepted for presentation at the 35th IEEE\n  International Conference on Collaborative Advances in Software and Computing.\n  Conference website:https://conf.researchr.org/home/cascon-2025", "summary": "Large language models (LLMs) are increasingly used in high-stakes settings,\nwhere explaining uncertainty is both technical and ethical. Probabilistic\nmethods are often opaque and misaligned with expectations of transparency. We\npropose a framework based on rule-based moral principles for handling\nuncertainty in LLM-generated text. Using insights from moral psychology and\nvirtue ethics, we define rules such as precaution, deference, and\nresponsibility to guide responses under epistemic or aleatoric uncertainty.\nThese rules are encoded in a lightweight Prolog engine, where uncertainty\nlevels (low, medium, high) trigger aligned system actions with plain-language\nrationales. Scenario-based simulations benchmark rule coverage, fairness, and\ntrust calibration. Use cases in clinical and legal domains illustrate how moral\nreasoning can improve trust and interpretability. Our approach offers a\ntransparent, lightweight alternative to probabilistic models for socially\nresponsible natural language generation.", "AI": {"tldr": "This paper proposes a framework incorporating rule-based moral principles to handle uncertainty in LLM-generated text, improving trust and interpretability in high-stakes settings.", "motivation": "The need for transparency in uncertainty management for LLMs in high-stakes environments such as clinical and legal settings.", "method": "A framework based on moral psychology and virtue ethics is implemented using a lightweight Prolog engine to guide response actions under uncertainty.", "result": "The approach demonstrates better rule coverage, fairness, and trust calibration in simulations, and highlights use cases where moral reasoning enhances interpretation.", "conclusion": "The proposed method offers a socially responsible and transparent alternative to existing probabilistic models for natural language generation.", "key_contributions": ["Framework integrates moral principles for handling uncertainty", "Use of Prolog engine for rule encoding", "Illustration of use cases that improve trust and interpretability"], "limitations": "", "keywords": ["Large Language Models", "Uncertainty Management", "Moral Reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.07819", "pdf": "https://arxiv.org/pdf/2509.07819.pdf", "abs": "https://arxiv.org/abs/2509.07819", "title": "LLMs in Wikipedia: Investigating How LLMs Impact Participation in Knowledge Communities", "authors": ["Moyan Zhou", "Soobin Cho", "Loren Terveen"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are reshaping knowledge production as community\nmembers increasingly incorporate them into their contribution workflows.\nHowever, participating in knowledge communities involves more than just\ncontributing content - it is also a deeply social process. While communities\nmust carefully consider appropriate and responsible LLM integration, the\nabsence of concrete norms has left individual editors to experiment and\nnavigate LLM use on their own. Understanding how LLMs influence community\nparticipation is therefore critical in shaping future norms and supporting\neffective adoption. To address this gap, we investigated Wikipedia, one of the\nlargest knowledge production communities, to understand 1) how LLMs influence\nthe ways editors contribute content, 2) what strategies editors leverage to\nalign LLM outputs with community norms, and 3) how other editors in the\ncommunity respond to LLM-assisted contributions. Through interviews with 16\nWikipedia editors who had used LLMs for their edits, we found that 1) LLMs\naffected the content contributions for experienced and new editors differently;\n2) aligning LLM outputs with community norms required tacit knowledge that\noften challenged newcomers; and 3) as a result, other editors responded to\nLLM-assisted edits differently depending on the editors' expertise level. Based\non these findings, we challenge existing models of newcomer involvement and\npropose design implications for LLMs that support community engagement through\nscaffolding, teaching, and context awareness.", "AI": {"tldr": "This study investigates how LLMs impact editorial contributions in Wikipedia, focusing on the effects on different editor experience levels and the alignment of LLM outputs with community norms.", "motivation": "Understanding the integration of LLMs in knowledge communities is essential to shape future norms and support effective usage.", "method": "Interviews with 16 Wikipedia editors who used LLMs for content contributions.", "result": "LLMs influenced content contributions differently for experienced and new editors; aligning LLM outputs with community norms was challenging for newcomers, and responses to LLM-assisted edits varied based on editor expertise.", "conclusion": "The findings challenge existing newcomer involvement models and suggest design implications for better supporting community engagement with LLMs.", "key_contributions": ["Investigation of LLM impact on editorial contributions in Wikipedia", "Identification of challenges faced by newcomers in aligning LLM outputs with community norms", "Proposed design implications for enhancing community engagement with LLMs"], "limitations": "", "keywords": ["Large Language Models", "Wikipedia", "Community Engagement", "Knowledge Production", "Newcomer Involvement"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.07274", "pdf": "https://arxiv.org/pdf/2509.07274.pdf", "abs": "https://arxiv.org/abs/2509.07274", "title": "LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade", "authors": ["Aida Kostikova", "Ole Pütz", "Steffen Eger", "Olga Sabelfeld", "Benjamin Paassen"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Migration has been a core topic in German political debate, from millions of\nexpellees post World War II over labor migration to refugee movements in the\nrecent past. Studying political speech regarding such wide-ranging phenomena in\ndepth traditionally required extensive manual annotations, limiting the scope\nof analysis to small subsets of the data. Large language models (LLMs) have the\npotential to partially automate even complex annotation tasks. We provide an\nextensive evaluation of a multiple LLMs in annotating (anti-)solidarity\nsubtypes in German parliamentary debates compared to a large set of thousands\nof human reference annotations (gathered over a year). We evaluate the\ninfluence of model size, prompting differences, fine-tuning, historical versus\ncontemporary data; and we investigate systematic errors. Beyond methodological\nevaluation, we also interpret the resulting annotations from a social science\nlense, gaining deeper insight into (anti-)solidarity trends towards migrants in\nthe German post-World War II period and recent past. Our data reveals a high\ndegree of migrant-directed solidarity in the postwar period, as well as a\nstrong trend towards anti-solidarity in the German parliament since 2015,\nmotivating further research. These findings highlight the promise of LLMs for\npolitical text analysis and the importance of migration debates in Germany,\nwhere demographic decline and labor shortages coexist with rising polarization.", "AI": {"tldr": "This paper evaluates the use of large language models (LLMs) in automating the annotation of political speech related to migration in Germany, comparing LLM-generated annotations with extensive human references. It unveils significant trends in (anti-)solidarity towards migrants in parliamentary debates over time.", "motivation": "To explore the potential of LLMs in automating the annotation process for analyzing political discourse on migration, an area traditionally relying on manual effort.", "method": "The study evaluates multiple LLMs in annotating (anti-)solidarity subtypes in German parliamentary debates through comparison against thousands of human annotations, assessing factors like model size, prompting techniques, and fine-tuning.", "result": "Findings highlight a significant presence of migrant-directed solidarity in the post-World War II period, contrasted by a rising trend of anti-solidarity in German parliament since 2015, providing valuable insights into evolving public sentiment.", "conclusion": "The research emphasizes the potential of LLMs for facilitating political text analysis while underlining the critical nature of migration debates in Germany amid demographic challenges and polarization.", "key_contributions": ["Evaluation of LLMs for political text annotation", "Insights into historical and contemporary solidarity trends", "Methodological framework for analyzing political speech using LLMs"], "limitations": "Potential systematic errors in model annotations and reliance on historical context could influence findings.", "keywords": ["Large Language Models", "Political Text Analysis", "Migration Studies"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.07863", "pdf": "https://arxiv.org/pdf/2509.07863.pdf", "abs": "https://arxiv.org/abs/2509.07863", "title": "NeuroGaze: A Hybrid EEG and Eye-Tracking Brain-Computer Interface for Hands-Free Interaction in Virtual Reality", "authors": ["Kyle Coutray", "Wanyea Barbel", "Zack Groth", "Joseph J LaViola Jr"], "categories": ["cs.HC"], "comment": null, "summary": "Brain-Computer Interfaces (BCIs) have traditionally been studied in clinical\nand laboratory contexts, but the rise of consumer-grade devices now allows\nexploration of their use in daily activities. Virtual reality (VR) provides a\nparticularly relevant domain, where existing input methods often force\ntrade-offs between speed, accuracy, and physical effort. This study introduces\nNeuroGaze, a hybrid interface combining electroencephalography (EEG) with eye\ntracking to enable hands-free interaction in immersive VR. Twenty participants\ncompleted a 360{\\deg} cube-selection task using three different input methods:\nVR controllers, gaze combined with a pinch gesture, and NeuroGaze. Performance\nwas measured by task completion time and error rate, while workload was\nevaluated using the NASA Task Load Index (NASA-TLX). NeuroGaze successfully\nsupported target selection with off-the-shelf hardware, producing fewer errors\nthan the alternative methods but requiring longer completion times, reflecting\na classic speed-accuracy tradeoff. Workload analysis indicated reduced physical\ndemand for NeuroGaze compared to controllers, though overall ratings and user\npreferences were mixed. These findings demonstrate the feasibility of hybrid\nEEG+gaze systems for everyday VR use, highlighting their ergonomic benefits and\ninclusivity potential. Although not yet competitive in speed, NeuroGaze points\ntoward a practical role for consumer-grade BCIs in accessibility and\nlong-duration applications, and underscores the need for improved EEG signal\nprocessing and adaptive multimodal integration to enhance future performance.", "AI": {"tldr": "NeuroGaze, a hybrid interface using EEG and eye tracking, enables hands-free interaction in VR, outperforming traditional input methods in error reduction but facing speed challenges.", "motivation": "To explore the use of Brain-Computer Interfaces (BCIs) in daily activities, particularly in Virtual Reality (VR), leveraging consumer-grade devices for hands-free interaction.", "method": "A 360° cube-selection task was performed by twenty participants using three input methods: VR controllers, gaze with a pinch gesture, and NeuroGaze; performance metrics included task completion time, error rates, and workload assessments using NASA-TLX.", "result": "NeuroGaze demonstrated fewer selection errors compared to VR controllers, but longer completion times, indicating a speed-accuracy tradeoff; it required less physical effort.", "conclusion": "NeuroGaze is a feasible solution for hands-free interaction in VR, supporting inclusivity and encouraging further development in EEG signal processing and multimodal integration.", "key_contributions": ["Introduction of NeuroGaze as a hybrid EEG and gaze-based interaction method for VR.", "Demonstration of fewer selection errors with NeuroGaze compared to traditional VR controllers.", "Highlighting the ergonomic advantages and inclusivity potential of BCIs in consumer applications."], "limitations": "NeuroGaze is not yet competitive in speed when compared to traditional methods and user preferences were mixed regarding overall interactions.", "keywords": ["Brain-Computer Interfaces", "Virtual Reality", "EEG", "Eye Tracking", "Hands-Free Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.07301", "pdf": "https://arxiv.org/pdf/2509.07301.pdf", "abs": "https://arxiv.org/abs/2509.07301", "title": "Causal Attention with Lookahead Keys", "authors": ["Zhuoqing Song", "Peng Sun", "Huizhuo Yuan", "Quanquan Gu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In standard causal attention, each token's query, key, and value (QKV) are\nstatic and encode only preceding context. We introduce CAuSal aTtention with\nLookahead kEys (CASTLE), an attention mechanism that continually updates each\ntoken's keys as the context unfolds. We term these updated keys lookahead keys\nbecause they belong to earlier positions yet integrate information from tokens\nthat appear later relative to those positions, while strictly preserving the\nautoregressive property. Although the mechanism appears sequential, we derive a\nmathematical equivalence that avoids explicitly materializing lookahead keys at\neach position and enables efficient parallel training. On language modeling\nbenchmarks, CASTLE consistently outperforms standard causal attention across\nmodel scales, reducing validation perplexity and improving performance on a\nrange of downstream tasks.", "AI": {"tldr": "CASTLE introduces an adaptive attention mechanism that updates keys based on upcoming context while preserving autoregressive properties, leading to improved language modeling performance.", "motivation": "Standard causal attention uses static QKV encodings that limit the integration of future contextual information, necessitating a more dynamic approach to information processing in autoregressive models.", "method": "CASTLE employs dynamically updated lookahead keys that leverage information from future tokens relative to their preceding positions without violating the sequential processing required in autoregressive settings.", "result": "CASTLE demonstrates superior performance over traditional causal attention across various language modeling benchmarks, indicated by reduced validation perplexity and enhanced downstream task outcomes.", "conclusion": "The proposed CASTLE mechanism provides an efficient alternative to standard causal attention, facilitating better parallel training and improved model performance in language tasks.", "key_contributions": ["Introduction of lookahead keys in causal attention", "Mathematical framework for efficient parallel processing", "Demonstrated improvement in language modeling tasks and perplexity metrics"], "limitations": "", "keywords": ["Causal attention", "lookahead keys", "language modeling", "autoregressive models", "machine learning"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2509.07871", "pdf": "https://arxiv.org/pdf/2509.07871.pdf", "abs": "https://arxiv.org/abs/2509.07871", "title": "An Enactivist Approach to Human-Computer Interaction: Bridging the Gap Between Human Agency and Affordances", "authors": ["Angjelin Hila"], "categories": ["cs.HC", "H.5"], "comment": "Published in HCI International 2025", "summary": "Emerging paradigms in XR, AI, and BCI contexts necessitate novel theoretical\nframeworks for understanding human autonomy and agency in HCI. Drawing from\nenactivist theories of cognition, we conceptualize human agents as\nself-organizing, operationally closed systems that actively enact their\ncognitive domains through dynamic interaction with their environments. To\ndevelop measurable variables aligned with this framework, we introduce\n\"feelings of agency\" (FoA) as an alternative to the established construct of\n\"sense of agency\" (SoA), refining Synofzyk's multifactorial weighting model and\noffering a novel conceptual pathway for overcoming gaps in the dominant\ncomparator model. We define FoA as comprising two subconstructs: affective\nengagement and volitional attention, which we operationalize through integrated\nneurodynamic indicators (valence, arousal, cross frequency coupling within the\ndorsal attention system) and first-person phenomenological reports. We argue\nthat these neurophenomenological indicators provide richer, more actionable\ninsights for digital affordance design, particularly in XR, BCI, Human AI\nInteraction (HAX), and generative AI environments. Our framework aims to inform\nand inspire design parameters that significantly enhance human agency in\nrapidly evolving interactive domains.", "AI": {"tldr": "This paper introduces a novel framework for understanding human autonomy in HCI by proposing 'feelings of agency' (FoA) as a measurable construct crucial for design in XR and AI contexts.", "motivation": "The need for new theoretical frameworks arises from emerging paradigms in XR, AI, and BCI contexts that challenge traditional views of human autonomy and agency in HCI.", "method": "The authors conceptualize human agents as self-organizing systems and operationalize the construct of 'feelings of agency' (FoA) with two subconstructs linked to neurodynamic indicators and phenomenological reports.", "result": "The framework offers actionable insights for designing digital affordances in XR, BCI, Human AI Interaction, and generative AI environments, with a focus on enhancing human agency.", "conclusion": "The proposed neurophenomenological indicators can lead to improved design parameters that enhance human agency in rapidly evolving interactive domains.", "key_contributions": ["Introduction of 'feelings of agency' (FoA) as a measurable construct.", "Refinement of the multifactorial weighting model using integrated neurodynamic indicators.", "Provision of actionable insights for digital affordance design in emerging technologies."], "limitations": "", "keywords": ["Human-Computer Interaction", "Agency", "Neurophenomenology", "XR", "AI"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2509.07308", "pdf": "https://arxiv.org/pdf/2509.07308.pdf", "abs": "https://arxiv.org/abs/2509.07308", "title": "Basis Vector Metric: A Method for Robust Open-Ended State Change Detection", "authors": ["David Oprea", "Sam Powers"], "categories": ["cs.CL", "cs.AI"], "comment": "24 pages", "summary": "We test a new method, which we will abbreviate using the acronym BVM (Basis\nVectors Method), in its ability to judge the state changes in images through\nusing language embeddings. We used the MIT-States dataset, containing about\n53,000 images, to gather all of our data, which has 225 nouns and 115\nadjectives, with each noun having about 9 different adjectives, forming\napproximately 1000 noun-adjective pairs. For our first experiment, we test our\nmethod's ability to determine the state of each noun class separately against\nother metrics for comparison. These metrics are cosine similarity, dot product,\nproduct quantization, binary index, Naive Bayes, and a custom neural network.\nAmong these metrics, we found that our proposed BVM performs the best in\nclassifying the states for each noun. We then perform a second experiment where\nwe try using BVM to determine if it can differentiate adjectives from one\nanother for each adjective separately. We compared the abilities of BVM to\ndifferentiate adjectives against the proposed method the MIT-States paper\nsuggests: using a logistic regression model. In the end, we did not find\nconclusive evidence that our BVM metric could perform better than the logistic\nregression model at discerning adjectives. Yet, we were able to find evidence\nfor possible improvements to our method; this leads to the chance of increasing\nour method's accuracy through certain changes in our methodologies.", "AI": {"tldr": "This paper introduces the Basis Vectors Method (BVM) for assessing image state changes using language embeddings, showing promising results in classifying nouns but inconclusive outcomes in differentiating adjectives compared to logistic regression.", "motivation": "The paper aims to improve the classification of state changes in images through a new method called BVM, leveraging the MIT-States dataset.", "method": "The authors utilized the MIT-States dataset and compared BVM against various metrics like cosine similarity, dot product, and logistic regression in two experiments: one for nouns and another for adjectives.", "result": "BVM outperformed traditional metrics in classifying noun states but did not conclusively outperform logistic regression in distinguishing between adjectives.", "conclusion": "While BVM shows potential for noun classification, further refinements are necessary to enhance its accuracy, especially in differentiating adjectives.", "key_contributions": ["Introduction of the Basis Vectors Method (BVM) for image state classification", "Performance comparison of BVM with existing metrics like cosine similarity and logistic regression", "Identification of areas for methodological improvements to increase accuracy"], "limitations": "Inconclusive evidence of BVM's superiority in differentiating adjectives compared to logistic regression and need for methodological refinements.", "keywords": ["Basis Vectors Method", "image state classification", "language embeddings"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2509.07873", "pdf": "https://arxiv.org/pdf/2509.07873.pdf", "abs": "https://arxiv.org/abs/2509.07873", "title": "A Robot That Listens: Enhancing Self-Disclosure and Engagement Through Sentiment-based Backchannels and Active Listening", "authors": ["Hieu Tran", "Go-Eum Cha", "Sooyeon Jeong"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "As social robots get more deeply integrated intoour everyday lives, they will\nbe expected to engage in meaningful conversations and exhibit socio-emotionally\nintelligent listening behaviors when interacting with people. Active listening\nand backchanneling could be one way to enhance robots' communicative\ncapabilities and enhance their effectiveness in eliciting deeper\nself-disclosure, providing a sense of empathy,and forming positive rapport and\nrelationships with people.Thus, we developed an LLM-powered social robot that\ncan exhibit contextually appropriate sentiment-based backchannelingand active\nlistening behaviors (active listening+backchanneling) and compared its efficacy\nin eliciting people's self-disclosurein comparison to robots that do not\nexhibit any of these listening behaviors (control) and a robot that only\nexhibitsbackchanneling behavior (backchanneling-only). Through ourexperimental\nstudy with sixty-five participants, we found theparticipants who conversed with\nthe active listening robot per-ceived the interactions more positively, in\nwhich they exhibited the highest self-disclosures, and reported the strongest\nsenseof being listened to. The results of our study suggest that the\nimplementation of active listening behaviors in social robotshas the potential\nto improve human-robot communication andcould further contribute to the\nbuilding of deeper human-robot relationships and rapport.", "AI": {"tldr": "This paper explores an LLM-powered social robot that implements active listening and backchanneling behaviors to enhance human-robot communication and rapport.", "motivation": "To improve social robots' capabilities in engaging in meaningful conversations and establishing rapport through socio-emotional intelligence.", "method": "An experimental study with sixty-five participants comparing interactions with a robot exhibiting active listening and backchanneling, a control robot, and a backchanneling-only robot.", "result": "Participants interacting with the active listening robot reported more positive perceptions, higher self-disclosure, and a stronger sense of being listened to compared to other conditions.", "conclusion": "Incorporating active listening behaviors in social robots can enhance communication and foster deeper human-robot relationships.", "key_contributions": ["Demonstrated the efficacy of active listening behaviors in a social robot context.", "Provided empirical evidence linking enhanced robot behaviors to increased self-disclosure by users.", "Highlighted the importance of socio-emotional intelligence in human-robot interactions."], "limitations": "", "keywords": ["social robots", "active listening", "human-robot interaction", "backchanneling", "self-disclosure"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.07309", "pdf": "https://arxiv.org/pdf/2509.07309.pdf", "abs": "https://arxiv.org/abs/2509.07309", "title": "Instance-level Performance Prediction for Long-form Generation Tasks", "authors": ["Chi-Yang Hsu", "Alexander Braylan", "Yiheng Su", "Omar Alonso", "Matthew Lease"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We motivate and share a new benchmark for instance-level performance\nprediction of long-form generation tasks having multi-faceted, fine-grained\nquality metrics. Our task-, model- and metric-agnostic formulation predicts\ncontinuous evaluation metric scores given only black-box model inputs and\noutputs. Beyond predicting point estimates of metric scores, the benchmark also\nrequires inferring prediction intervals to quantify uncertainty around point\nestimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs,\nbaselines, and metrics per task. We show that scores can be effectively\npredicted across long-form generation tasks using as few as 16 training\nexamples. Overall, we introduce a novel and useful task, a valuable benchmark\nto drive progress, and baselines ready for practical adoption today.", "AI": {"tldr": "This paper presents a benchmark for predicting performance in long-form generation tasks using a task-agnostic approach to evaluate model quality.", "motivation": "The need for a standardized method to evaluate long-form generation tasks, particularly with regards to predicting quality metrics.", "method": "The paper proposes a task-, model-, and metric-agnostic framework that predicts continuous evaluation metric scores based on black-box model inputs and outputs, while also inferring prediction intervals for uncertainty quantification.", "result": "Demonstrates that model scores can be predicted effectively across various long-form tasks using only a small amount of training data (16 examples).", "conclusion": "The introduced benchmark is novel, valuable for advancing research, and provides ready-to-use baselines for practical application.", "key_contributions": ["Introduction of a new benchmark for instance-level performance prediction in long-form generation tasks.", "A task-, model-, and metric-agnostic approach to predict evaluation scores.", "Demonstration of effective score prediction with minimal training examples."], "limitations": "", "keywords": ["Performance prediction", "Long-form generation", "Quality metrics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.07897", "pdf": "https://arxiv.org/pdf/2509.07897.pdf", "abs": "https://arxiv.org/abs/2509.07897", "title": "dciWebMapper2: Enhancing the dciWebMapper framework toward integrated, interactive visualization of linked multi-type maps, charts, and spatial statistics and analysis", "authors": ["Sarigai Sarigai", "Liping Yang", "Katie Slack", "Carolyn Fish", "Michaela Buenemann", "Qiusheng Wu", "Yan Lin", "Joseph A. Cook", "David Jacobs"], "categories": ["cs.HC", "cs.DB", "cs.GR"], "comment": "15 figures, 2 tables, and three advanced interactive web map apps\n  that are openly available to the public", "summary": "As interactive web-based geovisualization becomes increasingly vital across\ndisciplines, there is a growing need for open-source frameworks that support\ndynamic, multi-attribute spatial analysis and accessible design. This paper\nintroduces dciWebMapper2, a significant expansion of the original dciWebMapper\nframework, designed to enable exploratory analysis across domains such as\nclimate justice, food access, and social vulnerability. The enhanced framework\nintegrates multiple map types, including choropleth, proportional symbol, small\nmultiples, and heatmaps, with linked statistical charts (e.g., scatter plots,\nboxplots) and time sliders, all within a coordinated-view environment.\nDropdown-based controls allow flexible, high-dimensional comparisons while\nmaintaining visual clarity. Grounded in cartographic and information\nvisualization principles, dciWebMapper2 is fully open-source, self-contained,\nand server-free, supporting modularity, reproducibility, and long-term\nsustainability. Three applied use cases demonstrate its adaptability and\npotential to democratize interactive web cartography. This work offers a\nversatile foundation for inclusive spatial storytelling and transparent\ngeospatial analysis in research, education, and civic engagement.", "AI": {"tldr": "This paper presents dciWebMapper2, an open-source framework for interactive geovisualization that supports dynamic spatial analysis across various domains, including climate justice and food access.", "motivation": "The need for open-source frameworks in interactive geovisualization has increased due to its importance in various disciplines and the necessity for dynamic, multi-attribute spatial analysis and accessible design.", "method": "The dciWebMapper2 framework incorporates different map types (choropleth, proportional symbol, small multiples, heatmaps) and linked statistical charts along with dropdown-based controls for high-dimensional comparisons, all within a coordinated-view environment.", "result": "The framework's applicability is demonstrated through three use cases that highlight its adaptability for exploratory analysis and its potential to enhance inclusive spatial storytelling in research and civic engagement.", "conclusion": "dciWebMapper2 offers a versatile foundation for transparent geospatial analysis, promoting reproducibility and sustainable practices in cartography.", "key_contributions": ["Introduction of dciWebMapper2 framework for dynamic geovisualization.", "Integration of multiple map types and statistical charts in a single environment.", "Open-source and server-free design promoting modularity and reproducibility."], "limitations": "", "keywords": ["geovisualization", "open-source", "spatial analysis", "interactive maps", "cartography"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.07311", "pdf": "https://arxiv.org/pdf/2509.07311.pdf", "abs": "https://arxiv.org/abs/2509.07311", "title": "Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations", "authors": ["Sihyun Park"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have been driven by\npretraining, supervised fine tuning (SFT), and alignment tuning. Among these,\nSFT plays a crucial role in transforming a model 's general knowledge into\nstructured responses tailored to specific tasks. However, there is no clearly\nestablished methodology for effective training data selection. Simply\nincreasing the volume of data does not guarantee performance improvements,\nwhile preprocessing, sampling, and validation require substantial time and\ncost.\n  To address this issue, a variety of data selection methods have been\nproposed. Among them, knowledge based selection approaches identify suitable\ntraining data by analyzing the model 's responses. Nevertheless, these methods\ntypically rely on prompt engineering, making them sensitive to variations and\nincurring additional costs for prompt design.\n  In this study, we propose Knowledge Analysis via Model Internal\nRepresentations (KAMIR), a novel approach that overcomes these limitations by\nanalyzing data based on the model 's internal representations. KAMIR computes\nsimilarities between the hidden states of each layer (block) and the final\nhidden states for a given input to assess the data. Unlike prior methods that\nwere largely limited to multiple choice tasks, KAMIR can be applied to a wide\nrange of tasks such as machine reading comprehension and summarization.\nMoreover, it selects data useful for training based on the model 's familiarity\nwith the input, even with a small dataset and a simple classifier architecture.\nExperiments across diverse task datasets demonstrate that training with less\nfamiliar data leads to better generalization performance.", "AI": {"tldr": "This study introduces KAMIR, a method for selecting training data based on analyzing model internal representations to enhance performance in various tasks.", "motivation": "Effective training data selection is essential for improving the performance of language models, especially given the limitations of current methods reliant on prompt engineering.", "method": "KAMIR analyzes similarities between the hidden states across model layers and the final hidden states to select training data based on the model's internal familiarity with inputs.", "result": "Experiments show that using less familiar data can improve generalization performance across various tasks such as machine reading comprehension and summarization.", "conclusion": "KAMIR provides a more efficient and effective approach to data selection, avoiding the pitfalls of traditional prompt engineering methods.", "key_contributions": ["Introduction of KAMIR for effective training data selection", "Ability to analyze model internal representations for various tasks", "Demonstrated improved generalization performance with less familiar data"], "limitations": "", "keywords": ["large language models", "training data selection", "machine learning", "internal representations", "generalization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.07942", "pdf": "https://arxiv.org/pdf/2509.07942.pdf", "abs": "https://arxiv.org/abs/2509.07942", "title": "Knowledge Isn't Power: The Ethics of Social Robots and the Difficulty of Informed Consent", "authors": ["James M. Berzuk", "Lauren Corcoran", "Brannen McKenzie-Lefurgey", "Katie Szilagyi", "James E. Young"], "categories": ["cs.HC", "cs.RO"], "comment": "Submitted to the International Journal of Social Robotics. 18 pages,\n  1 figure", "summary": "Contemporary robots are increasingly mimicking human social behaviours to\nfacilitate interaction, such as smiling to signal approachability, or\nhesitating before taking an action to allow people time to react. Such\ntechniques can activate a person's entrenched social instincts, triggering\nemotional responses as though they are interacting with a fellow human, and can\nprompt them to treat a robot as if it truly possesses the underlying life-like\nprocesses it outwardly presents, raising significant ethical questions. We\nengage these issues through the lens of informed consent: drawing upon\nprevailing legal principles and ethics, we examine how social robots can\ninfluence user behaviour in novel ways, and whether under those circumstances\nusers can be appropriately informed to consent to these heightened\ninteractions. We explore the complex circumstances of human-robot interaction\nand highlight how it differs from more familiar interaction contexts, and we\napply legal principles relating to informed consent to social robots in order\nto reconceptualize the current ethical debates surrounding the field. From this\ninvestigation, we synthesize design goals for robot developers to achieve more\nethical and informed human-robot interaction.", "AI": {"tldr": "The paper examines the ethical implications of social robots mimicking human behaviors and their impact on user interactions and informed consent.", "motivation": "To address the ethical challenges posed by social robots that mimic human social behaviors, and investigate the implications for informed consent in human-robot interactions.", "method": "The authors analyze legal principles and ethical consideration surrounding informed consent, applying them to the context of human-robot interaction.", "result": "The paper identifies how social robots can influence user behavior and highlights the differences in consent dynamics compared to human-human interactions.", "conclusion": "The study proposes design goals for developers of social robots to ensure more ethical interactions and informed consent from users.", "key_contributions": ["Examines the impact of social robot behaviors on user interactions and emotional responses.", "Applies legal principles of informed consent to the context of social robotics.", "Synthesis of design goals aimed at improving ethical standards in human-robot interaction."], "limitations": "The study may not encompass all diverse interaction contexts and could be limited by rapidly advancing technology in robotics.", "keywords": ["social robots", "human-robot interaction", "informed consent", "ethical implications", "design goals"], "importance_score": 8, "read_time_minutes": 18}}
{"id": "2509.07324", "pdf": "https://arxiv.org/pdf/2509.07324.pdf", "abs": "https://arxiv.org/abs/2509.07324", "title": "Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation", "authors": ["Nakyung Lee", "Yeongoon Kim", "Minhae Oh", "Suhwan Kim", "Jin Woo Koo", "Hyewon Jo", "Jungwoo Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP 2025", "summary": "Transformer-based self-attention mechanism serves as the core of modern\nlanguage models, yet it often suffers from localization, where attentions\ncollapse onto a limited subset of tokens and fail to capture long-range\ndependencies. To address this issue, we propose Self-Attention One-step Belief\nPropagation (SAOBP), a refinement framework that injects multi-hop\nrelationships through a belief propagation process. To interpret and quantify\nthese interactions, we introduce Global Token Dependency (GTD) that captures\nthe relative contribution of multihop connections within the attention graph.\nEmpirical results indicate that SAOBP helps prevent entropy collapse in deeper\nlayers and adaptively maintains GTD at task-appropriate levels, thereby\nsupporting improvements in model performance. Importantly, we observe\ncompetitive gains in small-scale models, highlighting its potential for\nimproving inference quality in resource-constrained scenarios.", "AI": {"tldr": "The paper proposes Self-Attention One-step Belief Propagation (SAOBP) to enhance transformer models by addressing localization issues in self-attention mechanisms.", "motivation": "To improve the ability of transformer models to capture long-range dependencies and prevent attentions from collapsing onto a limited subset of tokens.", "method": "The authors introduce SAOBP, which uses a belief propagation process to enhance multi-hop relationships, and they develop Global Token Dependency (GTD) to quantify these interactions within the attention graph.", "result": "SAOBP prevents entropy collapse in deeper layers and maintains GTD at appropriate levels for tasks, leading to improved performance, especially in small-scale models.", "conclusion": "The results demonstrate that SAOBP can enhance inference quality, especially in resource-constrained environments, making it a valuable approach for improving transformer models.", "key_contributions": ["Introduction of Self-Attention One-step Belief Propagation (SAOBP).", "Development of Global Token Dependency (GTD) for quantifying multi-hop interactions.", "Empirical evidence showing competitive gains in small-scale models."], "limitations": "", "keywords": ["Transformer", "Self-attention", "Belief Propagation", "Global Token Dependency", "Machine Learning"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2509.07370", "pdf": "https://arxiv.org/pdf/2509.07370.pdf", "abs": "https://arxiv.org/abs/2509.07370", "title": "PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions", "authors": ["Yixuan Tang", "Yi Yang", "Ahmed Abbasi"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) demonstrate remarkable\ncapabilities across various fields. These developments have led to more direct\ncommunication between humans and LLMs in various situations, such as social\ncompanionship and psychological support. However, LLMs often exhibit\nlimitations in emotional perception and social competence during real-world\nconversations. These limitations partly originate from their inability to adapt\ntheir communication style and emotional expression to different social and task\ncontexts. In this work, we introduce PersonaFuse, a novel LLM post-training\nframework that enables LLMs to adapt and express different personalities for\nvarying situations. Inspired by Trait Activation Theory and the Big Five\npersonality model, PersonaFuse employs a Mixture-of-Expert architecture that\ncombines persona adapters with a dynamic routing network, enabling contextual\ntrait expression. Experimental results show that PersonaFuse substantially\noutperforms baseline models across multiple dimensions of social-emotional\nintelligence. Importantly, these gains are achieved without sacrificing general\nreasoning ability or model safety, which remain common limitations of direct\nprompting and supervised fine-tuning approaches. PersonaFuse also delivers\nconsistent improvements in downstream human-centered applications, such as\nmental health counseling and review-based customer service. Finally, human\npreference evaluations against leading LLMs, including GPT-4o and DeepSeek,\ndemonstrate that PersonaFuse achieves competitive response quality despite its\ncomparatively smaller model size. These findings demonstrate that\nPersonaFuse~offers a theoretically grounded and practical approach for\ndeveloping social-emotional enhanced LLMs, marking a significant advancement\ntoward more human-centric AI systems.", "AI": {"tldr": "PersonaFuse is a novel framework for adapting LLMs to express different personalities in varied contexts, improving social-emotional intelligence without losing general reasoning capabilities.", "motivation": "To enhance the emotional perception and social competence of LLMs for applications like mental health support and customer service.", "method": "PersonaFuse utilizes a Mixture-of-Expert architecture, featuring persona adapters and a dynamic routing network to enable contextual expression of personality traits.", "result": "PersonaFuse outperforms baseline models in social-emotional intelligence and maintains general reasoning and model safety, yielding better performance in human-centered applications.", "conclusion": "The framework supports the development of social-emotional enhanced LLMs, advancing human-centric AI systems.", "key_contributions": ["Introduces a post-training framework for LLM personality adaptation.", "Implements Mixture-of-Expert architecture with a dynamic routing mechanism.", "Demonstrates significant improvements in emotional intelligence without compromising reasoning abilities."], "limitations": "", "keywords": ["Large Language Models", "social-emotional intelligence", "PersonaFuse", "human-centered AI", "mental health applications"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.07190", "pdf": "https://arxiv.org/pdf/2509.07190.pdf", "abs": "https://arxiv.org/abs/2509.07190", "title": "Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation", "authors": ["Zahra Atf", "Peter R Lewis"], "categories": ["cs.CL", "cs.HC"], "comment": "This paper was accepted for presentation at the 35th IEEE\n  International Conference on Collaborative Advances in Software and Computing.\n  Conference website:https://conf.researchr.org/home/cascon-2025", "summary": "Large language models (LLMs) are increasingly used in high-stakes settings,\nwhere explaining uncertainty is both technical and ethical. Probabilistic\nmethods are often opaque and misaligned with expectations of transparency. We\npropose a framework based on rule-based moral principles for handling\nuncertainty in LLM-generated text. Using insights from moral psychology and\nvirtue ethics, we define rules such as precaution, deference, and\nresponsibility to guide responses under epistemic or aleatoric uncertainty.\nThese rules are encoded in a lightweight Prolog engine, where uncertainty\nlevels (low, medium, high) trigger aligned system actions with plain-language\nrationales. Scenario-based simulations benchmark rule coverage, fairness, and\ntrust calibration. Use cases in clinical and legal domains illustrate how moral\nreasoning can improve trust and interpretability. Our approach offers a\ntransparent, lightweight alternative to probabilistic models for socially\nresponsible natural language generation.", "AI": {"tldr": "The paper proposes a framework using moral principles to handle uncertainty in LLM-generated text, offering a transparent alternative to probabilistic models.", "motivation": "To address the challenge of explaining uncertainty in LLM outputs in high-stakes settings, ensuring transparency and ethical handling of information.", "method": "A framework based on rule-based moral principles is developed, employing a lightweight Prolog engine to encode rules for different levels of uncertainty.", "result": "The framework allows for scenario-based simulations that benchmark fairness, rule coverage, and trust calibration, demonstrating improved trust and interpretability in scenarios.", "conclusion": "The proposed method improves the transparency of LLM-generated outputs and provides a socially responsible approach to language generation in sensitive domains.", "key_contributions": ["Framework based on moral principles for LLM uncertainty", "Lightweight Prolog implementation for rule-based responses", "Application scenarios in clinical and legal domains"], "limitations": "", "keywords": ["large language models", "uncertainty", "moral psychology", "natural language generation", "trust calibration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.07389", "pdf": "https://arxiv.org/pdf/2509.07389.pdf", "abs": "https://arxiv.org/abs/2509.07389", "title": "Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents", "authors": ["Sankalp Tattwadarshi Swain", "Anshika Krishnatray", "Dhruv Kumar", "Jagat Sesh Challa"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Under review", "summary": "Existing evaluation studies on linguistic competence of large language models\n(LLM agents) have focused primarily on vocabulary learning, morphological rule\ninduction, syntactic generalization, pragmatic inference, and cross-linguistic\ntransfer. However, none assess whether LLM agents can acquire a language\nthrough pattern recognition and interactive feedback, a central feature of\nhuman language acquisition. We propose a novel experimental framework in which\nan LLM agent is evaluated on its ability to acquire and use a newly constructed\nlanguage (Tinkatongue) in conversation with a bot that understands only\nTinkatongue. Our findings show that LLM agents fail to establish a conversation\nwithin 100 responses, yet they adopt distinct strategies that mirror human\napproaches to language learning. The results suggest a new direction for\nevaluation benchmarks and open pathways to model designs that learn more\neffectively from interactive feedback.", "AI": {"tldr": "This paper examines whether large language models (LLMs) can learn a language through interaction and feedback, finding that while they struggled to converse in a new language, they employed strategies similar to human language learning.", "motivation": "To investigate the ability of LLM agents to acquire a new language through pattern recognition and interactive feedback, reflecting a key aspect of human language acquisition.", "method": "An experimental framework was developed where an LLM agent attempts to learn and converse in a newly constructed language (Tinkatongue) with a bot that only understands Tinkatongue.", "result": "LLM agents failed to establish a conversation within 100 responses but demonstrated strategies that resemble human language learning approaches.", "conclusion": "The study highlights the limitations of current LLM evaluation benchmarks and suggests the need for model designs that better incorporate interactive feedback mechanisms.", "key_contributions": ["Introduction of a new framework for evaluating LLM language acquisition", "Discovery of LLMs employing human-like learning strategies", "Recommendations for enhancing evaluation benchmarks in LLM research"], "limitations": "The LLM agents did not successfully establish effective communication within the experiment, indicating potential areas for improvement in model design.", "keywords": ["language acquisition", "large language models", "interactive feedback"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.07399", "pdf": "https://arxiv.org/pdf/2509.07399.pdf", "abs": "https://arxiv.org/abs/2509.07399", "title": "The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering", "authors": ["Yi-Jie Cheng", "Oscar Chew", "Yun-Nung Chen"], "categories": ["cs.CL"], "comment": "Extended from ACL 2025 SRW", "summary": "Integrating knowledge graphs (KGs) into the reasoning processes of large\nlanguage models (LLMs) has emerged as a promising approach to mitigate\nhallucination. However, existing work in this area often relies on proprietary\nor extremely large models, limiting accessibility and scalability. In this\nstudy, we investigate the capabilities of existing integration methods for\nsmall language models (SLMs) in KG-based question answering and observe that\ntheir performance is often constrained by their limited ability to traverse and\nreason over knowledge graphs. To address this limitation, we propose leveraging\nsimple and efficient exploration modules to handle knowledge graph traversal in\nplace of the language model itself. Experiment results demonstrate that these\nlightweight modules effectively improve the performance of small language\nmodels on knowledge graph question answering tasks. Source code:\nhttps://github.com/yijie-cheng/SLM-ToG/.", "AI": {"tldr": "This study explores the integration of knowledge graphs into small language models (SLMs) for improved question answering performance, proposing lightweight exploration modules to enhance their reasoning capabilities.", "motivation": "To mitigate hallucination in large language models by integrating knowledge graphs, while addressing the limitations of existing methods that rely on large or proprietary models.", "method": "The study investigates existing integration methods for SLMs in knowledge graph question answering and proposes new exploration modules to facilitate knowledge graph traversal.", "result": "Experiment results indicate that the proposed lightweight modules significantly enhance the performance of small language models in knowledge graph question answering tasks.", "conclusion": "The incorporation of efficient exploration modules allows small language models to better utilize knowledge graphs, improving their question answering capabilities.", "key_contributions": ["Introduction of lightweight exploration modules for knowledge graph traversal", "Demonstration of improved performance in small language models using knowledge graphs", "Accessibility and scalability improvements compared to traditional methods"], "limitations": "The study primarily focuses on small language models and may not directly apply to larger models.", "keywords": ["knowledge graphs", "small language models", "question answering", "exploration modules", "hallucination"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.07389", "pdf": "https://arxiv.org/pdf/2509.07389.pdf", "abs": "https://arxiv.org/abs/2509.07389", "title": "Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents", "authors": ["Sankalp Tattwadarshi Swain", "Anshika Krishnatray", "Dhruv Kumar", "Jagat Sesh Challa"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Under review", "summary": "Existing evaluation studies on linguistic competence of large language models\n(LLM agents) have focused primarily on vocabulary learning, morphological rule\ninduction, syntactic generalization, pragmatic inference, and cross-linguistic\ntransfer. However, none assess whether LLM agents can acquire a language\nthrough pattern recognition and interactive feedback, a central feature of\nhuman language acquisition. We propose a novel experimental framework in which\nan LLM agent is evaluated on its ability to acquire and use a newly constructed\nlanguage (Tinkatongue) in conversation with a bot that understands only\nTinkatongue. Our findings show that LLM agents fail to establish a conversation\nwithin 100 responses, yet they adopt distinct strategies that mirror human\napproaches to language learning. The results suggest a new direction for\nevaluation benchmarks and open pathways to model designs that learn more\neffectively from interactive feedback.", "AI": {"tldr": "This paper evaluates large language models (LLMs) for their ability to learn a new language through pattern recognition and feedback, a process akin to human language acquisition. The study finds LLMs struggle to maintain a conversation in the constructed language Tinkatongue but exhibit strategies similar to human learners.", "motivation": "To assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, an approach central to human language learning, which has not been adequately explored in existing studies.", "method": "A novel experimental framework was developed where an LLM agent was tasked with acquiring and using a constructed language (Tinkatongue) in interaction with a bot only capable of understanding Tinkatongue.", "result": "The findings indicate that LLM agents fail to engage in a conversation within 100 responses. However, they utilize distinct strategies reflecting human approaches to language acquisition.", "conclusion": "The study suggests the need for new evaluation benchmarks and encourages future model designs that learn more efficiently from interactive feedback.", "key_contributions": ["Introduction of a new experimental framework for evaluating LLM agents", "Demonstration of LLM strategies that mirror human language learning", "Insights for future benchmarks and model design in language acquisition."], "limitations": "The study is limited to a single constructed language and a specific framework, which may impact the generalizability of the findings.", "keywords": ["large language models", "language acquisition", "interactive feedback", "evaluation benchmarks", "human language learning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2509.07403", "pdf": "https://arxiv.org/pdf/2509.07403.pdf", "abs": "https://arxiv.org/abs/2509.07403", "title": "LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction", "authors": ["Weichu Liu", "Jing Xiong", "Yuxuan Hu", "Zixuan Li", "Minghuan Tan", "Ningning Mao", "Chenyang Zhao", "Zhongwei Wan", "Chaofan Tao", "Wendong Xu", "Hui Shen", "Chengming Li", "Lingpeng Kong", "Ngai Wong"], "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Large language models (LLMs) make significant progress in Emotional\nIntelligence (EI) and long-context understanding. However, existing benchmarks\ntend to overlook certain aspects of EI in long-context scenarios, especially\nunder realistic, practical settings where interactions are lengthy, diverse,\nand often noisy. To move towards such realistic settings, we present\nLongEmotion, a benchmark specifically designed for long-context EI tasks. It\ncovers a diverse set of tasks, including Emotion Classification, Emotion\nDetection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion\nExpression. On average, the input length for these tasks reaches 8,777 tokens,\nwith long-form generation required for Emotion Expression. To enhance\nperformance under realistic constraints, we incorporate Retrieval-Augmented\nGeneration (RAG) and Collaborative Emotional Modeling (CoEM), and compare them\nwith standard prompt-based methods. Unlike conventional approaches, our RAG\nmethod leverages both the conversation context and the large language model\nitself as retrieval sources, avoiding reliance on external knowledge bases. The\nCoEM method further improves performance by decomposing the task into five\nstages, integrating both retrieval augmentation and limited knowledge\ninjection. Experimental results show that both RAG and CoEM consistently\nenhance EI-related performance across most long-context tasks, advancing LLMs\ntoward more practical and real-world EI applications. Furthermore, we conducted\na comparative case study experiment on the GPT series to demonstrate the\ndifferences among various models in terms of EI. Code is available on GitHub at\nhttps://github.com/LongEmotion/LongEmotion, and the project page can be found\nat https://longemotion.github.io/.", "AI": {"tldr": "Introduction of LongEmotion, a benchmark for long-context Emotional Intelligence tasks in large language models.", "motivation": "Current benchmarks do not accurately assess Emotional Intelligence in long-context scenarios, which are common in practical applications.", "method": "LongEmotion benchmark is created for various Emotional Intelligence tasks, using Retrieval-Augmented Generation and Collaborative Emotional Modeling to improve performance.", "result": "RAG and CoEM methods demonstrate consistent enhancements in Emotional Intelligence performance across long-context tasks with LLMs.", "conclusion": "LongEmotion serves as a significant step toward practical applications of LLMs in Emotional Intelligence, with improved methods over traditional approaches.", "key_contributions": ["Introduction of LongEmotion benchmark for long-context EI tasks", "Development of RAG and CoEM methods to enhance LLM performance", "Experimental validation highlighting model differences in EI capabilities"], "limitations": "", "keywords": ["Emotional Intelligence", "Long-context", "Large Language Models", "Retrieval-Augmented Generation", "Collaborative Emotional Modeling"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.07459", "pdf": "https://arxiv.org/pdf/2509.07459.pdf", "abs": "https://arxiv.org/abs/2509.07459", "title": "AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training", "authors": ["Christian Rene Thelen", "Patrick Gustav Blaneck", "Tobias Bornheim", "Niklas Grieger", "Stephan Bialonski"], "categories": ["cs.CL"], "comment": "6 pages, 1 figure, 2 tables", "summary": "Positive, supportive online communication in social media (candy speech) has\nthe potential to foster civility, yet automated detection of such language\nremains underexplored, limiting systematic analysis of its impact. We\ninvestigate how candy speech can be reliably detected in a 46k-comment German\nYouTube corpus by monolingual and multilingual language models, including\nGBERT, Qwen3 Embedding, and XLM-RoBERTa. We find that a multilingual\nXLM-RoBERTa-Large model trained to detect candy speech at the span level\noutperforms other approaches, ranking first in both binary positive F1: 0.8906)\nand categorized span-based detection (strict F1: 0.6307) subtasks at the\nGermEval 2025 Shared Task on Candy Speech Detection. We speculate that\nspan-based training, multilingual capabilities, and emoji-aware tokenizers\nimproved detection performance. Our results demonstrate the effectiveness of\nmultilingual models in identifying positive, supportive language.", "AI": {"tldr": "Automated detection of positive online communication (candy speech) is explored using monolingual and multilingual language models, revealing the effectiveness of XLM-RoBERTa-Large in detecting this type of language.", "motivation": "To systematically analyze the impact of positive and supportive online communication, which is currently limited by the lack of automated detection methodologies.", "method": "The study employs monolingual and multilingual language models, particularly focusing on the performance of XLM-RoBERTa-Large, GBERT, and Qwen3 Embedding on a large German YouTube comment corpus.", "result": "The multilingual XLM-RoBERTa-Large model achieved the highest performance in detecting candy speech, with a binary positive F1 score of 0.8906 and a strict F1 score of 0.6307 for categorized span-based detection tasks.", "conclusion": "The findings highlight the potential of multilingual models, especially with span-based training and emoji-aware tokenizers, to effectively identify positive supportive language in online communications.", "key_contributions": ["Demonstration of the effectiveness of multilingual models in language detection tasks.", "Implementation of span-based training techniques for improved detection accuracy.", "Evaluation metrics for the automated detection of candy speech in a substantial YouTube comment dataset."], "limitations": "", "keywords": ["candy speech", "multilingual models", "positive communication", "social media", "language detection"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.07462", "pdf": "https://arxiv.org/pdf/2509.07462.pdf", "abs": "https://arxiv.org/abs/2509.07462", "title": "Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts", "authors": ["Yiliang Zhou", "Di Hu", "Tianchu Lyu", "Jasmine Dhillon", "Alexandra L. Beck", "Gelareh Sadigh", "Kai Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "Stigmatizing language results in healthcare inequities, yet there is no\nuniversally accepted or standardized lexicon defining which words, terms, or\nphrases constitute stigmatizing language in healthcare. We conducted a\nsystematic search of the literature to identify existing stigmatizing language\nlexicons and then analyzed them comparatively to examine: 1) similarities and\ndiscrepancies between these lexicons, and 2) the distribution of positive,\nnegative, or neutral terms based on an established sentiment dataset. Our\nsearch identified four lexicons. The analysis results revealed moderate\nsemantic similarity among them, and that most stigmatizing terms are related to\njudgmental expressions by clinicians to describe perceived negative behaviors.\nSentiment analysis showed a predominant proportion of negatively classified\nterms, though variations exist across lexicons. Our findings underscore the\nneed for a standardized lexicon and highlight challenges in defining\nstigmatizing language in clinical texts.", "AI": {"tldr": "This paper identifies and compares existing lexicons of stigmatizing language in healthcare, revealing moderate similarity among them and a predominance of negatively classified terms.", "motivation": "The paper addresses the lack of a standardized definition of stigmatizing language in healthcare, which contributes to healthcare inequities.", "method": "A systematic literature search was conducted to identify existing lexicons of stigmatizing language, followed by a comparative analysis of these lexicons and a sentiment analysis of their terms.", "result": "The analysis revealed moderate semantic similarity among the identified lexicons and indicated that most stigmatizing terms are linked to judgmental clinician expressions. Sentiment analysis showed a predominance of negative terms, with variations across lexicons.", "conclusion": "There is a critical need for a standardized lexicon to define stigmatizing language in healthcare, alongside challenges in its characterization in clinical texts.", "key_contributions": ["Identification of four existing stigmatizing language lexicons in healthcare", "Comparative analysis revealing moderate semantic similarity", "Highlighting the predominance of negatively classified terms"], "limitations": "", "keywords": ["stigmatizing language", "healthcare inequities", "sentiment analysis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.07471", "pdf": "https://arxiv.org/pdf/2509.07471.pdf", "abs": "https://arxiv.org/abs/2509.07471", "title": "From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation", "authors": ["Mardiyyah Oduwole", "Oluwatosin Olajide", "Jamiu Suleiman", "Faith Hunja", "Busayo Awobade", "Fatimo Adebanjo", "Comfort Akanni", "Chinonyelum Igwe", "Peace Ododo", "Promise Omoigui", "Steven Kolawole", "Abraham Owodunni"], "categories": ["cs.CL", "68T50", "I.7"], "comment": "8 pages, 3 tables. Exploratory work on Data Augmentation for African\n  Machine Translation", "summary": "The linguistic diversity across the African continent presents different\nchallenges and opportunities for machine translation. This study explores the\neffects of data augmentation techniques in improving translation systems in\nlow-resource African languages. We focus on two data augmentation techniques:\nsentence concatenation with back translation and switch-out, applying them\nacross six African languages. Our experiments show significant improvements in\nmachine translation performance, with a minimum increase of 25\\% in BLEU score\nacross all six languages.We provide a comprehensive analysis and highlight the\npotential of these techniques to improve machine translation systems for\nlow-resource languages, contributing to the development of more robust\ntranslation systems for under-resourced languages.", "AI": {"tldr": "This study explores data augmentation techniques to enhance machine translation for low-resource African languages, achieving significant improvements in performance.", "motivation": "Address the challenges of machine translation for low-resource African languages and enhance translation systems' performance.", "method": "Utilizing two data augmentation techniques: sentence concatenation with back translation and switch-out, and applying them to six African languages in experiments.", "result": "The application of data augmentation techniques resulted in a minimum increase of 25% in BLEU score across all six languages.", "conclusion": "Data augmentation techniques significantly improve machine translation performance, showcasing their potential to bolster translation systems for low-resource languages.", "key_contributions": ["Assessment of data augmentation techniques for African languages", "Demonstrated improvements in BLEU scores through experiments", "Comprehensive analysis of the impact on low-resource machine translation"], "limitations": "", "keywords": ["machine translation", "data augmentation", "African languages", "low-resource languages", "BLEU score"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2509.07869", "pdf": "https://arxiv.org/pdf/2509.07869.pdf", "abs": "https://arxiv.org/abs/2509.07869", "title": "Are Humans as Brittle as Large Language Models?", "authors": ["Jiahui Li", "Sean Papay", "Roman Klinger"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "The output of large language models (LLM) is unstable, due to both\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\ninstruction changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs.", "AI": {"tldr": "The study compares prompt brittleness in LLMs and human annotators during text classification tasks, revealing that both exhibit similar sensitivity to prompt modifications.", "motivation": "To investigate if prompt brittleness observed in large language models correlates with sensitivity in human annotations, particularly how instruction changes affect both.", "method": "The research conducts systematic comparisons by prompting both human annotators and LLMs on text classification tasks with varied prompts.", "result": "Both humans and LLMs show increased brittleness to specific prompt changes, especially those altering label sets, but human judgments are less impacted by typographical errors and label order reversals.", "conclusion": "Prompt brittleness may reflect genuine variances in human annotations rather than solely being a limitation of LLMs.", "key_contributions": ["Comparison of LLMs and human annotators in terms of prompt brittleness", "Findings suggest shared characteristics in response to prompt variations", "Insights into the relationship between human sensitivity to instructions and LLM behavior."], "limitations": "", "keywords": ["Large Language Models", "Prompt Brittleness", "Human Annotation", "Text Classification", "Instruction Sensitivity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.07475", "pdf": "https://arxiv.org/pdf/2509.07475.pdf", "abs": "https://arxiv.org/abs/2509.07475", "title": "HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention", "authors": ["Saumya Goswami", "Siddharth Kurra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detecting content that contradicts or is unsupported by a given source text\nis a critical challenge for the safe deployment of generative language models.\nWe introduce HALT-RAG, a post-hoc verification system designed to identify\nhallucinations in the outputs of Retrieval-Augmented Generation (RAG)\npipelines. Our flexible and task-adaptable framework uses a universal feature\nset derived from an ensemble of two frozen, off-the-shelf Natural Language\nInference (NLI) models and lightweight lexical signals. These features are used\nto train a simple, calibrated, and task-adapted meta-classifier. Using a\nrigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and\nproduce unbiased estimates, we evaluate our system on the HaluEval benchmark.\nBy pairing our universal feature set with a lightweight, task-adapted\nclassifier and a precision-constrained decision policy, HALT-RAG achieves\nstrong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,\nand dialogue tasks, respectively. The system's well-calibrated probabilities\nenable a practical abstention mechanism, providing a reliable tool for\nbalancing model performance with safety requirements.", "AI": {"tldr": "HALT-RAG is a verification system for identifying hallucinations in outputs of Retrieval-Augmented Generation pipelines, achieving strong performance across various tasks.", "motivation": "To address the challenge of detecting content that contradicts or is unsupported by source texts in generative language models.", "method": "HALT-RAG employs a flexible framework using a universal feature set from two NLI models and lexical signals to train a calibrated meta-classifier, evaluated using a 5-fold OOF training protocol.", "result": "HALT-RAG achieves OOF F1-scores of 0.7756, 0.9786, and 0.7391 for summarization, QA, and dialogue tasks, respectively.", "conclusion": "The system provides a reliable tool balancing model performance with safety, featuring well-calibrated probabilities for practical application.", "key_contributions": ["Introduction of HALT-RAG for hallucination detection in RAG outputs", "Use of a universal feature set with NLI models", "Implementation of a precision-constrained decision policy for safety"], "limitations": "", "keywords": ["hallucination detection", "Retrieval-Augmented Generation", "NLI models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.02088", "pdf": "https://arxiv.org/pdf/2411.02088.pdf", "abs": "https://arxiv.org/abs/2411.02088", "title": "Affordances and Design Principles of The Political Left and Right", "authors": ["Felix Anand Epp", "Jesse Haapoja", "Matti Nelimarkka"], "categories": ["cs.HC", "H.5"], "comment": "24 pages, 5 figures, to be published in Proc. ACM Hum.-Comput.\n  Interact., Vol. 9, No. 7, CSCW series", "summary": "Like any form of technology, social media services embed values. To examine\nhow societal values may be present in these systems, we focus on exploring\npolitical ideology as a value system. We organised four co-design workshops\nwith political representatives from five major parties in Finland to\ninvestigate what values they would incorporate into social media services. The\nparticipants were divided into one right-leaning group, two left-leaning\ngroups, and one mixed group. This approach allows us to examine the differences\nin social media services designed by groups with different political ideologies\ni.e., value systems. We analysed produced artefacts (early-stage paper mockups)\nto identify different features and affordances for each group and then\ncontrasted the ideological compositions. Our results revealed a clear\ndistinction between groups: the right-leaning group favoured market-based\nvisibility, while left-leaning groups rejected such design principles in favour\nof open profile work. Additionally, we found tentative differences in design\noutcomes along the liberal--conservative dimension. These findings underscore\nthe importance of acknowledging existing political value systems in the design\nof social computing systems. They also highlight the need for further research\nto map out political ideologies in technology design.", "AI": {"tldr": "The paper investigates how political ideology influences the design of social media services through co-design workshops with political representatives in Finland.", "motivation": "To explore how societal values, particularly political ideology, influence social media service design.", "method": "Conducted four co-design workshops with political representatives divided by ideology (right-leaning, left-leaning, and mixed) to generate early-stage design artefacts.", "result": "Identified significant differences in social media features preferred by different political groups, with right-leaning groups favoring market-based visibility and left-leaning groups advocating for more open profiles.", "conclusion": "Political ideologies have a clear impact on the design of social computing systems, necessitating further research to explore this relationship.", "key_contributions": ["Examined the influence of political ideologies on social media design", "Contrasted design preferences of different political groups", "Highlighted the need for inclusion of political values in technology design"], "limitations": "Study is limited to a specific context (Finland) and may not generalize to all cultures or political systems.", "keywords": ["social media", "political ideology", "co-design", "human-computer interaction", "technology values"], "importance_score": 6, "read_time_minutes": 24}}
{"id": "2509.07512", "pdf": "https://arxiv.org/pdf/2509.07512.pdf", "abs": "https://arxiv.org/abs/2509.07512", "title": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval", "authors": ["Zihan Chen", "Lei Shi", "Weize Wu", "Qiji Zhou", "Yue Zhang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Many contemporary data-driven research efforts in the natural sciences, such\nas chemistry and materials science, require large-scale, high-performance\nentity recognition from scientific datasets. Large language models (LLMs) have\nincreasingly been adopted to solve the entity recognition task, with the same\ntrend being observed on all-spectrum NLP tasks. The prevailing entity\nrecognition LLMs rely on fine-tuned technology, yet the fine-tuning process\noften incurs significant cost. To achieve a best performance-cost trade-off, we\npropose ALLabel, a three-stage framework designed to select the most\ninformative and representative samples in preparing the demonstrations for LLM\nmodeling. The annotated examples are used to construct a ground-truth retrieval\ncorpus for LLM in-context learning. By sequentially employing three distinct\nactive learning strategies, ALLabel consistently outperforms all baselines\nunder the same annotation budget across three specialized domain datasets.\nExperimental results also demonstrate that selectively annotating only 5\\%-10\\%\nof the dataset with ALLabel can achieve performance comparable to the method\nannotating the entire dataset. Further analyses and ablation studies verify the\neffectiveness and generalizability of our proposal.", "AI": {"tldr": "ALLabel is a three-stage framework for efficient entity recognition in large datasets using active learning in LLMs, achieving high performance with minimal annotations.", "motivation": "The need for large-scale entity recognition in scientific datasets drove the development of a cost-effective framework as existing methods are expensive due to fine-tuning requirements.", "method": "ALLabel utilizes a three-stage active learning process to select informative samples for LLM in-context learning, significantly reducing the required annotation budget.", "result": "ALLabel outperforms baseline models while using only 5%-10% of the dataset for annotations, demonstrating high efficiency in entity recognition tasks.", "conclusion": "The approach allows for achieving high performance in entity recognition with a fraction of the typical annotation effort, confirming its effectiveness and generalizability.", "key_contributions": ["Introduction of ALLabel as a novel framework for entity recognition using LLMs", "Demonstration of reduced annotation needs with maintained performance", "Validating the effectiveness across multiple specialized domain datasets"], "limitations": "", "keywords": ["entity recognition", "large language models", "active learning", "annotation efficiency", "natural sciences"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2509.07553", "pdf": "https://arxiv.org/pdf/2509.07553.pdf", "abs": "https://arxiv.org/abs/2509.07553", "title": "VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents", "authors": ["Zheng Wu", "Heyuan Huang", "Xingyu Lou", "Xiangmou Qu", "Pengzhou Cheng", "Zongru Wu", "Weiwen Liu", "Weinan Zhang", "Jun Wang", "Zhaoxiang Wang", "Zhuosheng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid progress of multimodal large language models, operating system\n(OS) agents become increasingly capable of automating tasks through on-device\ngraphical user interfaces (GUIs). However, most existing OS agents are designed\nfor idealized settings, whereas real-world environments often present\nuntrustworthy conditions. To mitigate risks of over-execution in such\nscenarios, we propose a query-driven human-agent-GUI interaction framework that\nenables OS agents to decide when to query humans for more reliable task\ncompletion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy\nOS agent trained with a two-stage learning paradigm that falicitate the\ndecoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent\nautonomously executes actions in normal conditions while proactively querying\nhumans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves\nthe average step-wise success rate by 20.64\\% in untrustworthy scenarios over\nthe state-of-the-art, without compromising normal performance. Analysis\nhighlights VeriOS-Agent's rationality, generalizability, and scalability. The\ncodes, datasets and models are available at\nhttps://github.com/Wuzheng02/VeriOS.", "AI": {"tldr": "Development of VeriOS-Agent, a human-agent-GUI interaction framework that enhances OS agent decision-making in untrustworthy environments.", "motivation": "To address the limitations of existing OS agents that usually operate in idealized settings, which do not account for the complexities and uncertainties of real-world environments.", "method": "A query-driven interaction framework allowing OS agents to determine when to ask humans for input, utilizing a two-stage learning paradigm for training the VeriOS-Agent.", "result": "VeriOS-Agent shows a 20.64% increase in average step-wise success rate in untrustworthy scenarios compared to state-of-the-art methods without degrading performance in normal conditions.", "conclusion": "VeriOS-Agent demonstrates improved rationality, generalizability, and scalability, making it a reliable tool for task automation in complex environments.", "key_contributions": ["Introduction of a query-driven human-agent-GUI interaction framework.", "Development of the VeriOS-Agent with a two-stage training process.", "Demonstrated significant performance improvement in untrustworthy scenarios."], "limitations": "", "keywords": ["OS agents", "human-agent interaction", "multimodal large language models", "trustworthiness", "automation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2412.02603", "pdf": "https://arxiv.org/pdf/2412.02603.pdf", "abs": "https://arxiv.org/abs/2412.02603", "title": "Generative AI as a Tool for Enhancing Reflective Learning in Students", "authors": ["Bo Yuan", "Jiazi Hu"], "categories": ["cs.HC"], "comment": "Accepted by IEEE TALE 2025", "summary": "Reflection is widely recognized as a cornerstone of student development,\nfostering critical thinking, self-regulation, and deep conceptual\nunderstanding. Traditionally, reflective skills have been cultivated through\nstructured feedback, mentorship, and guided self-assessment. However, these\napproaches often face challenges such as limited scalability, difficulties in\ndelivering individualized feedback, and a shortage of instructors proficient in\nfacilitating meaningful reflection. This study pioneers the use of generative\nAI, specifically large language models (LLMs), as an innovative solution to\nthese limitations. By leveraging the capacity of LLMs to deliver personalized,\ncontext-sensitive feedback at scale, this research investigates their potential\nto serve as effective facilitators of reflective exercises, sustaining deep\nengagement and promoting critical thinking. Through in-depth analyses of prompt\nengineering strategies and simulated multi-turn dialogues grounded in a\nproject-based learning (PBL) context, the study demonstrates that, with\npedagogically aligned prompts, LLMs can serve as accessible and adaptive tools\nfor scalable reflective guidance. Furthermore, LLM-assisted evaluation is\nemployed to objectively assess the performance of both tutors and students\nacross multiple dimensions of reflective learning. The findings contribute to\nthe evolving understanding of AI's role in reflective pedagogy and point to new\nopportunities for advancing AI-driven intelligent tutoring systems.", "AI": {"tldr": "This study explores the use of generative AI, particularly LLMs, to enhance reflective practice in education by providing scalable, personalized feedback.", "motivation": "To address the limitations of traditional methods of fostering reflective skills, which include scalability issues and challenges in providing individualized feedback.", "method": "Utilized large language models (LLMs) for prompt engineering and simulated multi-turn dialogues in a project-based learning context to analyze their effectiveness as facilitators of reflective exercises.", "result": "Found that LLMs, when paired with pedagogically aligned prompts, can enhance engagement and critical thinking, serving as effective tools for scalable reflective guidance.", "conclusion": "This research contributes to the understanding of AI in reflective pedagogy and opens up new avenues for AI-driven intelligent tutoring systems.", "key_contributions": ["Introduces LLMs as facilitators of reflective learning exercises", "Demonstrates effectiveness of prompt engineering for personalized feedback", "Evaluates performance of tutors and students using LLM-assisted assessment"], "limitations": "", "keywords": ["Generative AI", "Large Language Models", "Reflective Learning", "Intelligent Tutoring Systems", "Project-Based Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.07555", "pdf": "https://arxiv.org/pdf/2509.07555.pdf", "abs": "https://arxiv.org/abs/2509.07555", "title": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition", "authors": ["Yi Liu", "Xiangrong Zhu", "Xiangyu Liu", "Wei Wei", "Wei Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in EMNLP Findings 2025", "summary": "In a rapidly evolving world where information updates swiftly, knowledge in\nlarge language models (LLMs) becomes outdated quickly. Retraining LLMs is not a\ncost-effective option, making knowledge editing (KE) without modifying\nparameters particularly necessary. We find that although existing\nretrieval-augmented generation (RAG)-based KE methods excel at editing simple\nknowledge, they struggle with KE in multi-hop question answering due to the\nissue of \"edit skipping\", which refers to skipping the relevant edited fact in\ninference. In addition to the diversity of natural language expressions of\nknowledge, edit skipping also arises from the mismatch between the granularity\nof LLMs in problem-solving and the facts in the edited memory. To address this\nissue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing\nmethod with guided decomposition (IRAKE) through the guidance from single\nedited facts and entire edited cases. Experimental results demonstrate that\nIRAKE mitigates the failure of editing caused by edit skipping and outperforms\nstate-of-the-art methods for KE in multi-hop question answering.", "AI": {"tldr": "This paper presents IRAKE, a novel method for knowledge editing in large language models that addresses issues of edit skipping in multi-hop question answering.", "motivation": "To improve knowledge editing in large language models, particularly for multi-hop question answering, which current methods struggle with due to edit skipping.", "method": "The proposed method, IRAKE, utilizes guided decomposition through the guidance of single edited facts and entire edited cases, enhancing the capability of retrieval-augmented generation for knowledge editing.", "result": "Experimental results indicate that IRAKE significantly mitigates the failure of knowledge editing due to edit skipping and performs better than existing state-of-the-art methods in the realm of multi-hop question answering.", "conclusion": "IRAKE provides a robust solution for knowledge editing in LLMs, paving the way for more effective and efficient handling of updated information in multi-hop contexts.", "key_contributions": ["Introduction of IRAKE for guided decomposition in knowledge editing", "Reduction of edit skipping in multi-hop question answering", "Demonstrated superiority over existing methods for multi-hop question answering KE."], "limitations": "", "keywords": ["knowledge editing", "large language models", "multi-hop question answering", "retrieval-augmented generation", "edit skipping"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.13934", "pdf": "https://arxiv.org/pdf/2504.13934.pdf", "abs": "https://arxiv.org/abs/2504.13934", "title": "VoxCity: A Seamless Framework for Open Geospatial Data Integration, Grid-Based Semantic 3D City Model Generation, and Urban Environment Simulation", "authors": ["Kunihiko Fujiwara", "Ryuta Tsurumi", "Tomoki Kiyono", "Zicheng Fan", "Xiucheng Liang", "Binyu Lei", "Winston Yap", "Koichi Ito", "Filip Biljecki"], "categories": ["cs.HC"], "comment": null, "summary": "Three-dimensional urban environment simulation is a powerful tool for\ninformed urban planning. However, the intensive manual effort required to\nprepare input 3D city models has hindered its widespread adoption. To address\nthis challenge, we present VoxCity, an open-source Python package that provides\na one-stop solution for grid-based 3D city model generation and urban\nenvironment simulation for cities worldwide. VoxCity's `generator' subpackage\nautomatically downloads building heights, tree canopy heights, land cover, and\nterrain elevation within a specified target area, and voxelizes buildings,\ntrees, land cover, and terrain to generate an integrated voxel city model. The\n`simulator' subpackage enables users to conduct environmental simulations,\nincluding solar radiation and view index analyses. Users can export the\ngenerated models using several file formats compatible with external software,\nsuch as ENVI-met (INX), Blender, and Rhino (OBJ). We generated 3D city models\nfor eight global cities, and demonstrated the calculation of solar irradiance,\nsky view index, and green view index. We also showcased microclimate simulation\nand 3D rendering visualization through ENVI-met and Rhino, respectively,\nthrough the file export function. Additionally, we reviewed openly available\ngeospatial data to create guidelines to help users choose appropriate data\nsources depending on their target areas and purposes. VoxCity can significantly\nreduce the effort and time required for 3D city model preparation and promote\nthe utilization of urban environment simulations. This contributes to more\ninformed urban and architectural design that considers environmental impacts,\nand in turn, fosters sustainable and livable cities. VoxCity is released openly\nat https://github.com/kunifujiwara/VoxCity.", "AI": {"tldr": "VoxCity is an open-source Python package for grid-based 3D city model generation and environmental simulation, facilitating urban planning by automating data collection and simulation processes.", "motivation": "To reduce the manual effort in preparing 3D city models for urban environment simulations, thereby promoting their widespread use in informed urban planning.", "method": "VoxCity integrates a generator that collects 3D data of urban features and voxelizes them into a city model, and a simulator that conducts various environmental analyses like solar radiation and view index calculations.", "result": "VoxCity successfully generated 3D models for eight cities, demonstrating simulations for solar irradiance and microclimate visualization, while allowing exports to multiple formats.", "conclusion": "VoxCity can significantly streamline the city modeling process and enhance urban design efforts by providing vital simulation tools and guidelines for data selection.", "key_contributions": ["Development of an automated generator for voxel city model creation", "Integration of environmental simulations within the package", "Provision of a comprehensive guideline for selecting geospatial data"], "limitations": "", "keywords": ["3D city model", "urban simulation", "environmental analysis", "geospatial data", "Python package"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.07588", "pdf": "https://arxiv.org/pdf/2509.07588.pdf", "abs": "https://arxiv.org/abs/2509.07588", "title": "BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment", "authors": ["Andrey Sakhovskiy", "Elena Tutubalina"], "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3; J.3"], "comment": "9 pages, 1 figure, published in \"The 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR 2025)\"", "summary": "In recent years, there has been substantial progress in using pretrained\nLanguage Models (LMs) on a range of tasks aimed at improving the understanding\nof biomedical texts. Nonetheless, existing biomedical LLMs show limited\ncomprehension of complex, domain-specific concept structures and the factual\ninformation encoded in biomedical Knowledge Graphs (KGs). In this work, we\npropose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel\njoint LM and KG pre-training method that augments an LM with external knowledge\nby the simultaneous learning of a dedicated KG encoder and aligning the\nrepresentations of both the LM and the graph. For a given textual sequence, we\nlink biomedical concept mentions to the Unified Medical Language System (UMLS)\nKG and utilize local KG subgraphs as cross-modal positive samples for these\nmentions. Our empirical findings indicate that implementing our method on\nseveral leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves\ntheir performance on a range of language understanding tasks and the quality of\nentity representations, even with minimal pre-training on a small alignment\ndataset sourced from PubMed scientific abstracts.", "AI": {"tldr": "This paper presents BALI, a method for aligning Language Models with Biomedical Knowledge Graphs to improve understanding of biomedical texts.", "motivation": "To enhance the understanding of complex biomedical concepts and factual information through better alignment of Language Models with Knowledge Graphs.", "method": "BALI proposes a joint pre-training approach that employs a KG encoder to augment a Language Model by linking biomedical concepts to the UMLS Knowledge Graph and using local subgraphs as positive samples.", "result": "The method improves performance on tasks involving language understanding and enhances the quality of entity representations in leading biomedical Language Models like PubMedBERT and BioLinkBERT.", "conclusion": "BALI shows promise in enhancing biomedical LMs through effective alignment with Knowledge Graphs, demonstrating improvements even with limited pre-training data.", "key_contributions": ["Introduction of BALI, a novel alignment method for LMs and KGs", "Demonstration of improved performance on several language understanding tasks", "Utilization of minimal pre-training data for effective results"], "limitations": "", "keywords": ["Biomedical Language Models", "Knowledge Graphs", "BALI", "UMLS", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.07622", "pdf": "https://arxiv.org/pdf/2509.07622.pdf", "abs": "https://arxiv.org/abs/2509.07622", "title": "MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs", "authors": ["Libo Ren", "Yee Man Ng", "Lifeng Han"], "categories": ["cs.CL"], "comment": "system paper at CLEF 2025", "summary": "Efficient communication between patients and clinicians plays an important\nrole in shared decision-making. However, clinical reports are often lengthy and\nfilled with clinical jargon, making it difficult for domain experts to identify\nimportant aspects in the document efficiently. This paper presents the\nmethodology we applied in the MultiClinSUM shared task for summarising clinical\ncase documents. We used an Iterative Self-Prompting technique on large language\nmodels (LLMs) by asking LLMs to generate task-specific prompts and refine them\nvia example-based few-shot learning. Furthermore, we used lexical and embedding\nspace metrics, ROUGE and BERT-score, to guide the model fine-tuning with\nepochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved\nROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,\nR, F1) from the official evaluation on 3,396 clinical case reports from various\nspecialties extracted from open journals. The high BERTscore indicates that the\nmodel produced semantically equivalent output summaries compared to the\nreferences, even though the overlap at the exact lexicon level is lower, as\nreflected in the lower ROUGE scores. This work sheds some light on how\nperspective-aware ISP (PA-ISP) can be deployed for clinical report\nsummarisation and support better communication between patients and clinicians.", "AI": {"tldr": "This paper discusses a methodology for summarizing clinical case documents using large language models (LLMs), improving communication between patients and clinicians.", "motivation": "Efficient communication between patients and clinicians is critical for shared decision-making, yet clinical reports are often lengthy and complex, hindering efficient understanding.", "method": "The authors applied an Iterative Self-Prompting technique to large language models, employing example-based few-shot learning along with lexical and embedding metrics to fine-tune the models across multiple epochs.", "result": "The approach yielded ROUGE scores of 46.53, 24.68, and 30.77, and BERT-scores of 87.84, 83.25, and 85.46, indicating effective production of semantically aligned summaries from clinical reports despite lower lexical overlap.", "conclusion": "The study demonstrates the effectiveness of perspective-aware ISP for clinical summarization, enhancing communication between patients and clinicians.", "key_contributions": ["Introduction of Iterative Self-Prompting for clinical summarization", "Use of multiple evaluation metrics for model fine-tuning", "High semantic alignment in summary outputs using large language models"], "limitations": "", "keywords": ["clinical summarization", "large language models", "shared decision-making"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.07666", "pdf": "https://arxiv.org/pdf/2509.07666.pdf", "abs": "https://arxiv.org/abs/2509.07666", "title": "MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval", "authors": ["Xixi Wu", "Yanchao Tan", "Nan Hou", "Ruiyang Zhang", "Hong Cheng"], "categories": ["cs.CL", "cs.IR"], "comment": "EMNLP Main 2025", "summary": "Document Understanding is a foundational AI capability with broad\napplications, and Document Question Answering (DocQA) is a key evaluation task.\nTraditional methods convert the document into text for processing by Large\nLanguage Models (LLMs), but this process strips away critical multi-modal\ninformation like figures. While Large Vision-Language Models (LVLMs) address\nthis limitation, their constrained input size makes multi-page document\ncomprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate\nthis by selecting relevant pages, but they rely solely on semantic relevance,\nignoring logical connections between pages and the query, which is essential\nfor reasoning.\n  To this end, we propose MoLoRAG, a logic-aware retrieval framework for\nmulti-modal, multi-page document understanding. By constructing a page graph\nthat captures contextual relationships between pages, a lightweight VLM\nperforms graph traversal to retrieve relevant pages, including those with\nlogical connections often overlooked. This approach combines semantic and\nlogical relevance to deliver more accurate retrieval. After retrieval, the\ntop-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance\nflexibility, MoLoRAG offers two variants: a training-free solution for easy\ndeployment and a fine-tuned version to improve logical relevance checking.\nExperiments on four DocQA datasets demonstrate average improvements of 9.68% in\naccuracy over LVLM direct inference and 7.44% in retrieval precision over\nbaselines. Codes and datasets are released at\nhttps://github.com/WxxShirley/MoLoRAG.", "AI": {"tldr": "MoLoRAG is a logic-aware retrieval framework for improved multi-modal, multi-page document understanding in Document Question Answering tasks.", "motivation": "Traditional methods for Document Question Answering strip away critical multi-modal information, while existing Large Vision-Language Models face input size constraints that limit multi-page comprehension.", "method": "MoLoRAG constructs a page graph to capture contextual relationships between pages. It employs a lightweight VLM for graph traversal to retrieve pages based on both semantic and logical relevance, allowing retrieval of top-$K$ pages for question answering with LVLMs.", "result": "Experiments on four DocQA datasets show a 9.68% accuracy improvement over LVLM direct inference and a 7.44% increase in retrieval precision compared to baselines.", "conclusion": "MoLoRAG provides a better approach for Document Question Answering by improving retrieval accuracy through logical relevance, with options for training-free deployment or fine-tuning.", "key_contributions": ["Development of MoLoRAG for logic-aware retrieval in document understanding", "Introduction of a page graph to capture relationships between pages", "Demonstrated significant improvements in accuracy and retrieval precision on DocQA datasets."], "limitations": "", "keywords": ["Document Understanding", "Large Vision-Language Models", "Retrieval-Augmented Generation", "Multi-modal Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.07730", "pdf": "https://arxiv.org/pdf/2509.07730.pdf", "abs": "https://arxiv.org/abs/2509.07730", "title": "M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models", "authors": ["Zexuan Li", "Hongliang Dai", "Piji Li"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP2025 Main Conference", "summary": "For Relation Extraction (RE), the manual annotation of training data may be\nprohibitively expensive, since the sentences that contain the target relations\nin texts can be very scarce and difficult to find. It is therefore beneficial\nto develop an efficient method that can automatically extract training\ninstances from unlabeled texts for training RE models. Recently, large language\nmodels (LLMs) have been adopted in various natural language processing tasks,\nwith RE also benefiting from their advances. However, when leveraging LLMs for\nRE with predefined relation categories, two key challenges arise. First, in a\nmulti-class classification setting, LLMs often struggle to comprehensively\ncapture the semantics of every relation, leading to suboptimal results. Second,\nalthough employing binary classification for each relation individually can\nmitigate this issue, it introduces significant computational overhead,\nresulting in impractical time complexity for real-world applications.\nTherefore, this paper proposes a framework called M-BRe to extract training\ninstances from unlabeled texts for RE. It utilizes three modules to combine the\nadvantages of both of the above classification approaches: Relation Grouping,\nRelation Extraction, and Label Decision. Extensive experiments confirm its\nsuperior capability in discovering high-quality training samples from unlabeled\ntexts for RE.", "AI": {"tldr": "The paper proposes M-BRe, a framework for Relation Extraction that automatically extracts training instances from unlabeled texts, addressing challenges in using LLMs for multi-class and binary classification.", "motivation": "Manual annotation for Relation Extraction is costly and challenging due to the scarcity of relevant sentences, necessitating an efficient method to extract training data automatically.", "method": "M-BRe utilizes three modules: Relation Grouping, Relation Extraction, and Label Decision to enhance the extraction of training instances from unlabeled texts.", "result": "The framework demonstrates superior performance in finding high-quality training samples compared to traditional methods.", "conclusion": "The M-BRe framework effectively addresses the limitations of existing approaches in RE by leveraging unlabeled texts, thus providing a more efficient means of training RE models.", "key_contributions": ["Proposed a new framework (M-BRe) that automates training instance extraction for Relation Extraction.", "Introduced a novel combination of relation grouping and binary classification to improve classification efficiency.", "Provided empirical evidence of superior performance through extensive experiments."], "limitations": "", "keywords": ["Relation Extraction", "Large Language Models", "Natural Language Processing", "Unlabeled Texts", "Training Data Extraction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.07755", "pdf": "https://arxiv.org/pdf/2509.07755.pdf", "abs": "https://arxiv.org/abs/2509.07755", "title": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts", "authors": ["Rochana Prih Hastuti", "Rian Adam Rajagede", "Mansour Al Ghanim", "Mengxin Zheng", "Qian Lou"], "categories": ["cs.CL", "cs.CR"], "comment": "Accepted at EMNLP 2025 Findings", "summary": "As large language models (LLMs) adapted to sensitive domains such as\nmedicine, their fluency raises safety risks, particularly regarding provenance\nand accountability. Watermarking embeds detectable patterns to mitigate these\nrisks, yet its reliability in medical contexts remains untested. Existing\nbenchmarks focus on detection-quality tradeoffs, overlooking factual risks\nunder low-entropy settings often exploited by watermarking's reweighting\nstrategy. We propose a medical-focused evaluation workflow that jointly\nassesses factual accuracy and coherence. Using GPT-Judger and further human\nvalidation, we introduce the Factuality-Weighted Score (FWS), a composite\nmetric prioritizing factual accuracy beyond coherence to guide watermarking\ndeployment in medical domains. Our evaluation shows current watermarking\nmethods substantially compromise medical factuality, with entropy shifts\ndegrading medical entity representation. These findings underscore the need for\ndomain-aware watermarking approaches that preserve the integrity of medical\ncontent.", "AI": {"tldr": "The paper evaluates watermarking methods for large language models in medical contexts, highlighting the need for approaches that maintain factual accuracy.", "motivation": "To address safety risks associated with the application of large language models in sensitive medical domains, focusing on provenance and accountability.", "method": "A medical-focused evaluation workflow was proposed, utilizing the Factuality-Weighted Score (FWS) to assess factual accuracy and coherence. This included the use of GPT-Judger and human validation.", "result": "Current watermarking methods significantly compromise medical factuality, with entropy shifts leading to degraded representation of medical entities.", "conclusion": "There is a critical need for watermarking strategies that are aware of medical domain specifics to ensure the integrity of medical information.", "key_contributions": ["Introduction of a medical-focused evaluation workflow for watermarking in LLMs.", "Development of the Factuality-Weighted Score (FWS) to assess factual accuracy more rigorously.", "Highlighting serious shortcomings in existing watermarking methods related to medical content integrity."], "limitations": "The findings are based on a limited set of watermarking methods and may not encompass all potential risks in other sensitive domains.", "keywords": ["large language models", "watermarking", "medical informatics", "factual accuracy", "safety risks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.07768", "pdf": "https://arxiv.org/pdf/2509.07768.pdf", "abs": "https://arxiv.org/abs/2509.07768", "title": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning", "authors": ["Michele Joshua Maggini", "Dhia Merzougui", "Rabiraj Bandyopadhyay", "Gaël Dias", "Fabrice Maurel", "Pablo Gamallo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The spread of fake news, polarizing, politically biased, and harmful content\non online platforms has been a serious concern. With large language models\nbecoming a promising approach, however, no study has properly benchmarked their\nperformance across different models, usage methods, and languages. This study\npresents a comprehensive overview of different Large Language Models adaptation\nparadigms for the detection of hyperpartisan and fake news, harmful tweets, and\npolitical bias. Our experiments spanned 10 datasets and 5 different languages\n(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and\nmulticlass classification scenarios. We tested different strategies ranging\nfrom parameter efficient Fine-Tuning of language models to a variety of\ndifferent In-Context Learning strategies and prompts. These included zero-shot\nprompts, codebooks, few-shot (with both randomly-selected and\ndiversely-selected examples using Determinantal Point Processes), and\nChain-of-Thought. We discovered that In-Context Learning often underperforms\nwhen compared to Fine-Tuning a model. This main finding highlights the\nimportance of Fine-Tuning even smaller models on task-specific settings even\nwhen compared to the largest models evaluated in an In-Context Learning setup -\nin our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and\nQwen2.5-7B-Instruct.", "AI": {"tldr": "This study benchmarks different Large Language Models for detecting hyperpartisan, fake news, and political bias across multiple languages and datasets, highlighting the effectiveness of Fine-Tuning over In-Context Learning.", "motivation": "The prevalence of fake news and harmful content on online platforms necessitates the evaluation of LLMs for their effectiveness in detection tasks.", "method": "The study tested various adaptation paradigms, including Fine-Tuning and In-Context Learning, across 10 datasets and 5 languages, using techniques like zero-shot prompts and few-shot learning.", "result": "Fine-Tuning models outperformed In-Context Learning approaches in detecting hyperpartisan content, regardless of model size, indicating its critical importance for task-specific applications.", "conclusion": "Fine-Tuning smaller models is more effective than relying on larger models with In-Context Learning, particularly for specific detection tasks.", "key_contributions": ["Comprehensive benchmarking of LLMs for fake news detection across multiple languages.", "Evaluation of Fine-Tuning vs. In-Context Learning methods.", "Identification of strategies for effective model adaptation in political bias detection."], "limitations": "The study primarily focuses on a limited number of languages and models, which may not generalize to all contexts.", "keywords": ["Large Language Models", "Fake News Detection", "Political Bias", "Fine-Tuning", "In-Context Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.07801", "pdf": "https://arxiv.org/pdf/2509.07801.pdf", "abs": "https://arxiv.org/abs/2509.07801", "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP", "authors": ["Decheng Duan", "Yingyi Zhang", "Jitong Peng", "Chengzhi Zhang"], "categories": ["cs.CL", "cs.DL", "cs.IR"], "comment": "EMNLP 2025 Main", "summary": "Structured information extraction from scientific literature is crucial for\ncapturing core concepts and emerging trends in specialized fields. While\nexisting datasets aid model development, most focus on specific publication\nsections due to domain complexity and the high cost of annotating scientific\ntexts. To address this limitation, we introduce SciNLP - a specialized\nbenchmark for full-text entity and relation extraction in the Natural Language\nProcessing (NLP) domain. The dataset comprises 60 manually annotated full-text\nNLP publications, covering 7,072 entities and 1,826 relations. Compared to\nexisting research, SciNLP is the first dataset providing full-text annotations\nof entities and their relationships in the NLP domain. To validate the\neffectiveness of SciNLP, we conducted comparative experiments with similar\ndatasets and evaluated the performance of state-of-the-art supervised models on\nthis dataset. Results reveal varying extraction capabilities of existing models\nacross academic texts of different lengths. Cross-comparisons with existing\ndatasets show that SciNLP achieves significant performance improvements on\ncertain baseline models. Using models trained on SciNLP, we implemented\nautomatic construction of a fine-grained knowledge graph for the NLP domain.\nOur KG has an average node degree of 3.2 per entity, indicating rich semantic\ntopological information that enhances downstream applications. The dataset is\npublicly available at https://github.com/AKADDC/SciNLP.", "AI": {"tldr": "Introducing SciNLP, a benchmark dataset for full-text entity and relation extraction in NLP, encompassing 60 annotated publications and demonstrating performance improvements over existing models.", "motivation": "To facilitate effective extraction of core concepts and relationships in scientific literature by providing a dedicated dataset that encompasses full-text annotations.", "method": "Development of the SciNLP dataset with manual annotations of entities and relations from 60 full-text NLP publications; comparative experiments with existing datasets to evaluate extraction capabilities of models.", "result": "SciNLP shows significant performance improvements for certain baseline supervised models in extracting entities and relations compared to existing datasets.", "conclusion": "SciNLP is a valuable resource for enhancing the performance of NLP models and for constructing knowledge graphs in the domain.", "key_contributions": ["First dataset for full-text annotations of entities and relationships in NLP", "Publicly available dataset with substantial annotations", "Demonstration of improved model performance using SciNLP"], "limitations": "", "keywords": ["information extraction", "NLP", "knowledge graph", "dataset", "entity relation"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.07817", "pdf": "https://arxiv.org/pdf/2509.07817.pdf", "abs": "https://arxiv.org/abs/2509.07817", "title": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems", "authors": ["Xiaolin Chen", "Xuemeng Song", "Haokun Wen", "Weili Guan", "Xiangyu Zhao", "Liqiang Nie"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Textual response generation is pivotal for multimodal \\mbox{task-oriented}\ndialog systems, which aims to generate proper textual responses based on the\nmultimodal context. While existing efforts have demonstrated remarkable\nprogress, there still exist the following limitations: 1) \\textit{neglect of\nunstructured review knowledge} and 2) \\textit{underutilization of large\nlanguage models (LLMs)}. Inspired by this, we aim to fully utilize dual\nknowledge (\\textit{i.e., } structured attribute and unstructured review\nknowledge) with LLMs to promote textual response generation in multimodal\ntask-oriented dialog systems. However, this task is non-trivial due to two key\nchallenges: 1) \\textit{dynamic knowledge type selection} and 2)\n\\textit{intention-response decoupling}. To address these challenges, we propose\na novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for\nmultimodal dialog systems (named DK2R). To be specific, DK2R first extracts\nboth structured attribute and unstructured review knowledge from external\nknowledge base given the dialog context. Thereafter, DK2R uses an LLM to\nevaluate each knowledge type's utility by analyzing LLM-generated provisional\nprobe responses. Moreover, DK2R separately summarizes the intention-oriented\nkey clues via dedicated reasoning, which are further used as auxiliary signals\nto enhance LLM-based textual response generation. Extensive experiments\nconducted on a public dataset verify the superiority of DK2R. We have released\nthe codes and parameters.", "AI": {"tldr": "The paper introduces DK2R, a dual knowledge-enhanced reasoner that utilizes structured and unstructured knowledge to improve textual response generation in multimodal dialog systems.", "motivation": "To improve textual response generation in multimodal task-oriented dialog systems by addressing the limitations of neglecting unstructured review knowledge and underutilizing large language models (LLMs).", "method": "The proposed DK2R employs a two-stage reasoner that extracts both structured and unstructured knowledge from an external knowledge base and evaluates their utility through LLM-generated probe responses.", "result": "Experimental results on a public dataset demonstrate the superiority of DK2R over existing methods in generating responses.", "conclusion": "DK2R effectively enhances the response generation process by combining dual knowledge types and leveraging LLMs, indicating potential advancements in multimodal dialog systems.", "key_contributions": ["Introduction of a dual knowledge-enhanced reasoner (DK2R) for textual response generation.", "Addressing the challenges of dynamic knowledge type selection and intention-response decoupling in dialog systems.", "Demonstration of DK2R's effectiveness through extensive experiments."], "limitations": "", "keywords": ["multimodal dialog systems", "large language models", "dual knowledge", "textual response generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.07829", "pdf": "https://arxiv.org/pdf/2509.07829.pdf", "abs": "https://arxiv.org/abs/2509.07829", "title": "Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost", "authors": ["Mihai Nadas", "Laura Diosan", "Andreea Tomescu", "Andrei Piscoran"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 8 figures, includes datasets and models released on Hugging\n  Face", "summary": "Literary translation has recently gained attention as a distinct and complex\ntask in machine translation research. However, the translation by small open\nmodels remains an open problem. We contribute to this ongoing research by\nintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for\ndataset creation, fine tuning, and evaluation in English-Romanian literary\ntranslations, centred on the creation and open release of both a compact, fine\ntuned language model (TF2-12B) and large scale synthetic parallel datasets\n(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the\nlargest collection of synthetic English fables to date, we address the need for\nrich, high quality literary datasets in low resource languages such as\nRomanian. Our pipeline first generates 15k high quality Romanian references\nfrom the TF1 pool using a high performing LLM. We then apply a two stage fine\ntuning process to a 12B parameter open weight model: (i) instruction tuning to\ncapture genre specific narrative style, and (ii) adapter compression for\nefficient deployment. Evaluation combines corpus level BLEU and a five\ndimension LLM based rubric (accuracy, fluency, coherence, style, cultural\nadaptation) to provide a nuanced assessment of translation quality. Results\nshow that our fine tuned model achieves fluency and adequacy competitive with\ntop performing large proprietary models, while being open, accessible, and\nsignificantly more cost effective. Alongside the fine tuned model and both\ndatasets, we publicly release all scripts and evaluation prompts. TF2 thus\nprovides an end-to-end, reproducible pipeline for research on cost efficient\ntranslation, cross lingual narrative generation, and the broad adoption of open\nmodels for culturally significant literary content in low resource settings.", "AI": {"tldr": "The paper introduces TINYFABULIST TRANSLATION FRAMEWORK (TF2), focusing on creating and fine-tuning a language model for English-Romanian literary translation, accompanied by the release of significant datasets.", "motivation": "To address the gap in high-quality literary translation by small open models, particularly for low resource languages like Romanian.", "method": "The framework includes dataset creation, a two-stage fine-tuning process for a 12B parameter model, and nuanced quality evaluation using BLEU and an LLM-based rubric.", "result": "TF2's fine-tuned model matches the fluency and adequacy of leading proprietary models while being open and cost-effective.", "conclusion": "TF2 represents a reproducible approach to enhance literary translations and supports the use of open models for low resource languages.", "key_contributions": ["Introduction of the TF2 framework for dataset creation and model fine-tuning", "Development of the DS-TF2 datasets released for public use", "Evaluation methodology combining traditional metrics with LLM-based assessments"], "limitations": "", "keywords": ["literary translation", "machine translation", "low resource languages", "language model", "dataset creation"], "importance_score": 2, "read_time_minutes": 25}}
{"id": "2509.07869", "pdf": "https://arxiv.org/pdf/2509.07869.pdf", "abs": "https://arxiv.org/abs/2509.07869", "title": "Are Humans as Brittle as Large Language Models?", "authors": ["Jiahui Li", "Sean Papay", "Roman Klinger"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "The output of large language models (LLM) is unstable, due to both\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\ninstruction changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs.", "AI": {"tldr": "This paper compares the sensitivity of human annotators and LLMs to prompt modifications in text classification tasks.", "motivation": "To investigate whether prompt brittleness seen in LLMs is also present in human annotations, thus understanding the implications for LLM reliability.", "method": "The study systematically prompts both human annotators and LLMs with variations in instructions for text classification tasks to assess their sensitivity to changes.", "result": "Both humans and LLMs showed increased brittleness with certain prompt alterations; however, human responses were less affected by typographical errors and label order changes than LLM outputs.", "conclusion": "Prompt brittleness is not unique to LLMs; humans also exhibit this behavior but with varying degrees of sensitivity to types of prompt changes.", "key_contributions": ["Systematic comparison of LLMs and human annotators regarding prompt sensitivity.", "Identified specific prompt modifications that affect both LLMs and human judgments.", "Provided insights into the implications of prompt brittleness in the context of human annotation variances."], "limitations": "The study focuses on particular text classification tasks, which may not generalize to other tasks or models.", "keywords": ["large language models", "human annotators", "prompt brittleness", "text classification", "sensitivity analysis"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.07889", "pdf": "https://arxiv.org/pdf/2509.07889.pdf", "abs": "https://arxiv.org/abs/2509.07889", "title": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing", "authors": ["Chengyan Wu", "Yiqiang Cai", "Yufei Cheng", "Yun Xue"], "categories": ["cs.CL"], "comment": "NLPCC 2025", "summary": "This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which\nfocuses on sentence-level gender bias detection and mitigation in Chinese. The\ntask aims to promote fairness and controllability in natural language\ngeneration by automatically detecting, classifying, and mitigating gender bias.\nTo address this challenge, we adopt a fine-tuning approach based on large\nlanguage models (LLMs), efficiently adapt to the bias detection task via\nLow-Rank Adaptation (LoRA). In terms of data processing, we construct a more\nbalanced training set to alleviate class imbalance and introduce heterogeneous\nsamples from multiple sources to enhance model generalization. For the\ndetection and classification sub-tasks, we employ a majority voting strategy\nthat integrates outputs from multiple expert models to boost performance.\nAdditionally, to improve bias generation detection and mitigation, we design a\nmulti-temperature sampling mechanism to capture potential variations in bias\nexpression styles. Experimental results demonstrate the effectiveness of our\napproach in bias detection, classification, and mitigation. Our method\nultimately achieves an average score of 47.90%, ranking fourth in the shared\ntask.", "AI": {"tldr": "The paper addresses gender bias detection and mitigation in Chinese using LLMs, achieving a competitive ranking in a shared task.", "motivation": "To promote fairness and controllability in natural language generation by addressing gender bias.", "method": "A fine-tuning approach on LLMs using Low-Rank Adaptation (LoRA) and a balanced training set for effective bias detection and mitigation.", "result": "The method ranked fourth in the shared task with an average score of 47.90%, demonstrating effective bias detection and mitigation performance.", "conclusion": "The proposed methods improve effectiveness in detecting, classifying, and mitigating gender bias in text generation.", "key_contributions": ["Utilization of Low-Rank Adaptation (LoRA) for fine-tuning LLMs in gender bias tasks", "Construction of a balanced training set to address class imbalance", "Design of a multi-temperature sampling mechanism for bias expression variations."], "limitations": "", "keywords": ["gender bias", "natural language generation", "large language models", "Low-Rank Adaptation", "Chinese"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.07908", "pdf": "https://arxiv.org/pdf/2509.07908.pdf", "abs": "https://arxiv.org/abs/2509.07908", "title": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories", "authors": ["Donya Rooein", "Vilém Zouhar", "Debora Nozza", "Dirk Hovy"], "categories": ["cs.CL"], "comment": null, "summary": "Stories play a pivotal role in human communication, shaping beliefs and\nmorals, particularly in children. As parents increasingly rely on large\nlanguage models (LLMs) to craft bedtime stories, the presence of cultural and\ngender stereotypes in these narratives raises significant concerns. To address\nthis issue, we present Biased Tales, a comprehensive dataset designed to\nanalyze how biases influence protagonists' attributes and story elements in\nLLM-generated stories. Our analysis uncovers striking disparities. When the\nprotagonist is described as a girl (as compared to a boy), appearance-related\nattributes increase by 55.26%. Stories featuring non-Western children\ndisproportionately emphasize cultural heritage, tradition, and family themes\nfar more than those for Western children. Our findings highlight the role of\nsociocultural bias in making creative AI use more equitable and diverse.", "AI": {"tldr": "The paper presents 'Biased Tales', a dataset to analyze biases in LLM-generated stories, revealing significant disparities in attributes based on gender and cultural backgrounds.", "motivation": "To investigate and address concerns about cultural and gender stereotypes in bedtime stories generated by large language models as parents increasingly use them.", "method": "Analysis of the 'Biased Tales' dataset to study the impact of sociocultural bias on protagonist attributes and story elements in narratives generated by LLMs.", "result": "The study found that stories with girl protagonists highlight appearance-related attributes significantly more than those with boy protagonists, and stories featuring non-Western children emphasize cultural themes disproportionately.", "conclusion": "Addressing sociocultural bias in LLM-generated stories is crucial for creating more equitable and diverse narratives.", "key_contributions": ["Introduction of the 'Biased Tales' dataset for analyzing LLM-generated stories", "Quantitative analysis showing bias in character attributes based on gender and culture", "Emphasis on the importance of understanding sociocultural influences in AI-generated content."], "limitations": "The study may not cover all biases present in LLM-generated narratives and focuses primarily on specific attributes related to gender and culture.", "keywords": ["bias", "large language models", "storytelling", "sociocultural bias", "dataset"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.07925", "pdf": "https://arxiv.org/pdf/2509.07925.pdf", "abs": "https://arxiv.org/abs/2509.07925", "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models", "authors": ["Tuo Wang", "Adithya Kulkarni", "Tyler Cody", "Peter A. Beling", "Yujun Yan", "Dawei Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP 2025", "summary": "Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ.", "AI": {"tldr": "GENUINE is a framework for enhancing uncertainty estimation in Large Language Models using graph-based techniques.", "motivation": "Uncertainty estimation is critical for the reliability of LLMs in high-stakes applications, as current methods fail to incorporate semantic dependencies and structural relationships within text.", "method": "GENUINE utilizes dependency parse trees and hierarchical graph pooling combined with supervised learning to improve uncertainty quantification in LLM outputs.", "result": "GENUINE achieved up to 29% higher AUROC compared to existing semantic entropy-based methods and reduced calibration errors by over 15% across various NLP tasks.", "conclusion": "The results indicate that graph-based modeling significantly enhances uncertainty quantification in LLMs, making them more reliable for applications demanding high confidence assessments.", "key_contributions": ["Introduces a graph-based approach for uncertainty estimation in LLMs.", "Demonstrates significant improvements in AUROC and calibration errors through extensive experimentation.", "Provides an open-source implementation for further research and application."], "limitations": "", "keywords": ["Large Language Models", "Uncertainty Estimation", "Graph Theory", "Natural Language Processing", "Supervised Learning"], "importance_score": 10, "read_time_minutes": 10}}
{"id": "2509.07968", "pdf": "https://arxiv.org/pdf/2509.07968.pdf", "abs": "https://arxiv.org/abs/2509.07968", "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge", "authors": ["Lukas Haas", "Gal Yona", "Giovanni D'Antonio", "Sasha Goldshtein", "Dipanjan Das"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.", "AI": {"tldr": "Introduction of SimpleQA Verified, a 1,000-prompt benchmark for evaluating LLM factuality, improving upon OpenAI's existing benchmark.", "motivation": "To address limitations in OpenAI's SimpleQA benchmark such as noisy labels, biases, and redundancy, providing a more reliable evaluation framework for LLMs.", "method": "A rigorous multi-stage filtering process was employed to create the benchmark, ensuring de-duplication, topic balancing, and source reconciliation.", "result": "Gemini 2.5 Pro achieved a state-of-the-art F1-score of 55.6 on the new benchmark, outperforming other models like GPT-5.", "conclusion": "The SimpleQA Verified benchmark offers a higher-fidelity tool for evaluating LLM factuality and tracking model improvements, with resources available for the research community.", "key_contributions": ["Creation of SimpleQA Verified benchmark with 1,000 prompts", "Improvement over previous OpenAI benchmarks by addressing key limitations", "Providing tools for reliable evaluation of parametric model factuality"], "limitations": "", "keywords": ["Large Language Models", "factuality", "benchmarking", "machine learning", "evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.07980", "pdf": "https://arxiv.org/pdf/2509.07980.pdf", "abs": "https://arxiv.org/abs/2509.07980", "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning", "authors": ["Tong Zheng", "Hongming Zhang", "Wenhao Yu", "Xiaoyang Wang", "Xinyu Yang", "Runpeng Dai", "Rui Liu", "Huiwen Bao", "Chengsong Huang", "Heng Huang", "Dong Yu"], "categories": ["cs.CL"], "comment": "Project website: https://zhengkid.github.io/Parallel_R1.github.io/", "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1.", "AI": {"tldr": "This paper introduces Parallel-R1, a reinforcement learning framework that enhances large language models' reasoning by promoting parallel thinking through a curriculum-based training method.", "motivation": "The need to enhance reasoning abilities in large language models through a method that encourages exploration and generalization rather than imitation.", "method": "The proposed Parallel-R1 framework employs a two-stage training process: first, supervised fine-tuning on simpler tasks followed by reinforcement learning on more complex problems to instill and generalize parallel thinking behaviors.", "result": "Experiments demonstrate that Parallel-R1 achieves an 8.4% accuracy improvement over models trained directly on difficult tasks, and a 42.9% improvement on specific benchmarks.", "conclusion": "Parallel thinking serves as an effective exploratory strategy in reinforcement learning, leading to higher performance ceilings for models on real-world reasoning tasks.", "key_contributions": ["Introduction of Parallel-R1, the first RL framework for parallel thinking in LLMs.", "Development of a progressive curriculum to address cold-start issues in RL training.", "Empirical validation showing significant performance improvements in math reasoning tasks."], "limitations": "", "keywords": ["Parallel Thinking", "Reinforcement Learning", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.07202", "pdf": "https://arxiv.org/pdf/2509.07202.pdf", "abs": "https://arxiv.org/abs/2509.07202", "title": "Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data", "authors": ["Khushiyant"], "categories": ["cs.HC", "cs.CL", "I.2.7; I.2.6; J.3"], "comment": "15 pages, 10 figures, 5 tables", "summary": "Text generating capabilities have undergone a substantial transformation with\nthe introduction of large language models (LLMs). Electroencephalography\n(EEG)-based text production is still difficult, though, because it requires a\nlot of data and processing power. This paper introduces a new method that\ncombines the use of the Gemma 2B LLM with a classifier-LLM architecture to\nincorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically\nlowers the amount of data and compute power needed while achieving performance\nclose to that of cutting-edge methods. Notably, compared to current\nmethodologies, our methodology delivers an overall performance improvement of\n10%. The suggested architecture demonstrates the possibility of effective\ntransfer learning for EEG-based text production, remaining strong and\nfunctional even in the face of data limits. This work highlights the potential\nof integrating LLMs with EEG decoding to improve assistive technologies and\nimprove independence and communication for those with severe motor limitations.\nOur method pushes the limits of present capabilities and opens new paths for\nresearch and application in brain-computer interfaces by efficiently using the\nstrengths of pre-trained language models. This makes EEG-based text production\nmore accessible and efficient.", "AI": {"tldr": "This paper presents a new method that combines a large language model (LLM) with a classifier-LLM architecture to facilitate EEG-based text production, significantly reducing data and compute requirements while improving performance by 10%.", "motivation": "To address the challenges in EEG-based text generation which traditionally require substantial data and processing power.", "method": "The study introduces a new architecture that uses the Gemma 2B LLM alongside a classifier-LLM architecture with a Recurrent Neural Network (RNN) encoder.", "result": "The proposed method achieves a 10% performance improvement over existing methodologies while greatly reducing the data and computing power needed for EEG-based text generation.", "conclusion": "The integration of LLMs with EEG decoding significantly enhances assistive technologies, improving independence and communication for users with severe motor limitations and setting a new direction for research in brain-computer interfaces.", "key_contributions": ["Introduction of a novel LLM-driven approach for EEG-based text generation.", "Performance improvement of 10% in text production over existing methods.", "Demonstration of effective transfer learning in low-data scenarios."], "limitations": "", "keywords": ["EEG", "text generation", "large language models", "brain-computer interfaces", "assistive technologies"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2310.16582", "pdf": "https://arxiv.org/pdf/2310.16582.pdf", "abs": "https://arxiv.org/abs/2310.16582", "title": "UPLex: Fine-Grained Personality Control in Large Language Models via Unsupervised Lexical Modulation", "authors": ["Tianlong Li", "Wenhao Liu", "Muling Wu", "Shihan Dou", "Zhenghua Wang", "Changze Lv", "Xiaohua Wang", "Xiaoqing Zheng", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Personality is a crucial factor that shapes human communication patterns,\nthereby regulating the personalities of large language models (LLMs) holds\nsignificant potential in enhancing their user experiences. Previous approaches\neither relied on fine-tuning LLMs on specific corpora or required manually\ncrafted prompts to evoke specific personalities from LLMs. However, the former\nis inefficient and costly, while the latter cannot precisely manipulate\npersonality traits at a fine-grained level. To address these challenges, we\npropose UPLex, a method that uses an Unsupervisedly-Built Personalized Lexicon\n(UPL) during the decoding phase to manipulate LLM's personality traits. UPL can\nbe constructed from a newly built situational judgment test dataset in an\nunsupervised fashion, and used to modulate the personality expression of LLMs\nby dynamically altering their predicted probability of upcoming words in a\npluggable fashion. Extensive experimentation demonstrates the remarkable\neffectiveness and pluggability of our method for fine-grained manipulation of\nLLMs' personalities.", "AI": {"tldr": "The paper presents UPLex, a novel method for manipulating the personality traits of large language models (LLMs) using an unsupervised personalized lexicon during the decoding phase, allowing for fine-grained control over personality expression.", "motivation": "To enhance user experiences with LLMs by directly manipulating their personality traits, addressing the limitations of previous methods which were either costly or lacked precision.", "method": "UPLex constructs an Unsupervisedly-Built Personalized Lexicon (UPL) from a situational judgment test dataset and uses it to modulate LLM personality during the decoding phase by altering predicted word probabilities.", "result": "Experimental results show that UPLex effectively allows for fine-grained manipulation of LLM personalities with high adaptability.", "conclusion": "UPLex demonstrates a robust and pluggable solution for improving LLM interactions through the controlled expression of personality traits.", "key_contributions": ["Introduction of an unsupervised method for building a personalized lexicon", "Demonstration of fine-grained control over LLM personality traits", "Validation of method effectiveness through extensive experiments"], "limitations": "", "keywords": ["large language models", "personality manipulation", "unsupervised learning", "natural language processing", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2405.20404", "pdf": "https://arxiv.org/pdf/2405.20404.pdf", "abs": "https://arxiv.org/abs/2405.20404", "title": "JoPA:Explaining Large Language Model's Generation via Joint Prompt Attribution", "authors": ["Yurui Chang", "Bochuan Cao", "Yujia Wang", "Jinghui Chen", "Lu Lin"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of understanding the causality between input and output pairs.\nExisting works for providing prompt-specific explanation often confine model\noutput to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt\ntexts collaboratively influences the LLM's complete generation. Particularly,\nwe formulate the task of prompt attribution for generation interpretation as a\ncombinatorial optimization problem, and introduce a probabilistic algorithm to\nsearch for the casual input combination in the discrete space. We define and\nutilize multiple metrics to evaluate the produced explanations, demonstrating\nboth the faithfulness and efficiency of our framework.", "AI": {"tldr": "This study presents a framework for understanding how input prompts collaboratively influence language generation in Large Language Models, using counterfactual explanations and probabilistic optimization.", "motivation": "Understanding the relationship between input prompts and generated text in LLMs is crucial, yet remains underexplored, particularly in explaining complex text generation.", "method": "The authors introduce a framework called Joint Prompt Attribution (JoPA) which formulates prompt attribution as a combinatorial optimization problem and employs a probabilistic algorithm to identify influential input combinations.", "result": "The framework provides an explanation of input prompts' collaborative effects on a model's output, evaluated through various metrics for faithfulness and efficiency.", "conclusion": "The proposed framework improves understanding of LLM generation by addressing the combinatorial nature of prompt interactions, offering insights that existing methods may overlook.", "key_contributions": ["Introduction of a counterfactual explanation framework for LLMs.", "Formulation of prompt attribution as a combinatorial optimization problem.", "A probabilistic algorithm for identifying influential input combinations."], "limitations": "", "keywords": ["Large Language Models", "Prompt Attribution", "Combinatorial Optimization", "Language Generation", "Counterfactual Explanations"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2407.12791", "pdf": "https://arxiv.org/pdf/2407.12791.pdf", "abs": "https://arxiv.org/abs/2407.12791", "title": "CTourLLM: Enhancing LLMs with Chinese Tourism Knowledge", "authors": ["Qikai Wei", "Mingzhi Yang", "Jinqiang Wang", "Wenwei Mao", "Jiabo Xu", "Huansheng Ning"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated their effectiveness\nin various natural language processing (NLP) tasks. However, the lack of\ntourism knowledge limits the performance of LLMs in tourist attraction\npresentations and travel planning. To address this challenge, we constructed a\nsupervised fine-tuning dataset for the Chinese culture and tourism domain,\nnamed Cultour. This dataset consists of three parts: tourism knowledge base\ndata, travelogues data, and tourism QA data. Additionally, we propose CTourLLM,\na Qwen-based model supervised fine-tuned with Cultour, to improve the quality\nof information about attractions and travel planning. To evaluate the\nperformance of CTourLLM, we proposed a human evaluation criterion named RRA\n(Relevance, Readability, Availability), and employed both automatic and human\nevaluation. The experimental results demonstrate that CTourLLM outperforms\nChatGPT, achieving an improvement of 1.21 in BLEU-1 and 1.54 in Rouge-L,\nthereby validating the effectiveness of the response outcomes. Our proposed\nCultour is accessible at https://github.com/mrweiqk/Cultour.", "AI": {"tldr": "This paper introduces Cultour, a dataset for enhancing LLM performance in tourism applications, and CTourLLM, a model fine-tuned on Cultour, showing improved results over ChatGPT.", "motivation": "To improve the effectiveness of large language models in tourist attraction presentations and travel planning by providing better tourism knowledge.", "method": "A supervised fine-tuning dataset for the Chinese culture and tourism domain was created, consisting of tourism knowledge base data, travelogues data, and tourism QA data, along with the development of the CTourLLM model.", "result": "CTourLLM outperforms ChatGPT in various evaluations, improving BLEU-1 by 1.21 and Rouge-L by 1.54.", "conclusion": "The creation of the Cultour dataset and the development of the CTourLLM model effectively enhance the quality of information in tourism-related tasks.", "key_contributions": ["Introduction of the Cultour dataset for tourism-related NLP tasks.", "Development of CTourLLM model fine-tuned on Cultour data supporting better travel planning.", "Introduction of a new evaluation criterion (RRA) for assessing responses."], "limitations": "", "keywords": ["large language models", "tourism", "natural language processing", "fine-tuning", "chatbot performance"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2411.02886", "pdf": "https://arxiv.org/pdf/2411.02886.pdf", "abs": "https://arxiv.org/abs/2411.02886", "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection", "authors": ["Wei Wu", "Zhuoshi Pan", "Chao Wang", "Liyi Chen", "Yunchu Bai", "Tianfu Wang", "Kun Fu", "Zheng Wang", "Hui Xiong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP2025", "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.", "AI": {"tldr": "The paper introduces TokenSelect, a method for efficient long-context inference in LLMs that addresses performance degradation and long inference times.", "motivation": "To overcome the limitations of using Large Language Models for long-context scenarios due to degraded performance and long inference times.", "method": "TokenSelect employs a training-free approach that utilizes non-contiguous attention sparsity and QK dot products to evaluate the criticality of KV cache tokens at a token level, optimizing attention computation.", "result": "TokenSelect achieves up to 23.84 times speedup in attention computation and 2.28 times acceleration in end-to-end latency compared to existing long-context inference methods.", "conclusion": "The evaluation shows TokenSelect significantly improves the efficiency and performance of LLMs in long-context tasks without sacrificing accuracy.", "key_contributions": ["Introduction of Dynamic Token-Level KV Cache Selection (TokenSelect) for LLMs.", "Novel method that combines attention sparsity with a soft voting mechanism for improved efficiency.", "Empirical evaluation demonstrates significant performance gains over state-of-the-art methods."], "limitations": "", "keywords": ["Large Language Models", "Long-context inference", "Attention mechanism"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.18934", "pdf": "https://arxiv.org/pdf/2412.18934.pdf", "abs": "https://arxiv.org/abs/2412.18934", "title": "Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference", "authors": ["Libo Zhang", "Zhaoning Zhang", "Baizhou Xu", "Rui Li", "Zhiliang Tian", "Songzhu Mei", "Dongsheng Li"], "categories": ["cs.CL"], "comment": "14 pages, 6 figures", "summary": "With the continuous advancement in the performance of large language models\n(LLMs), their demand for computational resources and memory has significantly\nincreased, which poses major challenges for efficient inference on\nconsumer-grade devices and legacy servers. These devices typically feature\nrelatively weaker GPUs and stronger CPUs. Although techniques such as parameter\noffloading and partial offloading can alleviate GPU memory pressure to some\nextent, their effectiveness is limited due to communication latency and\nsuboptimal hardware resource utilization. To address this issue, we propose\nDovetail, a lossless inference acceleration method that leverages the\ncomplementary characteristics of heterogeneous devices and the advantages of\nspeculative decoding. Dovetail deploys a draft model on the GPU to perform\npreliminary predictions, while a target model running on the CPU validates\nthese outputs. By reducing the granularity of data transfer, Dovetail\nsignificantly minimizes communication overhead. To further improve efficiency,\nwe optimize the draft model specifically for heterogeneous hardware\nenvironments by reducing the number of draft tokens to lower parallel\nverification latency, increasing model depth to enhance predictive\ncapabilities, and introducing a Dynamic Gating Fusion (DGF) mechanism to\nimprove the integration of feature and embedding information. We conduct\ncomprehensive evaluations of Dovetail across various consumer-grade GPUs,\ncovering multiple tasks and mainstream models. Experimental results on 13B\nmodels demonstrate that Dovetail achieves inference speedups ranging from 1.79x\nto 10.1x across different devices, while maintaining consistency and stability\nin the distribution of generated texts.", "AI": {"tldr": "Dovetail is a lossless inference acceleration method for large language models that optimally utilizes CPU and GPU resources to improve performance on consumer-grade devices.", "motivation": "The performance of large language models has outpaced available computational resources on consumer-grade devices and legacy servers, necessitating efficient inference solutions.", "method": "Dovetail deploys a draft model on the GPU for preliminary predictions and a target model on the CPU for validation, optimizing data transfer and reducing verification latency.", "result": "Dovetail achieves inference speedups ranging from 1.79x to 10.1x on various consumer-grade GPUs, showing consistent and stable output generation across different tasks and models.", "conclusion": "The proposed method demonstrates significant efficiency improvements for LLM inference in heterogeneous hardware environments without sacrificing output quality.", "key_contributions": ["Introduction of Dovetail for efficient inference on consumer-grade devices", "Optimization of draft model for heterogeneous hardware environments", "Dynamic Gating Fusion mechanism for better feature integration"], "limitations": "", "keywords": ["Inference acceleration", "Large language models", "Heterogeneous devices"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.07128", "pdf": "https://arxiv.org/pdf/2502.07128.pdf", "abs": "https://arxiv.org/abs/2502.07128", "title": "Cardiverse: Harnessing LLMs for Novel Card Game Prototyping", "authors": ["Danrui Li", "Sen Zhang", "Sam S. Sohn", "Kaidong Hu", "Muhammad Usman", "Mubbasir Kapadia"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "37 pages, 13 figures, 8 tables. Accepted by EMNLP 2025", "summary": "The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game variations, an LLM-driven system for\nconsistent game code generation validated by gameplay records, and a gameplay\nAI constructing method that uses an ensemble of LLM-generated heuristic\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers. For code repo visit this http URL\nhttps://github.com/danruili/Cardiverse", "AI": {"tldr": "This paper presents an automated framework for card game prototyping using Large Language Models, addressing challenges in game design and AI development.", "motivation": "The need to automate and streamline the creative ideation and evaluation processes in card game design due to extensive human effort involved.", "method": "The proposed framework includes a graph-based indexing method for generating new game mechanics, an LLM-driven system for creating consistent game code, and a method for developing gameplay AI that utilizes an ensemble of LLM-generated heuristics optimized through self-play.", "result": "The framework successfully accelerates the prototyping process, reduces human labor, and lowers entry barriers for game developers.", "conclusion": "This innovative approach enhances the card game development process by leveraging advancements in LLMs to automate and optimize game design and evaluation tasks.", "key_contributions": ["Graph-based method for generating novel game variations", "LLM-driven consistent game code generation", "Gameplay AI using ensemble of LLM-generated heuristic functions"], "limitations": "", "keywords": ["Card Games", "Game Prototyping", "Large Language Models", "AI in Game Development", "Gameplay AI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.07322", "pdf": "https://arxiv.org/pdf/2502.07322.pdf", "abs": "https://arxiv.org/abs/2502.07322", "title": "MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject Batch Editing for LLMs", "authors": ["Zilu Dong", "Xiangqing Shen", "Rui Xia"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by ACL2025 findings", "summary": "As large language models continue to scale up, knowledge editing techniques\nthat modify models' internal knowledge without full retraining have gained\nsignificant attention. MEMIT, a prominent batch editing algorithm, stands out\nfor its capability to perform mass knowledge modifications. However, we uncover\nthat MEMIT's editing efficacy significantly deteriorates when processing\nbatches containing multiple edits sharing the same subject. Our analysis\nreveals this stems from MEMIT's key value modeling framework: identical keys\n(derived from the shared subject) are forced to represent different values\n(corresponding to different knowledge), resulting in update conflicts during\nediting. Addressing this issue, we propose MEMIT-Merge, an enhanced approach\nthat merges value computation processes for facts sharing the same subject,\neffectively resolving the performance degradation in samesubject batch editing\nscenarios. Experimental results demonstrate that when MEMIT's edit success rate\ndrops to around 50% at larger batch sizes, MEMIT-Merge maintains a success rate\nexceeding 90%, showcasing remarkable robustness to subject entity collisions.\nThe code is available at https://github.com/NUSTM/ MEMIT-Merge.", "AI": {"tldr": "MEMIT-Merge improves knowledge editing in language models by efficiently handling multiple edits of the same subject, showing higher success rates than MEMIT.", "motivation": "To address the performance decline of knowledge editing techniques, particularly MEMIT, when handling batches of edits concerning the same subject.", "method": "Proposes MEMIT-Merge, which merges value computations for facts sharing the same subject to mitigate update conflicts in the editing process.", "result": "MEMIT-Merge achieves a success rate exceeding 90% for larger batch sizes, compared to MEMIT's approximate 50% success rate, demonstrating improved editing efficacy.", "conclusion": "The proposed MEMIT-Merge proves to be a robust solution for batch editing in language models, particularly when dealing with conflicts arising from multiple edits of the same subject.", "key_contributions": ["Introduction of MEMIT-Merge to enhance MEMIT's performance in batch editing scenarios.", "Demonstration of improved success rates in knowledge editing tasks across larger batch sizes.", "Analysis of update conflicts due to identical keys in the original MEMIT framework."], "limitations": "", "keywords": ["language models", "knowledge editing", "MEMIT-Merge", "batch processing", "update conflicts"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.18993", "pdf": "https://arxiv.org/pdf/2502.18993.pdf", "abs": "https://arxiv.org/abs/2502.18993", "title": "MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering", "authors": ["Teng Lin", "Yuyu Luo", "Nan Tang"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Multi-entity question answering (MEQA) represents significant challenges for\nlarge language models (LLM) and retrieval-augmented generation (RAG) systems,\nwhich frequently struggle to consolidate scattered information across diverse\ndocuments. While existing methods excel at single-document comprehension, they\noften struggle with cross-document aggregation, particularly when resolving\nentity-dense questions like \"What is the distribution of ACM Fellows among\nvarious fields of study?\", which require integrating entity-centric insights\nfrom heterogeneous sources (e.g., Wikipedia pages). To address this gap, we\nintroduce MEBench, a novel multi-document, multi-entity benchmark designed to\nsystematically evaluate LLMs' capacity to retrieve, consolidate, and reason\nover fragmented information. Our benchmark comprises 4,780 questions which are\nsystematically categorized into three primary categories, further divided into\neight distinct types, ensuring broad coverage of real-world multi-entity\nreasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,\nLlama-3) and RAG pipelines reveal critical limitations: even advanced models\nachieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance\nof completeness and factual precision of information extraction in MEQA tasks,\nusing Entity-Attributed F1 (EA-F1) metric for granular evaluation of\nentity-level correctness and attribution validity. MEBench not only highlights\nsystemic weaknesses in current LLM frameworks but also provides a foundation\nfor advancing robust, entity-aware QA architectures.", "AI": {"tldr": "MEBench is a benchmark for evaluating large language models in multi-entity question answering (MEQA) by focusing on their ability to retrieve and integrate information from multiple documents.", "motivation": "Existing methods excel in single-document comprehension but falter in cross-document aggregation, particularly for entity-dense questions.", "method": "We developed the MEBench benchmark comprising 4,780 multi-entity questions categorized into three primary categories with eight distinct types, aimed at testing LLMs and RAG systems.", "result": "State-of-the-art LLMs, including GPT-4 and Llama-3, achieved only 59% accuracy on the MEBench, indicating significant limitations.", "conclusion": "MEBench reveals weaknesses in current LLM frameworks and emphasizes the need for improved entity-aware QA architectures.", "key_contributions": ["Introduction of the MEBench benchmark for MEQA tasks.", "Systematic categorization of questions into types for comprehensive evaluation.", "Granular evaluation metric (EA-F1) for assessing entity-level correctness."], "limitations": "Limited accuracy of advanced LLMs on the benchmark indicates potential areas for improvement in information extraction accuracy.", "keywords": ["multi-entity question answering", "large language models", "benchmark", "information extraction", "entity-aware QA"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.19548", "pdf": "https://arxiv.org/pdf/2502.19548.pdf", "abs": "https://arxiv.org/abs/2502.19548", "title": "When Large Language Models Meet Speech: A Survey on Integration Approaches", "authors": ["Zhengdong Yang", "Shuichiro Shimizu", "Yahan Yu", "Chenhui Chu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Findings of ACL 2025 (Long Paper)", "summary": "Recent advancements in large language models (LLMs) have spurred interest in\nexpanding their application beyond text-based tasks. A large number of studies\nhave explored integrating other modalities with LLMs, notably speech modality,\nwhich is naturally related to text. This paper surveys the integration of\nspeech with LLMs, categorizing the methodologies into three primary approaches:\ntext-based, latent-representation-based, and audio-token-based integration. We\nalso demonstrate how these methods are applied across various speech-related\napplications and highlight the challenges in this field to offer inspiration\nfor", "AI": {"tldr": "The paper surveys the integration of speech modalities with large language models, categorizing methodologies into text-based, latent-representation-based, and audio-token-based approaches.", "motivation": "Recent advancements in large language models have encouraged exploring their applications beyond text, particularly in integrating speech.", "method": "The paper categorizes methodologies into three approaches: text-based, latent-representation-based, and audio-token-based integration and discusses their application in speech-related tasks.", "result": "It highlights various methodologies and their implementation in speech-related applications.", "conclusion": "The survey identifies challenges in the field and aims to inspire future research in multimodal applications of LLMs.", "key_contributions": ["Categorizes methodologies for integrating speech with LLMs", "Explores applications in speech-related tasks", "Identifies key challenges for future research in the field"], "limitations": "", "keywords": ["large language models", "speech integration", "multimodal applications"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.21929", "pdf": "https://arxiv.org/pdf/2503.21929.pdf", "abs": "https://arxiv.org/abs/2503.21929", "title": "Local Normalization Distortion and the Thermodynamic Formalism of Decoding Strategies for Large Language Models", "authors": ["Tom Kempton", "Stuart Burrell"], "categories": ["cs.CL", "cs.LG", "math.DS"], "comment": null, "summary": "Advances in hardware and language model architecture have spurred a\nrevolution in natural language generation. However, autoregressive models\ncompute probability distributions over next-token choices, and sampling from\nthese distributions, known as decoding, has received significantly less\nattention than other design choices. Existing decoding strategies are largely\nbased on heuristics, resulting in methods that are difficult to apply or\nimprove in a principled manner. We develop the theory of decoding strategies\nfor language models by expressing popular decoding algorithms as equilibrium\nstates in the language of ergodic theory and stating the objective functions\nthey optimize. Using this, we analyze the effect of the local normalization\nstep required to make probabilities sum to one in top-k, nucleus, and\ntemperature sampling. We argue that local normalization distortion is a\nfundamental defect of decoding strategies and quantify the size of this\ndistortion and its effect on mathematical proxies for the quality and diversity\nof generated text. This yields conclusions for the design of decoding\nalgorithms and the detection of machine-generated text.", "AI": {"tldr": "This paper develops a theoretical framework for understanding and improving decoding strategies in autoregressive language models, revealing local normalization distortion as a key issue.", "motivation": "Decoding methods for language models are often heuristic-based and poorly understood, leading to difficulties in their application and improvement.", "method": "The authors use ergodic theory to express various decoding algorithms as equilibrium states, analyzing their objective functions and the impacts of local normalization on sampling methods.", "result": "The research quantifies the distortion caused by local normalization in prominent decoding strategies and assesses its influence on text generation quality and diversity.", "conclusion": "To enhance decoding strategies, it's crucial to address local normalization distortion, which affects the performance of language models.", "key_contributions": ["Development of a theoretical framework for decoding strategies in language models", "Identification and quantification of local normalization distortion", "Implications for improving decoding algorithms and detecting machine-generated text."], "limitations": "", "keywords": ["decoding strategies", "language models", "ergodic theory"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.01542", "pdf": "https://arxiv.org/pdf/2504.01542.pdf", "abs": "https://arxiv.org/abs/2504.01542", "title": "Register Always Matters: Analysis of LLM Pretraining Data Through the Lens of Language Variation", "authors": ["Amanda Myntti", "Erik Henriksson", "Veronika Laippala", "Sampo Pyysalo"], "categories": ["cs.CL"], "comment": null, "summary": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labelling systems,\ndatasets are divided into categories, frequently reducing to a binary: those\npassing the filters are deemed as valuable examples, others are discarded as\nuseless or detrimental. However, a more detailed understanding of the\ncontribution of different kinds of texts to model performance is still largely\nlacking. In this article, we present the first study utilising registers or\ngenres - a widely used standard in corpus linguistics to model linguistic\nvariation - to curate pretraining datasets and investigate the effect of\nregister on the performance of LLMs. We train small generative models with\nregister classified data and evaluate them using standard benchmarks, and show\nthat the register of pretraining data substantially affects model performance.\nWe uncover surprising relationships between the pretraining material and the\nresulting models: using the News register results in subpar performance, and on\nthe contrary, including the Opinion class, covering texts such as reviews and\nopinion blogs, is highly beneficial. While a model trained on the entire\nunfiltered dataset outperforms those trained on datasets limited to a single\nregister, combining well-performing registers like How-to-Instructions,\nInformational Description, and Opinion leads to major improvements.\nFurthermore, analysis of individual benchmark results reveals key differences\nin the strengths and drawbacks of specific register classes as pretraining\ndata. These findings show that register is an important explainer of model\nvariation and can facilitate more deliberate future data selection practices.", "AI": {"tldr": "This study explores the impact of different text registers on the performance of Large Language Models (LLMs) during pretraining, revealing that certain registers, like Opinion, enhance performance while others, like News, detract from it.", "motivation": "To better understand how different kinds of texts influence model performance and improve pretraining dataset curation for LLMs.", "method": "The authors utilize registers or genres from corpus linguistics to classify pretraining data and evaluate small generative models trained on this classified data using standard benchmarks.", "result": "Models trained with register-classified datasets demonstrate varying performance, with combinations of beneficial registers yielding major improvements, while models trained solely on News register underperform.", "conclusion": "Register significantly affects LLM performance, suggesting that more nuanced data selection strategies could enhance model training outcomes.", "key_contributions": ["First study applying registers to curate pretraining datasets for LLMs", "Demonstrated that some registers improve model performance while others hinder it", "Identified effective combinations of registers leading to improved outcomes."], "limitations": "", "keywords": ["Large Language Models", "pretraining data curation", "registers", "data selection", "model performance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.08776", "pdf": "https://arxiv.org/pdf/2504.08776.pdf", "abs": "https://arxiv.org/abs/2504.08776", "title": "SemCAFE: When Named Entities make the Difference Assessing Web Source Reliability through Entity-level Analytics", "authors": ["Gautam Kishore Shahi", "Oshani Seneviratne", "Marc Spaniol"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "With the shift from traditional to digital media, the online landscape now\nhosts not only reliable news articles but also a significant amount of\nunreliable content. Digital media has faster reachability by significantly\ninfluencing public opinion and advancing political agendas. While newspaper\nreaders may be familiar with their preferred outlets political leanings or\ncredibility, determining unreliable news articles is much more challenging. The\ncredibility of many online sources is often opaque, with AI generated content\nbeing easily disseminated at minimal cost. Unreliable news articles,\nparticularly those that followed the Russian invasion of Ukraine in 2022,\nclosely mimic the topics and writing styles of credible sources, making them\ndifficult to distinguish. To address this, we introduce SemCAFE, a system\ndesigned to detect news reliability by incorporating entity relatedness into\nits assessment. SemCAFE employs standard Natural Language Processing\ntechniques, such as boilerplate removal and tokenization, alongside entity\nlevel semantic analysis using the YAGO knowledge base. By creating a semantic\nfingerprint for each news article, SemCAFE could assess the credibility of\n46,020 reliable and 3,407 unreliable articles on the 2022 Russian invasion of\nUkraine. Our approach improved the macro F1 score by 12% over state of the art\nmethods. The sample data and code are available on GitHub", "AI": {"tldr": "Introduction of SemCAFE, a system to assess news reliability through entity relatedness.", "motivation": "The rise of unreliable news in digital media and the challenge of distinguishing it from credible sources.", "method": "SemCAFE uses NLP techniques, including boilerplate removal and tokenization, along with entity-level semantic analysis from the YAGO knowledge base, creating semantic fingerprints for articles.", "result": "Achieved a 12% improvement in macro F1 score over state-of-the-art methods by assessing the reliability of 46,020 reliable and 3,407 unreliable articles.", "conclusion": "SemCAFE effectively detects news reliability, addressing the challenges posed by misinformation in the digital age.", "key_contributions": ["Development of SemCAFE for news reliability assessment", "Utilization of entity relatedness in news evaluation", "Improvement of 12% in macro F1 score over existing methods"], "limitations": "", "keywords": ["news reliability", "NLP", "entity relatedness", "credibility assessment", "misinformation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.07899", "pdf": "https://arxiv.org/pdf/2506.07899.pdf", "abs": "https://arxiv.org/abs/2506.07899", "title": "MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs", "authors": ["Ke Wang", "Yiming Qin", "Nikolaos Dimitriadis", "Alessandro Favero", "Pascal Frossard"], "categories": ["cs.CL", "cs.LG"], "comment": "The first two authors contributed equally to this work", "summary": "Language models deployed in real-world systems often require post-hoc updates\nto incorporate new or corrected knowledge. However, editing such models\nefficiently and reliably-without retraining or forgetting previous\ninformation-remains a major challenge. Existing methods for lifelong model\nediting either compromise generalization, interfere with past edits, or fail to\nscale to long editing sequences. We propose MEMOIR, a novel scalable framework\nthat injects knowledge through a residual memory, i.e., a dedicated parameter\nmodule, while preserving the core capabilities of the pre-trained model. By\nsparsifying input activations through sample-dependent masks, MEMOIR confines\neach edit to a distinct subset of the memory parameters, minimizing\ninterference among edits. At inference, it identifies relevant edits by\ncomparing the sparse activation patterns of new queries to those stored during\nediting. This enables generalization to rephrased queries by activating only\nthe relevant knowledge while suppressing unnecessary memory activation for\nunrelated prompts. Experiments on question answering, hallucination correction,\nand out-of-distribution generalization benchmarks for LLaMA-3 and Mistral\nbackbones demonstrate that MEMOIR achieves state-of-the-art performance across\nreliability, generalization, and locality metrics, scaling to thousands of\nsequential edits with minimal forgetting.", "AI": {"tldr": "MEMOIR is a scalable framework for efficiently updating language models with new knowledge while preserving existing capabilities, minimizing interference among edits, and enhancing generalization.", "motivation": "Editing language models efficiently and reliably without retraining remains a significant challenge. Current methods either compromise generalization or interfere with previous edits.", "method": "MEMOIR introduces a residual memory to inject knowledge, using sample-dependent masks to sparsify input activations and confine each edit to a specific subset of memory parameters.", "result": "Experiments show that MEMOIR outperforms existing methods on multiple benchmarks for reliability, generalization, and locality, and scales effectively to numerous sequential edits.", "conclusion": "The proposed framework enables effective model updates without forgetting previous information, demonstrating state-of-the-art performance in various tasks.", "key_contributions": ["Introduction of MEMOIR framework for model editing", "Utilization of residual memory for knowledge injection", "Sample-dependent masks for minimizing edit interference"], "limitations": "", "keywords": ["language models", "model editing", "knowledge injection", "generalization", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
