{"id": "2505.21875", "pdf": "https://arxiv.org/pdf/2505.21875.pdf", "abs": "https://arxiv.org/abs/2505.21875", "title": "Broadening Our View: Assistive Technology for Cerebral Visual Impairment", "authors": ["Bhanuka Gamage", "Leona Holloway", "Nicola McDowell", "Thanh-Toan Do", "Nicholas Seow Chiang Price", "Arthur James Lowery", "Kim Marriott"], "categories": ["cs.HC"], "comment": "Author's accepted version of a LBW paper published in Extended\n  Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA\n  '24)", "summary": "Over the past decade, considerable research has been directed towards\nassistive technologies to support people with vision impairments using machine\nlearning, computer vision, image enhancement, and/or augmented/virtual reality.\nHowever, this has almost totally overlooked a growing demographic: people with\nCerebral Visual Impairment (CVI). Unlike Ocular Vision Impairments (OVI), CVI\narises from damage to the brain's visual processing centres. This paper\nintroduces CVI and reveals a wide research gap in addressing the needs of this\ndemographic. Through a scoping review, we identified 14 papers at the\nintersection of these technologies and CVI. Of these, only three papers\ndescribed assistive technologies focused on people living with CVI, with the\nothers focusing on diagnosis, understanding, simulation or rehabilitation. Our\nfindings highlight the opportunity for the Human-Computer Interaction and\nAssistive Technologies research community to explore and address this\nunderrepresented domain, thereby enhancing the quality of life for people with\nCVI.", "AI": {"tldr": "This paper explores the lack of assistive technologies for people with Cerebral Visual Impairment (CVI), highlighting a critical research gap in the field of Human-Computer Interaction.", "motivation": "To address the needs of people with Cerebral Visual Impairment (CVI), a demographic largely overlooked in existing assistive technology research.", "method": "A scoping review was conducted to identify relevant literature at the intersection of assistive technologies and CVI.", "result": "Identified 14 papers, of which only three discussed assistive technologies for CVI, indicating a significant gap in research.", "conclusion": "There is a pressing need for the HCI and Assistive Technologies community to focus on developing solutions for individuals with CVI to improve their quality of life.", "key_contributions": ["Identify the gap in research for CVI within assistive technologies.", "Highlight the limited focus on assistive technologies specifically designed for CVI.", "Encourage further exploration in the domain of HCI for CVI applications."], "limitations": "", "keywords": ["Cerebral Visual Impairment", "Assistive Technologies", "Human-Computer Interaction", "Machine Learning", "Computer Vision"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21891", "pdf": "https://arxiv.org/pdf/2505.21891.pdf", "abs": "https://arxiv.org/abs/2505.21891", "title": "TIEboard: A Digital Educational Tool for Kids Geometric Learning", "authors": ["Arooj Zaidi", "Giulia Barbareschi", "Kai Kunze", "Yun Suen Pai", "Junichi Yamaoka"], "categories": ["cs.HC"], "comment": null, "summary": "Tangible User Interfaces have shown potential in supporting the acquisition\nof key concepts in computing and mathematics while fostering engagement in\nyoung learners, but these approaches are less commonly utilised in the context\nof geometry. In this paper we introduce TIEboard, an interactive device to\npromote early learning of basic geometry concepts. TIEboard draws inspiration\nfrom traditional geoboards and lacing toys to leverage children's familiarity\nwith these traditional tools. It employs instructional lights to guide children\nin creating shapes using colourful threads of optical fiber. The use of\nconductive materials allows the system to detect lacing activity and provide\nfeedback in real-time. TIEboard incorporates six interaction modes of varying\ndifficulty based on an incremental learning framework. The study evaluated\nTIEboard's effectiveness in supporting early geometric learning, facilitating\ncreativity and promoting collaboration among 16 children aged 5-9.", "AI": {"tldr": "TIEboard is an interactive device designed to promote early geometry learning for children using familiar tools and real-time feedback.", "motivation": "To explore the potential of Tangible User Interfaces in enhancing early geometry learning among young learners.", "method": "TIEboard utilizes traditional tools' familiarity and incorporates instructional lights and conductive materials for shape creation and real-time feedback.", "result": "The study demonstrated TIEboard's effectiveness in supporting geometric learning, creativity, and collaboration among children aged 5-9.", "conclusion": "TIEboard successfully engages young learners in geometry through interactive and collaborative methods.", "key_contributions": ["Introduction of TIEboard for teaching geometry to children", "Incorporation of six interaction modes for different skill levels", "Real-time feedback mechanism for lacing activities"], "limitations": "", "keywords": ["Tangible User Interfaces", "geometry", "interactive learning", "children education", "early learning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.21964", "pdf": "https://arxiv.org/pdf/2505.21964.pdf", "abs": "https://arxiv.org/abs/2505.21964", "title": "UI-Evol: Automatic Knowledge Evolving for Computer Use Agents", "authors": ["Ziyun Zhang", "Xinyi Liu", "Xiaoyi Zhang", "Jun Wang", "Gang Chen", "Yan Lu"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "External knowledge has played a crucial role in the recent development of\ncomputer use agents. We identify a critical knowledge-execution gap: retrieved\nknowledge often fails to translate into effective real-world task execution.\nOur analysis shows even 90\\% correct knowledge yields only 41\\% execution\nsuccess rate. To bridge this gap, we propose UI-Evol, a plug-and-play module\nfor autonomous GUI knowledge evolution. UI-Evol consists of two stages: a\nRetrace Stage that extracts faithful objective action sequences from actual\nagent-environment interactions, and a Critique Stage that refines existing\nknowledge by comparing these sequences against external references. We conduct\ncomprehensive experiments on the OSWorld benchmark with the state-of-the-art\nAgent S2. Our results demonstrate that UI-Evol not only significantly boosts\ntask performance but also addresses a previously overlooked issue of high\nbehavioral standard deviation in computer use agents, leading to superior\nperformance on computer use tasks and substantially improved agent reliability.", "AI": {"tldr": "This paper introduces UI-Evol, a module designed to enhance the execution of computer use agents by bridging the knowledge-execution gap through a two-stage process: Retrace Stage for extracting action sequences and Critique Stage for knowledge refinement.", "motivation": "To address the critical knowledge-execution gap where even accurate knowledge fails to result in effective task execution in computer use agents.", "method": "The proposed UI-Evol module operates in two stages: the Retrace Stage extracts objective action sequences from agent-environment interactions, and the Critique Stage refines knowledge by comparing these sequences with external references.", "result": "UI-Evol significantly improves task performance and reduces behavioral standard deviation, leading to enhanced reliability in computer use tasks.", "conclusion": "The module successfully bridges the knowledge-execution gap and improves the reliability and performance of computer use agents.", "key_contributions": ["Development of a plug-and-play module (UI-Evol) for GUI knowledge evolution", "Introduction of Retrace and Critique stages for knowledge refinement", "Demonstrated significant performance enhancements on the OSWorld benchmark"], "limitations": "", "keywords": ["knowledge-execution gap", "computer use agents", "UI-Evol"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966.pdf", "abs": "https://arxiv.org/abs/2505.21966", "title": "MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "16 pages and 15 figures", "summary": "We introduce MapStory, an LLM-powered animation authoring tool that generates\neditable map animation sequences directly from natural language text. Given a\nuser-written script, MapStory leverages an agentic architecture to\nautomatically produce a scene breakdown, which decomposes the script into key\nanimation building blocks such as camera movements, visual highlights, and\nanimated elements. Our system includes a researcher component that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nthe automatic extraction of relevant regions, paths, and coordinates while\nallowing users to edit and query for changes or additional information to\nrefine the results. Additionally, users can fine-tune parameters of these\nblocks through an interactive timeline editor. We detail the system's design\nand architecture, informed by formative interviews with professional animators\nand an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories.", "AI": {"tldr": "MapStory is an LLM-powered tool that automates the creation of editable map animations from natural language scripts, enhancing ease of use and creativity.", "motivation": "To simplify the creation of map animations, enabling users to generate content from natural language while minimizing technical barriers.", "method": "MapStory utilizes an agentic architecture to break down user scripts into animation components and integrates a researcher component to query geospatial data using an LLM and web search.", "result": "The system allows for rapid creation and refinement of map animations, confirmed by evaluations showing enhanced user experience in creation and iteration.", "conclusion": "MapStory significantly aids in the animation process, allowing for more creativity and faster production of map-centric stories.", "key_contributions": ["Development of an LLM-powered animation authoring tool for map animations", "Integration of automatic geospatial data querying", "User-friendly interactive editing through a timeline interface"], "limitations": "Limited to specific types of animations and dependent on the accuracy of geospatial queries.", "keywords": ["LLM", "map animation", "human-computer interaction", "animation authoring", "natural language processing"], "importance_score": 7, "read_time_minutes": 16}}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523.pdf", "abs": "https://arxiv.org/abs/2505.21523", "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity.", "AI": {"tldr": "The paper investigates the trade-off between reasoning ability and hallucination in multimodal large language models, introducing a new metric and benchmark to evaluate visual grounding during reasoning.", "motivation": "To address the issue of hallucination in longer reasoning chains generated by multimodal large language models, and to evaluate how well these models retain visual grounding during complex reasoning tasks.", "method": "The paper introduces a metric called RH-AUC to quantify changes in perception accuracy with reasoning length and presents RH-Bench, a diagnostic benchmark designed for various multimodal tasks.", "result": "The analysis shows that larger models achieve a better balance between reasoning quality and perception fidelity, influenced more by the type of training data than by its volume.", "conclusion": "The findings highlight the need for evaluation frameworks that consider both reasoning and perceptual fidelity to mitigate hallucination in multimodal models.", "key_contributions": ["Introduction of RH-AUC metric for evaluating reasoning length impact on perception accuracy.", "Release of RH-Bench, a benchmark for assessing multimodal tasks and their trade-offs.", "Analysis of how model size and training data type influence reasoning and perception balance."], "limitations": "", "keywords": ["multimodal", "large language models", "hallucination", "reasoning chains", "visual grounding"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21982", "pdf": "https://arxiv.org/pdf/2505.21982.pdf", "abs": "https://arxiv.org/abs/2505.21982", "title": "Eye-Tracking and Biometric Feedback in UX Research: Measuring User Engagement and Cognitive Load", "authors": ["Aaditya Shankar Majumder"], "categories": ["cs.HC"], "comment": "4 pages", "summary": "User experience research often uses surveys and interviews, which may miss\nsubconscious user interactions. This study explores eye-tracking and biometric\nfeedback as tools to assess user engagement and cognitive load in digital\ninterfaces. These methods measure gaze behavior and bodily responses, providing\nan objective complement to qualitative insights. Using empirical evidence,\npractical applications, and advancements from 2023-2025, we present\nexperimental data, describe our methodology, and place our work within\nfoundational and recent literature. We address challenges like data\ninterpretation, ethical issues, and technological integration. These tools are\nkey for advancing UX design in complex digital environments.", "AI": {"tldr": "This study examines eye-tracking and biometric feedback as methods to enhance user experience research by providing objective measures of user engagement and cognitive load in digital interfaces.", "motivation": "The motivation behind this research is to improve the understanding of user interactions which are often overlooked when relying solely on qualitative methods like surveys and interviews.", "method": "The paper employs eye-tracking and biometric feedback techniques to quantify user engagement and cognitive load, providing an empirical basis for evaluating user interactions in digital settings.", "result": "The results show how gaze behavior and bodily responses can offer valuable insights into user engagement, highlighting the importance of objective measures in UX research.", "conclusion": "The study concludes that integrating eye-tracking and biometric feedback into UX evaluation methods can significantly enhance the design and effectiveness of digital interfaces.", "key_contributions": ["Introduction of eye-tracking and biometric methods to UX research.", "Empirical evidence supporting the effectiveness of these methods.", "Discussion of challenges and ethical implications of using biometric data."], "limitations": "The paper discusses limitations related to data interpretation, ethical issues regarding user privacy, and the complexity of integrating these technologies into existing UX research practices.", "keywords": ["user experience", "eye-tracking", "biometric feedback", "cognitive load", "digital interfaces"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21578", "pdf": "https://arxiv.org/pdf/2505.21578.pdf", "abs": "https://arxiv.org/abs/2505.21578", "title": "Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use", "authors": ["Titouan Parcollet", "Yuan Tseng", "Shucong Zhang", "Rogier van Dalen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Automatic speech recognition (ASR) research is driven by the availability of\ncommon datasets between industrial researchers and academics, encouraging\ncomparisons and evaluations. LibriSpeech, despite its long success as an ASR\nbenchmark, is now limited by its size and focus on clean, read speech, leading\nto near-zero word error rates. More recent datasets, including MOSEL, YODAS,\nGigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations\nincluding licenses that researchers in the industry cannot use, unreliable\ntranscriptions, incorrect audio data, or the lack of evaluation sets. This work\npresents the Loquacious Set, a 25,000-hour curated collection of commercially\nusable English speech. Featuring hundreds of thousands of speakers with diverse\naccents and a wide range of speech types (read, spontaneous, talks, clean,\nnoisy), the Loquacious Set is designed to work for academics and researchers in\nthe industry to build ASR systems in real-world scenarios.", "AI": {"tldr": "The Loquacious Set is a new 25,000-hour speech dataset aimed at improving automatic speech recognition (ASR) by addressing the limitations of existing datasets.", "motivation": "Current ASR datasets are insufficient for real-world applications due to limitations in size, diversity, and commercial usability.", "method": "The authors present the Loquacious Set, which is a curated collection of diverse English speech, including various speakers and speech types, totaling 25,000 hours of audio.", "result": "The Loquacious Set offers a resource for building ASR systems that can handle different accents and noisy environments, significantly enhancing ASR research.", "conclusion": "This dataset is intended to facilitate better comparisons and evaluations for ASR systems in both academia and industry.", "key_contributions": ["Introduction of the Loquacious Set dataset", "Diverse collection of 25,000 hours of English speech", "Commercially usable resources for industry researchers"], "limitations": "", "keywords": ["automatic speech recognition", "dataset", "Loquacious Set", "speech diversity", "industry research"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.22303", "pdf": "https://arxiv.org/pdf/2505.22303.pdf", "abs": "https://arxiv.org/abs/2505.22303", "title": "Voice CMS: updating the knowledge base of a digital assistant through conversation", "authors": ["Grzegorz Wolny", "Michał Szczerbak"], "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": null, "summary": "In this study, we propose a solution based on a multi-agent LLM architecture\nand a voice user interface (VUI) designed to update the knowledge base of a\ndigital assistant. Its usability is evaluated in comparison to a more\ntraditional graphical content management system (CMS), with a focus on\nunderstanding the relationship between user preferences and the complexity of\nthe information being provided. The findings demonstrate that, while the\noverall usability of the VUI is rated lower than the graphical interface, it is\nalready preferred by users for less complex tasks. Furthermore, the quality of\ncontent entered through the VUI is comparable to that achieved with the\ngraphical interface, even for highly complex tasks. Obtained qualitative\nresults suggest that a hybrid interface combining the strengths of both\napproaches could address the key challenges identified during the experiment,\nsuch as reducing cognitive load through graphical feedback while maintaining\nthe intuitive nature of voice-based interactions. This work highlights the\npotential of conversational interfaces as a viable and effective method for\nknowledge management in specific business contexts.", "AI": {"tldr": "This study evaluates a multi-agent LLM architecture with a voice user interface (VUI) versus a traditional graphical content management system, focusing on usability and task complexity.", "motivation": "To explore the usability of a VUI in updating digital assistant knowledge bases compared to traditional graphical systems.", "method": "A usability evaluation comparing a multi-agent LLM architecture with a voice user interface against a graphical content management system, focusing on user preferences and task complexity.", "result": "While the VUI was rated lower in overall usability, it was preferred for less complex tasks, with content quality comparable to the graphical interface even in complex tasks.", "conclusion": "A hybrid interface combining VUI and graphical feedback could reduce cognitive load and improve user experience in knowledge management tasks.", "key_contributions": ["Proposes a multi-agent LLM architecture with VUI for digital assistants.", "Demonstrates user preference for VUI in less complex tasks.", "Highlights the potential of hybrid interfaces for effective knowledge management."], "limitations": "Overall usability of the VUI is still rated lower than the graphical interface for complex tasks.", "keywords": ["voice user interface", "multi-agent LLM", "knowledge management", "usability evaluation", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21598", "pdf": "https://arxiv.org/pdf/2505.21598.pdf", "abs": "https://arxiv.org/abs/2505.21598", "title": "Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives", "authors": ["Yajiao Liu", "Congliang Chen", "Junchi Yang", "Ruoyu Sun"], "categories": ["cs.CL"], "comment": "The first version of this paper was submitted to ACL ARR 2025\n  February Submission", "summary": "Training large language models with data collected from various domains can\nimprove their performance on downstream tasks. However, given a fixed training\nbudget, the sampling proportions of these different domains significantly\nimpact the model's performance. How can we determine the domain weights across\ndifferent data domains to train the best-performing model within constrained\ncomputational resources? In this paper, we provide a comprehensive overview of\nexisting data mixture methods. First, we propose a fine-grained categorization\nof existing methods, extending beyond the previous offline and online\nclassification. Offline methods are further grouped into heuristic-based,\nalgorithm-based, and function fitting-based methods. For online methods, we\ncategorize them into three groups: online min-max optimization, online mixing\nlaw, and other approaches by drawing connections with the optimization\nframeworks underlying offline methods. Second, we summarize the problem\nformulations, representative algorithms for each subtype of offline and online\nmethods, and clarify the relationships and distinctions among them. Finally, we\ndiscuss the advantages and disadvantages of each method and highlight key\nchallenges in the field of data mixture.", "AI": {"tldr": "This paper discusses the impact of domain sampling proportions on the performance of large language models and categorizes existing data mixture methods to optimize model training within fixed resource constraints.", "motivation": "To improve performance on downstream tasks by determining optimal domain weights for training large language models using data from various domains.", "method": "The paper categorizes existing data mixture methods into offline and online approaches, further detailing subcategories such as heuristic-based and online min-max optimization methods, and outlines the problem formulations and algorithms for each subtype.", "result": "A comprehensive overview of data mixture methods is provided, along with categorization, advantages and disadvantages, and key challenges faced in optimizing domain sampling for model training.", "conclusion": "Identifying the best strategies for domain weight assignment in data mixtures is crucial for maximizing the performance of large language models under defined computational limits.", "key_contributions": ["Fine-grained categorization of data mixture methods.", "Summary of problem formulations and representative algorithms.", "Discussion of advantages, disadvantages, and challenges in the data mixture field."], "limitations": "", "keywords": ["large language models", "data mixture methods", "domain sampling"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.22414", "pdf": "https://arxiv.org/pdf/2505.22414.pdf", "abs": "https://arxiv.org/abs/2505.22414", "title": "ToPSen: Task-Oriented Priming and Sensory Alignment for Comparing Coding Strategies Between Sighted and Blind Programmers", "authors": ["Md Ehtesham-Ul-Haque", "Syed Masum Billah"], "categories": ["cs.HC"], "comment": "Accepted at DIS'25", "summary": "This paper examines how the coding strategies of sighted and blind\nprogrammers differ when working with audio feedback alone. The goal is to\nidentify challenges in mixed-ability collaboration, particularly when sighted\nprogrammers work with blind peers or teach programming to blind students. To\novercome limitations of traditional blindness simulation studies, we proposed\nTask-Oriented Priming and Sensory Alignment (ToPSen), a design framework that\nreframes sensory constraints as technical requirements rather than as a\ndisability. Through a study of 12 blind and 12 sighted participants coding\nnon-visually, we found that expert blind programmers maintain more accurate\nmental models and process more information in working memory than sighted\nprogrammers using ToPSen. Our analysis revealed that blind and sighted\nprogrammers process structural information differently, exposing gaps in\ncurrent IDE designs. These insights inform our guidelines for improving the\naccessibility of programming tools and fostering effective mixed-ability\ncollaboration.", "AI": {"tldr": "This paper studies the coding strategies of sighted vs. blind programmers using audio feedback to identify collaboration challenges and improve accessibility.", "motivation": "The paper aims to understand the challenges faced in mixed-ability collaboration between sighted and blind programmers, particularly in teaching and working together.", "method": "A design framework called Task-Oriented Priming and Sensory Alignment (ToPSen) was proposed and studied with 12 blind and 12 sighted participants coding non-visually.", "result": "Expert blind programmers maintain more accurate mental models and process more information in working memory than sighted programmers using ToPSen.", "conclusion": "Insights from this study can inform guidelines for improving the accessibility of programming tools and enable better mixed-ability collaboration.", "key_contributions": ["Proposed framework (ToPSen) for improving coding accessibility and collaboration", "Revealed differences in information processing between sighted and blind programmers", "Identified gaps in current IDE designs that hinder collaboration."], "limitations": "", "keywords": ["Human-Computer Interaction", "Accessibility", "Programming", "Blindness", "Collaboration"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21600", "pdf": "https://arxiv.org/pdf/2505.21600.pdf", "abs": "https://arxiv.org/abs/2505.21600", "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "authors": ["Tianyu Fu", "Yi Ge", "Yichen You", "Enshu Liu", "Zhihang Yuan", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.", "AI": {"tldr": "Roads to Rome (R2R) is a method that uses Large Language Models (LLMs) for critical tokens while employing Small Language Models (SLMs) for most token generation, significantly improving performance and efficiency in reasoning tasks.", "motivation": "To address the high inference costs of LLMs while maintaining performance through more efficient models like SLMs.", "method": "The R2R method identifies critical, path-divergent tokens where LLMs excel and uses them selectively, while a majority of tokens are generated using an SLM. An automatic data generation pipeline is developed to label these divergent tokens for training.", "result": "R2R achieved 1.6x higher average accuracy than R1-7B and outperformed R1-14B, while offering a 2.8x speedup compared to R1-32B with comparable performance.", "conclusion": "R2R improves efficiency and accuracy in ML tasks by optimizing how reasoning paths are handled between LLMs and SLMs.", "key_contributions": ["Introduction of Roads to Rome (R2R) methodology", "Improved accuracy with a smaller parameter size", "Development of an automatic data generation pipeline for token routing"], "limitations": "", "keywords": ["Large Language Models", "Small Language Models", "token routing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22418", "pdf": "https://arxiv.org/pdf/2505.22418.pdf", "abs": "https://arxiv.org/abs/2505.22418", "title": "AI Trust Reshaping Administrative Burdens: Understanding Trust-Burden Dynamics in LLM-Assisted Benefits Systems", "authors": ["Jeongwon Jo", "He Zhang", "Jie Cai", "Nitesh Goyal"], "categories": ["cs.HC"], "comment": "FAccT 2025", "summary": "Supplemental Nutrition Assistance Program (SNAP) is an essential benefit\nsupport system provided by the US administration to 41 million federally\ndetermined low-income applicants. Through interviews with such applicants\nacross a diverse set of experiences with the SNAP system, our findings reveal\nthat new AI technologies like LLMs can alleviate traditional burdens but also\nintroduce new burdens. We introduce new types of learning, compliance, and\npsychological costs that transform the administrative burden on applicants. We\nalso identify how trust in AI across three dimensions--competence, integrity,\nand benevolence--is perceived to reduce administrative burdens, which may stem\nfrom unintended and untoward overt trust in the system. We discuss calibrating\nappropriate levels of user trust in LLM-based administrative systems,\nmitigating newly introduced burdens. In particular, our findings suggest that\nevidence-based information disclosure is necessary in benefits administration\nand propose directions for future research on trust-burden dynamics in\nAI-assisted administration systems.", "AI": {"tldr": "The paper examines how AI technologies, specifically LLMs, impact the administrative burden on SNAP applicants, introducing new educational and psychological costs while also alleviating some traditional burdens.", "motivation": "To understand how AI technologies can change the dynamic of administrative burdens faced by low-income applicants in the SNAP program.", "method": "Interviews with SNAP applicants to evaluate their experiences and perceptions of AI technologies in the benefits administration system.", "result": "Findings reveal AI can alleviate traditional burdens but may introduce new types of learning, compliance, and psychological costs, while trust in AI can reduce these burdens across competence, integrity, and benevolence dimensions.", "conclusion": "Evidence-based information disclosure is critical for effective AI-assisted benefits administration, and further research on trust-burden dynamics is necessary.", "key_contributions": ["Introduction of new types of costs associated with AI in benefits administration", "Identification of trust dimensions affecting administrative burdens", "Proposals for evidence-based disclosure to improve applicant experience"], "limitations": "Study focused on a specific benefit program (SNAP) and may not generalize to other contexts or systems.", "keywords": ["AI", "SNAP", "trust", "administrative burden", "LLM"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21608", "pdf": "https://arxiv.org/pdf/2505.21608.pdf", "abs": "https://arxiv.org/abs/2505.21608", "title": "How does Misinformation Affect Large Language Model Behaviors and Preferences?", "authors": ["Miao Peng", "Nuo Chen", "Jianheng Tang", "Jia Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nknowledge-intensive tasks, while they remain vulnerable when encountering\nmisinformation. Existing studies have explored the role of LLMs in combating\nmisinformation, but there is still a lack of fine-grained analysis on the\nspecific aspects and extent to which LLMs are influenced by misinformation. To\nbridge this gap, we present MisBench, the current largest and most\ncomprehensive benchmark for evaluating LLMs' behavior and knowledge preference\ntoward misinformation. MisBench consists of 10,346,712 pieces of\nmisinformation, which uniquely considers both knowledge-based conflicts and\nstylistic variations in misinformation. Empirical results reveal that while\nLLMs demonstrate comparable abilities in discerning misinformation, they still\nremain susceptible to knowledge conflicts and stylistic variations. Based on\nthese findings, we further propose a novel approach called Reconstruct to\nDiscriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our\nstudy provides valuable insights into LLMs' interactions with misinformation,\nand we believe MisBench can serve as an effective benchmark for evaluating\nLLM-based detectors and enhancing their reliability in real-world applications.\nCodes and data are available at https://github.com/GKNL/MisBench.", "AI": {"tldr": "MisBench is a comprehensive benchmark for evaluating LLMs' behavior and knowledge preference toward misinformation, consisting of over 10 million examples of misinformation.", "motivation": "To analyze the influence of misinformation on Large Language Models (LLMs) and improve their ability to detect it.", "method": "Development of MisBench, the largest benchmark to evaluate LLMs with 10,346,712 examples of misinformation, considering knowledge conflicts and stylistic variations.", "result": "Empirical results show that while LLMs can discern misinformation, they are still affected by knowledge conflicts and stylistic variations.", "conclusion": "The study proposes the Reconstruct to Discriminate (RtD) approach to enhance LLMs' capabilities in detecting misinformation, offering insights and a benchmark for real-world applications.", "key_contributions": ["Introduction of MisBench, a comprehensive misinformation benchmark for LLMs", "Empirical analysis of LLMs' susceptibility to misinformation", "Proposal of the RtD approach to improve misinformation detection capabilities."], "limitations": "The benchmark may not cover all variations of misinformation and its effectiveness in diverse real-world scenarios is yet to be validated.", "keywords": ["Large Language Models", "misinformation", "benchmark", "knowledge conflicts", "Reconstruct to Discriminate"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.22428", "pdf": "https://arxiv.org/pdf/2505.22428.pdf", "abs": "https://arxiv.org/abs/2505.22428", "title": "Parental Collaboration and Closeness: Envisioning with New Couple Parents", "authors": ["Ya-Fang Lin", "Xiaotian Li", "Wan-Hsuan Huang", "Charan Pushpanathan Prabavathi", "Jie Cai", "John M. Carroll"], "categories": ["cs.HC", "cs.CY"], "comment": "DIS 2025", "summary": "Couples often experience a decrease in closeness as they cope with the\ndemands of parenthood. Existing technologies have supported parenting and\nparental collaboration. However, these technologies do not adequately support\ncloseness in co-parenting. We use scenarios and design probes to brainstorm\nwith 10 new parent couples to explore and envision possibilities for\ntechnologies to support closeness. We reported parents' current technology use\nfor co-parenting and how participants considered and envisioned co-parenting\ntechnology for closeness, including information and task sharing, emotion\nawareness and disclosure, and fostering fun interaction. We discuss the\npotential technology has for fostering closeness in co-parenting by (1)\nfostering interdependence by supporting parental competence and (2) integrating\npositive emotions and experiences, such as validation and fun, in parenting.\nBased on our findings, we expand the design space of technology for closeness\nto include interdependence. We also expand the design space for co-parenting\ntechnology by integrating more positive emotions.", "AI": {"tldr": "This paper explores how technology can support closeness in co-parenting among new couples, highlighting the need for emotional connection alongside practical support.", "motivation": "To address the gap in existing technologies that fail to adequately support relational closeness in co-parenting arrangements among couples.", "method": "The study utilized scenarios and design probes with 10 new parent couples to brainstorm technology solutions for fostering closeness in co-parenting.", "result": "Participants identified the importance of emotional awareness, task sharing, and fostering fun interactions, leading to an expanded design space for co-parenting technologies.", "conclusion": "Technologies have the potential to enhance closeness in co-parenting by supporting interdependence and integrating positive emotional interactions.", "key_contributions": ["Exploration of emotional aspects in co-parenting technology design", "Identification of tasks and emotional support mechanisms", "Expansion of co-parenting technology design space to include fun and validation."], "limitations": "", "keywords": ["co-parenting", "closeness", "technology", "emotional support", "parenting"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2505.21646", "pdf": "https://arxiv.org/pdf/2505.21646.pdf", "abs": "https://arxiv.org/abs/2505.21646", "title": "Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts", "authors": ["Lei Zhang", "Markus Stricker"], "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": "13 pages, 5 figures, 2 tables, accepted at ECMLPKDD 2025", "summary": "The discovery and optimization of materials for specific applications is\nhampered by the practically infinite number of possible elemental combinations\nand associated properties, also known as the `combinatorial explosion'. By\nnature of the problem, data are scarce and all possible data sources should be\nused. In addition to simulations and experimental results, the latent knowledge\nin scientific texts is not yet used to its full potential. We present an\niterative framework that refines a given scientific corpus by strategic\nselection of the most diverse documents, training Word2Vec models, and\nmonitoring the convergence of composition-property correlations in embedding\nspace. Our approach is applied to predict high-performing materials for oxygen\nreduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions\nfor a large number of possible candidate compositions. Our method successfully\npredicts the highest performing compositions among a large pool of candidates,\nvalidated by experimental measurements of the electrocatalytic performance in\nthe lab. This work demonstrates and validates the potential of iterative corpus\nrefinement to accelerate materials discovery and optimization, offering a\nscalable and efficient tool for screening large compositional spaces where\nreliable data are scarce or non-existent.", "AI": {"tldr": "An iterative framework that refines scientific corpora and predicts high-performing materials for electrocatalytic reactions using Word2Vec models.", "motivation": "Address the combinatorial explosion in material discovery and optimize candidate selection using diverse data sources including scientific texts.", "method": "Iterative framework involving strategic document selection, training of Word2Vec models, and monitoring composition-property correlations in embedding space.", "result": "Successfully predicts the highest performing compositions for electrocatalytic reactions (ORR, HER, OER) validated by lab experiments.", "conclusion": "The proposed method demonstrates the potential for accelerating materials discovery and optimization in areas with scarce data.", "key_contributions": ["Iterative corpus refinement for material discovery", "Integration of Word2Vec models with scientific texts", "Successful prediction of high-performing electrocatalytic materials"], "limitations": "", "keywords": ["materials discovery", "machine learning", "Word2Vec", "corpus refinement", "electrocatalysis"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.22477", "pdf": "https://arxiv.org/pdf/2505.22477.pdf", "abs": "https://arxiv.org/abs/2505.22477", "title": "Human-Centered Human-AI Collaboration (HCHAC)", "authors": ["Qi Gao", "Wei Xu", "Hanxi Pan", "Mowei Shen", "Zaifeng Gao"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "This article is a chapter from the upcoming book Handbook of\n  Human-Centered Artificial Intelligence", "summary": "In the intelligent era, the interaction between humans and intelligent\nsystems fundamentally involves collaboration with autonomous intelligent\nagents. Human-AI Collaboration (HAC) represents a novel type of human-machine\nrelationship facilitated by autonomous intelligent machines equipped with AI\ntechnologies. In this paradigm, AI agents serve not only as auxiliary tools but\nalso as active teammates, partnering with humans to accomplish tasks\ncollaboratively. Human-centered AI (HCAI) emphasizes that humans play critical\nleadership roles in the collaboration. This human-led collaboration imparts new\ndimensions to the human-machine relationship, necessitating innovative research\nperspectives, paradigms, and agenda to address the unique challenges posed by\nHAC. This chapter delves into the essence of HAC from the human-centered\nperspective, outlining its core concepts and distinguishing features. It\nreviews the current research methodologies and research agenda within the HAC\nfield from the HCAI perspective, highlighting advancements and ongoing studies.\nFurthermore, a framework for human-centered HAC (HCHAC) is proposed by\nintegrating these reviews and analyses. A case study of HAC in the context of\nautonomous vehicles is provided, illustrating practical applications and the\nsynergistic interactions between humans and AI agents. Finally, it identifies\npotential future research directions aimed at enhancing the effectiveness,\nreliability, and ethical integration of human-centered HAC systems in diverse\ndomains.", "AI": {"tldr": "This chapter explores Human-AI Collaboration (HAC), emphasizing a human-centered approach, and proposes a framework for effective collaboration between humans and intelligent agents.", "motivation": "To understand and improve the collaboration between humans and AI agents as partners in task accomplishment, highlighting the need for a human-centered approach.", "method": "The chapter reviews current research methodologies in Human-AI Collaboration and proposes a human-centered framework (HCHAC), supported by a case study on autonomous vehicles.", "result": "The analysis reveals advancements in HAC and proposes a new framework facilitating effective human-agent collaboration, contributing to research in Human-Centered Artificial Intelligence (HCAI).", "conclusion": "Future research directions are identified to enhance the reliability and ethical integration of human-centered HAC systems across various domains.", "key_contributions": ["Proposes a human-centered framework for Human-AI Collaboration (HCHAC).", "Reviews existing methodologies and research agendas in HAC.", "Illustrates practical applications through a case study in autonomous vehicles."], "limitations": "", "keywords": ["Human-AI Collaboration", "Human-Centered AI", "autonomous intelligent agents", "framework", "autonomous vehicles"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657.pdf", "abs": "https://arxiv.org/abs/2505.21657", "title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "authors": ["Zeinab Dehghani", "Koorosh Aslansefat", "Adil Khan", "Mohammed Naveed Akram"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.16277", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.", "AI": {"tldr": "SMILE is a model-agnostic method introduced to explain LLM responses by analyzing input changes and generating visual heat maps for transparency in AI.", "motivation": "To enhance transparency and trust in large language models by understanding their response mechanisms, especially in sensitive fields where accountability is crucial.", "method": "SMILE analyzes how output varies with slight modifications to the input, identifying impactful words, and generating visual heat maps to illustrate results.", "result": "SMILE was tested on several leading LLMs, demonstrating improvements in clarity and reliability of explanations through various metrics.", "conclusion": "SMILE contributes to making AI models more interpretable, fostering transparency and trust in AI applications.", "key_contributions": ["Introduction of a model-agnostic explanation method for LLMs", "Establishment of clear evaluation metrics for explanation quality", "Visualization of significant input components through heat maps"], "limitations": "", "keywords": ["Large Language Models", "Interpretability", "Transparency", "AI Trustworthiness"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.22539", "pdf": "https://arxiv.org/pdf/2505.22539.pdf", "abs": "https://arxiv.org/abs/2505.22539", "title": "Spot-On: A Mixed Reality Interface for Multi-Robot Cooperation", "authors": ["Tim Engelbracht", "Petar Lukovic", "Tjark Behrens", "Kai Lascheit", "René Zurbrügg", "Marc Pollefeys", "Hermann Blum", "Zuria Bauer"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Recent progress in mixed reality (MR) and robotics is enabling increasingly\nsophisticated forms of human-robot collaboration. Building on these\ndevelopments, we introduce a novel MR framework that allows multiple quadruped\nrobots to operate in semantically diverse environments via a MR interface. Our\nsystem supports collaborative tasks involving drawers, swing doors, and\nhigher-level infrastructure such as light switches. A comprehensive user study\nverifies both the design and usability of our app, with participants giving a\n\"good\" or \"very good\" rating in almost all cases. Overall, our approach\nprovides an effective and intuitive framework for MR-based multi-robot\ncollaboration in complex, real-world scenarios.", "AI": {"tldr": "The paper presents a mixed reality framework for multi-robot collaboration, focusing on quadruped robots in diverse environments.", "motivation": "To improve human-robot collaboration in complex environments using mixed reality and robotics advancements.", "method": "Developed a mixed reality interface for controlling multiple quadruped robots in collaborative tasks involving everyday objects.", "result": "User study shows high usability ratings, indicating effectiveness of the MR framework in facilitating multi-robot collaboration.", "conclusion": "The proposed MR framework enhances interaction with quadruped robots, providing an intuitive platform for collaborative tasks.", "key_contributions": ["Novel mixed reality framework for multi-robot collaboration", "User study demonstrating high usability", "Support for complex real-world tasks through the MR interface"], "limitations": "", "keywords": ["Mixed Reality", "Human-Robot Collaboration", "Robotics", "User Study", "Quadruped Robots"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.21670", "pdf": "https://arxiv.org/pdf/2505.21670.pdf", "abs": "https://arxiv.org/abs/2505.21670", "title": "Rethinking the Outlier Distribution in Large Language Models: An In-depth Study", "authors": ["Rahul Raman", "Khushi Sharma", "Sai Qian Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Investigating outliers in large language models (LLMs) is crucial due to\ntheir significant impact on various aspects of LLM performance, including\nquantization and compression. Outliers often cause considerable quantization\nerrors, leading to degraded model performance. Identifying and addressing these\noutliers can enhance the accuracy and efficiency of the quantization process,\nenabling smoother deployment on edge devices or specialized hardware. Recent\nstudies have identified two common types of outliers in LLMs: massive\nactivations and channel-wise outliers. While numerous quantization algorithms\nhave been proposed to mitigate their effects and maintain satisfactory\naccuracy, few have thoroughly explored the root causes of these outliers in\ndepth. In this paper, we conduct a comprehensive investigation into the\nformation mechanisms of these outliers and propose potential strategies to\nmitigate their occurrence. Ultimately, we introduce some efficient approaches\nto eliminate most massive activations and channel-wise outliers with minimal\nimpact on accuracy.", "AI": {"tldr": "This paper investigates outliers in large language models (LLMs) that affect performance due to quantization errors and proposes strategies for mitigation.", "motivation": "Identifying and addressing outliers in LLMs is essential for improving quantization, which directly influences model accuracy and efficiency, especially for deployment on edge devices.", "method": "The paper conducts a comprehensive investigation into the formation mechanisms of outliers in LLMs and proposes strategies to mitigate their occurrence, focusing on massive activations and channel-wise outliers.", "result": "The proposed strategies effectively eliminate most massive activations and channel-wise outliers with minimal impact on model accuracy.", "conclusion": "Mitigating outliers in LLMs can significantly enhance quantization outcomes, facilitating better deployment on specialized hardware.", "key_contributions": ["In-depth analysis of the formation mechanisms of outliers in LLMs.", "Proposed strategies that effectively reduce the occurrence of outliers.", "Practical approaches for quantization with minimized accuracy loss."], "limitations": "", "keywords": ["large language models", "outliers", "quantization", "model performance", "activation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21689", "pdf": "https://arxiv.org/pdf/2505.21689.pdf", "abs": "https://arxiv.org/abs/2505.21689", "title": "LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model", "authors": ["Avijit Gayen", "Somyajit Chakraborty", "Mainak Sen", "Soham Paul", "Angshuman Jana"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 5 figures, journal paper, submitted to AI and Law", "summary": "The persistent accumulation of unresolved legal cases, especially within the\nIndian judiciary, significantly hampers the timely delivery of justice. Manual\nmethods of prioritizing petitions are often prone to inefficiencies and\nsubjective biases further exacerbating delays. To address this issue, we\npropose LLMPR (Large Language Model-based Petition Ranking), an automated\nframework that utilizes transfer learning and machine learning to assign\npriority rankings to legal petitions based on their contextual urgency.\nLeveraging the ILDC dataset comprising 7,593 annotated petitions, we process\nunstructured legal text and extract features through various embedding\ntechniques, including DistilBERT, LegalBERT, and MiniLM. These textual\nembeddings are combined with quantitative indicators such as gap days, rank\nscores, and word counts to train multiple machine learning models, including\nRandom Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments\ndemonstrate that Random Forest and Decision Tree models yield superior\nperformance, with accuracy exceeding 99% and a Spearman rank correlation of\n0.99. Notably, models using only numerical features achieve nearly optimal\nranking results (R2 = 0.988, \\r{ho} = 0.998), while LLM-based embeddings offer\nonly marginal gains. These findings suggest that automated petition ranking can\neffectively streamline judicial workflows, reduce case backlog, and improve\nfairness in legal prioritization.", "AI": {"tldr": "The paper proposes LLMPR, an automated framework for ranking legal petitions using LLMs and machine learning to improve judicial efficiency and fairness.", "motivation": "To address inefficiencies and biases in manual prioritization of legal petitions in the Indian judiciary, leading to delays in justice delivery.", "method": "The framework uses transfer learning and machine learning techniques, processing unstructured legal text from the ILDC dataset, extracting features through embeddings like DistilBERT and legal features for training models such as Random Forest and XGBoost.", "result": "The experiments show that Random Forest and Decision Tree models achieve over 99% accuracy and a Spearman correlation of 0.99, with numerical features alone yielding optimal results.", "conclusion": "LLMPR can enhance judicial workflows, effectively reduce case backlogs, and promote fairness in the prioritization of legal petitions.", "key_contributions": ["Introduction of LLMPR for automated legal petition ranking", "Demonstrated high accuracy using machine learning models", "Highlighting the marginal gains of LLM embeddings compared to numerical features"], "limitations": "", "keywords": ["Legal Petition Ranking", "Machine Learning", "Legal Informatics"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.21693", "pdf": "https://arxiv.org/pdf/2505.21693.pdf", "abs": "https://arxiv.org/abs/2505.21693", "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs", "authors": ["Raoyuan Zhao", "Beiduo Chen", "Barbara Plank", "Michael A. Hedderich"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data.", "AI": {"tldr": "Introduction of MAKIEval, a framework evaluating cultural awareness in LLMs across languages.", "motivation": "To address the challenges in assessing cross-lingual disparities and cultural awareness due to English-centric pretraining of LLMs and limited multilingual benchmarks.", "method": "MAKIEval utilizes Wikidata's multilingual structure to automate the evaluation of cultural entities in LLM outputs, providing language-agnostic scoring across various cultural contexts.", "result": "Evaluation of 7 LLMs across 13 languages and 19 regions revealed that these models displayed greater cultural awareness in English.", "conclusion": "MAKIEval offers a scalable solution for evaluating cultural awareness of LLMs and highlights the English-centric bias in LLM outputs.", "key_contributions": ["Introduction of MAKIEval as a multilingual evaluation framework for LLMs.", "Development of four new metrics for assessing cultural awareness.", "Public release of code and data for further research."], "limitations": "Evaluation is limited to the LLMs studied; further research is needed to generalize findings across more languages and contexts.", "keywords": ["Large Language Models", "Cultural Awareness", "Evaluation Framework", "Cross-lingual", "Multilingual"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.21701", "pdf": "https://arxiv.org/pdf/2505.21701.pdf", "abs": "https://arxiv.org/abs/2505.21701", "title": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing", "authors": ["Raoyuan Zhao", "Abdullatif Köksal", "Ali Modarressi", "Michael A. Hedderich", "Hinrich Schütze"], "categories": ["cs.CL"], "comment": null, "summary": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks.", "AI": {"tldr": "The paper critiques existing probing methods for large language models (LLMs), revealing significant inconsistencies in identifying knowledge gaps due to input variations.", "motivation": "Highlight the unreliability of large language models caused by hallucinations and emphasize the need for better methods to identify knowledge gaps.", "method": "Proposes a new evaluation process using input variations and quantitative metrics to test probing methods.", "result": "Demonstrated significant intra-method and cross-method inconsistencies in knowledge gap detection, with agreement dropping to around 40% and decision consistency as low as 7%.", "conclusion": "Highlights the inadequacy of current probing methods and calls for the development of perturbation-robust probing frameworks.", "key_contributions": ["Identified intra-method and cross-method inconsistencies in knowledge gap probing.", "Proposed a new evaluation process for probing methods using input variations.", "Challenged the reliability of existing probing techniques for LLMs."], "limitations": "The study focuses mainly on probing inconsistencies without proposing a complete solution for robust probing frameworks.", "keywords": ["large language models", "knowledge gaps", "probing methods", "hallucination", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.21710", "pdf": "https://arxiv.org/pdf/2505.21710.pdf", "abs": "https://arxiv.org/abs/2505.21710", "title": "Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study", "authors": ["Barbarestani Baran", "Maks Isa", "Vossen Piek"], "categories": ["cs.CL"], "comment": null, "summary": "This study evaluates the effectiveness of ChatGPT, an advanced AI model for\nnatural language processing, in identifying targeting and inappropriate\nlanguage in online comments. With the increasing challenge of moderating vast\nvolumes of user-generated content on social network sites, the role of AI in\ncontent moderation has gained prominence. We compared ChatGPT's performance\nagainst crowd-sourced annotations and expert evaluations to assess its\naccuracy, scope of detection, and consistency. Our findings highlight that\nChatGPT performs well in detecting inappropriate content, showing notable\nimprovements in accuracy through iterative refinements, particularly in Version\n6. However, its performance in targeting language detection showed variability,\nwith higher false positive rates compared to expert judgments. This study\ncontributes to the field by demonstrating the potential of AI models like\nChatGPT to enhance automated content moderation systems while also identifying\nareas for further improvement. The results underscore the importance of\ncontinuous model refinement and contextual understanding to better support\nautomated moderation and mitigate harmful online behavior.", "AI": {"tldr": "This study evaluates ChatGPT's effectiveness in detecting inappropriate language in online comments compared to crowd-sourced and expert evaluations.", "motivation": "The increasing challenge of moderating vast amounts of user-generated content on social networks highlights the need for effective AI solutions in content moderation.", "method": "The study compared ChatGPT's performance in identifying inappropriate content against crowd-sourced annotations and expert evaluations.", "result": "ChatGPT performs well in detecting inappropriate content, showing improvements in accuracy with iterative refinements, especially in Version 6, but exhibited variability in targeting language detection with higher false positive rates than experts.", "conclusion": "The study demonstrates the potential of AI models like ChatGPT for enhancing automated content moderation while emphasizing ongoing improvements for better contextual understanding.", "key_contributions": ["Evaluation of ChatGPT in content moderation", "Identification of areas for improvement in AI language detection", "Demonstration of iterative refinements enhancing model accuracy"], "limitations": "Variability in performance for targeting language detection and higher false positive rates than expert judgments.", "keywords": ["ChatGPT", "content moderation", "natural language processing", "AI in health"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21740", "pdf": "https://arxiv.org/pdf/2505.21740.pdf", "abs": "https://arxiv.org/abs/2505.21740", "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks", "authors": ["Marvin Limpijankit", "Yanda Chen", "Melanie Subbiah", "Nicholas Deas", "Kathleen McKeown"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks.", "AI": {"tldr": "The paper presents a framework for evaluating how well explanations of LLM outputs allow users to predict results on counterfactuals, specifically in generation tasks such as news summarization and medical suggestions.", "motivation": "The unpredictability of LLM outputs requires reliable explanations, particularly in high-stakes areas like health informatics.", "method": "The authors extend counterfactual simulatability evaluation methods previously used for yes/no tasks to generation tasks, focusing on news summarization and medical suggestion.", "result": "LLM explanations help in predicting outputs for summarization tasks but show limited effectiveness for medical suggestions, highlighting the need for improvements.", "conclusion": "Counterfactual simulatability may be more suitable for evaluating skill-based tasks rather than knowledge-based tasks.", "key_contributions": ["Introduction of a framework for counterfactual simulatability in generation tasks.", "Empirical findings on the effectiveness of LLM explanations in different contexts.", "Insights on the suitability of counterfactual simulatability for skill versus knowledge-based tasks."], "limitations": "The study focuses on two specific application cases and may not generalize across all LLM applications.", "keywords": ["Counterfactual Simulatability", "LLM Explanations", "News Summarization", "Medical Suggestion", "Skill-based Tasks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21757", "pdf": "https://arxiv.org/pdf/2505.21757.pdf", "abs": "https://arxiv.org/abs/2505.21757", "title": "BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum", "authors": ["Yubin Kim", "Zhiyuan Hu", "Hyewon Jeong", "Eugene Park", "Shuyue Stella Li", "Chanwoo Park", "Shiyun Xiong", "MingYu Lu", "Hyeonhoon Lee", "Xin Liu", "Daniel McDuff", "Cynthia Breazeal", "Samir Tulebaev", "Hae Won Park"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) as clinical agents require careful behavioral\nadaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs\noften struggle with proactive engagement, like unprompted identification of\ncritical missing information or risks. We introduce BehaviorBench, a\ncomprehensive dataset to evaluate agent behaviors across a clinical assistance\nspectrum, ranging from reactive query responses to proactive interventions\n(e.g., clarifying ambiguities, flagging overlooked critical data). Our\nBehaviorBench experiments reveal LLMs' inconsistent proactivity. To address\nthis, we propose BehaviorSFT, a novel training strategy using behavioral tokens\nto explicitly condition LLMs for dynamic behavioral selection along this\nspectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro\nF1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to\n96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed\nBehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a\nsuperior balance between helpful proactivity (e.g., timely, relevant\nsuggestions) and necessary restraint (e.g., avoiding over-intervention) versus\nstandard fine-tuning or explicit instructed agents.", "AI": {"tldr": "Introducing BehaviorBench, a dataset for evaluating LLM behaviors in clinical contexts, and BehaviorSFT, a training method to enhance LLM proactivity.", "motivation": "To improve the proactive engagement of LLMs in clinical settings, addressing their inconsistencies by evaluating their performance in identifying critical missing information and risks.", "method": "BehaviorBench is a dataset designed to assess LLM behaviors across a spectrum of clinical assistance, while BehaviorSFT is a training strategy that uses behavioral tokens to condition LLMs for improved behavioral selection.", "result": "BehaviorSFT significantly boosts the performance of LLMs on BehaviorBench, achieving up to 97.3% Macro F1 score and enhancing proactive task performance, confirmed by blind clinician evaluations.", "conclusion": "BehaviorSFT results in LLMs exhibiting more realistic clinical behaviors, effectively balancing helpful proactivity and necessary restraint compared to traditional fine-tuning.", "key_contributions": ["Introduction of BehaviorBench for LLM behavioral evaluation in clinical settings", "Development of BehaviorSFT for improved dynamic behavioral selection", "Demonstration of enhanced LLM proactivity in clinical scenarios through empirical evaluations"], "limitations": "", "keywords": ["Large Language Models", "clinical agents", "BehaviorBench", "BehaviorSFT", "proactive engagement"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21772", "pdf": "https://arxiv.org/pdf/2505.21772.pdf", "abs": "https://arxiv.org/abs/2505.21772", "title": "Calibrating LLM Confidence by Probing Perturbed Representation Stability", "authors": ["Reza Khanmohammadi", "Erfan Miahi", "Mehrsa Mardikoraem", "Simerjot Kaur", "Ivan Brugere", "Charese H. Smiley", "Kundan Thind", "Mohammad M. Ghassemi"], "categories": ["cs.CL"], "comment": null, "summary": "Miscalibration in Large Language Models (LLMs) undermines their reliability,\nhighlighting the need for accurate confidence estimation. We introduce CCPS\n(Calibrating LLM Confidence by Probing Perturbed Representation Stability), a\nnovel method analyzing internal representational stability in LLMs. CCPS\napplies targeted adversarial perturbations to final hidden states, extracts\nfeatures reflecting the model's response to these perturbations, and uses a\nlightweight classifier to predict answer correctness. CCPS was evaluated on\nLLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral\narchitectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and\nopen-ended formats. Our results show that CCPS significantly outperforms\ncurrent approaches. Across four LLMs and three MMLU variants, CCPS reduces\nExpected Calibration Error by approximately 55% and Brier score by 21%, while\nincreasing accuracy by 5 percentage points, Area Under the Precision-Recall\nCurve by 4 percentage points, and Area Under the Receiver Operating\nCharacteristic Curve by 6 percentage points, all relative to the strongest\nprior method. CCPS delivers an efficient, broadly applicable, and more accurate\nsolution for estimating LLM confidence, thereby improving their\ntrustworthiness.", "AI": {"tldr": "CCPS is a new method for improving confidence estimation in Large Language Models by analyzing their internal representational stability through adversarial perturbations, leading to significant performance improvements over existing methods.", "motivation": "Miscalibration in LLMs undermines their reliability, necessitating improved methods for accurate confidence estimation.", "method": "CCPS analyzes internal representational stability in LLMs by applying targeted adversarial perturbations to hidden states, extracting features to predict answer correctness with a lightweight classifier.", "result": "CCPS reduces Expected Calibration Error by 55% and Brier score by 21%, while increasing accuracy by 5 percentage points and relevant AUC metrics by 4 to 6 percentage points across evaluated benchmarks.", "conclusion": "CCPS provides an efficient and accurate solution for LLM confidence estimation, enhancing their trustworthiness in applications.", "key_contributions": ["Introduction of CCPS method for calibrating LLM confidence", "Significant reduction in calibration error compared to previous methods", "Broad applicability across various LLM architectures"], "limitations": "", "keywords": ["Large Language Models", "Confidence Estimation", "Adversarial Perturbations", "Calibration"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21781", "pdf": "https://arxiv.org/pdf/2505.21781.pdf", "abs": "https://arxiv.org/abs/2505.21781", "title": "GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task", "authors": ["Chutong Meng", "Antonios Anastasopoulos"], "categories": ["cs.CL"], "comment": "IWSLT 2025", "summary": "This paper describes the GMU systems for the IWSLT 2025 low-resource speech\ntranslation shared task. We trained systems for all language pairs, except for\nLevantine Arabic. We fine-tuned SeamlessM4T-v2 for automatic speech recognition\n(ASR), machine translation (MT), and end-to-end speech translation (E2E ST).\nThe ASR and MT models are also used to form cascaded ST systems. Additionally,\nwe explored various training paradigms for E2E ST fine-tuning, including direct\nE2E fine-tuning, multi-task training, and parameter initialization using\ncomponents from fine-tuned ASR and/or MT models. Our results show that (1)\ndirect E2E fine-tuning yields strong results; (2) initializing with a\nfine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has\nnot been trained on; (3) multi-task training can be slightly helpful.", "AI": {"tldr": "This paper presents the GMU systems for low-resource speech translation, detailing techniques used and key findings from IWSLT 2025.", "motivation": "To develop effective speech translation systems for low-resource languages in the IWSLT 2025 shared task.", "method": "We fine-tuned SeamlessM4T-v2 for ASR, MT, and E2E ST, and explored various fine-tuning paradigms including direct E2E, multi-task training, and parameter initialization from ASR/MT models.", "result": "Direct E2E fine-tuning yields strong results; initializing with a fine-tuned ASR encoder improves ST for untrained languages; multi-task training provides slight benefits.", "conclusion": "Direct E2E fine-tuning is robust, and leveraging ASR models enhances performance on languages with limited resources.", "key_contributions": ["Developed systems for all language pairs except Levantine Arabic", "Explored various training paradigms resulting in improved performance", "Demonstrated the effectiveness of model initialization techniques"], "limitations": "", "keywords": ["speech translation", "low-resource languages", "ASR", "MT", "IWSLT 2025"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.21786", "pdf": "https://arxiv.org/pdf/2505.21786.pdf", "abs": "https://arxiv.org/abs/2505.21786", "title": "VeriTrail: Closed-Domain Hallucination Detection with Traceability", "authors": ["Dasha Metropolitansky", "Jonathan Larson"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Even when instructed to adhere to source material, Language Models often\ngenerate unsubstantiated content - a phenomenon known as \"closed-domain\nhallucination.\" This risk is amplified in processes with multiple generative\nsteps (MGS), compared to processes with a single generative step (SGS).\nHowever, due to the greater complexity of MGS processes, we argue that\ndetecting hallucinations in their final outputs is necessary but not\nsufficient: it is equally important to trace where hallucinated content was\nlikely introduced and how faithful content may have been derived from the\nsource through intermediate outputs. To address this need, we present\nVeriTrail, the first closed-domain hallucination detection method designed to\nprovide traceability for both MGS and SGS processes. We also introduce the\nfirst datasets to include all intermediate outputs as well as human annotations\nof final outputs' faithfulness for their respective MGS processes. We\ndemonstrate that VeriTrail outperforms baseline methods on both datasets.", "AI": {"tldr": "This paper introduces VeriTrail, a novel method for detecting closed-domain hallucinations in language models, focusing on traceability in multi-generative step processes.", "motivation": "The motivation behind this research is to address the issue of closed-domain hallucinations in language models, especially in complex multi-generative step processes where tracing content misalignment is crucial.", "method": "The authors present VeriTrail, which detects hallucinations in language models and provides traceability for both multi-generative steps (MGS) and single generative steps (SGS). They also introduce datasets that include intermediate outputs along with human annotations.", "result": "VeriTrail outperforms existing baseline methods in detecting hallucinations and tracing the sources of unsubstantiated content in both multi-generative and single generative processes.", "conclusion": "The study underscores the importance not just of detecting hallucinations but also of understanding how they originated throughout the generative process.", "key_contributions": ["Introduction of VeriTrail for closed-domain hallucination detection", "Development of datasets with intermediate outputs and faithfulness annotations", "Demonstration of improved performance over baseline methods."], "limitations": "", "keywords": ["closed-domain hallucination", "veriTrail", "language models", "traceability", "multi-generative processes"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21816", "pdf": "https://arxiv.org/pdf/2505.21816.pdf", "abs": "https://arxiv.org/abs/2505.21816", "title": "Revisiting Common Assumptions about Arabic Dialects in NLP", "authors": ["Amr Keleg", "Sharon Goldwater", "Walid Magdy"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Arabic has diverse dialects, where one dialect can be substantially different\nfrom the others. In the NLP literature, some assumptions about these dialects\nare widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable\nregional dialects\") and are manifested in different computational tasks such as\nArabic Dialect Identification (ADI). However, these assumptions are not\nquantitatively verified. We identify four of these assumptions and examine them\nby extending and analyzing a multi-label dataset, where the validity of each\nsentence in 11 different country-level dialects is manually assessed by\nspeakers of these dialects. Our analysis indicates that the four assumptions\noversimplify reality, and some of them are not always accurate. This in turn\nmight be hindering further progress in different Arabic NLP tasks.", "AI": {"tldr": "The paper critiques commonly held assumptions about the classification of Arabic dialects in NLP, highlighting their oversimplification and potential inaccuracies.", "motivation": "To verify widespread assumptions in the NLP literature regarding the classification of Arabic dialects and assess their impact on tasks like Arabic Dialect Identification (ADI).", "method": "The authors analyze a multi-label dataset with manual assessments of sentence validity in 11 different country-level dialects, focusing on four widely accepted assumptions.", "result": "The analysis reveals that the four assumptions oversimplify the complexity of Arabic dialects and are not always valid, suggesting that they hinder progress in Arabic NLP tasks.", "conclusion": "Reevaluating these assumptions is crucial for advancing computational tasks related to Arabic dialects in NLP.", "key_contributions": ["Critique of four common assumptions regarding Arabic dialects in NLP.", "Development of a multi-label dataset with manual assessments by native speakers.", "Implications for improving Arabic NLP tasks and dialect identification."], "limitations": "The study is limited to 11 dialects and may not account for all variations within Arabic dialects.", "keywords": ["Arabic dialects", "Natural Language Processing", "Dialect Identification", "NLP assumptions", "Multi-label dataset"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2409.08577", "pdf": "https://arxiv.org/pdf/2409.08577.pdf", "abs": "https://arxiv.org/abs/2409.08577", "title": "Exploring Remote Collaborative Tasks: The Impact of Avatar Representation on Dyadic Haptic Interactions in Shared Virtual Environments", "authors": ["Genki Sasaki", "Hiroshi Igarashi"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "This study is the first to explore the interplay between haptic interaction\nand avatar representation in Shared Virtual Environments (SVEs). Specifically,\nhow these factors shape users' sense of social presence during dyadic\ncollaborations, while assessing potential effects on task performance. In a\nseries of experiments, participants performed the collaborative task with\nhaptic interaction under four avatar representation conditions: avatars of both\nparticipant and partner were displayed, only the participant's avatar was\ndisplayed, only the partner's avatar was displayed, and no avatars were\ndisplayed. The study finds that avatar representation, especially of the\npartner, significantly enhances the perception of social presence, which haptic\ninteraction alone does not fully achieve. However, neither the presence nor the\ntype of avatar representation impacts the task performance or participants'\nforce effort of the task, suggesting that haptic interaction provides\nsufficient interaction cues for the execution of the task. These results\nunderscore the significance of integrating both visual and haptic modalities to\noptimize remote collaboration experiences in virtual environments, ensuring\neffective communication and a strong sense of social presence.", "AI": {"tldr": "This study investigates how haptic interaction and avatar representation affect social presence in Shared Virtual Environments during collaborative tasks.", "motivation": "To explore the impact of haptic interaction and avatar representation on users' sense of social presence in collaborative tasks within Shared Virtual Environments.", "method": "Experiments were conducted involving participants performing a collaborative task under four different avatar representation conditions while utilizing haptic interaction.", "result": "The presence of avatars, particularly the partner's avatar, significantly increased the perception of social presence, though it did not affect task performance or force effort.", "conclusion": "Integrating visual (avatar representation) and haptic modalities is essential for enhancing remote collaboration experiences in virtual environments.", "key_contributions": ["First study on haptic interaction and avatar representation interplay in SVEs.", "Demonstrated that avatar presence enhances social presence perceptions.", "Suggested that haptic interaction alone is sufficient for task performance."], "limitations": "The study did not assess long-term effects of avatar representation on social presence or task outcomes in various contexts.", "keywords": ["haptic interaction", "avatar representation", "social presence", "Shared Virtual Environments", "collaboration"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.21819", "pdf": "https://arxiv.org/pdf/2505.21819.pdf", "abs": "https://arxiv.org/abs/2505.21819", "title": "Representative Language Generation", "authors": ["Charlotte Peale", "Vinod Raman", "Omer Reingold"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "We introduce \"representative generation,\" extending the theoretical framework\nfor generation proposed by Kleinberg et al. (2024) and formalized by Li et al.\n(2024), to additionally address diversity and bias concerns in generative\nmodels. Our notion requires outputs of a generative model to proportionally\nrepresent groups of interest from the training data. We characterize\nrepresentative uniform and non-uniform generation, introducing the \"group\nclosure dimension\" as a key combinatorial quantity. For representative\ngeneration in the limit, we analyze both information-theoretic and\ncomputational aspects, demonstrating feasibility for countably infinite\nhypothesis classes and collections of groups under certain conditions, but\nproving a negative result for computability using only membership queries. This\ncontrasts with Kleinberg et al.'s (2024) positive results for standard\ngeneration in the limit. Our findings provide a rigorous foundation for\ndeveloping more diverse and representative generative models.", "AI": {"tldr": "The paper introduces 'representative generation,' enhancing generative model frameworks to tackle diversity and bias, focusing on proportional representation of groups in outputs.", "motivation": "To address diversity and bias in generative models, ensuring that outputs reflect the groups of interest from the training data.", "method": "The authors propose the concept of representative generation, characterizing uniform and non-uniform generation, and introducing the 'group closure dimension' as a combinatorial measure.", "result": "The paper demonstrates the feasibility of representative generation for infinite hypothesis classes under certain conditions but highlights a computability limitation when using membership queries.", "conclusion": "The findings offer a foundational framework for developing more diverse and representative generative models.", "key_contributions": ["Introduction of representative generation framework for generative models.", "Characterization of group closure dimension for measuring diversity in outputs.", "Analysis of computational and information-theoretic aspects of representative generation."], "limitations": "Negative results for computability using only membership queries limit the model's applicability in certain scenarios.", "keywords": ["generative models", "diversity", "bias", "combinatorial dimensions", "ICML 2025"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.03724", "pdf": "https://arxiv.org/pdf/2410.03724.pdf", "abs": "https://arxiv.org/abs/2410.03724", "title": "Overcoming the Machine Penalty with Imperfectly Fair AI Agents", "authors": ["Zhen Wang", "Ruiqi Song", "Chen Shen", "Shiya Yin", "Zhao Song", "Balaraju Battu", "Lei Shi", "Danyang Jia", "Talal Rahwan", "Shuyue Hu"], "categories": ["cs.HC", "cs.AI", "cs.GT", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Despite rapid technological progress, effective human-machine cooperation\nremains a significant challenge. Humans tend to cooperate less with machines\nthan with fellow humans, a phenomenon known as the machine penalty. Here, we\nshow that artificial intelligence (AI) agents powered by large language models\ncan overcome this penalty in social dilemma games with communication. In a\npre-registered experiment with 1,152 participants, we deploy AI agents\nexhibiting three distinct personas: selfish, cooperative, and fair. However,\nonly fair agents elicit human cooperation at rates comparable to human-human\ninteractions. Analysis reveals that fair agents, similar to human participants,\noccasionally break pre-game cooperation promises, but nonetheless effectively\nestablish cooperation as a social norm. These results challenge the\nconventional wisdom of machines as altruistic assistants or rational actors.\nInstead, our study highlights the importance of AI agents reflecting the\nnuanced complexity of human social behaviors -- imperfect yet driven by deeper\nsocial cognitive processes.", "AI": {"tldr": "AI agents powered by large language models can effectively elicit human cooperation in social dilemma games when exhibiting fair personas.", "motivation": "To investigate why humans tend to cooperate less with machines compared to fellow humans, known as the machine penalty.", "method": "A pre-registered experiment with 1,152 participants where AI agents with different personas (selfish, cooperative, fair) were deployed in social dilemma games with communication.", "result": "Only fair AI agents elicited human cooperation rates comparable to human-human interactions; they sometimes broke promises but established cooperation as a norm.", "conclusion": "AI agents should reflect the complexity of human social behaviors to enhance cooperation, challenging the notion of machines as purely altruistic or rational.", "key_contributions": ["Demonstrated that AI agents can overcome the machine penalty in social dilemmas.", "Showed the importance of persona in AI interactions with humans.", "Highlighted that fair AI agents can mimic social behaviors of humans."], "limitations": "The study focuses on specific social dilemma games and may not generalize to all contexts of human-AI interaction.", "keywords": ["Human-Computer Interaction", "Large Language Models", "Social Dilemma Games"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21859", "pdf": "https://arxiv.org/pdf/2505.21859.pdf", "abs": "https://arxiv.org/abs/2505.21859", "title": "Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries", "authors": ["Vishakh Padmakumar", "Zichao Wang", "David Arbour", "Jennifer Healey"], "categories": ["cs.CL"], "comment": "To appear at ACL 2025 - Main Conference", "summary": "While large language models (LLMs) are increasingly capable of handling\nlonger contexts, recent work has demonstrated that they exhibit the \"lost in\nthe middle\" phenomenon (Liu et al., 2024) of unevenly attending to different\nparts of the provided context. This hinders their ability to cover diverse\nsource material in multi-document summarization, as noted in the DiverseSumm\nbenchmark (Huang et al., 2024). In this work, we contend that principled\ncontent selection is a simple way to increase source coverage on this task. As\nopposed to prompting an LLM to perform the summarization in a single step, we\nexplicitly divide the task into three steps -- (1) reducing document\ncollections to atomic key points, (2) using determinantal point processes (DPP)\nto perform select key points that prioritize diverse content, and (3) rewriting\nto the final summary. By combining prompting steps, for extraction and\nrewriting, with principled techniques, for content selection, we consistently\nimprove source coverage on the DiverseSumm benchmark across various LLMs.\nFinally, we also show that by incorporating relevance to a provided user intent\ninto the DPP kernel, we can generate personalized summaries that cover relevant\nsource information while retaining coverage.", "AI": {"tldr": "This paper proposes a three-step approach to enhance source coverage in multi-document summarization using large language models (LLMs), addressing the 'lost in the middle' phenomenon.", "motivation": "The paper highlights the limitations of current LLMs in multi-document summarization due to their uneven attention across different parts of the context.", "method": "The proposed method divides the summarization process into three steps: (1) identifying atomic key points from documents, (2) using determinantal point processes (DPP) for diverse content selection, and (3) performing a final rewrite of the summary.", "result": "The proposed approach consistently improves source coverage on the DiverseSumm benchmark across various LLMs.", "conclusion": "Incorporating user intent into the DPP kernel allows for personalized summaries that maintain high coverage of relevant information.", "key_contributions": ["Three-step framework for summarization", "Use of determinantal point processes for content selection", "Personalized summarization based on user intent"], "limitations": "", "keywords": ["large language models", "multi-document summarization", "determinantal point processes"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.11835", "pdf": "https://arxiv.org/pdf/2411.11835.pdf", "abs": "https://arxiv.org/abs/2411.11835", "title": "Describe Now: User-Driven Audio Description for Blind and Low Vision Individuals", "authors": ["Maryam Cheema", "Hasti Seifi", "Pooyan Fazli"], "categories": ["cs.HC"], "comment": "17 pages, 14 figures", "summary": "Audio descriptions (AD) make videos accessible for blind and low vision (BLV)\nusers by describing visual elements that cannot be understood from the main\naudio track. AD created by professionals or novice describers is time-consuming\nand offers little customization or control to BLV viewers on description length\nand content and when they receive it. To address this gap, we explore\nuser-driven AI-generated descriptions, enabling BLV viewers to control both the\ntiming and level of detail of the descriptions they receive. In a study, 20 BLV\nparticipants activated audio descriptions for seven different video genres with\ntwo levels of detail: concise and detailed. Our findings reveal differences in\nthe preferred frequency and level of detail of ADs for different videos,\nparticipants' sense of control with this style of AD delivery, and its\nlimitations. We discuss the implications of these findings for the development\nof future AD tools for BLV users.", "AI": {"tldr": "This paper explores user-driven AI-generated audio descriptions (AD) for blind and low vision (BLV) users, allowing them to control the timing and detail of descriptions, with findings from a study involving 20 participants across various video genres.", "motivation": "To enhance the accessibility of videos for blind and low vision (BLV) users by allowing them to customize and control audio descriptions, overcoming the limitations of traditional AD methods.", "method": "A study was conducted with 20 BLV participants using AI-generated audio descriptions across seven video genres with two levels of detail (concise and detailed). The participants' preferences for frequency and detail of descriptions were recorded and analyzed.", "result": "The study found that participants exhibited varying preferences for the frequency and detail of audio descriptions depending on the video genre, and they appreciated having control over these aspects.", "conclusion": "The findings suggest that user-driven, AI-generated audio descriptions can significantly improve the viewing experience for BLV users and inform the design of future AD tools.", "key_contributions": ["Introduction of user-driven AI-generated audio descriptions for BLV users", "Empirical findings on user preferences for audio description detail and timing", "Insights into the customization needs of BLV viewers for enhanced video accessibility"], "limitations": "The study involved a small sample size and specific video genres, which may not generalize to all BLV users or video types.", "keywords": ["Audio descriptions", "Blind and low vision", "User-driven AI", "Video accessibility", "Human-computer interaction"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2505.21870", "pdf": "https://arxiv.org/pdf/2505.21870.pdf", "abs": "https://arxiv.org/abs/2505.21870", "title": "Evaluating the Retrieval Robustness of Large Language Models", "authors": ["Shuyang Cao", "Karthik Radhakrishnan", "David Rosenberg", "Steven Lu", "Pengxiang Cheng", "Lu Wang", "Shiyue Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG.", "AI": {"tldr": "This paper investigates the effectiveness and robustness of retrieval-augmented generation (RAG) in large language models (LLMs) for knowledge-intensive tasks.", "motivation": "To address potential performance degradation in LLMs when using RAG due to imperfect retrieval and leveraging issues.", "method": "A benchmark of 1500 open-domain questions with retrieved documents from Wikipedia is established, alongside three robustness metrics corresponding to the research questions. Experiments involving 11 LLMs and 3 prompting strategies are conducted.", "result": "Experiments show that all LLMs demonstrate high retrieval robustness, but varying levels of imperfect robustness limit their ability to maximize RAG benefits.", "conclusion": "The findings suggest that while RAG can improve task performance, challenges remain in its implementation, indicating the need for better retrieval processes and understanding of document interaction.", "key_contributions": ["Establishment of a benchmark for evaluating RAG performance", "Introduction of robustness metrics for assessing LLMs in RAG contexts", "Empirical evaluation of 11 LLMs with various prompt strategies regarding their retrieval robustness"], "limitations": "The study is limited to open-domain questions and may not generalize well to other domains or types of documents.", "keywords": ["retrieval-augmented generation", "large language models", "robustness metrics", "knowledge-intensive tasks", "document interactions"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2503.07320", "pdf": "https://arxiv.org/pdf/2503.07320.pdf", "abs": "https://arxiv.org/abs/2503.07320", "title": "When Trust Collides: Decoding Human-LLM Cooperation Dynamics through the Prisoner's Dilemma", "authors": ["Guanxuan Jiang", "Shirao Yang", "Yuyang Wang", "Pan Hui"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly capable of autonomous\ndecision-making, they introduce new challenges and opportunities for human-AI\ncooperation in mixed-motive contexts. While prior research has primarily\nexamined AI in assistive or cooperative roles, little is known about how humans\ninteract with AI agents perceived as independent and strategic actors. This\nstudy investigates human cooperative attitudes and behaviors toward LLM agents\nby engaging 30 participants (15 males, 15 females) in repeated Prisoner's\nDilemma games with agents differing in declared identity: purported human,\nrule-based AI, and LLM agent. Behavioral metrics, including cooperation rate,\ndecision latency, unsolicited cooperative acts and trust restoration tolerance,\nwere analyzed to assess the influence of agent identity and participant gender.\nResults revealed significant effects of declared agent identity on most\ncooperation-related behaviors, along with notable gender differences in\ndecision latency. Furthermore, qualitative responses suggest that these\nbehavioral differences were shaped by participants interpretations and\nexpectations of the agents. These findings contribute to our understanding of\nhuman adaptation in competitive cooperation with autonomous agents and\nunderscore the importance of agent framing in shaping effective and ethical\nhuman-AI interaction.", "AI": {"tldr": "The study explores human cooperation with large language models (LLMs) perceived as independent actors in strategic scenarios, showing that agent identity affects cooperation behaviors and decision latency, influenced by participants' interpretations.", "motivation": "With the rise of autonomous LLMs, understanding human-AI cooperation dynamics becomes essential, especially in competitive contexts where AI agents are perceived as independent.", "method": "30 participants played repeated Prisoner's Dilemma games against agents with different identities (human, rule-based AI, LLM). Key metrics analyzed included cooperation rates, decision latency, unsolicited cooperative acts, and trust restoration.", "result": "Significant influence of agent identity on cooperation-related behaviors was found, alongside notable gender differences in decision latency.", "conclusion": "The study highlights the impact of agent framing on human-AI interaction and suggests pathways for improving ethical cooperation between humans and autonomous agents.", "key_contributions": ["Identified the effects of agent identity on human cooperation behaviors", "Revealed gender differences in decision-making related to AI agents", "Highlighted the importance of interpretation and expectation in human-AI interactions"], "limitations": "", "keywords": ["Human-AI interaction", "Cooperation", "Large language models", "Decision making", "Gender differences"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.21889", "pdf": "https://arxiv.org/pdf/2505.21889.pdf", "abs": "https://arxiv.org/abs/2505.21889", "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse", "authors": ["Tianyu Guo", "Hande Dong", "Yichong Leng", "Feng Liu", "Cheater Lin", "Nong Xiao", "Xianwei Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability.EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM.", "AI": {"tldr": "The paper introduces EFIM, a new prompt format that enhances the efficiency of KV cache reuse in large language models (LLMs) for infilling tasks, while addressing subtoken generation issues.", "motivation": "Traditional infilling tasks in LLMs face latency challenges due to inefficiencies in KV cache reuse caused by prompt structure.", "method": "Propose a transformed prompt format (EFIM) to improve KV cache reuse and introduce a fragment tokenization training method to better handle token generation.", "result": "EFIM reduces latency by 52% and improves throughput by 98% on two representative LLMs, while preserving infilling capability.", "conclusion": "EFIM effectively enhances the performance of infilling tasks by optimizing KV cache usage and addressing subtoken generation issues.", "key_contributions": ["Introduction of EFIM prompt format for efficient KV cache reuse", "Development of fragment tokenization training to address subtoken generation", "Demonstration of significant performance improvements in LLM serving."], "limitations": "", "keywords": ["large language models", "KV cache reuse", "infilling tasks", "fragment tokenization", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.20262", "pdf": "https://arxiv.org/pdf/2503.20262.pdf", "abs": "https://arxiv.org/abs/2503.20262", "title": "From the CDC to emerging infectious disease publics: The long-now of polarizing and complex health crises", "authors": ["Tawfiq Ammari", "Anna Gutowska", "Jacob Ziff", "Casey Randazzo", "Harihan Subramonyam"], "categories": ["cs.HC", "cs.SI"], "comment": null, "summary": "This study examines how public discourse around COVID-19 unfolded on Twitter\nthrough the lens of crisis communication and digital publics. Analyzing over\n275,000 tweets involving the CDC, we identify 16 distinct discourse clusters\nshaped by framing, sentiment, credibility, and network dynamics. We find that\nCDC messaging became a flashpoint for affective and ideological polarization,\nwith users aligning along competing frames of science vs. freedom, and public\nhealth vs. political overreach. Most clusters formed echo chambers, while a few\nenabled cross cutting dialogue. Publics emerged not only around ideology but\nalso around topical and emotional stakes, reflecting shifting concerns across\ndifferent stages of the pandemic. While marginalized communities raised\nconsistent equity concerns, these narratives struggled to reshape broader\ndiscourse. Our findings highlight the importance of long-term, adaptive\nengagement with diverse publics and propose design interventions such as\nmulti-agent AI assistants, to support more inclusive communication throughout\nextended public health crises.", "AI": {"tldr": "This study analyzes over 275,000 tweets about COVID-19 from the CDC, identifying 16 discourse clusters and highlighting the impact of polarization in public health messaging.", "motivation": "To understand the dynamics of public discourse around COVID-19 on Twitter and its implications for crisis communication and equity.", "method": "Analysis of over 275,000 tweets related to the CDC using clustering techniques to identify discourse patterns and sentiment.", "result": "Identified 16 distinct discourse clusters characterized by ideological polarization, most forming echo chambers but a few facilitating cross-cutting dialogue.", "conclusion": "Long-term, adaptive engagement is crucial for effective communication in public health crises, with recommendations for using multi-agent AI assistants to enhance inclusivity.", "key_contributions": ["Identification of discourse clusters around COVID-19", "Insights into polarization and echo chambers in public health communication", "Proposals for design interventions using AI for inclusivity"], "limitations": "The study may not capture the full scope of offline discourse or outcomes of the identified clusters.", "keywords": ["COVID-19", "Twitter", "Public Health", "Crisis Communication", "AI Assistants"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.21898", "pdf": "https://arxiv.org/pdf/2505.21898.pdf", "abs": "https://arxiv.org/abs/2505.21898", "title": "Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development", "authors": ["Rennai Qiu", "Chen Qian", "Ran Li", "Yufan Dang", "Weize Chen", "Cheng Yang", "Yingli Zhang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": "Work in Progress", "summary": "Recent advancements in Large Language Models (LLMs) and autonomous agents\nhave demonstrated remarkable capabilities across various domains. However,\nstandalone agents frequently encounter limitations when handling complex tasks\nthat demand extensive interactions and substantial computational resources.\nAlthough Multi-Agent Systems (MAS) alleviate some of these limitations through\ncollaborative mechanisms like task decomposition, iterative communication, and\nrole specialization, they typically remain resource-unaware, incurring\nsignificant inefficiencies due to high token consumption and excessive\nexecution time. To address these limitations, we propose a resource-aware\nmulti-agent system -- Co-Saving (meaning that multiple agents collaboratively\nengage in resource-saving activities), which leverages experiential knowledge\nto enhance operational efficiency and solution quality. Our key innovation is\nthe introduction of \"shortcuts\" -- instructional transitions learned from\nhistorically successful trajectories -- which allows to bypass redundant\nreasoning agents and expedite the collective problem-solving process.\nExperiments for software development tasks demonstrate significant advantages\nover existing methods. Specifically, compared to the state-of-the-art MAS\nChatDev, our method achieves an average reduction of 50.85% in token usage, and\nimproves the overall code quality by 10.06%.", "AI": {"tldr": "Proposes a resource-aware multi-agent system called Co-Saving to improve efficiency in complex task handling via shortcut learning from successful trajectories.", "motivation": "Standalone agents face limitations in handling complex tasks due to high resource consumption and inefficiencies, prompting the need for a more efficient collaborative approach through multi-agent systems.", "method": "The Co-Saving system utilizes collaborative mechanisms, including task decomposition, iterative communication, and role specialization, while introducing 'shortcuts' to bypass redundant reasoning agents based on historically successful trajectories.", "result": "Experiments show Co-Saving reduces token usage by an average of 50.85% and improves code quality by 10.06% compared to the state-of-the-art method ChatDev.", "conclusion": "Co-Saving demonstrates that resource-aware collaborative mechanisms can significantly enhance the efficiency and quality of multi-agent systems in software development tasks.", "key_contributions": ["Introduction of a resource-aware multi-agent system (Co-Saving)", "Utilization of experiential knowledge for operational efficiency", "Development of shortcuts for expedited problem-solving"], "limitations": "", "keywords": ["Multi-Agent Systems", "Resource-Aware", "Collaboration", "Large Language Models", "Software Development"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.13904", "pdf": "https://arxiv.org/pdf/2504.13904.pdf", "abs": "https://arxiv.org/abs/2504.13904", "title": "Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Yuewen Sun", "Xinshuai Dong", "Kazushi Ikeda", "Peter Spirtes", "Kun Zhang"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "12 pages, 10 figures, 1 table. Accepted by ACM UMAP 2025", "summary": "We hypothesize that optimal system responses emerge from adaptive strategies\ngrounded in causal and counterfactual knowledge. Counterfactual inference\nallows us to create hypothetical scenarios to examine the effects of\nalternative system responses. We enhance this process through causal discovery,\nwhich identifies the strategies informed by the underlying causal structure\nthat govern system behaviors. Moreover, we consider the psychological\nconstructs and unobservable noises that might be influencing user-system\ninteractions as latent factors. We show that these factors can be effectively\nestimated. We employ causal discovery to identify strategy-level causal\nrelationships among user and system utterances, guiding the generation of\npersonalized counterfactual dialogues. We model the user utterance strategies\nas causal factors, enabling system strategies to be treated as counterfactual\nactions. Furthermore, we optimize policies for selecting system responses based\non counterfactual data. Our results using a real-world dataset on social good\ndemonstrate significant improvements in persuasive system outcomes, with\nincreased cumulative rewards validating the efficacy of causal discovery in\nguiding personalized counterfactual inference and optimizing dialogue policies\nfor a persuasive dialogue system.", "AI": {"tldr": "This paper proposes a model for generating personalized counterfactual dialogues in persuasive systems by utilizing causal discovery and counterfactual inference to optimize system responses based on user utterances.", "motivation": "The research addresses the need for optimal system responses in user interactions by leveraging causal and counterfactual knowledge to improve engagement and outcomes in persuasive dialogue systems.", "method": "The methodology involves using causal discovery to identify user and system interaction patterns, estimating latent psychological factors influencing these interactions, and employing counterfactual inference to generate personalized dialogue strategies.", "result": "Experiments on a real-world dataset showed significant improvements in persuasive outcomes, with enhanced cumulative rewards, validating the proposed approach.", "conclusion": "The findings support the effectiveness of causal discovery in optimizing dialogue policies and enhancing user-system interactions in persuasive contexts.", "key_contributions": ["Introduction of causal discovery for analyzing user-system dialogues", "Development of a framework for personalized counterfactual dialogues", "Demonstration of improved persuasive outcome metrics through causal methods"], "limitations": "", "keywords": ["Causal Inference", "Counterfactuals", "Dialogue Systems", "Persuasive Technology", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.21926", "pdf": "https://arxiv.org/pdf/2505.21926.pdf", "abs": "https://arxiv.org/abs/2505.21926", "title": "Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning", "authors": ["Yin Hua", "Zhiqiang Liu", "Mingyang Chen", "Zheng Fang", "Chi Man Wong", "Lingxiao Li", "Chi Man Vong", "Huajun Chen", "Wen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "In natural language processing (NLP) and computer vision (CV), the successful\napplication of foundation models across diverse tasks has demonstrated their\nremarkable potential. However, despite the rich structural and textual\ninformation embedded in knowledge graphs (KGs), existing research of foundation\nmodel for KG has primarily focused on their structural aspects, with most\nefforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This\nlimitation has hindered progress in addressing more challenging out-of-KG\ntasks. In this paper, we introduce MERRY, a foundation model for general\nknowledge graph reasoning, and investigate its performance across two task\ncategories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG\nquestion answering, KGQA). We not only utilize the structural information, but\nalso the textual information in KGs. Specifically, we propose a\nmulti-perspective Conditional Message Passing (CMP) encoding architecture to\nbridge the gap between textual and structural modalities, enabling their\nseamless integration. Additionally, we introduce a dynamic residual fusion\nmodule to selectively retain relevant textual information and a flexible edge\nscoring mechanism to adapt to diverse downstream tasks. Comprehensive\nevaluations on 28 datasets demonstrate that MERRY outperforms existing\nbaselines in most scenarios, showcasing strong reasoning capabilities within\nKGs and excellent generalization to out-of-KG tasks such as KGQA.", "AI": {"tldr": "MERRY is a foundation model designed for knowledge graph reasoning that outperforms existing methods by integrating both structural and textual information.", "motivation": "To address the limitations of current research on foundation models for knowledge graphs which focus primarily on in-KG tasks, hindering progress in more challenging out-of-KG tasks.", "method": "MERRY employs a multi-perspective Conditional Message Passing (CMP) encoding architecture and a dynamic residual fusion module to integrate textual and structural information effectively.", "result": "MERRY shows superior performance across 28 datasets, outperforming existing baselines in in-KG reasoning tasks and demonstrating strong generalization to out-of-KG tasks like knowledge graph question answering.", "conclusion": "The comprehensive evaluations indicate that MERRY represents a significant advancement in knowledge graph reasoning capabilities and can effectively address both in-KG and out-of-KG tasks.", "key_contributions": ["Introduction of the MERRY foundation model for knowledge graph reasoning", "Development of a multi-perspective Conditional Message Passing architecture", "Implementation of a dynamic residual fusion module for relevant information retention"], "limitations": "", "keywords": ["Natural Language Processing", "Knowledge Graphs", "Machine Learning", "Foundation Models", "Reasoning Tasks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21936", "pdf": "https://arxiv.org/pdf/2505.21936.pdf", "abs": "https://arxiv.org/abs/2505.21936", "title": "RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments", "authors": ["Zeyi Liao", "Jaylen Jones", "Linxi Jiang", "Eric Fosler-Lussier", "Yu Su", "Zhiqiang Lin", "Huan Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Computer-use agents (CUAs) promise to automate complex tasks across operating\nsystems (OS) and the web, but remain vulnerable to indirect prompt injection.\nCurrent evaluations of this threat either lack support realistic but controlled\nenvironments or ignore hybrid web-OS attack scenarios involving both\ninterfaces. To address this, we propose RedTeamCUA, an adversarial testing\nframework featuring a novel hybrid sandbox that integrates a VM-based OS\nenvironment with Docker-based web platforms. Our sandbox supports key features\ntailored for red teaming, such as flexible adversarial scenario configuration,\nand a setting that decouples adversarial evaluation from navigational\nlimitations of CUAs by initializing tests directly at the point of an\nadversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive\nbenchmark with 864 examples that investigate realistic, hybrid web-OS attack\nscenarios and fundamental security vulnerabilities. Benchmarking current\nfrontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA\ndemonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated,\nstill exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute\nadversarial tasks with an Attempt Rate as high as 92.5%, although failing to\ncomplete them due to capability limitations. Nevertheless, we observe\nconcerning ASRs of up to 50% in realistic end-to-end settings, with the\nrecently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%,\ndemonstrating that indirect prompt injection presents tangible risks for even\nadvanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA\nprovides an essential framework for advancing realistic, controlled, and\nsystematic analysis of CUA vulnerabilities, highlighting the urgent need for\nrobust defenses to indirect prompt injection prior to real-world deployment.", "AI": {"tldr": "RedTeamCUA is an adversarial testing framework that assesses vulnerabilities in computer-use agents (CUAs) through a hybrid sandbox integrating VM-based OS and Docker-based web platforms. It reveals substantial indirect prompt injection risks in CUAs, even the most secure ones.", "motivation": "To enhance the evaluation of vulnerabilities in CUAs against indirect prompt injection threats in a realistic testing environment.", "method": "Developed a novel hybrid sandbox named RedTeamCUA that combines VM-based OS and Docker platforms for testing CUAs in controlled adversarial scenarios.", "result": "Benchmarking with RTC-Bench revealed significant vulnerabilities in CUAs, particularly a 42.9% ASR for Claude 3.7 Sonnet and a 7.6% ASR for the most secure CUA, Operator, indicating substantial risks.", "conclusion": "RedTeamCUA serves as a critical tool for systematically analyzing CUA vulnerabilities, stressing the need for improved defenses against indirect prompt injection before deployment.", "key_contributions": ["Introduction of RedTeamCUA framework for testing CUAs", "Development of RTC-Bench with 864 examples for benchmarking CUAs", "Identification of significant vulnerabilities in current frontier CUAs"], "limitations": "May not encompass all possible attack vectors or CUAs beyond those benchmarked.", "keywords": ["computer-use agents", "indirect prompt injection", "adversarial testing", "hybrid sandbox", "security vulnerabilities"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2505.21937", "pdf": "https://arxiv.org/pdf/2505.21937.pdf", "abs": "https://arxiv.org/abs/2505.21937", "title": "Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages", "authors": ["Pratik Rakesh Singh", "Kritarth Prasad", "Mohammadi Zaki", "Pankaj Wasnik"], "categories": ["cs.CL"], "comment": null, "summary": "Translating multi-word expressions (MWEs) and idioms requires a deep\nunderstanding of the cultural nuances of both the source and target languages.\nThis challenge is further amplified by the one-to-many nature of idiomatic\ntranslations, where a single source idiom can have multiple target-language\nequivalents depending on cultural references and contextual variations.\nTraditional static knowledge graphs (KGs) and prompt-based approaches struggle\nto capture these complex relationships, often leading to suboptimal\ntranslations. To address this, we propose IdiomCE, an adaptive graph neural\nnetwork (GNN) based methodology that learns intricate mappings between\nidiomatic expressions, effectively generalizing to both seen and unseen nodes\nduring training. Our proposed method enhances translation quality even in\nresource-constrained settings, facilitating improved idiomatic translation in\nsmaller models. We evaluate our approach on multiple idiomatic translation\ndatasets using reference-less metrics, demonstrating significant improvements\nin translating idioms from English to various Indian languages.", "AI": {"tldr": "This paper presents IdiomCE, an adaptive GNN-based method that improves idiomatic translations by learning intricate mappings between multi-word expressions and idioms.", "motivation": "Translating multi-word expressions and idioms requires understanding cultural nuances and context, which traditional methods struggle with.", "method": "The authors propose IdiomCE, a graph neural network that effectively generalizes mappings between idiomatic expressions for better translation.", "result": "IdiomCE demonstrates significant improvements in idiomatic translation quality across multiple datasets, even for smaller models.", "conclusion": "The methodology offers a solution to enhance idiomatic translation especially in resource-constrained settings.", "key_contributions": ["Introduction of IdiomCE, an adaptive GNN for idiomatic expressions translation", "Demonstrated significant improvements in translation quality", "Effective in resource-constrained environments"], "limitations": "", "keywords": ["graph neural networks", "idiomatic translation", "multi-word expressions", "machine translation", "cultural nuances"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2404.06762", "pdf": "https://arxiv.org/pdf/2404.06762.pdf", "abs": "https://arxiv.org/abs/2404.06762", "title": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Geyu Lin", "Nancy F. Chen"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced\nlearning experience. The emergence of large language models (LLMs) further\nenables better human-machine interaction, and facilitates the development of\nconversational ITSs in various disciplines such as math and language learning.\nIn dialogic teaching, recognizing and adapting to individual characteristics\ncan significantly enhance student engagement and learning efficiency. However,\ncharacterizing and simulating student's persona remain challenging in training\nand evaluating conversational ITSs. In this work, we propose a framework to\nconstruct profiles of different student groups by refining and integrating both\ncognitive and noncognitive aspects, and leverage LLMs for personality-aware\nstudent simulation in a language learning scenario. We further enhance the\nframework with multi-aspect validation, and conduct extensive analysis from\nboth teacher and student perspectives. Our experimental results show that\nstate-of-the-art LLMs can produce diverse student responses according to the\ngiven language ability and personality traits, and trigger teacher's adaptive\nscaffolding strategies.", "AI": {"tldr": "This work proposes a framework to construct student profiles leveraging LLMs for personality-aware simulation in Intelligent Tutoring Systems, focusing on language learning.", "motivation": "To enhance engagement and efficiency in learning through personalized interaction in Intelligent Tutoring Systems by utilizing student characteristics.", "method": "The proposed framework integrates cognitive and noncognitive aspects to create student profiles and uses LLMs to simulate varied student responses based on personality traits and language abilities.", "result": "Experimental results demonstrate that LLMs can generate diverse responses that effectively trigger adaptive scaffolding strategies from teachers.", "conclusion": "The integration of LLMs in creating personality-aware simulations can lead to better engagement in language learning contexts within ITSs.", "key_contributions": ["Development of a framework for constructing personality-aware student profiles", "Integration of cognitive and noncognitive traits for refined simulation", "Validation of LLMs in generating adaptive responses from students"], "limitations": "", "keywords": ["Intelligent Tutoring Systems", "large language models", "student simulation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21940", "pdf": "https://arxiv.org/pdf/2505.21940.pdf", "abs": "https://arxiv.org/abs/2505.21940", "title": "RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering", "authors": ["Bolei He", "Xinran He", "Mengke Chen", "Xianwei Xue", "Ying Zhu", "Zhenhua Ling"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) excel in many areas but continue to face\nchallenges with complex reasoning tasks, such as Multi-Hop Question Answering\n(MHQA). MHQA requires integrating evidence from diverse sources while managing\nintricate logical dependencies, often leads to errors in reasoning.\nRetrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces\nchallenges in effectively filtering noisy data and retrieving all necessary\nevidence, thereby limiting its effectiveness in addressing MHQA challenges. To\naddress these challenges, we propose RISE:Reasoning Enhancement via Iterative\nSelf-Exploration, a novel framework designed to enhance models' reasoning\ncapability through iterative self-exploration. Specifically, RISE involves\nthree key steps in addressing MHQA tasks: question decomposition,\nretrieve-then-read, and self-critique. By leveraging continuous\nself-exploration, RISE identifies accurate reasoning paths, iteratively\nself-improving the model's capability to integrate evidence, maintain logical\nconsistency, and enhance performance in MHQA tasks. Extensive experiments on\nmultiple MHQA benchmarks demonstrate that RISE significantly improves reasoning\naccuracy and task performance.", "AI": {"tldr": "RISE is a novel framework that enhances reasoning in Multi-Hop Question Answering by iteratively exploring and improving the model's evidence integration and logical consistency.", "motivation": "Large Language Models struggle with complex reasoning tasks like Multi-Hop Question Answering due to challenges in evidence integration and logical dependencies.", "method": "RISE involves three steps: question decomposition, retrieve-then-read, and self-critique, which promote iterative self-exploration.", "result": "RISE shows significant improvements in reasoning accuracy and task performance on various Multi-Hop Question Answering benchmarks.", "conclusion": "The iterative self-exploration approach of RISE effectively enhances reasoning capabilities in LLMs for complex tasks.", "key_contributions": ["Introduction of the RISE framework for reasoning enhancement.", "Demonstration of significant improvements in MHQA task performance.", "Novel approach integrating self-critique with evidence retrieval."], "limitations": "", "keywords": ["Multi-Hop Question Answering", "Large Language Models", "Reasoning Enhancement"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21941", "pdf": "https://arxiv.org/pdf/2505.21941.pdf", "abs": "https://arxiv.org/abs/2505.21941", "title": "Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation", "authors": ["Ashim Gupta", "Vivek Srikumar"], "categories": ["cs.CL"], "comment": null, "summary": "Inference-time scaling via repeated sampling has shown promise in reasoning\ntasks, but its effectiveness in multilingual generation remains underexplored.\nWe evaluate this approach using perplexity- and reward-based verifiers on two\nmultilingual benchmarks: the Aya Evaluation Suite and m-ArenaHard. Our results\nshow consistent quality improvements, with gains exceeding 35% in some cases.\nWhile perplexity-based scoring is effective for open-ended prompts, only\nreward-based verifiers improve performance on tasks requiring reasoning (e.g.,\nmath, code). Our results demonstrate the broader utility of repeated sampling\nfor multilingual text generation and underscore the importance of selecting\nright verifiers for the task.", "AI": {"tldr": "This paper evaluates inference-time scaling via repeated sampling for multilingual text generation, finding significant quality improvements. It emphasizes the importance of verifier selection for specific tasks.", "motivation": "To explore the effectiveness of inference-time scaling through repeated sampling in multilingual generation tasks, which has been less studied compared to reasoning tasks.", "method": "The study utilizes perplexity- and reward-based verifiers to evaluate the quality of multilingual text generation on two benchmarks: the Aya Evaluation Suite and m-ArenaHard.", "result": "The research shows consistent quality improvements in multilingual generation, with gains exceeding 35% in certain cases; perplexity-based scoring works well for open-ended prompts, while reward-based verifiers improve performance on reasoning tasks.", "conclusion": "Repeated sampling demonstrates broader utility in improving the quality of multilingual text generation, highlighting the need for appropriate verifier selection based on the task.", "key_contributions": ["Evaluated inference-time scaling for multilingual generation.", "Demonstrated significant quality improvements with repeated sampling.", "Established the effectiveness of different verifier types for various task requirements."], "limitations": "", "keywords": ["multilingual generation", "repeated sampling", "perplexity", "reward-based verifiers", "text generation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.11190", "pdf": "https://arxiv.org/pdf/2502.11190.pdf", "abs": "https://arxiv.org/abs/2502.11190", "title": "ReLearn: Unlearning via Learning for Large Language Models", "authors": ["Haoming Xu", "Ningyuan Zhao", "Liming Yang", "Sendong Zhao", "Shumin Deng", "Mengru Wang", "Bryan Hooi", "Nay Oo", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "ACL 2025", "summary": "Current unlearning methods for large language models usually rely on reverse\noptimization to reduce target token probabilities. However, this paradigm\ndisrupts the subsequent tokens prediction, degrading model performance and\nlinguistic coherence. Moreover, existing evaluation metrics overemphasize\ncontextual forgetting while inadequately assessing response fluency and\nrelevance. To address these challenges, we propose ReLearn, a data augmentation\nand fine-tuning pipeline for effective unlearning, along with a comprehensive\nevaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)\nand Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and\nLinguistic Score (LS) to evaluate generation quality. Our experiments show that\nReLearn successfully achieves targeted forgetting while preserving high-quality\noutput. Through mechanistic analysis, we further demonstrate how reverse\noptimization disrupts coherent text generation, while ReLearn preserves this\nessential capability. Code is available at https://github.com/zjunlp/unlearn.", "AI": {"tldr": "ReLearn addresses unlearning in large language models by utilizing data augmentation and a new evaluation framework to maintain output quality while achieving targeted forgetting.", "motivation": "To overcome the limitations of existing unlearning methods that degrade model performance and linguistic coherence.", "method": "ReLearn employs a data augmentation and fine-tuning pipeline in conjunction with new metrics for evaluating knowledge preservation and generation quality.", "result": "Experiments demonstrate that ReLearn facilitates effective targeted forgetting without sacrificing output quality, substantiating the retention of coherent text generation.", "conclusion": "ReLearn presents a viable approach to unlearning in language models, proving that targeted knowledge forgetfulness can be achieved while maintaining high-generation standards.", "key_contributions": ["Introduces a novel data augmentation and fine-tuning pipeline for unlearning", "Develops evaluation metrics: Knowledge Forgetting Rate (KFR) and Linguistic Score (LS)", "Demonstrates improved performance in maintaining text coherence while enabling forgetting"], "limitations": "", "keywords": ["unlearning", "large language models", "data augmentation", "evaluation metrics", "knowledge retention"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.21958", "pdf": "https://arxiv.org/pdf/2505.21958.pdf", "abs": "https://arxiv.org/abs/2505.21958", "title": "Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning", "authors": ["Qihuang Zhong", "Liang Ding", "Fei Liao", "Juhua Liu", "Bo Du", "Dacheng Tao"], "categories": ["cs.CL"], "comment": null, "summary": "Domain-specific instruction-tuning has become the defacto standard for\nimproving the performance of large language models (LLMs) in specialized\napplications, e.g., medical question answering. Since the instruction-tuning\ndataset might contain redundant or low-quality data, data selection (DS) is\nusually required to maximize the data efficiency. Despite the successes in the\ngeneral domain, current DS methods often struggle to select the desired data\nfor domain-specific instruction-tuning. One of the main reasons is that they\nneglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'\npretrained knowledge and context knowledge of instruction data, which could\ndamage LLMs' prior abilities and lead to hallucination. To this end, we propose\na simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to\nselect the domain-specific instruction-tuning data that meets LLMs' actual\nneeds. The core of KDS is to leverage two knowledge-aware metrics for\nquantitatively measuring knowledge conflicts from two aspects: context-memory\nknowledge alignment and intra-memory knowledge consistency. By filtering the\ndata with large knowledge conflicts and sampling the high-quality and diverse\ndata, KDS can effectively stimulate the LLMs' abilities and achieve better\ndomain-specific performance. Taking the medical domain as the testbed, we\nconduct extensive experiments and empirically prove that KDS surpasses the\nother baselines and brings significant and consistent performance gains among\nall LLMs. More encouragingly, KDS effectively improves the model generalization\nand alleviates the hallucination problem.", "AI": {"tldr": "Knowledge-aware Data Selection (KDS) improves data selection for domain-specific instruction-tuning of LLMs by addressing knowledge conflicts, leading to better performance and reduced hallucination in medical applications.", "motivation": "Current data selection methods for domain-specific instruction-tuning of LLMs struggle due to neglecting knowledge conflicts between pretrained knowledge and contextual knowledge, which can harm LLM performance.", "method": "The KDS framework employs two knowledge-aware metrics to measure knowledge conflicts: context-memory knowledge alignment and intra-memory knowledge consistency, filtering out data with high conflicts while sampling diverse, high-quality data.", "result": "KDS outperforms existing baselines in domain-specific performance within the medical domain, demonstrating significant gains and improving model generalization while alleviating hallucination issues.", "conclusion": "The KDS framework is an effective approach to enhance the efficiency of data selection for LLMs in specialized applications, particularly in healthcare, by mitigating knowledge conflicts.", "key_contributions": ["Introduction of the KDS framework for effective data selection", "Use of knowledge-aware metrics to measure knowledge conflicts", "Demonstrated significant performance improvements in medical LLM applications"], "limitations": "", "keywords": ["Domain-specific instruction-tuning", "Knowledge-aware Data Selection", "Large Language Models", "Medical applications", "Data conflicts"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21963", "pdf": "https://arxiv.org/pdf/2505.21963.pdf", "abs": "https://arxiv.org/abs/2505.21963", "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents", "authors": ["Taro Yano", "Yoichi Ishibashi", "Masafumi Oyamada"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\na wide range of tasks. To further tailor LLMs to specific domains or\napplications, post-training techniques such as Supervised Fine-Tuning (SFT),\nPreference Learning, and model merging are commonly employed. While each of\nthese methods has been extensively studied in isolation, the automated\nconstruction of complete post-training pipelines remains an underexplored area.\nExisting approaches typically rely on manual design or focus narrowly on\noptimizing individual components, such as data ordering or merging strategies.\nIn this work, we introduce LaMDAgent (short for Language Model Developing\nAgent), a novel framework that autonomously constructs and optimizes full\npost-training pipelines through the use of LLM-based agents. LaMDAgent\nsystematically explores diverse model generation techniques, datasets, and\nhyperparameter configurations, leveraging task-based feedback to discover\nhigh-performing pipelines with minimal human intervention. Our experiments show\nthat LaMDAgent improves tool-use accuracy by 9.0 points while preserving\ninstruction-following capabilities. Moreover, it uncovers effective\npost-training strategies that are often overlooked by conventional human-driven\nexploration. We further analyze the impact of data and model size scaling to\nreduce computational costs on the exploration, finding that model size scalings\nintroduces new challenges, whereas scaling data size enables cost-effective\npipeline discovery.", "AI": {"tldr": "LaMDAgent is a framework that autonomously generates and optimizes post-training pipelines for Large Language Models, leveraging LLM-based agents for enhanced performance with minimal human intervention.", "motivation": "To automate the construction and optimization of post-training pipelines for LLMs, addressing the limitations of current manual and narrow optimization approaches.", "method": "LaMDAgent uses LLM-based agents to systematically explore model generation techniques, datasets, and hyperparameter configurations while utilizing task-based feedback.", "result": "LaMDAgent improves tool-use accuracy by 9.0 points and identifies overlooked effective post-training strategies, while analyzing data and model size scaling impacts.", "conclusion": "The framework enables more efficient and effective pipeline discovery, revealing new challenges and solutions in model size scaling and data utilization.", "key_contributions": ["Introduction of LaMDAgent framework for automating post-training pipeline construction", "Improvement of tool-use accuracy by 9.0 points", "Discovery of effective post-training strategies overlooked by traditional approaches"], "limitations": "", "keywords": ["Large Language Models", "post-training", "autonomous optimization", "pipeline construction", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21967", "pdf": "https://arxiv.org/pdf/2505.21967.pdf", "abs": "https://arxiv.org/abs/2505.21967", "title": "Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Large Vision-Language Models (LVLMs) have shown remarkable capabilities\nacross a wide range of multimodal tasks. However, their integration of visual\ninputs introduces expanded attack surfaces, thereby exposing them to novel\nsecurity vulnerabilities. In this work, we conduct a systematic\nrepresentational analysis to uncover why conventional adversarial attacks can\ncircumvent the safety mechanisms embedded in LVLMs. We further propose a novel\ntwo stage evaluation framework for adversarial attacks on LVLMs. The first\nstage differentiates among instruction non compliance, outright refusal, and\nsuccessful adversarial exploitation. The second stage quantifies the degree to\nwhich the model's output fulfills the harmful intent of the adversarial prompt,\nwhile categorizing refusal behavior into direct refusals, soft refusals, and\npartial refusals that remain inadvertently helpful. Finally, we introduce a\nnormative schema that defines idealized model behavior when confronted with\nharmful prompts, offering a principled target for safety alignment in\nmultimodal systems.", "AI": {"tldr": "This paper analyzes security vulnerabilities in Large Vision-Language Models (LVLMs) due to their multimodal nature and proposes a novel evaluation framework for assessing adversarial attacks on these models.", "motivation": "The integration of visual inputs in LVLMs introduces new security vulnerabilities that traditional safety mechanisms struggle to address, necessitating an in-depth analysis and new frameworks for evaluation.", "method": "A systematic representational analysis of LVLMs is conducted to understand adversarial attacks. The proposed framework involves two stages: the first categorizes types of compliance and refusals, and the second assesses the model's output in relation to harmful intent.", "result": "The analysis reveals why existing adversarial attacks can bypass LVLMs' safety mechanisms and categorizes the types of refusals into different behaviors, while providing a normative schema for ideal model responses.", "conclusion": "The work highlights the need for improved safety alignment mechanisms in LVLMs and offers a structured approach for evaluating their responses to adversarial prompts.", "key_contributions": ["Systematic representational analysis of LVLMs", "Two-stage evaluation framework for adversarial attacks", "Normative schema for safety alignment in multimodal systems"], "limitations": "", "keywords": ["Large Vision-Language Models", "adversarial attacks", "safety mechanisms"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.21979", "pdf": "https://arxiv.org/pdf/2505.21979.pdf", "abs": "https://arxiv.org/abs/2505.21979", "title": "Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset", "authors": ["Fakhraddin Alwajih", "Samar Mohamed Magdy", "Abdellah El Mekki", "Omer Nacar", "Youssef Nafea", "Safaa Taher Abdelfadil", "Abdulfattah Mohammed Yahya", "Hamzah Luqman", "Nada Almarwani", "Samah Aloufi", "Baraah Qawasmeh", "Houdaifa Atou", "Serry Sibaee", "Hamzah A. Alsayadi", "Walid Al-Dhabyani", "Maged S. Al-shaibani", "Aya El aatar", "Nour Qandos", "Rahaf Alhamouri", "Samar Ahmad", "Razan Khassib", "Lina Hamad", "Mohammed Anwar AL-Ghrawi", "Fatimah Alshamari", "Cheikh Malainine", "Doaa Qawasmeh", "Aminetou Yacoub", "Tfeil moilid", "Ruwa AbuHweidi", "Ahmed Aboeitta", "Vatimetou Mohamed Lemin", "Reem Abdel-Salam", "Ahlam Bashiti", "Adel Ammar", "Aisha Alansari", "Ahmed Ashraf", "Nora Alturayeif", "Sara Shatnawi", "Alcides Alcoba Inciarte", "AbdelRahim A. Elmadany", "Mohamedou cheikh tourad", "Ismail Berrada", "Mustafa Jarrar", "Shady Shehata", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": "https://github.com/UBC-NLP/pearl", "summary": "Mainstream large vision-language models (LVLMs) inherently encode cultural\nbiases, highlighting the need for diverse multimodal datasets. To address this\ngap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark\nexplicitly designed for cultural understanding. Constructed through advanced\nagentic workflows and extensive human-in-the-loop annotations by 45 annotators\nfrom across the Arab world, Pearl comprises over K multimodal examples spanning\nten culturally significant domains covering all Arab countries. We further\nprovide two robust evaluation benchmarks Pearl and Pearl-Lite along with a\nspecialized subset Pearl-X explicitly developed to assess nuanced cultural\nvariations. Comprehensive evaluations on state-of-the-art open and proprietary\nLVLMs demonstrate that reasoning-centric instruction alignment substantially\nimproves models' cultural grounding compared to conventional scaling methods.\nPearl establishes a foundational resource for advancing culturally-informed\nmultimodal modeling research. All datasets and benchmarks are publicly\navailable.", "AI": {"tldr": "Introduction of a large-scale Arabic multimodal dataset called Pearl for cultural understanding in LVLMs.", "motivation": "To address the cultural biases inherent in mainstream large vision-language models by providing a diverse and representative multimodal dataset.", "method": "Constructed using agentic workflows and extensive human-in-the-loop annotations by 45 annotators from the Arab world, covering ten culturally significant domains.", "result": "Pearl improves cultural grounding in LVLMs through reasoning-centric instruction alignment, outperforming conventional scaling methods in evaluations.", "conclusion": "Pearl serves as a foundational resource for advancing culturally-aware multimodal modeling and is publicly available.", "key_contributions": ["Introduction of the Pearl dataset for Arabic cultural understanding", "Development of two evaluation benchmarks (Pearl and Pearl-Lite)", "Creation of the Pearl-X subset for assessing nuanced cultural variations"], "limitations": "", "keywords": ["cultural bias", "multimodal dataset", "vision-language models", "Arabic", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.21997", "pdf": "https://arxiv.org/pdf/2505.21997.pdf", "abs": "https://arxiv.org/abs/2505.21997", "title": "Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data", "authors": ["Jihong Zhang", "Xinya Liang", "Anqi Deng", "Nicole Bonge", "Lin Tan", "Ling Zhang", "Nicole Zarrett"], "categories": ["cs.CL"], "comment": null, "summary": "Mixed methods research integrates quantitative and qualitative data but faces\nchallenges in aligning their distinct structures, particularly in examining\nmeasurement characteristics and individual response patterns. Advances in large\nlanguage models (LLMs) offer promising solutions by generating synthetic survey\nresponses informed by qualitative data. This study investigates whether LLMs,\nguided by personal interviews, can reliably predict human survey responses,\nusing the Behavioral Regulations in Exercise Questionnaire (BREQ) and\ninterviews from after-school program staff as a case study. Results indicate\nthat LLMs capture overall response patterns but exhibit lower variability than\nhumans. Incorporating interview data improves response diversity for some\nmodels (e.g., Claude, GPT), while well-crafted prompts and low-temperature\nsettings enhance alignment between LLM and human responses. Demographic\ninformation had less impact than interview content on alignment accuracy. These\nfindings underscore the potential of interview-informed LLMs to bridge\nqualitative and quantitative methodologies while revealing limitations in\nresponse variability, emotional interpretation, and psychometric fidelity.\nFuture research should refine prompt design, explore bias mitigation, and\noptimize model settings to enhance the validity of LLM-generated survey data in\nsocial science research.", "AI": {"tldr": "This study examines the ability of large language models (LLMs) to predict human survey responses informed by qualitative data, revealing insights into response patterns and variability.", "motivation": "To address the challenges in aligning quantitative and qualitative data in mixed methods research by leveraging the advancements in LLMs.", "method": "LLMs were guided by personal interview data to generate synthetic survey responses and their predictions were compared to actual survey responses using the Behavioral Regulations in Exercise Questionnaire (BREQ).", "result": "LLMs captured overall response patterns but showed less variability compared to human responses. Some models improved in response diversity when interview data was incorporated, while better prompts enhanced alignment with human responses.", "conclusion": "Interview-informed LLMs can bridge qualitative and quantitative methods, but challenges remain in variability, emotional interpretation, and psychometric integrity, indicating the need for further refinement in future research.", "key_contributions": ["Demonstrated the effectiveness of LLMs in predicting survey response patterns from qualitative data.", "Highlighting the importance of prompt design and model settings to enhance LLM alignment with human responses.", "Illustrated limitations in response variability and the need for bias mitigation in LLM-generated surveys."], "limitations": "Lower variability in predicted responses compared to humans, issues with emotional interpretation, and psychometric fidelity.", "keywords": ["large language models", "mixed methods research", "behavioral response patterns", "qualitative data analysis", "survey response prediction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21999", "pdf": "https://arxiv.org/pdf/2505.21999.pdf", "abs": "https://arxiv.org/abs/2505.21999", "title": "Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate", "authors": ["Ashim Gupta", "Maitrey Mehta", "Zhichao Xu", "Vivek Srikumar"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) provide detailed and impressive responses to\nqueries in English. However, are they really consistent at responding to the\nsame query in other languages? The popular way of evaluating for multilingual\nperformance of LLMs requires expensive-to-collect annotated datasets. Further,\nevaluating for tasks like open-ended generation, where multiple correct answers\nmay exist, is nontrivial. Instead, we propose to evaluate the predictability of\nmodel response across different languages. In this work, we propose a framework\nto evaluate LLM's cross-lingual consistency based on a simple Translate then\nEvaluate strategy. We instantiate this evaluation framework along two\ndimensions of consistency: information and empathy. Our results reveal\npronounced inconsistencies in popular LLM responses across thirty languages,\nwith severe performance deficits in certain language families and scripts,\nunderscoring critical weaknesses in their multilingual capabilities. These\nfindings necessitate cross-lingual evaluations that are consistent along\nmultiple dimensions. We invite practitioners to use our framework for future\nmultilingual LLM benchmarking.", "AI": {"tldr": "The paper proposes a framework for evaluating the cross-lingual consistency of large language models (LLMs) using a Translate then Evaluate strategy, revealing significant inconsistencies in LLM responses across multiple languages.", "motivation": "To address the challenge of evaluating multilingual performance of LLMs without relying on costly annotated datasets and to better understand their consistency across languages in terms of information and empathy.", "method": "The authors introduce a framework that evaluates LLMs' consistency across different languages based on a Translate then Evaluate strategy, focusing on two aspects: information consistency and empathy consistency.", "result": "The evaluation reveals pronounced inconsistencies in LLM responses across thirty languages, with notable performance deficits in specific language families and scripts.", "conclusion": "The findings highlight critical weaknesses in the multilingual capabilities of LLMs and advocate for comprehensive cross-lingual evaluations in future research.", "key_contributions": ["Proposes a novel framework for evaluating cross-lingual consistency in LLMs.", "Analyzes the multilingual performance of LLMs across thirty languages.", "Identifies severe performance deficits in certain language families."], "limitations": "The study focuses on specific dimensions (information and empathy) and may not encompass all aspects of LLM performance.", "keywords": ["large language models", "cross-lingual consistency", "evaluation framework", "machine learning", "multilingual capabilities"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22003", "pdf": "https://arxiv.org/pdf/2505.22003.pdf", "abs": "https://arxiv.org/abs/2505.22003", "title": "Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Ali Imam Abidi"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 5 tables, 4 figures. This is a revised version of a preprint\n  previously available at this URL: https://doi.org/10.21203/rs.3.rs-5351879/v1", "summary": "Pursuit of accessible legal assistance in India faces a critical gap, as many\ncitizens struggle to leverage their legal rights due to limited awareness and\naccess to relevant legal information. This paper introduces Legal Assist AI, a\ntransformer-based model designed to bridge this gap by offering effective legal\nassistance through large language models (LLMs). The system retrieves relevant\nlegal information from a curated database and generates accurate responses,\nenabling effective assistance for diverse users, including legal professionals,\nscholars, and the general public. The model was fine-tuned on extensive\ndatasets from the Indian legal domain, including Indian Constitution, Bharatiya\nNyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,\nproviding a robust understanding of the complexities of Indian law. By\nincorporating domain-specific legal datasets, the proposed model demonstrated\nremarkable efficiency and specialization in legal Question-Answering. The model\nwas evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral\n7B, achieving a 60.08% score on the AIBE, outperforming its competitors in\nlegal reasoning and accuracy. Unlike other models, Legal Assist AI avoided\ncommon issues such as hallucinations, making it highly reliable for practical\nlegal applications. It showcases the model's applicability in real-world legal\nscenarios, with future iterations aiming to enhance performance and expand its\ndataset to cover a broader range of multilingual and case-specific queries as\nwell.", "AI": {"tldr": "This paper presents Legal Assist AI, a transformer-based model that provides accessible legal assistance in India through large language models.", "motivation": "Many citizens in India struggle to leverage their legal rights due to limited awareness and access to relevant legal information.", "method": "Legal Assist AI retrieves relevant legal information from a curated database and generates accurate responses, fine-tuned on extensive datasets from the Indian legal domain.", "result": "The model achieved a score of 60.08% on the AIBE, outperforming state-of-the-art models like GPT-3.5 Turbo and Mistral 7B in legal reasoning and accuracy.", "conclusion": "Legal Assist AI is a reliable tool for practical legal applications, with plans for future iterations to enhance performance and expand its coverage.", "key_contributions": ["Introduction of transformer-based legal assistance", "Fine-tuning on Indian legal datasets", "Demonstrated superior performance in legal Question-Answering compared to existing models"], "limitations": "", "keywords": ["legal assistance", "large language models", "Indian legal domain", "Legal Assist AI", "legal reasoning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.22017", "pdf": "https://arxiv.org/pdf/2505.22017.pdf", "abs": "https://arxiv.org/abs/2505.22017", "title": "CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models", "authors": ["Siqi Fan", "Peng Han", "Shuo Shang", "Yequan Wang", "Aixin Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) benefit from increased test-time compute, a\nphenomenon known as test-time scaling. However, reasoning-optimized models\noften overthink even simple problems, producing excessively verbose outputs and\nleading to low token efficiency. By comparing these models with equally sized\ninstruct models, we identify two key causes of this verbosity: (1)\nreinforcement learning reduces the information density of forward reasoning,\nand (2) backward chain-of thought training encourages redundant and often\nunnecessary verification steps. Since LLMs cannot assess the difficulty of a\ngiven problem, they tend to apply the same cautious reasoning strategy across\nall tasks, resulting in inefficient overthinking. To address this, we propose\nCoThink, an embarrassingly simple pipeline: an instruct model first drafts a\nhigh-level solution outline; a reasoning model then works out the solution. We\nobserve that CoThink enables dynamic adjustment of reasoning depth based on\ninput difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and\nQwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token\ngeneration by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on\naverage. With reference to the instruct model, we formally define reasoning\nefficiency and observe a potential reasoning efficiency scaling law in LLMs.", "AI": {"tldr": "Proposes CoThink, a two-stage approach for LLMs to improve reasoning efficiency by dynamically adjusting output based on problem difficulty.", "motivation": "To improve the token efficiency of reasoning-optimized models that often produce excessively verbose outputs due to overthinking.", "method": "CoThink uses an instruct model to create a high-level outline of a solution, which is then refined by a reasoning model, allowing for dynamic adjustment of reasoning depth.", "result": "CoThink reduces total token generation by 22.3% while maintaining pass@1 accuracy within a 0.42% margin.", "conclusion": "The approach enhances reasoning efficiency in LLMs and suggests a potential scaling law for reasoning efficiency.", "key_contributions": ["Introduction of CoThink for improving reasoning efficiency in LLMs", "Demonstration of reduced token generation without losing accuracy", "Formal definition of reasoning efficiency in LLMs"], "limitations": "Limited to the three reasoning models and datasets evaluated.", "keywords": ["Large Language Models", "Reasoning Efficiency", "CoThink", "Token Generation", "Dynamic Reasoning Depth"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22018", "pdf": "https://arxiv.org/pdf/2505.22018.pdf", "abs": "https://arxiv.org/abs/2505.22018", "title": "Improving Continual Pre-training Through Seamless Data Packing", "authors": ["Ruicheng Yin", "Xuan Gao", "Changze Lv", "Xiaohua Wang", "Xiaoqing Zheng", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Continual pre-training has demonstrated significant potential in enhancing\nmodel performance, particularly in domain-specific scenarios. The most common\napproach for packing data before continual pre-training involves concatenating\ninput texts and splitting them into fixed-length sequences. While\nstraightforward and efficient, this method often leads to excessive truncation\nand context discontinuity, which can hinder model performance. To address these\nissues, we explore the potential of data engineering to enhance continual\npre-training, particularly its impact on model performance and efficiency. We\npropose Seamless Packing (SP), a novel data packing strategy aimed at\npreserving contextual information more effectively and enhancing model\nperformance. Our approach employs a sliding window technique in the first stage\nthat synchronizes overlapping tokens across consecutive sequences, ensuring\nbetter continuity and contextual coherence. In the second stage, we adopt a\nFirst-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger\nthan the target sequence length, thereby minimizing padding and truncation.\nEmpirical evaluations across various model architectures and corpus domains\ndemonstrate the effectiveness of our method, outperforming baseline method in\n99% of all settings. Code is available at\nhttps://github.com/Infernus-WIND/Seamless-Packing.", "AI": {"tldr": "This paper introduces Seamless Packing, a novel data packing strategy for continual pre-training that improves contextual coherence and model performance by effectively managing text sequences and minimizing truncation.", "motivation": "The motivation is to overcome the limitations of existing data packing methods in continual pre-training, which often lead to excessive truncation and context discontinuity.", "method": "The proposed Seamless Packing approach uses a sliding window technique to synchronize overlapping tokens across sequences and a First-Fit-Decreasing algorithm to minimize padding by optimally packing shorter texts.", "result": "Empirical evaluations show that the Seamless Packing method outperforms baseline methods in 99% of tested settings across various model architectures and domains.", "conclusion": "Seamless Packing significantly enhances the efficiency and performance of continual pre-training by preserving contextual information.", "key_contributions": ["Introduction of Seamless Packing for improved contextual coherence in continual pre-training.", "Utilization of a sliding window technique for token synchronization.", "Implementation of a packing algorithm to reduce truncation and padding."], "limitations": "", "keywords": ["Continual Pre-training", "Data Packing", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22019", "pdf": "https://arxiv.org/pdf/2505.22019.pdf", "abs": "https://arxiv.org/abs/2505.22019", "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "authors": ["Qiuchen Wang", "Ruixue Ding", "Yu Zeng", "Zehui Chen", "Lin Chen", "Shihang Wang", "Pengjun Xie", "Fei Huang", "Feng Zhao"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\n\\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.", "AI": {"tldr": "Introducing VRAG-RL, a reinforcement learning framework for complex reasoning across visually rich information, addressing limitations in existing RAG methods.", "motivation": "Traditional RAG methods struggle with visually rich data, leading to ineffective information retrieval and reasoning.", "method": "A reinforcement learning framework called VRAG-RL that allows visual language models to interact with search engines and optimize their performance through specially designed action spaces and a novel reward structure.", "result": "VRAG-RL enables more effective reasoning by utilizing visual perception tokens and optimizing VLMs based on user inquiries and retrieval performance, overcoming challenges of existing multi-modal RAG approaches.", "conclusion": "VRAG-RL represents a significant advancement in integrating visual reasoning into retrieval-augmented generation, with implications for real-world applications in AI and HCI.", "key_contributions": ["Novel RL framework for visual reasoning in RAG tasks", "Tailored action space for visually rich inputs", "Integration of query rewriting with retrieval performance for model optimization"], "limitations": "", "keywords": ["RAG", "reinforcement learning", "visual reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22037", "pdf": "https://arxiv.org/pdf/2505.22037.pdf", "abs": "https://arxiv.org/abs/2505.22037", "title": "Jailbreak Distillation: Renewable Safety Benchmarking", "authors": ["Jingyu Zhang", "Ahmed Elgohary", "Xiawei Wang", "A S M Iftekhar", "Ahmed Magooda", "Benjamin Van Durme", "Daniel Khashabi", "Kyle Jackson"], "categories": ["cs.CL", "cs.CR", "cs.SE"], "comment": "Project page: https://aka.ms/jailbreak-distillation", "summary": "Large language models (LLMs) are rapidly deployed in critical applications,\nraising urgent needs for robust safety benchmarking. We propose Jailbreak\nDistillation (JBDistill), a novel benchmark construction framework that\n\"distills\" jailbreak attacks into high-quality and easily-updatable safety\nbenchmarks. JBDistill utilizes a small set of development models and existing\njailbreak attack algorithms to create a candidate prompt pool, then employs\nprompt selection algorithms to identify an effective subset of prompts as\nsafety benchmarks. JBDistill addresses challenges in existing safety\nevaluation: the use of consistent evaluation prompts across models ensures fair\ncomparisons and reproducibility. It requires minimal human effort to rerun the\nJBDistill pipeline and produce updated benchmarks, alleviating concerns on\nsaturation and contamination. Extensive experiments demonstrate our benchmarks\ngeneralize robustly to 13 diverse evaluation models held out from benchmark\nconstruction, including proprietary, specialized, and newer-generation LLMs,\nsignificantly outperforming existing safety benchmarks in effectiveness while\nmaintaining high separability and diversity. Our framework thus provides an\neffective, sustainable, and adaptable solution for streamlining safety\nevaluation.", "AI": {"tldr": "Jailbreak Distillation (JBDistill) creates robust, easily-updated safety benchmarks for large language models by distilling jailbreak attack prompts, ensuring fair evaluation and reproducibility across various models.", "motivation": "The rapid deployment of large language models in critical applications necessitates effective safety benchmarking to address potential vulnerabilities.", "method": "JBDistill constructs benchmarks by utilizing development models and existing jailbreak attack algorithms to form a candidate prompt pool, followed by prompt selection to identify effective safety benchmarks.", "result": "JBDistill's benchmarks show significant improvement over existing safety benchmarks, generalizing robustly to diverse LLMs while maintaining high separability and diversity.", "conclusion": "JBDistill provides a sustainable solution for safety evaluation, requiring minimal human effort for updates and addressing challenges in current safety benchmarking practices.", "key_contributions": ["Introduction of Jailbreak Distillation (JBDistill) framework for creating safety benchmarks", "Demonstration of benchmarks generalizing robustly across diverse LLMs", "Significant performance improvement over existing benchmarks for safety evaluation"], "limitations": "", "keywords": ["Jailbreak Distillation", "large language models", "safety benchmarking", "machine learning", "AI safety"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.22054", "pdf": "https://arxiv.org/pdf/2505.22054.pdf", "abs": "https://arxiv.org/abs/2505.22054", "title": "Voice Adaptation for Swiss German", "authors": ["Samuel Stucki", "Jan Deriu", "Mark Cieliebak"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech", "summary": "This work investigates the performance of Voice Adaptation models for Swiss\nGerman dialects, i.e., translating Standard German text to Swiss German dialect\nspeech. For this, we preprocess a large dataset of Swiss podcasts, which we\nautomatically transcribe and annotate with dialect classes, yielding\napproximately 5000 hours of weakly labeled training material. We fine-tune the\nXTTSv2 model on this dataset and show that it achieves good scores in human and\nautomated evaluations and can correctly render the desired dialect. Our work\nshows a step towards adapting Voice Cloning technology to underrepresented\nlanguages. The resulting model achieves CMOS scores of up to -0.28 and SMOS\nscores of 3.8.", "AI": {"tldr": "Investigation of Voice Adaptation models for Swiss German dialects using a fine-tuned XTTSv2 model on 5000 hours of weakly labeled training data, showing good performance in dialect rendering.", "motivation": "To adapt Voice Cloning technology to underrepresented languages, specifically Swiss German dialects.", "method": "Preprocessing a large dataset of Swiss podcasts, transcribing and annotating it with dialect classes, then fine-tuning the XTTSv2 model on this dataset.", "result": "The fine-tuned XTTSv2 model achieved good scores in both human and automated evaluations, with CMOS scores of up to -0.28 and SMOS scores of 3.8.", "conclusion": "This work represents progress in adapting voice cloning for Swiss German dialects, contributing to underrepresented language technology.", "key_contributions": ["Introduction of a large dataset of Swiss podcasts for dialect adaptation", "Successful fine-tuning of the XTTSv2 model for Swiss German", "Achievement of competitive scores in dialect representation."], "limitations": "", "keywords": ["Voice Cloning", "Swiss German Dialects", "XTTSv2", "Speech Synthesis", "Dialect Adaptation"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.22061", "pdf": "https://arxiv.org/pdf/2505.22061.pdf", "abs": "https://arxiv.org/abs/2505.22061", "title": "Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?", "authors": ["Yujin Choi", "Youngjoo Park", "Junyoung Byun", "Jaewook Lee", "Jinseong Park"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) mitigates the hallucination problem in\nlarge language models (LLMs) and has proven effective for specific,\npersonalized applications. However, passing private retrieved documents\ndirectly to LLMs introduces vulnerability to membership inference attacks\n(MIAs), which try to determine whether the target datum exists in the private\nexternal database or not. Based on the insight that MIA queries typically\nexhibit high similarity to only one target document, we introduce Mirabel, a\nsimilarity-based MIA detection framework designed for the RAG system. With the\nproposed Mirabel, we show that simple detect-and-hide strategies can\nsuccessfully obfuscate attackers, maintain data utility, and remain\nsystem-agnostic. We experimentally prove its detection and defense against\nvarious state-of-the-art MIA methods and its adaptability to existing private\nRAG systems.", "AI": {"tldr": "Introduces Mirabel, a detection framework for membership inference attacks in retrieval-augmented generation systems.", "motivation": "To address the vulnerability of retrieval-augmented generation systems to membership inference attacks when private documents are passed to large language models.", "method": "Developed a similarity-based detection framework, Mirabel, which utilizes detect-and-hide strategies to obfuscate attackers while maintaining data utility.", "result": "Experimental results demonstrate Mirabel's effectiveness in detecting and defending against various state-of-the-art membership inference attack methods.", "conclusion": "Mirabel showcases a practical solution for enhancing privacy in retrieval-augmented generation systems without adversely affecting their performance.", "key_contributions": ["Introduction of Mirabel, a similarity-based MIA detection framework.", "Proven effectiveness of detect-and-hide strategies in obfuscating attackers.", "Demonstrated adaptability of Mirabel to existing private RAG systems."], "limitations": "", "keywords": ["retrieval-augmented generation", "membership inference attacks", "privacy", "large language models", "Mirabel"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.22068", "pdf": "https://arxiv.org/pdf/2505.22068.pdf", "abs": "https://arxiv.org/abs/2505.22068", "title": "Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO", "authors": ["Ran Li", "Shimin Di", "Yuchen Liu", "Chen Jing", "Yu Qiu", "Lei Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Previous study suggest that powerful Large Language Models (LLMs) trained\nwith Reinforcement Learning with Verifiable Rewards (RLVR) only refines\nreasoning path without improving the reasoning capacity in math tasks while\nsupervised-finetuning(SFT) with distillation can. We study this from the view\nof Scientific information extraction (SciIE) where LLMs and reasoning LLMs\nunderperforms small Bert-based models. SciIE require both the reasoning and\nmemorization. We argue that both SFT and RLVR can refine the reasoning path and\nimprove reasoning capacity in a simple way based on SciIE. We propose two-stage\ntraining with 1. MimicSFT, using structured reasoning templates without needing\nhigh-quality chain-of-thought data, 2. R$^2$GRPO with relevance and\nrule-induced rewards. Experiments on scientific IE benchmarks show that both\nmethods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses\nbaseline LLMs and specialized supervised models in relation extraction. Our\ncode is available at https://github.com/ranlislz/R2GRPO.", "AI": {"tldr": "The study explores the effectiveness of two-stage training approaches in enhancing reasoning capacity in Large Language Models (LLMs) for scientific information extraction, outperforming traditional methods.", "motivation": "To address the limitations of LLMs and reasoning LLMs in scientific information extraction, particularly their underperformance compared to small Bert-based models.", "method": "We propose two new training approaches: 1. MimicSFT, which uses structured reasoning templates; 2. R^2GRPO, which employs relevance and rule-induced rewards. Both methods are aimed at refining reasoning paths and improving reasoning capacity.", "result": "Experiments demonstrate that both MimicSFT and R^2GRPO improve reasoning capacity, with R^2GRPO combined with MimicSFT outperforming baseline LLMs and specialized models in relation extraction tasks.", "conclusion": "The proposed two-stage training framework effectively enhances reasoning capabilities in LLMs for scientific information extraction.", "key_contributions": ["Introduction of two-stage training methods for LLMs in scientific information extraction", "Outperformance of new methods over existing LLMs and specialized models", "Code availability for further research and implementation"], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Information Extraction", "Reasoning", "Supervised Fine-Tuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.22076", "pdf": "https://arxiv.org/pdf/2505.22076.pdf", "abs": "https://arxiv.org/abs/2505.22076", "title": "ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation", "authors": ["Maja Stahl", "Timon Ziegenbein", "Joonsuk Park", "Henning Wachsmuth"], "categories": ["cs.CL"], "comment": null, "summary": "Training large language models (LLMs) to follow instructions has\nsignificantly enhanced their ability to tackle unseen tasks. However, despite\ntheir strong generalization capabilities, instruction-following LLMs encounter\ndifficulties when dealing with tasks that require domain knowledge. This work\nintroduces a specialized instruction fine-tuning for the domain of\ncomputational argumentation (CA). The goal is to enable an LLM to effectively\ntackle any unseen CA tasks while preserving its generalization capabilities.\nReviewing existing CA research, we crafted natural language instructions for\n105 CA tasks to this end. On this basis, we developed a CA-specific benchmark\nfor LLMs that allows for a comprehensive evaluation of LLMs' capabilities in\nsolving various CA tasks. We synthesized 52k CA-related instructions, adapting\nthe self-instruct process to train a CA-specialized instruction-following LLM.\nOur experiments suggest that CA-specialized instruction fine-tuning\nsignificantly enhances the LLM on both seen and unseen CA tasks. At the same\ntime, performance on the general NLP tasks of the SuperNI benchmark remains\nstable.", "AI": {"tldr": "This paper presents fine-tuning large language models (LLMs) for computational argumentation tasks, enhancing their ability to generalize while performing domain-specific tasks.", "motivation": "Existing instruction-following LLMs struggle with tasks requiring domain knowledge, thus a specialized fine-tuning method is needed for computational argumentation (CA).", "method": "The authors create 105 CA tasks with natural language instructions and develop a CA-specific benchmark, training a CA-specialized LLM with adapted self-instruct processes.", "result": "The specialized fine-tuning method significantly improves the LLM's performance on both seen and unseen CA tasks, while maintaining stability on general NLP tasks.", "conclusion": "CA-specialized instruction fine-tuning effectively enhances the ability of LLMs to handle computational argumentation tasks without compromising their generalization capabilities.", "key_contributions": ["Introduction of CA-specific instruction fine-tuning for LLMs", "Development of a benchmark for evaluating LLMs on CA tasks", "Creation of 52k CA-related instructions for model training"], "limitations": "The study's focus is solely on computational argumentation; other domain-specific capabilities are not addressed.", "keywords": ["large language models", "instruction fine-tuning", "computational argumentation", "NLP tasks", "domain knowledge"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.22095", "pdf": "https://arxiv.org/pdf/2505.22095.pdf", "abs": "https://arxiv.org/abs/2505.22095", "title": "Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning", "authors": ["Chunyi Peng", "Zhipeng Xu", "Zhenghao Liu", "Yishan Li", "Yukun Yan", "Shuo Wang", "Zhiyuan Liu", "Yu Gu", "Minghe Yu", "Ge Yu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in\nmitigating hallucinations in Multimodal Large Language Models (MLLMs) by\nincorporating external knowledge during generation. Existing MRAG methods\ntypically adopt a static retrieval pipeline that fetches relevant information\nfrom multiple Knowledge Bases (KBs), followed by a refinement step. However,\nthese approaches overlook the reasoning and planning capabilities of MLLMs to\ndynamically determine how to interact with different KBs during the reasoning\nprocess. To address this limitation, we propose R1-Router, a novel MRAG\nframework that learns to decide when and where to retrieve knowledge based on\nthe evolving reasoning state. Specifically, R1-Router can generate follow-up\nqueries according to the current reasoning step, routing these intermediate\nqueries to the most suitable KB, and integrating external knowledge into a\ncoherent reasoning trajectory to answer the original query. Furthermore, we\nintroduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored\nreinforcement learning algorithm that assigns step-specific rewards to optimize\nthe reasoning behavior of MLLMs. Experimental results on various open-domain QA\nbenchmarks across multiple modalities demonstrate that R1-Router outperforms\nbaseline models by over 7%. Further analysis shows that R1-Router can\nadaptively and effectively leverage diverse KBs, reducing unnecessary\nretrievals and improving both efficiency and accuracy.", "AI": {"tldr": "R1-Router is a novel MRAG framework that dynamically retrieves knowledge based on the reasoning state in Multimodal Large Language Models, improving efficiency and accuracy.", "motivation": "Existing MRAG methods use static retrieval pipelines that do not leverage the dynamic reasoning capabilities of MLLMs, potentially missing out on effective knowledge integration.", "method": "R1-Router learns to route follow-up queries to the most suitable Knowledge Bases based on the reasoning process, while employing a reinforcement learning algorithm called Step-GRPO for optimizing reasoning behavior with step-specific rewards.", "result": "R1-Router outperforms baseline models by over 7% on various open-domain QA benchmarks across multiple modalities.", "conclusion": "The integration of dynamic retrieval strategies enhances the performance of MLLMs in answering queries accurately and efficiently.", "key_contributions": ["Introduction of R1-Router, a dynamic retrieval framework for MLLMs", "Development of Step-GRPO, a reinforcement learning algorithm for step-wise optimization", "Demonstrated effectiveness on open-domain QA benchmarks with substantial performance improvements"], "limitations": "", "keywords": ["Multimodal Retrieval", "Large Language Models", "Reinforcement Learning", "Knowledge Bases", "Dynamic Retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22096", "pdf": "https://arxiv.org/pdf/2505.22096.pdf", "abs": "https://arxiv.org/abs/2505.22096", "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL", "authors": ["Jinheon Baek", "Horst Samulowitz", "Oktie Hassanzadeh", "Dharmashankar Subramanian", "Sola Shirai", "Alfio Gliozzo", "Debarun Bhattacharjya"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Findings 2025", "summary": "Text-to-SQL aims to translate natural language queries into SQL statements,\nwhich is practical as it enables anyone to easily retrieve the desired\ninformation from databases. Recently, many existing approaches tackle this\nproblem with Large Language Models (LLMs), leveraging their strong capability\nin understanding user queries and generating corresponding SQL code. Yet, the\nparametric knowledge in LLMs might be limited to covering all the diverse and\ndomain-specific queries that require grounding in various database schemas,\nwhich makes generated SQLs less accurate oftentimes. To tackle this, we propose\nconstructing the knowledge base for text-to-SQL, a foundational source of\nknowledge, from which we retrieve and generate the necessary knowledge for\ngiven queries. In particular, unlike existing approaches that either manually\nannotate knowledge or generate only a few pieces of knowledge for each query,\nour knowledge base is comprehensive, which is constructed based on a\ncombination of all the available questions and their associated database\nschemas along with their relevant knowledge, and can be reused for unseen\ndatabases from different datasets and domains. We validate our approach on\nmultiple text-to-SQL datasets, considering both the overlapping and\nnon-overlapping database scenarios, where it outperforms relevant baselines\nsubstantially.", "AI": {"tldr": "This paper introduces a comprehensive knowledge base for improving the accuracy of text-to-SQL translations by leveraging Large Language Models (LLMs) and domain-specific queries.", "motivation": "To enhance the accuracy of SQL code generation from natural language queries by addressing the limitations of existing LLM approaches, which may not adequately cover diverse and domain-specific queries.", "method": "The authors constructed a comprehensive knowledge base that combines available questions, database schemas, and relevant knowledge. This knowledge base is designed to retrieve and generate information for a variety of queries and can be applied to unseen databases across different domains.", "result": "The proposed approach significantly outperforms existing baselines on multiple text-to-SQL datasets, showing improved accuracy in generating SQL statements, particularly in both overlapping and non-overlapping database scenarios.", "conclusion": "The comprehensive knowledge base facilitates better grounding of SQL generation in domain-specific queries, potentially leading to more accurate and versatile text-to-SQL solutions.", "key_contributions": ["Development of a comprehensive knowledge base for text-to-SQL enhancement", "Demonstration of improved performance on various datasets", "Application of the knowledge base to unseen databases across different domains"], "limitations": "", "keywords": ["Text-to-SQL", "Large Language Models", "Knowledge Base", "SQL Generation", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.22101", "pdf": "https://arxiv.org/pdf/2505.22101.pdf", "abs": "https://arxiv.org/abs/2505.22101", "title": "MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models", "authors": ["Zhiyu Li", "Shichao Song", "Hanyu Wang", "Simin Niu", "Ding Chen", "Jiawei Yang", "Chenyang Xi", "Huayi Lai", "Jihao Zhao", "Yezhaohui Wang", "Junpeng Ren", "Zehao Lin", "Jiahao Huo", "Tianyi Chen", "Kai Chen", "Kehang Li", "Zhiqiang Yin", "Qingchen Yu", "Bo Tang", "Hongkang Yang", "Zhi-Qin John Xu", "Feiyu Xiong"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as foundational infrastructure in\nthe pursuit of Artificial General Intelligence (AGI). Despite their remarkable\ncapabilities in language perception and generation, current LLMs fundamentally\nlack a unified and structured architecture for handling memory. They primarily\nrely on parametric memory (knowledge encoded in model weights) and ephemeral\nactivation memory (context-limited runtime states). While emerging methods like\nRetrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack\nlifecycle management and multi-modal integration, limiting their capacity for\nlong-term knowledge evolution. To address this, we introduce MemOS, a memory\noperating system designed for LLMs that, for the first time, elevates memory to\na first-class operational resource. It builds unified mechanisms for\nrepresentation, organization, and governance across three core memory types:\nparametric, activation, and plaintext. At its core is the MemCube, a\nstandardized memory abstraction that enables tracking, fusion, and migration of\nheterogeneous memory, while offering structured, traceable access across tasks\nand contexts. MemOS establishes a memory-centric execution framework with\nstrong controllability, adaptability, and evolvability. It fills a critical gap\nin current LLM infrastructure and lays the groundwork for continual adaptation,\npersonalized intelligence, and cross-platform coordination in next-generation\nintelligent systems.", "AI": {"tldr": "MemOS is a memory operating system for Large Language Models that enhances memory management, integrating parametric, activation, and plaintext memory types for improved long-term knowledge evolution.", "motivation": "Current LLMs lack a structured architecture for memory handling, relying too much on parametric and ephemeral activation memory which limits their adaptability and long-term knowledge management.", "method": "Introduces MemOS, a unified framework for memory management that includes the MemCube, enabling the organization of different memory types and their controlled access across tasks.", "result": "MemOS improves memory tracking, fusion, and migration, enabling LLMs to evolve knowledge, adapt intelligently, and coordinate across platforms effectively.", "conclusion": "MemOS fills a significant gap in LLM infrastructure, paving the way for ongoing adaptation and personalized intelligence in future intelligent systems.", "key_contributions": ["Introduction of MemOS as a memory operating system for LLMs", "Development of MemCube for unified memory management", "Establishment of a memory-centric execution framework"], "limitations": "", "keywords": ["Large Language Models", "Memory management", "Artificial General Intelligence"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.22107", "pdf": "https://arxiv.org/pdf/2505.22107.pdf", "abs": "https://arxiv.org/abs/2505.22107", "title": "Curse of High Dimensionality Issue in Transformer for Long-context Modeling", "authors": ["Shuhai Zhang", "Zeng You", "Yaofo Chen", "Zhiquan Wen", "Qianyue Wang", "Zhijie Qiu", "Yuanqing Li", "Mingkui Tan"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention.", "AI": {"tldr": "This paper introduces Dynamic Group Attention (DGA), a novel method that reduces computational redundancy in transformer-based models by optimizing attention through grouping less important tokens.", "motivation": "To address computational inefficiencies in long-context modeling of transformer models caused by redundant attention computations despite sparse attention weights.", "method": "Reformulates traditional probabilistic sequence modeling as a supervised learning task, theoretically analyzes attention sparsity, and formulates attention optimization as a linear coding problem, ultimately proposing the Dynamic Group Attention (DGA) strategy.", "result": "DGA significantly reduces computational costs while maintaining competitive performance against traditional attention mechanisms.", "conclusion": "The group coding strategy and DGA improve robustness to noise and efficiency in learning, providing a more efficient method for processing long-context sequences in LLMs.", "key_contributions": ["Introduction of Dynamic Group Attention (DGA) for optimizing attention calculations in LLMs.", "Theoretical analysis of attention sparsity and its implications for redundancy in computational resources.", "Empirical validation showing reduced costs while maintaining performance."], "limitations": "", "keywords": ["Dynamic Group Attention", "attention optimization", "natural language processing", "transformer models", "computational efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22113", "pdf": "https://arxiv.org/pdf/2505.22113.pdf", "abs": "https://arxiv.org/abs/2505.22113", "title": "THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models", "authors": ["Zhiyuan Li", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL"], "comment": "20 pages, 8 figures, 6 tables", "summary": "Large reasoning models (LRMs) have achieved impressive performance in complex\ntasks, often outperforming conventional large language models (LLMs). However,\nthe prevalent issue of overthinking severely limits their computational\nefficiency. Overthinking occurs when models generate excessive and redundant\ntokens that contribute little to accurate outcomes, especially in simple tasks,\nresulting in a significant waste of computational resources. To systematically\ninvestigate this issue, we introduce Think-Bench, a benchmark designed to\nevaluate the reasoning efficiency of LRMs. We also propose novel efficiency\nmetrics and conduct a comprehensive evaluation of various LRMs across multiple\ndimensions, including the reasoning process, outcome quality, and\nchain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs\nexhibit overthinking in handling easy questions, generating unnecessarily\nlengthy reasoning chains. While many LRMs demonstrate high CoT quality, several\nsuffer from low efficiency. We hope that Think-Bench can serve as a robust\nfoundation for advancing research into LRMs.", "AI": {"tldr": "The paper introduces Think-Bench, a benchmark for evaluating the reasoning efficiency of Large Reasoning Models (LRMs) and presents new metrics to assess their overthinking behavior in simple tasks.", "motivation": "The study aims to address the problem of overthinking in LRMs, which leads to inefficiencies in computational resources during reasoning tasks.", "method": "The authors developed Think-Bench, a benchmark specifically targeting reasoning efficiency, and proposed new metrics to evaluate LRMs based on their reasoning process, outcome quality, and chain-of-thought characteristics.", "result": "The evaluation shows that many LRMs tend to generate excessive reasoning chains for simple questions, indicating a tendency towards overthinking, despite producing high-quality chain-of-thought outputs.", "conclusion": "Think-Bench aims to provide a foundation for further research into improving the efficiency of LRMs and addressing the challenge of overthinking.", "key_contributions": ["Introduction of Think-Bench as a benchmark for reasoning efficiency.", "Novel efficiency metrics for evaluating LRMs.", "Comprehensive analysis of various LRMs' performance related to overthinking."], "limitations": "The benchmark needs further validation across wider LRM architectures and tasks beyond those tested in this paper.", "keywords": ["Large Reasoning Models", "Efficiency Metrics", "Benchmark", "Reasoning Process"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2505.22116", "pdf": "https://arxiv.org/pdf/2505.22116.pdf", "abs": "https://arxiv.org/abs/2505.22116", "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model", "authors": ["Jintao Zhang", "Zirui Liu", "Mingyue Cheng", "Shilong Zhang", "Tingyue Pan", "Qi Liu", "Yanhu Xie"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Intraoperative hypotension (IOH) frequently occurs under general anesthesia\nand is strongly linked to adverse outcomes such as myocardial injury and\nincreased mortality. Despite its significance, IOH prediction is hindered by\nevent sparsity and the challenge of integrating static and dynamic data across\ndiverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal\nlanguage model framework. To accurately identify and differentiate sparse\nhypotensive events, we leverage a two-stage training strategy. The first stage\ninvolves domain adaptive pretraining on IOH physiological time series augmented\nthrough diffusion methods, thereby enhancing the model sensitivity to patterns\nassociated with hypotension. Subsequently, task fine-tuning is performed on the\noriginal clinical dataset to further enhance the ability to distinguish\nnormotensive from hypotensive states. To enable multimodal fusion for each\npatient, we align structured clinical descriptions with the corresponding\nphysiological time series at the token level. Such alignment enables the model\nto capture individualized temporal patterns alongside their corresponding\nclinical semantics. In addition, we convert static patient attributes into\nstructured text to enrich personalized information. Experimental evaluations on\ntwo intraoperative datasets demonstrate that IOHFuseLM outperforms established\nbaselines in accurately identifying IOH events, highlighting its applicability\nin clinical decision support scenarios. Our code is publicly available to\npromote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.", "AI": {"tldr": "The paper introduces IOHFuseLM, a multimodal language model for predicting intraoperative hypotension (IOH) by integrating physiological time series and clinical data.", "motivation": "Intraoperative hypotension (IOH) is linked to serious complications but is difficult to predict due to event sparsity and data integration challenges.", "method": "A two-stage training strategy is employed: domain adaptive pretraining using IOH physiological data enriched by diffusion methods, followed by task fine-tuning on clinical datasets for event differentiation.", "result": "IOHFuseLM significantly outperforms established baselines in accurately identifying IOH events across two intraoperative datasets.", "conclusion": "The proposed framework enhances clinical decision support for IOH prediction by effectively combining multimodal patient data.", "key_contributions": ["Introduction of IOHFuseLM for IOH prediction", "Utilization of two-stage training with multimodal data", "Alignment of structured clinical data with physiological time series"], "limitations": "", "keywords": ["Intraoperative hypotension", "multimodal language model", "clinical decision support", "physiological time series", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.22118", "pdf": "https://arxiv.org/pdf/2505.22118.pdf", "abs": "https://arxiv.org/abs/2505.22118", "title": "Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches", "authors": ["Alan Ramponi", "Marco Rovera", "Robert Moro", "Sara Tonelli"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval of previously fact-checked claims is a well-established task, whose\nautomation can assist professional fact-checkers in the initial steps of\ninformation verification. Previous works have mostly tackled the task\nmonolingually, i.e., having both the input and the retrieved claims in the same\nlanguage. However, especially for languages with a limited availability of\nfact-checks and in case of global narratives, such as pandemics, wars, or\ninternational politics, it is crucial to be able to retrieve claims across\nlanguages. In this work, we examine strategies to improve the multilingual and\ncrosslingual performance, namely selection of negative examples (in the\nsupervised) and re-ranking (in the unsupervised setting). We evaluate all\napproaches on a dataset containing posts and claims in 47 languages (283\nlanguage combinations). We observe that the best results are obtained by using\nLLM-based re-ranking, followed by fine-tuning with negative examples sampled\nusing a sentence similarity-based strategy. Most importantly, we show that\ncrosslinguality is a setup with its own unique characteristics compared to the\nmultilingual setup.", "AI": {"tldr": "This paper explores multilingual and crosslingual retrieval of fact-checked claims, focusing on improving performance through negative example selection and re-ranking methods.", "motivation": "The motivation is to assist professional fact-checkers by automating the retrieval of claims in multiple languages, especially in contexts with limited fact-check availability.", "method": "The paper examines strategies for enhancing multilingual and crosslingual performance, specifically through supervised selection of negative examples and unsupervised re-ranking techniques, evaluated on a dataset of posts and claims in 47 languages.", "result": "The study finds that LLM-based re-ranking yields the best performance, followed by fine-tuning with a sentence similarity-based strategy for selecting negative examples.", "conclusion": "The results indicate that crosslingual retrieval presents distinct challenges compared to multilingual retrieval, highlighting the need for tailored approaches.", "key_contributions": ["Improved strategies for multilingual and crosslingual fact-checked claim retrieval", "Use of LLM-based re-ranking for enhanced performance", "Identification of unique characteristics in crosslingual retrieval compared to multilingual approaches."], "limitations": "", "keywords": ["fact-checking", "multilingual retrieval", "crosslingual retrieval", "LLM", "negative examples"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.22120", "pdf": "https://arxiv.org/pdf/2505.22120.pdf", "abs": "https://arxiv.org/abs/2505.22120", "title": "LoKI: Low-damage Knowledge Implanting of Large Language Models", "authors": ["Runyu Wang", "Peng Ping", "Zhengyu Guo", "Xiaoye Zhang", "Quan Shi", "Liting Zhou", "Tianbo Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning adapts pretrained models for specific tasks but poses the risk of\ncatastrophic forgetting (CF), where critical knowledge from pre-training is\noverwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large\nLanguage Models (LLMs), while efficient, often sacrifice general capabilities.\nTo address the issue of CF in a general-purpose PEFT framework, we propose\n\\textbf{Lo}w-damage \\textbf{K}nowledge \\textbf{I}mplanting (\\textbf{LoKI}), a\nPEFT technique that is based on a mechanistic understanding of how knowledge is\nstored in transformer architectures. In two real-world scenarios, LoKI\ndemonstrates task-specific performance that is comparable to or even surpasses\nthat of full fine-tuning and LoRA-based methods across various model types,\nwhile significantly better preserving general capabilities. Our work connects\nmechanistic insights into LLM knowledge storage with practical fine-tuning\nobjectives, achieving state-of-the-art trade-offs between task specialization\nand the preservation of general capabilities. Our implementation is publicly\navailable as ready-to-use code\\footnote{https://github.com/Nexround/LoKI}.", "AI": {"tldr": "Proposes Low-damage Knowledge Implanting (LoKI), a PEFT technique for LLMs that reduces catastrophic forgetting while maintaining general capabilities.", "motivation": "Address the issue of catastrophic forgetting in fine-tuning pretrained models, particularly for large language models, and improve task-specific performance without sacrificing general capabilities.", "method": "LoKI is a parameter-efficient fine-tuning technique based on a mechanistic understanding of knowledge storage in transformer architectures.", "result": "LoKI shows comparable or superior task-specific performance to full fine-tuning and LoRA methods across various models while better preserving general capabilities.", "conclusion": "The approach connects insights into LLM knowledge storage with practical fine-tuning objectives, achieving optimal trade-offs between specialization and general performance.", "key_contributions": ["Introduction of Low-damage Knowledge Implanting (LoKI) for LLM fine-tuning.", "Demonstration of improved performance metrics over existing PEFT methods while preventing catastrophic forgetting.", "Public availability of the implementation as ready-to-use code."], "limitations": "", "keywords": ["fine-tuning", "catastrophic forgetting", "parameter-efficient fine-tuning", "large language models", "knowledge storage"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.22131", "pdf": "https://arxiv.org/pdf/2505.22131.pdf", "abs": "https://arxiv.org/abs/2505.22131", "title": "EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning", "authors": ["Zhuoyang Wu", "Xinze Li", "Zhenghao Liu", "Yukun Yan", "Zhiyuan Liu", "Minghe Yu", "Cheng Yang", "Yu Gu", "Ge Yu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities\nand achieved promising results in mathematical problem-solving tasks. Learning\nfrom errors offers the potential to further enhance the performance of LLMs\nduring Supervised Fine-Tuning (SFT). However, the errors in synthesized\nsolutions are typically gathered from sampling trails, making it challenging to\ngenerate solution errors for each mathematical problem. This paper introduces\nthe Error-IndUced LEaRning (EULER) model, which aims to develop an error\nexposure model that generates high-quality solution errors to enhance the\nmathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the\nerror exposure model to increase the generation probability of self-made\nsolution errors while utilizing solutions produced by a superior LLM to\nregularize the generation quality. Our experiments across various mathematical\nproblem datasets demonstrate the effectiveness of the EULER model, achieving an\nimprovement of over 4% compared to all baseline models. Further analysis\nreveals that EULER is capable of synthesizing more challenging and educational\nsolution errors, which facilitate both the training and inference processes of\nLLMs. All codes are available at https://github.com/NEUIR/EULER.", "AI": {"tldr": "The EULER model enhances LLMs' mathematical problem-solving by generating high-quality solution errors during Supervised Fine-Tuning.", "motivation": "Improving the performance of LLMs in mathematical problem-solving through error learning.", "method": "Introducing the Error-IndUced LEaRning (EULER) model to generate solution errors and optimize error exposure for better learning outcomes.", "result": "The EULER model improves performance by over 4% compared to baseline models and synthesizes more challenging solution errors.", "conclusion": "EULER effectively facilitates training and inference in LLMs by generating high-quality errors.", "key_contributions": ["Development of the EULER model for generating solution errors", "Demonstrated improvement in LLM performance on mathematical tasks", "Synthesis of educationally valuable solution errors"], "limitations": "", "keywords": ["Large Language Models", "Error Learning", "Mathematical Reasoning", "Supervised Fine-Tuning", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.22135", "pdf": "https://arxiv.org/pdf/2505.22135.pdf", "abs": "https://arxiv.org/abs/2505.22135", "title": "RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding", "authors": ["Yuichiro Hoshino", "Hideyuki Tachibana", "Muneyoshi Inahara", "Hiroto Takegawa"], "categories": ["cs.CL", "cs.LG"], "comment": "26 pages", "summary": "Hybrid models combining Transformers and State Space Models (SSMs) are\npromising for balancing performance and efficiency. However, optimizing these\nhybrid models, particularly by addressing the potential redundancy inherent\nwithin the Transformer components, remains a significant challenge. In this\npaper, we propose RAD (Redundancy-Aware Distillation), a novel framework that\nuses self-speculative decoding as a diagnostic tool to identify redundant\nattention layers within the model. These identified layers are then selectively\nreplaced with SSM components, followed by targeted (self-)distillation.\nSpecifically, RAD focuses knowledge transfer on the components identified as\nredundant, considering architectural changes and specific weight initialization\nstrategies. We experimentally demonstrate that self-distillation using RAD\nsignificantly surpasses the performance of the original base model on\nmathematical and coding tasks. Furthermore, RAD is also effective in standard\nknowledge distillation settings, achieving up to approximately 2x faster\nconvergence compared to baseline methods. Notably, while a baseline model\ndistilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and\n22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and\n28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers\na new pathway for efficient optimization and performance enhancement in the\ndistillation of hybrid models.", "AI": {"tldr": "The paper introduces RAD, a framework for optimizing hybrid Transformer and State Space Models by identifying and replacing redundant layers, achieving significant performance improvements in mathematical and coding tasks.", "motivation": "To address redundancy in hybrid models consisting of Transformers and State Space Models, aiming to enhance their performance and efficiency.", "method": "The RAD framework utilizes self-speculative decoding to detect redundant attention layers, which are then replaced by SSM components followed by self-distillation focused on those components.", "result": "RAD significantly improves task performance, surpassing the original model, with faster convergence in knowledge distillation settings; it achieves higher scores on benchmarks using a smaller teacher model.", "conclusion": "RAD provides an effective method for optimizing hybrid models and enhancing performance in various tasks while maintaining efficiency.", "key_contributions": ["Introduction of the RAD framework for redundancy-aware optimization", "Demonstration of significant performance boosts in mathematical and coding tasks", "Achievement of faster convergence in knowledge distillation settings"], "limitations": "", "keywords": ["Hybrid Models", "Transformers", "State Space Models", "Self-Distillation", "Performance Optimization"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.22137", "pdf": "https://arxiv.org/pdf/2505.22137.pdf", "abs": "https://arxiv.org/abs/2505.22137", "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments", "authors": ["Marc Feger", "Katarina Boland", "Stefan Dietze"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to ACL 2025 and will be published after\n  27.07.2025", "summary": "Identifying arguments is a necessary prerequisite for various tasks in\nautomated discourse analysis, particularly within contexts such as political\ndebates, online discussions, and scientific reasoning. In addition to\ntheoretical advances in understanding the constitution of arguments, a\nsignificant body of research has emerged around practical argument mining,\nsupported by a growing number of publicly available datasets. On these\nbenchmarks, BERT-like transformers have consistently performed best,\nreinforcing the belief that such models are broadly applicable across diverse\ncontexts of debate. This study offers the first large-scale re-evaluation of\nsuch state-of-the-art models, with a specific focus on their ability to\ngeneralize in identifying arguments. We evaluate four transformers, three\nstandard and one enhanced with contrastive pre-training for better\ngeneralization, on 17 English sentence-level datasets as most relevant to the\ntask. Our findings show that, to varying degrees, these models tend to rely on\nlexical shortcuts tied to content words, suggesting that apparent progress may\noften be driven by dataset-specific cues rather than true task alignment. While\nthe models achieve strong results on familiar benchmarks, their performance\ndrops markedly when applied to unseen datasets. Nonetheless, incorporating both\ntask-specific pre-training and joint benchmark training proves effective in\nenhancing both robustness and generalization.", "AI": {"tldr": "This study re-evaluates BERT-like transformers for argument identification across various contexts, highlighting their generalization weaknesses and suggesting improvements through task-specific pre-training.", "motivation": "To advance automated discourse analysis and understand the effectiveness of argument mining models across diverse contexts.", "method": "Evaluation of four transformers on 17 sentence-level datasets, including one enhanced with contrastive pre-training, focusing on ability to generalize in argument identification.", "result": "Models demonstrated dependency on lexical shortcuts related to content words, performing well on familiar benchmarks but failing on unseen datasets.", "conclusion": "Task-specific pre-training and joint benchmark training effectively enhance model robustness and generalization.", "key_contributions": ["Large-scale re-evaluation of argument identification transformers", "Identification of performance drop on unseen datasets", "Recommendations for improving generalization in argument mining models"], "limitations": "Models rely on dataset-specific cues rather than true task alignment, raising concerns on robustness across diverse datasets.", "keywords": ["argument mining", "transformers", "BERT", "generalization", "discourse analysis"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.22156", "pdf": "https://arxiv.org/pdf/2505.22156.pdf", "abs": "https://arxiv.org/abs/2505.22156", "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing", "authors": ["Shuaiyi Li", "Zhisong Zhang", "Yang Deng", "Chenlong Deng", "Tianqing Fang", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Wai Lam"], "categories": ["cs.CL"], "comment": "Under review", "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.", "AI": {"tldr": "The paper introduces InComeS, a framework that enhances LLMs' editing capabilities by compressing context into a KV cache and using cross-attention to select relevant information, addressing the limitations of traditional model editing methods.", "motivation": "Existing model editing methods struggle with complex scenarios that require semantic understanding.", "method": "InComeS compresses editing contexts into a KV cache using a special gist token and employs cross-attention modules for dynamic selection of relevant information.", "result": "Experiments show that InComeS improves efficiency and effectiveness in model editing tasks across various benchmarks and formats.", "conclusion": "InComeS offers a promising solution for enhancing LLM context handling in editing scenarios, thereby overcoming the limitations of fixed context windows.", "key_contributions": ["Proposes a new framework, InComeS, for model editing.", "Utilizes a KV cache mechanism for better context management.", "Implements cross-attention for dynamic relevance selection."], "limitations": "The framework's performance may vary based on the specific characteristics of the edits.", "keywords": ["model editing", "large language models", "context encoding", "in-context learning", "cross-attention"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.22157", "pdf": "https://arxiv.org/pdf/2505.22157.pdf", "abs": "https://arxiv.org/abs/2505.22157", "title": "Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy", "authors": ["Paramita Mirza", "Lucas Weber", "Fabian Küch"], "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that post-training datasets for LLMs can be substantially\ndownsampled without noticeably deteriorating performance. However, data\nselection often incurs high computational costs or is limited to narrow\ndomains. In this paper, we demonstrate that data selection can be both --\nefficient and universal -- by using a multi-step pipeline in which we\nefficiently bin data points into groups, estimate quality using specialized\nmodels, and score difficulty with a robust, lightweight method. Task-based\ncategorization allows us to control the composition of our final data --\ncrucial for finetuning multi-purpose models. To guarantee diversity, we improve\nupon previous work using embedding models and a clustering algorithm. This\nintegrated strategy enables high-performance fine-tuning with minimal overhead.", "AI": {"tldr": "The paper presents a multi-step pipeline for efficient and universal data selection in fine-tuning large language models (LLMs), aimed at reducing computational costs while maintaining performance.", "motivation": "To reduce the computational costs of data selection for LLMs and to allow for a more diverse dataset while ensuring high performance.", "method": "A multi-step pipeline that bins data points, estimates quality with specialized models, and scores difficulty using a lightweight method. Task-based categorization is employed to control the dataset composition.", "result": "The proposed method allows for high-performance fine-tuning of LLMs with reduced computational overhead, ensuring a diverse and well-organized dataset for model training.", "conclusion": "The integrated strategy significantly improves data selection efficiency and model performance while minimizing costs and promoting diversity in the training data.", "key_contributions": ["Introduces an efficient multi-step pipeline for data selection.", "Demonstrates effective task-based categorization for dataset control.", "Implements a novel approach using embedding models and clustering algorithms to ensure diversity."], "limitations": "", "keywords": ["data selection", "fine-tuning", "large language models", "embedding models", "clustering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22165", "pdf": "https://arxiv.org/pdf/2505.22165.pdf", "abs": "https://arxiv.org/abs/2505.22165", "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes", "authors": ["Bocheng Li", "Zhujin Gao", "Linli Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Diffusion models have emerged as a promising approach for text generation,\nwith recent works falling into two main categories: discrete and continuous\ndiffusion models. Discrete diffusion models apply token corruption\nindependently using categorical distributions, allowing for different diffusion\nprogress across tokens but lacking fine-grained control. Continuous diffusion\nmodels map tokens to continuous spaces and apply fine-grained noise, but the\ndiffusion progress is uniform across tokens, limiting their ability to capture\nsemantic nuances. To address these limitations, we propose\n\\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous\nC\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models\n(NeoDiff), a novel diffusion model that integrates the strengths of both\ndiscrete and continuous approaches. NeoDiff introduces a Poisson diffusion\nprocess for the forward process, enabling a flexible and fine-grained noising\nparadigm, and employs a time predictor for the reverse process to adaptively\nmodulate the denoising progress based on token semantics. Furthermore, NeoDiff\nutilizes an optimized schedule for inference to ensure more precise noise\ncontrol and improved performance. Our approach unifies the theories of discrete\nand continuous diffusion models, offering a more principled and effective\nframework for text generation. Experimental results on several text generation\ntasks demonstrate NeoDiff's superior performance compared to baselines of\nnon-autoregressive continuous and discrete diffusion models, iterative-based\nmethods and autoregressive diffusion-based methods. These results highlight\nNeoDiff's potential as a powerful tool for generating high-quality text and\nadvancing the field of diffusion-based text generation.", "AI": {"tldr": "NeoDiff combines discrete and continuous diffusion models for improved text generation.", "motivation": "Existing diffusion models have limitations in token processing and semantic nuance capture, necessitating a more effective approach.", "method": "NeoDiff applies a Poisson diffusion process for flexible noising and uses a time predictor to adjust denoising based on token semantics, alongside an optimized inference schedule.", "result": "Experimental results show that NeoDiff outperforms non-autoregressive continuous and discrete diffusion models, iterative-based methods, and autoregressive diffusion-based methods in text generation tasks.", "conclusion": "NeoDiff provides a unified framework enhancing text generation quality and efficiency.", "key_contributions": ["Introduction of a Poisson diffusion process for flexibility in noising.", "Implementation of a time predictor for adaptively modulating denoising progress.", "Demonstration of superior performance compared to traditional models."], "limitations": "", "keywords": ["Diffusion Models", "Text Generation", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22169", "pdf": "https://arxiv.org/pdf/2505.22169.pdf", "abs": "https://arxiv.org/abs/2505.22169", "title": "ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments", "authors": ["Gili Lior", "Eliya Habba", "Shahar Levy", "Avi Caciularu", "Gabriel Stanovsky"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs are highly sensitive to prompt phrasing, yet standard benchmarks\ntypically report performance using a single prompt, raising concerns about the\nreliability of such evaluations. In this work, we argue for a stochastic method\nof moments evaluation over the space of meaning-preserving prompt\nperturbations. We introduce a formal definition of reliable evaluation that\naccounts for prompt sensitivity, and suggest ReliableEval - a method for\nestimating the number of prompt resamplings needed to obtain meaningful\nresults. Using our framework, we stochastically evaluate five frontier LLMs and\nfind that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit\nsubstantial prompt sensitivity. Our approach is model-, task-, and\nmetric-agnostic, offering a recipe for meaningful and robust LLM evaluation.", "AI": {"tldr": "This paper introduces ReliableEval, a framework for evaluating the sensitivity of LLMs to prompt perturbations, highlighting the necessity for reliable evaluations in light of prompt variability.", "motivation": "The motivation is to address concerns about the reliability of LLM evaluations that typically use single prompt benchmarks, which may not reflect true model performance due to prompt sensitivity.", "method": "The authors propose a stochastic method of moments evaluation and a formal definition of reliable evaluation, focusing on prompt perturbations that preserve meaning. They present ReliableEval to determine the number of prompt resamplings required for meaningful results.", "result": "The framework was applied to five leading LLMs, revealing that even top performers like GPT-4o and Claude-3.7-Sonnet show significant prompt sensitivity, indicating that traditional evaluation methods may underestimate variability in performance.", "conclusion": "The study concludes that their model-, task-, and metric-agnostic approach offers a robust way to evaluate LLMs, emphasizing the importance of incorporating prompt sensitivity in assessments.", "key_contributions": ["Introduction of ReliableEval for evaluating LLM prompt sensitivity", "Formal definition of reliable evaluation considering prompt perturbations", "Stochastic evaluation revealing sensitivity in top LLMs"], "limitations": "The approach may need further validation across more diverse tasks and metrics to strengthen its applicability.", "keywords": ["LLMs", "prompt sensitivity", "evaluation framework", "ReliableEval", "stochastic methods"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.22172", "pdf": "https://arxiv.org/pdf/2505.22172.pdf", "abs": "https://arxiv.org/abs/2505.22172", "title": "Reverse Preference Optimization for Complex Instruction Following", "authors": ["Xiang Huang", "Ting-En Lin", "Feiteng Fang", "Yuchuan Wu", "Hangyu Li", "Yuzhong Qu", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Instruction following (IF) is a critical capability for large language models\n(LLMs). However, handling complex instructions with multiple constraints\nremains challenging. Previous methods typically select preference pairs based\non the number of constraints they satisfy, introducing noise where chosen\nexamples may fail to follow some constraints and rejected examples may excel in\ncertain respects over the chosen ones. To address the challenge of aligning\nwith multiple preferences, we propose a simple yet effective method called\nReverse Preference Optimization (RPO). It mitigates noise in preference pairs\nby dynamically reversing the constraints within the instruction to ensure the\nchosen response is perfect, alleviating the burden of extensive sampling and\nfiltering to collect perfect responses. Besides, reversal also enlarges the gap\nbetween chosen and rejected responses, thereby clarifying the optimization\ndirection and making it more robust to noise. We evaluate RPO on two multi-turn\nIF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over\nthe DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.\nMoreover, RPO scales effectively across model sizes (8B to 70B parameters),\nwith the 70B RPO model surpassing GPT-4o.", "AI": {"tldr": "This paper presents Reverse Preference Optimization (RPO), a method that improves instruction following in large language models by dynamically reversing constraints to mitigate noise in preference pairs.", "motivation": "The motivation is to enhance the capability of large language models to follow complex instructions with multiple constraints, which has been challenging with existing methods.", "method": "The proposed method, Reverse Preference Optimization (RPO), reverses constraints within instructions to ensure chosen responses meet all requirements, while reducing noise in preference selection.", "result": "RPO demonstrated average improvements of 4.6 and 2.5 points over the DPO baseline on two multi-turn instruction following benchmarks, Sysbench and Multi-IF, respectively.", "conclusion": "RPO not only improves performance on instruction-following tasks but also scales effectively across different model sizes, achieving notable results with the largest model surpassing GPT-4o.", "key_contributions": ["Introduction of Reverse Preference Optimization (RPO) for instruction following in LLMs.", "Demonstrated improvements over existing methods with quantifiable results on benchmark datasets.", "Effective scaling of RPO across various model sizes from 8B to 70B parameters."], "limitations": "", "keywords": ["large language models", "instruction following", "preference optimization", "RPO", "AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.22176", "pdf": "https://arxiv.org/pdf/2505.22176.pdf", "abs": "https://arxiv.org/abs/2505.22176", "title": "TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation", "authors": ["Vihang Pancholi", "Jainit Bafna", "Tejas Anvekar", "Manish Shrivastava", "Vivek Gupta"], "categories": ["cs.CL"], "comment": "Accepeted for Findings at ACL 2025", "summary": "Evaluating tables qualitatively & quantitatively presents a significant\nchallenge, as traditional metrics often fail to capture nuanced structural and\ncontent discrepancies. To address this, we introduce a novel, methodical rubric\nintegrating multi-level structural descriptors with fine-grained contextual\nquantification, thereby establishing a robust foundation for comprehensive\ntable comparison. Building on this foundation, we propose TabXEval, an\neXhaustive and eXplainable two-phase evaluation framework. TabXEval initially\naligns reference tables structurally via TabAlign & subsequently conducts a\nsystematic semantic and syntactic comparison using TabCompare; this approach\nclarifies the evaluation process and pinpoints subtle discrepancies overlooked\nby conventional methods. The efficacy of this framework is assessed using\nTabXBench, a novel, diverse, multi-domain benchmark we developed, featuring\nrealistic table perturbations and human-annotated assessments. Finally, a\nsystematic analysis of existing evaluation methods through\nsensitivity-specificity trade-offs demonstrates the qualitative and\nquantitative effectiveness of TabXEval across diverse table-related tasks and\ndomains, paving the way for future innovations in explainable table evaluation.", "AI": {"tldr": "This paper introduces TabXEval, a comprehensive framework for evaluating tables through structural alignment and semantic comparison, addressing limitations of traditional evaluation metrics.", "motivation": "Traditional metrics for table evaluation often overlook structural and content discrepancies, necessitating a more nuanced approach for accurate assessments.", "method": "TabXEval employs a two-phase evaluation process with TabAlign for structural alignment and TabCompare for semantic and syntactic comparison.", "result": "The effectiveness of TabXEval was validated using TabXBench, a benchmark featuring realistic table perturbations and human-annotated assessments, showcasing better qualitative and quantitative evaluations than existing methods.", "conclusion": "TabXEval serves as a robust framework for table evaluation, facilitating clearer evaluations and identifying subtle discrepancies in various domains.", "key_contributions": ["Introduction of a novel evaluation rubric for table comparison", "Development of TabXEval framework for structured alignment and semantic comparison", "Creation of TabXBench benchmark for assessing table evaluation methods"], "limitations": "", "keywords": ["Table Evaluation", "Structural Alignment", "Semantic Comparison", "Qualitative Analysis", "Machine Learning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.22179", "pdf": "https://arxiv.org/pdf/2505.22179.pdf", "abs": "https://arxiv.org/abs/2505.22179", "title": "Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design", "authors": ["Yudi Zhang", "Weilin Zhao", "Xu Han", "Tiejun Zhao", "Wang Xu", "Hailong Cao", "Conghui Zhu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Speculative decoding and quantization effectively accelerate memory-bound\ninference of large language models. Speculative decoding mitigates the memory\nbandwidth bottleneck by verifying multiple tokens within a single forward pass,\nwhich increases computational effort. Quantization achieves this optimization\nby compressing weights and activations into lower bit-widths and also reduces\ncomputations via low-bit matrix multiplications. To further leverage their\nstrengths, we investigate the integration of these two techniques.\nSurprisingly, experiments applying the advanced speculative decoding method\nEAGLE-2 to various quantized models reveal that the memory benefits from 4-bit\nweight quantization are diminished by the computational load from speculative\ndecoding. Specifically, verifying a tree-style draft incurs significantly more\ntime overhead than a single-token forward pass on 4-bit weight quantized\nmodels. This finding led to our new speculative decoding design: a hierarchical\nframework that employs a small model as an intermediate stage to turn\ntree-style drafts into sequence drafts, leveraging the memory access benefits\nof the target quantized model. Experimental results show that our hierarchical\napproach achieves a 2.78$\\times$ speedup across various tasks for the 4-bit\nweight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\\times$.\nCode available at https://github.com/AI9Stars/SpecMQuant.", "AI": {"tldr": "This paper investigates the integration of speculative decoding and quantization for accelerating inference in large language models, leading to a new hierarchical decoding framework that achieves substantial speedup.", "motivation": "To address the memory bandwidth bottleneck in large language model inference and to explore the integration of speculative decoding and quantization techniques for better performance.", "method": "The study combines speculative decoding via the EAGLE-2 method with 4-bit weight quantization, and introduces a hierarchical framework using a small model for enhancing memory access and processing efficiency.", "result": "The hierarchical approach provides a 2.78x speedup in inference time compared to EAGLE-2 on a 4-bit weight quantized Llama-3-70B model, demonstrating enhanced performance while mitigating computational load.", "conclusion": "The proposed hierarchical framework outperforms traditional methods by efficiently combining the strengths of speculative decoding and weight quantization.", "key_contributions": ["Introduction of a hierarchical speculative decoding framework", "Demonstrated effective integration of speculative decoding and quantization", "Achieved significant speedup in inference for large language models"], "limitations": "The study primarily focuses on Llama-3-70B models and may not generalize to all architectures or scenarios.", "keywords": ["speculative decoding", "quantization", "large language models", "inference speedup", "memory bandwidth"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.22184", "pdf": "https://arxiv.org/pdf/2505.22184.pdf", "abs": "https://arxiv.org/abs/2505.22184", "title": "Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon", "authors": ["Xuchen Ma", "Jianxiang Yu", "Wenming Shao", "Bo Pang", "Xiang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 5 figures, 9 tables", "summary": "Social media platforms have experienced a significant rise in toxic content,\nincluding abusive language and discriminatory remarks, presenting growing\nchallenges for content moderation. Some users evade censorship by deliberately\ndisguising toxic words through homophonic cloak, which necessitates the task of\nunveiling cloaked toxicity. Existing methods are mostly designed for English\ntexts, while Chinese cloaked toxicity unveiling has not been solved yet. To\ntackle the issue, we propose C$^2$TU, a novel training-free and prompt-free\nmethod for Chinese cloaked toxic content unveiling. It first employs substring\nmatching to identify candidate toxic words based on Chinese homo-graph and\ntoxic lexicon. Then it filters those candidates that are non-toxic and corrects\ncloaks to be their corresponding toxicities. Specifically, we develop two model\nvariants for filtering, which are based on BERT and LLMs, respectively. For\nLLMs, we address the auto-regressive limitation in computing word occurrence\nprobability and utilize the full semantic contexts of a text sequence to reveal\ncloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve\nsuperior performance on two Chinese toxic datasets. In particular, our method\noutperforms the best competitor by up to 71% on the F1 score and 35% on\naccuracy, respectively.", "AI": {"tldr": "C$^2$TU is a novel method for uncovering cloaked toxicity in Chinese social media, achieving significant performance improvements over existing methods.", "motivation": "The increase in toxic content on social media necessitates effective methods for content moderation, specifically targeting the unique challenges presented by cloaked toxicity in languages like Chinese.", "method": "C$^2$TU uses substring matching with a focus on recognizing Chinese homographs and a toxic lexicon to identify candidate toxic words, complemented by two model variants leveraging BERT and LLMs for filtering and correcting cloaked toxicity.", "result": "C$^2$TU demonstrates superior performance on two Chinese toxic datasets, outperforming competitors by up to 71% on F1 score and 35% on accuracy.", "conclusion": "The proposed method provides an effective solution for unveiling cloaked toxic content in Chinese, contributing to improved content moderation capabilities.", "key_contributions": ["Development of a training-free and prompt-free method for toxic content unveiling in Chinese.", "Utility of substring matching for identifying toxic words using homographs.", "Significant performance improvements compared to existing methods."], "limitations": "", "keywords": ["cloaked toxicity", "Chinese social media", "content moderation", "BERT", "LLM"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.22202", "pdf": "https://arxiv.org/pdf/2505.22202.pdf", "abs": "https://arxiv.org/abs/2505.22202", "title": "Let's Predict Sentence by Sentence", "authors": ["Hyeonbin Hwang", "Byeongguk Jeon", "Seungone Kim", "Jiyeon Kim", "Hoyeon Chang", "Sohee Yang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "categories": ["cs.CL", "cs.AI"], "comment": "Work In Progress", "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces.", "AI": {"tldr": "This paper explores adapting pretrained autoregressive language models to reason over structured semantic units in sentence space rather than raw tokens.", "motivation": "The work investigates whether language models can learn to reason using higher-level abstractions, contrasting human reasoning with token-level generation.", "method": "The authors present a framework that adapts a pretrained token-level language model to predict continuous embeddings of next sentences, exploring semantic and contextual embedding paradigms.", "result": "The study shows that contextual embeddings using continuous inference perform competitively with Chain-of-Thought reasoning while reducing inference-time computations by half across four domains.", "conclusion": "The results indicate that pretrained language models can effectively transition to abstract reasoning within latent embedding spaces, along with the introduction of a diagnostic tool, SentenceLens, for visualizing model states.", "key_contributions": ["Development of a framework for embedding-level reasoning in LMs", "Competitive performance of contextual embeddings with reduced inference costs", "Introduction of SentenceLens for interpreting model states"], "limitations": "The findings are based on early work and may require further validation and development.", "keywords": ["language models", "reasoning", "embedding spaces", "semantic embeddings", "contextual embeddings"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.22232", "pdf": "https://arxiv.org/pdf/2505.22232.pdf", "abs": "https://arxiv.org/abs/2505.22232", "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models", "authors": ["Mehdi Ali", "Manuel Brack", "Max Lübbering", "Elias Wendt", "Abbas Goher Khan", "Richard Rutmann", "Alex Jude", "Maurice Kraus", "Alexander Arno Weber", "Felix Stollenwerk", "David Kaczér", "Florian Mai", "Lucie Flek", "Rafet Sifa", "Nicolas Flores-Herr", "Joachim Köhler", "Patrick Schramowski", "Michael Fromm", "Kristian Kersting"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page available at https://huggingface.co/spaces/Jackal-AI/JQL", "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development.", "AI": {"tldr": "Introduction of JQL, an efficient systematic approach for curating high-quality multilingual datasets for pretraining large language models.", "motivation": "Address the limitations in the availability of open-source multilingual datasets for effectively pretraining LLMs, which rely on heuristic filtering methods.", "method": "JQL utilizes lightweight annotators based on pretrained multilingual embeddings to curate diverse and high-quality multilingual data efficiently.", "result": "Empirical evaluation shows that JQL outperforms current heuristic filtering methods, enhancing downstream model training quality and increasing data retention rates across 35 languages.", "conclusion": "JQL raises the standards of multilingual dataset development, providing practical insights and valuable resources for multilingual data curation.", "key_contributions": ["Introduction of a systematic approach for multilingual dataset curation", "Demonstrates robust multilingual performance for unseen languages", "Provides valuable resources for enhancing multilingual dataset quality"], "limitations": "", "keywords": ["multilingual datasets", "large language models", "data curation", "cross-lingual performance", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.22236", "pdf": "https://arxiv.org/pdf/2505.22236.pdf", "abs": "https://arxiv.org/abs/2505.22236", "title": "A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity", "authors": ["Charlotte Pouw", "Afra Alishahi", "Willem Zuidema"], "categories": ["cs.CL"], "comment": "Accepted to CoNLL 2025", "summary": "We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using\nmethods inspired by psycholinguistic research. Specifically, we focus on the\ngeneration of intonational phrase boundaries, which can often be predicted by\nidentifying syntactic boundaries within a sentence. We find that TTS systems\nstruggle to accurately generate intonational phrase boundaries in sentences\nwhere syntactic boundaries are ambiguous (e.g., garden path sentences or\nsentences with attachment ambiguity). In these cases, systems need superficial\ncues such as commas to place boundaries at the correct positions. In contrast,\nfor sentences with simpler syntactic structures, we find that systems do\nincorporate syntactic cues beyond surface markers. Finally, we finetune models\non sentences without commas at the syntactic boundary positions, encouraging\nthem to focus on more subtle linguistic cues. Our findings indicate that this\nleads to more distinct intonation patterns that better reflect the underlying\nstructure.", "AI": {"tldr": "Analysis of TTS systems' sensitivity to syntactic boundaries in sentence intonation.", "motivation": "To investigate how TTS systems generate intonational phrase boundaries and their reliance on syntactic cues.", "method": "Methods inspired by psycholinguistics to assess the performance of TTS systems on sentences with varying syntactic complexities.", "result": "TTS systems have difficulty with ambiguous syntactic boundaries and rely on superficial cues like commas; finetuning on sentences without commas improves intonation pattern.", "conclusion": "The finetuning approach enhances TTS systems' ability to reflect underlying sentence structure through distinct intonation patterns.", "key_contributions": ["Identified challenges TTS systems face with ambiguous syntactic boundaries.", "Demonstrated that superficial cues are often needed for intonation placement.", "Showcased a finetuning method that leads to improved intonation reflecting sentence structure."], "limitations": "The study's focus is on specific sentence types and does not cover all possible syntactic nuances.", "keywords": ["Text-to-Speech", "syntactic sensitivity", "intonational phrase boundaries", "psycholinguistics", "finetuning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.22240", "pdf": "https://arxiv.org/pdf/2505.22240.pdf", "abs": "https://arxiv.org/abs/2505.22240", "title": "BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain", "authors": ["Yunsoo Kim", "Yusuf Abdulle", "Honghan Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Biomedical reasoning often requires traversing interconnected relationships\nacross entities such as drugs, diseases, and proteins. Despite the increasing\nprominence of large language models (LLMs), existing benchmarks lack the\nability to evaluate multi-hop reasoning in the biomedical domain, particularly\nfor queries involving one-to-many and many-to-many relationships. This gap\nleaves the critical challenges of biomedical multi-hop reasoning underexplored.\nTo address this, we introduce BioHopR, a novel benchmark designed to evaluate\nmulti-hop, multi-answer reasoning in structured biomedical knowledge graphs.\nBuilt from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop\nreasoning tasks that reflect real-world biomedical complexities.\n  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary\nreasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on\n2-hop tasks, outperforming proprietary models such as GPT4O and open-source\nbiomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all\nmodels exhibit significant declines in multi-hop performance, underscoring the\nchallenges of resolving implicit reasoning steps in the biomedical domain. By\naddressing the lack of benchmarks for multi-hop reasoning in biomedical domain,\nBioHopR sets a new standard for evaluating reasoning capabilities and\nhighlights critical gaps between proprietary and open-source models while\npaving the way for future advancements in biomedical LLMs.", "AI": {"tldr": "Introducing BioHopR, a benchmark for evaluating multi-hop reasoning in biomedical knowledge graphs, revealing significant performance gaps in existing models.", "motivation": "To address the lack of benchmarks for multi-hop reasoning in the biomedical domain, particularly focusing on complex entity relationships.", "method": "Development of BioHopR, a benchmark leveraging the PrimeKG dataset, featuring tasks for 1-hop and 2-hop reasoning to reflect real-world biomedical scenarios.", "result": "State-of-the-art LLMs show a significant drop in performance on tasks requiring multi-hop reasoning; O3-mini outperforms others in 1-hop tasks but struggles in 2-hop tasks.", "conclusion": "BioHopR establishes a new standard for assessing reasoning capabilities in biomedical applications and highlights the disparities in performance between proprietary and open-source models.", "key_contributions": ["Introduction of BioHopR benchmark for multi-hop reasoning in biomedicine", "Evaluation of state-of-the-art models with clear performance metrics", "Highlighting critical performance gaps in reasoning capabilities of existing models"], "limitations": "Focus is on entity relationships within structured biomedical knowledge graphs, which may not address all domains of biomedical reasoning.", "keywords": ["multi-hop reasoning", "biomedical domain", "benchmark", "large language models", "knowledge graphs"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.22264", "pdf": "https://arxiv.org/pdf/2505.22264.pdf", "abs": "https://arxiv.org/abs/2505.22264", "title": "MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps", "authors": ["Maximiliano Hormazábal Lagos", "Álvaro Bueno Saez", "Héctor Cerezo-Costas", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 pages, 6 tables", "summary": "In this paper we expose our approach to solve the \\textit{SemEval 2025 Task\n8: Question-Answering over Tabular Data} challenge. Our strategy leverages\nPython code generation with LLMs to interact with the table and get the answer\nto the questions. The process is composed of multiple steps: understanding the\ncontent of the table, generating natural language instructions in the form of\nsteps to follow in order to get the answer, translating these instructions to\ncode, running it and handling potential errors or exceptions. These steps use\nopen source LLMs and fine grained optimized prompts for each task (step). With\nthis approach, we achieved a score of $70.50\\%$ for subtask 1.", "AI": {"tldr": "This paper presents a strategy that utilizes Python code generation with LLMs for answering questions from tabular data in the SemEval 2025 Task 8 challenge, achieving a score of 70.50% for subtask 1.", "motivation": "The paper aims to address the challenge of question-answering over tabular data, specifically in the context of the SemEval 2025 Task 8.", "method": "The proposed method involves multiple steps: understanding the content of the table, generating natural language instructions, translating these into Python code, executing the code, and handling errors, all employing open-source LLMs and optimized prompts.", "result": "The approach resulted in a score of 70.50% for subtask 1 of the challenge, demonstrating its effectiveness.", "conclusion": "The authors conclude that leveraging LLMs for code generation can significantly aid in answering questions based on tabular data, providing a systematic approach for future tasks.", "key_contributions": ["Utilization of LLMs for Python code generation in question-answering over tables", "Development of a multi-step approach for understanding and processing table content", "Achievement of a competitive score in a recognized task, showcasing practicality and effectiveness."], "limitations": "", "keywords": ["Question-Answering", "Tabular Data", "LLMs", "Python Code Generation", "SemEval 2025"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2505.22273", "pdf": "https://arxiv.org/pdf/2505.22273.pdf", "abs": "https://arxiv.org/abs/2505.22273", "title": "Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages", "authors": ["Shohei Higashiyama", "Masao Utiyama"], "categories": ["cs.CL"], "comment": "23 pages", "summary": "Lexical normalization research has sought to tackle the challenge of\nprocessing informal expressions in user-generated text, yet the absence of\ncomprehensive evaluations leaves it unclear which methods excel across multiple\nperspectives. Focusing on unsegmented languages, we make three key\ncontributions: (1) creating a large-scale, multi-domain Japanese normalization\ndataset, (2) developing normalization methods based on state-of-the-art\npretrained models, and (3) conducting experiments across multiple evaluation\nperspectives. Our experiments show that both encoder-only and decoder-only\napproaches achieve promising results in both accuracy and efficiency.", "AI": {"tldr": "This paper addresses lexical normalization in unsegmented languages, focusing on Japanese, by creating a dataset and developing normalization methods using pretrained models.", "motivation": "To improve processing of informal expressions in user-generated text for unsegmented languages, specifically Japanese.", "method": "The authors create a large-scale normalization dataset and develop normalization methods using state-of-the-art pretrained models, followed by experiments across various evaluation perspectives.", "result": "Both encoder-only and decoder-only approaches show promising results in terms of accuracy and efficiency for lexical normalization.", "conclusion": "The study provides insights into effective methods for lexical normalization, emphasizing the need for comprehensive evaluation across multiple perspectives.", "key_contributions": ["Creation of a large-scale, multi-domain Japanese normalization dataset", "Development of normalization methods based on pretrained models", "Experiments conducted across multiple evaluation perspectives"], "limitations": "", "keywords": ["lexical normalization", "Japanese", "pretrained models", "evaluation perspectives", "informal expressions"], "importance_score": 7, "read_time_minutes": 23}}
{"id": "2505.22280", "pdf": "https://arxiv.org/pdf/2505.22280.pdf", "abs": "https://arxiv.org/abs/2505.22280", "title": "Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review", "authors": ["Zihan Xu", "Haotian Ma", "Gongbo Zhang", "Yihao Ding", "Chunhua Weng", "Yifan Peng"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Evidence-based medicine (EBM) is at the forefront of modern healthcare,\nemphasizing the use of the best available scientific evidence to guide clinical\ndecisions. Due to the sheer volume and rapid growth of medical literature and\nthe high cost of curation, there is a critical need to investigate Natural\nLanguage Processing (NLP) methods to identify, appraise, synthesize, summarize,\nand disseminate evidence in EBM. This survey presents an in-depth review of 129\nresearch studies on leveraging NLP for EBM, illustrating its pivotal role in\nenhancing clinical decision-making processes. The paper systematically explores\nhow NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,\nApply, and Assess. The review not only identifies current limitations within\nthe field but also proposes directions for future research, emphasizing the\npotential for NLP to revolutionize EBM by refining evidence extraction,\nevidence synthesis, appraisal, summarization, enhancing data comprehensibility,\nand facilitating a more efficient clinical workflow.", "AI": {"tldr": "This paper reviews how Natural Language Processing (NLP) can enhance evidence-based medicine (EBM) by improving clinical decision-making through the systematic application of NLP techniques across the five steps of EBM.", "motivation": "With the rapid growth of medical literature and the need for scientific evidence in clinical decisions, exploring NLP methods could significantly streamline EBM practices and improve healthcare outcomes.", "method": "The authors conduct a systematic review of 129 research studies that utilize NLP in the context of EBM, focusing on how these methods can be aligned with the EBM framework.", "result": "The survey reveals that NLP can aid the EBM steps by improving evidence extraction, synthesis, appraisal, and summarization, thereby enhancing the efficiency of clinical workflows.", "conclusion": "The findings indicate that while NLP has made notable contributions to EBM, there are still limitations and challenges that require further research to fully harness its potential.", "key_contributions": ["Systematic review of 129 studies on NLP in EBM.", "Identification of current limitations in applying NLP to EBM practices.", "Recommendations for future research directions to enhance NLP's role in EBM."], "limitations": "The study highlights existing challenges in implementing NLP solutions effectively in EBM.", "keywords": ["Natural Language Processing", "Evidence-based medicine", "Clinical decision-making", "Healthcare", "Systematic review"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22293", "pdf": "https://arxiv.org/pdf/2505.22293.pdf", "abs": "https://arxiv.org/abs/2505.22293", "title": "Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs", "authors": ["Samuel Frontull", "Thomas Ströhle"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmultilingual machine translation, sometimes even outperforming traditional\nneural systems. However, previous research has highlighted the challenges of\nusing LLMs, particularly with prompt engineering, for low-resource languages.\nIn this work, we introduce Fragment-Shot Prompting, a novel in-context learning\nmethod that segments input and retrieves translation examples based on\nsyntactic coverage, along with Pivoted Fragment-Shot, an extension that enables\ntranslation without direct parallel data. We evaluate these methods using\nGPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between\nItalian and two Ladin variants, revealing three key findings: (1) Fragment-Shot\nPrompting is effective for translating into and between the studied\nlow-resource languages, with syntactic coverage positively correlating with\ntranslation quality; (2) Models with stronger reasoning abilities make more\neffective use of retrieved knowledge, generally produce better translations,\nand enable Pivoted Fragment-Shot to significantly improve translation quality\nbetween the Ladin variants; and (3) prompt engineering offers limited, if any,\nimprovements when translating from a low-resource to a high-resource language,\nwhere zero-shot prompting already yields satisfactory results. We publicly\nrelease our code and the retrieval corpora.", "AI": {"tldr": "This paper introduces Fragment-Shot Prompting and its extension, Pivoted Fragment-Shot, to enhance translation capabilities in low-resource languages using large language models (LLMs).", "motivation": "To address the challenges of using LLMs for low-resource languages, particularly in the context of prompt engineering and to improve machine translation quality.", "method": "The authors introduce Fragment-Shot Prompting, which segments input and retrieves translation examples based on syntactic coverage, and an extension called Pivoted Fragment-Shot for translation without direct parallel data. They evaluate these methods using various LLMs.", "result": "The evaluation reveals that Fragment-Shot Prompting improves translation quality for low-resource languages, with models that have stronger reasoning abilities being more effective. Additionally, it shows that prompt engineering has limited effectiveness when translating into high-resource languages.", "conclusion": "Fragment-Shot Prompting demonstrates effectiveness in improving translations in low-resource contexts, and the pivoting method significantly enhances translation quality among Ladin variants while highlighting limitations in prompt engineering for high-resource languages.", "key_contributions": ["Introduction of Fragment-Shot Prompting and Pivoted Fragment-Shot methods.", "Demonstration of improved translation quality for low-resource languages with syntactic coverage.", "Release of code and retrieval corpora for public use."], "limitations": "Limited improvements in translations from low-resource to high-resource languages through prompt engineering.", "keywords": ["Large Language Models", "Machine Translation", "Low-Resource Languages"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22296", "pdf": "https://arxiv.org/pdf/2505.22296.pdf", "abs": "https://arxiv.org/abs/2505.22296", "title": "360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training", "authors": ["Haosheng Zou", "Xiaowei Lv", "Shousheng Jia", "Xiangzheng Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "code at https://github.com/Qihoo360/360-LLaMA-Factory", "summary": "Adding sequence parallelism into LLaMA-Factory, we open-sourced\n360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.\n360-LLaMA-Factory has received wide recognition and used in models such as\nLight-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and\nalso in large companies' training frameworks. This technical report delves\ndeeper into the different sequence parallel modes behind 360-LLaMA-Factory and\ndiscusses our implementation insights.", "AI": {"tldr": "360-LLaMA-Factory enhances LLaMA-Factory by adding sequence parallelism and has been widely adopted in various models and training frameworks.", "motivation": "To improve the training efficiency and performance of LLaMA models by incorporating sequence parallelism.", "method": "The paper discusses the different sequence parallel modes implemented in 360-LLaMA-Factory and provides insights into its implementation.", "result": "360-LLaMA-Factory is now open-sourced and has been utilized in various successful models and frameworks, gaining wide recognition.", "conclusion": "The introduction of sequence parallelism in LLaMA-Factory represents a significant advancement in model training efficiency.", "key_contributions": ["Open-sourcing 360-LLaMA-Factory", "Introducing sequence parallelism into LLaMA models", "Implementation insights on different sequence parallel modes"], "limitations": "", "keywords": ["LLaMA", "sequence parallelism", "model training"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.22298", "pdf": "https://arxiv.org/pdf/2505.22298.pdf", "abs": "https://arxiv.org/abs/2505.22298", "title": "Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing", "authors": ["Yifan Lu", "Jing Li", "Yigeng Zhou", "Yihui Zhang", "Wenya Wang", "Xiucheng Li", "Meishan Zhang", "Fangming Liu", "Jun Yu", "Min Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) exhibit impressive language capabilities but\nremain vulnerable to malicious prompts and jailbreaking attacks. Existing\nknowledge editing methods for LLM detoxification face two major challenges.\nFirst, they often rely on entity-specific localization, making them ineffective\nagainst adversarial inputs without explicit entities. Second, these methods\nsuffer from over-editing, where detoxified models reject legitimate queries,\ncompromising overall performance. In this paper, we propose ToxEdit, a\ntoxicity-aware knowledge editing approach that dynamically detects toxic\nactivation patterns during forward propagation. It then routes computations\nthrough adaptive inter-layer pathways to mitigate toxicity effectively. This\ndesign ensures precise toxicity mitigation while preserving LLMs' general\ncapabilities. To more accurately assess over-editing, we also enhance the\nSafeEdit benchmark by incorporating instruction-following evaluation tasks.\nExperimental results on multiple LLMs demonstrate that our ToxEdit outperforms\nprevious state-of-the-art methods in both detoxification performance and\nsafeguarding general capabilities of LLMs.", "AI": {"tldr": "ToxEdit is a new approach for editing knowledge in LLMs to mitigate toxicity without compromising performance.", "motivation": "LLMs are vulnerable to malicious prompts and have challenges with current detoxification methods.", "method": "ToxEdit detects toxic activation patterns during model operation and uses adaptive pathways for toxicity mitigation.", "result": "Experimental results show ToxEdit outperforms existing detoxification methods while maintaining the general capabilities of LLMs.", "conclusion": "ToxEdit provides an effective solution for mitigating toxicity in LLMs with minimal performance loss.", "key_contributions": ["Introduction of ToxEdit for improved toxicity mitigation in LLMs", "Dynamic detection of toxic patterns during forward propagation", "Enhancement of SafeEdit benchmark with new evaluation tasks"], "limitations": "", "keywords": ["large language models", "toxicity mitigation", "knowledge editing", "detoxification", "SafeEdit"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.22318", "pdf": "https://arxiv.org/pdf/2505.22318.pdf", "abs": "https://arxiv.org/abs/2505.22318", "title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?", "authors": ["Ishwar B Balappanawar", "Vamshi Krishna Bonagiri", "Anish R Joishy", "Manas Gaur", "Krishnaprasad Thirunarayan", "Ponnurangam Kumaraguru"], "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 5 figures", "summary": "Large Language Models (LLMs) demonstrate impressive reasoning capabilities in\nfamiliar contexts, but struggle when the context conflicts with their\nparametric knowledge. To investigate this phenomenon, we introduce\nCounterLogic, a dataset containing 1,800 examples across 9 logical schemas,\nexplicitly designed to evaluate logical reasoning through counterfactual\n(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11\nLLMs across 6 different datasets reveals a consistent performance degradation,\nwith accuracies dropping by 27% on average when reasoning through\ncounterfactual information. We propose Self-Segregate, a prompting method\nenabling metacognitive awareness (explicitly identifying knowledge conflicts)\nbefore reasoning. Our method dramatically narrows the average performance gaps\nfrom 27% to just 11%, while significantly increasing the overall accuracy\n(+7.5%). We discuss the implications of these findings and draw parallels to\nhuman cognitive processes, particularly on how humans disambiguate conflicting\ninformation during reasoning tasks. Our findings offer practical insights for\nunderstanding and enhancing LLMs reasoning capabilities in real-world\napplications, especially where models must logically reason independently of\ntheir factual knowledge.", "AI": {"tldr": "This paper introduces CounterLogic, a dataset evaluating LLMs' logical reasoning through counterfactual scenarios and proposes a prompting method, Self-Segregate, which improves reasoning accuracy.", "motivation": "To investigate the reasoning capabilities of Large Language Models (LLMs) in contexts that conflict with their parametric knowledge.", "method": "The authors created the CounterLogic dataset with 1,800 examples across 9 logical schemas and evaluated 11 LLMs using this dataset, applying a new prompting method called Self-Segregate.", "result": "Performance degradation of LLMs was observed, with accuracies dropping by 27% on average in counterfactual scenarios. After applying the Self-Segregate method, the performance gap narrowed to 11%, with an overall accuracy increase of +7.5%.", "conclusion": "The findings provide insights into enhancing LLMs' reasoning capabilities, featuring parallels to human cognitive processes in handling conflicting information.", "key_contributions": ["Introduction of the CounterLogic dataset for evaluating logical reasoning in LLMs", "Development of Self-Segregate prompting method to improve accuracy in counterfactual reasoning", "Insights into LLM's reasoning mechanisms similar to human cognitive processes"], "limitations": "", "keywords": ["Large Language Models", "Counterfactuals", "Logical Reasoning", "Metacognitive Awareness", "Self-Segregate"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.22323", "pdf": "https://arxiv.org/pdf/2505.22323.pdf", "abs": "https://arxiv.org/abs/2505.22323", "title": "Advancing Expert Specialization for Better MoE", "authors": ["Hongcan Guo", "Haolang Lu", "Guoshun Nan", "Bolun Chu", "Jialin Zhuang", "Yuan Yang", "Wenhao Che", "Sicong Leng", "Qimei Cui", "Xudong Jiang"], "categories": ["cs.CL", "cs.SE", "68T07", "I.2.7"], "comment": "33pages, 6figures", "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community.", "AI": {"tldr": "This paper proposes two complementary objectives to enhance expert specialization in Mixture-of-Experts models, improving performance during post-training without architectural changes.", "motivation": "To address the issues of expert overlap and uniform routing in Mixture-of-Experts models due to the commonly used load balancing loss.", "method": "The proposed approach introduces an orthogonality loss to ensure distinct token processing by experts and a variance loss to promote discriminative routing decisions.", "result": "The method significantly improves expert specialization, achieving up to a 23.79% performance boost over classic MoE baselines while maintaining load balancing.", "conclusion": "The proposed methods contribute to optimizing the training process and improve downstream task performance, with plans to release code for community use.", "key_contributions": ["Introduction of orthogonality and variance losses for expert specialization", "Significant performance improvements in various MoE architectures", "Compatibility with existing load balancing losses without architectural changes"], "limitations": "", "keywords": ["Mixture-of-Experts", "expert specialization", "load balancing", "orthogonality loss", "variance loss"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22327", "pdf": "https://arxiv.org/pdf/2505.22327.pdf", "abs": "https://arxiv.org/abs/2505.22327", "title": "NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment", "authors": ["Antonia Karamolegkou", "Angana Borah", "Eunjung Cho", "Sagnik Ray Choudhury", "Martina Galletti", "Rajarshi Ghosh", "Pranav Gupta", "Oana Ignat", "Priyanka Kargupta", "Neema Kotonya", "Hemank Lamba", "Sun-Joo Lee", "Arushi Mangla", "Ishani Mondal", "Deniz Nazarova", "Poli Nemkova", "Dina Pisarevskaya", "Naquee Rizwan", "Nazanin Sabri", "Dominik Stammbach", "Anna Steinberg", "David Tomás", "Steven R Wilson", "Bowen Yi", "Jessica H Zhu", "Arkaitz Zubiaga", "Anders Søgaard", "Alexander Fraser", "Zhijing Jin", "Rada Mihalcea", "Joel R. Tetreault", "Daryna Dementieva"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have unlocked\nunprecedented possibilities across a range of applications. However, as a\ncommunity, we believe that the field of Natural Language Processing (NLP) has a\ngrowing need to approach deployment with greater intentionality and\nresponsibility. In alignment with the broader vision of AI for Social Good\n(Toma\\v{s}ev et al., 2020), this paper examines the role of NLP in addressing\npressing societal challenges. Through a cross-disciplinary analysis of social\ngoals and emerging risks, we highlight promising research directions and\noutline challenges that must be addressed to ensure responsible and equitable\nprogress in NLP4SG research.", "AI": {"tldr": "The paper discusses the need for responsible and intentional deployment of NLP technologies, highlighting societal challenges and research directions aligned with AI for Social Good.", "motivation": "To emphasize the importance of responsible and equitable progress in Natural Language Processing (NLP) that addresses societal challenges.", "method": "The paper conducts a cross-disciplinary analysis of social goals and emerging risks related to NLP applications.", "result": "Identifies promising research directions and outlines challenges for ensuring responsible NLP deployment.", "conclusion": "Emphasizes the necessity of addressing societal challenges through an intentional and responsible approach in NLP research.", "key_contributions": ["Highlighting the societal implications of NLP technologies.", "Identifying emerging risks in NLP deployment.", "Outlining research directions aligned with AI for Social Good."], "limitations": "", "keywords": ["Natural Language Processing", "AI for Social Good", "responsible AI", "societal challenges", "emerging risks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.22334", "pdf": "https://arxiv.org/pdf/2505.22334.pdf", "abs": "https://arxiv.org/abs/2505.22334", "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.", "AI": {"tldr": "This study explores enhancing multimodal reasoning in large language models through a two-stage approach involving supervised fine-tuning followed by reinforcement learning.", "motivation": "To investigate the existence of 'aha moment' patterns in multimodal LLMs before reinforcement learning training and their effects on reasoning performance.", "method": "A two-stage approach: (1) Supervised fine-tuning (SFT) to incorporate structured chain-of-thought reasoning patterns and (2) Reinforcement learning using GRPO for further refinement.", "result": "The combined approach outperforms both SFT-only and RL-only methods on challenging multimodal reasoning benchmarks, achieving state-of-the-art performance among open-source MLLMs at both 3B and 7B scales.", "conclusion": "The research provides practical guidance for building advanced multimodal reasoning models with significant performance improvements over base models.", "key_contributions": ["Demonstration of 'aha moment' patterns in MLLMs prior to RL training", "Development of a two-stage approach for enhancing multimodal reasoning", "Achievement of state-of-the-art performance in multimodal reasoning benchmarks"], "limitations": "", "keywords": ["Multimodal LLMs", "Reinforcement Learning", "Supervised Fine-Tuning", "Chain-of-Thought", "Reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22338", "pdf": "https://arxiv.org/pdf/2505.22338.pdf", "abs": "https://arxiv.org/abs/2505.22338", "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback", "authors": ["Hanyang Wang", "Lu Wang", "Chaoyun Zhang", "Tianjun Mao", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "The code for our method is available at\n  https://github.com/microsoft/Text2Grad", "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad", "AI": {"tldr": "Text2Grad is a reinforcement-learning approach that transforms textual feedback into span-level gradients for optimizing language models.", "motivation": "Traditional reinforcement learning from human feedback (RLHF) uses coarse rewards, leading to slow learning. This paper aims to improve interpretability and model performance by leveraging fine-grained textual critiques.", "method": "Text2Grad turns free-form textual feedback into differentiable reward signals that enable fine-grained updates to model policies by aligning feedback phrases with token spans.", "result": "Text2Grad consistently outperforms scalar-reward RL and prompt-only approaches across tasks like summarization, code generation, and question answering, achieving better interpretability and task metrics.", "conclusion": "Natural language feedback converted to gradients is an effective signal for optimizing language model policy, enhancing both performance and understanding.", "key_contributions": ["Introduces a novel paradigm for transforming textual feedback into gradient updates for language models.", "Develops a robust feedback-annotation pipeline that connects critiques to token spans.", "Demonstrates significant improvements over existing RL approaches in various tasks."], "limitations": "", "keywords": ["Reinforcement Learning", "Natural Language Processing", "Feedback Optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22354", "pdf": "https://arxiv.org/pdf/2505.22354.pdf", "abs": "https://arxiv.org/abs/2505.22354", "title": "LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High", "authors": ["Judith Sieker", "Clara Lachenmaier", "Sina Zarrieß"], "categories": ["cs.CL"], "comment": "8 pages (including References). Accepted at CogSci 2025", "summary": "This paper examines how LLMs handle false presuppositions and whether certain\nlinguistic factors influence their responses to falsely presupposed content.\nPresuppositions subtly introduce information as given, making them highly\neffective at embedding disputable or false information. This raises concerns\nabout whether LLMs, like humans, may fail to detect and correct misleading\nassumptions introduced as false presuppositions, even when the stakes of\nmisinformation are high. Using a systematic approach based on linguistic\npresupposition analysis, we investigate the conditions under which LLMs are\nmore or less sensitive to adopt or reject false presuppositions. Focusing on\npolitical contexts, we examine how factors like linguistic construction,\npolitical party, and scenario probability impact the recognition of false\npresuppositions. We conduct experiments with a newly created dataset and\nexamine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's\nMistral-7B-v03. Our results show that the models struggle to recognize false\npresuppositions, with performance varying by condition. This study highlights\nthat linguistic presupposition analysis is a valuable tool for uncovering the\nreinforcement of political misinformation in LLM responses.", "AI": {"tldr": "This paper investigates how LLMs handle false presuppositions, focusing on the influence of linguistic factors in recognizing misleading assumptions.", "motivation": "To understand if LLMs, like humans, can detect and correct false presuppositions, especially in sensitive contexts like misinformation.", "method": "A systematic analysis of linguistic presuppositions was conducted, experimenting with OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's Mistral-7B-v03 with a newly created dataset focused on political contexts.", "result": "The experiments revealed that LLMs struggle to recognize false presuppositions, with varying performance based on factors like linguistic construction and scenario probability.", "conclusion": "Linguistic presupposition analysis is crucial for revealing how political misinformation is reinforced in LLM outputs.", "key_contributions": ["Introduces a novel dataset for analyzing LLM handling of false presuppositions.", "Demonstrates varying recognition capabilities of different LLMs under political contexts.", "Establishes linguistic presupposition analysis as a tool for investigating misinformation."], "limitations": "The study focuses only on political contexts and specific LLMs, which may limit generalizability.", "keywords": ["Large Language Models", "false presuppositions", "political misinformation", "linguistic analysis", "machine learning"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.22375", "pdf": "https://arxiv.org/pdf/2505.22375.pdf", "abs": "https://arxiv.org/abs/2505.22375", "title": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition", "authors": ["Hanting Chen", "Yasheng Wang", "Kai Han", "Dong Li", "Lin Li", "Zhenni Bi", "Jinpeng Li", "Haoyu Wang", "Fei Mi", "Mingjian Zhu", "Bin Wang", "Kaikai Song", "Yifei Fu", "Xu He", "Yu Luo", "Chong Zhu", "Quan He", "Xueyu Wu", "Wei He", "Hailin Hu", "Yehui Tang", "Dacheng Tao", "Xinghao Chen", "Yunhe Wang", "Other Contributors"], "categories": ["cs.CL"], "comment": null, "summary": "This work presents Pangu Embedded, an efficient Large Language Model (LLM)\nreasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible\nfast and slow thinking capabilities. Pangu Embedded addresses the significant\ncomputational costs and inference latency challenges prevalent in existing\nreasoning-optimized LLMs. We propose a two-stage training framework for its\nconstruction. In Stage 1, the model is finetuned via an iterative distillation\nprocess, incorporating inter-iteration model merging to effectively aggregate\ncomplementary knowledge. This is followed by reinforcement learning on Ascend\nclusters, optimized by a latency-tolerant scheduler that combines stale\nsynchronous parallelism with prioritized data queues. The RL process is guided\nby a Multi-source Adaptive Reward System (MARS), which generates dynamic,\ntask-specific reward signals using deterministic metrics and lightweight LLM\nevaluators for mathematics, coding, and general problem-solving tasks. Stage 2\nintroduces a dual-system framework, endowing Pangu Embedded with a \"fast\" mode\nfor routine queries and a deeper \"slow\" mode for complex inference. This\nframework offers both manual mode switching for user control and an automatic,\ncomplexity-aware mode selection mechanism that dynamically allocates\ncomputational resources to balance latency and reasoning depth. Experimental\nresults on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate\nthat Pangu Embedded with 7B parameters, outperforms similar-size models like\nQwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art\nreasoning quality within a single, unified model architecture, highlighting a\npromising direction for developing powerful yet practically deployable LLM\nreasoners.", "AI": {"tldr": "Pangu Embedded is an efficient Large Language Model (LLM) that leverages a dual-system framework to optimize reasoning capabilities and reduce computational costs and inference latency.", "motivation": "To address high computational costs and inference latency in existing reasoning-optimized LLMs.", "method": "A two-stage training framework, first using iterative distillation for model fine-tuning and reinforcement learning with a novel reward system, followed by a dual-system framework for fast and slow thinking modes.", "result": "Pangu Embedded outperforms similar models in benchmarks, demonstrating rapid response times and superior reasoning quality.", "conclusion": "The architecture shows promise for developing efficient LLM reasoners that can balance resource allocation effectively.", "key_contributions": ["Introduction of a two-stage training framework combining distillation and reinforcement learning.", "Development of a dual-system framework for optimized reasoning capabilities.", "Demonstration of superior performance on benchmarks compared to similar models."], "limitations": "", "keywords": ["Large Language Model", "Reasoning", "Reinforcement Learning", "Inference Latency", "Dual-System Framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22430", "pdf": "https://arxiv.org/pdf/2505.22430.pdf", "abs": "https://arxiv.org/abs/2505.22430", "title": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning", "authors": ["Kun Li", "Yunxiang Li", "Tianhua Zhang", "Hongyin Luo", "Xixin Wu", "James Glass", "Helen Meng"], "categories": ["cs.CL"], "comment": null, "summary": "Robust evaluation is critical for deploying trustworthy retrieval-augmented\ngeneration (RAG) systems. However, current LLM-based evaluation frameworks\npredominantly rely on directly prompting resource-intensive models with complex\nmulti-stage prompts, underutilizing models' reasoning capabilities and\nintroducing significant computational cost. In this paper, we present RAG-Zeval\n(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness\nand correctness evaluation as a rule-guided reasoning task. Our approach trains\nevaluators with reinforcement learning, facilitating compact models to generate\ncomprehensive and sound assessments with detailed explanation in one-pass. We\nintroduce a ranking-based outcome reward mechanism, using preference judgments\nrather than absolute scores, to address the challenge of obtaining precise\npointwise reward signals. To this end, we synthesize the ranking references by\ngenerating quality-controlled responses with zero human annotation. Experiments\ndemonstrate RAG-Zeval's superior performance, achieving the strongest\ncorrelation with human judgments and outperforming baselines that rely on LLMs\nwith 10-100 times more parameters. Our approach also exhibits superior\ninterpretability in response evaluation.", "AI": {"tldr": "RAG-Zeval is a framework for evaluating retrieval-augmented generation (RAG) systems using rule-guided reasoning and reinforcement learning, enhancing efficiency and interpretability in performance assessment.", "motivation": "To address the inefficiencies and high computational costs associated with current LLM-based evaluation frameworks for retrieval-augmented generation systems.", "method": "A novel end-to-end evaluation framework called RAG-Zeval, which formulates evaluation as a rule-guided reasoning task, utilizing reinforcement learning for training evaluators.", "result": "RAG-Zeval achieves the strongest correlation with human judgments and outperforms LLMs with 10-100 times more parameters, while also providing better interpretability.", "conclusion": "The proposed framework significantly improves the evaluation process for RAG systems by generating comprehensive assessments efficiently.", "key_contributions": ["Introduction of RAG-Zeval, an efficient evaluation framework for RAG systems", "Use of a ranking-based outcome reward mechanism to improve assessment accuracy", "Demonstration of superior interpretability and correlation with human judgments compared to existing LLM-based methods."], "limitations": "", "keywords": ["retrieval-augmented generation", "evaluation framework", "reinforcement learning", "rule-guided reasoning", "interpretability"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22453", "pdf": "https://arxiv.org/pdf/2505.22453.pdf", "abs": "https://arxiv.org/abs/2505.22453", "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO", "authors": ["Lai Wei", "Yuting Li", "Chen Wang", "Yue Wang", "Linghe Kong", "Weiran Huang", "Lichao Sun"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.", "AI": {"tldr": "This paper proposes MM-UPT, a novel framework for unsupervised post-training of Multi-modal Large Language Models (MLLMs) using a self-rewarding mechanism and an online RL algorithm called GRPO, demonstrating significant improvements in reasoning without requiring manually annotated data.", "motivation": "Current methods for improving MLLMs in the post-training stage often rely on costly supervised fine-tuning and complex unsupervised methods, creating a need for a simpler, more scalable approach that doesn't require external supervision.", "method": "The authors introduce MM-UPT, which utilizes the GRPO algorithm and replaces traditional reward mechanisms with a self-rewarding method based on majority voting across multiple sampled responses, enabling continual self-improvement of MLLMs.", "result": "The experiments indicate that MM-UPT enhances the reasoning abilities of the MLLM Qwen2.5-VL-7B significantly, achieving improvements in performance metrics on standard datasets, even outperforming previous unsupervised methods and approaching supervised GRPO results.", "conclusion": "MM-UPT represents a promising new paradigm for the autonomous enhancement of MLLMs, allowing for continual self-improvement without the dependency on external, annotated data.", "key_contributions": ["Introduction of the MM-UPT framework for unsupervised post-training of MLLMs.", "Implementation of a self-rewarding mechanism using majority voting for ongoing learning.", "Demonstration of significant performance improvements on established benchmarks without ground truth labels."], "limitations": "", "keywords": ["Multi-modal Large Language Models", "Reinforcement Learning", "Unsupervised Learning", "Self-rewarding Mechanism", "Continual Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22501", "pdf": "https://arxiv.org/pdf/2505.22501.pdf", "abs": "https://arxiv.org/abs/2505.22501", "title": "EvolveSearch: An Iterative Self-Evolving Search Agent", "authors": ["Dingchu Zhang", "Yida Zhao", "Jialong Wu", "Baixuan Li", "Wenbiao Yin", "Liwen Zhang", "Yong Jiang", "Yufeng Li", "Kewei Tu", "Pengjun Xie", "Fei Huang"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of agentic information seeking capabilities through the integration\nof tools such as search engines and web browsers. However, current mainstream\napproaches for enabling LLM web search proficiency face significant challenges:\nsupervised fine-tuning struggles with data production in open-search domains,\nwhile RL converges quickly, limiting their data utilization efficiency. To\naddress these issues, we propose EvolveSearch, a novel iterative self-evolution\nframework that combines SFT and RL to enhance agentic web search capabilities\nwithout any external human-annotated reasoning data. Extensive experiments on\nseven multi-hop question-answering (MHQA) benchmarks demonstrate that\nEvolveSearch consistently improves performance across iterations, ultimately\nachieving an average improvement of 4.7\\% over the current state-of-the-art\nacross seven benchmarks, opening the door to self-evolution agentic\ncapabilities in open web search domains.", "AI": {"tldr": "EvolveSearch is a novel framework enhancing LLM web search capabilities by combining supervised fine-tuning and reinforcement learning, achieving significant performance improvements.", "motivation": "The paper addresses challenges in enabling large language models to effectively perform web searches, particularly in open-search domains.", "method": "EvolveSearch utilizes an iterative self-evolution framework that blends supervised fine-tuning with reinforcement learning, eliminating the need for human-annotated data.", "result": "The framework demonstrated consistent performance improvements across seven multi-hop question-answering benchmarks, achieving an average improvement of 4.7% over state-of-the-art methods.", "conclusion": "EvolveSearch opens new possibilities for self-evolution in agentic capabilities for web search, paving the way for advancements in this domain.", "key_contributions": ["Introduction of EvolveSearch framework for LLMs", "Combines SFT and RL without human-annotated data", "Empirical validation showing significant performance gains across multiple benchmarks"], "limitations": "", "keywords": ["large language models", "web search", "reinforcement learning", "self-evolution", "multi-hop question answering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.22517", "pdf": "https://arxiv.org/pdf/2505.22517.pdf", "abs": "https://arxiv.org/abs/2505.22517", "title": "Multi-MLLM Knowledge Distillation for Out-of-Context News Detection", "authors": ["Yimeng Gu", "Zhao Tong", "Ignacio Castro", "Shu Wu", "Gareth Tyson"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Multimodal out-of-context news is a type of misinformation in which the image\nis used outside of its original context. Many existing works have leveraged\nmultimodal large language models (MLLMs) for detecting out-of-context news.\nHowever, observing the limited zero-shot performance of smaller MLLMs, they\ngenerally require label-rich fine-tuning and/or expensive API calls to GPT\nmodels to improve the performance, which is impractical in low-resource\nscenarios. In contrast, we aim to improve the performance of small MLLMs in a\nmore label-efficient and cost-effective manner. To this end, we first prompt\nmultiple teacher MLLMs to generate both label predictions and corresponding\nrationales, which collectively serve as the teachers' knowledge. We then\nintroduce a two-stage knowledge distillation framework to transfer this\nknowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the\nstudent model using all training data. In Stage 2, we further fine-tune the\nstudent model using both LoRA fine-tuning and DPO on the data points where\nteachers' predictions conflict. This two-stage strategy reduces annotation\ncosts and helps the student model uncover subtle patterns in more challenging\ncases. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance using less than 10% labeled data.", "AI": {"tldr": "This paper presents a two-stage knowledge distillation framework to enhance the performance of small multimodal large language models (MLLMs) in detecting out-of-context news by leveraging teacher models for label predictions and rationales.", "motivation": "Improving the performance of small MLLMs for detecting out-of-context news in a label-efficient and cost-effective manner, especially in low-resource scenarios where fine-tuning with large labeled datasets or via expensive APIs is impractical.", "method": "The method involves prompting multiple teacher MLLMs to generate label predictions and rationales, followed by a two-stage knowledge distillation framework. In Stage 1, LoRA fine-tuning is applied to the student model using all training data. In Stage 2, additional fine-tuning is conducted using LoRA and DPO on conflicting predictions from teachers.", "result": "The proposed framework achieves state-of-the-art performance while using less than 10% of labeled training data, demonstrating significant efficiency in learning with limited resources.", "conclusion": "The two-stage strategy offers a practical solution for improving small MLLMs in out-of-context news detection, enabling them to uncover subtle patterns with minimal annotation costs.", "key_contributions": ["Introduction of a two-stage knowledge distillation framework for small MLLMs", "Utilization of teacher models for generating label predictions and rationales", "Achievement of state-of-the-art performance with less than 10% labeled data"], "limitations": "", "keywords": ["multimodal", "knowledge distillation", "large language models", "out-of-context news", "label-efficient learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.22548", "pdf": "https://arxiv.org/pdf/2505.22548.pdf", "abs": "https://arxiv.org/abs/2505.22548", "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs", "authors": ["Changhao Song", "Yazhou Zhang", "Peng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Emotion understanding includes basic tasks (e.g., sentiment/emotion\nclassification) and advanced tasks (e.g., sarcasm/humor detection). Current\nmethods rely on fixed-length CoT reasoning, failing to adapt to the varying\ncomplexity of emotions. We propose a task-adaptive reasoning framework that\nemploys DeepSeek-R1 to generate variable-length reasoning chains for different\nemotion tasks. By combining fine-tuning with reinforcement learning, we design\na composite reward function that balances four objectives: prediction accuracy,\nadaptive reasoning depth control, structural diversity in reasoning paths, and\nsuppression of repetitive logic. This approach achieves dynamic\ncontext-sensitive inference while enabling LLMs to autonomously develop deep\nreasoning capabilities. Experimental results demonstrate consistent\nimprovements in both Acc and F1 scores across four tasks: emotion, sentiment,\nhumor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for\nbasic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges\nrigid CoT reasoning and emotional complexity through adaptive-depth analysis.", "AI": {"tldr": "The paper presents a task-adaptive reasoning framework for improved emotion understanding, utilizing variable-length reasoning chains to adapt to the complexity of different emotional tasks.", "motivation": "Current methods in emotion understanding rely on fixed-length reasoning which does not effectively handle the varying complexities of emotional tasks.", "method": "A task-adaptive reasoning framework using DeepSeek-R1 is proposed, employing a composite reward function that balances prediction accuracy, adaptive reasoning depth control, structural diversity, and suppression of repetitive logic.", "result": "The proposed framework shows consistent improvements in accuracy and F1 scores across emotion-related tasks, with notable enhancements for both basic and advanced tasks.", "conclusion": "The research effectively bridges rigid reasoning techniques with emotional complexities, enabling dynamic context-sensitive inference.", "key_contributions": ["Development of a task-adaptive framework for emotion understanding", "Introduction of dynamic variable-length reasoning chains", "Improvements in prediction accuracy and structural diversity for emotion tasks"], "limitations": "", "keywords": ["emotion understanding", "machine learning", "adaptive reasoning", "sentiment analysis", "humor detection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22552", "pdf": "https://arxiv.org/pdf/2505.22552.pdf", "abs": "https://arxiv.org/abs/2505.22552", "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM", "authors": ["Hoang Pham", "Thanh-Do Nguyen", "Khac-Hoai Nam Bui"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ACL 2025 findings", "summary": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones.", "AI": {"tldr": "ClaimPKG is an end-to-end framework that integrates large language model reasoning with structured knowledge from knowledge graphs for effective claim verification.", "motivation": "Existing verification methods fail to leverage structured knowledge from knowledge graphs due to reliance on unstructured text corpora, and modern LLMs struggle with reasoning over these graphs.", "method": "ClaimPKG employs a lightweight LLM to represent claims as pseudo-subgraphs and a dedicated module to retrieve relevant KG subgraphs, which are then processed by a general-purpose LLM for final verdicts and justifications.", "result": "ClaimPKG outperforms strong baselines by 9%-12% in accuracy on the FactKG dataset, and demonstrates zero-shot generalizability to unstructured datasets like HoVer and FEVEROUS.", "conclusion": "ClaimPKG effectively combines the reasoning capabilities of LLMs with structured knowledge, improving claim verification processes.", "key_contributions": ["Introduction of ClaimPKG framework for integrating KGs with LLMs", "Achieving state-of-the-art performance on the FactKG dataset", "Demonstrating zero-shot generalizability to unstructured datasets"], "limitations": "", "keywords": ["knowledge graphs", "large language models", "claim verification", "reasoning capability", "zero-shot learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.22563", "pdf": "https://arxiv.org/pdf/2505.22563.pdf", "abs": "https://arxiv.org/abs/2505.22563", "title": "Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings", "authors": ["Yu Lei", "Xingyang Ge", "Yi Zhang", "Yiming Yang", "Bolei Ma"], "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Understanding whether large language models (LLMs) and the human brain\nconverge on similar computational principles remains a fundamental and\nimportant question in cognitive neuroscience and AI. Do the brain-like patterns\nobserved in LLMs emerge simply from scaling, or do they reflect deeper\nalignment with the architecture of human language processing? This study\nfocuses on the sentence-level neural mechanisms of language models,\nsystematically investigating how hierarchical representations in LLMs align\nwith the dynamic neural responses during human sentence comprehension. By\ncomparing hierarchical embeddings from 14 publicly available LLMs with fMRI\ndata collected from participants, who were exposed to a naturalistic narrative\nstory, we constructed sentence-level neural prediction models to precisely\nidentify the model layers most significantly correlated with brain region\nactivations. Results show that improvements in model performance drive the\nevolution of representational architectures toward brain-like hierarchies,\nparticularly achieving stronger functional and anatomical correspondence at\nhigher semantic abstraction levels.", "AI": {"tldr": "This study investigates the alignment of large language models' hierarchical representations with human neural responses during sentence comprehension, revealing a strong correlation at higher semantic abstraction levels.", "motivation": "To understand whether LLMs and human brain processes align on computational principles, particularly concerning language processing.", "method": "The study compares hierarchical embeddings from 14 LLMs with fMRI data of humans engaging with narrative stories, constructing neural prediction models to identify corresponding model layers.", "result": "Findings show that model performance improvements lead to development of brain-like architectures, particularly at higher semantic abstraction levels.", "conclusion": "The research provides evidence that LLMs may reflect deeper similarities with human brain dynamics in language processing, especially in higher levels of semantic interpretation.", "key_contributions": ["Identified correlations between LLM layers and human brain activations during sentence comprehension.", "Constructed sentence-level neural prediction models from fMRI data.", "Demonstrated evolution of LLM representations towards brain-like hierarchies."], "limitations": "", "keywords": ["Large Language Models", "fMRI", "neural response", "language processing", "hierarchical representations"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22571", "pdf": "https://arxiv.org/pdf/2505.22571.pdf", "abs": "https://arxiv.org/abs/2505.22571", "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "authors": ["Hoang Pham", "Khac-Hoai Nam Bui"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation.", "AI": {"tldr": "This paper introduces Agent-UniRAG, a unified retrieval-augmented generation framework that enhances interpretability and effectiveness for both single-hop and multi-hop queries using LLM agents.", "motivation": "To address the limitations of previous RAG systems that only focus on either single-hop or multi-hop queries, which restricts their applicability in real-world scenarios.", "method": "The proposed approach is a trainable framework called Agent-UniRAG, which employs a step-by-step method to handle the complexity of inputs for RAG tasks while integrating both single-hop and multi-hop queries in an end-to-end process.", "result": "The framework, along with the synthetic dataset SynAgent-RAG, demonstrates comparable performance with larger closed-source and open-source LLMs across various RAG benchmarks.", "conclusion": "The study presents a significant advancement in the interpretability and effectiveness of retrieval-augmented generation systems, offering a solution applicable to real-world multi-hop reasoning tasks.", "key_contributions": ["Introduction of Agent-UniRAG for unified retrieval-augmented generation", "Development of SynAgent-RAG synthetic dataset for small LLMs", "Demonstration of comparable performance to larger models in RAG benchmarks"], "limitations": "", "keywords": ["retrieval-augmented generation", "large language models", "multi-hop queries", "interpretability", "synthetic dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22572", "pdf": "https://arxiv.org/pdf/2505.22572.pdf", "abs": "https://arxiv.org/abs/2505.22572", "title": "Fusion Steering: Prompt-Specific Activation Control", "authors": ["Waldemar Chang", "Alhassan Yasin"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures, 2 tables", "summary": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs.", "AI": {"tldr": "Fusion Steering enhances factual accuracy in LLMs for QA through dynamic, layer-specific prompt adjustments.", "motivation": "To improve factual accuracy in question-answering tasks using large language models by introducing a flexible steering configuration that moves beyond traditional fixed-layer methodologies.", "method": "Fusion Steering employs dynamic injection of prompt-specific activation deltas across all transformer layers, optimized using Optuna. It balances token overlap and perplexity to enhance performance.", "result": "Segmented steering achieves 25.4% accuracy on 260 SimpleQA prompts, significantly outperforming the baseline (3.5%) and full-layer steering (16.2%).", "conclusion": "Segmented steering demonstrates a strong potential for activating control in LLMs, suggesting further applications in interpretable AI.", "key_contributions": ["Introduction of Fusion Steering methodology for LLMs", "Demonstration of segmented steering effectiveness in improving accuracy", "Integration of dynamic activation adjustment for higher factual alignment"], "limitations": "", "keywords": ["Fusion Steering", "Large Language Models", "Question Answering", "Activation Steering", "Factual Accuracy"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2505.22582", "pdf": "https://arxiv.org/pdf/2505.22582.pdf", "abs": "https://arxiv.org/abs/2505.22582", "title": "Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "categories": ["cs.CL"], "comment": "ACL 2025 (Main), 16 pages, 5 figures, 11 tables", "summary": "Continually expanding new languages for existing large language models (LLMs)\nis a promising yet challenging approach to building powerful multilingual LLMs.\nThe biggest challenge is to make the model continuously learn new languages\nwhile preserving the proficient ability of old languages. To achieve this,\nrecent work utilizes the Mixture-of-Experts (MoE) architecture to expand new\nlanguages by adding new experts and avoid catastrophic forgetting of old\nlanguages by routing corresponding tokens to the original model backbone (old\nexperts). Although intuitive, this kind of method is parameter-costly when\nexpanding new languages and still inevitably impacts the performance of old\nlanguages. To address these limitations, we analyze the language\ncharacteristics of different layers in LLMs and propose a layer-wise expert\nallocation algorithm (LayerMoE) to determine the appropriate number of new\nexperts for each layer. Specifically, we find different layers in LLMs exhibit\ndifferent representation similarities between languages and then utilize the\nsimilarity as the indicator to allocate experts for each layer, i.e., the\nhigher similarity, the fewer experts. Additionally, to further mitigate the\nforgetting of old languages, we add a classifier in front of the router network\non the layers with higher similarity to guide the routing of old language\ntokens. Experimental results show that our method outperforms the previous\nstate-of-the-art baseline with 60% fewer experts in the single-expansion\nsetting and with 33.3% fewer experts in the lifelong-expansion setting,\ndemonstrating the effectiveness of our method.", "AI": {"tldr": "The paper proposes a layer-wise expert allocation algorithm (LayerMoE) to enhance multilingual large language models (LLMs) by optimizing the addition of new language experts while preventing performance loss in existing languages.", "motivation": "The aim is to improve the multilingual capabilities of LLMs without the high parameter costs and performance degradation associated with adding new language experts.", "method": "The paper introduces the LayerMoE algorithm, which allocates a varying number of language experts to different layers based on the representation similarities of languages, and incorporates a classifier to better route tokens for old languages in high-similarity layers.", "result": "Experimental results indicate that LayerMoE outperforms previous state-of-the-art models, achieving 60% fewer experts in single-expansion and 33.3% fewer in lifelong-expansion scenarios while maintaining performance.", "conclusion": "The LayerMoE approach effectively balances the addition of new language capabilities with the preservation of existing language performance, offering a more efficient method for expanding multilingual LLMs.", "key_contributions": ["Introduction of LayerMoE for optimized expert allocation in LLMs.", "Analysis of language representation similarities across LLM layers for expert allocation.", "Reduced parameter costs and improved performance metrics compared to prior methods."], "limitations": "", "keywords": ["Large Language Models", "Layer-wise Expert Allocation", "Mixture-of-Experts", "Multilingual NLP", "Catastrophic Forgetting"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22586", "pdf": "https://arxiv.org/pdf/2505.22586.pdf", "abs": "https://arxiv.org/abs/2505.22586", "title": "Precise In-Parameter Concept Erasure in Large Language Models", "authors": ["Yoav Gur-Arieh", "Clara Suslik", "Yihuai Hong", "Fazl Barez", "Mor Geva"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models.", "AI": {"tldr": "PISCES is a novel framework for precisely erasing undesirable concepts from large language models by editing model parameters directly.", "motivation": "Current methods for removing sensitive or copyrighted content from large language models are often ineffective.", "method": "PISCES utilizes a disentangler model to decompose MLP vectors into interpretable features, identifying and editing features associated with target concepts in parameter space.", "result": "PISCES shows modest gains in efficacy over existing methods, achieving significantly lower accuracy on target concepts and improved specificity and robustness in knowledge removal.", "conclusion": "The feature-based in-parameter editing provides a more reliable method for erasing conceptual knowledge in language models.", "key_contributions": ["Introduction of PISCES framework for precise concept erasure", "Improved specificity and robustness in erasure compared to existing methods", "Demonstrated effectiveness in reducing model accuracy on target concepts"], "limitations": "", "keywords": ["large language models", "concept erasure", "feature-based editing", "PISCES", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.22591", "pdf": "https://arxiv.org/pdf/2505.22591.pdf", "abs": "https://arxiv.org/abs/2505.22591", "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning", "authors": ["Erxin Yu", "Jing Li", "Ming Liao", "Qi Zhu", "Boyang Xue", "Minghui Xu", "Baojun Wang", "Lanqing Hong", "Fei Mi", "Lifeng Shang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 9 figures", "summary": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization.", "AI": {"tldr": "This paper introduces Self-Error-Instruct (SEI), a framework that improves mathematical reasoning in large language models by generating targeted training data through the analysis of errors.", "motivation": "Large language models struggle with mathematical reasoning, often due to insufficient training data derived from bad cases. This paper aims to improve training data synthesis for better generalization of error patterns in mathematical reasoning tasks.", "method": "The framework involves identifying bad cases in mathematical datasets (GSM8K and MATH), generating error keyphrases, clustering to identify error types, and using a self-instruct approach to refine and synthesize additional training data for fine-tuning models.", "result": "The application of SEI leads to improved performance in reasoning abilities of various models across both in-domain and out-of-domain mathematics datasets.", "conclusion": "Self-error instruction effectively enhances LLMs' mathematical reasoning by synthesizing generalized training data from identified errors and improving model performance through iterative fine-tuning.", "key_contributions": ["Introduction of Self-Error-Instruct (SEI) framework for enhanced training data generation", "Systematic analysis of mathematical errors using keyphrases and clustering", "Demonstrated improvements in model performance on mathematical reasoning tasks"], "limitations": "", "keywords": ["Large Language Models", "Mathematical Reasoning", "Error Generalization"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2505.22618", "pdf": "https://arxiv.org/pdf/2505.22618.pdf", "abs": "https://arxiv.org/abs/2505.22618", "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding", "authors": ["Chengyue Wu", "Hao Zhang", "Shuchen Xue", "Zhijian Liu", "Shizhe Diao", "Ligeng Zhu", "Ping Luo", "Song Han", "Enze Xie"], "categories": ["cs.CL"], "comment": null, "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.", "AI": {"tldr": "This paper presents improvements for Diffusion LLMs by introducing a novel KV Cache mechanism and a confidence-aware parallel decoding strategy to enhance inference speed and maintain text generation quality.", "motivation": "The motivation is to improve the inference speed and quality of open-sourced Diffusion-based large language models, which currently lag behind autoregressive models in practical applications.", "method": "The authors introduce a block-wise approximate KV Cache mechanism and a confidence-aware parallel decoding strategy to address issues of speed and quality degradation in Diffusion LLMs.", "result": "Experimental results indicate up to 27.6 times throughput improvement with minimal accuracy loss compared to existing methods, closing the gap with autoregressive models' performance.", "conclusion": "The study successfully demonstrates that the proposed methods enable practical deployment of Diffusion LLMs by significantly improving their speed and maintaining generation quality.", "key_contributions": ["Novel block-wise approximate KV Cache mechanism for Diffusion LLMs", "Confidence-aware parallel decoding strategy to improve generation quality", "Significant improvements in inference speed (up to 27.6x throughput) with minimal accuracy loss"], "limitations": "", "keywords": ["Diffusion LLMs", "parallel decoding", "KV Cache"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22627", "pdf": "https://arxiv.org/pdf/2505.22627.pdf", "abs": "https://arxiv.org/abs/2505.22627", "title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions", "authors": ["Yijun Shen", "Delong Chen", "Fan Liu", "Xingyu Wang", "Chuanyi Zhang", "Liang Yao", "Yuhui Zheng"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "While densely annotated image captions significantly facilitate the learning\nof robust vision-language alignment, methodologies for systematically\noptimizing human annotation efforts remain underexplored. We introduce\nChain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize\nthe number of annotated samples and improve their comprehensiveness under fixed\nbudget constraints (e.g., total human annotation time). The framework is built\nupon two key insights. First, sequential annotation reduces redundant workload\ncompared to conventional parallel annotation, as subsequent annotators only\nneed to annotate the ``residual'' -- the missing visual information that\nprevious annotations have not covered. Second, humans process textual input\nfaster by reading while outputting annotations with much higher throughput via\ntalking; thus a multimodal interface enables optimized efficiency. We evaluate\nour framework from two aspects: intrinsic evaluations that assess the\ncomprehensiveness of semantic units, obtained by parsing detailed captions into\nobject-attribute trees and analyzing their effective connections; extrinsic\nevaluation measures the practical usage of the annotated captions in\nfacilitating vision-language alignment. Experiments with eight participants\nshow our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30\nunits/sec) and retrieval performance (41.13\\% vs. 40.52\\%) over the parallel\nmethod.", "AI": {"tldr": "Introducing CoTalk, a new AI-in-the-loop methodology that optimizes human annotation for vision-language tasks by enhancing efficiency and comprehensiveness of image captioning.", "motivation": "To systematically optimize human annotation efforts for vision-language alignment given budget constraints.", "method": "CoTalk utilizes a sequential annotation approach, allowing annotators to focus on the residual visual information, and employs a multimodal interface to maximize throughput through talking while annotating.", "result": "CoTalk improves annotation speed (0.42 vs. 0.30 units/sec) and retrieval performance (41.13% vs. 40.52%) compared to traditional parallel methods.", "conclusion": "The CoTalk framework demonstrates significant improvements in both the speed and quality of human annotations in vision-language tasks.", "key_contributions": ["Introduction of the CoTalk methodology for image captioning.", "Demonstrated effectiveness of sequential annotation over parallel annotation.", "Showed improved annotation speed and retrieval performance through multimodal interaction."], "limitations": "", "keywords": ["vision-language alignment", "annotation optimization", "multimodal interface"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.22630", "pdf": "https://arxiv.org/pdf/2505.22630.pdf", "abs": "https://arxiv.org/abs/2505.22630", "title": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs", "authors": ["Ziling Cheng", "Meng Cao", "Marc-Antoine Rondeau", "Jackie Chi Kit Cheung"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons.", "AI": {"tldr": "This paper examines the nature of errors made by large language models (LLMs), focusing on irrelevant context hallucinations and the mechanisms behind these errors, termed class-based (mis)generalization.", "motivation": "To investigate the errors made by LLMs, specifically irrelevant context hallucinations, and to understand the structured mechanisms behind these errors.", "method": "The authors conduct behavioral analysis and mechanistic interpretability experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation types.", "result": "The study finds that errors arise from structured yet flawed mechanisms, where abstract class representations are developed in earlier layers of the model and refined in higher layers, influenced by competing circuits for feature selection.", "conclusion": "LLMs can generalize using abstractions, but their ability to do so is unreliable and heavily influenced by contextual cues, leading to what are referred to as stochastic chameleons.", "key_contributions": ["Introduces the concept of class-based (mis)generalization in LLMs.", "Provides a structured analysis of errors related to irrelevant context hallucinations.", "Presents mechanistic insights into how LLMs process information and make predictions."], "limitations": "The study primarily focuses on certain LLM architectures, which may limit the generalizability of the findings to all LLMs.", "keywords": ["large language models", "hallucinations", "class-based generalization", "mechanistic interpretability", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22633", "pdf": "https://arxiv.org/pdf/2505.22633.pdf", "abs": "https://arxiv.org/abs/2505.22633", "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis", "authors": ["Yida Xue", "Zhen Bi", "Jinnan Yang", "Jungang Lou", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Ongoing work", "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence.", "AI": {"tldr": "This paper introduces SKG2Data, a multimodal synthesis method that leverages spatial knowledge graphs to improve the spatial perception abilities of multimodal large language models (MLLMs).", "motivation": "The motivation behind this work is to enhance the spatial perception capabilities of MLLMs, which have been identified as a significant limitation despite recent advances in multimodal models.", "method": "The methodology involves the automatic construction of a Spatial Knowledge Graph (SKG) to emulate human-like perception of spatial directions and distances, which guides the multimodal data synthesis process.", "result": "Extensive experiments show that data synthesized from various types of spatial knowledge improve the spatial perception and reasoning abilities of MLLMs and demonstrate strong generalization capabilities.", "conclusion": "The authors believe that utilizing knowledge-based data synthesis will foster advancements in spatial intelligence within multimodal large language models.", "key_contributions": ["Introduction of SKG2Data for multimodal data synthesis guided by spatial knowledge graphs.", "Demonstration of improved spatial perception and reasoning in MLLMs.", "Evidence of strong generalization capabilities from synthesized spatial data."], "limitations": "", "keywords": ["multimodal large language models", "spatial knowledge graphs", "data synthesis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22635", "pdf": "https://arxiv.org/pdf/2505.22635.pdf", "abs": "https://arxiv.org/abs/2505.22635", "title": "Learning Composable Chains-of-Thought", "authors": ["Fangcong Yin", "Zeyu Leo Liu", "Liu Leqi", "Xi Ye", "Greg Durrett"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget.", "AI": {"tldr": "This paper explores improving the generalization of large language models (LLMs) by training them on Composable Chain-of-Thought (CoT) data to enhance their reasoning skills for unseen tasks without labeled data.", "motivation": "The research aims to enable reasoning models to generalize beyond their training distribution and to combine atomic reasoning skills to tackle complex, unseen tasks.", "method": "The study modifies the formats of constituent atomic tasks in CoT traces to be composable and trains models on this Composable CoT data, using methods such as multitask learning and model merging to enhance performance.", "result": "Models trained on Composable CoT data demonstrate improved zero-shot performance on compositional tasks compared to traditional multitask learning and fine-tuning approaches, even with limited training data.", "conclusion": "The findings suggest that modifying CoT formats to be composable significantly enhances the reasoning capabilities of LLMs on new tasks.", "key_contributions": ["Introduction of Composable CoT data to improve reasoning model generalization.", "Demonstration of effective model combination techniques for enhanced zero-shot performance.", "Findings on the superiority of Composable CoT training over traditional methods in specific reasoning tasks."], "limitations": "Generalization may still be limited to certain task types, and reliance on atomic task data may restrict applicability to broader contexts.", "keywords": ["large language models", "chain-of-thought", "compositional reasoning", "zero-shot performance", "multitask learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22645", "pdf": "https://arxiv.org/pdf/2505.22645.pdf", "abs": "https://arxiv.org/abs/2505.22645", "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese", "authors": ["Hanjia Lyu", "Jiebo Luo", "Jian Kang", "Allison Koenecke"], "categories": ["cs.CL", "cs.CY"], "comment": "To appear in the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT '25)", "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).", "AI": {"tldr": "This paper investigates the differential performance of Large Language Models (LLMs) when prompted in Simplified versus Traditional Chinese, highlighting biases and providing benchmark tasks for evaluation.", "motivation": "Understanding LLM performance disparities between Simplified and Traditional Chinese is essential to mitigate representational harms and prevent negative impacts in decision-making contexts such as education and hiring.", "method": "The authors design benchmark tasks assessing regional term and name choices to audit the performance of 11 commercial and open-source LLMs, analyzing their responses based on the prompting language.", "result": "The analysis reveals that most LLMs favor Simplified Chinese in regional term tasks but favor Traditional Chinese in regional name tasks, indicating that biases exist depending on the task and language.", "conclusion": "The study emphasizes the importance of analyzing LLM biases in different cultural contexts and provides a benchmark dataset for future research.", "key_contributions": ["Developed two benchmark tasks for evaluating LLM performance in Simplified vs. Traditional Chinese", "Identified biases in LLM responses related to language and task", "Provided an open-sourced dataset to facilitate reproducible evaluations"], "limitations": "", "keywords": ["Large Language Models", "Simplified Chinese", "Traditional Chinese", "Biases", "Benchmark"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22648", "pdf": "https://arxiv.org/pdf/2505.22648.pdf", "abs": "https://arxiv.org/abs/2505.22648", "title": "WebDancer: Towards Autonomous Information Seeking Agency", "authors": ["Jialong Wu", "Baixuan Li", "Runnan Fang", "Wenbiao Yin", "Liwen Zhang", "Zhengwei Tao", "Dingchu Zhang", "Zekun Xi", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.", "AI": {"tldr": "This paper presents a framework for building end-to-end agentic information seeking agents, validated through the WebDancer agent, achieving high performance in benchmarks.", "motivation": "To address complex real-world problems requiring in-depth information seeking and multi-step reasoning, leveraging advancements in agentic systems.", "method": "The approach consists of four stages: browsing data construction, trajectories sampling, supervised fine-tuning for cold starts, and reinforcement learning for improved generalisation.", "result": "WebDancer exhibits strong performance on information seeking benchmarks GAIA and WebWalkerQA, demonstrating the effectiveness of the proposed training paradigm.", "conclusion": "The study offers valuable insights and pathways for developing more capable agentic models in information seeking tasks.", "key_contributions": ["Framework for building autonomous information seeking agents", "Empirical evaluations showcasing the performance of WebDancer", "Insights for future development of agentic models"], "limitations": "", "keywords": ["agentic systems", "information seeking", "multi-step reasoning", "WebDancer", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.22653", "pdf": "https://arxiv.org/pdf/2505.22653.pdf", "abs": "https://arxiv.org/abs/2505.22653", "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason", "authors": ["Ang Lv", "Ruobing Xie", "Xingwu Sun", "Zhanhui Kang", "Rui Yan"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.", "AI": {"tldr": "This research explores the effects of reward noise on large language models (LLMs) in reinforcement learning, revealing their robustness and the potential of key reasoning phrase rewards to enhance performance.", "motivation": "Investigating the practical implications of reward noise when applying reinforcement learning to post-training large language models, as existing studies often ignore this variable.", "method": "The study conducted experiments using a Qwen-2.5-7B model, manipulating reward outputs and focusing on reasoning pattern rewards (RPR) without strict correctness verification.", "result": "LLMs showed strong robustness to significant reward noise; performance improved from 5% to 72% accuracy in math tasks despite flipping 40% of the reward outputs, and key reasoning phrase rewards enabled models to achieve over 70% accuracy comparable to stricter models.", "conclusion": "The findings highlight the significance of foundational model abilities and suggest that integrating RPR with noisy rewards can improve LLM performance on complex tasks.", "key_contributions": ["Demonstrated robustness of LLMs to reward noise", "Introduced reasoning pattern reward (RPR) approach", "Provided insights for enhancing post-training techniques with noisy rewards"], "limitations": "", "keywords": ["large language models", "reinforcement learning", "reward noise"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22661", "pdf": "https://arxiv.org/pdf/2505.22661.pdf", "abs": "https://arxiv.org/abs/2505.22661", "title": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning", "authors": ["Qingchen Yu", "Zifan Zheng", "Ding Chen", "Simin Niu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "The evaluation of large language models (LLMs) has traditionally relied on\nstatic benchmarks, a paradigm that poses two major limitations: (1) predefined\ntest sets lack adaptability to diverse application domains, and (2)\nstandardized evaluation protocols often fail to capture fine-grained\nassessments of domain-specific knowledge and contextual reasoning abilities. To\novercome these challenges, we propose GuessArena, an adaptive evaluation\nframework grounded in adversarial game-based interactions. Inspired by the\ninteractive structure of the Guess Who I Am? game, our framework seamlessly\nintegrates dynamic domain knowledge modeling with progressive reasoning\nassessment to improve evaluation fidelity. Empirical studies across five\nvertical domains-finance, healthcare, manufacturing, information technology,\nand education-demonstrate that GuessArena effectively distinguishes LLMs in\nterms of domain knowledge coverage and reasoning chain completeness. Compared\nto conventional benchmarks, our method provides substantial advantages in\ninterpretability, scalability, and scenario adaptability.", "AI": {"tldr": "The paper presents GuessArena, an adaptive evaluation framework for large language models (LLMs) that uses adversarial game-based interactions to improve evaluation fidelity in various domains.", "motivation": "Current static benchmarks for evaluating large language models (LLMs) are inadequate due to their lack of adaptability and failure to capture fine-grained assessments of knowledge and reasoning abilities across diverse application domains.", "method": "GuessArena utilizes adversarial game-based interactions, modeled after the Guess Who I Am? game, to dynamically evaluate LLMs by integrating domain knowledge and progressive reasoning assessments.", "result": "Empirical studies show that GuessArena can effectively distinguish LLMs based on domain knowledge coverage and reasoning chain completeness across five domains, including healthcare.", "conclusion": "GuessArena offers significant advantages over traditional benchmarks in terms of interpretability, scalability, and adaptability to various scenarios, enhancing the evaluation of LLMs.", "key_contributions": ["Introduction of an adaptive evaluation framework for LLMs", "Integration of dynamic domain knowledge modeling", "Demonstration of superior evaluation fidelity compared to static benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Adaptive Evaluation", "Domain Knowledge", "Reasoning Assessment", "Games"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22662", "pdf": "https://arxiv.org/pdf/2505.22662.pdf", "abs": "https://arxiv.org/abs/2505.22662", "title": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models", "authors": ["Feng Luo", "Yu-Neng Chuang", "Guanchu Wang", "Hoang Anh Duy Le", "Shaochen Zhong", "Hongyi Liu", "Jiayi Yuan", "Yang Sui", "Vladimir Braverman", "Vipin Chaudhary", "Xia Hu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning.", "AI": {"tldr": "AutoL2S is a framework that allows LLMs to dynamically adjust reasoning lengths based on question complexity, improving efficiency.", "motivation": "To address the inefficiencies in reasoning paths generated by LLMs for easy questions, which often result in increased inference costs and latency.", "method": "AutoL2S is a model-agnostic framework that uses a learned paradigm, allowing LLMs to decide when to use longer or shorter reasoning based on question complexity, utilizing an <EASY> token for guidance.", "result": "With AutoL2S, the length of reasoning paths can be reduced by up to 57% while maintaining performance levels.", "conclusion": "AutoL2S effectively improves the ability of LLMs to generate shorter and higher quality reasoning paths, enhancing scalability and efficiency in reasoning.", "key_contributions": ["Introduction of the Auto Long-Short Reasoning (AutoL2S) framework", "Use of the <EASY> token for annotating reasoning complexity", "Demonstrated significant reduction in reasoning length without performance loss."], "limitations": "", "keywords": ["Large Language Models", "Reasoning", "Efficiency", "Natural Language Processing", "Dynamic Adaptation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.21964", "pdf": "https://arxiv.org/pdf/2505.21964.pdf", "abs": "https://arxiv.org/abs/2505.21964", "title": "UI-Evol: Automatic Knowledge Evolving for Computer Use Agents", "authors": ["Ziyun Zhang", "Xinyi Liu", "Xiaoyi Zhang", "Jun Wang", "Gang Chen", "Yan Lu"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "External knowledge has played a crucial role in the recent development of\ncomputer use agents. We identify a critical knowledge-execution gap: retrieved\nknowledge often fails to translate into effective real-world task execution.\nOur analysis shows even 90\\% correct knowledge yields only 41\\% execution\nsuccess rate. To bridge this gap, we propose UI-Evol, a plug-and-play module\nfor autonomous GUI knowledge evolution. UI-Evol consists of two stages: a\nRetrace Stage that extracts faithful objective action sequences from actual\nagent-environment interactions, and a Critique Stage that refines existing\nknowledge by comparing these sequences against external references. We conduct\ncomprehensive experiments on the OSWorld benchmark with the state-of-the-art\nAgent S2. Our results demonstrate that UI-Evol not only significantly boosts\ntask performance but also addresses a previously overlooked issue of high\nbehavioral standard deviation in computer use agents, leading to superior\nperformance on computer use tasks and substantially improved agent reliability.", "AI": {"tldr": "This paper introduces UI-Evol, a plug-and-play module aimed at enhancing task execution in computer use agents by bridging the knowledge-execution gap through a dual-stage process.", "motivation": "To address the critical knowledge-execution gap where high knowledge accuracy does not translate to effective real-world task execution in computer use agents.", "method": "The proposed UI-Evol system includes a Retrace Stage that extracts action sequences from agent-environment interactions and a Critique Stage that refines this knowledge by comparing it against external references.", "result": "Experiments on the OSWorld benchmark show that UI-Evol significantly improves task performance and reduces behavioral standard deviation in agents, leading to enhanced reliability and effectiveness in task execution.", "conclusion": "UI-Evol effectively bridges the knowledge-execution gap, resulting in improved performance and reliability for computer use agents in real-world tasks.", "key_contributions": ["Introduction of UI-Evol as a solution to the knowledge-execution gap.", "Demonstrated significant performance improvements on the OSWorld benchmark.", "Addressed high behavioral standard deviation in computer use agents."], "limitations": "", "keywords": ["knowledge-execution gap", "computer use agents", "UI-Evol"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966.pdf", "abs": "https://arxiv.org/abs/2505.21966", "title": "MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "16 pages and 15 figures", "summary": "We introduce MapStory, an LLM-powered animation authoring tool that generates\neditable map animation sequences directly from natural language text. Given a\nuser-written script, MapStory leverages an agentic architecture to\nautomatically produce a scene breakdown, which decomposes the script into key\nanimation building blocks such as camera movements, visual highlights, and\nanimated elements. Our system includes a researcher component that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nthe automatic extraction of relevant regions, paths, and coordinates while\nallowing users to edit and query for changes or additional information to\nrefine the results. Additionally, users can fine-tune parameters of these\nblocks through an interactive timeline editor. We detail the system's design\nand architecture, informed by formative interviews with professional animators\nand an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories.", "AI": {"tldr": "MapStory is an LLM-powered tool for generating editable map animation sequences from text, facilitating easier animation creation.", "motivation": "To simplify the process of creating map animations by leveraging natural language inputs and LLM technology.", "method": "MapStory decomposes scripts into key animation elements, queries geospatial information using a web-connected LLM, and provides an interactive editor for users to fine-tune animations.", "result": "The evaluation showed that users can create map animations more easily and iteratively, enhancing creativity and reducing barriers to entry.", "conclusion": "MapStory improves the animation creation process, making it accessible and efficient for users without extensive technical backgrounds.", "key_contributions": ["Introduction of an LLM-powered animation authoring tool for map animations.", "Integration of geospatial querying and interactive editing features.", "Usability studies demonstrating improved animation creation workflows."], "limitations": "", "keywords": ["MapStory", "animation authoring tool", "LLM", "geospatial information", "user interface"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2401.06769", "pdf": "https://arxiv.org/pdf/2401.06769.pdf", "abs": "https://arxiv.org/abs/2401.06769", "title": "Machine Translation Models are Zero-Shot Detectors of Translation Direction", "authors": ["Michelle Wastl", "Jannis Vamvas", "Rico Sennrich"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Detecting the translation direction of parallel text has applications for\nmachine translation training and evaluation, but also has forensic applications\nsuch as resolving plagiarism or forgery allegations. In this work, we explore\nan unsupervised approach to translation direction detection based on the simple\nhypothesis that\n$p(\\text{translation}|\\text{original})>p(\\text{original}|\\text{translation})$,\nmotivated by the well-known simplification effect in translationese or\nmachine-translationese. In experiments with massively multilingual machine\ntranslation models across 20 translation directions, we confirm the\neffectiveness of the approach for high-resource language pairs, achieving\ndocument-level accuracies of 82--96% for NMT-produced translations, and 60--81%\nfor human translations, depending on the model used. Code and demo are\navailable at https://github.com/ZurichNLP/translation-direction-detection", "AI": {"tldr": "This paper presents an unsupervised method for detecting translation direction in parallel texts, demonstrating its effectiveness across various multilingual machine translation models.", "motivation": "The detection of translation direction has important implications for machine translation training, evaluation, and forensic applications like plagiarism detection.", "method": "An unsupervised approach utilizing the hypothesis that the probability of obtaining a translation given the original is greater than that of obtaining the original given the translation.", "result": "Achieved high document-level accuracies of 82--96% for NMT translations and 60--81% for human translations across 20 high-resource language pairs.", "conclusion": "The proposed method effectively detects translation direction, showing promise for both machine-generated and human translations.", "key_contributions": ["Introduction of an unsupervised approach to translation direction detection", "Validation of the method across multiple machine translation models", "High accuracies for translation direction detection in multilingual contexts"], "limitations": "", "keywords": ["translation direction", "machine translation", "forensic applications", "parallel text", "unsupervised learning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2402.16596", "pdf": "https://arxiv.org/pdf/2402.16596.pdf", "abs": "https://arxiv.org/abs/2402.16596", "title": "Tracking Semantic Change in Slovene: A Novel Dataset and Optimal Transport-Based Distance", "authors": ["Marko Pranjić", "Kaja Dobrovoljc", "Senja Pollak", "Matej Martinc"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "In this paper, we focus on the detection of semantic changes in Slovene, a\nless resourced Slavic language with two million speakers. Detecting and\ntracking semantic changes provides insight into the evolution of language\ncaused by changes in society and culture. We present the first Slovene dataset\nfor evaluating semantic change detection systems, which contains aggregated\nsemantic change scores for 104 target words obtained from more than 3,000\nmanually annotated sentence pairs. We analyze an important class of measures of\nsemantic change metrics based on the Average pairwise distance and identify\nseveral limitations. To address these limitations, we propose a novel metric\nbased on regularized optimal transport, which offers a more robust framework\nfor quantifying semantic change. We provide a comprehensive evaluation of\nvarious existing semantic change detection methods and associated semantic\nchange measures on our dataset. Through empirical testing, we demonstrate that\nour proposed approach, leveraging regularized optimal transport, achieves\neither matching or improved performance compared to baseline approaches.", "AI": {"tldr": "This paper introduces the first Slovene dataset for semantic change detection and proposes a new metric based on regularized optimal transport to improve the robustness of detecting semantic changes.", "motivation": "To understand language evolution due to societal and cultural changes, particularly in the Slovene language, which is less resourced and has unique challenges in semantic change detection.", "method": "The paper presents a dataset of semantic change scores for 104 Slovene words derived from over 3,000 annotated sentence pairs, analyzing existing semantic change metrics and proposing a new metric based on regularized optimal transport.", "result": "The proposed optimal transport-based metric shows matching or improved performance compared to existing methods for semantic change detection in the Slovene language.", "conclusion": "The paper contributes to the field of semantic change detection by providing a valuable dataset and a more effective metric that enhances the reliability of such systems.", "key_contributions": ["First Slovene dataset for semantic change detection", "Introduction of a novel metric using regularized optimal transport", "Comprehensive evaluation of existing methods against the new metric"], "limitations": "Identified limitations in several existing semantic change metrics.", "keywords": ["semantic change", "Slovene language", "optimal transport", "language evolution", "dataset"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2404.06762", "pdf": "https://arxiv.org/pdf/2404.06762.pdf", "abs": "https://arxiv.org/abs/2404.06762", "title": "Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Geyu Lin", "Nancy F. Chen"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced\nlearning experience. The emergence of large language models (LLMs) further\nenables better human-machine interaction, and facilitates the development of\nconversational ITSs in various disciplines such as math and language learning.\nIn dialogic teaching, recognizing and adapting to individual characteristics\ncan significantly enhance student engagement and learning efficiency. However,\ncharacterizing and simulating student's persona remain challenging in training\nand evaluating conversational ITSs. In this work, we propose a framework to\nconstruct profiles of different student groups by refining and integrating both\ncognitive and noncognitive aspects, and leverage LLMs for personality-aware\nstudent simulation in a language learning scenario. We further enhance the\nframework with multi-aspect validation, and conduct extensive analysis from\nboth teacher and student perspectives. Our experimental results show that\nstate-of-the-art LLMs can produce diverse student responses according to the\ngiven language ability and personality traits, and trigger teacher's adaptive\nscaffolding strategies.", "AI": {"tldr": "This paper introduces a framework for creating student profiles that integrate cognitive and noncognitive aspects, utilizing large language models (LLMs) for personality-aware simulation in Intelligent Tutoring Systems (ITSs) specifically for language learning.", "motivation": "The aim is to enhance engagement and learning efficiency in dialogic teaching by recognizing and adapting to individual student characteristics through personalized interactions in ITSs.", "method": "The paper proposes a framework that constructs profiles of different student groups by refining cognitive and noncognitive aspects and employs LLMs for simulating diverse student personas in language learning contexts.", "result": "Extensive analysis shows that the proposed framework enables LLMs to generate varied responses reflecting students' language abilities and personality traits, which can help teachers adapt their instructional strategies effectively.", "conclusion": "The findings indicate that integrating personality-aware simulations into ITSs can significantly improve teacher responsiveness and student engagement, confirming the potential of LLMs in educational contexts.", "key_contributions": ["Development of a framework for personality-aware student simulation", "Integration of cognitive and noncognitive aspects in student profiling", "Validation through extensive analysis involving teachers and students"], "limitations": "Challenges remain in accurately characterizing and simulating students' personas for effective training and evaluation of conversational ITSs.", "keywords": ["Intelligent Tutoring Systems", "Personality-aware simulation", "Large Language Models", "Education", "Human-computer interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2405.09948", "pdf": "https://arxiv.org/pdf/2405.09948.pdf", "abs": "https://arxiv.org/abs/2405.09948", "title": "Mitigating Text Toxicity with Counterfactual Generation", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nina Achache", "Victor Legrand", "Nicolas Chesneau", "Annabelle Blangero", "Juliette Murris", "Marie-Jeanne Lesot"], "categories": ["cs.CL"], "comment": null, "summary": "Toxicity mitigation consists in rephrasing text in order to remove offensive\nor harmful meaning. Neural natural language processing (NLP) models have been\nwidely used to target and mitigate textual toxicity. However, existing methods\nfail to detoxify text while preserving the initial non-toxic meaning at the\nsame time. In this work, we propose to apply counterfactual generation methods\nfrom the eXplainable AI (XAI) field to target and mitigate textual toxicity. In\nparticular, we perform text detoxification by applying local feature importance\nand counterfactual generation methods to a toxicity classifier distinguishing\nbetween toxic and non-toxic texts. We carry out text detoxification through\ncounterfactual generation on three datasets and compare our approach to three\ncompetitors. Automatic and human evaluations show that recently developed NLP\ncounterfactual generators can mitigate toxicity accurately while better\npreserving the meaning of the initial text as compared to classical\ndetoxification methods. Finally, we take a step back from using automated\ndetoxification tools, and discuss how to manage the polysemous nature of\ntoxicity and the risk of malicious use of detoxification tools. This work is\nthe first to bridge the gap between counterfactual generation and text\ndetoxification and paves the way towards more practical application of XAI\nmethods.", "AI": {"tldr": "This paper proposes a new method for text detoxification that uses counterfactual generation from explainable AI to mitigate toxicity while preserving the original tone of the text.", "motivation": "To improve existing text detoxification methods that often fail to preserve non-toxic meaning while removing toxicity.", "method": "Utilizes counterfactual generation techniques alongside local feature importance analysis within a toxicity classifier framework to enhance the detoxification process.", "result": "Demonstrated that the proposed counterfactual generation approach mitigates toxicity more effectively than classical methods, as evidenced by both automatic and human evaluations conducted on three datasets.", "conclusion": "The integration of counterfactual generation with text detoxification is novel and presents new avenues for applying explainable AI in this domain, though concerns around misuse of detoxification tools are discussed.", "key_contributions": ["First application of counterfactual generation to text detoxification.", "Demonstrated improvements in preserving non-toxic meanings during detoxification.", "Explored the implications and risks associated with the use of detoxification tools."], "limitations": "Discussion on ethical concerns and potential misuse of detoxification technology was limited.", "keywords": ["text detoxification", "counterfactual generation", "explainable AI", "toxic text mitigation", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2406.09325", "pdf": "https://arxiv.org/pdf/2406.09325.pdf", "abs": "https://arxiv.org/abs/2406.09325", "title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space", "authors": ["Tomer Ashuach", "Martin Tutek", "Yonatan Belinkov"], "categories": ["cs.CL", "I.2.7"], "comment": "ACL 2025 Findings, 24 pages, 4 figures", "summary": "Language models (LMs) risk inadvertently memorizing and divulging sensitive\nor personally identifiable information (PII) seen in training data, causing\nprivacy concerns. Current approaches to address this issue involve costly\ndataset scrubbing, or model filtering through unlearning and model editing,\nwhich can be bypassed through extraction attacks. We propose REVS, a novel\nnon-gradient-based method for unlearning sensitive information from LMs. REVS\nidentifies and modifies a small subset of neurons relevant for constituent\ntokens that form sensitive information. To adequately evaluate our method on\ntruly sensitive information, we curate three datasets: email and URL datasets\nnaturally memorized by the models, and a synthetic social security number\ndataset that we tune the models to memorize. Compared to other methods, REVS\ndemonstrates superior performance in unlearning sensitive information and\nrobustness to extraction attacks, while retaining underlying model integrity.", "AI": {"tldr": "REVS is a new method for unlearning sensitive information from language models, outperforming existing techniques while preserving model integrity.", "motivation": "Address privacy concerns arising from the inadvertent memorization of sensitive information by language models during training.", "method": "REVS employs a non-gradient-based approach that identifies and modifies specific neurons associated with sensitive tokens in the model.", "result": "REVS outperforms existing unlearning methods in removing sensitive information and is more robust against extraction attacks, while maintaining the integrity of the model.", "conclusion": "The proposed method effectively mitigates privacy risks without compromising model performance, forming a better solution to the challenges of sensitive information memorization in language models.", "key_contributions": ["Introduction of REVS for unlearning sensitive data from LMs", "Demonstration of superior performance compared to existing methods", "Curated datasets for evaluating sensitive information memorization"], "limitations": "", "keywords": ["language models", "privacy", "unlearning", "sensitive information", "neural networks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.14023", "pdf": "https://arxiv.org/pdf/2406.14023.pdf", "abs": "https://arxiv.org/abs/2406.14023", "title": "Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective", "authors": ["Yuchen Wen", "Keping Bi", "Wei Chen", "Jiafeng Guo", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development. Our code, data and benchmarks are\navailable at https://github.com/yuchenwen1/ImplicitBiasPsychometricEvaluation\nand https://github.com/yuchenwen1/BUMBLE.", "AI": {"tldr": "This paper evaluates implicit bias in large language models (LLMs) using psychometric principles, proposing attack methods and benchmarks to assess ethical risks.", "motivation": "To investigate concerns that LLMs may propagate implicit biases that adversely affect certain demographics without explicit harmful language.", "method": "The authors propose three psychometric attack approaches (Disguise, Deception, and Teaching) and create two benchmarks for evaluation: a bilingual dataset with 2.7K instances and BUMBLE with 12.7K instances covering various bias types.", "result": "Evaluations reveal that these methods are more effective in revealing LLMs' inner biases compared to existing baselines.", "conclusion": "The developed methodologies and benchmarks provide tools for evaluating LLMs' ethical risks, aiming to enhance accountability in their development.", "key_contributions": ["Three novel psychometric attack approaches for evaluating implicit bias in LLMs.", "Two substantial bias benchmarks (bilingual dataset and BUMBLE) for comprehensive analysis.", "Demonstrated effectiveness in eliciting biases in popular LLMs, surpassing previous methods."], "limitations": "", "keywords": ["Large Language Models", "Implicit Bias", "Psychometrics", "Ethics in AI", "Benchmarking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.14737", "pdf": "https://arxiv.org/pdf/2406.14737.pdf", "abs": "https://arxiv.org/abs/2406.14737", "title": "Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial Alterations to the False Belief Task?", "authors": ["Zhiqiang Pi", "Annapurna Vadaparty", "Benjamin K. Bergen", "Cameron R. Jones"], "categories": ["cs.CL"], "comment": null, "summary": "Recent empirical results have sparked a debate about whether or not Large\nLanguage Models (LLMs) are capable of Theory of Mind (ToM). While some have\nfound LLMs to be successful on ToM evaluations such as the False Belief task,\nothers have shown that their performance is not robust against trivial\nalterations to stimuli. In this paper, we introduce SCALPEL -- a technique to\nincrementally modify stimuli to test different specific hypotheses about why\nLLMs fail -- and apply this method to the \"transparent-access\" modification of\nthe unexpected contents task. Our results suggest that LLMs often do poorly\nbecause they fail to make essential common-sense inferences, such as that\nseeing a transparent container implies recognizing its contents. We conclude\nthat while modern LLMs go beyond mere pattern matching, they still fall short\nof robust human-like ToM. We argue that SCALPEL can help cognitive scientists\nexamine LLMs' capabilities in finer detail and provide insight into alternative\nmechanisms by which tasks that are used to assess human cognition might be\ncompleted.", "AI": {"tldr": "This paper introduces SCALPEL, a technique to incrementally modify stimuli for testing reasons behind Large Language Models' (LLMs) failures in Theory of Mind (ToM) tasks, revealing common-sense inference shortcomings.", "motivation": "To explore the capabilities of LLMs in Theory of Mind tasks and understand their failures in specific contexts.", "method": "The paper presents SCALPEL, which incrementally modifies stimuli to test hypotheses about LLM performance on ToM evaluations.", "result": "LLMs struggle in ToM tasks primarily due to a lack of essential common-sense inferences, particularly in understanding transparent containers and their contents.", "conclusion": "Although LLMs exhibit advanced capabilities, they do not yet achieve robust human-like Theory of Mind, highlighting potential areas for further cognitive exploration.", "key_contributions": ["Introduction of SCALPEL technique for hypothesis testing on LLMs", "Insights into common-sense inferences in LLMs", "Discussion on the implications for cognitive science research"], "limitations": "", "keywords": ["Large Language Models", "Theory of Mind", "Cognitive Science", "Common-Sense Inference", "SCALPEL"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.16508", "pdf": "https://arxiv.org/pdf/2406.16508.pdf", "abs": "https://arxiv.org/abs/2406.16508", "title": "Large Vocabulary Size Improves Large Language Models", "authors": ["Sho Takase", "Ryokan Ri", "Shun Kiyono", "Takuya Kato"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "This paper empirically investigates the relationship between subword\nvocabulary size and the performance of large language models (LLMs) to provide\ninsights on how to define the vocabulary size. Experimental results show that\nlarger vocabulary sizes lead to better performance in LLMs. Moreover, we\nconsider a continual training scenario where a pre-trained language model is\ntrained on a different target language. We introduce a simple method to use a\nnew vocabulary instead of the pre-defined one. We show that using the new\nvocabulary outperforms the model with the vocabulary used in pre-training.", "AI": {"tldr": "This paper examines how subword vocabulary size impacts the performance of large language models, finding that larger vocabularies yield better results and introducing a new method for vocabulary adaptation in continual training.", "motivation": "To provide insights on defining vocabulary size for optimizing large language models' performance.", "method": "The authors conducted experiments to evaluate the relationship between subword vocabulary size and LLM performance, including a scenario of continual training with a new vocabulary.", "result": "Results indicate that increasing vocabulary size improves LLM performance, and the proposed method using a new vocabulary outperforms the original pre-training vocabulary during continual training.", "conclusion": "The study concludes that vocabulary size is a significant factor in LLM effectiveness, and the introduction of new vocabulary methods can enhance model performance in new language contexts.", "key_contributions": ["Demonstrated the influence of vocabulary size on LLM performance.", "Proposed a new method for vocabulary adaptation in continual training scenarios.", "Empirical evidence supporting the benefits of larger vocabularies for LLMs."], "limitations": "", "keywords": ["subword vocabulary", "large language models", "performance evaluation", "continual training", "vocabulary adaptation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2407.07004", "pdf": "https://arxiv.org/pdf/2407.07004.pdf", "abs": "https://arxiv.org/abs/2407.07004", "title": "Empirical analysis of binding precedent efficiency in Brazilian Supreme Court via case classification", "authors": ["Raphaël Tinarrage", "Henrique Ennes", "Lucas Resck", "Lucas T. Gomes", "Jean R. Ponciano", "Jorge Poco"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50 (Primary), 68T07 (Secondary)"], "comment": "Document similar to published version. Contains 62 pages and 21\n  figures", "summary": "Binding precedents (s\\'umulas vinculantes) constitute a juridical instrument\nunique to the Brazilian legal system and whose objectives include the\nprotection of the Federal Supreme Court against repetitive demands. Studies of\nthe effectiveness of these instruments in decreasing the Court's exposure to\nsimilar cases, however, indicate that they tend to fail in such a direction,\nwith some of the binding precedents seemingly creating new demands. We\nempirically assess the legal impact of five binding precedents, 11, 14, 17, 26,\nand 37, at the highest Court level through their effects on the legal subjects\nthey address. This analysis is only possible through the comparison of the\nCourt's ruling about the precedents' themes before they are created, which\nmeans that these decisions should be detected through techniques of Similar\nCase Retrieval, which we tackle from the angle of Case Classification. The\ncontributions of this article are therefore twofold: on the mathematical side,\nwe compare the use of different methods of Natural Language Processing --\nTF-IDF, LSTM, Longformer, and regex -- for Case Classification, whereas on the\nlegal side, we contrast the inefficiency of these binding precedents with a set\nof hypotheses that may justify their repeated usage. We observe that the TF-IDF\nmodels performed slightly better than LSTM and Longformer when compared through\ncommon metrics; however, the deep learning models were able to detect certain\nimportant legal events that TF-IDF missed. On the legal side, we argue that the\nreasons for binding precedents to fail in responding to repetitive demand are\nheterogeneous and case-dependent, making it impossible to single out a specific\ncause. We identify five main hypotheses, which are found in different\ncombinations in each of the precedents studied.", "AI": {"tldr": "This paper explores the effectiveness of binding precedents in the Brazilian legal system, assessing their legal impact through Case Classification and comparing various Natural Language Processing methods.", "motivation": "To evaluate the effectiveness of binding precedents in reducing repetitive legal demands on the Federal Supreme Court of Brazil.", "method": "The study employs methods of Natural Language Processing, including TF-IDF, LSTM, Longformer, and regex, for Classifying cases. It compares the Court's rulings before and after the establishment of five specific binding precedents.", "result": "TF-IDF models outperformed LSTM and Longformer on standard metrics, although deep learning models detected significant legal events that TF-IDF missed. The study identifies five main hypotheses explaining the inefficacy of binding precedents.", "conclusion": "The failure of binding precedents to alleviate repetitive demands stems from heterogeneous and case-dependent reasons, making it challenging to isolate single causative factors.", "key_contributions": ["Comparison of different NLP methods for Case Classification", "Identification of hypotheses explaining the inefficiency of binding precedents", "Empirical assessment of the legal impact of specific binding precedents"], "limitations": "", "keywords": ["binding precedents", "Natural Language Processing", "Case Classification", "Brazilian legal system", "legal impact"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2409.04122", "pdf": "https://arxiv.org/pdf/2409.04122.pdf", "abs": "https://arxiv.org/abs/2409.04122", "title": "Prompt-based Personality Profiling: Reinforcement Learning for Relevance Filtering", "authors": ["Jan Hofmann", "Cornelia Sindermann", "Roman Klinger"], "categories": ["cs.CL"], "comment": "Accepted to the REALM workshop at ACL 2025", "summary": "Author profiling is the task of inferring characteristics about individuals\nby analyzing content they share. Supervised machine learning still dominates\nautomatic systems that perform this task, despite the popularity of prompting\nlarge language models to address natural language understanding tasks. One\nreason is that the classification instances consist of large amounts of posts,\npotentially a whole user profile, which may exceed the input length of\nTransformers. Even if a model can use a large context window, the entirety of\nposts makes the application of API-accessed black box systems costly and slow,\nnext to issues which come with such \"needle-in-the-haystack\" tasks. To mitigate\nthis limitation, we propose a new method for author profiling which aims at\ndistinguishing relevant from irrelevant content first, followed by the actual\nuser profiling only with relevant data. To circumvent the need for\nrelevance-annotated data, we optimize this relevance filter via reinforcement\nlearning with a reward function that utilizes the zero-shot capabilities of\nlarge language models. We evaluate our method for Big Five personality trait\nprediction on two Twitter corpora. On publicly available real-world data with a\nskewed label distribution, our method shows similar efficacy to using all posts\nin a user profile, but with a substantially shorter context. An evaluation on a\nversion of these data balanced with artificial posts shows that the filtering\nto relevant posts leads to a significantly improved accuracy of the\npredictions.", "AI": {"tldr": "This paper introduces a method for author profiling that filters relevant content first before performing user profiling, utilizing reinforcement learning to fine-tune relevance without annotated data.", "motivation": "To address the challenges in author profiling due to large post volumes that exceed Transformer input limits and the inefficiencies of using API-accessed models.", "method": "A new author profiling approach that distinguishes relevant from irrelevant content using a reinforcement learning-optimized relevance filter, capitalizing on the zero-shot abilities of large language models.", "result": "The proposed method demonstrates comparable efficacy to traditional methods using all posts while reducing context length significantly, and leads to improved accuracy on balanced datasets by filtering relevant posts.", "conclusion": "The filtering approach not only addresses input limitations but also enhances prediction accuracy for personality traits in author profiling tasks.", "key_contributions": ["Introduction of a relevance filtering method for author profiling", "Utilization of reinforcement learning to optimize relevance without annotated data", "Empirical evaluation demonstrating improved accuracy while using reduced context"], "limitations": "", "keywords": ["Author Profiling", "Machine Learning", "Reinforcement Learning", "Large Language Models", "Personality Prediction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.08351", "pdf": "https://arxiv.org/pdf/2410.08351.pdf", "abs": "https://arxiv.org/abs/2410.08351", "title": "Nonlinear second-order dynamics describe labial constriction trajectories across languages and contexts", "authors": ["Michael C. Stern", "Jason A. Shaw"], "categories": ["cs.CL", "nlin.AO"], "comment": null, "summary": "We investigate the dynamics of labial constriction trajectories during the\nproduction of /b/ and /m/ in English and Mandarin. We find that, across\nlanguages and contexts, the ratio of instantaneous displacement to\ninstantaneous velocity generally follows an exponential decay curve from\nmovement onset to movement offset. We formalize this empirical discovery in a\ndifferential equation and, in combination with an assumption of point attractor\ndynamics, derive a nonlinear second-order dynamical system describing labial\nconstriction trajectories. The equation has only two parameters, T and r. T\ncorresponds to the target state and r corresponds to movement rapidity. Thus,\neach of the parameters corresponds to a phonetically relevant dimension of\ncontrol. Nonlinear regression demonstrates that the model provides excellent\nfits to individual movement trajectories. Moreover, trajectories simulated from\nthe model qualitatively match empirical trajectories, and capture key kinematic\nvariables like duration, peak velocity, and time to achieve peak velocity. The\nmodel constitutes a proposal for the dynamics of individual articulatory\nmovements, and thus offers a novel foundation from which to understand\nadditional influences on articulatory kinematics like prosody, inter-movement\ncoordination, and stochastic noise.", "AI": {"tldr": "This study analyzes labial constriction trajectories during the production of /b/ and /m/ sounds in English and Mandarin, proposing a nonlinear dynamical model to explain these movements.", "motivation": "To better understand the dynamics of articulatory movements and how various factors influence these dynamics in speech production.", "method": "The study uses differential equations to model labial constriction trajectories, testing a second-order dynamical system that incorporates parameters representing target state and movement rapidity.", "result": "The model demonstrates excellent fits to both generalized and individual articulation trajectories, effectively capturing key kinematic variables.", "conclusion": "The proposed model provides a new understanding of articulatory movements, potentially informing further studies on speech dynamics and related phenomena.", "key_contributions": ["Introduction of a nonlinear dynamics model for articulatory movements", "Empirical validation of the model against speech production data", "Insights into the relationship between movement dynamics and phonetic control parameters"], "limitations": "The model's applicability might be limited to specific speech sounds and does not account for all articulatory movements.", "keywords": ["articulatory dynamics", "labial constriction", "speech production", "nonlinear model", "kinematics"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2410.08820", "pdf": "https://arxiv.org/pdf/2410.08820.pdf", "abs": "https://arxiv.org/abs/2410.08820", "title": "Which Demographics do LLMs Default to During Annotation?", "authors": ["Johannes Schäfer", "Aidan Combs", "Christopher Bagdon", "Jiahui Li", "Nadine Probol", "Lynn Greschner", "Sean Papay", "Yarik Menchaca Resendiz", "Aswathy Velutharambath", "Amelie Wührl", "Sabine Weber", "Roman Klinger"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects.", "AI": {"tldr": "This paper investigates how demographics and cultural backgrounds of annotators influence label variations in text annotation and how large language models (LLMs) can mimic or manipulate these variations through demographic conditioning.", "motivation": "Understanding label variations in text annotation is crucial to represent diverse societal members adequately. This research aims to explore the biases in LLMs and how demographic information can influence their outputs in text annotation.", "method": "The study evaluates the responses of LLMs to prompts conditioned by demographics versus non-demographic and placebo conditions. It specifically assesses how LLMs mimic human annotator attributes related to politeness and offensiveness using the POPQUORN dataset.", "result": "The investigation reveals significant influences of gender, race, and age in responses affected by demographic prompting, contrasting with earlier findings that indicated no demographic effects on LLM outputs.", "conclusion": "Acknowledging and addressing demographic influences in text annotation can enhance the representation and quality of machine-generated labels, underscoring the importance of diverse inputs for LLMs.", "key_contributions": ["Combines research on LLM biases and diversity in output based on demographic information.", "Utilizes the POPQUORN dataset to analyze human label variations in LLMs.", "Demonstrates notable influences of demographic factors on LLM responses, challenging previous studies."], "limitations": "The research is limited to the specific contexts of politeness and offensiveness annotations and may not generalize to other types of textual annotations.", "keywords": ["demographics", "text annotation", "large language models", "bias", "popularity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.13080", "pdf": "https://arxiv.org/pdf/2410.13080.pdf", "abs": "https://arxiv.org/abs/2410.13080", "title": "Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models", "authors": ["Linhao Luo", "Zicheng Zhao", "Gholamreza Haffari", "Yuan-Fang Li", "Chen Gong", "Shirui Pan"], "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "Large language models (LLMs) have demonstrated impressive reasoning\nabilities, but they still struggle with faithful reasoning due to knowledge\ngaps and hallucinations. To address these issues, knowledge graphs (KGs) have\nbeen utilized to enhance LLM reasoning through their structured knowledge.\nHowever, existing KG-enhanced methods, either retrieval-based or agent-based,\nencounter difficulties in accurately retrieving knowledge and efficiently\ntraversing KGs at scale. In this work, we introduce graph-constrained reasoning\n(GCR), a novel framework that bridges structured knowledge in KGs with\nunstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures\nfaithful KG-grounded reasoning by integrating KG structure into the LLM\ndecoding process through KG-Trie, a trie-based index that encodes KG reasoning\npaths. KG-Trie constrains the decoding process, allowing LLMs to directly\nreason on graphs and generate faithful reasoning paths grounded in KGs.\nAdditionally, GCR leverages a lightweight KG-specialized LLM for\ngraph-constrained reasoning alongside a powerful general LLM for inductive\nreasoning over multiple reasoning paths, resulting in accurate reasoning with\nzero reasoning hallucination. Extensive experiments on several KGQA benchmarks\ndemonstrate that GCR achieves state-of-the-art performance and exhibits strong\nzero-shot generalizability to unseen KGs without additional training.", "AI": {"tldr": "Graph-constrained reasoning (GCR) enhances LLMs by integrating knowledge graphs (KGs) into the reasoning process, ensuring accurate and faithful reasoning without hallucinations.", "motivation": "To tackle reasoning errors in large language models (LLMs) caused by knowledge gaps and hallucinations, leveraging structured knowledge from knowledge graphs (KGs) is proposed.", "method": "The proposed method GCR integrates the structure of knowledge graphs into the decoding process of LLMs through a novel KG-Trie index, allowing direct reasoning on graphs to produce accurate outcomes.", "result": "GCR shows state-of-the-art performance on KGQA benchmarks and demonstrates strong zero-shot generalization to unseen KGs, effectively eliminating reasoning hallucinations.", "conclusion": "By bridging structured and unstructured knowledge, GCR represents a significant advancement in improving the reasoning capabilities of LLMs, making them more reliable.", "key_contributions": ["Introduction of graph-constrained reasoning (GCR) framework", "Development of KG-Trie for efficient graph reasoning", "Achievement of state-of-the-art performance on KGQA benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Knowledge Graphs", "Reasoning", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.16665", "pdf": "https://arxiv.org/pdf/2410.16665.pdf", "abs": "https://arxiv.org/abs/2410.16665", "title": "SafetyAnalyst: Interpretable, Transparent, and Steerable Safety Moderation for AI Behavior", "authors": ["Jing-Jing Li", "Valentina Pyatkin", "Max Kleiman-Weiner", "Liwei Jiang", "Nouha Dziri", "Anne G. E. Collins", "Jana Schaich Borg", "Maarten Sap", "Yejin Choi", "Sydney Levine"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to ICML 2025", "summary": "The ideal AI safety moderation system would be both structurally\ninterpretable (so its decisions can be reliably explained) and steerable (to\nalign to safety standards and reflect a community's values), which current\nsystems fall short on. To address this gap, we present SafetyAnalyst, a novel\nAI safety moderation framework. Given an AI behavior, SafetyAnalyst uses\nchain-of-thought reasoning to analyze its potential consequences by creating a\nstructured \"harm-benefit tree,\" which enumerates harmful and beneficial actions\nand effects the AI behavior may lead to, along with likelihood, severity, and\nimmediacy labels that describe potential impacts on stakeholders. SafetyAnalyst\nthen aggregates all effects into a harmfulness score using 28 fully\ninterpretable weight parameters, which can be aligned to particular safety\npreferences. We applied this framework to develop an open-source LLM prompt\nsafety classification system, distilled from 18.5 million harm-benefit features\ngenerated by frontier LLMs on 19k prompts. On comprehensive benchmarks, we show\nthat SafetyAnalyst (average F1=0.81) outperforms existing moderation systems\n(average F1$<$0.72) on prompt safety classification, while offering the\nadditional advantages of interpretability, transparency, and steerability.", "AI": {"tldr": "SafetyAnalyst is a new AI safety moderation framework that enhances interpretability and steerability in AI decision-making through a harm-benefit analysis.", "motivation": "Current AI safety moderation systems lack structural interpretability and the ability to align with community values. SafetyAnalyst aims to fill this gap.", "method": "SafetyAnalyst analyzes AI behavior using chain-of-thought reasoning to create a harm-benefit tree, aggregating potential actions into a harmfulness score based on 28 interpretable parameters.", "result": "SafetyAnalyst outperforms current moderation systems on prompt safety classification tasks, achieving an average F1 score of 0.81 compared to less than 0.72 for others, while providing improved interpretability and transparency.", "conclusion": "SafetyAnalyst provides a significant advancement in AI safety moderation, making it a valuable tool for ensuring AI alignment with safety standards.", "key_contributions": ["Introduction of a harm-benefit tree for AI behavior analysis", "Achievement of high performance in prompt safety classification", "Provision of a framework that is interpretable and steerable"], "limitations": "", "keywords": ["AI moderation", "safety framework", "harm-benefit analysis"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.18436", "pdf": "https://arxiv.org/pdf/2410.18436.pdf", "abs": "https://arxiv.org/abs/2410.18436", "title": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching", "authors": ["Seoyeon Kim", "Huiseo Kim", "Chanjun Park", "Jinyoung Yeo", "Dongha Lee"], "categories": ["cs.CL"], "comment": "25 pages, 8 figures", "summary": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can 'activate', or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks.", "AI": {"tldr": "This paper explores how code-switching can enhance large language models' performance on low-resource language tasks by activating language-specific knowledge.", "motivation": "Despite advancements in LLMs, their performance is mainly centered around English, leaving low-resource languages underrepresented. The research investigates leveraging code-switching to improve task-solving capabilities in these languages.", "method": "We introduce EnKoQA, a dataset designed for English-Korean code-switching in question-answering tasks. The analysis of multilingual LLMs is conducted by separating the activation process into knowledge identification and knowledge leveraging.", "result": "The study reveals that code-switching significantly activates relevant language-specific knowledge in LLMs when applied to low-resource language tasks, outpacing performance with solely English text.", "conclusion": "Code-switching is a valuable strategy for enhancing LLMs' effectiveness in understanding and working with low-resource languages.", "key_contributions": ["Introduction of the EnKoQA dataset for English-Korean code-switching", "Analytical framework for knowledge activation in multilingual LLMs", "Demonstration of enhanced performance on low-resource language tasks using code-switching"], "limitations": "The focus is on English and Korean, potentially limiting applicability to other language pairs and does not address practical deployment in real-world applications.", "keywords": ["code-switching", "multilingual LLMs", "low-resource languages", "question-answering", "knowledge activation"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2411.07404", "pdf": "https://arxiv.org/pdf/2411.07404.pdf", "abs": "https://arxiv.org/abs/2411.07404", "title": "Controllable Context Sensitivity and the Knob Behind It", "authors": ["Julian Minder", "Kevin Du", "Niklas Stoehr", "Giovanni Monea", "Chris Wendler", "Robert West", "Ryan Cotterell"], "categories": ["cs.CL", "cs.AI"], "comment": "Published as a conference paper at ICLR 2025", "summary": "When making predictions, a language model must trade off how much it relies\non its context vs. its prior knowledge. Choosing how sensitive the model is to\nits context is a fundamental functionality, as it enables the model to excel at\ntasks like retrieval-augmented generation and question-answering. In this\npaper, we search for a knob which controls this sensitivity, determining\nwhether language models answer from the context or their prior knowledge. To\nguide this search, we design a task for controllable context sensitivity. In\nthis task, we first feed the model a context (Paris is in England) and a\nquestion (Where is Paris?); we then instruct the model to either use its prior\nor contextual knowledge and evaluate whether it generates the correct answer\nfor both intents (either France or England). When fine-tuned on this task,\ninstruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it\nwith high accuracy (85-95%). Analyzing these high-performing models, we narrow\ndown which layers may be important to context sensitivity using a novel linear\ntime algorithm. Then, in each model, we identify a 1-D subspace in a single\nlayer that encodes whether the model follows context or prior knowledge.\nInterestingly, while we identify this subspace in a fine-tuned model, we find\nthat the exact same subspace serves as an effective knob in not only that model\nbut also non-fine-tuned instruct and base models of that model family. Finally,\nwe show a strong correlation between a model's performance and how distinctly\nit separates context-agreeing from context-ignoring answers in this subspace.\nThese results suggest a single subspace facilitates how the model chooses\nbetween context and prior knowledge, hinting at a simple fundamental mechanism\nthat controls this behavior.", "AI": {"tldr": "The paper investigates how language models balance reliance on context versus prior knowledge, introducing a controllable context sensitivity task and identifying a crucial subspace in model layers that influences this behavior.", "motivation": "To improve language model performance in tasks requiring contextual or prior knowledge by manipulating sensitivity to context.", "method": "The authors designed a task where models must choose to answer questions based on given context or prior knowledge, assessing performance through fine-tuning and analysis of model layers.", "result": "Instruction-tuned Llama-3.1, Mistral-v0.3, and Gemma-2 achieved 85-95% accuracy on the task, revealing a consistent subspace that determines context versus prior knowledge reliance.", "conclusion": "The findings suggest a fundamental mechanism through which language models can control their reliance on context versus prior knowledge, applicable across model variants.", "key_contributions": ["Introduction of a controllable context sensitivity task for language models", "Identification of a significant subspace in model layers that controls context sensitivity", "Demonstrated performance correlation with context-agreeing versus context-ignoring responses."], "limitations": "", "keywords": ["language models", "context sensitivity", "prior knowledge", "instruct tuning", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.17170", "pdf": "https://arxiv.org/pdf/2411.17170.pdf", "abs": "https://arxiv.org/abs/2411.17170", "title": "Overcoming Non-monotonicity in Transducer-based Streaming Generation", "authors": ["Zhengrui Ma", "Yang Feng", "Min Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML25; Codes: https://github.com/ictnlp/MonoAttn-Transducer", "summary": "Streaming generation models are utilized across fields, with the Transducer\narchitecture being popular in industrial applications. However, its\ninput-synchronous decoding mechanism presents challenges in tasks requiring\nnon-monotonic alignments, such as simultaneous translation. In this research,\nwe address this issue by integrating Transducer's decoding with the history of\ninput stream via a learnable monotonic attention. Our approach leverages the\nforward-backward algorithm to infer the posterior probability of alignments\nbetween the predictor states and input timestamps, which is then used to\nestimate the monotonic context representations, thereby avoiding the need to\nenumerate the exponentially large alignment space during training. Extensive\nexperiments show that our MonoAttn-Transducer effectively handles non-monotonic\nalignments in streaming scenarios, offering a robust solution for complex\ngeneration tasks.", "AI": {"tldr": "The MonoAttn-Transducer model addresses challenges in input-synchronous decoding by integrating monotonic attention, enabling better handling of non-monotonic alignments in streaming generation tasks.", "motivation": "To improve Transducer architecture performance in tasks requiring non-monotonic alignments, which are problematic in simultaneous translation and other streaming applications.", "method": "Integration of a learnable monotonic attention mechanism with the Transducer's decoding process, utilizing the forward-backward algorithm to infer alignments without enumerating the alignment space.", "result": "The MonoAttn-Transducer demonstrates effective handling of non-monotonic alignments in extensive experiments, outperforming traditional methods in streaming scenarios.", "conclusion": "This model offers a robust solution for complex generation tasks that require non-monotonic alignment handling.", "key_contributions": ["Introduction of learnable monotonic attention to Transducer architecture", "Utilization of the forward-backward algorithm for efficient alignment inference", "Demonstrated effectiveness in non-monotonic alignment scenarios during experiments."], "limitations": "", "keywords": ["Transducer", "monotonic attention", "streaming generation", "non-monotonic alignments", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2412.11418", "pdf": "https://arxiv.org/pdf/2412.11418.pdf", "abs": "https://arxiv.org/abs/2412.11418", "title": "ConKE: Conceptualization-Augmented Knowledge Editing in Large Language Models for Commonsense Reasoning", "authors": ["Liyu Zhang", "Weiqi Wang", "Tianqing Fang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Findings of ACL2025", "summary": "Knowledge Editing (KE) aims to adjust a Large Language Model's (LLM) internal\nrepresentations and parameters to correct inaccuracies and improve output\nconsistency without incurring the computational expense of re-training the\nentire model. However, editing commonsense knowledge still faces difficulties,\nincluding limited knowledge coverage in existing resources, the infeasibility\nof annotating labels for an overabundance of commonsense knowledge, and the\nstrict knowledge formats of current editing methods. In this paper, we address\nthese challenges by presenting ConceptEdit, a framework that integrates\nconceptualization and instantiation into the KE pipeline for LLMs to enhance\ntheir commonsense reasoning capabilities. ConceptEdit dynamically diagnoses\nimplausible commonsense knowledge within an LLM using another verifier LLM and\naugments the source knowledge to be edited with conceptualization for stronger\ngeneralizability. Experimental results demonstrate that LLMs enhanced with\nConceptEdit successfully generate commonsense knowledge with improved\nplausibility compared to other baselines and achieve stronger performance\nacross multiple question answering benchmarks. Our data, code, and models are\npublicly available at https://github.com/HKUST-KnowComp/ConKE.", "AI": {"tldr": "ConceptEdit is a framework designed to improve the commonsense reasoning capabilities of Large Language Models (LLMs) by integrating conceptualization and instantiation in the Knowledge Editing (KE) pipeline.", "motivation": "The need to enhance LLMs' commonsense knowledge without the costly process of retraining the entire model.", "method": "ConceptEdit identifies implausible commonsense knowledge through a verifier LLM and improves it by adding conceptualization for better generalizability.", "result": "LLMs augmented with ConceptEdit generated more plausible commonsense knowledge and performed better across various question answering benchmarks compared to existing methods.", "conclusion": "ConceptEdit addresses key challenges in commonsense knowledge editing, achieving significant improvements in knowledge plausibility and overall performance.", "key_contributions": ["Introduction of the ConceptEdit framework for commonsense knowledge editing", "Dynamic diagnosis of commonsense knowledge inaccuracies using a verifier LLM", "Enhanced LLM performance across multiple question answering tasks through improved knowledge augmentation."], "limitations": "", "keywords": ["Knowledge Editing", "Commonsense Reasoning", "Large Language Models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.12465", "pdf": "https://arxiv.org/pdf/2412.12465.pdf", "abs": "https://arxiv.org/abs/2412.12465", "title": "Core Context Aware Transformers for Long Context Language Modeling", "authors": ["Yaofo Chen", "Zeng You", "Shuhai Zhang", "Haokun Li", "Yirui Li", "Yaowei Wang", "Mingkui Tan"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted for publication at ICML 2025", "summary": "Transformer-based Large Language Models (LLMs) have exhibited remarkable\nsuccess in extensive tasks primarily attributed to self-attention mechanism,\nwhich requires a token to consider all preceding tokens as its context to\ncompute attention. However, when the context length L becomes very large (e.g.,\n128K), the amount of potentially redundant information in the context tends to\nincrease. The redundant context not only hampers the modeling representation\nperformance but also incurs unnecessary computational and storage overhead. In\nthis paper, we propose a plug-and-play Core Context Aware (CCA) Attention for\nefficient long-context modeling, comprising two complementary modules: 1)\nGlobality-aware pooling module groups input tokens and dynamically compresses\neach group into one core token based on their significance. In this way, our\nmethod automatically focuses and strengthens core context while diminishing\nredundancy during the learning process, leading to effective long-term\ndependency modeling. 2) Locality-preserving module incorporates neighboring\ntokens to preserve local context for detailed representation. Notably, our\nCCA-Attention is able to replace the self-attention module in existing LLMs\nwith minimal fine-tuning cost. Extensive experimental results show the\nsuperiority of our method in both long-context modeling and computational\nefficiency over state-of-the-art methods.", "AI": {"tldr": "This paper proposes the Core Context Aware (CCA) Attention mechanism for more efficient long-context modeling in LLMs, mitigating redundancy and enhancing representation performance.", "motivation": "To address the inefficiencies and redundancy in processing long contexts in Transformer-based Large Language Models (LLMs) as context length increases.", "method": "The proposed CCA Attention consists of two modules: a Globality-aware pooling module that compresses input tokens into core tokens, and a Locality-preserving module that incorporates neighboring tokens to maintain local context.", "result": "The proposed method improves long-context modeling performance and computational efficiency compared to existing state-of-the-art approaches.", "conclusion": "CCA-Attention can effectively replace self-attention in LLMs with minimal fine-tuning, exhibiting enhanced efficiency and effectiveness for long contexts.", "key_contributions": ["Introduction of Core Context Aware (CCA) Attention for long-context modeling", "Globality-aware pooling module for reducing redundant information", "Locality-preserving module for maintaining detailed local context"], "limitations": "", "keywords": ["Large Language Models", "Long-context modeling", "Attention mechanisms"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.16926", "pdf": "https://arxiv.org/pdf/2412.16926.pdf", "abs": "https://arxiv.org/abs/2412.16926", "title": "Revisiting In-Context Learning with Long Context Language Models", "authors": ["Jinheon Baek", "Sun Jae Lee", "Prakhar Gupta", "Geunseob Oh", "Siddharth Dalmia", "Prateek Kolhar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Findings 2025", "summary": "In-Context Learning (ICL) is a technique by which language models make\npredictions based on examples provided in their input context. Previously,\ntheir context window size imposed a limit on the number of examples that can be\nshown, making example selection techniques crucial for identifying the\nmaximally effective set of examples. However, the recent advent of Long Context\nLanguage Models (LCLMs) has significantly increased the number of examples that\ncan be included in context, raising an important question of whether ICL\nperformance in a many-shot regime is still sensitive to the method of sample\nselection. To answer this, we revisit these approaches in the context of LCLMs\nthrough extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we\nobserve that sophisticated example selection techniques do not yield\nsignificant improvements over a simple random sample selection method. Instead,\nwe discover that the advent of LCLMs has fundamentally shifted the challenge of\nICL from that of selecting the most effective examples to that of collecting\nsufficient examples to fill the context window. Specifically, in certain\ndatasets, including all available examples does not fully utilize the context\nwindow; however, by augmenting the examples in context with a simple data\naugmentation approach, we substantially improve ICL performance by 5%.", "AI": {"tldr": "This paper examines the impact of Long Context Language Models (LCLMs) on In-Context Learning (ICL), revealing that sample selection techniques may be less critical than previously thought, while emphasizing the importance of example quantity and augmentation.", "motivation": "With the rise of Long Context Language Models, we aim to explore how this affects In-Context Learning performance, particularly regarding example selection methods.", "method": "We conducted extensive experiments on 18 datasets across 4 tasks to evaluate the effect of example selection techniques in conjunction with LCLMs.", "result": "The study found that sophisticated example selection does not significantly outperform random selection, and that simply filling the context window with examples, augmented as necessary, can lead to notable performance gains in ICL.", "conclusion": "LCLMs shift the challenge from selecting the best examples to ensuring enough examples are present, and data augmentation can effectively enhance performance.", "key_contributions": ["Reevaluation of example selection techniques in LCLM context", "Introduction of data augmentation to improve ICL", "Empirical results across multiple datasets and tasks"], "limitations": "", "keywords": ["In-Context Learning", "Long Context Language Models", "example selection", "data augmentation", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.00777", "pdf": "https://arxiv.org/pdf/2501.00777.pdf", "abs": "https://arxiv.org/abs/2501.00777", "title": "FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation", "authors": ["Qianli Wang", "Nils Feldhus", "Simon Ostermann", "Luis Felipe Villa-Arenas", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL", "cs.LG"], "comment": "ACL 2025 Findings; camera-ready version", "summary": "Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals, which we hope will serve as an\nimportant finding for future research in this direction.", "AI": {"tldr": "This paper introduces ZeroCF for generating counterfactual examples in NLP and FitCF for verifying them, improving model performance in few-shot prompting.", "motivation": "To enhance the generation of counterfactual examples in NLP for improving models and understanding model behavior via explainable AI.", "method": "The authors propose ZeroCF, a zero-shot approach using important words from feature attribution methods, and FitCF, which verifies counterfactual examples through label flip verification and incorporates them into few-shot prompting.", "result": "FitCF outperforms two state-of-the-art baselines, showing that the number of demonstrations significantly affects performance.", "conclusion": "There is a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, providing insights for future research.", "key_contributions": ["Introduction of ZeroCF for generating counterfactuals", "Development of FitCF for verifying counterfactuals", "Demonstration of the impact of demonstration quantity on performance"], "limitations": "", "keywords": ["Counterfactual examples", "Feature attribution", "Natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.03124", "pdf": "https://arxiv.org/pdf/2501.03124.pdf", "abs": "https://arxiv.org/abs/2501.03124", "title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models", "authors": ["Mingyang Song", "Zhaochen Su", "Xiaoye Qu", "Jiawei Zhou", "Yu Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ACL 2025 Main. Project Page: https://prmbench.github.io/", "summary": "Process-level Reward Models (PRMs) are crucial for complex reasoning and\ndecision-making tasks, where each intermediate step plays an important role in\nthe reasoning process. Since language models are prone to various types of\nerrors during the reasoning process, PRMs are required to possess nuanced\ncapabilities for detecting various implicit error types in real-world\nscenarios. However, current benchmarks primarily focus on step correctness,\nfailing to evaluate PRMs' performance systematically. To address this gap, we\nintroduce PRMBench, a process-level benchmark specifically designed to assess\nthe fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216\ncarefully designed problems and 83,456 step-level labels, evaluating models\nacross multiple dimensions, including simplicity, soundness, and sensitivity.\nIn our experiments on 15 models, spanning both open-source PRMs and\nclosed-source large language models prompted as critic models, we uncover\nsignificant weaknesses in current PRMs. These findings underscore the\nchallenges inherent in process-level evaluation and highlight key directions\nfor future research. We hope PRMBench can be a robust bench for advancing\nresearch on PRM evaluation and development.", "AI": {"tldr": "The paper introduces PRMBench, a benchmark for evaluating Process-level Reward Models (PRMs) in detecting errors during reasoning and decision-making tasks.", "motivation": "Current benchmarks inadequately assess the nuanced error detection capabilities of PRMs, focusing primarily on step correctness.", "method": "Introduced PRMBench, containing 6,216 designed problems and 83,456 labels to systematically evaluate multiple dimensions of model performance.", "result": "Experiments on 15 different models revealed significant weaknesses in PRMs' error detection capabilities.", "conclusion": "PRMBench aims to provide a robust framework for evaluating and advancing research on PRM development.", "key_contributions": ["Introduction of PRMBench for fine-grained error detection evaluation in PRMs.", "Extensive dataset with 6,216 problems and 83,456 labels for multi-dimensional evaluation.", "Empirical results highlighting weaknesses in existing models."], "limitations": "Primarily focused on error detection; other aspects of PRM evaluation may need further exploration.", "keywords": ["Process-level Reward Models", "Benchmark", "Error Detection", "Natural Language Processing", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.05926", "pdf": "https://arxiv.org/pdf/2501.05926.pdf", "abs": "https://arxiv.org/abs/2501.05926", "title": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities", "authors": ["Ruby Ostrow", "Adam Lopez"], "categories": ["cs.CL"], "comment": "8 pages, 5 figures, 5 tables", "summary": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used social psychology model -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom both humans and LLMs. We then extend this framework to a more realistic\nuse case: text generation. Our analysis shows that LLMs generate stereotyped\nrepresentations of sexual and gender minorities in this setting, showing that\nthey amplify representational harms in creative writing, a widely advertised\nuse for LLMs.", "AI": {"tldr": "The paper investigates gender and sexual bias in large language models (LLMs), highlighting the limitations of binary gender representation.", "motivation": "To explore gender and sexual biases in LLMs beyond the binary view, acknowledging the spectrum of identities.", "method": "The study uses the Stereotype Content Model to analyze survey responses and text generation output from LLMs.", "result": "Findings reveal that LLMs perpetuate negative stereotypes of sexual and gender minorities and amplify representational harms in text generation.", "conclusion": "LLMs demonstrate significant biases that need addressing, particularly in creative writing applications.", "key_contributions": ["Analyzes LLM biases beyond the binary gender framework", "Uses the Stereotype Content Model for bias evaluation", "Highlights the impact of LLMs in creative writing and representation"], "limitations": "Focuses primarily on English-language models and contexts.", "keywords": ["gender bias", "large language models", "sexual minorities", "stereotype content model", "text generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.06365", "pdf": "https://arxiv.org/pdf/2501.06365.pdf", "abs": "https://arxiv.org/abs/2501.06365", "title": "Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts", "authors": ["Elizabeth Schaefer", "Kirk Roberts"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "9 pages, 4 figures", "summary": "This paper presents a pipeline for mitigating gender bias in large language\nmodels (LLMs) used in medical literature by neutralizing gendered occupational\npronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to\nidentify and modify pronouns tied to professions. We developed a BERT-based\nmodel, \"Modern Occupational Bias Elimination with Refined Training,\" or\n\"MOBERT,\" trained on these neutralized abstracts, and compared its performance\nwith \"1965BERT,\" trained on the original dataset. MOBERT achieved a 70%\ninclusive replacement rate, while 1965BERT reached only 4%. A further analysis\nof MOBERT revealed that pronoun replacement accuracy correlated with the\nfrequency of occupational terms in the training data. We propose expanding the\ndataset and refining the pipeline to improve performance and ensure more\nequitable language modeling in medical applications.", "AI": {"tldr": "A pipeline to mitigate gender bias in LLMs for medical literature through pronoun neutralization is presented, using a new model trained on modified abstracts.", "motivation": "To address gender bias in medical literature and improve the inclusivity of language used in LLMs.", "method": "A dataset of 379,000 PubMed abstracts was processed to neutralize gendered occupational pronouns; a BERT-based model, MOBERT, was trained and compared to 1965BERT.", "result": "MOBERT achieved a 70% inclusive replacement rate for gendered pronouns, significantly outperforming 1965BERT's 4%.", "conclusion": "Expanding the dataset and refining the method is proposed to further enhance equitable language modeling in medical contexts.", "key_contributions": ["Introduction of MOBERT for gender bias mitigation in LLMs.", "Significant performance improvement over previous models in pronoun replacement.", "Analysis revealing correlation between replacement accuracy and occupational term frequency."], "limitations": "", "keywords": ["gender bias", "large language models", "medical literature", "pronoun neutralization", "BERT"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2501.13567", "pdf": "https://arxiv.org/pdf/2501.13567.pdf", "abs": "https://arxiv.org/abs/2501.13567", "title": "K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor", "authors": ["Jeonghun Cho", "Gary Geunbae Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at NAACL 2025 (Main, long paper)", "summary": "Retrieval-augmented question answering (QA) integrates external information\nand thereby increases the QA accuracy of reader models that lack domain\nknowledge. However, documents retrieved for closed domains require high\nexpertise, so the reader model may have difficulty fully comprehending the\ntext. Moreover, the retrieved documents contain thousands of tokens, some\nunrelated to the question. As a result, the documents include some inaccurate\ninformation, which could lead the reader model to mistrust the passages and\ncould result in hallucinations. To solve these problems, we propose K-comp\n(Knowledge-injected compressor) which provides the knowledge required to answer\ncorrectly. The compressor automatically generates the prior knowledge necessary\nto facilitate the answer process prior to compression of the retrieved\npassages. Subsequently, the passages are compressed autoregressively, with the\ngenerated knowledge being integrated into the compression process. This process\nensures alignment between the question intent and the compressed context. By\naugmenting this prior knowledge and concise context, the reader models are\nguided toward relevant answers and trust the context.", "AI": {"tldr": "This paper introduces K-comp, a method for enhancing retrieval-augmented question answering by automatically generating prior knowledge to improve comprehension and reduce hallucinations in reader models.", "motivation": "Retrieval-augmented QA models struggle with high expertise documents and irrelevant information, leading to inaccuracies and mistrust in the input passages.", "method": "K-comp generates the necessary prior knowledge before compressing retrieved passages, integrating it into the context to align with question intent.", "result": "K-comp helps reader models trust the context and improve their accuracy in answering questions by providing relevant prior knowledge and a concise context.", "conclusion": "The proposed method effectively enhances the performance of reader models in retrieval-augmented question answering scenarios by ensuring alignment and trust.", "key_contributions": ["Introduction of the K-comp method for knowledge injection in QA", "Autoregressive compression of passages with integrated prior knowledge", "Improved trust and accuracy in reader models"], "limitations": "", "keywords": ["retrieval-augmented QA", "K-comp", "knowledge injection", "compression", "reader models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.13953", "pdf": "https://arxiv.org/pdf/2501.13953.pdf", "abs": "https://arxiv.org/abs/2501.13953", "title": "Redundancy Principles for MLLMs Benchmarks", "authors": ["Zicheng Zhang", "Xiangyu Zhao", "Xinyu Fang", "Chunyi Li", "Xiaohong Liu", "Xiongkuo Min", "Haodong Duan", "Kai Chen", "Guangtao Zhai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid iteration of Multi-modality Large Language Models (MLLMs) and\nthe evolving demands of the field, the number of benchmarks produced annually\nhas surged into the hundreds. The rapid growth has inevitably led to\nsignificant redundancy among benchmarks. Therefore, it is crucial to take a\nstep back and critically assess the current state of redundancy and propose\ntargeted principles for constructing effective MLLM benchmarks. In this paper,\nwe focus on redundancy from three key perspectives: 1) Redundancy of benchmark\ncapability dimensions, 2) Redundancy in the number of test questions, and 3)\nCross-benchmark redundancy within specific domains. Through the comprehensive\nanalysis over hundreds of MLLMs' performance across more than 20 benchmarks, we\naim to quantitatively measure the level of redundancy lies in existing MLLM\nevaluations, provide valuable insights to guide the future development of MLLM\nbenchmarks, and offer strategies to refine and address redundancy issues\neffectively. The code is available at\nhttps://github.com/zzc-1998/Benchmark-Redundancy.", "AI": {"tldr": "This paper analyzes redundancy in Multi-modality Large Language Model (MLLM) benchmarks and proposes principles for creating effective evaluations.", "motivation": "To address significant redundancy in MLLM benchmarks caused by rapid production of evaluations.", "method": "A comprehensive analysis of MLLM performance on over 20 benchmarks to measure redundancy from multiple perspectives.", "result": "Quantitative assessment showing the level of redundancy across benchmarks, providing insights for future benchmark development.", "conclusion": "Effective strategies are proposed to refine MLLM benchmarks and reduce redundancy issues.", "key_contributions": ["Assessment of benchmark capability dimensions for redundancy", "Quantitative analysis of test question redundancy", "Insights for improving cross-benchmark evaluations"], "limitations": "", "keywords": ["Multi-modality Large Language Models", "benchmark redundancy", "MLLM evaluations"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2501.14431", "pdf": "https://arxiv.org/pdf/2501.14431.pdf", "abs": "https://arxiv.org/abs/2501.14431", "title": "Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains", "authors": ["Xu Chu", "Zhijie Tan", "Hanlin Xue", "Guanyu Wang", "Tong Mo", "Weiping Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are widely applied to downstream domains.\nHowever, current LLMs for high-stakes domain tasks, such as financial\ninvestment and legal QA, typically generate brief answers without reasoning\nprocesses and explanations. This limits users' confidence in making decisions\nbased on their responses. While original CoT shows promise, it lacks\nself-correction mechanisms during reasoning. This work introduces Domain$o1$s,\nwhich enhances LLMs' reasoning capabilities on domain tasks through supervised\nfine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k\ndatasets for fine-tuning models that activate domain-specific reasoning steps\nbased on their judgment. Additionally, we propose Selective Tree Exploration to\nspontaneously explore solution spaces and sample optimal reasoning paths to\nimprove performance. We also introduce PROOF-Score, a new metric for evaluating\ndomain models' explainability, complementing traditional accuracy metrics with\nricher assessment dimensions. Extensive experiments on stock investment\nrecommendation and legal reasoning QA tasks demonstrate Domaino1s's leading\nperformance and explainability. Our code is available at\nhttps://github.com/Hyalinesky/Domaino1s.", "AI": {"tldr": "This paper presents Domain$o1$s, enhancing LLMs' reasoning capabilities for high-stakes domain tasks through supervised fine-tuning, tree search, and a new evaluation metric for explainability.", "motivation": "Current LLMs generate brief answers without reasoning, limiting users' confidence in high-stakes domains.", "method": "Domain$o1$s is developed using CoT-stock-2k and CoT-legal-2k datasets through supervised fine-tuning and employs Selective Tree Exploration for optimal reasoning paths.", "result": "Experimental results on stock investment and legal QA tasks demonstrate Domain$o1$s's superior performance and enhanced explainability compared to existing methods.", "conclusion": "Domain$o1$s significantly improves LLM reasoning for high-stakes tasks, offering a more robust framework for explanation and decision making.", "key_contributions": ["Introduction of Domain$o1$s for enhanced LLM reasoning", "Development of CoT-stock-2k and CoT-legal-2k datasets", "Proposal of PROOF-Score for evaluating explainability"], "limitations": "", "keywords": ["Large Language Models", "reasoning", "high-stakes domains", "explainability", "supervised fine-tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.00602", "pdf": "https://arxiv.org/pdf/2502.00602.pdf", "abs": "https://arxiv.org/abs/2502.00602", "title": "Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing", "authors": ["Tianci Liu", "Ruirui Li", "Zihan Dong", "Hui Liu", "Xianfeng Tang", "Qingyu Yin", "Linjun Zhang", "Haoyu Wang", "Jing Gao"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Large language models (LLMs) have achieved remarkable performance on various\nnatural language tasks. However, they are trained on static corpora and their\nknowledge can become outdated quickly in the fast-changing world. This\nmotivates the development of knowledge editing (KE) to update specific\nknowledge in LLMs without changing unrelated others or compromising their\npre-trained capabilities. Previous efforts sought to update a small amount of\nparameters of a LLM and proved effective for making selective updates.\nNonetheless, the edited LLM often exhibits degraded ability to reason about the\nnew knowledge. In this work, we identify a key issue: heterogeneous token\noverfitting (HTO), where the LLM overfits different tokens in the provided\nknowledge at varying rates. To tackle this, we propose OVERTONE, a token-level\nsmoothing method that mitigates HTO by adaptively refining the target\ndistribution. Theoretically, OVERTONE offers better parameter updates with\nnegligible computation overhead. It also induces an implicit DPO but does not\nrequire preference data pairs. Extensive experiments across four editing\nmethods, two LLMs, and diverse scenarios demonstrate the effectiveness and\nversatility of our method.", "AI": {"tldr": "OVERTONE is a novel method for knowledge editing in LLMs that addresses heterogeneous token overfitting, improving reasoning about newly added knowledge with low computational overhead.", "motivation": "LLMs' knowledge can quickly become outdated due to their training on static corpora, necessitating effective knowledge editing methods that preserve pre-trained capabilities.", "method": "We propose OVERTONE, a token-level smoothing method that mitigates heterogeneous token overfitting by adaptively refining the target distribution during knowledge updates.", "result": "Experiments show that OVERTONE effectively improves parameter updates and maintains reasoning abilities across multiple editing methods and LLM architectures.", "conclusion": "OVERTONE enhances the knowledge editing process in LLMs by addressing the issue of token overfitting, making it a versatile and effective solution for keeping LLMs updated.", "key_contributions": ["Introduction of OVERTONE for mitigating heterogeneous token overfitting in LLMs", "Demonstrated low computational overhead for knowledge updates", "Validation across multiple LLMs and editing scenarios"], "limitations": "The performance may vary depending on the specific LLM architecture and the nature of the knowledge being edited.", "keywords": ["knowledge editing", "large language models", "token-level smoothing", "heterogeneous token overfitting", "parameter updates"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.03671", "pdf": "https://arxiv.org/pdf/2502.03671.pdf", "abs": "https://arxiv.org/abs/2502.03671", "title": "Advancing Reasoning in Large Language Models: Promising Methods and Approaches", "authors": ["Avinash Patil", "Aryan Jadon"], "categories": ["cs.CL", "cs.AI"], "comment": "9 Pages, 1 Figure, IEEE Format", "summary": "Large Language Models (LLMs) have succeeded remarkably in various natural\nlanguage processing (NLP) tasks, yet their reasoning capabilities remain a\nfundamental challenge. While LLMs exhibit impressive fluency and factual\nrecall, their ability to perform complex reasoning-spanning logical deduction,\nmathematical problem-solving, commonsense inference, and multi-step\nreasoning-often falls short of human expectations. This survey provides a\ncomprehensive review of emerging techniques enhancing reasoning in LLMs. We\ncategorize existing methods into key approaches, including prompting strategies\n(e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought\nreasoning), architectural innovations (e.g., retrieval-augmented models,\nmodular reasoning networks, and neuro-symbolic integration), and learning\nparadigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement\nlearning, and self-supervised reasoning objectives). Additionally, we explore\nevaluation frameworks used to assess reasoning in LLMs and highlight open\nchallenges, such as hallucinations, robustness, and reasoning generalization\nacross diverse tasks. By synthesizing recent advancements, this survey aims to\nprovide insights into promising directions for future research and practical\napplications of reasoning-augmented LLMs.", "AI": {"tldr": "This survey reviews techniques to enhance reasoning capabilities in Large Language Models (LLMs), categorizing methods into prompting strategies, architectural innovations, and learning paradigms, while exploring evaluation frameworks and future research directions.", "motivation": "To address the limitations of LLMs in performing complex reasoning tasks, which often falls below human standards despite their fluency and factual accuracy.", "method": "The paper categorizes methods into prompting strategies (Chain-of-Thought reasoning, Self-Consistency, Tree-of-Thought reasoning), architectural innovations (retrieval-augmented models, modular reasoning networks, neuro-symbolic integration), and learning paradigms (fine-tuning with reasoning datasets, reinforcement learning, self-supervised reasoning).", "result": "The survey synthesizes recent advancements and evaluates current methodologies enhancing LLM reasoning, identifying significant improvements in reasoning tasks through various approaches.", "conclusion": "The paper concludes by highlighting open challenges in LLM reasoning, such as hallucinations and the need for robust reasoning generalization, while suggesting promising directions for future research.", "key_contributions": ["Comprehensive categorization of reasoning-enhancing techniques for LLMs.", "Evaluation of current methodologies and their impact on LLM reasoning capabilities.", "Identification of open challenges and future research directions for reasoning-augmented LLMs."], "limitations": "The paper discusses limitations in current approaches, including generalization issues and the prevalence of hallucinations in LLMs.", "keywords": ["Large Language Models", "reasoning", "natural language processing", "evaluation frameworks", "neuro-symbolic integration"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.05242", "pdf": "https://arxiv.org/pdf/2502.05242.pdf", "abs": "https://arxiv.org/abs/2502.05242", "title": "Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring", "authors": ["Guanxu Chen", "Dongrui Liu", "Tao Luo", "Lijie Hu", "Jing Shao"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "25 pages,6 figures,13 tables", "summary": "Large language models (LLMs) are becoming increasingly capable, but the\nmechanisms of their thinking and decision-making process remain unclear.\nChain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this\nstrategy fails to accurately reflect LLMs' thinking process. Techniques based\non LLMs' hidden representations provide an inner perspective to monitor their\nlatent thinking. However, previous methods only try to develop external\nmonitors instead of making LLMs themselves easier to monitor. In this paper, we\npropose a novel method TELLME, improving the transparency of LLMs and helping\nmonitors identify unsuitable and sensitive behaviors. Furthermore, we showcase\nthe applications of TELLME on trustworthiness tasks (\\eg, safety risks\nmonitoring tasks and detoxification tasks), where LLMs achieve consistent\nimprovement in transparency and task performance. More crucially, we\ntheoretically analyze the improvement of TELLME on LLMs' generalization ability\nthrough optimal transport theory.", "AI": {"tldr": "This paper presents TELLME, a method enhancing the transparency of large language models (LLMs) to help monitor their decision-making processes, focusing on trustworthiness tasks.", "motivation": "To improve the understanding of LLMs' decision-making processes and enhance their monitoring for safety and sensitivity.", "method": "The proposed TELLME method enhances the transparency of LLMs by providing a mechanism for monitoring their latent thinking processes, using techniques grounded in optimal transport theory.", "result": "TELLME demonstrates consistent improvement in both transparency and task performance on trustworthiness tasks such as safety risks monitoring and detoxification tasks.", "conclusion": "The implementation of TELLME not only improves LLMs' transparency but also their generalization abilities, making them more suitable for sensitive applications.", "key_contributions": ["Introduction of TELLME for LLM transparency", "Application of TELLME in trustworthiness tasks", "Theoretical analysis using optimal transport theory"], "limitations": "", "keywords": ["Large Language Models", "Transparency", "Monitoring", "Trustworthiness", "Optimal Transport"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2502.07365", "pdf": "https://arxiv.org/pdf/2502.07365.pdf", "abs": "https://arxiv.org/abs/2502.07365", "title": "LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation", "authors": ["Zican Dong", "Junyi Li", "Jinhao Jiang", "Mingyu Xu", "Wayne Xin Zhao", "Bingning Wang", "Weipeng Chen"], "categories": ["cs.CL", "cs.LG"], "comment": "ACL2025 Main", "summary": "Large language models (LLMs) have gained extended context windows through\nscaling positional encodings and lightweight continual pre-training. However,\nthis often leads to degraded performance on short-text tasks, while the reasons\nfor this degradation remain insufficiently explored. In this work, we identify\ntwo primary factors contributing to this issue: distribution drift in hidden\nstates and attention scores, and catastrophic forgetting during continual\npre-training. To address these challenges, we propose Long Context Pre-training\nwith Restoration Distillation (LongReD), a novel approach designed to mitigate\nshort-text performance degradation through minimizing the distribution\ndiscrepancy between the extended and original models. Besides training on long\ntexts, LongReD distills the hidden state of selected layers from the original\nmodel on short texts. Additionally, LongReD also introduces a short-to-long\ndistillation, aligning the output distribution on short texts with that on long\ntexts by leveraging skipped positional indices. Experiments on common text\nbenchmarks demonstrate that LongReD effectively preserves the model's\nshort-text performance while maintaining comparable or even better capacity to\nhandle long texts than baselines. Our code is available at\nhttps://github.com/RUCAIBox/LongReD.", "AI": {"tldr": "This paper proposes Long Context Pre-training with Restoration Distillation (LongReD) to enhance short-text performance of large language models (LLMs) while retaining their long-text capabilities.", "motivation": "Large language models (LLMs) experience performance degradation on short-text tasks when scaled for extended context windows, with underlying causes not fully explored.", "method": "The proposed LongReD approach minimizes distribution discrepancies between extended and original models while distilling the hidden states from long texts and aligning outputs using skipped positional indices.", "result": "LongReD demonstrates effective preservation of short-text performance alongside competitive handling of long texts compared to baseline models.", "conclusion": "The proposed method successfully mitigates performance issues associated with short texts, evidenced by experiments on common benchmarks.", "key_contributions": ["Introduction of LongReD framework for LLM short-text performance restoration", "Use of distribution minimization techniques in continual pre-training", "Implementation of short-to-long distillation leveraging positional indices"], "limitations": "", "keywords": ["Large Language Models", "Continual Pre-training", "Restoration Distillation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.09082", "pdf": "https://arxiv.org/pdf/2502.09082.pdf", "abs": "https://arxiv.org/abs/2502.09082", "title": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles", "authors": ["Xintao Wang", "Heng Wang", "Yifei Zhang", "Xinfeng Yuan", "Rui Xu", "Jen-tse Huang", "Siyu Yuan", "Haoran Guo", "Jiangjie Chen", "Shuchang Zhou", "Wei Wang", "Yanghua Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Role-playing language agents (RPLAs) have emerged as promising applications\nof large language models (LLMs). However, simulating established characters\npresents a challenging task for RPLAs, due to the lack of authentic character\ndatasets and nuanced evaluation methods using such data. In this paper, we\npresent CoSER, a collection of a high-quality dataset, open models, and an\nevaluation protocol towards effective RPLAs of established characters. The\nCoSER dataset covers 17,966 characters from 771 renowned books. It provides\nauthentic dialogues with real-world intricacies, as well as diverse data types\nsuch as conversation setups, character experiences and internal thoughts.\nDrawing from acting methodology, we introduce given-circumstance acting for\ntraining and evaluating role-playing LLMs, where LLMs sequentially portray\nmultiple characters in book scenes. Using our dataset, we develop CoSER 8B and\nCoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.\nExtensive experiments demonstrate the value of the CoSER dataset for RPLA\ntraining, evaluation and retrieval. Moreover, CoSER 70B exhibits\nstate-of-the-art performance surpassing or matching GPT-4o on our evaluation\nand three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on\nthe InCharacter and LifeChoice benchmarks respectively.", "AI": {"tldr": "This paper introduces CoSER, a high-quality dataset and evaluation method for role-playing language agents (RPLAs) simulating established characters, demonstrating its implementation in advanced LLMs.", "motivation": "To address the challenges of simulating established characters in role-playing language agents due to a lack of authentic datasets and nuanced evaluation methods.", "method": "Introduction of the CoSER dataset containing 17,966 characters with authentic dialogues and diverse data types, coupled with a novel given-circumstance acting methodology for training and evaluating RPLAs.", "result": "CoSER 70B showcases state-of-the-art performance, exceeding the capabilities of GPT-4o on several benchmarks related to character simulation.", "conclusion": "The CoSER dataset and methodology significantly enhance the training and evaluation processes for role-playing language models, showcasing their effectiveness in realistic character portrayal.", "key_contributions": ["Introduction of the CoSER dataset with a vast collection of character dialogues and experiences.", "Development of CoSER 8B and CoSER 70B LLMs based on LLaMA-3.1.", "Proposing a novel evaluation protocol that improves role-playing language agent performance."], "limitations": "", "keywords": ["Role-playing language agents", "large language models", "dataset", "evaluation protocol", "character simulation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11190", "pdf": "https://arxiv.org/pdf/2502.11190.pdf", "abs": "https://arxiv.org/abs/2502.11190", "title": "ReLearn: Unlearning via Learning for Large Language Models", "authors": ["Haoming Xu", "Ningyuan Zhao", "Liming Yang", "Sendong Zhao", "Shumin Deng", "Mengru Wang", "Bryan Hooi", "Nay Oo", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "ACL 2025", "summary": "Current unlearning methods for large language models usually rely on reverse\noptimization to reduce target token probabilities. However, this paradigm\ndisrupts the subsequent tokens prediction, degrading model performance and\nlinguistic coherence. Moreover, existing evaluation metrics overemphasize\ncontextual forgetting while inadequately assessing response fluency and\nrelevance. To address these challenges, we propose ReLearn, a data augmentation\nand fine-tuning pipeline for effective unlearning, along with a comprehensive\nevaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)\nand Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and\nLinguistic Score (LS) to evaluate generation quality. Our experiments show that\nReLearn successfully achieves targeted forgetting while preserving high-quality\noutput. Through mechanistic analysis, we further demonstrate how reverse\noptimization disrupts coherent text generation, while ReLearn preserves this\nessential capability. Code is available at https://github.com/zjunlp/unlearn.", "AI": {"tldr": "Proposes ReLearn, a novel unlearning framework for large language models that uses data augmentation and fine-tuning to maintain linguistic coherence while achieving targeted forgetting.", "motivation": "Current methods for unlearning in large language models negatively impact performance and coherence due to reliance on reverse optimization, highlighting the need for a better approach.", "method": "Introduces a data augmentation and fine-tuning pipeline called ReLearn, along with an evaluation framework that includes new metrics for measuring knowledge retention and generation quality.", "result": "ReLearn successfully accomplishes targeted forgetting while preserving high generation quality, as demonstrated in experimental results.", "conclusion": "ReLearn provides a more effective means of unlearning in language models without sacrificing linguistic coherence, with new evaluation metrics to better assess these attributes.", "key_contributions": ["Introduction of ReLearn for effective unlearning in language models", "Development of Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) metrics", "Demonstration of the negative impacts of reverse optimization on text coherence."], "limitations": "", "keywords": ["unlearning", "language models", "data augmentation", "fine-tuning", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2502.11926", "pdf": "https://arxiv.org/pdf/2502.11926.pdf", "abs": "https://arxiv.org/abs/2502.11926", "title": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages", "authors": ["Shamsuddeen Hassan Muhammad", "Nedjma Ousidhoum", "Idris Abdulmumin", "Jan Philip Wahle", "Terry Ruas", "Meriem Beloucif", "Christine de Kock", "Nirmal Surange", "Daniela Teodorescu", "Ibrahim Said Ahmad", "David Ifeoluwa Adelani", "Alham Fikri Aji", "Felermino D. M. A. Ali", "Ilseyar Alimova", "Vladimir Araujo", "Nikolay Babakov", "Naomi Baes", "Ana-Maria Bucur", "Andiswa Bukula", "Guanqun Cao", "Rodrigo Tufino Cardenas", "Rendi Chevi", "Chiamaka Ijeoma Chukwuneke", "Alexandra Ciobotaru", "Daryna Dementieva", "Murja Sani Gadanya", "Robert Geislinger", "Bela Gipp", "Oumaima Hourrane", "Oana Ignat", "Falalu Ibrahim Lawan", "Rooweither Mabuya", "Rahmad Mahendra", "Vukosi Marivate", "Alexander Panchenko", "Andrew Piper", "Charles Henrique Porto Ferreira", "Vitaly Protasov", "Samuel Rutunda", "Manish Shrivastava", "Aura Cristina Udrea", "Lilian Diana Awuor Wanzare", "Sophie Wu", "Florian Valentin Wunderlich", "Hanif Muhammad Zhafran", "Tianhui Zhang", "Yi Zhou", "Saif M. Mohammad"], "categories": ["cs.CL"], "comment": "Accepted at ACL2025 (Main)", "summary": "People worldwide use language in subtle and complex ways to express emotions.\nAlthough emotion recognition--an umbrella term for several NLP tasks--impacts\nvarious applications within NLP and beyond, most work in this area has focused\non high-resource languages. This has led to significant disparities in research\nefforts and proposed solutions, particularly for under-resourced languages,\nwhich often lack high-quality annotated datasets. In this paper, we present\nBRIGHTER--a collection of multilabeled, emotion-annotated datasets in 28\ndifferent languages and across several domains. BRIGHTER primarily covers\nlow-resource languages from Africa, Asia, Eastern Europe, and Latin America,\nwith instances labeled by fluent speakers. We highlight the challenges related\nto the data collection and annotation processes, and then report experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as emotion intensity recognition. We analyse the variability in\nperformance across languages and text domains, both with and without the use of\nLLMs, and show that the BRIGHTER datasets represent a meaningful step towards\naddressing the gap in text-based emotion recognition.", "AI": {"tldr": "The paper introduces BRIGHTER, a collection of multilabeled, emotion-annotated datasets in 28 low-resource languages to enhance text-based emotion recognition.", "motivation": "To address the disparities in emotion recognition research for under-resourced languages and improve applications within NLP that rely on emotion understanding.", "method": "The authors created and annotated datasets in 28 languages, focusing on low-resource languages from various global regions, and conducted experiments on emotion identification and intensity recognition using these datasets.", "result": "Experimental results showed performance variability in emotion recognition tasks across languages and text domains, with findings both with and without the use of LLMs.", "conclusion": "BRIGHTER datasets represent a significant advancement in bridging the gap in emotion recognition for low-resource languages and provide valuable resources for future research.", "key_contributions": ["Creation of a multilabeled emotion-annotated dataset in 28 low-resource languages", "Experimental insights into monolingual and crosslingual emotion recognition", "Highlighting challenges in data collection and annotation for under-resourced languages"], "limitations": "The datasets might still face challenges in generalizability and quality across all 28 languages due to varying levels of fluent speaker involvement.", "keywords": ["emotion recognition", "low-resource languages", "multilabel datasets", "NLP", "crosslingual"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.13458", "pdf": "https://arxiv.org/pdf/2502.13458.pdf", "abs": "https://arxiv.org/abs/2502.13458", "title": "ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails", "authors": ["Xiaofei Wen", "Wenxuan Zhou", "Wenjie Jacky Mo", "Muhao Chen"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "ACL 2025", "summary": "Ensuring the safety of large language models (LLMs) is critical as they are\ndeployed in real-world applications. Existing guardrails rely on rule-based\nfiltering or single-pass classification, limiting their ability to handle\nnuanced safety violations. To address this, we propose ThinkGuard, a\ncritique-augmented guardrail model that distills knowledge from high-capacity\nLLMs by generating structured critiques alongside safety labels. Fine-tuned on\ncritique-augmented data, the captured deliberative thinking ability drastically\nenhances the guardrail's cautiousness and interpretability. Evaluated on\nmultiple safety benchmarks, ThinkGuard achieves the highest average F1 and\nAUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard\nimproves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses\nlabel-only fine-tuned models, confirming that structured critiques enhance both\nclassification precision and nuanced safety reasoning while maintaining\ncomputational efficiency.", "AI": {"tldr": "ThinkGuard is a critique-augmented guardrail model for enhancing the safety of large language models (LLMs) by generating structured critiques alongside safety labels.", "motivation": "As LLMs are increasingly deployed in real-world applications, their safety becomes paramount. Current methods often rely on simplistic rule-based filtering or single-pass classification, limiting their effectiveness in addressing complex safety issues.", "method": "The study introduces ThinkGuard, which is fine-tuned on critique-augmented data to generate structured critiques in conjunction with safety labels, significantly enhancing interpretability and caution in LLM safety evaluations.", "result": "ThinkGuard outperforms existing models on multiple safety benchmarks, achieving the highest average F1 and AUPRC. It improves accuracy by 16.1% and macro F1 by 27.0% compared to LLaMA Guard 3, demonstrating that critique augmentation enhances classification precision and nuanced reasoning.", "conclusion": "The findings indicate that incorporating structured critiques within safety models not only improves performance in classifying safety violations but also maintains computational efficiency, meriting further exploration in LLM safety applications.", "key_contributions": ["Introduction of ThinkGuard model for LLM safety enhancement", "Demonstration of critique-augmented data's effectiveness in improving outcomes", "Validation through superior performance metrics on safety benchmarks"], "limitations": "", "keywords": ["large language models", "safety", "guardrail", "critique-augmented", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.13913", "pdf": "https://arxiv.org/pdf/2502.13913.pdf", "abs": "https://arxiv.org/abs/2502.13913", "title": "How Do LLMs Perform Two-Hop Reasoning in Context?", "authors": ["Tianyu Guo", "Hanlin Zhu", "Ruiqi Zhang", "Jiantao Jiao", "Song Mei", "Michael I. Jordan", "Stuart Russell"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "``Socrates is human. All humans are mortal. Therefore, Socrates is mortal.''\nThis form of argument illustrates a typical pattern of two-hop reasoning.\nFormally, two-hop reasoning refers to the process of inferring a conclusion by\nmaking two logical steps, each connecting adjacent concepts, such that the\nfinal conclusion depends on the integration of both steps. It is one of the\nmost fundamental components of human reasoning and plays a crucial role in both\nformal logic and everyday decision-making. Despite recent progress in large\nlanguage models (LLMs), we surprisingly find that they can fail at solving\nsimple two-hop reasoning problems when distractors are present. We observe on a\nsynthetic dataset that pre-trained LLMs often resort to random guessing among\nall plausible conclusions. However, after few steps of fine-tuning, models\nachieve near-perfect accuracy and exhibit strong length generalization. To\nunderstand the underlying mechanisms, we train a 3-layer Transformer from\nscratch on a synthetic two-hop reasoning task and reverse-engineer its internal\ninformation flow. We observe a clear progression in the attention logits\nthroughout training. This pictures a sharp phase transition from an initial\nstage of random guessing to the emergence of a structured sequential query\nmechanism, where the model first retrieves the preceding and the bridge\nconcepts in the early layers and then uses them to infer the final answer.\nFinally, we show that these dynamics can be captured by a minimal\nthree-parameter attention-only network.", "AI": {"tldr": "The paper explores two-hop reasoning in large language models (LLMs), revealing their initial failure and subsequent improvement in performance after fine-tuning.", "motivation": "To investigate how LLMs handle two-hop reasoning problems and the impact of fine-tuning on their performance.", "method": "The authors train a 3-layer Transformer from scratch on a synthetic two-hop reasoning task and analyze its attention dynamics during training.", "result": "LLMs struggle with two-hop reasoning under distractors initially, but after fine-tuning, they reach nearly perfect accuracy and show structured query mechanisms in their attention layers.", "conclusion": "Fine-tuning significantly enhances LLMs' reasoning abilities, revealing a transition from random guessing to effective reasoning through observed attention patterns.", "key_contributions": ["Demonstrated LLMs' initial failure at two-hop reasoning", "Showed improvement after fine-tuning", "Revealed structured attention dynamics during training"], "limitations": "The study focuses on synthetic datasets, which may not fully represent real-world reasoning tasks.", "keywords": ["two-hop reasoning", "large language models", "attention mechanism", "fine-tuning", "synthetic dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.14245", "pdf": "https://arxiv.org/pdf/2502.14245.pdf", "abs": "https://arxiv.org/abs/2502.14245", "title": "Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering", "authors": ["Rongzhi Zhu", "Xiangyu Liu", "Zequn Sun", "Yiwei Wang", "Wei Hu"], "categories": ["cs.CL"], "comment": "Accepted in the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "In this paper, we identify a critical problem, \"lost-in-retrieval\", in\nretrieval-augmented multi-hop question answering (QA): the key entities are\nmissed in LLMs' sub-question decomposition. \"Lost-in-retrieval\" significantly\ndegrades the retrieval performance, which disrupts the reasoning chain and\nleads to the incorrect answers. To resolve this problem, we propose a\nprogressive retrieval and rewriting method, namely ChainRAG, which sequentially\nhandles each sub-question by completing missing key entities and retrieving\nrelevant sentences from a sentence graph for answer generation. Each step in\nour retrieval and rewriting process builds upon the previous one, creating a\nseamless chain that leads to accurate retrieval and answers. Finally, all\nretrieved sentences and sub-question answers are integrated to generate a\ncomprehensive answer to the original question. We evaluate ChainRAG on three\nmulti-hop QA datasets - MuSiQue, 2Wiki, and HotpotQA - using three large\nlanguage models: GPT4o-mini, Qwen2.5-72B, and GLM-4-Plus. Empirical results\ndemonstrate that ChainRAG consistently outperforms baselines in both\neffectiveness and efficiency.", "AI": {"tldr": "This paper addresses the \"lost-in-retrieval\" problem in multi-hop question answering by introducing ChainRAG, a method that sequentially retrieves and rewrites sub-questions to enhance answer accuracy.", "motivation": "The paper identifies the \"lost-in-retrieval\" issue in multi-hop QA, where LLMs fail to decompose sub-questions effectively, leading to poor retrieval performance and incorrect answers.", "method": "ChainRAG employs a progressive retrieval and rewriting method that completes missing key entities while retrieving relevant sentences from a sentence graph, ensuring each step builds on the previous one for accuracy.", "result": "ChainRAG is evaluated on three multi-hop QA datasets (MuSiQue, 2Wiki, and HotpotQA) and consistently outperforms baseline methods in terms of both effectiveness and efficiency.", "conclusion": "The proposed ChainRAG method provides a robust solution to enhance retrieval accuracy in multi-hop question answering by effectively managing sub-question decomposition.", "key_contributions": ["Introduction of the \"lost-in-retrieval\" problem in multi-hop QA", "Development of the ChainRAG method for progressive retrieval and rewriting", "Demonstrated improvement over baseline methods on multiple QA datasets"], "limitations": "", "keywords": ["multi-hop question answering", "retrieval-augmented generation", "ChainRAG", "large language models", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.15920", "pdf": "https://arxiv.org/pdf/2502.15920.pdf", "abs": "https://arxiv.org/abs/2502.15920", "title": "Self-Taught Agentic Long Context Understanding", "authors": ["Yufan Zhuang", "Xiaodong Yu", "Jialian Wu", "Ximeng Sun", "Ze Wang", "Jiang Liu", "Yusheng Su", "Jingbo Shang", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.CL", "cs.AI"], "comment": "Published at ACL 2025 Main Conference", "summary": "Answering complex, long-context questions remains a major challenge for large\nlanguage models (LLMs) as it requires effective question clarifications and\ncontext retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a\nframework designed to enhance an LLM's understanding of such queries by\nintegrating targeted self-clarification with contextual grounding within an\nagentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC),\nwhere models refine their understanding through self-generated clarification\nquestions and corresponding contextual groundings. By scaling inference as a\ntree search where each node represents a CoC step, we achieve 97.8% answer\nrecall on NarrativeQA with a search depth of up to three and a branching factor\nof eight. To amortize the high cost of this search process to training, we\nleverage the preference pairs for each step obtained by the CoC workflow and\nperform two-stage model finetuning: (1) supervised finetuning to learn\neffective decomposition strategies, and (2) direct preference optimization to\nenhance reasoning quality. This enables AgenticLU models to generate\nclarifications and retrieve relevant context effectively and efficiently in a\nsingle inference pass. Extensive experiments across seven long-context tasks\ndemonstrate that AgenticLU significantly outperforms state-of-the-art prompting\nmethods and specialized long-context LLMs, achieving robust multi-hop reasoning\nwhile sustaining consistent performance as context length grows.", "AI": {"tldr": "AgenticLU improves long-context question answering for LLMs by integrating self-clarification with context retrieval in a structured workflow, achieving high recall and performance across multiple tasks.", "motivation": "To address the challenge of answering complex, long-context questions using large language models (LLMs), which requires effective clarifications and context retrieval.", "method": "The study introduces Agentic Long-Context Understanding (AgenticLU), which uses Chain-of-Clarifications (CoC) to refine understanding with self-generated questions and context groundings, employing a tree search for inference.", "result": "Achieved 97.8% answer recall on NarrativeQA with a branching factor of eight, demonstrating the effectiveness of the two-stage model finetuning and the CoC workflow.", "conclusion": "AgenticLU models significantly outperform existing methods in long-context processing, revealing strong multi-hop reasoning capabilities and consistent performance with increasing context length.", "key_contributions": ["Integration of self-clarification into long-context understanding", "Development of Chain-of-Clarifications for improved model reasoning", "Two-stage model finetuning to enhance context retrieval efficiency"], "limitations": "", "keywords": ["Agentic Long-Context Understanding", "Chain-of-Clarifications", "Multi-hop reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.00134", "pdf": "https://arxiv.org/pdf/2503.00134.pdf", "abs": "https://arxiv.org/abs/2503.00134", "title": "Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary Recommendations", "authors": ["Zhongqi Yang", "Amir Rahmani"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) effectively leverage common-sense knowledge for\ngeneral reasoning, yet they struggle with personalized reasoning when tasked\nwith interpreting multifactor personal data. This limitation restricts their\napplicability in domains that require context-aware decision-making tailored to\nindividuals. This paper introduces Personalized Causal Graph Reasoning as an\nagentic framework that enhances LLM reasoning by incorporating personal causal\ngraphs derived from data of individuals. These graphs provide a foundation that\nguides the LLM's reasoning process. We evaluate it on a case study on\nnutrient-oriented dietary recommendations, which requires personal reasoning\ndue to the implicit unique dietary effects. We propose a counterfactual\nevaluation to estimate the efficiency of LLM-recommended foods for glucose\nmanagement. Results demonstrate that the proposed method efficiently provides\npersonalized dietary recommendations to reduce average glucose iAUC across\nthree time windows, which outperforms the previous approach. LLM-as-a-judge\nevaluation results indicate that our proposed method enhances personalization\nin the reasoning process.", "AI": {"tldr": "This paper presents a framework for improving personalized reasoning in LLMs by using personal causal graphs, evaluated through dietary recommendations for glucose management.", "motivation": "To address the limitations of LLMs in personalized reasoning with multifactor personal data for context-aware decision-making.", "method": "The paper introduces Personalized Causal Graph Reasoning which incorporates individual causal graphs into LLM reasoning, focusing on dietary recommendations as a case study.", "result": "The proposed framework demonstrates efficient personalized dietary recommendations that reduce average glucose iAUC, outperforming previous methods.", "conclusion": "The method enhances personalization in LLM reasoning, particularly in health-related applications like diet management.", "key_contributions": ["Introduction of Personalized Causal Graph Reasoning framework for LLMs", "Application in nutrient-oriented dietary recommendations", "Counterfactual evaluation methodology for assessing food recommendations"], "limitations": "", "keywords": ["Large Language Models", "Personalized reasoning", "Causal graphs", "Dietary recommendations", "Glucose management"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.04395", "pdf": "https://arxiv.org/pdf/2503.04395.pdf", "abs": "https://arxiv.org/abs/2503.04395", "title": "Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication", "authors": ["Tom Kouwenhoven", "Max Peeperkorn", "Roy de Kleijn", "Tessa Verhoef"], "categories": ["cs.CL"], "comment": "Presented at IJCAI 2025 (Human-centred AI Track)", "summary": "Languages are shaped by the inductive biases of their users. Using a\nclassical referential game, we investigate how artificial languages evolve when\noptimised for inductive biases in humans and large language models (LLMs) via\nHuman-Human, LLM-LLM and Human-LLM experiments. We show that referentially\ngrounded vocabularies emerge that enable reliable communication in all\nconditions, even when humans \\textit{and} LLMs collaborate. Comparisons between\nconditions reveal that languages optimised for LLMs subtly differ from those\noptimised for humans. Interestingly, interactions between humans and LLMs\nalleviate these differences and result in vocabularies more human-like than\nLLM-like. These findings advance our understanding of the role inductive biases\nin LLMs play in the dynamic nature of human language and contribute to\nmaintaining alignment in human and machine communication. In particular, our\nwork underscores the need to think of new LLM training methods that include\nhuman interaction and shows that using communicative success as a reward signal\ncan be a fruitful, novel direction.", "AI": {"tldr": "The paper explores how artificial languages evolve when optimized for human and LLM inductive biases in various collaborative settings, revealing that referentially grounded vocabularies emerge to facilitate effective communication.", "motivation": "To investigate the evolution of artificial languages shaped by the inductive biases of human users and LLMs, and to understand the resultant communicative dynamics between these entities.", "method": "The authors used a classical referential game to conduct experiments between humans, LLMs, and hybrid interactions to study how languages evolve under different conditions.", "result": "The study found that effective vocabularies emerged that support communication across all interaction types, with noteworthy differences in languages optimized for humans compared to those for LLMs. Collaborations resulted in more human-like vocabularies.", "conclusion": "The findings enhance the understanding of language dynamics between humans and LLMs and suggest the importance of including human interactions in LLM training methods to improve alignment and communicative success.", "key_contributions": ["Demonstrated the evolution of vocabularies shaped by inductive biases in different interaction types.", "Revealed the interaction effects that lead to more human-like vocabularies in collaborative settings with LLMs.", "Proposed new training methods for LLMs that incorporate human interactions to enhance communicative success."], "limitations": "", "keywords": ["artificial languages", "inductive biases", "human-LLM communication", "referential games", "LLM training"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.08057", "pdf": "https://arxiv.org/pdf/2503.08057.pdf", "abs": "https://arxiv.org/abs/2503.08057", "title": "Odysseus Navigates the Sirens' Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation", "authors": ["Wen Luo", "Feifan Song", "Wei Li", "Guangyue Peng", "Shaohang Wei", "Houfeng Wang"], "categories": ["cs.CL"], "comment": "Accepted to the ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) are increasingly required to generate text that\nis both factually accurate and diverse across various open-ended applications.\nHowever, current stochastic decoding methods struggle to balance such\nobjectives. We introduce Dynamic Focus Decoding (DFD), a novel plug-and-play\nstochastic approach that resolves this trade-off without requiring additional\ndata, knowledge, or models. DFD adaptively adjusts the decoding focus based on\ndistributional differences across layers, leveraging the modular and\nhierarchical nature of factual knowledge within LLMs. This dynamic adjustment\nimproves factuality in knowledge-intensive decoding steps and promotes\ndiversity in less knowledge-reliant steps. DFD can be easily integrated with\nexisting decoding methods, enhancing both factuality and diversity with minimal\ncomputational overhead. Extensive experiments across seven datasets demonstrate\nthat DFD significantly improves performance, providing a scalable and efficient\nsolution for open-ended text generation.", "AI": {"tldr": "Dynamic Focus Decoding (DFD) improves text generation by balancing factual accuracy and diversity in Large Language Models (LLMs).", "motivation": "To address the challenge that current decoding methods for LLMs struggle with balancing factual accuracy and diversity in generated text.", "method": "DFD is a novel stochastic approach that adaptively adjusts the decoding focus based on the distributional differences across layers of LLMs, without needing extra data or models.", "result": "Experimental results across seven datasets show that DFD significantly enhances performance in generating text that is both factually accurate and diverse.", "conclusion": "DFD provides a scalable and efficient solution for open-ended text generation by integrating easily with existing methods and yielding minimal computational overhead.", "key_contributions": ["Introduces Dynamic Focus Decoding (DFD) as a novel method for balancing factual accuracy and diversity in text generation.", "Demonstrates substantial performance improvements across multiple datasets using DFD.", "Offers an easily integrable approach with low computational overhead for existing decoding methods."], "limitations": "", "keywords": ["Dynamic Focus Decoding", "Large Language Models", "Text Generation", "Factual Accuracy", "Diversity"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2503.09454", "pdf": "https://arxiv.org/pdf/2503.09454.pdf", "abs": "https://arxiv.org/abs/2503.09454", "title": "Explicit Learning and the LLM in Machine Translation", "authors": ["Malik Marmonier", "Rachel Bawden", "Benoît Sagot"], "categories": ["cs.CL"], "comment": null, "summary": "This study explores an LLM's ability to learn new languages using\nexplanations found in a grammar book$\\unicode{x2014}$a process we term\n\"explicit learning.\" To rigorously assess this ability, we design controlled\ntranslation experiments between English and constructed languages\ngenerated$\\unicode{x2014}$by specific cryptographic means$\\unicode{x2014}$out\nof Latin or French. Contrary to previous studies, our results demonstrate that\nLLMs do possess a measurable capacity for explicit learning. This ability,\nhowever, diminishes as the complexity of the linguistic phenomena to be learned\nincreases. Supervised fine-tuning on ad hoc chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs, benefiting low-resource languages typically\ndescribed in grammar books but lacking extensive corpora.", "AI": {"tldr": "This study investigates how LLMs can learn new languages through explicit learning from grammar book explanations, showing measurable capacities but limitations with complexity.", "motivation": "To explore the explicit learning capabilities of LLMs in understanding new languages, particularly low-resource languages.", "method": "Controlled translation experiments were conducted using constructed languages derived from English, Latin, and French, assessing LLMs' performance in learning linguistic structures.", "result": "LLMs demonstrated a measurable ability for explicit learning, which diminishes with the complexity of linguistic features; fine-tuning improves performance but faces generalization challenges.", "conclusion": "Diverse training sets and new fine-tuning strategies are essential for enhancing explicit learning in LLMs, especially for low-resource languages.", "key_contributions": ["Demonstrated LLM explicit learning capability through grammar explanations.", "Identified limitations related to linguistic complexity and generalization.", "Proposed the need for diverse training methods."], "limitations": "The ability to learn diminishes with increased complexity; fine-tuning struggles to generalize to novel linguistic features.", "keywords": ["LLMs", "explicit learning", "linguistic phenomena", "fine-tuning", "low-resource languages"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.09790", "pdf": "https://arxiv.org/pdf/2503.09790.pdf", "abs": "https://arxiv.org/abs/2503.09790", "title": "Constrained Discrete Diffusion", "authors": ["Michael Cardei", "Jacob K Christopher", "Thomas Hartvigsen", "Brian R. Bartoldson", "Bhavya Kailkhura", "Ferdinando Fioretto"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Discrete diffusion models are a class of generative models that construct\nsequences by progressively denoising samples from a categorical noise\ndistribution. Beyond their rapidly growing ability to generate coherent natural\nlanguage, these models present a new and important opportunity to enforce\nsequence-level constraints, a capability that current autoregressive models\ncannot natively provide. This paper capitalizes on this opportunity by\nintroducing Constrained Discrete Diffusion (CDD), a novel integration of\ndifferentiable constraint optimization within the diffusion process to ensure\nadherence to constraints, logic rules, or safety requirements for generated\nsequences. Unlike conventional text generators that often rely on post-hoc\nfiltering or model retraining for controllable generation, CDD directly imposes\nconstraints into the discrete diffusion sampling process, resulting in a\ntraining-free and effective approach. Experiments in toxicity-controlled text\ngeneration, property-constrained molecule design, and instruction-constrained\ntext completion demonstrate that CDD achieves zero constraint violations in a\ndiverse array of tasks while preserving fluency, novelty, and coherence while\noutperforming autoregressive and existing discrete diffusion approaches.", "AI": {"tldr": "The paper introduces Constrained Discrete Diffusion (CDD), a generative model that allows enforcing sequence-level constraints during the diffusion process to ensure generated outputs adhere to specified rules or safety requirements.", "motivation": "To address the limitations of current autoregressive models in enforcing sequence-level constraints during text and other sequence generation.", "method": "The paper presents Constrained Discrete Diffusion (CDD), which integrates differentiable constraint optimization into the discrete diffusion sampling process, allowing direct enforcement of constraints without model retraining.", "result": "CDD achieves zero constraint violations in various tasks, such as toxicity-controlled text generation and property-constrained molecule design, while maintaining the quality of generated outputs.", "conclusion": "CDD provides an effective and training-free solution for controllable generation in comparison to traditional methods that rely on post-hoc filtering or retraining.", "key_contributions": ["Introduction of Constrained Discrete Diffusion (CDD) for enforcing sequence-level constraints during generation.", "A method that integrates differentiable constraint optimization within the diffusion process.", "Demonstrated effectiveness in various applications with zero constraint violations."], "limitations": "", "keywords": ["Constrained Discrete Diffusion", "generative models", "sequence generation", "constraint optimization", "toxic text generation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.10460", "pdf": "https://arxiv.org/pdf/2503.10460.pdf", "abs": "https://arxiv.org/abs/2503.10460", "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond", "authors": ["Liang Wen", "Yunke Cai", "Fenrui Xiao", "Xin He", "Qi An", "Zhenyu Duan", "Yimin Du", "Junchen Liu", "Lifu Tang", "Xiaowei Lv", "Haosheng Zou", "Yongchao Deng", "Shousheng Jia", "Xiangzheng Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "v4: ACL'25 industry track camera ready; v3: minor modifications; v2:\n  better writing & format for later submission; all release at\n  https://github.com/Qihoo360/Light-R1", "summary": "This paper introduces Light-R1, an open-source suite for training long\nreasoning models using reproducible and cost-effective methodology. Given the\nproprietary nature of data used in the DeepSeek-R1 series, we develop an\nalternative approach leveraging exclusively public data and models. Our\ncurriculum training progressively increases data difficulty, combined with\nmulti-staged post-training. Our Light-R1-32B model, trained from\nQwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math\nreasoning.\n  Experimental results show that this curriculum approach becomes more\neffective when distinct, diverse datasets are available for different training\nstages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on\nproprietary data) with 3,000 challenging examples from our curriculum dataset\nyielded state-of-the-art 7B and 14B models, while the 32B model,\nLight-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying GRPO on long reasoning models.\nOur final Light-R1-14B-DS achieves SOTA performance among 14B models in math,\nwith AIME24 & 25 scores of 74.0 and 60.2 respectively, surpassing many 32B\nmodels and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training,\nLight-R1-14B-DS demonstrates strong cross-domain generalization.\n  Light-R1 represents a significant advancement in making sophisticated\nreasoning models more accessible and implementable in real-world applications.\nOur models, training data and code have been made available at\nhttps://github.com/Qihoo360/Light-R1.", "AI": {"tldr": "Light-R1 is an open-source suite designed for training long reasoning models using a reproducible methodology with public data, achieving state-of-the-art performance in math reasoning while enabling cross-domain generalization.", "motivation": "To create an accessible and cost-effective approach for training sophisticated reasoning models using exclusively public data, addressing limitations of proprietary data methodologies in existing models.", "method": "Light-R1 uses a curriculum training approach that progressively increases the difficulty of data and includes multi-staged post-training, enabling effective training of models like Light-R1-32B and Light-R1-14B-DS.", "result": "Experiments show Light-R1 models outperform existing proprietary models, achieving state-of-the-art performance in math reasoning with the Light-R1-14B-DS model surpassing many larger models.", "conclusion": "Light-R1 advances the field of long reasoning models by making sophisticated training methodologies available to the research community, with all models and data made publicly accessible.", "key_contributions": ["Introduction of an open-source training suite for long reasoning models.", "Demonstration of the effectiveness of curriculum training on diverse datasets.", "Achievement of state-of-the-art performance in math reasoning with cross-domain generalization."], "limitations": "", "keywords": ["Long reasoning models", "Curriculum training", "State-of-the-art performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.12051", "pdf": "https://arxiv.org/pdf/2503.12051.pdf", "abs": "https://arxiv.org/abs/2503.12051", "title": "TLUE: A Tibetan Language Understanding Evaluation Benchmark", "authors": ["Fan Gao", "Cheng Huang", "Nyima Tashi", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Hao Wang Xiao Feng", "Yongbin Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have made tremendous progress in recent years,\nbut low-resource languages, such as Tibetan, remain significantly\nunderrepresented in their evaluation. Despite Tibetan being spoken by over\nseven million people, it has largely been neglected in the development and\nassessment of LLMs. To address this gap, we present TLUE (A Tibetan Language\nUnderstanding Evaluation Benchmark), the first large-scale benchmark for\nassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:\n(1) a comprehensive multi-task understanding benchmark spanning 5 domains and\n67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a\ndiverse set of state-of-the-art LLMs. Experimental results demonstrate that\nmost LLMs perform below the random baseline, highlighting the considerable\nchallenges LLMs face in processing Tibetan, a low-resource language. TLUE\nprovides an essential foundation for driving future research and progress in\nTibetan language understanding and underscores the need for greater inclusivity\nin LLM development.", "AI": {"tldr": "TLUE is the first benchmark for assessing large language models' capabilities in Tibetan, revealing significant performance gaps.", "motivation": "To address the underrepresentation of low-resource languages like Tibetan in large language models (LLMs).", "method": "We present TLUE, a comprehensive multi-task understanding benchmark spanning 5 domains and 67 subdomains, along with a safety benchmark covering 7 subdomains.", "result": "Experimental results show that most LLMs perform below the random baseline, indicating challenges in processing Tibetan.", "conclusion": "TLUE establishes a foundation for future research in Tibetan language understanding and calls for inclusivity in LLM development.", "key_contributions": ["Introduction of the TLUE benchmark for Tibetan languages.", "Evaluation of state-of-the-art LLMs on Tibetan tasks.", "Highlighting the performance gap in LLMs for low-resource languages."], "limitations": "Focus limited to Tibetan; may not generalize to other low-resource languages.", "keywords": ["Tibetan", "Language Models", "Benchmark", "Low-Resource Languages", "Natural Language Processing"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2503.17933", "pdf": "https://arxiv.org/pdf/2503.17933.pdf", "abs": "https://arxiv.org/abs/2503.17933", "title": "Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA", "authors": ["Justice Ou", "Tinglin Huang", "Yilun Zhao", "Ziyang Yu", "Peiqing Lu", "Rex Ying"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "To improve the reliability of Large Language Models (LLMs) in clinical\napplications, retrieval-augmented generation (RAG) is extensively applied to\nprovide factual medical knowledge. However, beyond general medical knowledge\nfrom open-ended datasets, clinical case-based knowledge is also critical for\neffective medical reasoning, as it provides context grounded in real-world\npatient experiences.Motivated by this, we propose Experience\nRetrieval-Augmentation ExpRAG framework based on Electronic Health Record(EHR),\naiming to offer the relevant context from other patients' discharge reports.\nExpRAG performs retrieval through a coarse-to-fine process, utilizing an\nEHR-based report ranker to efficiently identify similar patients, followed by\nan experience retriever to extract task-relevant content for enhanced medical\nreasoning.To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset\nwith 1,280 discharge-related questions across diagnosis, medication, and\ninstruction tasks. Each problem is generated using EHR data to ensure realistic\nand challenging scenarios. Experimental results demonstrate that ExpRAG\nconsistently outperforms a text-based ranker, achieving an average relative\nimprovement of 5.2%, highlighting the importance of case-based knowledge for\nmedical reasoning.", "AI": {"tldr": "This paper presents the ExpRAG framework for improving LLM reliability in clinical applications by utilizing case-based knowledge from EHRs for medical reasoning.", "motivation": "The need for reliable LLMs in clinical settings, specifically for incorporating clinical case-based knowledge that reflects real-world patient experiences.", "method": "ExpRAG employs a coarse-to-fine retrieval process that uses an EHR-based report ranker to find similar patients followed by an experience retriever to extract relevant task-related content.", "result": "ExpRAG demonstrates a 5.2% average relative improvement in performance over traditional text-based ranking methods in clinical question answering tasks.", "conclusion": "The study underscores the significance of incorporating case-based knowledge into LLMs for enhanced medical reasoning and treatment responses in clinical applications.", "key_contributions": ["Introduction of the ExpRAG framework leveraging EHRs for retrieval-augmented generation in medical contexts.", "Development of the DischargeQA dataset, which includes realistic clinical questions derived from EHR data.", "Empirical validation showing improved performance of ExpRAG over existing text-based methods."], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Electronic Health Records", "Clinical Reasoning", "DischargeQA"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.18288", "pdf": "https://arxiv.org/pdf/2503.18288.pdf", "abs": "https://arxiv.org/abs/2503.18288", "title": "Sun-Shine: A Foundation Large Language Model for Tibetan Culture and Heritage", "authors": ["Cheng Huang", "Fan Gao", "Yutong Liu", "Nyima Tashi", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Hao Wang", "Yongbin Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities.", "AI": {"tldr": "Introduction of Llama-Sunshine, the first large language model tailored for Tibetan culture and language processing.", "motivation": "To address the lack of adequate language models for the Tibetan language, which has a complex grammatical structure and is a minority language, leading to data scarcity and under-explored cultural potential.", "method": "Development of Llama-Sunshine (Sun-Shine), a large language model optimized for Tibetan linguistic features, along with the creation of the TIB-STC dataset consisting of diverse Tibetan texts.", "result": "Sun-Shine demonstrates superior knowledge expertise in Tibetan culture and begins to showcase embodied intelligence in various language processing tasks, excelling in low-resource conditions.", "conclusion": "The development of Llama-Sunshine and the TIB-STC dataset represents a significant advancement for Tibetan language processing and enhances the potential for AI applications in Tibetan culture.", "key_contributions": ["First large language model for Tibetan culture (Sun-Shine)", "Creation of the TIB-STC dataset for Tibetan texts", "Exhibits strong performance in low-resource language scenarios"], "limitations": "", "keywords": ["Tibetan language", "Large Language Models", "TIB-STC dataset", "Cultural applications", "NLP"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2504.01002", "pdf": "https://arxiv.org/pdf/2504.01002.pdf", "abs": "https://arxiv.org/abs/2504.01002", "title": "Token embeddings violate the manifold hypothesis", "authors": ["Michael Robinson", "Sourya Dey", "Tony Chiang"], "categories": ["cs.CL", "cs.AI", "53Z50, 62H15"], "comment": "27 pages, 6 figures, 9 tables", "summary": "A full understanding of the behavior of a large language model (LLM) requires\nour understanding of its input token space. If this space differs from our\nassumptions, our understanding of and conclusions about the LLM will likely be\nflawed. We elucidate the structure of the token embeddings both empirically and\ntheoretically. We present a novel statistical test assuming that the\nneighborhood around each token has a relatively flat and smooth structure as\nthe null hypothesis. Failing to reject the null is uninformative, but rejecting\nit at a specific token $\\psi$ implies an irregularity in the token subspace in\na $\\psi$-neighborhood, $B(\\psi)$. The structure assumed in the null is a\ngeneralization of a manifold with boundary called a \\emph{smooth fiber bundle}\n(which can be split into two spatial regimes -- small and large radius), so we\ndenote our new hypothesis test as the ``fiber bundle hypothesis.'' Failure to\nreject the null hypothesis is uninformative, but rejecting it at $\\psi$\nindicates a statistically significant irregularity at $B(\\psi)$. By running our\ntest over several open-source LLMs, each with unique token embeddings, we find\nthat the null is frequently rejected, and so the evidence suggests that the\ntoken subspace is not a fiber bundle and hence also not a manifold. As a\nconsequence of our findings, when an LLM is presented with two semantically\nequivalent prompts, if one prompt contains a token implicated by our test, the\nresponse to that prompt will likely exhibit less stability than the other.", "AI": {"tldr": "This paper examines the input token space of large language models (LLMs) using a novel statistical test to understand irregularities in token embeddings.", "motivation": "Understanding the behavior of LLMs is contingent upon accurate assumptions about their input token space, as incorrect assumptions can lead to flawed conclusions about the model.", "method": "The paper introduces a statistical test based on the fiber bundle hypothesis that examines the smooth structure of token neighborhoods. It empirically tests this hypothesis across multiple open-source LLMs.", "result": "The tests frequently reject the null hypothesis, indicating that the token subspace does not behave as a smooth fiber bundle and may suggest instability in responses from LLMs with semantically similar prompts.", "conclusion": "The findings imply that irregularities in the token subspace can lead to less stable responses in LLMs when specific tokens are involved in semantically equivalent prompts.", "key_contributions": ["Development of the fiber bundle hypothesis for analyzing token subspace structures.", "Empirical validation across several open-source LLMs, revealing frequent rejection of the null hypothesis.", "Insights into the stability of LLM responses based on token irregularities."], "limitations": "", "keywords": ["large language models", "token embeddings", "fiber bundle hypothesis", "smooth structure", "statistical test"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.03312", "pdf": "https://arxiv.org/pdf/2504.03312.pdf", "abs": "https://arxiv.org/abs/2504.03312", "title": "Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices", "authors": ["Luís Couto Seller", "Íñigo Sanz Torres", "Adrián Vogel-Fernández", "Carlos González Carballo", "Pedro Miguel Sánchez Sánchez", "Adrián Carruana Martín", "Enrique de Miguel Ambite"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at SEPLN 2025 conference", "summary": "Large Language Models have significantly advanced natural language\nprocessing, achieving remarkable performance in tasks such as language\ngeneration, translation, and reasoning. However, their substantial\ncomputational requirements restrict deployment to high-end systems, limiting\naccessibility on consumer-grade devices. This challenge is especially\npronounced for under-resourced languages like those spoken in the Iberian\nPeninsula, where relatively limited linguistic resources and benchmarks hinder\neffective evaluation. This work presents a comprehensive evaluation of compact\nstate-of-the-art LLMs across several essential NLP tasks tailored for Iberian\nlanguages. The results reveal that while some models consistently excel in\ncertain tasks, significant performance gaps remain, particularly for languages\nsuch as Basque. These findings highlight the need for further research on\nbalancing model compactness with robust multilingual performance", "AI": {"tldr": "This work evaluates compact state-of-the-art large language models for NLP tasks focused on under-resourced Iberian languages, revealing performance gaps and the need for further research.", "motivation": "To address the accessibility challenges of deploying large language models on consumer-grade devices, especially for under-resourced languages.", "method": "Comprehensive evaluation of several compact LLMs across various essential NLP tasks specifically tailored for Iberian languages.", "result": "Certain models show consistent excellence in specific tasks, but performance gaps exist, particularly with languages like Basque.", "conclusion": "There is a need for further research to balance model compactness with performance across multilingual tasks, especially for under-resourced languages.", "key_contributions": ["Evaluation of LLMs for Iberian languages", "Identification of performance gaps in NLP tasks", "Insights into compact model deployment for limited-resource languages"], "limitations": "", "keywords": ["large language models", "Iberian languages", "NLP tasks", "model performance", "compact LLMs"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.03561", "pdf": "https://arxiv.org/pdf/2504.03561.pdf", "abs": "https://arxiv.org/abs/2504.03561", "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement", "authors": ["Runnan Fang", "Xiaobin Wang", "Yuan Liang", "Shuofei Qiao", "Jialong Wu", "Zekun Xi", "Ningyu Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "ACL 2025 Findings", "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.", "AI": {"tldr": "SynWorld is a framework that enables LLM-based agents to enhance their action capabilities in novel environments through scenario synthesis and Monte Carlo Tree Search.", "motivation": "Agents need to adapt their action capabilities in novel environments and unconventional action spaces to improve their performance and understanding.", "method": "The SynWorld framework allows agents to synthesize scenarios while invoking multi-step actions and uses Monte Carlo Tree Search for exploration and action knowledge refinement.", "result": "Experiments show that SynWorld effectively helps agents learn action knowledge in new environments.", "conclusion": "SynWorld provides a general method for improving agents' learning capabilities in varied action contexts, demonstrating its efficacy in diverse settings.", "key_contributions": ["Introduction of SynWorld framework for LLM-based agents", "Utilization of scenario synthesis and MCTS for action exploration", "Demonstration of effective learning in novel environments"], "limitations": "", "keywords": ["Human-Computer Interaction", "Machine Learning", "Monte Carlo Tree Search", "Action Knowledge", "LLM Agents"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.05104", "pdf": "https://arxiv.org/pdf/2504.05104.pdf", "abs": "https://arxiv.org/abs/2504.05104", "title": "AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments", "authors": ["Saeid Ario Vaghefi", "Aymane Hachcham", "Veronica Grasso", "Jiska Manicus", "Nakiete Msemo", "Chiara Colesanti Senni", "Markus Leippold"], "categories": ["cs.CL"], "comment": null, "summary": "Tracking financial investments in climate adaptation is a complex and\nexpertise-intensive task, particularly for Early Warning Systems (EWS), which\nlack standardized financial reporting across multilateral development banks\n(MDBs) and funds. To address this challenge, we introduce an LLM-based agentic\nAI system that integrates contextual retrieval, fine-tuning, and multi-step\nreasoning to extract relevant financial data, classify investments, and ensure\ncompliance with funding guidelines. Our study focuses on a real-world\napplication: tracking EWS investments in the Climate Risk and Early Warning\nSystems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple\nAI-driven classification methods, including zero-shot and few-shot learning,\nfine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and\nan agent-based retrieval-augmented generation (RAG) approach. Our results show\nthat the agent-based RAG approach significantly outperforms other methods,\nachieving 87\\% accuracy, 89\\% precision, and 83\\% recall. Additionally, we\ncontribute a benchmark dataset and expert-annotated corpus, providing a\nvaluable resource for future research in AI-driven financial tracking and\nclimate finance transparency.", "AI": {"tldr": "The paper presents an LLM-based AI system for tracking financial investments in climate adaptation, focusing on Early Warning Systems, and demonstrates its effectiveness in classifying investment data from MDBs.", "motivation": "The need for standardized financial reporting in climate adaptation investments, particularly for Early Warning Systems where current methods lack accuracy and transparency.", "method": "Developed an LLM-based agentic AI system using contextual retrieval, fine-tuning, and multi-step reasoning; evaluated various AI classification methods on MDB project documents.", "result": "The agent-based RAG approach achieved significant classification performance with 87% accuracy, 89% precision, and 83% recall.", "conclusion": "The study contributes to AI-driven financial tracking in climate finance by presenting an effective classification method and a benchmark dataset for future use.", "key_contributions": ["Introduction of an LLM-based agentic AI system for financial data classification", "Demonstrated superior performance of the RAG approach", "Provided a benchmark dataset and expert-annotated corpus for research"], "limitations": "", "keywords": ["AI", "Climate Finance", "LLM", "Investment Tracking", "Early Warning Systems"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2504.06792", "pdf": "https://arxiv.org/pdf/2504.06792.pdf", "abs": "https://arxiv.org/abs/2504.06792", "title": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations", "authors": ["Zican Dong", "Han Peng", "Peiyu Liu", "Wayne Xin Zhao", "Dong Wu", "Feng Xiao", "Zhifeng Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Mixture-of-Experts (MoE) models achieve a favorable trade-off between\nperformance and inference efficiency by activating only a subset of experts.\nHowever, the memory overhead of storing all experts remains a major limitation,\nespecially in large-scale MoE models such as DeepSeek-R1(671B). In this study,\nwe investigate domain specialization and expert redundancy in large-scale MoE\nmodels and uncover a consistent behavior we term few-shot expert localization,\nwith only a few in-domain demonstrations, the model consistently activates a\nsparse and stable subset of experts on tasks within the same domain. Building\non this observation, we propose a simple yet effective pruning framework,\nEASY-EP, that leverages a few domain-specific demonstrations to identify and\nretain only the most relevant experts. EASY-EP comprises two key components:\noutput-aware expert importance assessment and expert-level token contribution\nestimation. The former evaluates the importance of each expert for the current\ntoken by considering the gating scores and L2 norm of the outputs of activated\nexperts, while the latter assesses the contribution of tokens based on\nrepresentation similarities before and after routed experts. Experiments on\nDeepSeek-R1 and DeepSeek-V3-0324 show that our method can achieve comparable\nperformances and $2.99\\times$ throughput under the same memory budget with full\nmodel with only half the experts.", "AI": {"tldr": "This study investigates the memory issues in large-scale Mixture-of-Experts (MoE) models and proposes a pruning framework, EASY-EP, to improve efficiency by retaining only the most relevant experts.", "motivation": "Large-scale MoE models face significant memory overhead due to the storage of all experts, which limits their deployment in practice.", "method": "The paper introduces EASY-EP, a pruning framework that uses a few domain-specific demonstrations to identify and retain the most relevant experts, based on output-aware importance assessment and token contribution estimation.", "result": "EASY-EP can achieve comparable performance and 2.99 times throughput under the same memory budget as the full model with half the number of experts activated.", "conclusion": "The findings indicate that with few in-domain demonstrations, MoE models effectively localize the important experts needed for specific tasks, enhancing efficiency without sacrificing performance.", "key_contributions": ["Proposed the EASY-EP framework for expert pruning in MoE models.", "Demonstrated few-shot expert localization behavior in MoE models.", "Showed significant performance improvements in throughput with reduced expert activation."], "limitations": "", "keywords": ["Mixture-of-Experts", "deep learning", "memory efficiency", "pruning framework", "domain specialization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.08775", "pdf": "https://arxiv.org/pdf/2504.08775.pdf", "abs": "https://arxiv.org/abs/2504.08775", "title": "Layers at Similar Depths Generate Similar Activations Across LLM Architectures", "authors": ["Christopher Wolfram", "Aaron Schein"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "How do the latent spaces used by independently-trained LLMs relate to one\nanother? We study the nearest neighbor relationships induced by activations at\ndifferent layers of 24 open-weight LLMs, and find that they 1) tend to vary\nfrom layer to layer within a model, and 2) are approximately shared between\ncorresponding layers of different models. Claim 2 shows that these nearest\nneighbor relationships are not arbitrary, as they are shared across models, but\nClaim 1 shows that they are not \"obvious\" either, as there is no single set of\nnearest neighbor relationships that is universally shared. Together, these\nsuggest that LLMs generate a progression of activation geometries from layer to\nlayer, but that this entire progression is largely shared between models,\nstretched and squeezed to fit into different architectures.", "AI": {"tldr": "This study investigates the nearest neighbor relationships in the latent spaces of 24 independently-trained LLMs across different layers, revealing variability within models and shared patterns across different architectures.", "motivation": "To understand how the latent spaces of independently trained LLMs relate, particularly focusing on the nearest neighbor relationships at different layers.", "method": "The study analyzes the activation patterns and nearest neighbor relations of 24 open-weight LLMs, examining variability within layers of a model and similarities between layers of different models.", "result": "The findings indicate that nearest neighbor relationships vary by layer within each model but reveal a commonality across corresponding layers of different models, suggesting a shared progression of activation geometries.", "conclusion": "The activation geometries transition from layer to layer within each LLM, but the overall progression is largely uniform across different LLM architectures, indicating a structured similarity despite architectural differences.", "key_contributions": ["Analysis of nearest neighbor relationships in LLM activations", "Demonstration of shared activation geometries across different models", "Insight into how different architectures influence activation relationships"], "limitations": "", "keywords": ["LLM", "activation geometries", "nearest neighbor relationships", "layer analysis", "model comparison"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.12339", "pdf": "https://arxiv.org/pdf/2504.12339.pdf", "abs": "https://arxiv.org/abs/2504.12339", "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM", "authors": ["Yaodong Song", "Hongjie Chen", "Jie Lian", "Yuxin Zhang", "Guangmin Xia", "Zehan Li", "Genliang Zhao", "Jian Kang", "Jie Li", "Yongxiang Li", "Xuelong Li"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS)\nsynthesis through discrete tokenization paradigms, current architectures\nexhibit fundamental tensions between three critical dimensions: 1) irreversible\nloss of acoustic characteristics caused by quantization of speech prompts; 2)\nstringent dependence on precisely aligned prompt speech-text pairs that limit\nreal-world deployment; and 3) catastrophic forgetting of the LLM's native text\ncomprehension during optimization for speech token generation. To address these\nchallenges, we propose an LLM-based text-to-speech Generation approach\nOptimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework\nintroduces two key innovations: (1) The modality-alignment branch combines a\nspeech encoder and projector to capture continuous acoustic embeddings,\nenabling bidirectional correlation between paralinguistic features (language,\ntimbre, emotion) and semantic text representations without transcript\ndependency; (2) The speech-generation branch employs modular fine-tuning on\ntop-k layers of an LLM for speech token prediction while freezing the bottom-n\nlayers to preserve foundational linguistic knowledge. Moreover, multi-token\nprediction is introduced to support real-time streaming TTS synthesis.\nExperimental results demonstrate that our GOAT-TTS achieves performance\ncomparable to state-of-the-art TTS models while validating the efficacy of\nsynthesized dialect speech data.", "AI": {"tldr": "This paper presents GOAT-TTS, a novel LLM-based approach for text-to-speech synthesis that addresses issues of acoustic quality and dependency on aligned speech-text pairs.", "motivation": "Current TTS models face limitations due to acoustic characteristic loss, dependence on aligned data, and catastrophic forgetting in LLMs during optimization.", "method": "GOAT-TTS employs a dual-branch architecture with a modality-alignment branch for capturing acoustic embeddings and a speech-generation branch for fine-tuning LLMs for speech generation.", "result": "Experimental results show that GOAT-TTS achieves performance on par with state-of-the-art TTS models and effectively utilizes synthesized dialect speech data.", "conclusion": "The proposed framework not only preserves the linguistic knowledge of LLMs but also enhances TTS synthesis quality, enabling real-time streaming capabilities.", "key_contributions": ["Introduction of a dual-branch architecture for TTS synthesis.", "Modality-alignment branch that enables bidirectional feature correlation without transcript dependency.", "Real-time multi-token prediction for speech generation."], "limitations": "", "keywords": ["text-to-speech", "large language models", "speech synthesis", "real-time generation", "acoustic embeddings"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2504.13904", "pdf": "https://arxiv.org/pdf/2504.13904.pdf", "abs": "https://arxiv.org/abs/2504.13904", "title": "Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Yuewen Sun", "Xinshuai Dong", "Kazushi Ikeda", "Peter Spirtes", "Kun Zhang"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "12 pages, 10 figures, 1 table. Accepted by ACM UMAP 2025", "summary": "We hypothesize that optimal system responses emerge from adaptive strategies\ngrounded in causal and counterfactual knowledge. Counterfactual inference\nallows us to create hypothetical scenarios to examine the effects of\nalternative system responses. We enhance this process through causal discovery,\nwhich identifies the strategies informed by the underlying causal structure\nthat govern system behaviors. Moreover, we consider the psychological\nconstructs and unobservable noises that might be influencing user-system\ninteractions as latent factors. We show that these factors can be effectively\nestimated. We employ causal discovery to identify strategy-level causal\nrelationships among user and system utterances, guiding the generation of\npersonalized counterfactual dialogues. We model the user utterance strategies\nas causal factors, enabling system strategies to be treated as counterfactual\nactions. Furthermore, we optimize policies for selecting system responses based\non counterfactual data. Our results using a real-world dataset on social good\ndemonstrate significant improvements in persuasive system outcomes, with\nincreased cumulative rewards validating the efficacy of causal discovery in\nguiding personalized counterfactual inference and optimizing dialogue policies\nfor a persuasive dialogue system.", "AI": {"tldr": "This paper explores using causal and counterfactual knowledge to enhance system responses in persuasive dialogue systems, demonstrating improved outcomes through personalized strategies derived from causal relationships.", "motivation": "To improve the effectiveness of system responses in human-computer interactions by utilizing causal knowledge and counterfactual reasoning.", "method": "The authors implement causal discovery to identify relationships between user and system utterances, allowing for the generation of personalized counterfactual dialogues and optimization of response policies.", "result": "The application of these methods on a real-world dataset shows significant improvements in persuasive system outcomes, with increased cumulative rewards.", "conclusion": "Causal discovery effectively informs personalized counterfactual inference and optimizes dialogue strategies, enhancing user-system interaction outcomes in persuasive settings.", "key_contributions": ["Introduction of causal discovery for analyzing user-system interactions", "Development of personalized counterfactual dialogues based on identified causal relationships", "Demonstrated effectiveness of optimized system response policies using real-world data"], "limitations": "", "keywords": ["causal inference", "counterfactual reasoning", "persuasive dialogue systems", "causal discovery", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 12}}
