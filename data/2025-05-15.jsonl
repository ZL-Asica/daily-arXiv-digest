{"id": "2505.08894", "pdf": "https://arxiv.org/pdf/2505.08894.pdf", "abs": "https://arxiv.org/abs/2505.08894", "title": "WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp", "authors": ["Hiba Eltigani", "Rukhshan Haroon", "Asli Kocak", "Abdullah Bin Faisal", "Noah Martin", "Fahad Dogar"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advances in generative AI, such as ChatGPT, have transformed access to\ninformation in education, knowledge-seeking, and everyday decision-making.\nHowever, in many developing regions, access remains a challenge due to the\npersistent digital divide. To help bridge this gap, we developed WaLLM - a\ncustom AI chatbot over WhatsApp, a widely used communication platform in\ndeveloping regions. Beyond answering queries, WaLLM offers several features to\nenhance user engagement: a daily top question, suggested follow-up questions,\ntrending and recent queries, and a leaderboard-based reward system. Our service\nhas been operational for over 6 months, amassing over 14.7K queries from\napproximately 100 users. In this paper, we present WaLLM's design and a\nsystematic analysis of logs to understand user interactions. Our results show\nthat 55% of user queries seek factual information. \"Health and well-being\" was\nthe most popular topic (28%), including queries about nutrition and disease,\nsuggesting users view WaLLM as a reliable source. Two-thirds of users' activity\noccurred within 24 hours of the daily top question. Users who accessed the\n\"Leaderboard\" interacted with WaLLM 3x as those who did not. We conclude by\ndiscussing implications for culture-based customization, user interface design,\nand appropriate calibration of users' trust in AI systems for developing\nregions."}
{"id": "2505.08902", "pdf": "https://arxiv.org/pdf/2505.08902.pdf", "abs": "https://arxiv.org/abs/2505.08902", "title": "Performance Gains of LLMs With Humans in a World of LLMs Versus Humans", "authors": ["Lucas McCullum", "Pelagie Ami Agassi", "Leo Anthony Celi", "Daniel K. Ebner", "Chrystinne Oliveira Fernandes", "Rachel S. Hicklen", "Mkliwa Koumbia", "Lisa Soleymani Lehmann", "David Restrepo"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Currently, a considerable research effort is devoted to comparing LLMs to a\ngroup of human experts, where the term \"expert\" is often ill-defined or\nvariable, at best, in a state of constantly updating LLM releases. Without\nproper safeguards in place, LLMs will threaten to cause harm to the established\nstructure of safe delivery of patient care which has been carefully developed\nthroughout history to keep the safety of the patient at the forefront. A key\ndriver of LLM innovation is founded on community research efforts which, if\ncontinuing to operate under \"humans versus LLMs\" principles, will expedite this\ntrend. Therefore, research efforts moving forward must focus on effectively\ncharacterizing the safe use of LLMs in clinical settings that persist across\nthe rapid development of novel LLM models. In this communication, we\ndemonstrate that rather than comparing LLMs to humans, there is a need to\ndevelop strategies enabling efficient work of humans with LLMs in an almost\nsymbiotic manner."}
{"id": "2505.08939", "pdf": "https://arxiv.org/pdf/2505.08939.pdf", "abs": "https://arxiv.org/abs/2505.08939", "title": "Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work", "authors": ["Suchismita Naik", "Prakash Shukla", "Ike Obi", "Jessica Backus", "Nancy Rasche", "Paul Parsons"], "categories": ["cs.HC", "cs.AI"], "comment": "5 pages, 2 Tables, In Creativity and Cognition 2025, June 23--25,\n  2025, Virtual, United Kingdom", "summary": "As generative AI tools become integrated into design workflows, students\nincreasingly engage with these tools not just as aids, but as collaborators.\nThis study analyzes reflections from 33 student teams in an HCI design course\nto examine the kinds of judgments students make when using AI tools. We found\nboth established forms of design judgment (e.g., instrumental, appreciative,\nquality) and emergent types: agency-distribution judgment and reliability\njudgment. These new forms capture how students negotiate creative\nresponsibility with AI and assess the trustworthiness of its outputs. Our\nfindings suggest that generative AI introduces new layers of complexity into\ndesign reasoning, prompting students to reflect not only on what AI produces,\nbut also on how and when to rely on it. By foregrounding these judgments, we\noffer a conceptual lens for understanding how students engage in co-creative\nsensemaking with AI in design contexts."}
{"id": "2505.09047", "pdf": "https://arxiv.org/pdf/2505.09047.pdf", "abs": "https://arxiv.org/abs/2505.09047", "title": "Positioning Monocular Optical See Through Head Worn Displays in Glasses for Everyday Wear", "authors": ["Parth Arora", "Ethan Kimmel", "Katherine Huang", "Tyler Kwok", "Yukun Song", "Sofia Vempala", "Georgianna Lin", "Ozan Cakmakci", "Thad Starner"], "categories": ["cs.HC"], "comment": null, "summary": "Head-worn displays for everyday wear in the form of regular eyeglasses are\ntechnically feasible with recent advances in waveguide technology. One major\ndesign decision is determining where in the user's visual field to position the\ndisplay. Centering the display in the principal point of gaze (PPOG) allows the\nuser to switch attentional focus between the virtual and real images quickly,\nand best performance often occurs when the display is centered in PPOG or is\ncentered vertically below PPOG. However, these positions are often undesirable\nin that they are considered interruptive or are associated with negative social\nperceptions by users. Offsetting the virtual image may be preferred when tasks\ninvolve driving, walking, or social interaction. This paper consolidates\nfindings from recent studies on monocular optical see-through HWDs (OST-HWDs),\nfocusing on potential for interruption, comfort, performance, and social\nperception. For text-based tasks, which serve as a proxy for many monocular\nOST-HWD tasks, we recommend a 15{\\deg} horizontal field of view (FOV) with the\nvirtual image in the right lens vertically centered but offset to +8.7{\\deg} to\n+23.7{\\deg} toward the ear. Glanceable content can be offset up to +30{\\deg}\nfor short interactions."}
{"id": "2505.08828", "pdf": "https://arxiv.org/pdf/2505.08828.pdf", "abs": "https://arxiv.org/abs/2505.08828", "title": "Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence", "authors": ["Eduardo Araujo Oliveira", "Madhavi Mohoni", "Sonsoles López-Pernas", "Mohammed Saqr"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "19 pages, 10 figures, 11 tables", "summary": "As human-AI collaboration becomes increasingly prevalent in educational\ncontexts, understanding and measuring the extent and nature of such\ninteractions pose significant challenges. This research investigates the use of\nauthorship verification (AV) techniques not as a punitive measure, but as a\nmeans to quantify AI assistance in academic writing, with a focus on promoting\ntransparency, interpretability, and student development. Building on prior\nwork, we structured our investigation into three stages: dataset selection and\nexpansion, AV method development, and systematic evaluation. Using three\ndatasets - including a public dataset (PAN-14) and two from University of\nMelbourne students from various courses - we expanded the data to include\nLLM-generated texts, totalling 1,889 documents and 540 authorship problems from\n506 students. We developed an adapted Feature Vector Difference AV methodology\nto construct robust academic writing profiles for students, designed to capture\nmeaningful, individual characteristics of their writing. The method's\neffectiveness was evaluated across multiple scenarios, including distinguishing\nbetween student-authored and LLM-generated texts and testing resilience against\nLLMs' attempts to mimic student writing styles. Results demonstrate the\nenhanced AV classifier's ability to identify stylometric discrepancies and\nmeasure human-AI collaboration at word and sentence levels while providing\neducators with a transparent tool to support academic integrity investigations.\nThis work advances AV technology, offering actionable insights into the\ndynamics of academic writing in an AI-driven era."}
{"id": "2505.09054", "pdf": "https://arxiv.org/pdf/2505.09054.pdf", "abs": "https://arxiv.org/abs/2505.09054", "title": "EcoSphere: A Decision-Support Tool for Automated Carbon Emission and Cost Optimization in Sustainable Urban Development", "authors": ["Siavash Ghorbany", "Ming Hu", "Siyuan Yao", "Matthew Sisk", "Chaoli Wang"], "categories": ["cs.HC", "cs.CY", "stat.AP"], "comment": "Proc of the 23rd CIB World Building Congress, 19th to 23rd May 2025,\n  Purdue University, West Lafayette, USA", "summary": "The construction industry is a major contributor to global greenhouse gas\nemissions, with embodied carbon being a key component. This study develops\nEcoSphere, an innovative software designed to evaluate and balance embodied and\noperational carbon emissions with construction and environmental costs in urban\nplanning. Using high-resolution data from the National Structure Inventory,\ncombined with computer vision and natural language processing applied to Google\nStreet View and satellite imagery, EcoSphere categorizes buildings by\nstructural and material characteristics with a bottom-up approach, creating a\nbaseline emissions dataset. By simulating policy scenarios and mitigation\nstrategies, EcoSphere provides policymakers and non-experts with actionable\ninsights for sustainable development in cities and provide them with a vision\nof the environmental and financial results of their decisions. Case studies in\nChicago and Indianapolis showcase how EcoSphere aids in assessing policy\nimpacts on carbon emissions and costs, supporting data-driven progress toward\ncarbon neutrality."}
{"id": "2505.08891", "pdf": "https://arxiv.org/pdf/2505.08891.pdf", "abs": "https://arxiv.org/abs/2505.08891", "title": "Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives", "authors": ["Daeun Hwang", "Samuel Shields", "Alex Calderwood", "Shi Johnson-Bey", "Michael Mateas", "Noah Wardrip-Fruin", "Edward F. Melcer"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 1 table, 1 appendix. Workshop paper, CHI 2025\n  Augmented Educators and AI", "summary": "Motivation is an important factor underlying successful learning. Previous\nresearch has demonstrated the positive effects that static interactive\nnarrative games can have on motivation. Concurrently, advances in AI have made\ndynamic and adaptive approaches to interactive narrative increasingly\naccessible. However, limited work has explored the impact that dynamic\nnarratives can have on learner motivation. In this paper, we compare two\nversions of Academical, a choice-based educational interactive narrative game\nabout research ethics. One version employs a traditional hand-authored\nbranching plot (i.e., static narrative) while the other dynamically sequences\nplots during play (i.e., dynamic narrative). Results highlight the importance\nof responsive content and a variety of choices for player engagement, while\nalso illustrating the challenge of balancing pedagogical goals with the dynamic\naspects of narrative. We also discuss design implications that arise from these\nfindings. Ultimately, this work provides initial steps to illuminate the\nemerging potential of AI-driven dynamic narrative in educational games."}
{"id": "2505.09065", "pdf": "https://arxiv.org/pdf/2505.09065.pdf", "abs": "https://arxiv.org/abs/2505.09065", "title": "Display Content, Display Methods and Evaluation Methods of the HCI in Explainable Recommender Systems: A Survey", "authors": ["Weiqing Li", "Yue Xu", "Yuefeng Li", "Yinghui Huang"], "categories": ["cs.HC", "cs.IR"], "comment": "2 Tables, 29 figures", "summary": "Explainable Recommender Systems (XRS) aim to provide users with\nunderstandable reasons for the recommendations generated by these systems,\nrepresenting a crucial research direction in artificial intelligence (AI).\nRecent research has increasingly focused on the algorithms, display, and\nevaluation methodologies of XRS. While current research and reviews primarily\nemphasize the algorithmic aspects, with fewer studies addressing the\nHuman-Computer Interaction (HCI) layer of XRS. Additionally, existing reviews\nlack a unified taxonomy for XRS and there is insufficient attention given to\nthe emerging area of short video recommendations. In this study, we synthesize\nexisting literature and surveys on XRS, presenting a unified framework for its\nresearch and development. The main contributions are as follows: 1) We adopt a\nlifecycle perspective to systematically summarize the technologies and methods\nused in XRS, addressing challenges posed by the diversity and complexity of\nalgorithmic models and explanation techniques. 2) For the first time, we\nhighlight the application of multimedia, particularly video-based explanations,\nalong with its potential, technical pathways, and challenges in XRS. 3) We\nprovide a structured overview of evaluation methods from both qualitative and\nquantitative dimensions. These findings provide valuable insights for the\nsystematic design, progress, and testing of XRS."}
{"id": "2505.08996", "pdf": "https://arxiv.org/pdf/2505.08996.pdf", "abs": "https://arxiv.org/abs/2505.08996", "title": "A suite of LMs comprehend puzzle statements as well as humans", "authors": ["Adele E Goldberg", "Supantho Rakshit", "Jennifer Hu", "Kyle Mahowald"], "categories": ["cs.CL"], "comment": null, "summary": "Recent claims suggest that large language models (LMs) underperform humans in\ncomprehending minimally complex English statements (Dentella et al., 2024).\nHere, we revisit those findings and argue that human performance was\noverestimated, while LLM abilities were underestimated. Using the same stimuli,\nwe report a preregistered study comparing human responses in two conditions:\none allowed rereading (replicating the original study), and one that restricted\nrereading (a more naturalistic comprehension test). Human accuracy dropped\nsignificantly when rereading was restricted (73%), falling below that of\nFalcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect\naccuracy. Results further show that both humans and models are\ndisproportionately challenged by queries involving potentially reciprocal\nactions (e.g., kissing), suggesting shared pragmatic sensitivities rather than\nmodel-specific deficits. Additional analyses using Llama-2-70B log\nprobabilities, a recoding of open-ended model responses, and grammaticality\nratings of other sentences reveal systematic underestimation of model\nperformance. We find that GPT-4o can align with either naive or expert\ngrammaticality judgments, depending on prompt framing. These findings\nunderscore the need for more careful experimental design and coding practices\nin LLM evaluation, and they challenge the assumption that current models are\ninherently weaker than humans at language comprehension."}
{"id": "2505.09094", "pdf": "https://arxiv.org/pdf/2505.09094.pdf", "abs": "https://arxiv.org/abs/2505.09094", "title": "PLanet: Formalizing Experimental Design", "authors": ["London Bielicke", "Anna Zhang", "Shruti Tyagi", "Emery Berger", "Adam Chlipala", "Eunice Jun"], "categories": ["cs.HC"], "comment": "14 pages, 4 tables, 6 figures, human-computer interaction, domain\n  specific language, experimental design", "summary": "Carefully constructed experimental designs are essential for drawing valid,\ngeneralizable conclusions from scientific studies. Unfortunately, experimental\ndesign plans can be difficult to specify, communicate clearly, and relate to\nalternatives. In response, we introduce a grammar of experimental design that\nprovides composable operators for constructing assignment procedures (e.g.,\nLatin square). We implement this grammar in PLanet, a domain-specific language\n(DSL) that constructs assignment plans in three stages: experimental unit\nspecification, trial-order construction, and order-to-unit mapping. We evaluate\nPLanet's expressivity by taking a purposive sample of recent CHI and UIST\npublications, representing their experiments as programs in PLanet, and\nidentifying ambiguities and alternatives. In our evaluation, PLanet could\nexpress 11 out of 12 experiments found in sampled papers. Additionally, we\nfound that PLanet constructs helped make complex design choices explicit when\nthe researchers omit technical language describing their study designs."}
{"id": "2505.09005", "pdf": "https://arxiv.org/pdf/2505.09005.pdf", "abs": "https://arxiv.org/abs/2505.09005", "title": "For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies", "authors": ["Nicole Cuneo", "Eleanor Graves", "Supantho Rakshit", "Adele E. Goldberg"], "categories": ["cs.CL"], "comment": null, "summary": "It remains debated how well any LM understands natural language or generates\nreliable metalinguistic judgments. Moreover, relatively little work has\ndemonstrated that LMs can represent and respect subtle relationships between\nform and function proposed by linguists. We here focus on a particular such\nrelationship established in recent work: English speakers' judgments about the\ninformation structure of canonical sentences predicts independently collected\nacceptability ratings on corresponding 'long distance dependency' [LDD]\nconstructions, across a wide array of base constructions and multiple types of\nLDDs. To determine whether any LM captures this relationship, we probe GPT-4 on\nthe same tasks used with humans and new extensions.Results reveal reliable\nmetalinguistic skill on the information structure and acceptability tasks,\nreplicating a striking interaction between the two, despite the zero-shot,\nexplicit nature of the tasks, and little to no chance of contamination [Studies\n1a, 1b]. Study 2 manipulates the information structure of base sentences and\nconfirms a causal relationship: increasing the prominence of a constituent in a\ncontext sentence increases the subsequent acceptability ratings on an LDD\nconstruction. The findings suggest a tight relationship between natural and\nGPT-4 generated English, and between information structure and syntax, which\nbegs for further exploration."}
{"id": "2505.09115", "pdf": "https://arxiv.org/pdf/2505.09115.pdf", "abs": "https://arxiv.org/abs/2505.09115", "title": "PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence", "authors": ["Yu Lun Hsu", "Yun-Rung Chou", "Chiao-Ju Chang", "Yu-Cheng Chang", "Zer-Wei Lee", "Rokas Gipiškis", "Rachel Li", "Chih-Yuan Shih", "Jen-Kuei Peng", "Hsien-Liang Huang", "Jaw-Shiun Tsai", "Mike Y. Chen"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Advance Care Planning (ACP) allows individuals to specify their preferred\nend-of-life life-sustaining treatments before they become incapacitated by\ninjury or terminal illness (e.g., coma, cancer, dementia). While online ACP\noffers high accessibility, it lacks key benefits of clinical consultations,\nincluding personalized value exploration, immediate clarification of decision\nconsequences. To bridge this gap, we conducted two formative studies: 1)\nshadowed and interviewed 3 ACP teams consisting of physicians, nurses, and\nsocial workers (18 patients total), and 2) interviewed 14 users of ACP\nwebsites. Building on these insights, we designed PreCare in collaboration with\n6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed\nto guide users through exploring personal values, gaining ACP knowledge, and\nsupporting informed decision-making. A usability study (n=12) showed that\nPreCare achieved a System Usability Scale (SUS) rating of excellent. A\ncomparative evaluation (n=12) showed that PreCare's AI assistants significantly\nimproved exploration of personal values, knowledge, and decisional confidence,\nand was preferred by 92% of participants."}
{"id": "2505.09039", "pdf": "https://arxiv.org/pdf/2505.09039.pdf", "abs": "https://arxiv.org/abs/2505.09039", "title": "Atomic Consistency Preference Optimization for Long-Form Question Answering", "authors": ["Jingfeng Chen", "Raghuveer Thirukovalluru", "Junlin Wang", "Kaiwei Luo", "Bhuwan Dhingra"], "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Large Language Models (LLMs) frequently produce factoid hallucinations -\nplausible yet incorrect answers. A common mitigation strategy is model\nalignment, which improves factual accuracy by training on curated factual and\nnon-factual pairs. However, this approach often relies on a stronger model\n(e.g., GPT-4) or an external knowledge base to assess factual correctness,\nwhich may not always be accessible. To address this, we propose Atomic\nConsistency Preference Optimization (ACPO), a self-supervised preference-tuning\nmethod that enhances factual accuracy without external supervision. ACPO\nleverages atomic consistency signals, i.e., the agreement of individual facts\nacross multiple stochastic responses, to identify high- and low-quality data\npairs for model alignment. By eliminating the need for costly GPT calls, ACPO\nprovides a scalable and efficient approach to improving factoid\nquestion-answering. Despite being self-supervised, empirical results\ndemonstrate that ACPO outperforms FactAlign, a strong supervised alignment\nbaseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its\neffectiveness in enhancing factual reliability without relying on external\nmodels or knowledge bases."}
{"id": "2505.09166", "pdf": "https://arxiv.org/pdf/2505.09166.pdf", "abs": "https://arxiv.org/abs/2505.09166", "title": "An Initial Exploration of Default Images in Text-to-Image Generation", "authors": ["Hannu Simonen", "Atte Kiviniemi", "Jonas Oppenlaender"], "categories": ["cs.HC", "cs.AI", "H.5.m; I.2.m"], "comment": "16 pages, 6 figures", "summary": "In the creative practice of text-to-image generation (TTI), images are\ngenerated from text prompts. However, TTI models are trained to always yield an\noutput, even if the prompt contains unknown terms. In this case, the model may\ngenerate what we call \"default images\": images that closely resemble each other\nacross many unrelated prompts. We argue studying default images is valuable for\ndesigning better solutions for TTI and prompt engineering. In this paper, we\nprovide the first investigation into default images on Midjourney, a popular\nimage generator. We describe our systematic approach to create input prompts\ntriggering default images, and present the results of our initial experiments\nand several small-scale ablation studies. We also report on a survey study\ninvestigating how default images affect user satisfaction. Our work lays the\nfoundation for understanding default images in TTI and highlights challenges\nand future research directions."}
{"id": "2505.09056", "pdf": "https://arxiv.org/pdf/2505.09056.pdf", "abs": "https://arxiv.org/abs/2505.09056", "title": "A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias", "authors": ["Brandon Smith", "Mohamed Reda Bouadjenek", "Tahsin Alamgir Kheya", "Phillip Dawson", "Sunil Aryal"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) represent a major step toward artificial general\nintelligence, significantly advancing our ability to interact with technology.\nWhile LLMs perform well on Natural Language Processing tasks -- such as\ntranslation, generation, code writing, and summarization -- questions remain\nabout their output similarity, variability, and ethical implications. For\ninstance, how similar are texts generated by the same model? How does this\ncompare across different models? And which models best uphold ethical\nstandards? To investigate, we used 5{,}000 prompts spanning diverse tasks like\ngeneration, explanation, and rewriting. This resulted in approximately 3\nmillion texts from 12 LLMs, including proprietary and open-source systems from\nOpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs\nfrom the same LLM are more similar to each other than to human-written texts;\n(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4\nproduces more varied responses; (3) LLM writing styles differ significantly,\nwith Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for\ndistinctiveness; (4) differences in vocabulary and tone underscore the\nlinguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate\ngreater gender balance and reduced bias. These results offer new insights into\nthe behavior and diversity of LLM outputs, helping guide future development and\nethical evaluation."}
{"id": "2505.09208", "pdf": "https://arxiv.org/pdf/2505.09208.pdf", "abs": "https://arxiv.org/abs/2505.09208", "title": "Educational impacts of generative artificial intelligence on learning and performance of engineering students in China", "authors": ["Lei Fan", "Kunyang Deng", "Fangxue Liu"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "With the rapid advancement of generative artificial intelligence(AI), its\npotential applications in higher education have attracted significant\nattention. This study investigated how 148 students from diverse engineering\ndisciplines and regions across China used generative AI, focusing on its impact\non their learning experience and the opportunities and challenges it poses in\nengineering education. Based on the surveyed data, we explored four key areas:\nthe frequency and application scenarios of AI use among engineering students,\nits impact on students' learning and performance, commonly encountered\nchallenges in using generative AI, and future prospects for its adoption in\nengineering education. The results showed that more than half of the\nparticipants reported a positive impact of generative AI on their learning\nefficiency, initiative, and creativity, with nearly half believing it also\nenhanced their independent thinking. However, despite acknowledging improved\nstudy efficiency, many felt their actual academic performance remained largely\nunchanged and expressed concerns about the accuracy and domain-specific\nreliability of generative AI. Our findings provide a first-hand insight into\nthe current benefits and challenges generative AI brings to students,\nparticularly Chinese engineering students, while offering several\nrecommendations, especially from the students' perspective, for effectively\nintegrating generative AI into engineering education."}
{"id": "2505.09068", "pdf": "https://arxiv.org/pdf/2505.09068.pdf", "abs": "https://arxiv.org/abs/2505.09068", "title": "S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment", "authors": ["Jennifer Haase", "Paul H. P. Hanel", "Sebastian Pokutta"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper introduces S-DAT (Synthetic-Divergent Association Task), a\nscalable, multilingual framework for automated assessment of divergent thinking\n(DT) -a core component of human creativity. Traditional creativity assessments\nare often labor-intensive, language-specific, and reliant on subjective human\nratings, limiting their scalability and cross-cultural applicability. In\ncontrast, S-DAT leverages large language models and advanced multilingual\nembeddings to compute semantic distance -- a language-agnostic proxy for DT. We\nevaluate S-DAT across eleven diverse languages, including English, Spanish,\nGerman, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating\nrobust and consistent scoring across linguistic contexts. Unlike prior DAT\napproaches, the S-DAT shows convergent validity with other DT measures and\ncorrect discriminant validity with convergent thinking. This cross-linguistic\nflexibility allows for more inclusive, global-scale creativity research,\naddressing key limitations of earlier approaches. S-DAT provides a powerful\ntool for fairer, more comprehensive evaluation of cognitive flexibility in\ndiverse populations and can be freely assessed online:\nhttps://sdat.iol.zib.de/."}
{"id": "2505.09283", "pdf": "https://arxiv.org/pdf/2505.09283.pdf", "abs": "https://arxiv.org/abs/2505.09283", "title": "A Note on Semantic Diffusion", "authors": ["Alexander P. Ryjov", "Alina A. Egorova"], "categories": ["cs.HC", "03E72, 68T37, 94D05", "I.2.8; I.2.10"], "comment": "8 figures", "summary": "This paper provides an in-depth examination of the concept of semantic\ndiffusion as a complementary instrument to large language models (LLMs) for\ndesign applications. Conventional LLMs and diffusion models fail to induce a\nconvergent, iterative refinement process: each invocation of the diffusion\nmechanism spawns a new stochastic cycle, so successive outputs do not relate to\nprior ones and convergence toward a desired design is not guaranteed. The\nproposed hybrid framework - \"LLM + semantic diffusion\" - resolves this\nlimitation by enforcing an approximately convergent search procedure, thereby\nformally addressing the problem of localized design refinement."}
{"id": "2505.09082", "pdf": "https://arxiv.org/pdf/2505.09082.pdf", "abs": "https://arxiv.org/abs/2505.09082", "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM", "authors": ["Sophie Zhang", "Zhiming Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models."}
{"id": "2505.09376", "pdf": "https://arxiv.org/pdf/2505.09376.pdf", "abs": "https://arxiv.org/abs/2505.09376", "title": "AfforDance: Personalized AR Dance Learning System with Visual Affordance", "authors": ["Hyunyoung Han", "Jongwon Jang", "Kitaeg Shim", "Sang Ho Yoon"], "categories": ["cs.HC"], "comment": "CHI 2025 Workshop on Beyond Glasses: Future Directions for XR\n  Interactions within the Physical World", "summary": "We propose AfforDance, an augmented reality (AR)-based dance learning system\nthat generates personalized learning content and enhances learning through\nvisual affordances. Our system converts user-selected dance videos into\ninteractive learning experiences by integrating 3D reference avatars, audio\nsynchronization, and adaptive visual cues that guide movement execution. This\nwork contributes to personalized dance education by offering an adaptable,\nuser-centered learning interface."}
{"id": "2505.09269", "pdf": "https://arxiv.org/pdf/2505.09269.pdf", "abs": "https://arxiv.org/abs/2505.09269", "title": "How an unintended Side Effect of a Research Project led to Boosting the Power of UML", "authors": ["Ulrich Frank", "Pierre Maier"], "categories": ["cs.CL"], "comment": null, "summary": "This paper describes the design, implementation and use of a new UML modeling\ntool that represents a significant advance over conventional tools. Among other\nthings, it allows the integration of class diagrams and object diagrams as well\nas the execution of objects. This not only enables new software architectures\ncharacterized by the integration of software with corresponding object models,\nbut is also ideal for use in teaching, as it provides students with a\nparticularly stimulating learning experience. A special feature of the project\nis that it has emerged from a long-standing international research project,\nwhich is aimed at a comprehensive multi-level architecture. The project is\ntherefore an example of how research can lead to valuable results that arise as\na side effect of other work."}
{"id": "2505.09402", "pdf": "https://arxiv.org/pdf/2505.09402.pdf", "abs": "https://arxiv.org/abs/2505.09402", "title": "Utilization of Skin Color Change for Image-based Tactile Sensing", "authors": ["Seitaro Kaneko", "Hiroki Ishizuka", "Hidenori Yoshimura", "Hiroyuki Kajimoto"], "categories": ["cs.HC"], "comment": "This is the accepted version of the following article:Medical\n  Engineering & Physics, which has been published in final form at\n  https://doi.org/10.1016/j.medengphy.2025.104357", "summary": "Measurement of pressure distribution applied to a fingertip is crucial for\nthe teleoperation of robots and human computer interface. Previous studies have\nacquired pressure distribution by affixing a sensor array to the fingertip or\nby optically recording the deformation of an object. However, these existing\nmethods inhibit the fingertip from directly contacting the texture, and the\npressure applied to the fingertip is measured indirectly. In this study, we\npropose a method to measure pressure distribution by directly touching a\ntransparent object, focusing on the change in skin color induced by the applied\npressure, caused by blood flow. We evaluated the relationship between pressure\nand skin color change when local pressure is applied, and found a correlation\nbetween the pressure and the color change. However, the contact area and the\ncolor change area did not align perfectly. We further explored the factor\ncausing the spatial non-uniformity of the color change, by accounting for the\nstress distribution using finite element analysis. These results suggest that\nthe proposed measurement method can be utilized to measure the internal stress\ndistribution, and it is anticipated to serve as a simple sensor in the field of\nhuman computer interface."}
{"id": "2505.09286", "pdf": "https://arxiv.org/pdf/2505.09286.pdf", "abs": "https://arxiv.org/abs/2505.09286", "title": "A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data", "authors": ["Jiin Park", "Misuk Kim"], "categories": ["cs.CL"], "comment": "36 pages, 3 figures", "summary": "Effectively analyzing online review data is essential across industries.\nHowever, many existing studies are limited to specific domains and languages or\ndepend on supervised learning approaches that require large-scale labeled\ndatasets. To address these limitations, we propose a multilingual, scalable,\nand unsupervised framework for cross-domain aspect detection. This framework is\ndesigned for multi-aspect labeling of multilingual and multi-domain review\ndata. In this study, we apply automatic labeling to Korean and English review\ndatasets spanning various domains and assess the quality of the generated\nlabels through extensive experiments. Aspect category candidates are first\nextracted through clustering, and each review is then represented as an\naspect-aware embedding vector using negative sampling. To evaluate the\nframework, we conduct multi-aspect labeling and fine-tune several pretrained\nlanguage models to measure the effectiveness of the automatically generated\nlabels. Results show that these models achieve high performance, demonstrating\nthat the labels are suitable for training. Furthermore, comparisons with\npublicly available large language models highlight the framework's superior\nconsistency and scalability when processing large-scale data. A human\nevaluation also confirms that the quality of the automatic labels is comparable\nto those created manually. This study demonstrates the potential of a robust\nmulti-aspect labeling approach that overcomes limitations of supervised methods\nand is adaptable to multilingual, multi-domain environments. Future research\nwill explore automatic review summarization and the integration of artificial\nintelligence agents to further improve the efficiency and depth of review\nanalysis."}
{"id": "2505.09478", "pdf": "https://arxiv.org/pdf/2505.09478.pdf", "abs": "https://arxiv.org/abs/2505.09478", "title": "Card Sorting Simulator: Augmenting Design of Logical Information Architectures with Large Language Models", "authors": ["Eduard Kuric", "Peter Demcak", "Matus Krajcovic"], "categories": ["cs.HC"], "comment": null, "summary": "Card sorting is a common ideation technique that elicits information on\nusers' mental organization of content and functionality by having them sort\nitems into categories. For more robust card sorting research, digital card\nsorting tools could benefit from providing quick automated feedback. Our\nobjective of this research is to advance toward an instrument that applies\nartificial intelligence (AI) to augment card sorting. For this purpose, we\ndevelop the Card Sorting Simulator, a prototype tool that leverages Large\nLanguage Models (LLMs) to generate informative categorizations of cards. To\nilluminate how aligned the simulation is with card sorting by actual\nparticipants, and to inform the instrument's design decisions, we conducted a\ngeneralizability-focused comparative study. We obtained 28 pre-existing card\nsorting studies from real practitioners, comprising 1,399 participants, along\nwith diverse contents and origins. With this dataset, we conducted a\ncomprehensive and nuanced analysis of the agreement between actual card sorting\nresults (clusterings of cards) and synthetic clusterings across a multitude of\nLLMs and prompt designs. Mutual information scores indicate a good degree of\nagreement to real result clustering, although similarity matrices also\ndemonstrate inconsistencies from mental models, which can be attributed to\ntheir top-down nature. Furthermore, the number of cards or complexity of their\nlabels impact the accuracy of its simulation. These findings bolster the case\nfor AI augmentation in card sorting research as a source of meaningful\npreliminary feedback and highlight the need for further study for the\ndevelopment and validation of intelligent user research tools."}
{"id": "2505.09316", "pdf": "https://arxiv.org/pdf/2505.09316.pdf", "abs": "https://arxiv.org/abs/2505.09316", "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging", "authors": ["Hongjin Qian", "Zheng Liu"], "categories": ["cs.CL", "cs.IR"], "comment": "16 pages", "summary": "Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents."}
{"id": "2505.09509", "pdf": "https://arxiv.org/pdf/2505.09509.pdf", "abs": "https://arxiv.org/abs/2505.09509", "title": "Partnership through Play: Investigating How Long-Distance Couples Use Digital Games to Facilitate Intimacy", "authors": ["Nisha Devasia", "Adrian Rodriguez", "Logan Tuttle", "Julie Kientz"], "categories": ["cs.HC"], "comment": "Proceedings of Designing Interactive Systems (DIS '25)", "summary": "Long-distance relationships (LDRs) have become more common in the last few\ndecades, primarily among young adults pursuing educational or employment\nopportunities. A common way for couples in LDRs to spend time together is by\nplaying multiplayer video games, which are often a shared hobby and therefore a\npreferred joint activity. However, games are relatively understudied in the\ncontext of relational maintenance for LDRs. In this work, we used a\nmixed-methods approach to collect data on the experiences of 13 couples in LDRs\nwho frequently play games together. We investigated different values around\nvarious game mechanics and modalities and found significant differences in\ncouple play styles, and also detail how couples appropriate game mechanics to\nexpress affection to each other virtually. We also created prototypes and\ndesign implications based on couples' needs surrounding the lack of physical\nsensation and memorabilia storage in most popular games."}
{"id": "2505.09338", "pdf": "https://arxiv.org/pdf/2505.09338.pdf", "abs": "https://arxiv.org/abs/2505.09338", "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs", "authors": ["Jingcheng Niu", "Xingdi Yuan", "Tong Wang", "Hamidreza Saghir", "Amir H. Abdi"], "categories": ["cs.CL"], "comment": null, "summary": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem."}
{"id": "2505.09526", "pdf": "https://arxiv.org/pdf/2505.09526.pdf", "abs": "https://arxiv.org/abs/2505.09526", "title": "Evaluation Metrics for Misinformation Warning Interventions: Challenges and Prospects", "authors": ["Hussaini Zubairu", "Abdelrahaman Abdou", "Ashraf Matrawy"], "categories": ["cs.HC"], "comment": "10 pages, 2 figures", "summary": "Misinformation has become a widespread issue in the 21st century, impacting\nnumerous areas of society and underscoring the need for effective intervention\nstrategies. Among these strategies, user-centered interventions, such as\nwarning systems, have shown promise in reducing the spread of misinformation.\nMany studies have used various metrics to evaluate the effectiveness of these\nwarning interventions. However, no systematic review has thoroughly examined\nthese metrics in all studies. This paper provides a comprehensive review of\nexisting metrics for assessing the effectiveness of misinformation warnings,\ncategorizing them into four main groups: behavioral impact, trust and\ncredulity, usability, and cognitive and psychological effects. Through this\nreview, we identify critical challenges in measuring the effectiveness of\nmisinformation warnings, including inconsistent use of cognitive and\nattitudinal metrics, the lack of standardized metrics for affective and\nemotional impact, variations in user trust, and the need for more inclusive\nwarning designs. We present an overview of these metrics and propose areas for\nfuture research."}
{"id": "2505.09388", "pdf": "https://arxiv.org/pdf/2505.09388.pdf", "abs": "https://arxiv.org/abs/2505.09388", "title": "Qwen3 Technical Report", "authors": ["An Yang", "Anfeng Li", "Baosong Yang", "Beichen Zhang", "Binyuan Hui", "Bo Zheng", "Bowen Yu", "Chang Gao", "Chengen Huang", "Chenxu Lv", "Chujie Zheng", "Dayiheng Liu", "Fan Zhou", "Fei Huang", "Feng Hu", "Hao Ge", "Haoran Wei", "Huan Lin", "Jialong Tang", "Jian Yang", "Jianhong Tu", "Jianwei Zhang", "Jianxin Yang", "Jiaxi Yang", "Jing Zhou", "Jingren Zhou", "Junyang Lin", "Kai Dang", "Keqin Bao", "Kexin Yang", "Le Yu", "Lianghao Deng", "Mei Li", "Mingfeng Xue", "Mingze Li", "Pei Zhang", "Peng Wang", "Qin Zhu", "Rui Men", "Ruize Gao", "Shixuan Liu", "Shuang Luo", "Tianhao Li", "Tianyi Tang", "Wenbiao Yin", "Xingzhang Ren", "Xinyu Wang", "Xinyu Zhang", "Xuancheng Ren", "Yang Fan", "Yang Su", "Yichang Zhang", "Yinger Zhang", "Yu Wan", "Yuqiong Liu", "Zekun Wang", "Zeyu Cui", "Zhenru Zhang", "Zhipeng Zhou", "Zihan Qiu"], "categories": ["cs.CL"], "comment": null, "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0."}
{"id": "2505.09583", "pdf": "https://arxiv.org/pdf/2505.09583.pdf", "abs": "https://arxiv.org/abs/2505.09583", "title": "Beyond Likes: How Normative Feedback Complements Engagement Signals on Social Media", "authors": ["Yuchen Wu", "Mingduo Zhao", "John Canny"], "categories": ["cs.HC"], "comment": null, "summary": "Many online platforms incorporate engagement signals--such as likes and\nupvotes--into their content ranking systems and interface design. These signals\nare designed to boost user engagement. However, they can unintentionally\nelevate content that is less inclusive and may not support normatively\ndesirable behavior. This issue becomes especially concerning when toxic content\ncorrelates strongly with popularity indicators such as likes and upvotes. In\nthis study, we propose structured prosocial feedback as a complementary signal\nto likes and upvotes--one that highlights content quality based on normative\ncriteria to help address the limitations of conventional engagement signals. We\nbegin by designing and implementing a machine learning feedback system powered\nby a large language model (LLM), which evaluates user comments based on\nprinciples of positive psychology, such as individual well-being, constructive\nsocial media use, and character strengths. We then conduct a pre-registered\nuser study to examine how existing peer-based and the new expert-based feedback\ninteract to shape users' selection of comments in a social media setting.\nResults show that peer feedback increases conformity to popularity cues, while\nexpert feedback shifts preferences toward normatively higher-quality content.\nMoreover, incorporating expert feedback alongside peer evaluations improves\nalignment with expert assessments and contributes to a less toxic community\nenvironment. This illustrates the added value of normative cues--such as expert\nscores generated by LLMs using psychological rubrics--and underscores the\npotential benefits of incorporating such signals into platform feedback systems\nto foster healthier online environments."}
{"id": "2505.09407", "pdf": "https://arxiv.org/pdf/2505.09407.pdf", "abs": "https://arxiv.org/abs/2505.09407", "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits", "authors": ["Subrit Dikshit", "Ritu Tiwari", "Priyank Jain"], "categories": ["cs.CL", "cs.AI", "cs.ET"], "comment": "12 pages, 12 figures", "summary": "Cloud-based multilingual translation services like Google Translate and\nMicrosoft Translator achieve state-of-the-art translation capabilities. These\nservices inherently use large multilingual language models such as GRU, LSTM,\nBERT, GPT, T5, or similar encoder-decoder architectures with attention\nmechanisms as the backbone. Also, new age natural language systems, for\ninstance ChatGPT and DeepSeek, have established huge potential in multiple\ntasks in natural language processing. At the same time, they also possess\noutstanding multilingual translation capabilities. However, these models use\nthe classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder\nAttention-based Convolutional Variational Circuits) is an alternate solution\nthat explores the quantum computing realm instead of the classical computing\nrealm to study and demonstrate multilingual machine translation. QEDACVC\nintroduces the quantum encoder-decoder architecture that simulates and runs on\nquantum computing hardware via quantum convolution, quantum pooling, quantum\nvariational circuit, and quantum attention as software alterations. QEDACVC\nachieves an Accuracy of 82% when trained on the OPUS dataset for English,\nFrench, German, and Hindi corpora for multilingual translations."}
{"id": "2505.08904", "pdf": "https://arxiv.org/pdf/2505.08904.pdf", "abs": "https://arxiv.org/abs/2505.08904", "title": "FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations", "authors": ["Varun Nagaraj Rao", "Samantha Dalal", "Andrew Schwartz", "Amna Liaqat", "Dana Calacci", "Andrés Monroy-Hernández"], "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "What happens when a rideshare driver is suddenly locked out of the platform\nconnecting them to riders, wages, and daily work? Deactivation-the abrupt\nremoval of gig workers' platform access-typically occurs through arbitrary AI\nand algorithmic decisions with little explanation or recourse. This represents\none of the most severe forms of algorithmic control and often devastates\nworkers' financial stability. Recent U.S. state policies now mandate appeals\nprocesses and recovering compensation during the period of wrongful\ndeactivation based on past earnings. Yet, labor organizers still lack effective\ntools to support these complex, error-prone workflows. We designed FareShare, a\ncomputational tool automating lost wage estimation for deactivated drivers,\nthrough a 6 month partnership with the State of Washington's largest rideshare\nlabor union. Over the following 3 months, our field deployment of FareShare\nregistered 178 account signups. We observed that the tool could reduce lost\nwage calculation time by over 95%, eliminate manual data entry errors, and\nenable legal teams to generate arbitration-ready reports more efficiently.\nBeyond these gains, the deployment also surfaced important socio-technical\nchallenges around trust, consent, and tool adoption in high-stakes labor\ncontexts."}
{"id": "2505.09519", "pdf": "https://arxiv.org/pdf/2505.09519.pdf", "abs": "https://arxiv.org/abs/2505.09519", "title": "PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning", "authors": ["Zongqian Li", "Yixuan Su", "Nigel Collier"], "categories": ["cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting\nlarge language models, yet existing approaches exhibit counter-intuitive\nphenomena: integrating router into prompt tuning (PT) increases training\nefficiency yet does not improve performance universally; parameter reduction\nthrough matrix decomposition can improve performance in specific domains.\nMotivated by these observations and the modular nature of PT, we propose\nPT-MoE, a novel framework that integrates matrix decomposition with\nmixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets\ndemonstrate that PT-MoE achieves state-of-the-art performance in both question\nanswering (QA) and mathematical problem solving tasks, improving F1 score by\n1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing\nmathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all\nwhile using 25% fewer parameters than LoRA. Our analysis reveals that while PT\nmethods generally excel in QA tasks and LoRA-based methods in math datasets,\nthe integration of matrix decomposition and MoE in PT-MoE yields complementary\nbenefits: decomposition enables efficient parameter sharing across experts\nwhile MoE provides dynamic adaptation, collectively enabling PT-MoE to\ndemonstrate cross-task consistency and generalization abilities. These\nfindings, along with ablation studies on routing mechanisms and architectural\ncomponents, provide insights for future PEFT methods."}
{"id": "2505.09068", "pdf": "https://arxiv.org/pdf/2505.09068.pdf", "abs": "https://arxiv.org/abs/2505.09068", "title": "S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment", "authors": ["Jennifer Haase", "Paul H. P. Hanel", "Sebastian Pokutta"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper introduces S-DAT (Synthetic-Divergent Association Task), a\nscalable, multilingual framework for automated assessment of divergent thinking\n(DT) -a core component of human creativity. Traditional creativity assessments\nare often labor-intensive, language-specific, and reliant on subjective human\nratings, limiting their scalability and cross-cultural applicability. In\ncontrast, S-DAT leverages large language models and advanced multilingual\nembeddings to compute semantic distance -- a language-agnostic proxy for DT. We\nevaluate S-DAT across eleven diverse languages, including English, Spanish,\nGerman, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating\nrobust and consistent scoring across linguistic contexts. Unlike prior DAT\napproaches, the S-DAT shows convergent validity with other DT measures and\ncorrect discriminant validity with convergent thinking. This cross-linguistic\nflexibility allows for more inclusive, global-scale creativity research,\naddressing key limitations of earlier approaches. S-DAT provides a powerful\ntool for fairer, more comprehensive evaluation of cognitive flexibility in\ndiverse populations and can be freely assessed online:\nhttps://sdat.iol.zib.de/."}
{"id": "2505.09595", "pdf": "https://arxiv.org/pdf/2505.09595.pdf", "abs": "https://arxiv.org/abs/2505.09595", "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models", "authors": ["Abdullah Mushtaq", "Imran Taj", "Rafay Naeem", "Ibrahim Ghaznavi", "Junaid Qadir"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "comment": "Preprint. Submitted to the Journal of Artificial Intelligence\n  Research (JAIR) on April 29, 2025", "summary": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems."}
{"id": "2109.10132", "pdf": "https://arxiv.org/pdf/2109.10132.pdf", "abs": "https://arxiv.org/abs/2109.10132", "title": "Manifesto for Putting 'Chartjunk' in the Trash 2021!", "authors": ["Derya Akbaba", "Jack Wilburn", "Main T. Nance", "Miriah Meyer"], "categories": ["cs.HC"], "comment": "For the associated site, see https://jackwilb.github.io/chart-junk/", "summary": "In this provocation we ask the visualization research community to join us in\nremoving chartjunk from our research lexicon. We present an etymology of\nchartjunk, framing its provocative origins as misaligned, and harmful, to the\nways the term is currently used by visualization researchers. We call on the\ncommunity to dissolve chartjunk from the ways we talk about, write about, and\nthink about the graphical devices we design and study. As a step towards this\ngoal we contribute a performance of maintenance through a trio of acts: editing\nthe Wikipedia page on chartjunk, cutting out chartjunk from IEEE papers, and\nscanning and posting a repository of the pages with chartjunk removed to invite\nthe community to re-imagine how we describe visualizations. This contribution\nblurs the boundaries between research, activism, and maintenance art, and is\nintended to inspire the community to join us in taking out the trash."}
{"id": "2505.08795", "pdf": "https://arxiv.org/pdf/2505.08795.pdf", "abs": "https://arxiv.org/abs/2505.08795", "title": "The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures", "authors": ["Andres Anabalon", "Hugo Garces", "Julio Oliva", "Jose Cifuentes"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "7 pages, 3 figures", "summary": "We show that there is a fast algorithm that embeds hierarchical structures in\nthree-dimensional Minkowski spacetime. The correlation of data ends up purely\nencoded in the causal structure. Our model relies solely on oriented token\npairs -- local hierarchical signals -- with no access to global symbolic\nstructure. We apply our method to the corpus of \\textit{WordNet}. We provide a\nperfect embedding of the mammal sub-tree including ambiguities (more than one\nhierarchy per node) in such a way that the hierarchical structures get\ncompletely codified in the geometry and exactly reproduce the ground-truth. We\nextend this to a perfect embedding of the maximal unambiguous subset of the\n\\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We\nintroduce a novel retrieval mechanism in which causality, not distance, governs\nhierarchical access. Our results seem to indicate that all discrete data has a\nperfect geometrical representation that is three-dimensional. The resulting\nembeddings are nearly conformally invariant, indicating deep connections with\ngeneral relativity and field theory. These results suggest that concepts,\ncategories, and their interrelations, namely hierarchical meaning itself, is\ngeometric."}
{"id": "2505.03185", "pdf": "https://arxiv.org/pdf/2505.03185.pdf", "abs": "https://arxiv.org/abs/2505.03185", "title": "Behavioral Sensing and Intervention Paradigm: A Review of Closed-Loop Approaches for Ingestion Health", "authors": ["Jun Fang", "Yanuo Zhou", "Ka I Chan", "Jiajin Li", "Zeyi Sun", "Zhengnan Li", "Zicong Fu", "Hongjing Piao", "Haodong Xu", "Yuanchun Shi", "Yuntao Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Ingestive behavior plays a critical role in health, yet many existing\ninterventions remain limited to static guidance or manual self-tracking. With\nthe increasing integration of sensors and perceptual computing, recent systems\nhave begun to support closed-loop interventions that dynamically sense user\nbehavior and provide feedback during or around ingestion episodes. In this\nsurvey, we review 136 studies that leverage sensor-enabled or\ninteraction-mediated approaches to influence eating behavior. We propose a\nbehavioral closed-loop paradigm comprising three core components: target\nbehaviors, sensing modalities, and feedback strategies. A taxonomy of sensing\nand intervention modalities is presented, organized along human- and\nenvironment-based dimensions. Our analysis also examines evaluation methods and\ndesign trends across different modality-behavior pairings. This review reveals\nprevailing patterns and critical gaps, offering design insights for future\nadaptive and context-aware ingestion health interventions."}
{"id": "2505.08823", "pdf": "https://arxiv.org/pdf/2505.08823.pdf", "abs": "https://arxiv.org/abs/2505.08823", "title": "An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits", "authors": ["Cody Steinmetz", "Gavin Childress", "Aaron Herbst", "Gavin Jones", "Jasdeep Singh", "Eli Vang", "Keagan Weinstock"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have transformed natural-language processing,\nyet their scale makes real-world deployment costly. Post-training quantization\nreduces memory and computation but often degrades accuracy, while\nquantization-aware training can recover performance at the cost of extra\ntraining. Pushing quantization to the ternary (2-bit) regime yields even larger\nsavings but is notoriously unstable. Building on recent work showing that a\nbias-free, RMS-normalized Transformer with straight-through estimation can\nreach 1.58-bit precision, we demonstrate that simply inserting RMS\nnormalization before every linear projection and applying a gradual, layer-wise\nquantization schedule stably fine-tunes full-precision checkpoints into ternary\nLLMs. Our approach matches or surpasses more elaborate knowledge-distillation\npipelines on standard language-modeling benchmarks without adding model\ncomplexity. These results indicate that careful normalization alone can close\nmuch of the accuracy gap between ternary and full-precision LLMs, making\nultra-low-bit inference practical."}
{"id": "2505.04260", "pdf": "https://arxiv.org/pdf/2505.04260.pdf", "abs": "https://arxiv.org/abs/2505.04260", "title": "Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering", "authors": ["Jessica Y. Bo", "Tianyu Xu", "Ishan Chatterjee", "Katrina Passarella-Ward", "Achin Kulshrestha", "D Shin"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces."}
{"id": "2505.08842", "pdf": "https://arxiv.org/pdf/2505.08842.pdf", "abs": "https://arxiv.org/abs/2505.08842", "title": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries", "authors": ["Zekun Wu", "Seonglae Cho", "Umar Mohammed", "Cristian Munoz", "Kleyton Costa", "Xin Guan", "Theo King", "Ze Wang", "Emre Kazim", "Adriano Koshiyama"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Open-source AI libraries are foundational to modern AI systems but pose\nsignificant, underexamined risks across security, licensing, maintenance,\nsupply chain integrity, and regulatory compliance. We present LibVulnWatch, a\ngraph-based agentic assessment framework that performs deep, source-grounded\nevaluations of these libraries. Built on LangGraph, the system coordinates a\ndirected acyclic graph of specialized agents to extract, verify, and quantify\nrisk using evidence from trusted sources such as repositories, documentation,\nand vulnerability databases. LibVulnWatch generates reproducible,\ngovernance-aligned scores across five critical domains, publishing them to a\npublic leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely\nused libraries, including ML frameworks, LLM inference engines, and agent\norchestration tools, our system covers up to 88% of OpenSSF Scorecard checks\nwhile uncovering up to 19 additional risks per library. These include critical\nRemote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials\n(SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in\nregulatory documentation and auditability. By translating high-level governance\nprinciples into practical, verifiable metrics, LibVulnWatch advances technical\nAI governance with a scalable, transparent mechanism for continuous supply\nchain risk assessment and informed library selection."}
{"id": "2505.08048", "pdf": "https://arxiv.org/pdf/2505.08048.pdf", "abs": "https://arxiv.org/abs/2505.08048", "title": "Partisan Fact-Checkers' Warnings Can Effectively Correct Individuals' Misbeliefs About Political Misinformation", "authors": ["Sian Lee", "Haeseung Seo", "Aiping Xiong", "Dongwon Lee"], "categories": ["cs.HC"], "comment": "To appear in International AAAI Conference on Web and Social Media\n  (ICWSM) 2025", "summary": "Political misinformation, particularly harmful when it aligns with\nindividuals' preexisting beliefs and political ideologies, has become\nwidespread on social media platforms. In response, platforms like Facebook and\nX introduced warning messages leveraging fact-checking results from third-party\nfact-checkers to alert users against false content. However, concerns persist\nabout the effectiveness of these fact-checks, especially when fact-checkers are\nperceived as politically biased. To address these concerns, this study presents\nfindings from an online human-subject experiment (N=216) investigating how the\npolitical stances of fact-checkers influence their effectiveness in correcting\nmisbeliefs about political misinformation. Our findings demonstrate that\npartisan fact-checkers can decrease the perceived accuracy of political\nmisinformation and correct misbeliefs without triggering backfire effects. This\ncorrection is even more pronounced when the misinformation aligns with\nindividuals' political ideologies. Notably, while previous research suggests\nthat fact-checking warnings are less effective for conservatives than liberals,\nour results suggest that explicitly labeled partisan fact-checkers, positioned\nas political counterparts to conservatives, are particularly effective in\nreducing conservatives' misbeliefs toward pro-liberal misinformation."}
{"id": "2505.08902", "pdf": "https://arxiv.org/pdf/2505.08902.pdf", "abs": "https://arxiv.org/abs/2505.08902", "title": "Performance Gains of LLMs With Humans in a World of LLMs Versus Humans", "authors": ["Lucas McCullum", "Pelagie Ami Agassi", "Leo Anthony Celi", "Daniel K. Ebner", "Chrystinne Oliveira Fernandes", "Rachel S. Hicklen", "Mkliwa Koumbia", "Lisa Soleymani Lehmann", "David Restrepo"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Currently, a considerable research effort is devoted to comparing LLMs to a\ngroup of human experts, where the term \"expert\" is often ill-defined or\nvariable, at best, in a state of constantly updating LLM releases. Without\nproper safeguards in place, LLMs will threaten to cause harm to the established\nstructure of safe delivery of patient care which has been carefully developed\nthroughout history to keep the safety of the patient at the forefront. A key\ndriver of LLM innovation is founded on community research efforts which, if\ncontinuing to operate under \"humans versus LLMs\" principles, will expedite this\ntrend. Therefore, research efforts moving forward must focus on effectively\ncharacterizing the safe use of LLMs in clinical settings that persist across\nthe rapid development of novel LLM models. In this communication, we\ndemonstrate that rather than comparing LLMs to humans, there is a need to\ndevelop strategies enabling efficient work of humans with LLMs in an almost\nsymbiotic manner."}
{"id": "2311.00810", "pdf": "https://arxiv.org/pdf/2311.00810.pdf", "abs": "https://arxiv.org/abs/2311.00810", "title": "A Call to Arms: AI Should be Critical for Social Media Analysis of Conflict Zones", "authors": ["Afia Abedin", "Abdul Bais", "Cody Buntain", "Laura Courchesne", "Brian McQuinn", "Matthew E. Taylor", "Muhib Ullah"], "categories": ["cs.CY", "cs.CV", "cs.HC"], "comment": null, "summary": "The massive proliferation of social media data represents a transformative\nopportunity for conflict studies and for tracking the proliferation and use of\nweaponry, as conflicts are increasingly documented in these online spaces. At\nthe same time, the scale and types of data available are problematic for\ntraditional open-source intelligence. This paper focuses on identifying\nspecific weapon systems and the insignias of the armed groups using them as\ndocumented in the Ukraine war, as these tasks are critical to operational\nintelligence and tracking weapon proliferation, especially given the scale of\ninternational military aid given to Ukraine. The large scale of social media\nmakes manual assessment difficult, however, so this paper presents early work\nthat uses computer vision models to support this task. We demonstrate that\nthese models can both identify weapons embedded in images shared in social\nmedia and how the resulting collection of military-relevant images and their\npost times interact with the offline, real-world conflict. Not only can we then\ntrack changes in the prevalence of images of tanks, land mines, military\ntrucks, etc., we find correlations among time series data associated with these\nimages and the daily fatalities in this conflict. This work shows substantial\nopportunity for examining similar online documentation of conflict contexts,\nand we also point to future avenues where computer vision can be further\nimproved for these open-source intelligence tasks."}
{"id": "2505.08905", "pdf": "https://arxiv.org/pdf/2505.08905.pdf", "abs": "https://arxiv.org/abs/2505.08905", "title": "Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora", "authors": ["Michael Majurski", "Cynthia Matuszek"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Language Models (LMs) continue to advance, improving response quality and\ncoherence. Given Internet-scale training datasets, LMs have likely encountered\nmuch of what users might ask them to generate in some form during their\ntraining. A plethora of evaluation benchmarks have been constructed to assess\nmodel quality, response appropriateness, and reasoning capabilities. However,\nthe human effort required for benchmark construction is limited and being\nrapidly outpaced by the size and scope of the models under evaluation.\nAdditionally, having humans build a benchmark for every possible domain of\ninterest is impractical. Therefore, we propose a methodology for automating the\nconstruction of fact-based synthetic data model evaluations grounded in\ndocument populations. This work leverages those very same LMs to evaluate\ndomain-specific knowledge automatically, using only grounding documents (e.g.,\na textbook) as input. This synthetic data benchmarking approach corresponds\nwell with human curated questions with a Spearman ranking correlation of 0.96\nand a benchmark evaluation Pearson accuracy correlation of 0.79. This novel\ntool supports generating both multiple choice and open-ended synthetic data\nquestions to gain diagnostic insight of LM capability. We apply this\nmethodology to evaluate model performance on a recent relevant arXiv preprint,\ndiscovering a surprisingly strong performance from Gemma3 models."}
{"id": "2505.02004", "pdf": "https://arxiv.org/pdf/2505.02004.pdf", "abs": "https://arxiv.org/abs/2505.02004", "title": "Triple-identity Authentication: The Future of Secure Access", "authors": ["Suyun Borjigin"], "categories": ["cs.CR", "cs.ET", "cs.HC", "cs.SY", "eess.SY"], "comment": "10 pages, 2 figures,", "summary": "In a typical authentication process, the local system verifies the user's\nidentity using a stored hash value generated by a cross-system hash algorithm.\nThis article shifts the research focus from traditional password encryption to\nthe establishment of gatekeeping mechanisms for effective interactions between\na system and the outside world. Here, we propose a triple-identity\nauthentication system to achieve this goal. Specifically, this local system\nopens the inner structure of its hash algorithm to all user credentials,\nincluding the login name, login password, and authentication password. When a\nlogin credential is entered, the local system hashes it and then creates a\nunique identifier using intermediate hash elements randomly selected from the\nopen algorithm. Importantly, this locally generated unique identifier (rather\nthan the stored hash produced by the open algorithm) is utilized to verify the\nuser's combined identity, which is generated by combining the entered\ncredential with the International Mobile Equipment Identity and the\nInternational Mobile Subscriber Identity. The verification process is\nimplemented at each interaction point: the login name field, the login password\nfield, and the server's authentication point. Thus, within the context of this\ntriple-identity authentication system, we establish a robust gatekeeping\nmechanism for system interactions, ultimately providing a level of security\nthat is equivalent to multi-factor authentication."}
{"id": "2505.08910", "pdf": "https://arxiv.org/pdf/2505.08910.pdf", "abs": "https://arxiv.org/abs/2505.08910", "title": "Behind Maya: Building a Multilingual Vision Language Model", "authors": ["Nahid Alam", "Karthik Reddy Kanjula", "Surya Guthikonda", "Timothy Chung", "Bala Krishna S Vegesna", "Abhipsha Das", "Anthony Susevski", "Ryan Sze-Yin Chan", "S M Iftekhar Uddin", "Shayekh Bin Islam", "Roshan Santhosh", "Snegha A", "Drishti Sharma", "Chen Liu", "Isha Chaturvedi", "Genta Indra Winata", "Ashvanth. S", "Snehanshu Mukherjee", "Alham Fikri Aji"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at VLM4ALL CVPR 2025 Workshop", "summary": "In recent times, we have seen a rapid development of large Vision-Language\nModels (VLMs). They have shown impressive results on academic benchmarks,\nprimarily in widely spoken languages but lack performance on low-resource\nlanguages and varied cultural contexts. To address these limitations, we\nintroduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a\nmultilingual image-text pretraining dataset in eight languages, based on the\nLLaVA pretraining dataset; and 2) a multilingual image-text model supporting\nthese languages, enhancing cultural and linguistic comprehension in\nvision-language tasks. Code available at https://github.com/nahidalam/maya."}
{"id": "2505.08941", "pdf": "https://arxiv.org/pdf/2505.08941.pdf", "abs": "https://arxiv.org/abs/2505.08941", "title": "ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers", "authors": ["Gavin Hull", "Alex Bihlo"], "categories": ["cs.LG", "cs.CL"], "comment": "16 pages, 13 figures", "summary": "Predicting the future citation rates of academic papers is an important step\ntoward the automation of research evaluation and the acceleration of scientific\nprogress. We present $\\textbf{ForeCite}$, a simple but powerful framework to\nappend pre-trained causal language models with a linear head for average\nmonthly citation rate prediction. Adapting transformers for regression tasks,\nForeCite achieves a test correlation of $\\rho = 0.826$ on a curated dataset of\n900K+ biomedical papers published between 2000 and 2024, a 27-point improvement\nover the previous state-of-the-art. Comprehensive scaling-law analysis reveals\nconsistent gains across model sizes and data volumes, while temporal holdout\nexperiments confirm practical robustness. Gradient-based saliency heatmaps\nsuggest a potentially undue reliance on titles and abstract texts. These\nresults establish a new state-of-the-art in forecasting the long-term influence\nof academic research and lay the groundwork for the automated, high-fidelity\nevaluation of scientific contributions."}
{"id": "2505.08971", "pdf": "https://arxiv.org/pdf/2505.08971.pdf", "abs": "https://arxiv.org/abs/2505.08971", "title": "Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training", "authors": ["Yangyi Chen", "Hao Peng", "Tong Zhang", "Heng Ji"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "The code will be available at https://github.com/Yangyi-Chen/PRIOR", "summary": "In standard large vision-language models (LVLMs) pre-training, the model\ntypically maximizes the joint probability of the caption conditioned on the\nimage via next-token prediction (NTP); however, since only a small subset of\ncaption tokens directly relates to the visual content, this naive NTP\nunintentionally fits the model to noise and increases the risk of\nhallucination. We present PRIOR, a simple vision-language pre-training approach\nthat addresses this issue by prioritizing image-related tokens through\ndifferential weighting in the NTP loss, drawing from the importance sampling\nframework. PRIOR introduces a reference model-a text-only large language model\n(LLM) trained on the captions without image inputs, to weight each token based\non its probability for LVLMs training. Intuitively, tokens that are directly\nrelated to the visual inputs are harder to predict without the image and thus\nreceive lower probabilities from the text-only reference LLM. During training,\nwe implement a token-specific re-weighting term based on the importance scores\nto adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs\nwith visual encoders and LVLMs without visual encoders. We observe 19% and 8%\naverage relative improvement, respectively, on several vision-language\nbenchmarks compared to NTP. In addition, PRIOR exhibits superior scaling\nproperties, as demonstrated by significantly higher scaling coefficients,\nindicating greater potential for performance gains compared to NTP given\nincreasing compute and data."}
{"id": "2505.09024", "pdf": "https://arxiv.org/pdf/2505.09024.pdf", "abs": "https://arxiv.org/abs/2505.09024", "title": "Automated Meta Prompt Engineering for Alignment with the Theory of Mind", "authors": ["Aaron Baughman", "Rahul Agarwal", "Eduardo Morales", "Gozde Akay"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "9 pages, 6 figures, 3 tables", "summary": "We introduce a method of meta-prompting that jointly produces fluent text for\ncomplex tasks while optimizing the similarity of neural states between a\nhuman's mental expectation and a Large Language Model's (LLM) neural\nprocessing. A technique of agentic reinforcement learning is applied, in which\nan LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,\nhow to produce content by interpreting the intended and unintended generated\ntext traits. To measure human mental beliefs around content production, users\nmodify long form AI-generated text articles before publication at the US Open\n2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)\nalignment problem by anticipating and including human edits within the creation\nof text from an LLM. Throughout experimentation and by interpreting the results\nof a live production system, the expectations of human content reviewers had\n100% of alignment with AI 53.8% of the time with an average iteration count of\n4.38. The geometric interpretation of content traits such as factualness,\nnovelty, repetitiveness, and relevancy over a Hilbert vector space combines\nspatial volume (all trait importance) with vertices alignment (individual trait\nrelevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an\nincrease in content quality by extending the coverage of tennis action. Our\nwork that was deployed at the US Open 2024 has been used across other live\nevents within sports and entertainment."}
{"id": "2505.09031", "pdf": "https://arxiv.org/pdf/2505.09031.pdf", "abs": "https://arxiv.org/abs/2505.09031", "title": "Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification", "authors": ["Adarsh Kumar", "Hwiyoon Kim", "Jawahar Sai Nathani", "Neil Roy"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Hallucination, where large language models (LLMs) generate confident but\nincorrect or irrelevant information, remains a key limitation in their\napplication to complex, open-ended tasks. Chain-of-thought (CoT) prompting has\nemerged as a promising method for improving multistep reasoning by guiding\nmodels through intermediate steps. However, CoT alone does not fully address\nthe hallucination problem. In this work, we investigate how combining CoT with\nretrieval-augmented generation (RAG), as well as applying self-consistency and\nself-verification strategies, can reduce hallucinations and improve factual\naccuracy. By incorporating external knowledge sources during reasoning and\nenabling models to verify or revise their own outputs, we aim to generate more\naccurate and coherent responses. We present a comparative evaluation of\nbaseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification\ntechniques. Our results highlight the effectiveness of each method and identify\nthe most robust approach for minimizing hallucinations while preserving fluency\nand reasoning depth."}
{"id": "2505.09083", "pdf": "https://arxiv.org/pdf/2505.09083.pdf", "abs": "https://arxiv.org/abs/2505.09083", "title": "Ornithologist: Towards Trustworthy \"Reasoning\" about Central Bank Communications", "authors": ["Dominic Zaun Eu Jones"], "categories": ["econ.GN", "cs.CL", "q-fin.EC", "J.4; I.2.7"], "comment": "16 pages, 6 figures", "summary": "I develop Ornithologist, a weakly-supervised textual classification system\nand measure the hawkishness and dovishness of central bank text. Ornithologist\nuses ``taxonomy-guided reasoning'', guiding a large language model with\nhuman-authored decision trees. This increases the transparency and\nexplainability of the system and makes it accessible to non-experts. It also\nreduces hallucination risk. Since it requires less supervision than traditional\nclassification systems, it can more easily be applied to other problems or\nsources of text (e.g. news) without much modification. Ornithologist\nmeasurements of hawkishness and dovishness of RBA communication carry\ninformation about the future of the cash rate path and of market expectations."}
{"id": "2505.09246", "pdf": "https://arxiv.org/pdf/2505.09246.pdf", "abs": "https://arxiv.org/abs/2505.09246", "title": "Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases", "authors": ["Derian Boer", "Stephen Roth", "Stefan Kramer"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever ."}
{"id": "2505.09436", "pdf": "https://arxiv.org/pdf/2505.09436.pdf", "abs": "https://arxiv.org/abs/2505.09436", "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios", "authors": ["Raghav Garg", "Kapil Sharma", "Karan Gupta"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques."}
{"id": "2505.09610", "pdf": "https://arxiv.org/pdf/2505.09610.pdf", "abs": "https://arxiv.org/abs/2505.09610", "title": "Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors", "authors": ["Nicolas Dupuis", "Ravi Nair", "Shyam Ramji", "Sean McClintock", "Nishant Chauhan", "Priyanka Nagpal", "Bart Blaner", "Ken Valk", "Leon Stok", "Ruchir Puri"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world."}
{"id": "2505.09614", "pdf": "https://arxiv.org/pdf/2505.09614.pdf", "abs": "https://arxiv.org/abs/2505.09614", "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?", "authors": ["Anthony GX-Chen", "Dongyan Lin", "Mandana Samiei", "Doina Precup", "Blake A. Richards", "Rob Fergus", "Kenneth Marino"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Language model (LM) agents are increasingly used as autonomous\ndecision-makers who need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established \"Blicket Test\" paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not children-like). Finally, we\npropose a test-time sampling method which explicitly samples and eliminates\nhypotheses about causal relationships from the LM. This scalable approach\nsignificantly reduces the disjunctive bias and moves LMs closer to the goal of\nscientific, causally rigorous reasoning."}
{"id": "2402.01383", "pdf": "https://arxiv.org/pdf/2402.01383.pdf", "abs": "https://arxiv.org/abs/2402.01383", "title": "LLM-based NLG Evaluation: Current Status and Challenges", "authors": ["Mingqi Gao", "Xinyu Hu", "Jie Ruan", "Xiao Pu", "Xiaojun Wan"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating natural language generation (NLG) is a vital but challenging\nproblem in natural language processing. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM\ncollaborative evaluation. In this survey, we first give a taxonomy of LLM-based\nNLG evaluation methods, and discuss their pros and cons, respectively. Lastly,\nwe discuss several open problems in this area and point out future research\ndirections."}
{"id": "2404.03080", "pdf": "https://arxiv.org/pdf/2404.03080.pdf", "abs": "https://arxiv.org/abs/2404.03080", "title": "Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model", "authors": ["Yanpeng Ye", "Jie Ren", "Shaozhou Wang", "Yuwei Wan", "Imran Razzak", "Bram Hoex", "Haofen Wang", "Tong Xie", "Wenjie Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 7 figures, 3 tables; Accepted by 38th Conference on Neural\n  Information Processing Systems (NeurIPS 2024)", "summary": "Knowledge in materials science is widely dispersed across extensive\nscientific literature, posing significant challenges to the efficient discovery\nand integration of new materials. Traditional methods, often reliant on costly\nand time-consuming experimental approaches, further complicate rapid\ninnovation. Addressing these challenges, the integration of artificial\nintelligence with materials science has opened avenues for accelerating the\ndiscovery process, though it also demands precise annotation, data extraction,\nand traceability of information. To tackle these issues, this article\nintroduces the Materials Knowledge Graph (MKG), which utilizes advanced natural\nlanguage processing techniques integrated with large language models to extract\nand systematically organize a decade's worth of high-quality research into\nstructured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes\ninformation into comprehensive labels such as Name, Formula, and Application,\nstructured around a meticulously designed ontology, thus enhancing data\nusability and integration. By implementing network-based algorithms, MKG not\nonly facilitates efficient link prediction but also significantly reduces\nreliance on traditional experimental methods. This structured approach not only\nstreamlines materials research but also lays the groundwork for more\nsophisticated science knowledge graphs."}
{"id": "2410.04526", "pdf": "https://arxiv.org/pdf/2410.04526.pdf", "abs": "https://arxiv.org/abs/2410.04526", "title": "FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering", "authors": ["Siqiao Xue", "Xiaojing Li", "Fan Zhou", "Qingyang Dai", "Zhixuan Chu", "Hongyuan Mei"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we introduce FAMMA, an open-source benchmark for\n\\underline{f}in\\underline{a}ncial \\underline{m}ultilingual\n\\underline{m}ultimodal question \\underline{a}nswering (QA). Our benchmark aims\nto evaluate the abilities of large language models (LLMs) in answering complex\nreasoning questions that require advanced financial knowledge. The benchmark\nhas two versions: FAMMA-Basic consists of 1,945 questions extracted from\nuniversity textbooks and exams, along with human-annotated answers and\nrationales; FAMMA-LivePro consists of 103 novel questions created by human\ndomain experts, with answers and rationales held out from the public for a\ncontamination-free evaluation. These questions cover advanced knowledge of 8\nmajor subfields in finance (e.g., corporate finance, derivatives, and portfolio\nmanagement). Some are in Chinese or French, while a majority of them are in\nEnglish. Each question has some non-text data such as charts, diagrams, or\ntables. Our experiments reveal that FAMMA poses a significant challenge on\nLLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,\nwe curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,\nand fine-tuned a series of open-source Qwen models using this reasoning data.\nWe found that training a model on these reasoning trajectories can\nsignificantly improve its performance on FAMMA-LivePro. We released our\nleaderboard, data, code, and trained models at\nhttps://famma-bench.github.io/famma/."}
{"id": "2411.09116", "pdf": "https://arxiv.org/pdf/2411.09116.pdf", "abs": "https://arxiv.org/abs/2411.09116", "title": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs", "authors": ["Yidan Zhang", "Yu Wan", "Boyi Deng", "Baosong Yang", "Haoran Wei", "Fei Huang", "Bowen Yu", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) showcase varied\nmultilingual capabilities across tasks like translation, code generation, and\nreasoning. Previous assessments often limited their scope to fundamental\nnatural language processing (NLP) or isolated capability-specific tasks. To\nalleviate this drawback, we aim to present a comprehensive multilingual\nmultitask benchmark. First, we introduce P-MMEval, a large-scale benchmark\ncovering effective fundamental and capability-specialized datasets.\nFurthermore, P-MMEval delivers consistent language coverage across various\ndatasets and provides parallel samples. Finally, we conduct extensive\nexperiments on representative multilingual model series to compare performances\nacross models and tasks, explore the relationship between multilingual\nperformances and factors such as tasks, model sizes, languages, and prompts,\nand examine the effectiveness of knowledge transfer from English to other\nlanguages. The resulting insights are intended to offer valuable guidance for\nfuture research. The dataset is available at\nhttps://huggingface.co/datasets/Qwen/P-MMEval."}
{"id": "2502.09650", "pdf": "https://arxiv.org/pdf/2502.09650.pdf", "abs": "https://arxiv.org/abs/2502.09650", "title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples", "authors": ["Chengqian Gao", "Haonan Li", "Liu Liu", "Zeke Xie", "Peilin Zhao", "Zhiqiang Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "The alignment of large language models (LLMs) often assumes that using more\nclean data yields better outcomes, overlooking the match between model capacity\nand example difficulty. Challenging this, we propose a new principle:\nPreference data vary in difficulty, and overly difficult examples hinder\nalignment, by exceeding the model's capacity. Through systematic\nexperimentation, we validate this principle with three key findings: (1)\npreference examples vary in difficulty, as evidenced by consistent learning\norders across alignment runs; (2) overly difficult examples significantly\ndegrade performance across four LLMs and two datasets; and (3) the capacity of\na model dictates its threshold for handling difficult examples, underscoring a\ncritical relationship between data selection and model capacity. Building on\nthis principle, we introduce Selective DPO, which filters out overly difficult\nexamples. This simple adjustment improves alignment performance by 9-16% in win\nrates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a\nseries of DPO variants with different algorithmic adjustments. Together, these\nresults illuminate the importance of aligning data difficulty with model\ncapacity, offering a transformative perspective for improving alignment\nstrategies in LLMs. Code is available at\nhttps://github.com/glorgao/SelectiveDPO."}
{"id": "2502.10725", "pdf": "https://arxiv.org/pdf/2502.10725.pdf", "abs": "https://arxiv.org/abs/2502.10725", "title": "PropNet: a White-Box and Human-Like Network for Sentence Representation", "authors": ["Fei Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Clarified some ambiguities in the previous version", "summary": "Transformer-based embedding methods have dominated the field of sentence\nrepresentation in recent years. Although they have achieved remarkable\nperformance on NLP missions, such as semantic textual similarity (STS) tasks,\ntheir black-box nature and large-data-driven training style have raised\nconcerns, including issues related to bias, trust, and safety. Many efforts\nhave been made to improve the interpretability of embedding models, but these\nproblems have not been fundamentally resolved. To achieve inherent\ninterpretability, we propose a purely white-box and human-like sentence\nrepresentation network, PropNet. Inspired by findings from cognitive science,\nPropNet constructs a hierarchical network based on the propositions contained\nin a sentence. While experiments indicate that PropNet has a significant gap\ncompared to state-of-the-art (SOTA) embedding models in STS tasks, case studies\nreveal substantial room for improvement. Additionally, PropNet enables us to\nanalyze and understand the human cognitive processes underlying STS benchmarks."}
{"id": "2503.10652", "pdf": "https://arxiv.org/pdf/2503.10652.pdf", "abs": "https://arxiv.org/abs/2503.10652", "title": "Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference", "authors": ["Han Wang", "Jacek Pawlak", "Aruna Sivakumar"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Survey research plays a crucial role in studies by capturing consumer\npreferences and informing policy decisions. Stated preference (SP) surveys help\nresearchers understand how individuals make trade-offs in hypothetical,\npotentially futuristic, scenarios. However, traditional methods are costly,\ntime-consuming, and affected by respondent fatigue and ethical constraints.\nLarge language models (LLMs) have shown remarkable capabilities in generating\nhuman-like responses, prompting interest in their use in survey research. This\nstudy investigates LLMs for simulating consumer choices in energy-related SP\nsurveys and explores their integration into data collection and analysis\nworkflows. Test scenarios were designed to assess the simulation performance of\nseveral LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and\naggregated levels, considering prompt design, in-context learning (ICL),\nchain-of-thought (CoT) reasoning, model types, integration with traditional\nchoice models, and potential biases. While LLMs achieve accuracy above random\nguessing, performance remains insufficient for practical simulation use.\nCloud-based LLMs do not consistently outperform smaller local models.\nDeepSeek-R1 achieves the highest average accuracy (77%) and outperforms\nnon-reasoning LLMs in accuracy, factor identification, and choice distribution\nalignment. Previous SP choices are the most effective input; longer prompts\nwith more factors reduce accuracy. Mixed logit models can support LLM prompt\nrefinement. Reasoning LLMs show potential in data analysis by indicating factor\nsignificance, offering a qualitative complement to statistical models. Despite\nlimitations, pre-trained LLMs offer scalability and require minimal historical\ndata. Future work should refine prompts, further explore CoT reasoning, and\ninvestigate fine-tuning techniques."}
{"id": "2503.17599", "pdf": "https://arxiv.org/pdf/2503.17599.pdf", "abs": "https://arxiv.org/abs/2503.17599", "title": "Evaluating Clinical Competencies of Large Language Models with a General Practice Benchmark", "authors": ["Zheqing Li", "Yiying Yang", "Jiping Lang", "Wenhao Jiang", "Yuhang Zhao", "Shuang Li", "Dingqian Wang", "Zhu Lin", "Xuanna Li", "Yuze Tang", "Jiexian Qiu", "Xiaolin Lu", "Hongji Yu", "Shuang Chen", "Yuhua Bi", "Xiaofei Zeng", "Yixian Chen", "Junrong Chen", "Lin Yao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated considerable potential in\ngeneral practice. However, existing benchmarks and evaluation frameworks\nprimarily depend on exam-style or simplified question-answer formats, lacking a\ncompetency-based structure aligned with the real-world clinical\nresponsibilities encountered in general practice. Consequently, the extent to\nwhich LLMs can reliably fulfill the duties of general practitioners (GPs)\nremains uncertain. In this work, we propose a novel evaluation framework to\nassess the capability of LLMs to function as GPs. Based on this framework, we\nintroduce a general practice benchmark (GPBench), whose data are meticulously\nannotated by domain experts in accordance with routine clinical practice\nstandards. We evaluate ten state-of-the-art LLMs and analyze their\ncompetencies. Our findings indicate that current LLMs are not yet ready for\ndeployment in such settings without human oversight, and further optimization\nspecifically tailored to the daily responsibilities of GPs is essential."}
{"id": "2503.21696", "pdf": "https://arxiv.org/pdf/2503.21696.pdf", "abs": "https://arxiv.org/abs/2503.21696", "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks", "authors": ["Wenqi Zhang", "Mengna Wang", "Gangao Liu", "Xu Huixin", "Yiwei Jiang", "Yongliang Shen", "Guiyang Hou", "Zhe Zheng", "Hang Zhang", "Xin Li", "Weiming Lu", "Peng Li", "Yueting Zhuang"], "categories": ["cs.CL", "cs.CV"], "comment": "Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner", "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases."}
{"id": "2503.21813", "pdf": "https://arxiv.org/pdf/2503.21813.pdf", "abs": "https://arxiv.org/abs/2503.21813", "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching", "authors": ["Zhangcheng Qiang", "Kerry Taylor", "Weiqing Wang", "Jing Jiang"], "categories": ["cs.CL", "cs.IR"], "comment": "14 pages, 4 figures, 4 tables, 2 prompt templates", "summary": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks."}
{"id": "2503.24293", "pdf": "https://arxiv.org/pdf/2503.24293.pdf", "abs": "https://arxiv.org/abs/2503.24293", "title": "Is analogy enough to draw novel adjective-noun inferences?", "authors": ["Hayley Ross", "Kathryn Davidson", "Najoung Kim"], "categories": ["cs.CL"], "comment": "9 pages (17 pages with appendix). Accepted to SCiL 2025", "summary": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition."}
{"id": "2504.04717", "pdf": "https://arxiv.org/pdf/2504.04717.pdf", "abs": "https://arxiv.org/abs/2504.04717", "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models", "authors": ["Yubo Li", "Xiaobin Shen", "Xinyu Yao", "Xueying Ding", "Yidi Miao", "Ramayya Krishnan", "Rema Padman"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs."}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949.pdf", "abs": "https://arxiv.org/abs/2505.00949", "title": "Llama-Nemotron: Efficient Reasoning Models", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Shaona Ghosh", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Chris Alexiuk", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM."}
{"id": "2505.05084", "pdf": "https://arxiv.org/pdf/2505.05084.pdf", "abs": "https://arxiv.org/abs/2505.05084", "title": "Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction", "authors": ["Xiaowei Zhu", "Yubing Ren", "Yanan Cao", "Xixun Lin", "Fang Fang", "Yangxi Li"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models has raised significant\nconcerns regarding their potential misuse by malicious actors. As a result,\ndeveloping effective detectors to mitigate these risks has become a critical\npriority. However, most existing detection methods focus excessively on\ndetection accuracy, often neglecting the societal risks posed by high false\npositive rates (FPRs). This paper addresses this issue by leveraging Conformal\nPrediction (CP), which effectively constrains the upper bound of FPRs. While\ndirectly applying CP constrains FPRs, it also leads to a significant reduction\nin detection performance. To overcome this trade-off, this paper proposes a\nZero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal\nPrediction (MCP), which both enforces the FPR constraint and improves detection\nperformance. This paper also introduces RealDet, a high-quality dataset that\nspans a wide range of domains, ensuring realistic calibration and enabling\nsuperior detection performance when combined with MCP. Empirical evaluations\ndemonstrate that MCP effectively constrains FPRs, significantly enhances\ndetection performance, and increases robustness against adversarial attacks\nacross multiple detectors and datasets."}
{"id": "2505.07890", "pdf": "https://arxiv.org/pdf/2505.07890.pdf", "abs": "https://arxiv.org/abs/2505.07890", "title": "TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks", "authors": ["Kutay Ertürk", "Furkan Altınışık", "İrem Sarıaltın", "Ömer Nezih Gerek"], "categories": ["cs.CL", "eess.IV"], "comment": null, "summary": "This study presents TSLFormer, a light and robust word-level Turkish Sign\nLanguage (TSL) recognition model that treats sign gestures as ordered,\nstring-like language. Instead of using raw RGB or depth videos, our method only\nworks with 3D joint positions - articulation points - extracted using Google's\nMediapipe library, which focuses on the hand and torso skeletal locations. This\ncreates efficient input dimensionality reduction while preserving important\nsemantic gesture information.\n  Our approach revisits sign language recognition as sequence-to-sequence\ntranslation, inspired by the linguistic nature of sign languages and the\nsuccess of transformers in natural language processing. Since TSLFormer uses\nthe self-attention mechanism, it effectively captures temporal co-occurrence\nwithin gesture sequences and highlights meaningful motion patterns as words\nunfold.\n  Evaluated on the AUTSL dataset with over 36,000 samples and 227 different\nwords, TSLFormer achieves competitive performance with minimal computational\ncost. These results show that joint-based input is sufficient for enabling\nreal-time, mobile, and assistive communication systems for hearing-impaired\nindividuals."}
{"id": "2505.08037", "pdf": "https://arxiv.org/pdf/2505.08037.pdf", "abs": "https://arxiv.org/abs/2505.08037", "title": "TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation", "authors": ["Yutong Liu", "Feng Xiao", "Ziyue Zhang", "Yongbin Yu", "Cheng Huang", "Fan Gao", "Xiangxiang Wang", "Ma-bao Ban", "Manping Fan", "Thupten Tsering", "Cheng Huang", "Gadeng Luosang", "Renzeng Duojie", "Nyima Tashi"], "categories": ["cs.CL", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Multi-level Tibetan spelling correction addresses errors at both the\ncharacter and syllable levels within a unified model. Existing methods focus\nmainly on single-level correction and lack effective integration of both\nlevels. Moreover, there are no open-source datasets or augmentation methods\ntailored for this task in Tibetan. To tackle this, we propose a data\naugmentation approach using unlabeled text to generate multi-level corruptions,\nand introduce TiSpell, a semi-masked model capable of correcting both\ncharacter- and syllable-level errors. Although syllable-level correction is\nmore challenging due to its reliance on global context, our semi-masked\nstrategy simplifies this process. We synthesize nine types of corruptions on\nclean sentences to create a robust training set. Experiments on both simulated\nand real-world data demonstrate that TiSpell, trained on our dataset,\noutperforms baseline models and matches the performance of state-of-the-art\napproaches, confirming its effectiveness."}
{"id": "2505.08167", "pdf": "https://arxiv.org/pdf/2505.08167.pdf", "abs": "https://arxiv.org/abs/2505.08167", "title": "Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage", "authors": ["Ruilin Liu", "Zhixiao Zhao", "Jieqiong Li", "Chang Liu", "Dongbo Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 5 figures", "summary": "The rapid development of large language models (LLMs) has provided\nsignificant support and opportunities for the advancement of domain-specific\nLLMs. However, fine-tuning these large models using Intangible Cultural\nHeritage (ICH) data inevitably faces challenges such as bias, incorrect\nknowledge inheritance, and catastrophic forgetting. To address these issues, we\npropose a novel training method that integrates a bidirectional chains of\nthought and a reward mechanism. This method is built upon ICH-Qwen, a large\nlanguage model specifically designed for the field of intangible cultural\nheritage. The proposed method enables the model to not only perform forward\nreasoning but also enhances the accuracy of the generated answers by utilizing\nreverse questioning and reverse reasoning to activate the model's latent\nknowledge. Additionally, a reward mechanism is introduced during training to\noptimize the decision-making process. This mechanism improves the quality of\nthe model's outputs through structural and content evaluations with different\nweighting schemes. We conduct comparative experiments on ICH-Qwen, with results\ndemonstrating that our method outperforms 0-shot, step-by-step reasoning,\nknowledge distillation, and question augmentation methods in terms of accuracy,\nBleu-4, and Rouge-L scores on the question-answering task. Furthermore, the\npaper highlights the effectiveness of combining the bidirectional chains of\nthought and reward mechanism through ablation experiments. In addition, a\nseries of generalizability experiments are conducted, with results showing that\nthe proposed method yields improvements on various domain-specific datasets and\nadvanced models in areas such as Finance, Wikidata, and StrategyQA. This\ndemonstrates that the method is adaptable to multiple domains and provides a\nvaluable approach for model training in future applications across diverse\nfields."}
{"id": "2505.08435", "pdf": "https://arxiv.org/pdf/2505.08435.pdf", "abs": "https://arxiv.org/abs/2505.08435", "title": "Hakim: Farsi Text Embedding Model", "authors": ["Mehran Sarmadi", "Morteza Alikhani", "Erfan Zinvandi", "Zahra Pourbahman"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advancements in text embedding have significantly improved natural\nlanguage understanding across many languages, yet Persian remains notably\nunderrepresented in large-scale embedding research. In this paper, we present\nHakim, a novel state-of-the-art Persian text embedding model that achieves a\n8.5% performance improvement over existing approaches on the FaMTEB benchmark,\noutperforming all previously developed Persian language models. As part of this\nwork, we introduce three new datasets - Corpesia, Pairsia-sup, and\nPairsia-unsup - to support supervised and unsupervised training scenarios.\nAdditionally, Hakim is designed for applications in chatbots and\nretrieval-augmented generation (RAG) systems, particularly addressing retrieval\ntasks that require incorporating message history within these systems. We also\npropose a new baseline model built on the BERT architecture. Our language model\nconsistently achieves higher accuracy across various Persian NLP tasks, while\nthe RetroMAE-based model proves particularly effective for textual information\nretrieval applications. Together, these contributions establish a new\nfoundation for advancing Persian language understanding."}
{"id": "2411.03343", "pdf": "https://arxiv.org/pdf/2411.03343.pdf", "abs": "https://arxiv.org/abs/2411.03343", "title": "What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks", "authors": ["Nathalie Kirch", "Constantin Weisser", "Severin Field", "Helen Yannakoudakis", "Stephen Casper"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Jailbreaks have been a central focus of research regarding the safety and\nreliability of large language models (LLMs), yet the mechanisms underlying\nthese attacks remain poorly understood. While previous studies have\npredominantly relied on linear methods to detect jailbreak attempts and model\nrefusals, we take a different approach by examining both linear and non-linear\nfeatures in prompts that lead to successful jailbreaks. First, we introduce a\nnovel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack\nmethods. Leveraging this dataset, we train probes to classify successful from\nunsuccessful jailbreaks using the latent representations corresponding to\nprompt tokens. Notably, we find that even when probes achieve high accuracy in\npredicting the success of jailbreaks, their performance often fails to\ngeneralize to unseen attack methods. This reveals that different jailbreaking\nstrategies exploit different non-linear, non-universal features. Next, we\ndemonstrate that non-linear probes provide a powerful tool for steering model\nbehavior. Specifically, we use these probes to guide targeted latent space\nperturbations, enabling us to effectively modulate the model's robustness\nagainst jailbreaks. Overall, our findings challenge the assumption that\njailbreaks can be fully understood through linear or simple universal prompt\nfeatures alone, highlighting the importance of a nuanced understanding of the\nmechanisms behind LLM vulnerabilities."}
{"id": "2502.04405", "pdf": "https://arxiv.org/pdf/2502.04405.pdf", "abs": "https://arxiv.org/abs/2502.04405", "title": "FAS: Fast ANN-SNN Conversion for Spiking Large Language Models", "authors": ["Long Chen", "Xiaotian Song", "Andy Song", "BaDong Chen", "Jiancheng Lv", "Yanan Sun"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Spiking Large Language Models have been shown as a good alternative to LLMs\nin various scenarios. Existing methods for creating Spiking LLMs, i.e., direct\ntraining and ANN-SNN conversion, often suffer from performance degradation and\nrelatively high computational costs. To address these issues, we propose a\nnovel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking\nLLMs in two stages. The first stage employs a full-parameter fine-tuning of\npre-trained models, so it does not need any direct training from scratch. The\nsecond stage introduces a coarse-to-fine calibration method to reduce\nconversion errors and improve accuracy. Experiments on both language and\nvision-language tasks across four different scales of LLMs demonstrate that FAS\ncan achieve state-of-the-art performance yet with significantly reduced\ninference latency and computational costs. Notably, FAS only takes eight\ntimesteps to achieve an accuracy of 3\\% higher than that of the OPT-7B model,\nwhile reducing energy consumption by 96.63\\%. The source code is available at\nhttps://github.com/lc783/FAS"}
{"id": "2502.15507", "pdf": "https://arxiv.org/pdf/2502.15507.pdf", "abs": "https://arxiv.org/abs/2502.15507", "title": "Activation Steering in Neural Theorem Provers", "authors": ["Shashank Kirtania"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "incorrect explanation for a concept, need to revise and update!", "summary": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments."}
{"id": "2502.15823", "pdf": "https://arxiv.org/pdf/2502.15823.pdf", "abs": "https://arxiv.org/abs/2502.15823", "title": "InductionBench: LLMs Fail in the Simplest Complexity Class", "authors": ["Wenyue Hua", "Tyler Wong", "Sun Fei", "Liangming Pan", "Adam Jardine", "William Yang Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.FL"], "comment": "25 pages, 10 figures, more details including examples and prompts are\n  added", "summary": "Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark."}
{"id": "2502.16560", "pdf": "https://arxiv.org/pdf/2502.16560.pdf", "abs": "https://arxiv.org/abs/2502.16560", "title": "An Analytical Emotion Framework of Rumour Threads on Social Media", "authors": ["Rui Xing", "Boyang Sun", "Kun Zhang", "Preslav Nakov", "Timothy Baldwin", "Jey Han Lau"], "categories": ["cs.AI", "cs.CL", "cs.SI"], "comment": "Accepted to ICWSM 2025 MisD Workshop", "summary": "Rumours in online social media pose significant risks to modern society,\nmotivating the need for better understanding of how they develop. We focus\nspecifically on the interface between emotion and rumours in threaded\ndiscourses, building on the surprisingly sparse literature on the topic which\nhas largely focused on single aspect of emotions within the original rumour\nposts themselves, and largely overlooked the comparative differences between\nrumours and non-rumours. In this work, we take one step further to provide a\ncomprehensive analytical emotion framework with multi-aspect emotion detection,\ncontrasting rumour and non-rumour threads and provide both correlation and\ncausal analysis of emotions. We applied our framework on existing widely-used\nrumour datasets to further understand the emotion dynamics in online social\nmedia threads. Our framework reveals that rumours trigger more negative\nemotions (e.g., anger, fear, pessimism), while non-rumours evoke more positive\nones. Emotions are contagious, rumours spread negativity, non-rumours spread\npositivity. Causal analysis shows surprise bridges rumours and other emotions;\npessimism comes from sadness and fear, while optimism arises from joy and love."}
{"id": "2503.11197", "pdf": "https://arxiv.org/pdf/2503.11197.pdf", "abs": "https://arxiv.org/abs/2503.11197", "title": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering", "authors": ["Gang Li", "Jizhong Liu", "Heinrich Dinkel", "Yadong Niu", "Junbo Zhang", "Jian Luan"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa."}
