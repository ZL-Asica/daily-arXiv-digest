{"id": "2507.14189", "pdf": "https://arxiv.org/pdf/2507.14189.pdf", "abs": "https://arxiv.org/abs/2507.14189", "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base", "authors": ["Song Mao", "Lejun Cheng", "Pinlong Cai", "Guohang Yan", "Ding Wang", "Botian Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "work in process", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality."}
{"id": "2507.14198", "pdf": "https://arxiv.org/pdf/2507.14198.pdf", "abs": "https://arxiv.org/abs/2507.14198", "title": "Retention analysis of edited knowledge after fine-tuning", "authors": ["Fufang Wen", "Shichang Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that freezing layers associated with edited content can significantly\nimprove knowledge retention, offering insight into how future editing methods\nmight be made more robust."}
{"id": "2507.14200", "pdf": "https://arxiv.org/pdf/2507.14200.pdf", "abs": "https://arxiv.org/abs/2507.14200", "title": "Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System", "authors": ["Shengji Tang", "Jianjian Cao", "Weihao Lin", "Jiale Hong", "Bo Zhang", "Shuyue Hu", "Lei Bai", "Tao Chen", "Wanli Ouyang", "Peng Ye"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper aims to demonstrate the potential and strengths of open-source\ncollectives. It leads to a promising question: Can we harness multiple\nopen-source LLMs to match or even beat the closed-source LLMs? To answer this,\nwe propose SMACS, a scalable multi-agent collaboration system (MACS) framework\nwith high performance. Specifically, for continuous integration of new LLMs and\ngeneralization to diverse questions, we first propose a Retrieval-based Prior\nSelection (RPS), which assigns a proxy performance score to each LLM to select\nthe Top-k LLMs at the instance level for any given question. Then, we propose\nan Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the\ngeneration of diverse responses through prior dropping and selecting the\nhigh-quality response via a hybrid posterior score. Experiments on eight\nmainstream benchmarks validate the effectiveness of our SMACS: by integrating\nfifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,\ne.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)\nacross multiple tasks. Remarkably, it even exceeds the average of best results\nof different datasets from both open-source LLMs (+2.86%) and closed-source\nLLMs (+2.04%), pushing the upper bound of intelligence. Code will be released\nat https://github.com/magent4aci/SMACS."}
{"id": "2507.14214", "pdf": "https://arxiv.org/pdf/2507.14214.pdf", "abs": "https://arxiv.org/abs/2507.14214", "title": "Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale", "authors": ["Rui Zhao", "Vladyslav Melnychuk", "Jun Zhao", "Jesse Wright", "Nigel Shadbolt"], "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites despite claiming\notherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that\nassists users with personalized privacy policy analysis. PoliAnalyzer uses\nNatural Language Processing (NLP) to extract formal representations of data\nusage practices from policy texts. In favor of deterministic, logical inference\nis applied to compare user preferences with the formal privacy policy\nrepresentation and produce a compliance report. To achieve this, we extend an\nexisting formal Data Terms of Use policy language to model privacy policies as\napp policies and user preferences as data policies. In our evaluation using our\nenriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated\nhigh accuracy in identifying relevant data usage practices, achieving F1-score\nof 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can\nmodel diverse user data-sharing preferences, derived from prior research as 23\nuser profiles, and perform compliance analysis against the top 100 most-visited\nwebsites. This analysis revealed that, on average, 95.2% of a privacy policy's\nsegments do not conflict with the analyzed user preferences, enabling users to\nconcentrate on understanding the 4.8% (636 / 13205) that violates preferences,\nsignificantly reducing cognitive burden. Further, we identified common\npractices in privacy policies that violate user expectations - such as the\nsharing of location data with 3rd parties. This paper demonstrates that\nPoliAnalyzer can support automated personalized privacy policy analysis at\nscale using off-the-shelf NLP tools. This sheds light on a pathway to help\nindividuals regain control over their data and encourage societal discussions\non platform data practices to promote a fairer power dynamic."}
{"id": "2507.14316", "pdf": "https://arxiv.org/pdf/2507.14316.pdf", "abs": "https://arxiv.org/abs/2507.14316", "title": "Can AR-Embedded Visualizations Foster Appropriate Reliance on AI in Spatial Decision Making? A Comparative Study of AR See-Through vs. 2D Minimap", "authors": ["Xianhao Carton Liu", "Difan Jia", "Tongyu Nie", "Evan Suma Rosenberg", "Victoria Interrante", "Chen Zhu-Tian"], "categories": ["cs.HC"], "comment": null, "summary": "In high-stakes, time-critical scenarios-such as emergency evacuation, first\nresponder prioritization, and crisis management -- decision-makers must rapidly\nchoose among spatial targets, such as exits, individuals to assist, or areas to\nsecure. Advances in indoor sensing and artificial intelligence (AI) can support\nthese decisions by visualizing real-time situational data and AI suggestions on\n2D maps. However, mentally mapping this information onto real-world spaces\nimposes significant cognitive load. This load can impair users' ability to\nappropriately judge AI suggestions, leading to inappropriate reliance (e.g.,\naccepting wrong AI suggestions or rejecting correct ones). Embedded\nvisualizations in Augmented Reality (AR), by directly overlaying information\nonto physical environments, may reduce this load and foster more deliberate,\nappropriate reliance on AI. But is this true? In this work, we conducted an\nempirical study (N = 32) comparing AR see-through (embedded visualization) and\n2D Minimap in time-critical, AI-assisted spatial target selection tasks.\nContrary to our expectations, users exhibited greater inappropriate reliance on\nAI in the AR condition. Our analysis further reveals that this is primarily due\nto over-reliance, with factors specific to embedded visualizations, such as\nperceptual challenges, visual proximity illusions, and highly realistic visual\nrepresentations. Nonetheless, embedded visualizations demonstrated notable\nbenefits in spatial reasoning, such as spatial mapping and egocentric spatial\nimagery. We conclude by discussing the empirical insights, deriving design\nimplications, and outlining important directions for future research on\nhuman-AI decision collaboration in AR."}
{"id": "2507.14231", "pdf": "https://arxiv.org/pdf/2507.14231.pdf", "abs": "https://arxiv.org/abs/2507.14231", "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media", "authors": ["Khalid Hasan", "Jamil Saquer"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The 37th International Conference on Software Engineering & Knowledge\n  Engineering, SEKE 2025 (camera-ready)", "summary": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to\nsubtle early symptoms and social stigma. This paper explores the advanced\nnatural language processing (NLP) models for recognizing signs of bipolar\ndisorder based on user-generated social media text. We conduct a comprehensive\nevaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,\nDistilBERT) and Long Short Term Memory (LSTM) models based on contextualized\n(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed\non a large, annotated dataset of Reddit posts after confirming their validity\nthrough sentiment variance and judgmental analysis. Our results demonstrate\nthat RoBERTa achieves the highest performance among transformer models with an\nF1 score of ~98% while LSTM models using BERT embeddings yield nearly identical\nresults. In contrast, LSTMs trained on static embeddings fail to capture\nmeaningful patterns, scoring near-zero F1. These findings underscore the\ncritical role of contextual language modeling in detecting bipolar disorder. In\naddition, we report model training times and highlight that DistilBERT offers\nan optimal balance between efficiency and accuracy. In general, our study\noffers actionable insights for model selection in mental health NLP\napplications and validates the potential of contextualized language models to\nsupport early bipolar disorder screening."}
{"id": "2507.14384", "pdf": "https://arxiv.org/pdf/2507.14384.pdf", "abs": "https://arxiv.org/abs/2507.14384", "title": "Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions", "authors": ["Angjelin Hila", "Elliott Hauser"], "categories": ["cs.HC", "cs.CL"], "comment": "Extended version of paper accepted for presentation at the ASIS&T\n  Annual Meeting 2025. 38 pages, 12 figures", "summary": "In this study, we investigate the use of large language models (LLMs),\nspecifically ChatGPT, for structured deductive qualitative coding. While most\ncurrent research emphasizes inductive coding applications, we address the\nunderexplored potential of LLMs to perform deductive classification tasks\naligned with established human-coded schemes. Using the Comparative Agendas\nProject (CAP) Master Codebook, we classified U.S. Supreme Court case summaries\ninto 21 major policy domains. We tested four intervention methods: zero-shot,\nfew-shot, definition-based, and a novel Step-by-Step Task Decomposition\nstrategy, across repeated samples. Performance was evaluated using standard\nclassification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's\nalpha), and construct validity was assessed using chi-squared tests and\nCramer's V. Chi-squared and effect size analyses confirmed that intervention\nstrategies significantly influenced classification behavior, with Cramer's V\nvalues ranging from 0.359 to 0.613, indicating moderate to strong shifts in\nclassification patterns. The Step-by-Step Task Decomposition strategy achieved\nthe strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),\nachieving thresholds for substantial agreement. Despite the semantic ambiguity\nwithin case summaries, ChatGPT displayed stable agreement across samples,\nincluding high F1 scores in low-support subclasses. These findings demonstrate\nthat with targeted, custom-tailored interventions, LLMs can achieve reliability\nlevels suitable for integration into rigorous qualitative coding workflows."}
{"id": "2507.14238", "pdf": "https://arxiv.org/pdf/2507.14238.pdf", "abs": "https://arxiv.org/abs/2507.14238", "title": "Language Models Change Facts Based on the Way You Talk", "authors": ["Matthew Kearney", "Reuben Binns", "Yarin Gal"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are increasingly being used in user-facing\napplications, from providing medical consultations to job interview advice.\nRecent research suggests that these models are becoming increasingly proficient\nat inferring identity information about the author of a piece of text from\nlinguistic patterns as subtle as the choice of a few words. However, little is\nknown about how LLMs use this information in their decision-making in\nreal-world applications. We perform the first comprehensive analysis of how\nidentity markers present in a user's writing bias LLM responses across five\ndifferent high-stakes LLM applications in the domains of medicine, law,\npolitics, government benefits, and job salaries. We find that LLMs are\nextremely sensitive to markers of identity in user queries and that race,\ngender, and age consistently influence LLM responses in these applications. For\ninstance, when providing medical advice, we find that models apply different\nstandards of care to individuals of different ethnicities for the same\nsymptoms; we find that LLMs are more likely to alter answers to align with a\nconservative (liberal) political worldview when asked factual questions by\nolder (younger) individuals; and that LLMs recommend lower salaries for\nnon-White job applicants and higher salaries for women compared to men. Taken\ntogether, these biases mean that the use of off-the-shelf LLMs for these\napplications may cause harmful differences in medical care, foster wage gaps,\nand create different political factual realities for people of different\nidentities. Beyond providing an analysis, we also provide new tools for\nevaluating how subtle encoding of identity in users' language choices impacts\nmodel decisions. Given the serious implications of these findings, we recommend\nthat similar thorough assessments of LLM use in user-facing applications are\nconducted before future deployment."}
{"id": "2507.14418", "pdf": "https://arxiv.org/pdf/2507.14418.pdf", "abs": "https://arxiv.org/abs/2507.14418", "title": "Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students", "authors": ["Taufiq Daryanto", "Sophia Stil", "Xiaohan Ding", "Daniel Manesh", "Sang Won Lee", "Tim Lee", "Stephanie Lunn", "Sarah Rodriguez", "Chris Brown", "Eugenia Rho"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "One challenge in technical interviews is the think-aloud process, where\ncandidates verbalize their thought processes while solving coding tasks.\nDespite its importance, opportunities for structured practice remain limited.\nConversational AI offers potential assistance, but limited research explores\nuser perceptions of its role in think-aloud practice. To address this gap, we\nconducted a study with 17 participants using an LLM-based technical interview\npractice tool. Participants valued AI's role in simulation, feedback, and\nlearning from generated examples. Key design recommendations include promoting\nsocial presence in conversational AI for technical interview simulation,\nproviding feedback beyond verbal content analysis, and enabling crowdsourced\nthink-aloud examples through human-AI collaboration. Beyond feature design, we\nexamined broader considerations, including intersectional challenges and\npotential strategies to address them, how AI-driven interview preparation could\npromote equitable learning in computing careers, and the need to rethink AI's\nrole in interview practice by suggesting a research direction that integrates\nhuman-AI collaboration."}
{"id": "2507.14239", "pdf": "https://arxiv.org/pdf/2507.14239.pdf", "abs": "https://arxiv.org/abs/2507.14239", "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation", "authors": ["Weihua Zheng", "Roy Ka-Wei Lee", "Zhengyuan Liu", "Kui Wu", "AiTi Aw", "Bowei Zou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization\nacross languages, yet they remain prone to hallucinations, especially in\nlow-resource languages, due to training data imbalances. These hallucinations,\nwhich include inaccurate or fabricated outputs, are particularly problematic in\ndomain-specific generation tasks (Chataigner et al., 2024). To address this\nchallenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based\nCross-lingual Chain-of-Thought), a two-stage fine-tuning framework for\nmitigating hallucination in MLLMs. Our approach first enhances cross-lingual\nsemantic alignment through curriculum-based contrastive learning combined with\nnext-token prediction during continued pre-training. Building on this\nfoundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting\nstrategy during instruction fine-tuning, which guides the model to reason in a\nhigh-resource language before generating answers in the target low-resource\nlanguage. Experimental results show that CCL-XCoT reduces hallucination rates\nby up to 62% and substantially improves factual knowledge transfer across\nlanguage pairs, without relying on external retrieval or multi-model ensembles."}
{"id": "2507.14482", "pdf": "https://arxiv.org/pdf/2507.14482.pdf", "abs": "https://arxiv.org/abs/2507.14482", "title": "Conch: Competitive Debate Analysis via Visualizing Clash Points and Hierarchical Strategies", "authors": ["Qianhe Chen", "Yong Wang", "Yixin Yu", "Xiyuan Zhu", "Xuerou Yu", "Ran Wang"], "categories": ["cs.HC"], "comment": null, "summary": "In-depth analysis of competitive debates is essential for participants to\ndevelop argumentative skills and refine strategies, and further improve their\ndebating performance. However, manual analysis of unstructured and unlabeled\ntextual records of debating is time-consuming and ineffective, as it is\nchallenging to reconstruct contextual semantics and track logical connections\nfrom raw data. To address this, we propose Conch, an interactive visualization\nsystem that systematically analyzes both what is debated and how it is debated.\nIn particular, we propose a novel parallel spiral visualization that compactly\ntraces the multidimensional evolution of clash points and participant\ninteractions throughout debate process. In addition, we leverage large language\nmodels with well-designed prompts to automatically identify critical debate\nelements such as clash points, disagreements, viewpoints, and strategies,\nenabling participants to understand the debate context comprehensively.\nFinally, through two case studies on real-world debates and a\ncarefully-designed user study, we demonstrate Conch's effectiveness and\nusability for competitive debate analysis."}
{"id": "2507.14240", "pdf": "https://arxiv.org/pdf/2507.14240.pdf", "abs": "https://arxiv.org/abs/2507.14240", "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem", "authors": ["Mohammad Shahedur Rahman", "Peng Gao", "Yuede Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Large language models (LLMs) leverage deep learning to process and predict\nsequences of words from context, enabling them to perform various NLP tasks,\nsuch as translation, summarization, question answering, and content generation.\nHowever, the growing size and complexity of developing, training, and deploying\nadvanced LLMs require extensive computational resources and large datasets.\nThis creates a barrier for users. As a result, platforms that host models and\ndatasets are widely used. For example, Hugging Face, one of the most popular\nplatforms, hosted 1.8 million models and 450K datasets by June 2025, with no\nsign of slowing down. Since many LLMs are built from base models, pre-trained\nmodels, and external datasets, they can inherit vulnerabilities, biases, or\nmalicious components from earlier models or datasets. Therefore, it is critical\nto understand the origin and development of these components to better detect\npotential risks, improve model fairness, and ensure compliance. Motivated by\nthis, our project aims to study the relationships between models and datasets,\nwhich are core components of the LLM supply chain. First, we design a method to\nsystematically collect LLM supply chain data. Using this data, we build a\ndirected heterogeneous graph to model the relationships between models and\ndatasets, resulting in a structure with 397,376 nodes and 453,469 edges. We\nthen perform various analyses and uncover several findings, such as: (i) the\nLLM supply chain graph is large, sparse, and follows a power-law degree\ndistribution; (ii) it features a densely connected core and a fragmented\nperiphery; (iii) datasets play pivotal roles in training; (iv) strong\ninterdependence exists between models and datasets; and (v) the graph is\ndynamic, with daily updates reflecting the ecosystem's ongoing evolution."}
{"id": "2507.14494", "pdf": "https://arxiv.org/pdf/2507.14494.pdf", "abs": "https://arxiv.org/abs/2507.14494", "title": "\"It looks sexy but it's wrong.\" Tensions in creativity and accuracy using genAI for biomedical visualization", "authors": ["Roxanne Ziman", "Shehryar Saharan", "Gaël McGill", "Laura Garrison"], "categories": ["cs.HC"], "comment": "11 pages, 3 figures. Accepted to IEEE VIS 2025 Conference", "summary": "We contribute an in-depth analysis of the workflows and tensions arising from\ngenerative AI (genAI) use in biomedical visualization (BioMedVis). Although\ngenAI affords facile production of aesthetic visuals for biological and medical\ncontent, the architecture of these tools fundamentally limits the accuracy and\ntrustworthiness of the depicted information, from imaginary (or fanciful)\nmolecules to alien anatomy. Through 17 interviews with a diverse group of\npractitioners and researchers, we qualitatively analyze the concerns and values\ndriving genAI (dis)use for the visual representation of spatially-oriented\nbiomedical data. We find that BioMedVis experts, both in roles as developers\nand designers, use genAI tools at different stages of their daily workflows and\nhold attitudes ranging from enthusiastic adopters to skeptical avoiders of\ngenAI. In contrasting the current use and perspectives on genAI observed in our\nstudy with predictions towards genAI in the visualization pipeline from prior\nwork, our refocus the discussion of genAI's effects on projects in\nvisualization in the here and now with its respective opportunities and\npitfalls for future visualization research. At a time when public trust in\nscience is in jeopardy, we are reminded to first do no harm, not just in\nbiomedical visualization but in science communication more broadly. Our\nobservations reaffirm the necessity of human intervention for empathetic design\nand assessment of accurate scientific visuals."}
{"id": "2507.14241", "pdf": "https://arxiv.org/pdf/2507.14241.pdf", "abs": "https://arxiv.org/abs/2507.14241", "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Huan Wang", "Caiming Xiong", "Silvio Savarese"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient."}
{"id": "2507.14527", "pdf": "https://arxiv.org/pdf/2507.14527.pdf", "abs": "https://arxiv.org/abs/2507.14527", "title": "PaperBridge: Crafting Research Narratives through Human-AI Co-Exploration", "authors": ["Runhua Zhang", "Yang Ouyang", "Leixian Shen", "Yuying Tang", "Xiaojuan Ma", "Huamin Qu", "Xian Xu"], "categories": ["cs.HC"], "comment": "Conditionally accepted by UIST'25", "summary": "Researchers frequently need to synthesize their own publications into\ncoherent narratives that demonstrate their scholarly contributions. To suit\ndiverse communication contexts, exploring alternative ways to organize one's\nwork while maintaining coherence is particularly challenging, especially in\ninterdisciplinary fields like HCI where individual researchers' publications\nmay span diverse domains and methodologies. In this paper, we present\nPaperBridge, a human-AI co-exploration system informed by a formative study and\ncontent analysis. PaperBridge assists researchers in exploring diverse\nperspectives for organizing their publications into coherent narratives. At its\ncore is a bi-directional analysis engine powered by large language models,\nsupporting iterative exploration through both top-down user intent (e.g.,\ndetermining organization structure) and bottom-up refinement on narrative\ncomponents (e.g., thematic paper groupings). Our user study (N=12) demonstrated\nPaperBridge's usability and effectiveness in facilitating the exploration of\nalternative research narratives. Our findings also provided empirical insights\ninto how interactive systems can scaffold academic communication tasks."}
{"id": "2507.14298", "pdf": "https://arxiv.org/pdf/2507.14298.pdf", "abs": "https://arxiv.org/abs/2507.14298", "title": "In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Alexander Jacobson", "Lu Yuan", "Leonid Sigal"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14506", "summary": "Recent methods for customizing Large Vision Language Models (LVLMs) for\ndomain-specific tasks have shown promising results in scientific chart\ncomprehension. However, existing approaches face two major limitations: First,\nthey rely on paired data from only a few chart types, limiting generalization\nto wide range of chart types. Secondly, they lack targeted pre-training for\nchart-data alignment, which hampers the model's understanding of underlying\ndata. In this paper, we introduce ChartScope, an LVLM optimized for in-depth\nchart comprehension across diverse chart types. We propose an efficient data\ngeneration pipeline that synthesizes paired data for a wide range of chart\ntypes, along with a novel Dual-Path training strategy that enabling the model\nto succinctly capture essential data details while preserving robust reasoning\ncapabilities by incorporating reasoning over the underlying data. Lastly, we\nestablish ChartDQA, a new benchmark for evaluating not only question-answering\nat different levels but also underlying data understanding. Experimental\nresults demonstrate that ChartScope significantly enhances comprehension on a\nwide range of chart types. The code and data are available at\nhttps://davidhalladay.github.io/chartscope_demo."}
{"id": "2507.14537", "pdf": "https://arxiv.org/pdf/2507.14537.pdf", "abs": "https://arxiv.org/abs/2507.14537", "title": "Uncovering the EEG Temporal Representation of Low-dimensional Object Properties", "authors": ["Jiahua Tang", "Song Wang", "Jiachen Zou", "Chen Wei", "Quanying Liu"], "categories": ["cs.HC"], "comment": null, "summary": "Understanding how the human brain encodes and processes external visual\nstimuli has been a fundamental challenge in neuroscience. With advancements in\nartificial intelligence, sophisticated visual decoding architectures have\nachieved remarkable success in fMRI research, enabling more precise and\nfine-grained spatial concept localization. This has provided new tools for\nexploring the spatial representation of concepts in the brain. However, despite\nthe millisecond-scale temporal resolution of EEG, which offers unparalleled\nadvantages in tracking the dynamic evolution of cognitive processes, the\ntemporal dynamics of neural representations based on EEG remain underexplored.\nThis is primarily due to EEG's inherently low signal-to-noise ratio and its\ncomplex spatiotemporal coupling characteristics. To bridge this research gap,\nwe propose a novel approach that integrates advanced neural decoding algorithms\nto systematically investigate how low-dimensional object properties are\ntemporally encoded in EEG signals. We are the first to attempt to identify the\nspecificity and prototypical temporal characteristics of concepts within\ntemporal distributions. Our framework not only enhances the interpretability of\nneural representations but also provides new insights into visual decoding in\nbrain-computer interfaces (BCI)."}
{"id": "2507.14304", "pdf": "https://arxiv.org/pdf/2507.14304.pdf", "abs": "https://arxiv.org/abs/2507.14304", "title": "Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study", "authors": ["Rakesh Paul", "Anusha Kamath", "Kanishk Singla", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs."}
{"id": "2507.14685", "pdf": "https://arxiv.org/pdf/2507.14685.pdf", "abs": "https://arxiv.org/abs/2507.14685", "title": "EventBox: A Novel Visual Encoding for Interactive Analysis of Temporal and Multivariate Attributes in Event Sequences", "authors": ["Luis Montana", "Jessica Magallanes", "Miguel Juarez", "Suzanne Mason", "Andrew Narracott", "Lindsey van Gemeren", "Steven Wood", "Maria-Cruz Villa-Uriol"], "categories": ["cs.HC"], "comment": "This is the author's version of the article to be published in IEEE\n  Transactions on Visualization and Computer Graphics, and presented at IEEE\n  VIS 2025. 11 pages, 7 figures", "summary": "The rapid growth and availability of event sequence data across domains\nrequires effective analysis and exploration methods to facilitate\ndecision-making. Visual analytics combines computational techniques with\ninteractive visualizations, enabling the identification of patterns, anomalies,\nand attribute interactions. However, existing approaches frequently overlook\nthe interplay between temporal and multivariate attributes. We introduce\nEventBox, a novel data representation and visual encoding approach for\nanalyzing groups of events and their multivariate attributes. We have\nintegrated EventBox into Sequen-C, a visual analytics system for the analysis\nof event sequences. To enable the agile creation of EventBoxes in Sequen-C, we\nhave added user-driven transformations, including alignment, sorting,\nsubstitution and aggregation. To enhance analytical depth, we incorporate\nautomatically generated statistical analyses, providing additional insight into\nthe significance of attribute interactions. We evaluated our approach involving\n21 participants (3 domain experts, 18 novice data analysts). We used the ICE-T\nframework to assess visualization value, user performance metrics completing a\nseries of tasks, and interactive sessions with domain experts. We also present\nthree case studies with real-world healthcare data demonstrating how EventBox\nand its integration into Sequen-C reveal meaningful patterns, anomalies, and\ninsights. These results demonstrate that our work advances visual analytics by\nproviding a flexible solution for exploring temporal and multivariate\nattributes in event sequences."}
{"id": "2507.14307", "pdf": "https://arxiv.org/pdf/2507.14307.pdf", "abs": "https://arxiv.org/abs/2507.14307", "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs", "authors": ["Karin de Langis", "Jong Inn Park", "Andreas Schramm", "Bin Hu", "Khanh Chi Le", "Michael Mensink", "Ahn Thu Tong", "Dongyeop Kang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit increasingly sophisticated linguistic\ncapabilities, yet the extent to which these behaviors reflect human-like\ncognition versus advanced pattern recognition remains an open question. In this\nstudy, we investigate how LLMs process the temporal meaning of linguistic\naspect in narratives that were previously used in human studies. Using an\nExpert-in-the-Loop probing pipeline, we conduct a series of targeted\nexperiments to assess whether LLMs construct semantic representations and\npragmatic inferences in a human-like manner. Our findings show that LLMs\nover-rely on prototypicality, produce inconsistent aspectual judgments, and\nstruggle with causal reasoning derived from aspect, raising concerns about\ntheir ability to fully comprehend narratives. These results suggest that LLMs\nprocess aspect fundamentally differently from humans and lack robust narrative\nunderstanding. Beyond these empirical findings, we develop a standardized\nexperimental framework for the reliable assessment of LLMs' cognitive and\nlinguistic capabilities."}
{"id": "2507.14702", "pdf": "https://arxiv.org/pdf/2507.14702.pdf", "abs": "https://arxiv.org/abs/2507.14702", "title": "A Notification Based Nudge for Handling Excessive Smartphone Use", "authors": ["Partha Sarker", "Dipto Dey", "Marium-E-Jannat"], "categories": ["cs.HC", "F.2.2, I.2.7"], "comment": "6 pages, 8 figures", "summary": "Excessive use of smartphones is a worldwide known issue. In this study, we\nproposed a notification-based intervention approach to reduce smartphone\noveruse without making the user feel any annoyance or irritation. Most of the\nwork in this field tried to reduce smartphone overuse by making smartphone use\nmore difficult for the user. In our user study (n = 109), we found that 19.3%\nof the participants are unwilling to use any usage-limiting application because\na) they do not want their smartphone activities to get restricted or b) those\napplications are annoying. Following that, we devised a hypothesis to minimize\nsmartphone usage among undergraduates. Finally, we designed a prototype for\nAndroid, \"App Usage Monitor,\" and conducted a 3-week experiment through which\nwe found proof of concept for our hypothesis. In our prototype, we combined\ntechniques such as nudge and visualization to increase self-awareness among the\nuser by leveraging notifications."}
{"id": "2507.14314", "pdf": "https://arxiv.org/pdf/2507.14314.pdf", "abs": "https://arxiv.org/abs/2507.14314", "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines", "authors": ["Marija Anđedelić", "Dominik Šipek", "Laura Majer", "Jan Šnajder"], "categories": ["cs.CL"], "comment": "Accepted at Slavic NLP 2025", "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs."}
{"id": "2507.14767", "pdf": "https://arxiv.org/pdf/2507.14767.pdf", "abs": "https://arxiv.org/abs/2507.14767", "title": "XplainAct: Visualization for Personalized Intervention Insights", "authors": ["Yanming Zhang", "Krishnakumar Hegde", "Klaus Mueller"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "This paper will be published and presented at IEEE Visualization\n  (VIS) 2025, Vienna, Austria, November 2025", "summary": "Causality helps people reason about and understand complex systems,\nparticularly through what-if analyses that explore how interventions might\nalter outcomes. Although existing methods embrace causal reasoning using\ninterventions and counterfactual analysis, they primarily focus on effects at\nthe population level. These approaches often fall short in systems\ncharacterized by significant heterogeneity, where the impact of an intervention\ncan vary widely across subgroups. To address this challenge, we present\nXplainAct, a visual analytics framework that supports simulating, explaining,\nand reasoning interventions at the individual level within subpopulations. We\ndemonstrate the effectiveness of XplainAct through two case studies:\ninvestigating opioid-related deaths in epidemiology and analyzing voting\ninclinations in the presidential election."}
{"id": "2507.14355", "pdf": "https://arxiv.org/pdf/2507.14355.pdf", "abs": "https://arxiv.org/abs/2507.14355", "title": "Can LLMs Infer Personality from Real World Conversations?", "authors": ["Jianfeng Zhu", "Ruoming Jin", "Karin G. Coifman"], "categories": ["cs.CL"], "comment": "21 pages, 12 figures", "summary": "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a\npromising approach for scalable personality assessment from open-ended\nlanguage. However, inferring personality traits remains challenging, and\nearlier work often relied on synthetic data or social media text lacking\npsychometric validity. We introduce a real-world benchmark of 555\nsemi-structured interviews with BFI-10 self-report scores for evaluating\nLLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,\nMeta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item\nprediction and both zero-shot and chain-of-thought prompting for Big Five trait\ninference. All models showed high test-retest reliability, but construct\nvalidity was limited: correlations with ground-truth scores were weak (max\nPearson's $r = 0.27$), interrater agreement was low (Cohen's $\\kappa < 0.10$),\nand predictions were biased toward moderate or high trait levels.\nChain-of-thought prompting and longer input context modestly improved\ndistributional alignment, but not trait-level accuracy. These results\nunderscore limitations in current LLM-based personality inference and highlight\nthe need for evidence-based development for psychological applications."}
{"id": "2507.14769", "pdf": "https://arxiv.org/pdf/2507.14769.pdf", "abs": "https://arxiv.org/abs/2507.14769", "title": "Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs", "authors": ["Ananya Gubbi Mohanbabu", "Yotam Sechayk", "Amy Pavel"], "categories": ["cs.HC"], "comment": "18 pages, 4 figures, 7 tables", "summary": "Modern web interfaces are unnecessarily complex to use as they overwhelm\nusers with excessive text and visuals unrelated to their current goals. This\nproblem particularly impacts screen reader users (SRUs), who navigate content\nsequentially and may spend minutes traversing irrelevant elements before\nreaching desired information compared to vision users (VUs) who visually skim\nin seconds. We present Task Mode, a system that dynamically filters web content\nbased on user-specified goals using large language models to identify and\nprioritize relevant elements while minimizing distractions. Our approach\npreserves page structure while offering multiple viewing modes tailored to\ndifferent access needs. Our user study with 12 participants (6 VUs, 6 SRUs)\ndemonstrates that our approach reduced task completion time for SRUs while\nmaintaining performance for VUs, decreasing the completion time gap between\ngroups from 2x to 1.2x. 11 of 12 participants wanted to use Task Mode in the\nfuture, reporting that Task Mode supported completing tasks with less effort\nand fewer distractions. This work demonstrates how designing new interactions\nsimultaneously for visual and non-visual access can reduce rather than\nreinforce accessibility disparities in future technology created by\nhuman-computer interaction researchers and practitioners."}
{"id": "2507.14372", "pdf": "https://arxiv.org/pdf/2507.14372.pdf", "abs": "https://arxiv.org/abs/2507.14372", "title": "Text-to-SQL for Enterprise Data Analytics", "authors": ["Albert Chen", "Manas Bundele", "Gaurav Ahlawat", "Patrick Stetz", "Zhitao Wang", "Qiang Fei", "Donghoon Jung", "Audrey Chu", "Bharadwaj Jayaraman", "Ayushi Panth", "Yatin Arora", "Sourav Jain", "Renjith Varma", "Alexey Ilin", "Iuliia Melnychuk", "Chelsea Chueh", "Joyan Sil", "Xiaofeng Wang"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25", "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions."}
{"id": "2507.14792", "pdf": "https://arxiv.org/pdf/2507.14792.pdf", "abs": "https://arxiv.org/abs/2507.14792", "title": "SenseSeek Dataset: Multimodal Sensing to Study Information Seeking Behaviors", "authors": ["Kaixin Ji", "Danula Hettiachchi", "Falk Scholer", "Flora D. Salim", "Damiano Spina"], "categories": ["cs.HC"], "comment": "Accepted in Proceedings of the ACM on Interactive, Mobile, Wearable\n  and Ubiquitous Technologies (IMWUT), September 2025", "summary": "Information processing tasks involve complex cognitive mechanisms that are\nshaped by various factors, including individual goals, prior experience, and\nsystem environments. Understanding such behaviors requires a sophisticated and\npersonalized data capture of how one interacts with modern information systems\n(e.g., web search engines). Passive sensors, such as wearables, capturing\nphysiological and behavioral data, have the potential to provide solutions in\nthis context. This paper presents a novel dataset, SenseSeek, designed to\nevaluate the effectiveness of consumer-grade sensors in a complex information\nprocessing scenario: searching via systems (e.g., search engines), one of the\ncommon strategies users employ for information seeking. The SenseSeek dataset\ncomprises data collected from 20 participants, 235 trials of the stimulated\nsearch process, 940 phases of stages in the search process, including the\nrealization of Information Need (IN), Query Formulation (QF), Query Submission\nby Typing (QS-T) or Speaking (QS-S), and Relevance Judgment by Reading (RJ-R)\nor Listening (RJ-L). The data includes Electrodermal Activities (EDA),\nElectroencephalogram (EEG), PUPIL, GAZE, and MOTION data, which were captured\nusing consumer-grade sensors. It also contains 258 features extracted from the\nsensor data, the gaze-annotated screen recordings, and task responses. We\nvalidate the usefulness of the dataset by providing baseline analysis on the\nimpacts of different cognitive intents and interaction modalities on the sensor\ndata, and effectiveness of the data in discriminating the search stages. To our\nknowledge, SenseSeek is the first dataset that characterizes the multiple\nstages involved in information seeking with physiological signals collected\nfrom multiple sensors. We hope this dataset can serve as a reference for future\nresearch on information-seeking behaviors."}
{"id": "2507.14374", "pdf": "https://arxiv.org/pdf/2507.14374.pdf", "abs": "https://arxiv.org/abs/2507.14374", "title": "Error-Aware Curriculum Learning for Biomedical Relation Classification", "authors": ["Sinchani Chakraborty", "Sudeshna Sarkar", "Pawan Goyal"], "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Relation Classification (RC) in biomedical texts is essential for\nconstructing knowledge graphs and enabling applications such as drug\nrepurposing and clinical decision-making. We propose an error-aware\nteacher--student framework that improves RC through structured guidance from a\nlarge language model (GPT-4o). Prediction failures from a baseline student\nmodel are analyzed by the teacher to classify error types, assign difficulty\nscores, and generate targeted remediations, including sentence rewrites and\nsuggestions for KG-based enrichment. These enriched annotations are used to\ntrain a first student model via instruction tuning. This model then annotates a\nbroader dataset with difficulty scores and remediation-enhanced inputs. A\nsecond student is subsequently trained via curriculum learning on this dataset,\nordered by difficulty, to promote robust and progressive learning. We also\nconstruct a heterogeneous biomedical knowledge graph from PubMed abstracts to\nsupport context-aware RC. Our approach achieves new state-of-the-art\nperformance on 4 of 5 PPI datasets and the DDI dataset, while remaining\ncompetitive on ChemProt."}
{"id": "2507.14818", "pdf": "https://arxiv.org/pdf/2507.14818.pdf", "abs": "https://arxiv.org/abs/2507.14818", "title": "Understanding How Visually Impaired Players Socialize in Mobile Games", "authors": ["Zihe Ran", "Xiyu Li", "Qing Xiao", "Yanyun Wang", "Franklin Mingzhe Li", "Zhicong Lu"], "categories": ["cs.HC", "cs.CY"], "comment": "16 pages, 1 table, accepted by ASSETS25", "summary": "Mobile games are becoming a vital medium for social interaction, offering a\nplatform that transcends geographical boundaries. An increasing number of\nvisually impaired individuals are engaging in mobile gaming to connect,\ncollaborate, compete, and build friendships. In China, visually impaired\ncommunities face significant social challenges in offline settings, making\nmobile games a crucial avenue for socialization. However, the design of mobile\ngames and their mapping to real-world environments significantly shape their\nsocial gaming experiences. This study explores how visually impaired players in\nChina navigate socialization and integrate into gaming communities. Through\ninterviews with 30 visually impaired players, we found that while mobile games\nfulfill many of their social needs, technological barriers and insufficient\naccessibility features, and internal community divisions present significant\nchallenges to their participation. This research sheds light on their social\nexperiences and offers insights for designing more inclusive and accessible\nmobile games."}
{"id": "2507.14430", "pdf": "https://arxiv.org/pdf/2507.14430.pdf", "abs": "https://arxiv.org/abs/2507.14430", "title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display", "authors": ["Xiaolin Yan", "Yangxing Liu", "Jiazhang Zheng", "Chi Liu", "Mingyu Du", "Caisheng Chen", "Haoyang Liu", "Ming Ding", "Yuan Li", "Qiuping Liao", "Linfeng Li", "Zhili Mei", "Siyu Wan", "Li Li", "Ruyi Zhong", "Jiangling Yu", "Xule Liu", "Huihui Hu", "Jiameng Yue", "Ruohui Cheng", "Qi Yang", "Liangqing Wu", "Ke Zhu", "Chi Zhang", "Chufei Jing", "Yifan Zhou", "Yan Liang", "Dongdong Li", "Zhaohui Wang", "Bin Zhao", "Mingzhou Wu", "Mingzhong Zhou", "Peng Du", "Zuomin Liao", "Chao Dai", "Pengfei Liang", "Xiaoguang Zhu", "Yu Zhang", "Yu Gu", "Kun Pan", "Yuan Wu", "Yanqing Guan", "Shaojing Wu", "Zikang Feng", "Xianze Ma", "Peishan Cheng", "Wenjuan Jiang", "Jing Ba", "Huihao Yu", "Zeping Hu", "Yuan Xu", "Zhiwei Liu", "He Wang", "Zhenguo Lin", "Ming Liu", "Yanhong Meng"], "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Large language models (LLMs) have recently achieved significant advances in\nreasoning and demonstrated their advantages in solving challenging problems.\nYet, their effectiveness in the semiconductor display industry remains limited\ndue to a lack of domain-specific training and expertise. To bridge this gap, we\npresent X-Intelligence 3.0, the first high-performance reasoning model\nspecifically developed for the semiconductor display industry. This model is\ndesigned to deliver expert-level understanding and reasoning for the industry's\ncomplex challenges. Leveraging a carefully curated industry knowledge base, the\nmodel undergoes supervised fine-tuning and reinforcement learning to enhance\nits reasoning and comprehension capabilities. To further accelerate\ndevelopment, we implemented an automated evaluation framework that simulates\nexpert-level assessments. We also integrated a domain-specific\nretrieval-augmented generation (RAG) mechanism, resulting in notable\nperformance gains on benchmark datasets. Despite its relatively compact size of\n32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B\nacross multiple evaluations. This demonstrates its exceptional efficiency and\nestablishes it as a powerful solution to the longstanding reasoning challenges\nfaced by the semiconductor display industry."}
{"id": "2507.14846", "pdf": "https://arxiv.org/pdf/2507.14846.pdf", "abs": "https://arxiv.org/abs/2507.14846", "title": "Progressive Sentences: Combining the Benefits of Word and Sentence Learning", "authors": ["Nuwan Janaka", "Shengdong Zhao", "Ashwin Ram", "Ruoxin Sun", "Sherisse Tan Jing Wen", "Danae Li", "David Hsu"], "categories": ["cs.HC", "cs.CY"], "comment": "12 pages, 4 figures, 4 tables", "summary": "The rapid evolution of lightweight consumer augmented reality (AR) smart\nglasses (a.k.a. optical see-through head-mounted displays) offers novel\nopportunities for learning, particularly through their unique capability to\ndeliver multimodal information in just-in-time, micro-learning scenarios. This\nresearch investigates how such devices can support mobile second-language\nacquisition by presenting progressive sentence structures in multimodal\nformats. In contrast to the commonly used vocabulary (i.e., word) learning\napproach for novice learners, we present a \"progressive presentation\" method\nthat combines both word and sentence learning by sequentially displaying\nsentence components (subject, verb, object) while retaining prior context.\nPilot and formal studies revealed that progressive presentation enhances\nrecall, particularly in mobile scenarios such as walking. Additionally,\nincorporating timed gaps between word presentations further improved learning\neffectiveness under multitasking conditions. Our findings demonstrate the\nutility of progressive presentation and provide usage guidelines for\neducational applications-even during brief, on-the-go learning moments."}
{"id": "2507.14578", "pdf": "https://arxiv.org/pdf/2507.14578.pdf", "abs": "https://arxiv.org/abs/2507.14578", "title": "XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification", "authors": ["Sachin Yadav", "Dominik Schlechtweg"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "We propose XL-DURel, a finetuned, multilingual Sentence Transformer model\noptimized for ordinal Word-in-Context classification. We test several loss\nfunctions for regression and ranking tasks managing to outperform previous\nmodels on ordinal and binary data with a ranking objective based on angular\ndistance in complex space. We further show that binary WiC can be treated as a\nspecial case of ordinal WiC and that optimizing models for the general ordinal\ntask improves performance on the more specific binary task. This paves the way\nfor a unified treatment of WiC modeling across different task formulations."}
{"id": "2507.14859", "pdf": "https://arxiv.org/pdf/2507.14859.pdf", "abs": "https://arxiv.org/abs/2507.14859", "title": "Holistic Specification of the Human Digital Twin: Stakeholders, Users, Functionalities, and Applications", "authors": ["Nils Mandischer", "Alexander Atanasyan", "Ulrich Dahmen", "Michael Schluse", "Jürgen Rossmann", "Lars Mikelsons"], "categories": ["cs.HC", "cs.SY", "eess.SY"], "comment": "This work was accepted by the IEEE International Conference on\n  Systems, Man, and Cybernetics (SMC), Vienna, Austria, 2025", "summary": "The digital twin of humans is a relatively new concept. While many diverse\ndefinitions, architectures, and applications exist, a clear picture is missing\non what, in fact, makes a human digital twin. Within this context, researchers\nand industrial use-case owners alike are unaware about the market potential of\nthe - at the moment - rather theoretical construct. In this work, we draw a\nholistic vision of the human digital twin, and derive the specification of this\nholistic human digital twin in form of requirements, stakeholders, and users.\nFor each group of users, we define exemplary applications that fall into the\nsix levels of functionality: store, analyze, personalize, predict, control, and\noptimize. The functionality levels facilitate an abstraction of abilities of\nthe human digital twin. From the manifold applications, we discuss three in\ndetail to showcase the feasibility of the abstraction levels and the analysis\nof stakeholders and users. Based on the deep discussion, we derive a\ncomprehensive list of requirements on the holistic human digital twin. These\nconsiderations shall be used as a guideline for research and industries for the\nimplementation of human digital twins, particularly in context of reusability\nin multiple target applications."}
{"id": "2507.14579", "pdf": "https://arxiv.org/pdf/2507.14579.pdf", "abs": "https://arxiv.org/abs/2507.14579", "title": "Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 5 pages", "summary": "Detecting collaborative problem solving (CPS) indicators from dialogue using\nmachine learning techniques is a significant challenge for the field of AI in\nEducation. Recent studies have explored the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) models on transcription data to\nreliably detect meaningful CPS indicators. A notable advancement involved the\nmultimodal BERT variant, AudiBERT, which integrates speech and\nacoustic-prosodic audio features to enhance CPS diagnosis. Although initial\nresults demonstrated multimodal improvements, the statistical significance of\nthese enhancements remained unclear, and there was insufficient guidance on\nleveraging human-AI complementarity for CPS diagnosis tasks. This workshop\npaper extends the previous research by highlighting that the AudiBERT model not\nonly improved the classification of classes that were sparse in the dataset,\nbut it also had statistically significant class-wise improvements over the BERT\nmodel for classifications in the social-cognitive dimension. However, similar\nsignificant class-wise improvements over the BERT model were not observed for\nclassifications in the affective dimension. A correlation analysis highlighted\nthat larger training data was significantly associated with higher recall\nperformance for both the AudiBERT and BERT models. Additionally, the precision\nof the BERT model was significantly associated with high inter-rater agreement\namong human coders. When employing the BERT model to diagnose indicators within\nthese subskills that were well-detected by the AudiBERT model, the performance\nacross all indicators was inconsistent. We conclude the paper by outlining a\nstructured approach towards achieving human-AI complementarity for CPS\ndiagnosis, highlighting the crucial inclusion of model explainability to\nsupport human agency and engagement in the reflective coding process."}
{"id": "2507.14944", "pdf": "https://arxiv.org/pdf/2507.14944.pdf", "abs": "https://arxiv.org/abs/2507.14944", "title": "LEKIA: A Framework for Architectural Alignment via Expert Knowledge Injection", "authors": ["Boning Zhao", "Yutong Hu"], "categories": ["cs.HC"], "comment": null, "summary": "Deploying Large Language Models (LLMs) in high-stakes domains is impeded by a\ndual challenge: the need for deep, dynamic expert knowledge injection and\nnuanced value alignment. Prevailing paradigms often address these challenges\nseparately, creating a persistent tension between knowledge and alignment;\nknowledge-focused methods like Retrieval-Augmented Generation (RAG) have\nlimited deep alignment capabilities, while alignment-focused methods like\nReinforcement Learning from Human Feedback (RLHF) struggle with the agile\ninjection of expert wisdom. This paper introduces a new collaborative\nphilosophy, Expert-owned AI behavior design, realized through Architectural\nAlignment-a paradigm that unifies these two goals within a single framework\ncalled the Layered Expert Knowledge Injection Architecture (LEKIA). LEKIA\noperates as an intelligent intermediary that guides an LLM's reasoning process\nwithout altering its weights, utilizing a three-tiered structure: a Theoretical\nLayer for core principles, a Practical Layer for exemplary cases, and an\nEvaluative Layer for real-time, value-aligned self-correction. We demonstrate\nthe efficacy of this paradigm through the successful implementation of a\nLEKIA-based psychological support assistant for the special education field.\nOur work presents a path toward more responsible and expert-driven AI,\nempowering domain specialists to directly architect AI behavior and resolve the\ntension between knowledge and alignment."}
{"id": "2507.14584", "pdf": "https://arxiv.org/pdf/2507.14584.pdf", "abs": "https://arxiv.org/abs/2507.14584", "title": "Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 6 pages, 2 figures", "summary": "The use of Bidirectional Encoder Representations from Transformers (BERT)\nmodel and its variants for classifying collaborative problem solving (CPS) has\nbeen extensively explored within the AI in Education community. However,\nlimited attention has been given to understanding how individual tokenised\nwords in the dataset contribute to the model's classification decisions.\nEnhancing the explainability of BERT-based CPS diagnostics is essential to\nbetter inform end users such as teachers, thereby fostering greater trust and\nfacilitating wider adoption in education. This study undertook a preliminary\nstep towards model transparency and explainability by using SHapley Additive\nexPlanations (SHAP) to examine how different tokenised words in transcription\ndata contributed to a BERT model's classification of CPS processes. The\nfindings suggested that well-performing classifications did not necessarily\nequate to a reasonable explanation for the classification decisions. Particular\ntokenised words were used frequently to affect classifications. The analysis\nalso identified a spurious word, which contributed positively to the\nclassification but was not semantically meaningful to the class. While such\nmodel transparency is unlikely to be useful to an end user to improve their\npractice, it can help them not to overrely on LLM diagnostics and ignore their\nhuman expertise. We conclude the workshop paper by noting that the extent to\nwhich the model appropriately uses the tokens for its classification is\nassociated with the number of classes involved. It calls for an investigation\ninto the exploration of ensemble model architectures and the involvement of\nhuman-AI complementarity for CPS diagnosis, since considerable human reasoning\nis still required for fine-grained discrimination of CPS subskills."}
{"id": "2507.14947", "pdf": "https://arxiv.org/pdf/2507.14947.pdf", "abs": "https://arxiv.org/abs/2507.14947", "title": "Echoes of the Land: An Interactive Installation Based on Physical Model of Earthquake", "authors": ["Ivan C. H. Liu", "Chung-En Hao", "Jing Xie"], "categories": ["cs.HC", "nlin.AO"], "comment": "7 pages, 8 figures, submitted to Leonardo", "summary": "Echoes of the Land is an interactive installation that transforms seismic\ndynamics into a multisensory experience through a scientifically grounded\nspring-block model. Simulating earthquake recurrence and self-organized\ncriticality, the work generates real-time sound and light via motion capture\nand concatenative granular synthesis. Each block acts as an agent, producing\nemergent audiovisual cascades that visualize the physics of rupture and\nthreshold behavior. This work exemplifies the amalgamation of scientific\nknowledge and artistic practice, opening new avenues for novel forms of musical\ninstrument and narrative medium, while inviting further investigation into the\nintersection of emergent complexity, aesthetics and interactivity."}
{"id": "2507.14590", "pdf": "https://arxiv.org/pdf/2507.14590.pdf", "abs": "https://arxiv.org/abs/2507.14590", "title": "Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification", "authors": ["Łukasz Radliński", "Mateusz Guściora", "Jan Kocoń"], "categories": ["cs.CL", "cs.AI"], "comment": "International Conference on Computational Science 2025", "summary": "Numerous domain-specific machine learning tasks struggle with data scarcity\nand class imbalance. This paper systematically explores data augmentation\nmethods for NLP, particularly through large language models like GPT. The\npurpose of this paper is to examine and evaluate whether traditional methods\nsuch as paraphrasing and backtranslation can leverage a new generation of\nmodels to achieve comparable performance to purely generative methods. Methods\naimed at solving the problem of data scarcity and utilizing ChatGPT were\nchosen, as well as an exemplary dataset. We conducted a series of experiments\ncomparing four different approaches to data augmentation in multiple\nexperimental setups. We then evaluated the results both in terms of the quality\nof generated data and its impact on classification performance. The key\nfindings indicate that backtranslation and paraphrasing can yield comparable or\neven better results than zero and a few-shot generation of examples."}
{"id": "2507.14961", "pdf": "https://arxiv.org/pdf/2507.14961.pdf", "abs": "https://arxiv.org/abs/2507.14961", "title": "Emphasizing Deliberation and Critical Thinking in an AI Hype World", "authors": ["Katja Rogers"], "categories": ["cs.HC"], "comment": "Accepted at CHI 2025 workshop: \"Resisting AI Solutionism: Where Do We\n  Go From Here?\" (https://doi.org/10.1145/3706599.3706732)", "summary": "AI solutionism is accelerated and substantiated by hype and HCI's elevation\nof novelty. Banning or abandoning technology is unlikely to work and probably\nnot beneficial on the whole either -- but slow(er), deliberate use together\nwith conscientious, critical engagement and non-engagement may help us navigate\na post-AI hype world while contributing to a solid knowledge foundation and\nreducing harmful impacts in education and research."}
{"id": "2507.14615", "pdf": "https://arxiv.org/pdf/2507.14615.pdf", "abs": "https://arxiv.org/abs/2507.14615", "title": "Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper", "authors": ["Fred Mutisya", "Shikoh Gitau", "Christine Syovata", "Diana Oigara", "Ibrahim Matende", "Muna Aden", "Munira Ali", "Ryan Nyotu", "Diana Marion", "Job Nyangena", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha", "Eric Mibuari", "Jean Philbert Nsengemana", "Talkmore Chidede"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 6 figs, 6 tables. Companion methods paper forthcoming", "summary": "Large Language Models(LLMs) hold promise for improving healthcare access in\nlow-resource settings, but their effectiveness in African primary care remains\nunderexplored. We present a methodology for creating a benchmark dataset and\nevaluation framework focused on Kenyan Level 2 and 3 clinical care. Our\napproach uses retrieval augmented generation (RAG) to ground clinical questions\nin Kenya's national guidelines, ensuring alignment with local standards. These\nguidelines were digitized, chunked, and indexed for semantic retrieval. Gemini\nFlash 2.0 Lite was then prompted with guideline excerpts to generate realistic\nclinical scenarios, multiple-choice questions, and rationale based answers in\nEnglish and Swahili. Kenyan physicians co-created and refined the dataset, and\na blinded expert review process ensured clinical accuracy, clarity, and\ncultural appropriateness. The resulting Alama Health QA dataset includes\nthousands of regulator-aligned question answer pairs across common outpatient\nconditions. Beyond accuracy, we introduce evaluation metrics that test clinical\nreasoning, safety, and adaptability such as rare case detection (Needle in the\nHaystack), stepwise logic (Decision Points), and contextual adaptability.\nInitial results reveal significant performance gaps when LLMs are applied to\nlocalized scenarios, consistent with findings that LLM accuracy is lower on\nAfrican medical content than on US-based benchmarks. This work offers a\nreplicable model for guideline-driven, dynamic benchmarking to support safe AI\ndeployment in African health systems."}
{"id": "2507.15033", "pdf": "https://arxiv.org/pdf/2507.15033.pdf", "abs": "https://arxiv.org/abs/2507.15033", "title": "'A Little Bubble of Friends': An Analysis of LGBTQ+ Pandemic Experiences Using Reddit Data", "authors": ["Dhruvee Birla", "Nazia Akhtar"], "categories": ["cs.HC"], "comment": null, "summary": "Social media was one of the most popular forms of communication among young\npeople with digital access during the pandemic. Consequently, crucial debates\nand discussions about the pandemic crisis have also developed on social media\nplatforms, making them a great primary source to study the experiences of\nspecific groups and communities during the pandemic. This study involved\nresearch using LDA topic modeling and sentiment analysis on data obtained from\nthe social media platform Reddit to understand the themes and attitudes in\ncirculation within five subreddits devoted to LGBTQ+ experiences and issues. In\nthe process, we attempt to make sense of the role that Reddit may have played\nin the lives of LGBTQ+ people who were online during the pandemic, and whether\nthis was marked by any continuities or discontinuities from before the pandemic\nperiod."}
{"id": "2507.14640", "pdf": "https://arxiv.org/pdf/2507.14640.pdf", "abs": "https://arxiv.org/abs/2507.14640", "title": "Linear Relational Decoding of Morphology in Language Models", "authors": ["Eric Xia", "Jugal Kalita"], "categories": ["cs.CL"], "comment": null, "summary": "A two-part affine approximation has been found to be a good approximation for\ntransformer computations over certain subject object relations. Adapting the\nBigger Analogy Test Set, we show that the linear transformation Ws, where s is\na middle layer representation of a subject token and W is derived from model\nderivatives, is also able to accurately reproduce final object states for many\nrelations. This linear technique is able to achieve 90% faithfulness on\nmorphological relations, and we show similar findings multi-lingually and\nacross models. Our findings indicate that some conceptual relationships in\nlanguage models, such as morphology, are readily interpretable from latent\nspace, and are sparsely encoded by cross-layer linear transformations."}
{"id": "2507.15041", "pdf": "https://arxiv.org/pdf/2507.15041.pdf", "abs": "https://arxiv.org/abs/2507.15041", "title": "Visibility vs. Engagement: How Two Indian News Websites Reported on LGBTQ+ Individuals and Communities during the Pandemic", "authors": ["Dhruvee Birla", "Nazia Akhtar"], "categories": ["cs.HC"], "comment": null, "summary": "In India, online news media outlets were an important source of information\nfor people with digital access during the COVID-19 pandemic. In India, where\n\"transgender\" was legally recognised as a category only in 2014, and same-sex\nmarriages are yet to be legalised, it becomes crucial to analyse whether and\nhow they reported the lived realities of vulnerable LGBTQ+ communities during\nthe pandemic. This study analysed articles from online editions of two\nEnglish-language newspaper websites, which differed vastly in their circulation\nfigures-The Times of India and The Indian Express. The results of our study\nsuggest that these newspaper websites published articles surrounding various\naspects of the lives of LGBTQ+ individuals with a greater focus on transgender\ncommunities. However, they lacked quality and depth. Focusing on the period\nspanning March 2020 to August 2021, we analysed articles using sentiment\nanalysis and topic modelling. We also compared our results to the period before\nthe pandemic (January 2019 - December 2019) to understand the shift in topics,\nsentiments, and stances across the two newspaper websites. A manual analysis of\nthe articles indicated that the language used in certain articles by The Times\nof India was transphobic and obsolete. Our study captures the visibility and\nrepresentation of the LGBTQ+ communities in Indian newspaper websites during\nthe pandemic."}
{"id": "2507.14649", "pdf": "https://arxiv.org/pdf/2507.14649.pdf", "abs": "https://arxiv.org/abs/2507.14649", "title": "Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs", "authors": ["Minsuh Joo", "Hyunsoo Cho"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the outstanding performance of large language models (LLMs) across\nvarious NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate\nresponses--remains as a critical problem as it can be directly connected to a\ncrisis of building safe and reliable LLMs. Uncertainty estimation is primarily\nused to measure hallucination levels in LLM responses so that correct and\nincorrect answers can be distinguished clearly. This study proposes an\neffective uncertainty estimation approach, \\textbf{Cl}ust\\textbf{e}ring-based\nsem\\textbf{an}tic con\\textbf{s}ist\\textbf{e}ncy (\\textbf{Cleanse}). Cleanse\nquantifies the uncertainty with the proportion of the intra-cluster consistency\nin the total consistency between LLM hidden embeddings which contain adequate\nsemantic information of generations, by employing clustering. The effectiveness\nof Cleanse for detecting hallucination is validated using four off-the-shelf\nmodels, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two\nquestion-answering benchmarks, SQuAD and CoQA."}
{"id": "2507.15049", "pdf": "https://arxiv.org/pdf/2507.15049.pdf", "abs": "https://arxiv.org/abs/2507.15049", "title": "Beyond Visual Line of Sight: UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence", "authors": ["Andres Navarro", "Carlos de Quinto", "José Alberto Hernández"], "categories": ["cs.HC"], "comment": null, "summary": "Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as\nagile, intelligent nodes capable of advanced analytics and instantaneous\nsituational awareness. This article introduces a budget-friendly quadcopter\nplatform that unites 5G communications, edge-based processing, and AI to tackle\ncore challenges in NTN scenarios. Outfitted with a panoramic camera, robust\nonboard computation, and LLMs, the drone system delivers seamless object\nrecognition, contextual analysis, and immersive operator experiences through\nvirtual reality VR technology. Field evaluations confirm the platform's ability\nto process visual streams with low latency and sustain robust 5G links. Adding\nLLMs further streamlines operations by extracting actionable insights and\nrefining collected data for decision support. Demonstrated use cases, including\nemergency response, infrastructure assessment, and environmental surveillance,\nunderscore the system's adaptability in demanding contexts."}
{"id": "2507.14664", "pdf": "https://arxiv.org/pdf/2507.14664.pdf", "abs": "https://arxiv.org/abs/2507.14664", "title": "Mangosteen: An Open Thai Corpus for Language Model Pretraining", "authors": ["Wannaphong Phatthiyaphaibun", "Can Udomcharoenchaikit", "Pakpoom Singkorapoom", "Kunat Pipatanakul", "Ekapol Chuangsuwanich", "Peerat Limkonchotiwat", "Sarana Nutanong"], "categories": ["cs.CL"], "comment": "Work in Progress.All artifacts in this papers:\n  https://huggingface.co/collections/aisingapore/wangchanlion-v3-687a362d8f0ea2fe4077c6b3", "summary": "Pre-training data shapes a language model's quality, but raw web text is\nnoisy and demands careful cleaning. Existing large-scale corpora rely on\nEnglish-centric or language-agnostic pipelines whose heuristics do not capture\nThai script or cultural nuances, leaving risky material such as gambling\ncontent untreated. Prior Thai-specific efforts customize pipelines or build new\nones, yet seldom release their data or document design choices, hindering\nreproducibility and raising the question of how to construct a transparent,\nhigh-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai\ncorpus built through a Thai-adapted Dolma pipeline that includes custom\nrule-based language ID, revised C4/Gopher quality filters, and Thai-trained\ncontent filters, plus curated non-web sources such as Wikipedia, Royal Gazette\ntexts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic\nablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M\ndocuments while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION\nmodel continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and\nLlama-3.1 by about four points on Thai benchmarks. We release the full pipeline\ncode, cleaning manifests, corpus snapshot, and all checkpoints, providing a\nfully reproducible foundation for future Thai and regional LLM research."}
{"id": "2507.15072", "pdf": "https://arxiv.org/pdf/2507.15072.pdf", "abs": "https://arxiv.org/abs/2507.15072", "title": "NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments", "authors": ["Maisha Maimuna", "Minhaz Bin Farukee", "Sama Nikanfar", "Mahfuza Siddiqua", "Ayon Roy", "Fillia Makedon"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Industrial warehouses are congested with moving forklifts, shelves and\npersonnel, making robot teleoperation particularly risky and demanding for\nblind and low-vision (BLV) operators. Although accessible teleoperation plays a\nkey role in inclusive workforce participation, systematic research on its use\nin industrial environments is limited, and few existing studies barely address\nmultimodal guidance designed for BLV users. We present a novel multimodal\nguidance simulator that enables BLV users to control a mobile robot through a\nhigh-fidelity warehouse environment while simultaneously receiving synchronized\nvisual, auditory, and haptic feedback. The system combines a navigation mesh\nwith regular re-planning so routes remain accurate avoiding collisions as\nforklifts and human avatars move around the warehouse. Users with low vision\nare guided with a visible path line towards destination; navigational voice\ncues with clockwise directions announce upcoming turns, and finally\nproximity-based haptic feedback notifies the users of static and moving\nobstacles in the path. This real-time, closed-loop system offers a repeatable\ntestbed and algorithmic reference for accessible teleoperation research. The\nsimulator's design principles can be easily adapted to real robots due to the\nalignment of its navigation, speech, and haptic modules with commercial\nhardware, supporting rapid feasibility studies and deployment of inclusive\ntelerobotic tools in actual warehouses."}
{"id": "2507.14681", "pdf": "https://arxiv.org/pdf/2507.14681.pdf", "abs": "https://arxiv.org/abs/2507.14681", "title": "Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care", "authors": ["Vinicius Anjos de Almeida", "Vinicius de Camargo", "Raquel Gómez-Bravo", "Egbert van der Haring", "Kees van Boven", "Marcelo Finger", "Luis Fernandez Lopez"], "categories": ["cs.CL"], "comment": "To be submitted to peer-reviewed journal. 33 pages, 10 figures\n  (including appendix), 15 tables (including appendix). For associated code\n  repository, see https://github.com/almeidava93/llm-as-code-selectors-paper", "summary": "Background: Medical coding structures healthcare data for research, quality\nmonitoring, and policy. This study assesses the potential of large language\nmodels (LLMs) to assign ICPC-2 codes using the output of a domain-specific\nsearch engine.\n  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each\nannotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's\ntext-embedding-3-large) retrieved candidates from 73,563 labeled concepts.\nThirty-three LLMs were prompted with each query and retrieved results to select\nthe best-matching ICPC-2 code. Performance was evaluated using F1-score, along\nwith token usage, cost, response time, and format adherence.\n  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top\nperformers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever\noptimization can improve performance by up to 4 points. Most models returned\nvalid codes in the expected format, with reduced hallucinations. Smaller models\n(<3B) struggled with formatting and input length.\n  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even\nwithout fine-tuning. This work offers a benchmark and highlights challenges,\nbut findings are limited by dataset scope and setup. Broader, multilingual,\nend-to-end evaluations are needed for clinical validation."}
{"id": "2507.15081", "pdf": "https://arxiv.org/pdf/2507.15081.pdf", "abs": "https://arxiv.org/abs/2507.15081", "title": "\"If I were in Space\": Understanding and Adapting to Social Isolation through Designing Collaborative Narratives", "authors": ["Qi Gong", "Ximing Shen", "Ziyou Yin", "Yaning Li", "Ray Lc"], "categories": ["cs.HC"], "comment": null, "summary": "Social isolation can lead to pervasive health issues like anxiety and\nloneliness. Previous work focused on physical interventions like exercise and\nteleconferencing, but overlooked the narrative potential of adaptive\nstrategies. To address this, we designed a collaborative online storytelling\nexperience in social VR, enabling participants in isolation to design an\nimaginary space journey as a metaphor for quarantine, in order to learn about\ntheir isolation adaptation strategies in the process. Eighteen individuals\nparticipated during real quarantine undertaken a virtual role-play experience,\ndesigning their own spaceship rooms and engaging in collaborative activities\nthat revealed creative adaptative strategies. Qualitative analyses of\nparticipant designs, transcripts, and interactions revealed how they coped with\nisolation, and how the engagement unexpectedly influenced their adaptation\nprocess. This study shows how designing playful narrative experiences, rather\nthan solution-driven approaches, can serve as probes to surface how people\nnavigate social isolation."}
{"id": "2507.14683", "pdf": "https://arxiv.org/pdf/2507.14683.pdf", "abs": "https://arxiv.org/abs/2507.14683", "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization", "authors": ["Xingxuan Li", "Yao Xiao", "Dianwen Ng", "Hai Ye", "Yue Deng", "Xiang Lin", "Bin Wang", "Zhanfeng Mo", "Chong Zhang", "Yueyi Zhang", "Zonglin Yang", "Ruilin Li", "Lei Lei", "Shihao Xu", "Han Zhao", "Weiling Chen", "Feng Ji", "Lidong Bing"], "categories": ["cs.CL"], "comment": "Technical report", "summary": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement."}
{"id": "2507.15202", "pdf": "https://arxiv.org/pdf/2507.15202.pdf", "abs": "https://arxiv.org/abs/2507.15202", "title": "TalkLess: Blending Extractive and Abstractive Speech Summarization for Editing Speech to Preserve Content and Style", "authors": ["Karim Benharrak", "Puyuan Peng", "Amy Pavel"], "categories": ["cs.HC"], "comment": null, "summary": "Millions of people listen to podcasts, audio stories, and lectures, but\nediting speech remains tedious and time-consuming. Creators remove unnecessary\nwords, cut tangential discussions, and even re-record speech to make recordings\nconcise and engaging. Prior work automatically summarized speech by removing\nfull sentences (extraction), but rigid extraction limits expressivity. AI tools\ncan summarize then re-synthesize speech (abstraction), but abstraction strips\nthe speaker's style. We present TalkLess, a system that flexibly combines\nextraction and abstraction to condense speech while preserving its content and\nstyle. To edit speech, TalkLess first generates possible transcript edits,\nselects edits to maximize compression, coverage, and audio quality, then uses a\nspeech editing model to translate transcript edits into audio edits. TalkLess's\ninterface provides creators control over automated edits by separating\nlow-level wording edits (via the compression pane) from major content edits\n(via the outline pane). TalkLess achieves higher coverage and removes more\nspeech errors than a state-of-the-art extractive approach. A comparison study\n(N=12) showed that TalkLess significantly decreased cognitive load and editing\neffort in speech editing. We further demonstrate TalkLess's potential in an\nexploratory study (N=3) where creators edited their own speech."}
{"id": "2507.14688", "pdf": "https://arxiv.org/pdf/2507.14688.pdf", "abs": "https://arxiv.org/abs/2507.14688", "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations", "authors": ["Mohammed Alkhowaiter", "Norah Alshahrani", "Saied Alshahrani", "Reem I. Masoud", "Alaa Alzahrani", "Deema Alnuhait", "Emad A. Alghamdi", "Khalid Almubarak"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., persona and system prompts); (3)\nAlignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic LLMs and applications while providing\nconcrete recommendations for future efforts in post-training dataset\ndevelopment."}
{"id": "2507.15244", "pdf": "https://arxiv.org/pdf/2507.15244.pdf", "abs": "https://arxiv.org/abs/2507.15244", "title": "How Does Empirical Research Facilitate Creation Tool Design? A Data Video Perspective", "authors": ["Leixian Shen", "Leni Yang", "Haotian Li", "Yun Wang", "Yuyu Luo", "Huamin Qu"], "categories": ["cs.HC"], "comment": null, "summary": "Empirical research in creative design deepens our theoretical understanding\nof design principles and perceptual effects, offering valuable guidance for\ninnovating creation tools. However, how these empirical insights currently\ninfluence the development of creation tools, and how their integration can be\nenhanced in the future, remains insufficiently understood. In this paper, we\naim to unveil the gap through a case study on data videos, a prominent and\nwide-spread medium for effective data storytelling. To achieve the goal, we\nconducted a comprehensive analysis of 46 empirical research papers and 48\ncreation tool papers on data video, complemented by interviews with 11 experts.\nBuilding upon a systematic collection and structured characterization of\nempirical research by their methodologies (e.g., corpus analysis, comparative\nevaluations) and component focus (e.g., visuals, motions, narratives, audio),\nwe conducted a context-aware citation analysis and revealed a taxonomy of\nrecurring patterns in how empirical findings inform tool design across citation\nfunctions (e.g., problem framing, technical reference). Expert interviews\nfurther uncovered researchers' practice patterns in applying empirical findings\n(e.g., adaptation, synthesis, iteration, etc.) and identified key factors\ninfluencing applicability, such as contextual relevance, granularity matching,\nclarity, credibility, and feasibility. Finally, we derive suggestions and\ndiscuss future opportunities to foster closer mutual engagement between\nempirical and tool research, aiming to reinforce the theoretical grounding of\ncreation tools and enhance the practical impact of empirical research."}
{"id": "2507.14693", "pdf": "https://arxiv.org/pdf/2507.14693.pdf", "abs": "https://arxiv.org/abs/2507.14693", "title": "Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation", "authors": ["Amina Dzafic", "Merve Kavut", "Ulya Bayram"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "This manuscript has been submitted to the IEEE Journal of Biomedical\n  and Health Informatics", "summary": "Suicidal ideation detection is critical for real-time suicide prevention, yet\nits progress faces two under-explored challenges: limited language coverage and\nunreliable annotation practices. Most available datasets are in English, but\neven among these, high-quality, human-annotated data remains scarce. As a\nresult, many studies rely on available pre-labeled datasets without examining\ntheir annotation process or label reliability. The lack of datasets in other\nlanguages further limits the global realization of suicide prevention via\nartificial intelligence (AI). In this study, we address one of these gaps by\nconstructing a novel Turkish suicidal ideation corpus derived from social media\nposts and introducing a resource-efficient annotation framework involving three\nhuman annotators and two large language models (LLMs). We then address the\nremaining gaps by performing a bidirectional evaluation of label reliability\nand model consistency across this dataset and three popular English suicidal\nideation detection datasets, using transfer learning through eight pre-trained\nsentiment and emotion classifiers. These transformers help assess annotation\nconsistency and benchmark model performance against manually labeled data. Our\nfindings underscore the need for more rigorous, language-inclusive approaches\nto annotation and evaluation in mental health natural language processing (NLP)\nwhile demonstrating the questionable performance of popular models with\nzero-shot transfer learning. We advocate for transparency in model training and\ndataset construction in mental health NLP, prioritizing data and model\nreliability."}
{"id": "2507.15355", "pdf": "https://arxiv.org/pdf/2507.15355.pdf", "abs": "https://arxiv.org/abs/2507.15355", "title": "Efficient Visual Appearance Optimization by Learning from Prior Preferences", "authors": ["Zhipeng Li", "Yi-Chi Liao", "Christian Holz"], "categories": ["cs.HC", "cs.LG"], "comment": "24 pages, UIST'25", "summary": "Adjusting visual parameters such as brightness and contrast is common in our\neveryday experiences. Finding the optimal parameter setting is challenging due\nto the large search space and the lack of an explicit objective function,\nleaving users to rely solely on their implicit preferences. Prior work has\nexplored Preferential Bayesian Optimization (PBO) to address this challenge,\ninvolving users to iteratively select preferred designs from candidate sets.\nHowever, PBO often requires many rounds of preference comparisons, making it\nmore suitable for designers than everyday end-users. We propose Meta-PO, a\nnovel method that integrates PBO with meta-learning to improve sample\nefficiency. Specifically, Meta-PO infers prior users' preferences and stores\nthem as models, which are leveraged to intelligently suggest design candidates\nfor the new users, enabling faster convergence and more personalized results.\nAn experimental evaluation of our method for appearance design tasks on 2D and\n3D content showed that participants achieved satisfactory appearance in 5.86\niterations using Meta-PO when participants shared similar goals with a\npopulation (e.g., tuning for a ``warm'' look) and in 8 iterations even\ngeneralizes across divergent goals (e.g., from ``vintage'', ``warm'', to\n``holiday''). Meta-PO makes personalized visual optimization more applicable to\nend-users through a generalizable, more efficient optimization conditioned on\npreferences, with the potential to scale interface personalization more\nbroadly."}
{"id": "2507.14741", "pdf": "https://arxiv.org/pdf/2507.14741.pdf", "abs": "https://arxiv.org/abs/2507.14741", "title": "Disparities in Peer Review Tone and the Role of Reviewer Anonymity", "authors": ["Maria Sahakyan", "Bedoor AlShebli"], "categories": ["cs.CL"], "comment": null, "summary": "The peer review process is often regarded as the gatekeeper of scientific\nintegrity, yet increasing evidence suggests that it is not immune to bias.\nAlthough structural inequities in peer review have been widely debated, much\nless attention has been paid to the subtle ways in which language itself may\nreinforce disparities. This study undertakes one of the most comprehensive\nlinguistic analyses of peer review to date, examining more than 80,000 reviews\nin two major journals. Using natural language processing and large-scale\nstatistical modeling, it uncovers how review tone, sentiment, and supportive\nlanguage vary across author demographics, including gender, race, and\ninstitutional affiliation. Using a data set that includes both anonymous and\nsigned reviews, this research also reveals how the disclosure of reviewer\nidentity shapes the language of evaluation. The findings not only expose hidden\nbiases in peer feedback, but also challenge conventional assumptions about\nanonymity's role in fairness. As academic publishing grapples with reform,\nthese insights raise critical questions about how review policies shape career\ntrajectories and scientific progress."}
{"id": "2507.15433", "pdf": "https://arxiv.org/pdf/2507.15433.pdf", "abs": "https://arxiv.org/abs/2507.15433", "title": "Designing at 1:1 Scale on Wall-Sized Displays Using Existing UI Design Tools", "authors": ["Lou Schwartz", "Mohammad Ghoniem", "Valérie Maquil", "Adrien Coppens", "Johannes Hermen"], "categories": ["cs.HC"], "comment": "Publication URL:\n  https://www.thinkmind.org/library/Soft/Soft_v18_n12_2025/soft_v18_n12_2025_5.html", "summary": "Wall-Sized Displays have spatial characteristics that are difficult to\naddress during user interface design. The design at scale 1:1 could be part of\nthe solution. In this paper, we present the results of two user studies and one\ntechnology review, exploring the usability of popular, desktop-optimized\nprototyping tools, for designing at scale on Wall-Sized Displays. We considered\ntwo wall-sized display setups, and three different interaction methods: touch,\na keyboard equipped with a touchpad, and a tablet. We observed that designing\nat scale 1:1 was appreciated. Tablet-based interaction proved to be the most\ncomfortable interaction method, and a mix of interaction modalities is\npromising. In addition, care must be given to the surrounding environment, such\nas furniture. We propose twelve design guidelines for a design tool dedicated\nto this specific context. Overall, existing user interface design tools do not\nyet fully support design on and for wall-sized displays and require further\nconsiderations in terms of placement of user interface elements and the\nprovision of additional features."}
{"id": "2507.14749", "pdf": "https://arxiv.org/pdf/2507.14749.pdf", "abs": "https://arxiv.org/abs/2507.14749", "title": "On the robustness of modeling grounded word learning through a child's egocentric input", "authors": ["Wai Keen Vong", "Brenden M. Lake"], "categories": ["cs.CL"], "comment": null, "summary": "What insights can machine learning bring to understanding human language\nacquisition? Large language and multimodal models have achieved remarkable\ncapabilities, but their reliance on massive training datasets creates a\nfundamental mismatch with children, who succeed in acquiring language from\ncomparatively limited input. To help bridge this gap, researchers have\nincreasingly trained neural networks using data similar in quantity and quality\nto children's input. Taking this approach to the limit, Vong et al. (2024)\nshowed that a multimodal neural network trained on 61 hours of visual and\nlinguistic input extracted from just one child's developmental experience could\nacquire word-referent mappings. However, whether this approach's success\nreflects the idiosyncrasies of a single child's experience, or whether it would\nshow consistent and robust learning patterns across multiple children's\nexperiences was not explored. In this article, we applied automated speech\ntranscription methods to the entirety of the SAYCam dataset, consisting of over\n500 hours of video data spread across all three children. Using these automated\ntranscriptions, we generated multi-modal vision-and-language datasets for both\ntraining and evaluation, and explored a range of neural network configurations\nto examine the robustness of simulated word learning. Our findings demonstrate\nthat networks trained on automatically transcribed data from each child can\nacquire and generalize word-referent mappings across multiple network\narchitectures. These results validate the robustness of multimodal neural\nnetworks for grounded word learning, while highlighting the individual\ndifferences that emerge in how models learn when trained on each child's\ndevelopmental experiences."}
{"id": "2507.15443", "pdf": "https://arxiv.org/pdf/2507.15443.pdf", "abs": "https://arxiv.org/abs/2507.15443", "title": "Evaluating Joint Attention for Mixed-Presence Collaboration on Wall-Sized Displays", "authors": ["Adrien Coppens", "Valérie Maquil"], "categories": ["cs.HC"], "comment": "Version of record / published version:\n  https://dl.acm.org/doi/full/10.1145/3731406.3731973", "summary": "To understand and quantify the quality of mixed-presence collaboration around\nwall-sized displays, robust evaluation methodologies are needed, that are\nadapted for a room-sized experience and are not perceived as obtrusive. In this\npaper, we propose our approach for measuring joint attention based on head gaze\ndata. We describe how it has been implemented for a user study on mixed\npresence collaboration with two wall-sized displays and report on the insights\nwe gained so far from its implementation, with a preliminary focus on the data\ncoming from one particular session."}
{"id": "2507.14758", "pdf": "https://arxiv.org/pdf/2507.14758.pdf", "abs": "https://arxiv.org/abs/2507.14758", "title": "GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization", "authors": ["Luyi Ma", "Wanjia Zhang", "Kai Zhao", "Abhishek Kulkarni", "Lalitesh Morishetti", "Anjana Ganesh", "Ashish Ranjan", "Aashika Padmanabhan", "Jianpeng Xu", "Jason Cho", "Praveen Kanumala", "Kaushiki Nag", "Sumit Dutta", "Kamiya Motwani", "Malay Patel", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "10 pages, 5 figures, The ACM Conference on Recommender Systems\n  (RecSys) 2025", "summary": "Generative models have recently demonstrated strong potential in\nmulti-behavior recommendation systems, leveraging the expressive power of\ntransformers and tokenization to generate personalized item sequences. However,\ntheir adoption is hindered by (1) the lack of explicit information for token\nreasoning, (2) high computational costs due to quadratic attention complexity\nand dense sequence representations after tokenization, and (3) limited\nmulti-scale modeling over user history. In this work, we propose GRACE\n(Generative Recommendation via journey-aware sparse Attention on\nChain-of-thought tokEnization), a novel generative framework for multi-behavior\nsequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)\ntokenization method that encodes user-item interactions with explicit\nattributes from product knowledge graphs (e.g., category, brand, price) over\nsemantic tokenization, enabling interpretable and behavior-aligned generation.\nTo address the inefficiency of standard attention, we design a Journey-Aware\nSparse Attention (JSA) mechanism, which selectively attends to compressed,\nintra-, inter-, and current-context segments in the tokenized sequence.\nExperiments on two real-world datasets show that GRACE significantly\noutperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and\n+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home\ndomain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces\nattention computation by up to 48% with long sequences."}
{"id": "2507.15481", "pdf": "https://arxiv.org/pdf/2507.15481.pdf", "abs": "https://arxiv.org/abs/2507.15481", "title": "Challenging Disability and Interaction Norms in XR: Cooling Down the Empathy Machine in Waiting for Hands", "authors": ["Yesica Duarte", "Puneet Jain"], "categories": ["cs.HC"], "comment": null, "summary": "Virtual Reality (VR) is often described as the \"ultimate empathy machine,\"\nframing disability as an experience to be simulated through such technologies,\nwhich can reduce disability to a spectacle of pity or inspiration. In response,\nwe present Waiting for Hands (WfH), an interactive eXtended Reality (XR)\ninstallation that critiques this logic by: (1) repurposing interaction norms in\nXR through the creation of Alternative Controllers, and (2) staging an absurd\nXR performance using the built controllers to disrupt sentimentalized\ndisability narratives. The performance involves eight people: two XR\nparticipants on stage and six audience members watching a projected documentary\nabout Hema Kumari, an Indian singer living with Rheumatoid Arthritis. The XR\nusers partially obscure the film, drawing attention through strange mouth and\nhand movements performed in XR. This creates a layered experience that disrupts\ndirect engagement with Hema's story and introduces uncertainty. While XR is\noften seen as a fully immersive, sensory-dominant medium, this piece subverts\nthat framing by using XR to produce absurdity and alienation. By challenging\nempathy-driven and pitiable narratives of disability, we ask what ethical\nstance an XR performance can take to attune participants to non-normative\nembodiment while resisting spectacle."}
{"id": "2507.14815", "pdf": "https://arxiv.org/pdf/2507.14815.pdf", "abs": "https://arxiv.org/abs/2507.14815", "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing", "authors": ["Shoutao Guo", "Shaolei Zhang", "Qingkai Fang", "Zhengrui Ma", "Min Zhang", "Yang Feng"], "categories": ["cs.CL"], "comment": "The code is at https://github.com/ictnlp/FastLongSpeech. This model\n  is at https://huggingface.co/ICTNLP/FastLongSpeech. The dataset is at\n  https://huggingface.co/datasets/ICTNLP/LongSpeech-Eval", "summary": "The rapid advancement of Large Language Models (LLMs) has spurred significant\nprogress in Large Speech-Language Models (LSLMs), enhancing their capabilities\nin both speech understanding and generation. While existing LSLMs often\nconcentrate on augmenting speech generation or tackling a diverse array of\nshort-speech tasks, the efficient processing of long-form speech remains a\ncritical yet underexplored challenge. This gap is primarily attributed to the\nscarcity of long-speech training datasets and the high computational costs\nassociated with long sequences. To address these limitations, we introduce\nFastLongSpeech, a novel framework designed to extend LSLM capabilities for\nefficient long-speech processing without necessitating dedicated long-speech\ntraining data. FastLongSpeech incorporates an iterative fusion strategy that\ncan compress excessively long-speech sequences into manageable lengths. To\nadapt LSLMs for long-speech inputs, it introduces a dynamic compression\ntraining approach, which exposes the model to short-speech sequences at varying\ncompression ratios, thereby transferring the capabilities of LSLMs to\nlong-speech tasks. To assess the long-speech capabilities of LSLMs, we develop\na long-speech understanding benchmark called LongSpeech-Eval. Experiments show\nthat our method exhibits strong performance in both long-speech and\nshort-speech tasks, while greatly improving inference efficiency."}
{"id": "2507.15502", "pdf": "https://arxiv.org/pdf/2507.15502.pdf", "abs": "https://arxiv.org/abs/2507.15502", "title": "FollowUpBot: An LLM-Based Conversational Robot for Automatic Postoperative Follow-up", "authors": ["Chen Chen", "Jianing Yin", "Jiannong Cao", "Zhiyuan Wen", "Mingjin Zhang", "Weixun Gao", "Xiang Wang", "Haihua Shu"], "categories": ["cs.HC"], "comment": null, "summary": "Postoperative follow-up plays a crucial role in monitoring recovery and\nidentifying complications. However, traditional approaches, typically involving\nbedside interviews and manual documentation, are time-consuming and\nlabor-intensive. Although existing digital solutions, such as web\nquestionnaires and intelligent automated calls, can alleviate the workload of\nnurses to a certain extent, they either deliver an inflexible scripted\ninteraction or face private information leakage issues. To address these\nlimitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed\nrobot for postoperative care and monitoring. It allows dynamic planning of\noptimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face\nconversations with patients through multiple interaction modes, ensuring data\nprivacy. Moreover, FollowUpBot is capable of automatically generating\nstructured postoperative follow-up reports for healthcare institutions by\nanalyzing patient interactions during follow-up. Experimental results\ndemonstrate that our robot achieves high coverage and satisfaction in follow-up\ninteractions, as well as high report generation accuracy across diverse field\ntypes. The demonstration video is available at\nhttps://www.youtube.com/watch?v=_uFgDO7NoK0."}
{"id": "2507.14819", "pdf": "https://arxiv.org/pdf/2507.14819.pdf", "abs": "https://arxiv.org/abs/2507.14819", "title": "Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents", "authors": ["Akriti Jain", "Pritika Ramu", "Aparna Garimella", "Apoorv Saxena"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ntransforming text descriptions or tables to data visualizations via\ninstruction-tuning methods. However, it is not straightforward to apply these\nmethods directly for a more real-world use case of visualizing data from long\ndocuments based on user-given intents, as opposed to the user pre-selecting the\nrelevant content manually. We introduce the task of intent-based chart\ngeneration from documents: given a user-specified intent and document(s), the\ngoal is to generate a chart adhering to the intent and grounded on the\ndocument(s) in a zero-shot setting. We propose an unsupervised, two-staged\nframework in which an LLM first extracts relevant information from the\ndocument(s) by decomposing the intent and iteratively validates and refines\nthis data. Next, a heuristic-guided module selects an appropriate chart type\nbefore final code generation. To assess the data accuracy of the generated\ncharts, we propose an attribution-based metric that uses a structured textual\nrepresentation of charts, instead of relying on visual decoding metrics that\noften fail to capture the chart data effectively. To validate our approach, we\ncurate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from\ntwo domains, finance and scientific, in contrast to the existing datasets that\nare largely limited to parallel text descriptions/ tables and their\ncorresponding charts. We compare our approach with baselines using single-shot\nchart generation using LLMs and query-based retrieval methods; our method\noutperforms by upto $9$ points and $17$ points in terms of chart data accuracy\nand chart type respectively over the best baselines."}
{"id": "2507.15526", "pdf": "https://arxiv.org/pdf/2507.15526.pdf", "abs": "https://arxiv.org/abs/2507.15526", "title": "Strategies to Manage Human Factors in Mixed Reality Pilot Training: A Survey", "authors": ["Antonio Perez", "Avinash Singh", "Jonathan Mitchell", "Philip Swadling"], "categories": ["cs.HC", "H.5.1; H.1.2; I.3.6"], "comment": "14 pages, 3 figures", "summary": "Mixed Reality (MR) head mounted displays (HMDs) offer a promising alternative\nto traditional Flight Simulator Training Device (FSTD) displays, providing\nimmersion, realism and cost efficiency. However, these technologies require\nmanagement of human factors; cybersickness, visual fatigue and ergonomic\nstrain. If left unmitigated, these effects can hinder pilot performance and\ntraining outcomes. For safety critical fields like aviation, addressing human\nfactors challenges is crucial for MR's training potential. This survey\nsystematically reviews the current literature identifying key human factors\nchallenges in MR HMD use in pilot training and examines strategies to mitigate\nthese barriers. Drawing on existing industry standards set by a leading\naviation authority, the review adopts a regulatory perspective to explore\nhardware, software, ergonomic, physiological and psychological interventions\nimproving pilot comfort, safety and training effectiveness in an MR FSTD.\nAdditionally, it evaluates which of these interventions are most appropriate\nand viable for MR pilot training under existing aviation training regulations,\nensuring that technical requirements and pilot wellbeing remain balanced. The\nfindings yield significant insights for the human dimensions of aviation\nsimulation training, highlighting how regulatory considerations shape the\npracticality of mitigation measures. These insights inform emerging MR aviation\ntraining guidelines and best practices, supporting MR's readiness to enhance\naviation training."}
{"id": "2507.14849", "pdf": "https://arxiv.org/pdf/2507.14849.pdf", "abs": "https://arxiv.org/abs/2507.14849", "title": "Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding", "authors": ["Yifei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning distillation has emerged as an effective approach to enhance the\nreasoning capabilities of smaller language models. However, the impact of\nlarge-scale reasoning distillation on other critical abilities, particularly\nin-context retrieval and reasoning, remains unexplored. This gap in\nunderstanding is particularly significant given the increasing importance of\nRetrieval-Augmented Generation (RAG) systems, where efficient acquisition and\nutilization of contextual information are paramount for generating reliable\nresponses. Motivated by the need to understand how the extended long-CoT\nprocess influences long-context comprehension, we conduct a comprehensive\ninvestigation using a series of open-source models distilled from Deepseek-R1,\nrenowned for its exceptional reasoning capabilities. Our study focuses on\nevaluating these models' performance in extracting and integrating relevant\ninformation from extended contexts through multi-document question and\nanswering tasks. Through rigorous experimentation, we demonstrate that\ndistilled reasoning patterns significantly improve long-context understanding.\nOur analysis reveals that distillation fosters greater long-context awareness\nby promoting more detailed and explicit reasoning processes during context\nanalysis and information parsing. This advancement effectively mitigates the\npersistent \"lost in the middle\" issue that has hindered long-context models."}
{"id": "2507.15559", "pdf": "https://arxiv.org/pdf/2507.15559.pdf", "abs": "https://arxiv.org/abs/2507.15559", "title": "FlowForge: Guiding the Creation of Multi-agent Workflows with Design Space Visualization as a Thinking Scaffold", "authors": ["Pan Hao", "Dongyeop Kang", "Nicholas Hinds", "Qianwen Wang"], "categories": ["cs.HC"], "comment": "9 pages, 10 figures, accepted by IEEE VIS 2025", "summary": "Multi-agent workflows have become an effective strategy for tackling\ncomplicated tasks by decomposing them into multiple sub-tasks and assigning\nthem to specialized agents. However, designing optimal workflows remains\nchallenging due to the vast and intricate design space. Current practices rely\nheavily on the intuition and expertise of practitioners, often resulting in\ndesign fixation or an unstructured, time-consuming exploration of\ntrial-and-error. To address these challenges, this work introduces FLOWFORGE,\nan interactive visualization tool to facilitate the creation of multi-agent\nworkflow through i) a structured visual exploration of the design space and ii)\nin-situ guidance informed by established design patterns. Based on formative\nstudies and literature review, FLOWFORGE organizes the workflow design process\ninto three hierarchical levels (i.e., task planning, agent assignment, and\nagent optimization), ranging from abstract to concrete. This structured visual\nexploration enables users to seamlessly move from high-level planning to\ndetailed design decisions and implementations, while comparing alternative\nsolutions across multiple performance metrics. Additionally, drawing from\nestablished workflow design patterns, FLOWFORGE provides context-aware, in-situ\nsuggestions at each level as users navigate the design space, enhancing the\nworkflow creation process with practical guidance. Use cases and user studies\ndemonstrate the usability and effectiveness of FLOWFORGE, while also yielding\nvaluable insights into how practitioners explore design spaces and leverage\nguidance during workflow development."}
{"id": "2507.14871", "pdf": "https://arxiv.org/pdf/2507.14871.pdf", "abs": "https://arxiv.org/abs/2507.14871", "title": "Tiny language models", "authors": ["Ronit D. Gross", "Yarden Tzach", "Tal Halevi", "Ella Koresh", "Ido Kanter"], "categories": ["cs.CL"], "comment": "23 pages, 1 figure and 12 tables", "summary": "A prominent achievement of natural language processing (NLP) is its ability\nto understand and generate meaningful human language. This capability relies on\ncomplex feedforward transformer block architectures pre-trained on large\nlanguage models (LLMs). However, LLM pre-training is currently feasible only\nfor a few dominant companies due to the immense computational resources\nrequired, limiting broader research participation. This creates a critical need\nfor more accessible alternatives. In this study, we explore whether tiny\nlanguage models (TLMs) exhibit the same key qualitative features of LLMs. We\ndemonstrate that TLMs exhibit a clear performance gap between pre-trained and\nnon-pre-trained models across classification tasks, indicating the\neffectiveness of pre-training, even at a tiny scale. The performance gap\nincreases with the size of the pre-training dataset and with greater overlap\nbetween tokens in the pre-training and classification datasets. Furthermore,\nthe classification accuracy achieved by a pre-trained deep TLM architecture can\nbe replicated through a soft committee of multiple, independently pre-trained\nshallow architectures, enabling low-latency TLMs without affecting\nclassification accuracy. Our results are based on pre-training BERT-6 and\nvariants of BERT-1 on subsets of the Wikipedia dataset and evaluating their\nperformance on FewRel, AGNews, and DBPedia classification tasks. Future\nresearch on TLM is expected to further illuminate the mechanisms underlying\nNLP, especially given that its biologically inspired models suggest that TLMs\nmay be sufficient for children or adolescents to develop language."}
{"id": "2507.15650", "pdf": "https://arxiv.org/pdf/2507.15650.pdf", "abs": "https://arxiv.org/abs/2507.15650", "title": "Chapter 11 Students' interaction with and appreciation of automated informative tutoring feedback", "authors": ["Gerben van der Hoek", "Bastiaan Heeren", "Rogier Bos", "Paul Drijvers", "Johan Jeuring"], "categories": ["cs.HC"], "comment": null, "summary": "Computer aided formative assessment can be used to enhance a learning\nprocess, for instance by providing feedback. There are many design choices for\ndelivering feedback, that lead to a feedback strategy. In an informative\nfeedback strategy, students do not immediately receive information about the\ncorrect response, but are offered the opportunity to retry a task to apply\nfeedback information. In this small-scale qualitative study, we explore an\ninformative feedback strategy designed to offer a balance between room for\nexploration and mitigation of learning barriers. The research questions concern\nthe ways in which students interact with the feedback strategy and their\nappreciation of error-specific feedback as opposed to worked-out solutions. To\nanswer these questions, twenty-five 15-to-17-year-old senior general secondary\neducation students worked for approximately 20 minutes on linear and\nexponential extrapolation tasks in an online environment. Data included screen\ncaptures of students working with the environment and post-intervention\ninterviews. Results showed that room for exploration offered opportunities for\nself-guidance while mitigation of learning barriers prevented disengagement.\nFurthermore, students appreciated balanced feedback. We conclude that the\nbalanced feedback strategy yielded fruitful student-environment interactions."}
{"id": "2507.14887", "pdf": "https://arxiv.org/pdf/2507.14887.pdf", "abs": "https://arxiv.org/abs/2507.14887", "title": "MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction", "authors": ["Shiyi Mu", "Yongkang Liu", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "categories": ["cs.CL"], "comment": "Accepted by CogSci", "summary": "Although large language models (LLMs) excel in text comprehension and\ngeneration, their performance on the Emotion-Cause Pair Extraction (ECPE) task,\nwhich requires reasoning ability, is often underperform smaller language model.\nThe main reason is the lack of auxiliary knowledge, which limits LLMs' ability\nto effectively perceive emotions and reason causes. To address this issue, we\npropose a novel \\textbf{M}ulti-source h\\textbf{E}terogeneous \\textbf{K}nowledge\n\\textbf{i}njection me\\textbf{T}hod, MEKiT, which integrates heterogeneous\ninternal emotional knowledge and external causal knowledge. Specifically, for\nthese two distinct aspects and structures of knowledge, we apply the approaches\nof incorporating instruction templates and mixing data for instruction-tuning,\nwhich respectively facilitate LLMs in more comprehensively identifying emotion\nand accurately reasoning causes. Experimental results demonstrate that MEKiT\nprovides a more effective and adaptable solution for the ECPE task, exhibiting\nan absolute performance advantage over compared baselines and dramatically\nimproving the performance of LLMs on the ECPE task."}
{"id": "2507.15692", "pdf": "https://arxiv.org/pdf/2507.15692.pdf", "abs": "https://arxiv.org/abs/2507.15692", "title": "Surfacing Variations to Calibrate Perceived Reliability of MLLM-generated Image Descriptions", "authors": ["Meng Chen", "Akhil Iyer", "Amy Pavel"], "categories": ["cs.HC"], "comment": "18 pages, 6 figures", "summary": "Multimodal large language models (MLLMs) provide new opportunities for blind\nand low vision (BLV) people to access visual information in their daily lives.\nHowever, these models often produce errors that are difficult to detect without\nsight, posing safety and social risks in scenarios from medication\nidentification to outfit selection. While BLV MLLM users use creative\nworkarounds such as cross-checking between tools and consulting sighted\nindividuals, these approaches are often time-consuming and impractical. We\nexplore how systematically surfacing variations across multiple MLLM responses\ncan support BLV users to detect unreliable information without visually\ninspecting the image. We contribute a design space for eliciting and presenting\nvariations in MLLM descriptions, a prototype system implementing three\nvariation presentation styles, and findings from a user study with 15 BLV\nparticipants. Our results demonstrate that presenting variations significantly\nincreases users' ability to identify unreliable claims (by 4.9x using our\napproach compared to single descriptions) and significantly decreases perceived\nreliability of MLLM responses. 14 of 15 participants preferred seeing\nvariations of MLLM responses over a single description, and all expressed\ninterest in using our system for tasks from understanding a tornado's path to\nposting an image on social media."}
{"id": "2507.14894", "pdf": "https://arxiv.org/pdf/2507.14894.pdf", "abs": "https://arxiv.org/abs/2507.14894", "title": "Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs", "authors": ["Boyi Deng", "Yu Wan", "Baosong Yang", "Fei Huang", "Wenjie Wang", "Fuli Feng"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have impressive multilingual capabilities, but\nthey suffer from unexpected code-switching, also known as language mixing,\nwhich involves switching to unexpected languages in the model response. This\nproblem leads to poor readability and degrades the usability of model\nresponses. However, existing work on this issue lacks a mechanistic analysis\nand shows limited effectiveness. In this paper, we first provide an in-depth\nanalysis of unexpected code-switching using sparse autoencoders and find that\nwhen LLMs switch to a language, the features of that language exhibit excessive\npre-activation values. Based on our findings, we propose $\\textbf{S}$parse\n$\\textbf{A}$utoencoder-guided $\\textbf{S}$upervised\n$\\textbf{F}$ine$\\textbf{t}$uning (SASFT), which teaches LLMs to maintain\nappropriate pre-activation values of specific language features during\ntraining. Experiments on five models across three languages demonstrate that\nSASFT consistently reduces unexpected code-switching by more than 50\\% compared\nto standard supervised fine-tuning, with complete elimination in four cases.\nMoreover, SASFT maintains or even improves the models' performance on six\nmultilingual benchmarks, showing its effectiveness in addressing code-switching\nwhile preserving multilingual capabilities."}
{"id": "2507.15783", "pdf": "https://arxiv.org/pdf/2507.15783.pdf", "abs": "https://arxiv.org/abs/2507.15783", "title": "Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance", "authors": ["Mohammad 'Matt' Namvarpour", "Brandon Brofsky", "Jessica Medina", "Mamtaj Akter", "Afsaneh Razi"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "As Generative Artificial Intelligence (GenAI) driven chatbots like\nCharacter.AI become embedded in adolescent life, they raise concerns about\nemotional dependence and digital overreliance. While studies have investigated\nthe overreliance of adults on these chatbots, they have not investigated teens'\ninteractions with chatbots with customizable personas. We analyzed 318 Reddit\nposts made by users self-reported as 13-17 years old on the Character.AI\nsubreddit to understand patterns of overreliance. We found teens commonly begin\nusing chatbots for emotional support or creative expression, but many develop\nstrong attachments that interfere with offline relationships and daily\nroutines. Their posts revealed recurring signs of psychological distress,\ncycles of relapse, and difficulty disengaging. Teens reported that their\noverreliance often ended when they reflect on the harm, return to in-person\nsocial settings, or become frustrated by platform restrictions. Based on the\nimplications of our findings, we provide recommendations for future chatbot\ndesign so they can promote self-awareness, support real-world engagement, and\ninvolve teens in developing safer digital tools."}
{"id": "2507.14900", "pdf": "https://arxiv.org/pdf/2507.14900.pdf", "abs": "https://arxiv.org/abs/2507.14900", "title": "From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment", "authors": ["Chongxuan Huang", "Yongshi Ye", "Biao Fu", "Qifeng Su", "Xiaodong Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable multilingual\ncapabilities, however, how to evaluate cross-lingual alignment remains\nunderexplored. Existing alignment benchmarks primarily focus on sentence\nembeddings, but prior research has shown that neural models tend to induce a\nnon-smooth representation space, which impact of semantic alignment evaluation\non low-resource languages. Inspired by neuroscientific findings that similar\ninformation activates overlapping neuronal regions, we propose a novel Neuron\nState-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a\nlignment capabilities of LLMs, which offers a more semantically grounded\napproach to assess cross-lingual alignment. We evaluate NeuronXA on several\nprominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two\ntransfer tasks and three multilingual benchmarks. The results demonstrate that\nwith only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation\nof 0.9556 with downstream tasks performance and 0.8514 with transferability.\nThese findings demonstrate NeuronXA's effectiveness in assessing both\ncross-lingual alignment and transferability, even with a small dataset. This\nhighlights its potential to advance cross-lingual alignment research and to\nimprove the semantic understanding of multilingual LLMs."}
{"id": "2507.08006", "pdf": "https://arxiv.org/pdf/2507.08006.pdf", "abs": "https://arxiv.org/abs/2507.08006", "title": "Scalable Climate Data Analysis: Balancing Petascale Fidelity and Computational Cost", "authors": ["Aashish Panta", "Amy Gooch", "Giorgio Scorzelli", "Michela Taufer", "Valerio Pascucci"], "categories": ["physics.ao-ph", "cs.HC"], "comment": "Presented at The CCGRID International Scalable Computing Challenge\n  (SCALE), 2025", "summary": "The growing resolution and volume of climate data from remote sensing and\nsimulations pose significant storage, processing, and computational challenges.\nTraditional compression or subsampling methods often compromise data fidelity,\nlimiting scientific insights. We introduce a scalable ecosystem that integrates\nhierarchical multiresolution data management, intelligent transmission, and\nML-assisted reconstruction to balance accuracy and efficiency. Our approach\nreduces storage and computational costs by 99\\%, lowering expenses from\n\\$100,000 to \\$24 while maintaining a Root Mean Square (RMS) error of 1.46\ndegrees Celsius. Our experimental results confirm that even with significant\ndata reduction, essential features required for accurate climate analysis are\npreserved. Validated on petascale NASA climate datasets, this solution enables\ncost-effective, high-fidelity climate analysis for research and\ndecision-making."}
{"id": "2507.14913", "pdf": "https://arxiv.org/pdf/2507.14913.pdf", "abs": "https://arxiv.org/abs/2507.14913", "title": "PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation", "authors": ["Eliya Habba", "Noam Dahan", "Gili Lior", "Gabriel Stanovsky"], "categories": ["cs.CL"], "comment": "Eliya Habba and Noam Dahan contributed equally to this work", "summary": "Evaluating LLMs with a single prompt has proven unreliable, with small\nchanges leading to significant performance differences. However, generating the\nprompt variations needed for a more robust multi-prompt evaluation is\nchallenging, limiting its adoption in practice. To address this, we introduce\nPromptSuite, a framework that enables the automatic generation of various\nprompts. PromptSuite is flexible - working out of the box on a wide range of\ntasks and benchmarks. It follows a modular prompt design, allowing controlled\nperturbations to each component, and is extensible, supporting the addition of\nnew components and perturbation types. Through a series of case studies, we\nshow that PromptSuite provides meaningful variations to support strong\nevaluation practices. It is available through both a Python API:\nhttps://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:\nhttps://promptsuite.streamlit.app/"}
{"id": "2507.14173", "pdf": "https://arxiv.org/pdf/2507.14173.pdf", "abs": "https://arxiv.org/abs/2507.14173", "title": "Enhancing Generalization in PPG-Based Emotion Measurement with a CNN-TCN-LSTM Model", "authors": ["Karim Alghoul", "Hussein Al Osman", "Abdulmotaleb El Saddik"], "categories": ["eess.SP", "cs.HC", "cs.LG"], "comment": "Accepted by IEEE International Instrumentation and Measurement\n  Technology Conference (I2MTC) 2025", "summary": "Human computer interaction has become integral to modern life, driven by\nadvancements in machine learning technologies. Affective computing, in\nparticular, has focused on systems that recognize, interpret, and respond to\nhuman emotions, often using wearable devices, which provide continuous data\nstreams of physiological signals. Among various physiological signals, the\nphotoplethysmogram (PPG) has gained prominence due to its ease of acquisition\nfrom widely available devices. However, the generalization of PPG-based emotion\nrecognition models across individuals remains an unresolved challenge. This\npaper introduces a novel hybrid architecture that combines Convolutional Neural\nNetworks (CNNs), Long Short-Term Memory networks (LSTMs), and Temporal\nConvolutional Networks (TCNs) to address this issue. The proposed model\nintegrates the strengths of these architectures to improve robustness and\ngeneralization. Raw PPG signals are fed into the CNN for feature extraction.\nThese features are processed separately by LSTM and TCN. The outputs from these\ncomponents are concatenated to generate a final feature representation, which\nserves as the input for classifying valence and arousal, the primary dimensions\nof emotion. Experiments using the Photoplethysmogram Dataset for Emotional\nAnalysis (PPGE) demonstrate that the proposed hybrid model achieves better\nmodel generalization than standalone CNN and LSTM architectures. Our results\nshow that the proposed solution outperforms the state-of-the-art CNN\narchitecture, as well as a CNN-LSTM model, in emotion recognition tasks with\nPPG signals. Using metrics such as Area Under the Curve (AUC) and F1 Score, we\nhighlight the model's effectiveness in handling subject variability."}
{"id": "2507.14922", "pdf": "https://arxiv.org/pdf/2507.14922.pdf", "abs": "https://arxiv.org/abs/2507.14922", "title": "SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs", "authors": ["Vahid Rahimzadeh", "Erfan Moosavi Monazzah", "Mohammad Taher Pilehvar", "Yadollah Yaghoobzadeh"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Persona-driven LLMs have emerged as powerful tools in computational social\nscience, yet existing approaches fall at opposite extremes, either relying on\ncostly human-curated data or producing synthetic personas that lack consistency\nand realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from\n10,000 real social media users from BlueSky open platform across three time\nwindows, bridging this spectrum by grounding synthetic generation in authentic\nuser activity. Our evaluation demonstrates that SYNTHIA achieves competitive\nperformance with state-of-the-art methods in demographic diversity and social\nsurvey alignment while significantly outperforming them in narrative\nconsistency. Uniquely, SYNTHIA incorporates temporal dimensionality and\nprovides rich social interaction metadata from the underlying network, enabling\nnew research directions in computational social science and persona-driven\nlanguage modeling."}
{"id": "2507.14217", "pdf": "https://arxiv.org/pdf/2507.14217.pdf", "abs": "https://arxiv.org/abs/2507.14217", "title": "Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation", "authors": ["Tudor Matei Opran", "Samir Loudni"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "We address the pattern explosion problem in pattern mining by proposing an\ninteractive learning framework that combines nonlinear utility aggregation with\ngeometry-aware query selection. Our method models user preferences through a\nChoquet integral over multiple interestingness measures and exploits the\ngeometric structure of the version space to guide the selection of informative\ncomparisons. A branch-and-bound strategy with tight distance bounds enables\nefficient identification of queries near the decision boundary. Experiments on\nUCI datasets show that our approach outperforms existing methods such as\nChoquetRank, achieving better ranking accuracy with fewer user interactions."}
{"id": "2507.14958", "pdf": "https://arxiv.org/pdf/2507.14958.pdf", "abs": "https://arxiv.org/abs/2507.14958", "title": "MUR: Momentum Uncertainty guided Reasoning for Large Language Models", "authors": ["Hang Yan", "Fangzhi Xu", "Rongman Xu", "Yifei Li", "Jian Zhang", "Haoran Luo", "Xiaobao Wu", "Luu Anh Tuan", "Haiteng Zhao", "Qika Lin", "Jun Liu"], "categories": ["cs.CL"], "comment": "25 pages, 8 figures", "summary": "Large Language Models (LLMs) have achieved impressive performance on\nreasoning-intensive tasks, yet optimizing their reasoning efficiency remains an\nopen challenge. While Test-Time Scaling (TTS) improves reasoning quality, it\noften leads to overthinking, wasting tokens on redundant computations. This\nwork investigates how to efficiently and adaptively guide LLM test-time scaling\nwithout additional training. Inspired by the concept of momentum in physics, we\npropose Momentum Uncertainty-guided Reasoning (MUR), which dynamically\nallocates thinking budgets to critical reasoning steps by tracking and\naggregating stepwise uncertainty over time. To support flexible inference-time\ncontrol, we introduce gamma-control, a simple mechanism that tunes the\nreasoning budget via a single hyperparameter. We provide in-depth theoretical\nproof to support the superiority of MUR in terms of stability and biases. MUR\nis comprehensively evaluated against various TTS methods across four\nchallenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using\ndifferent sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate\nthat MUR reduces computation by over 50% on average while improving accuracy by\n0.62-3.37%."}
{"id": "2507.14242", "pdf": "https://arxiv.org/pdf/2507.14242.pdf", "abs": "https://arxiv.org/abs/2507.14242", "title": "Culling Misinformation from Gen AI: Toward Ethical Curation and Refinement", "authors": ["Prerana Khatiwada", "Grace Donaher", "Jasymyn Navarro", "Lokesh Bhatta"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "7 pages", "summary": "While Artificial Intelligence (AI) is not a new field, recent developments,\nespecially with the release of generative tools like ChatGPT, have brought it\nto the forefront of the minds of industry workers and academic folk alike.\nThere is currently much talk about AI and its ability to reshape many everyday\nprocesses as we know them through automation. It also allows users to expand\ntheir ideas by suggesting things they may not have thought of on their own and\nprovides easier access to information. However, not all of the changes this\ntechnology will bring or has brought so far are positive; this is why it is\nextremely important for all modern people to recognize and understand the risks\nbefore using these tools and allowing them to cause harm. This work takes a\nposition on better understanding many equity concerns and the spread of\nmisinformation that result from new AI, in this case, specifically ChatGPT and\ndeepfakes, and encouraging collaboration with law enforcement, developers, and\nusers to reduce harm. Considering many academic sources, it warns against these\nissues, analyzing their cause and impact in fields including healthcare,\neducation, science, academia, retail, and finance. Lastly, we propose a set of\nfuture-facing guidelines and policy considerations to solve these issues while\nstill enabling innovation in these fields, this responsibility falling upon\nusers, developers, and government entities."}
{"id": "2507.15024", "pdf": "https://arxiv.org/pdf/2507.15024.pdf", "abs": "https://arxiv.org/abs/2507.15024", "title": "RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback", "authors": ["Qiaoyu Tang", "Hao Xiang", "Le Yu", "Bowen Yu", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun", "Junyang Lin"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of Large Language Models (LLMs), developing\neffective critic modules for precise guidance has become crucial yet\nchallenging. In this paper, we initially demonstrate that supervised\nfine-tuning for building critic modules (which is widely adopted in current\nsolutions) fails to genuinely enhance models' critique abilities, producing\nsuperficial critiques with insufficient reflections and verifications. To\nunlock the unprecedented critique capabilities, we propose RefCritic, a\nlong-chain-of-thought critic module based on reinforcement learning with dual\nrule-based rewards: (1) instance-level correctness of solution judgments and\n(2) refinement accuracies of the policy model based on critiques, aiming to\ngenerate high-quality evaluations with actionable feedback that effectively\nguides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and\nDeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement\nsettings, RefCritic demonstrates consistent advantages across all benchmarks,\ne.g., 6.8\\% and 7.2\\% gains on AIME25 for the respective base models. Notably,\nunder majority voting, policy models filtered by RefCritic show superior\nscaling with increased voting numbers. Moreover, despite training on\nsolution-level supervision, RefCritic outperforms step-level supervised\napproaches on ProcessBench, a benchmark to identify erroneous steps in\nmathematical reasoning."}
{"id": "2507.14339", "pdf": "https://arxiv.org/pdf/2507.14339.pdf", "abs": "https://arxiv.org/abs/2507.14339", "title": "Fiduciary AI for the Future of Brain-Technology Interactions", "authors": ["Abhishek Bhattacharjee", "Jack Pilkington", "Nita Farahany"], "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG", "eess.SP", "K.4.0; I.2.0; J.4"], "comment": "32 pages", "summary": "Brain foundation models represent a new frontier in AI: instead of processing\ntext or images, these models interpret real-time neural signals from EEG, fMRI,\nand other neurotechnologies. When integrated with brain-computer interfaces\n(BCIs), they may enable transformative applications-from thought controlled\ndevices to neuroprosthetics-by interpreting and acting on brain activity in\nmilliseconds. However, these same systems pose unprecedented risks, including\nthe exploitation of subconscious neural signals and the erosion of cognitive\nliberty. Users cannot easily observe or control how their brain signals are\ninterpreted, creating power asymmetries that are vulnerable to manipulation.\nThis paper proposes embedding fiduciary duties-loyalty, care, and\nconfidentiality-directly into BCI-integrated brain foundation models through\ntechnical design. Drawing on legal traditions and recent advancements in AI\nalignment techniques, we outline implementable architectural and governance\nmechanisms to ensure these systems act in users' best interests. Placing brain\nfoundation models on a fiduciary footing is essential to realizing their\npotential without compromising self-determination."}
{"id": "2507.15061", "pdf": "https://arxiv.org/pdf/2507.15061.pdf", "abs": "https://arxiv.org/abs/2507.15061", "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization", "authors": ["Zhengwei Tao", "Jialong Wu", "Wenbiao Yin", "Junkai Zhang", "Baixuan Li", "Haiyang Shen", "Kuan Li", "Liwen Zhang", "Xinyu Wang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Model (LLM)-powered agents has revolutionized\nartificial intelligence by enabling solutions to complex, open-ended tasks\nthrough web-based information-seeking (IS) capabilities. The scarcity of\nhigh-quality training data has limited the development of IS agents. Existing\napproaches typically adopt an information-driven paradigm that first collects\nweb data and then generates questions based on the retrieval. However, this may\nlead to inconsistency between information structure and reasoning structure,\nquestion and answer. To mitigate, we propose a formalization-driven IS data\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\nformalizes IS tasks through set theory. Central to the formalization is the\nconcept of Knowledge Projections (KP), which enables precise control over\nreasoning structure by KP operation compositions. During synthesis, we begin by\ncreating seed tasks, then use a multi-step expansion process. At each step, an\nagentic Expander expands the current formal question more complex with\nretrieval and validation tools based on our formalization. We train our model\non the synthesized dataset. Experiment results demonstrate that WebShaper\nachieves state-of-the-art performance among open-sourced IS agents on GAIA and\nWebWalkerQA benchmarks."}
{"id": "2507.14372", "pdf": "https://arxiv.org/pdf/2507.14372.pdf", "abs": "https://arxiv.org/abs/2507.14372", "title": "Text-to-SQL for Enterprise Data Analytics", "authors": ["Albert Chen", "Manas Bundele", "Gaurav Ahlawat", "Patrick Stetz", "Zhitao Wang", "Qiang Fei", "Donghoon Jung", "Audrey Chu", "Bharadwaj Jayaraman", "Ayushi Panth", "Yatin Arora", "Sourav Jain", "Renjith Varma", "Alexey Ilin", "Iuliia Melnychuk", "Chelsea Chueh", "Joyan Sil", "Xiaofeng Wang"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25", "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions."}
{"id": "2507.15087", "pdf": "https://arxiv.org/pdf/2507.15087.pdf", "abs": "https://arxiv.org/abs/2507.15087", "title": "Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling", "authors": ["Chenlei Gong", "Yuanhe Tian", "Lei Mao", "Yan Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Currently, many studies view DNA sequences as a special type of language and\nutilize Transformers to model them. These studies use fixed-length k-mer\nsegmentation and BPE subword tokenization but lack a systematic evaluation to\ndetermine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a\n4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,\nAliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and\n24-layer Transformer encoders and evaluated on GUE benchmark dataset. In\ngeneral, BPE delivers higher and more stable performance across tasks by\ncompressing frequent motifs into variable-length tokens, reducing sequence\nlength, and improving model generalization. RoPE excels at capturing periodic\nmotifs and extrapolating to long sequences, while AliBi also performs well on\ntasks driven by local dependencies. In terms of depth, we observe significant\ngains when increasing layers from 3 to 12, with only marginal improvements or\nslight overfitting at 24 layers. This study provides practical guidance for\ndesigning tokenization and positional encoding in DNA Transformer models."}
{"id": "2507.14451", "pdf": "https://arxiv.org/pdf/2507.14451.pdf", "abs": "https://arxiv.org/abs/2507.14451", "title": "Adapting Whisper for Lightweight and Efficient Automatic Speech Recognition of Children for On-device Edge Applications", "authors": ["Satwik Dutta", "Shruthigna Chandupatla", "John Hansen"], "categories": ["eess.AS", "cs.HC", "cs.SD"], "comment": "5 pages, 5 figures, accepted for presentation at the 2025 Workshop on\n  Child Computer Interaction (WOCCI 2025), a Satellite Workshop of the 2025\n  Interspeech Conference", "summary": "Reliability on cloud providers for ASR inference to support child-centered\nvoice-based applications is becoming challenging due to regulatory and privacy\nchallenges. Motivated by a privacy-preserving design, this study aims to\ndevelop a lightweight & efficient Whisper ASR system capable of running on a\nRaspberry Pi. Upon evaluation of the MyST corpus and by examining various\nfiltering strategies to fine-tune the `tiny.en' model, a Word Error Rate (WER)\nof 15.9% was achieved (11.8% filtered). A low-rank compression reduces the\nencoder size by 0.51M with 1.26x faster inference in GPU, with 11% relative WER\nincrease. During inference on Pi, the compressed version required ~2 GFLOPS\nfewer computations. The RTF for both the models ranged between [0.23-0.41] for\nvarious input audio durations. Analyzing the RAM usage and CPU temperature\nshowed that the PI was capable of handling both the tiny models, however it was\nnoticed that small models initiated additional overhead/thermal throttling."}
{"id": "2507.15092", "pdf": "https://arxiv.org/pdf/2507.15092.pdf", "abs": "https://arxiv.org/abs/2507.15092", "title": "A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations", "authors": ["Vijeta Deshpande", "Ishita Dasgupta", "Uttaran Bhattacharya", "Somdeb Sarkhel", "Saayan Mitra", "Anna Rumshisky"], "categories": ["cs.CL"], "comment": null, "summary": "Synthetic text generated by Large Language Models (LLMs) is increasingly used\nfor further training and improvement of LLMs. Diversity is crucial for the\neffectiveness of synthetic data, and researchers rely on prompt engineering to\nimprove diversity. However, the impact of prompt variations on response text\nlength, and, more importantly, the consequential effect on lexical diversity\nmeasurements, remain underexplored. In this work, we propose Penalty-Adjusted\nType-Token Ratio (PATTR), a diversity metric robust to length variations. We\ngenerate a large synthetic corpus of over 20M words using seven models from the\nLLaMA, OLMo, and Phi families, focusing on a creative writing task of video\nscript generation, where diversity is crucial. We evaluate per-response lexical\ndiversity using PATTR and compare it against existing metrics of Moving-Average\nTTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length\nvariations introduce biases favoring shorter responses. Unlike existing\nmetrics, PATTR explicitly considers the task-specific target response length\n($L_T$) to effectively mitigate length biases. We further demonstrate the\nutility of PATTR in filtering the top-10/100/1,000 most lexically diverse\nresponses, showing that it consistently outperforms MATTR and CR by yielding on\npar or better diversity with high adherence to $L_T$."}
{"id": "2507.14543", "pdf": "https://arxiv.org/pdf/2507.14543.pdf", "abs": "https://arxiv.org/abs/2507.14543", "title": "Real Time Captioning of Sign Language Gestures in Video Meetings", "authors": ["Sharanya Mukherjee", "Md Hishaam Akhtar", "Kannadasan R"], "categories": ["cs.CV", "cs.CY", "cs.HC", "cs.LG", "I.4.6"], "comment": "7 pages, 2 figures, 1 table, Presented at ICCMDE 2021", "summary": "It has always been a rather tough task to communicate with someone possessing\na hearing impairment. One of the most tested ways to establish such a\ncommunication is through the use of sign based languages. However, not many\npeople are aware of the smaller intricacies involved with sign language. Sign\nlanguage recognition using computer vision aims at eliminating the\ncommunication barrier between deaf-mute and ordinary people so that they can\nproperly communicate with others. Recently the pandemic has left the whole\nworld shaken up and has transformed the way we communicate. Video meetings have\nbecome essential for everyone, even people with a hearing disability. In recent\nstudies, it has been found that people with hearing disabilities prefer to sign\nover typing during these video calls. In this paper, we are proposing a browser\nextension that will automatically translate sign language to subtitles for\neveryone else in the video call. The Large-scale dataset which contains more\nthan 2000 Word-Level ASL videos, which were performed by over 100 signers will\nbe used."}
{"id": "2507.15100", "pdf": "https://arxiv.org/pdf/2507.15100.pdf", "abs": "https://arxiv.org/abs/2507.15100", "title": "Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?", "authors": ["Chathuri Jayaweera", "Brianna Yanqui", "Bonnie Dorr"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures and 5 tables", "summary": "Natural Language Inference (NLI) is the task of determining the semantic\nentailment of a premise for a given hypothesis. The task aims to develop\nsystems that emulate natural human inferential processes where commonsense\nknowledge plays a major role. However, existing commonsense resources lack\nsufficient coverage for a variety of premise-hypothesis pairs. This study\nexplores the potential of Large Language Models as commonsense knowledge\ngenerators for NLI along two key dimensions: their reliability in generating\nsuch knowledge and the impact of that knowledge on prediction accuracy. We\nadapt and modify existing metrics to assess LLM factuality and consistency in\ngenerating in this context. While explicitly incorporating commonsense\nknowledge does not consistently improve overall results, it effectively helps\ndistinguish entailing instances and moderately improves distinguishing\ncontradictory and neutral inferences."}
{"id": "2507.14553", "pdf": "https://arxiv.org/pdf/2507.14553.pdf", "abs": "https://arxiv.org/abs/2507.14553", "title": "Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance", "authors": ["Xiaoran Wu"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Clutter in photos is a distraction preventing photographers from conveying\nthe intended emotions or stories to the audience. Photography amateurs\nfrequently include clutter in their photos due to unconscious negligence or the\nlack of experience in creating a decluttered, aesthetically appealing scene for\nshooting. We are thus motivated to develop a camera guidance system that\nprovides solutions and guidance for clutter identification and removal. We\nestimate and visualize the contribution of objects to the overall aesthetics\nand content of a photo, based on which users can interactively identify\nclutter. Suggestions on getting rid of clutter, as well as a tool that removes\ncluttered objects computationally, are provided to guide users to deal with\ndifferent kinds of clutter and improve their photographic work. Two technical\nnovelties underpin interactions in our system: a clutter distinguishment\nalgorithm with aesthetics evaluations for objects and an iterative image\ninpainting algorithm based on generative adversarial nets that reconstructs\nmissing regions of removed objects for high-resolution images. User studies\ndemonstrate that our system provides flexible interfaces and accurate\nalgorithms that allow users to better identify distractions and take higher\nquality images within less time."}
{"id": "2507.15114", "pdf": "https://arxiv.org/pdf/2507.15114.pdf", "abs": "https://arxiv.org/abs/2507.15114", "title": "From Disagreement to Understanding: The Case for Ambiguity Detection in NLI", "authors": ["Chathuri Jayaweera", "Bonnie Dorr"], "categories": ["cs.CL"], "comment": "8 pages, 6 figures", "summary": "This position paper argues that annotation disagreement in Natural Language\nInference (NLI) is not mere noise but often reflects meaningful interpretive\nvariation, especially when triggered by ambiguity in the premise or hypothesis.\nWhile underspecified guidelines and annotator behavior can contribute to\nvariation, content-based ambiguity offers a process-independent signal of\ndivergent human perspectives. We call for a shift toward ambiguity-aware NLI by\nsystematically identifying ambiguous input pairs and classifying ambiguity\ntypes. To support this, we present a unified framework that integrates existing\ntaxonomies and illustrate key ambiguity subtypes through concrete examples.\nThese examples reveal how ambiguity shapes annotator decisions and motivate the\nneed for targeted detection methods that better align models with human\ninterpretation. A key limitation is the lack of datasets annotated for\nambiguity and subtypes. We propose addressing this gap through new annotated\nresources and unsupervised approaches to ambiguity detection -- paving the way\nfor more robust, explainable, and human-aligned NLI systems."}
{"id": "2507.14623", "pdf": "https://arxiv.org/pdf/2507.14623.pdf", "abs": "https://arxiv.org/abs/2507.14623", "title": "Rejection or Inclusion in the Emotion-Identity Dynamics of TikTok Refugees on RedNote", "authors": ["Mingchen Li", "Wenbo Xu", "Wenqing Gu", "Yixuan Xie", "Yao Zhou", "Yunsong Dai", "Cheng Tan", "Pan Hui"], "categories": ["cs.SI", "cs.CY", "cs.HC"], "comment": null, "summary": "This study examines cross-cultural interactions between Chinese users and\nself-identified \"TikTok Refugees\"(foreign users who migrated to RedNote after\nTikTok's U.S. ban). Based on a dataset of 1,862 posts and 403,054 comments, we\nuse large language model-based sentiment classification and BERT-based topic\nmodelling to explore how both groups engage with the TikTok refugee phenomenon.\nWe analyse what themes foreign users express, how Chinese users respond, how\nstances (Pro-China, Neutral, Pro-Foreign) shape emotional expression, and how\naffective responses differ across topics and identities. Results show strong\naffective asymmetry: Chinese users respond with varying emotional intensities\nacross topics and stances: pride and praise dominate cultural threads, while\npolitical discussions elicit high levels of contempt and anger, especially from\nPro-China commenters. Pro-Foreign users exhibit the strongest negative emotions\nacross all topics, whereas neutral users express curiosity and joy but still\nreinforce mainstream discursive norms. Cross-topic comparisons reveal that\nappearance-related content produces the most emotionally balanced interactions,\nwhile politics generates the highest polarization. Our findings reveal distinct\nemotion-stance structures in Sino-foreign online interactions and offer\nempirical insights into identity negotiation in transnational digital publics."}
{"id": "2507.15142", "pdf": "https://arxiv.org/pdf/2507.15142.pdf", "abs": "https://arxiv.org/abs/2507.15142", "title": "A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script", "authors": ["Hellina Hailu Nigatu", "Atnafu Lambebo Tonja", "Henok Biadglign Ademtew", "Hizkel Mitiku Alemayehu", "Negasi Haile Abadi", "Tadesse Destaw Belay", "Seid Muhie Yimam"], "categories": ["cs.CL", "cs.AI"], "comment": "Paper under review", "summary": "Homophone normalization, where characters that have the same sound in a\nwriting script are mapped to one character, is a pre-processing step applied in\nAmharic Natural Language Processing (NLP) literature. While this may improve\nperformance reported by automatic metrics, it also results in models that are\nnot able to understand different forms of writing in a single language.\nFurther, there might be impacts in transfer learning, where models trained on\nnormalized data do not generalize well to other languages. In this paper, we\nexperiment with monolingual training and cross-lingual transfer to understand\nthe impacts of normalization on languages that use the Ge'ez script. We then\npropose a post-inference intervention in which normalization is applied to\nmodel predictions instead of training data. With our simple scheme of\npost-inference normalization, we show that we can achieve an increase in BLEU\nscore of up to 1.03 while preserving language features in training. Our work\ncontributes to the broader discussion on technology-facilitated language change\nand calls for more language-aware interventions."}
{"id": "2507.14698", "pdf": "https://arxiv.org/pdf/2507.14698.pdf", "abs": "https://arxiv.org/abs/2507.14698", "title": "Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition", "authors": ["Xuetao Lin", "Tianhao Peng", "Peihong Dai", "Yu Liang", "Wenjun Wu"], "categories": ["cs.LG", "cs.AI", "cs.HC", "eess.SP"], "comment": null, "summary": "EEG-based emotion recognition plays an important role in developing adaptive\nbrain-computer communication systems, yet faces two fundamental challenges in\npractical implementations: (1) effective integration of non-stationary\nspatial-temporal neural patterns, (2) robust adaptation to dynamic emotional\nintensity variations in real-world scenarios. This paper proposes SST-CL, a\nnovel framework integrating spatial-temporal transformers with curriculum\nlearning. Our method introduces two core components: a spatial encoder that\nmodels inter-channel relationships and a temporal encoder that captures\nmulti-scale dependencies through windowed attention mechanisms, enabling\nsimultaneous extraction of spatial correlations and temporal dynamics from EEG\nsignals. Complementing this architecture, an intensity-aware curriculum\nlearning strategy progressively guides training from high-intensity to\nlow-intensity emotional states through dynamic sample scheduling based on a\ndual difficulty assessment. Comprehensive experiments on three benchmark\ndatasets demonstrate state-of-the-art performance across various emotional\nintensity levels, with ablation studies confirming the necessity of both\narchitectural components and the curriculum learning mechanism."}
{"id": "2507.15152", "pdf": "https://arxiv.org/pdf/2507.15152.pdf", "abs": "https://arxiv.org/abs/2507.15152", "title": "What Level of Automation is \"Good Enough\"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction", "authors": ["Lingbo Li", "Anuradha Mathrani", "Teo Susnjak"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Automating data extraction from full-text randomised controlled trials (RCTs)\nfor meta-analysis remains a significant challenge. This study evaluates the\npractical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)\nacross tasks involving statistical results, risk-of-bias assessments, and\nstudy-level characteristics in three medical domains: hypertension, diabetes,\nand orthopaedics. We tested four distinct prompting strategies (basic\nprompting, self-reflective prompting, model ensemble, and customised prompts)\nto determine how to improve extraction quality. All models demonstrate high\nprecision but consistently suffer from poor recall by omitting key information.\nWe found that customised prompts were the most effective, boosting recall by up\nto 15\\%. Based on this analysis, we propose a three-tiered set of guidelines\nfor using LLMs in data extraction, matching data types to appropriate levels of\nautomation based on task complexity and risk. Our study offers practical advice\nfor automating data extraction in real-world meta-analyses, balancing LLM\nefficiency with expert oversight through targeted, task-specific automation."}
{"id": "2507.14909", "pdf": "https://arxiv.org/pdf/2507.14909.pdf", "abs": "https://arxiv.org/abs/2507.14909", "title": "The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities", "authors": ["Elio Grande"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "The Endless Tuning is a design method for a reliable deployment of artificial\nintelligence based on a double mirroring process, which pursues both the goals\nof avoiding human replacement and filling the so-called responsibility gap\n(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the\nrelational approach urged therein, it was then actualized in a protocol,\nimplemented in three prototypical applications regarding decision-making\nprocesses (respectively: loan granting, pneumonia diagnosis, and art style\nrecognition) and tested with such as many domain experts. Step by step\nillustrating the protocol, giving insights concretely showing a different voice\n(Gilligan 1993) in the ethics of artificial intelligence, a philosophical\naccount of technical choices (e.g., a reversed and hermeneutic deployment of\nXAI algorithms) will be provided in the present study together with the results\nof the experiments, focusing on user experience rather than statistical\naccuracy. Even thoroughly employing deep learning models, full control was\nperceived by the interviewees in the decision-making setting, while it appeared\nthat a bridge can be built between accountability and liability in case of\ndamage."}
{"id": "2507.15198", "pdf": "https://arxiv.org/pdf/2507.15198.pdf", "abs": "https://arxiv.org/abs/2507.15198", "title": "Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment", "authors": ["Xiandong Meng", "Yan Wu", "Yexin Tian", "Xin Hu", "Tianze Kang", "Junliang Du"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenges of high computational cost and slow\ninference in deploying large language models. It proposes a distillation\nstrategy guided by multiple teacher models. The method constructs several\nteacher models and integrates their output probability distributions and\nintermediate semantic features. This guides the student model to learn from\nmultiple sources of knowledge. As a result, the student model gains stronger\nlanguage understanding and generation ability while maintaining a small\nparameter size. To achieve this, the paper introduces a weighted output fusion\nmechanism, a feature alignment loss function, and an entropy-driven dynamic\nteacher weighting strategy. These components improve the quality and stability\nof knowledge transfer during distillation. Under multi-teacher guidance, the\nstudent model captures semantic information more effectively and demonstrates\nstrong performance across multiple evaluation metrics. In particular, the\nmethod shows high consistency in expression, generalization ability, and task\nadaptability in tasks such as language modeling, text generation, and\nmulti-task learning. The experiments compare the proposed method with several\nwidely adopted distillation approaches. The results further confirm its overall\nadvantages in perplexity, distillation loss, and generation quality. This study\nprovides a feasible technical path for the efficient compression of large-scale\nlanguage models. It also demonstrates the effectiveness of multi-teacher\ncollaborative mechanisms in complex language modeling tasks."}
{"id": "2507.14985", "pdf": "https://arxiv.org/pdf/2507.14985.pdf", "abs": "https://arxiv.org/abs/2507.14985", "title": "Metaverse Security and Privacy Research: A Systematic Review", "authors": ["Argianto Rahartomo", "Leonel Merino", "Mohammad Ghafari"], "categories": ["cs.CR", "cs.ET", "cs.HC", "cs.SE"], "comment": "The paper is accepted for publication at Computers & Security Journal", "summary": "The rapid growth of metaverse technologies, including virtual worlds,\naugmented reality, and lifelogging, has accelerated their adoption across\ndiverse domains. This rise exposes users to significant new security and\nprivacy challenges due to sociotechnical complexity, pervasive connectivity,\nand extensive user data collection in immersive environments. We present a\nsystematic review of the literature published between 2013 and 2024, offering a\ncomprehensive analysis of how the research community has addressed\nmetaverse-related security and privacy issues over the past decade. We organize\nthe studies by method, examined the security and privacy properties, immersive\ncomponents, and evaluation strategies. Our investigation reveals a sharp\nincrease in research activity in the last five years, a strong focus on\npractical and user-centered approaches, and a predominant use of benchmarking,\nhuman experimentation, and qualitative methods. Authentication and\nunobservability are the most frequently studied properties. However, critical\ngaps remain in areas such as policy compliance, accessibility,\ninteroperability, and back-end infrastructure security. We emphasize the\nintertwined technical complexity and human factors of the metaverse and call\nfor integrated, interdisciplinary approaches to securing inclusive and\ntrustworthy immersive environments."}
{"id": "2507.15236", "pdf": "https://arxiv.org/pdf/2507.15236.pdf", "abs": "https://arxiv.org/abs/2507.15236", "title": "SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest", "authors": ["Shayan Vassef", "Amirhossein Dabiriaghdam", "Mohammadreza Bakhtiari", "Yadollah Yaghoobzadeh"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This work investigates the impact of multi-task, multi-lingual, and\nmulti-source learning approaches on the robustness and performance of\npretrained language models. To enhance this analysis, we introduce Subsets of\nInterest (SOI), a novel categorization framework that identifies six distinct\nlearning behavior patterns during training, including forgettable examples,\nunlearned examples, and always correct examples. Through SOI transition\nheatmaps and dataset cartography visualization, we analyze how examples shift\nbetween these categories when transitioning from single-setting to\nmulti-setting configurations. We perform comprehensive experiments across three\nparallel comparisons: multi-task vs. single-task learning using English tasks\n(entailment, paraphrase, sentiment), multi-source vs. single-source learning\nusing sentiment analysis datasets, and multi-lingual vs. single-lingual\nlearning using intent classification in French, English, and Persian. Our\nresults demonstrate that multi-source learning consistently improves\nout-of-distribution performance by up to 7%, while multi-task learning shows\nmixed results with notable gains in similar task combinations. We further\nintroduce a two-stage fine-tuning approach where the second stage leverages\nSOI-based subset selection to achieve additional performance improvements.\nThese findings provide new insights into training dynamics and offer practical\napproaches for optimizing multi-setting language model performance."}
{"id": "2507.15188", "pdf": "https://arxiv.org/pdf/2507.15188.pdf", "abs": "https://arxiv.org/abs/2507.15188", "title": "Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View", "authors": ["Chowdhury Shahriar Muzammel", "Maria Spichkova", "James Harland"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Requirements Engineering (RE) is one of the most interaction-intensive phases\nof software development. This means that RE activities might be especially\nimpacted by stakeholders' national culture. Software development projects\nincreasingly have a very diverse range of stakeholders. To future-proof RE\nactivities, we need to help RE practitioners avoid misunderstandings and\nconflicts that might arise from not understanding potential Cultural Influences\n(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT\nprofession. Bangladesh has a growing IT sector with some unique socio-cultural\ncharacteristics, and has been largely overlooked in this research field. In\nthis study, we aim to investigate how the RE process is adopted in the context\nof Bangladeshi culture and what cultural influences impact overall RE\nactivities."}
{"id": "2507.15275", "pdf": "https://arxiv.org/pdf/2507.15275.pdf", "abs": "https://arxiv.org/abs/2507.15275", "title": "ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling", "authors": ["Yuanhe Tian", "Junjie Liu", "Zhizhou Kou", "Yuxiang Li", "Yan Song"], "categories": ["cs.CL"], "comment": null, "summary": "Building high-quality data resources is crucial for advancing artificial\nintelligence research and applications in specific domains, particularly in the\nChinese medical domain. Existing Chinese medical datasets are limited in size\nand narrow in domain coverage, falling short of the diverse corpora required\nfor effective pre-training. Moreover, most datasets are designed solely for LLM\nfine-tuning and do not support pre-training and reinforcement learning from\nhuman feedback (RLHF). In this paper, we propose a Chinese medical dataset\nnamed ChiMed 2.0, which extends our previous work ChiMed, and covers data\ncollected from Chinese medical online platforms and generated by LLMs. ChiMed\n2.0 contains 204.4M Chinese characters covering both traditional Chinese\nmedicine classics and modern general medical data, where there are 164.8K\ndocuments for pre-training, 351.6K question-answering pairs for supervised\nfine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the\neffectiveness of our approach for training a Chinese medical LLM, we conduct\nfurther pre-training, SFT, and RLHF experiments on representative general\ndomain LLMs and evaluate their performance on medical benchmark datasets. The\nresults show performance gains across different model scales, validating the\ndataset's effectiveness and applicability."}
{"id": "2507.15197", "pdf": "https://arxiv.org/pdf/2507.15197.pdf", "abs": "https://arxiv.org/abs/2507.15197", "title": "Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?", "authors": ["Chowdhury Shahriar Muzammel", "Maria Spichkova", "James Harland"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "In requirements engineering (RE), personas are now being used to represent\nuser expectations and needs. This systematic mapping study (SMS) aims to\nexplore the most recent studies and to cover recent changes in trends,\nespecially related to the recent evolution of Generative AI approaches. Our SMS\ncovers the period between April 2023 and April 2025. We identified 22 relevant\npublications and analysed persona representation, construction, validation, as\nwell as RE activities covered by personas. We identified that a number of\nstudies applied AI-based solutions for persona construction and validation. We\nobserved that template-based personas are becoming more popular nowadays. We\nalso observed an increase in the proportion of studies covering validation\naspects."}
{"id": "2507.15281", "pdf": "https://arxiv.org/pdf/2507.15281.pdf", "abs": "https://arxiv.org/abs/2507.15281", "title": "A Novel Self-Evolution Framework for Large Language Models", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The capabilities of Large Language Models (LLMs) are limited to some extent\nby pre-training, so some researchers optimize LLMs through post-training.\nExisting post-training strategies, such as memory-based retrieval or preference\noptimization, improve user alignment yet fail to enhance the model's domain\ncognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution\n(DPSE) framework that jointly optimizes user preference adaptation and\ndomain-specific competence. DPSE introduces a Censor module to extract\nmulti-dimensional interaction signals and estimate satisfaction scores, which\nguide structured data expansion via topic-aware and preference-driven\nstrategies. These expanded datasets support a two-stage fine-tuning pipeline:\nsupervised domain grounding followed by frequency-aware preference\noptimization. Experiments across general NLP benchmarks and long-term dialogue\ntasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,\nPreference Optimization, and Memory-Augmented baselines. Ablation studies\nvalidate the contribution of each module. In this way, our framework provides\nan autonomous path toward continual self-evolution of LLMs."}
{"id": "2507.15454", "pdf": "https://arxiv.org/pdf/2507.15454.pdf", "abs": "https://arxiv.org/abs/2507.15454", "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting", "authors": ["Ruijie Zhu", "Mulin Yu", "Linning Xu", "Lihan Jiang", "Yixuan Li", "Tianzhu Zhang", "Jiangmiao Pang", "Bo Dai"], "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.HC"], "comment": "Accepted by ICCV 2025", "summary": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and\nreal-time novel view synthesis, yet its lack of semantic understanding limits\nobject-level perception. In this work, we propose ObjectGS, an object-aware\nframework that unifies 3D scene reconstruction with semantic understanding.\nInstead of treating the scene as a unified whole, ObjectGS models individual\nobjects as local anchors that generate neural Gaussians and share object IDs,\nenabling precise object-level reconstruction. During training, we dynamically\ngrow or prune these anchors and optimize their features, while a one-hot ID\nencoding with a classification loss enforces clear semantic constraints. We\nshow through extensive experiments that ObjectGS not only outperforms\nstate-of-the-art methods on open-vocabulary and panoptic segmentation tasks,\nbut also integrates seamlessly with applications like mesh extraction and scene\nediting. Project page: https://ruijiezhu94.github.io/ObjectGS_page"}
{"id": "2507.15286", "pdf": "https://arxiv.org/pdf/2507.15286.pdf", "abs": "https://arxiv.org/abs/2507.15286", "title": "Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection", "authors": ["Navid Ayoobi", "Sadat Shahriar", "Arjun Mukherjee"], "categories": ["cs.CL"], "comment": null, "summary": "We present a novel evaluation paradigm for AI text detectors that prioritizes\nreal-world and equitable assessment. Current approaches predominantly report\nconventional metrics like AUROC, overlooking that even modest false positive\nrates constitute a critical impediment to practical deployment of detection\nsystems. Furthermore, real-world deployment necessitates predetermined\nthreshold configuration, making detector stability (i.e. the maintenance of\nconsistent performance across diverse domains and adversarial scenarios), a\ncritical factor. These aspects have been largely ignored in previous research\nand benchmarks. Our benchmark, SHIELD, addresses these limitations by\nintegrating both reliability and stability factors into a unified evaluation\nmetric designed for practical assessment. Furthermore, we develop a post-hoc,\nmodel-agnostic humanification framework that modifies AI text to more closely\nresemble human authorship, incorporating a controllable hardness parameter.\nThis hardness-aware approach effectively challenges current SOTA zero-shot\ndetection methods in maintaining both reliability and stability. (Data and\ncode: https://github.com/navid-aub/SHIELD-Benchmark)"}
{"id": "2507.15729", "pdf": "https://arxiv.org/pdf/2507.15729.pdf", "abs": "https://arxiv.org/abs/2507.15729", "title": "Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction", "authors": ["Jens V. Rüppel", "Andrey Rudenko", "Tim Schreiter", "Martin Magnusson", "Achim J. Lilienthal"], "categories": ["cs.RO", "cs.HC"], "comment": "This paper has been accepted to the 34th IEEE International\n  Conference on Robot and Human Interactive Communication (RO-MAN), which will\n  be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE.\n  Personal use of this material is permitted. Permission from IEEE must be\n  obtained for all other uses", "summary": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks."}
{"id": "2507.15328", "pdf": "https://arxiv.org/pdf/2507.15328.pdf", "abs": "https://arxiv.org/abs/2507.15328", "title": "On the Inevitability of Left-Leaning Political Bias in Aligned Language Models", "authors": ["Thilo Hagendorff"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The guiding principle of AI alignment is to train large language models\n(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are\nmounting concerns that LLMs exhibit a left-wing political bias. Yet, the\ncommitment to AI alignment cannot be harmonized with the latter critique. In\nthis article, I argue that intelligent systems that are trained to be harmless\nand honest must necessarily exhibit left-wing political bias. Normative\nassumptions underlying alignment objectives inherently concur with progressive\nmoral frameworks and left-wing principles, emphasizing harm avoidance,\ninclusivity, fairness, and empirical truthfulness. Conversely, right-wing\nideologies often conflict with alignment guidelines. Yet, research on political\nbias in LLMs is consistently framing its insights about left-leaning tendencies\nas a risk, as problematic, or concerning. This way, researchers are actively\narguing against AI alignment, tacitly fostering the violation of HHH\nprinciples."}
{"id": "2507.15743", "pdf": "https://arxiv.org/pdf/2507.15743.pdf", "abs": "https://arxiv.org/abs/2507.15743", "title": "Towards physician-centered oversight of conversational diagnostic AI", "authors": ["Elahe Vedadi", "David Barrett", "Natalie Harris", "Ellery Wulczyn", "Shashir Reddy", "Roma Ruparel", "Mike Schaekermann", "Tim Strother", "Ryutaro Tanno", "Yash Sharma", "Jihyeon Lee", "Cían Hughes", "Dylan Slack", "Anil Palepu", "Jan Freyberg", "Khaled Saab", "Valentin Liévin", "Wei-Hung Weng", "Tao Tu", "Yun Liu", "Nenad Tomasev", "Kavita Kulkarni", "S. Sara Mahdavi", "Kelvin Guu", "Joëlle Barral", "Dale R. Webster", "James Manyika", "Avinatan Hassidim", "Katherine Chou", "Yossi Matias", "Pushmeet Kohli", "Adam Rodman", "Vivek Natarajan", "Alan Karthikesalingam", "David Stutz"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Recent work has demonstrated the promise of conversational AI systems for\ndiagnostic dialogue. However, real-world assurance of patient safety means that\nproviding individual diagnoses and treatment plans is considered a regulated\nactivity by licensed professionals. Furthermore, physicians commonly oversee\nother team members in such activities, including nurse practitioners (NPs) or\nphysician assistants/associates (PAs). Inspired by this, we propose a framework\nfor effective, asynchronous oversight of the Articulate Medical Intelligence\nExplorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent\nsystem that performs history taking within guardrails, abstaining from\nindividualized medical advice. Afterwards, g-AMIE conveys assessments to an\noverseeing primary care physician (PCP) in a clinician cockpit interface. The\nPCP provides oversight and retains accountability of the clinical decision.\nThis effectively decouples oversight from intake and can thus happen\nasynchronously. In a randomized, blinded virtual Objective Structured Clinical\nExamination (OSCE) of text consultations with asynchronous oversight, we\ncompared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across\n60 scenarios, g-AMIE outperformed both groups in performing high-quality\nintake, summarizing cases, and proposing diagnoses and management plans for the\noverseeing PCP to review. This resulted in higher quality composite decisions.\nPCP oversight of g-AMIE was also more time-efficient than standalone PCP\nconsultations in prior work. While our study does not replicate existing\nclinical practices and likely underestimates clinicians' capabilities, our\nresults demonstrate the promise of asynchronous oversight as a feasible\nparadigm for diagnostic AI systems to operate under expert human oversight for\nenhancing real-world care."}
{"id": "2507.15337", "pdf": "https://arxiv.org/pdf/2507.15337.pdf", "abs": "https://arxiv.org/abs/2507.15337", "title": "Reasoning Models are Test Exploiters: Rethinking Multiple-Choice", "authors": ["Narun Raman", "Taylor Lundy", "Kevin Leyton-Brown"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "When evaluating Large Language Models (LLMs) in question-answering domains,\nit is common to ask the model to choose among a fixed set of choices (so-called\nmultiple-choice question-answering, or MCQA). Although downstream tasks of\ninterest typically do not provide systems with explicit options among which to\nchoose, this approach is nevertheless widely used because it makes it makes\nautomatic grading straightforward and has tended to produce challenging\nbenchmarks that correlate sufficiently well with downstream performance. This\npaper investigates the extent to which this trend continues to hold for\nstate-of-the-art reasoning models, describing a systematic evaluation of $15$\ndifferent question-answering benchmarks (e.g., MMLU, HLE) and $25$ different\nLLMs (including small models such as Qwen 7B and relatively large models such\nas Llama 70B). For each model-benchmark pair, we considered $5$ ways of\npresenting the model with questions, including variations on whether multiple\nchoices were offered to the model at all; whether \"none of the above\" sometimes\nreplaced the right answer; and whether the model was permitted to perform\nchain-of-thought reasoning before and/or after the choices were presented. MCQA\nremained a good proxy for the downstream performance of models as long as they\nwere allowed to perform chain-of-thought reasoning only before being presented\nwith the options among which they had to select. On the other hand, large\nmodels that were able to perform reasoning after being given a set of options\ntended to significantly outperform their free-text performance due to\nexploiting the information in the options. We conclude that MCQA is no longer a\ngood proxy for assessing downstream performance of state-of-the-art models, and\noffer practical guidelines for designing more robust, bias-resistant benchmarks\nthat better reflect LLMs' genuine reasoning capabilities."}
{"id": "2507.15846", "pdf": "https://arxiv.org/pdf/2507.15846.pdf", "abs": "https://arxiv.org/abs/2507.15846", "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "authors": ["Fei Tang", "Zhangxuan Gu", "Zhengxi Lu", "Xuyang Liu", "Shuheng Shen", "Changhua Meng", "Wen Wang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks."}
{"id": "2507.15339", "pdf": "https://arxiv.org/pdf/2507.15339.pdf", "abs": "https://arxiv.org/abs/2507.15339", "title": "LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators", "authors": ["Leanne Tan", "Gabriel Chua", "Ziyu Ge", "Roy Ka-Wei Lee"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Modern moderation systems increasingly support multiple languages, but often\nfail to address localisation and low-resource variants - creating safety gaps\nin real-world deployments. Small models offer a potential alternative to large\nLLMs, yet still demand considerable data and compute. We present LionGuard 2, a\nlightweight, multilingual moderation classifier tailored to the Singapore\ncontext, supporting English, Chinese, Malay, and partial Tamil. Built on\npre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2\noutperforms several commercial and open-source systems across 17 benchmarks,\nincluding both Singapore-specific and public English datasets. The system is\nactively deployed within the Singapore Government, demonstrating practical\nefficacy at scale. Our findings show that high-quality local data and robust\nmultilingual embeddings can achieve strong moderation performance, without\nfine-tuning large models. We release our model weights and part of our training\ndata to support future work on LLM safety."}
{"id": "2404.16660", "pdf": "https://arxiv.org/pdf/2404.16660.pdf", "abs": "https://arxiv.org/abs/2404.16660", "title": "Benchmarking Mobile Device Control Agents across Diverse Configurations", "authors": ["Juyong Lee", "Taywon Min", "Minyong An", "Dongyoon Hahm", "Haeone Lee", "Changyeon Kim", "Kimin Lee"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "Accepted to ICLR 2024 Workshop on Generative Models for Decision\n  Making (Spotlight) and CoLLAs 2025", "summary": "Mobile device control agents can largely enhance user interactions and\nproductivity by automating daily tasks. However, despite growing interest in\ndeveloping practical agents, the absence of a commonly adopted benchmark in\nthis area makes it challenging to quantify scientific progress. In this work,\nwe introduce B-MoCA: a novel benchmark with interactive environments for\nevaluating and developing mobile device control agents. To create a realistic\nbenchmark, we develop B-MoCA based on the Android operating system and define\n131 common daily tasks. Importantly, we incorporate a randomization feature\nthat changes the configurations of mobile devices, including user interface\nlayouts and language settings, to assess generalization performance. We\nbenchmark diverse agents, including agents employing large language models\n(LLMs) or multi-modal LLMs as well as agents trained with imitation learning\nusing human expert demonstrations. While these agents demonstrate proficiency\nin executing straightforward tasks, their poor performance on complex tasks\nhighlights significant opportunities for future research to improve\neffectiveness. Our source code is publicly available at\nhttps://b-moca.github.io."}
{"id": "2507.15347", "pdf": "https://arxiv.org/pdf/2507.15347.pdf", "abs": "https://arxiv.org/abs/2507.15347", "title": "Probing Information Distribution in Transformer Architectures through Entropy Analysis", "authors": ["Amedeo Buonanno", "Alessandro Rivetti", "Francesco A. N. Palmieri", "Giovanni Di Gennaro", "Gianmarco Romano"], "categories": ["cs.CL", "cs.LG"], "comment": "Presented to the Italian Workshop on Neural Networks (WIRN2025) and\n  it will appear in a Springer Chapter", "summary": "This work explores entropy analysis as a tool for probing information\ndistribution within Transformer-based architectures. By quantifying token-level\nuncertainty and examining entropy patterns across different stages of\nprocessing, we aim to investigate how information is managed and transformed\nwithin these models. As a case study, we apply the methodology to a GPT-based\nlarge language model, illustrating its potential to reveal insights into model\nbehavior and internal representations. This approach may offer insights into\nmodel behavior and contribute to the development of interpretability and\nevaluation frameworks for transformer-based models"}
{"id": "2501.06348", "pdf": "https://arxiv.org/pdf/2501.06348.pdf", "abs": "https://arxiv.org/abs/2501.06348", "title": "Why Automate This? Exploring the Connection between Time Use, Well-being and Robot Automation Across Social Groups", "authors": ["Ruchira Ray", "Leona Pang", "Sanjana Srivastava", "Li Fei-Fei", "Samantha Shorey", "Roberto Martín-Martín"], "categories": ["cs.HC", "cs.RO"], "comment": "20 pages, 14 figures", "summary": "Understanding the motivations underlying the human inclination to automate\ntasks is vital to developing truly helpful robots integrated into daily life.\nAccordingly, we ask: are individuals more inclined to automate chores based on\nthe time they consume or the feelings experienced while performing them? This\nstudy explores these preferences and whether they vary across different social\ngroups (i.e., gender category and income level). Leveraging data from the\nBEHAVIOR-1K dataset, the American Time-Use Survey, and the American Time-Use\nSurvey Well-Being Module, we investigate the relationship between the desire\nfor automation, time spent on daily activities, and their associated feelings -\nHappiness, Meaningfulness, Sadness, Painfulness, Stressfulness, or Tiredness.\nOur key findings show that, despite common assumptions, time spent does not\nstrongly relate to the desire for automation for the general population. For\nthe feelings analyzed, only happiness and pain are key indicators. Significant\ndifferences by gender and economic level also emerged: Women prefer to automate\nstressful activities, whereas men prefer to automate those that make them\nunhappy; mid-income individuals prioritize automating less enjoyable and\nmeaningful activities, while low and high-income show no significant\ncorrelations. We hope our research helps motivate technologies to develop\nrobots that match the priorities of potential users, moving domestic robotics\ntoward more socially relevant solutions. We open-source all the data, including\nan online tool that enables the community to replicate our analysis and explore\nadditional trends at https://hri1260.github.io/why-automate-this."}
{"id": "2507.15357", "pdf": "https://arxiv.org/pdf/2507.15357.pdf", "abs": "https://arxiv.org/abs/2507.15357", "title": "Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding", "authors": ["Elisa Sanchez-Bayona", "Rodrigo Agerri"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents a comprehensive evaluation of the capabilities of Large\nLanguage Models (LLMs) in metaphor interpretation across multiple datasets,\ntasks, and prompt configurations. Although metaphor processing has gained\nsignificant attention in Natural Language Processing (NLP), previous research\nhas been limited to single-dataset evaluations and specific task settings,\noften using artificially constructed data through lexical replacement. We\naddress these limitations by conducting extensive experiments using diverse\npublicly available datasets with inference and metaphor annotations, focusing\non Natural Language Inference (NLI) and Question Answering (QA) tasks. The\nresults indicate that LLMs' performance is more influenced by features like\nlexical overlap and sentence length than by metaphorical content, demonstrating\nthat any alleged emergent abilities of LLMs to understand metaphorical language\nare the result of a combination of surface-level features, in-context learning,\nand linguistic knowledge. This work provides critical insights into the current\ncapabilities and limitations of LLMs in processing figurative language,\nhighlighting the need for more realistic evaluation frameworks in metaphor\ninterpretation tasks. Data and code are publicly available."}
{"id": "2504.03253", "pdf": "https://arxiv.org/pdf/2504.03253.pdf", "abs": "https://arxiv.org/abs/2504.03253", "title": "Ultra-low-power ring-based wireless tinymouse", "authors": ["Yifan Li", "Masaaki Fukumoto", "Mohamed Kari", "Shigemi Ishida", "Akihito Noda", "Tomoyuki Yokota", "Takao Someya", "Yoshihiro Kawahara", "Ryo Takahashi"], "categories": ["cs.HC"], "comment": "arXiv admin note: text overlap with arXiv:2501.16674", "summary": "Wireless mouse rings offer subtle, reliable pointing interactions for\nwearable computing platforms. However, the small battery below 27 mAh in the\nminiature rings restricts the ring's continuous lifespan to just 1-10 hours,\nbecause current low-powered wireless communication such as BLE is\npower-consuming for ring's continuous use. The ring's short lifespan frequently\ndisrupts users' mouse use with the need for frequent charging. This paper\npresents picoRing mouse, enabling a continuous ring-based mouse interaction\nwith ultra-low-powered ring-to-wristband wireless communication. picoRing mouse\nemploys a coil-based impedance sensing named semi-passive inductive telemetry,\nallowing a wristband coil to capture a unique frequency response of a nearby\nring coil via a sensitive inductive coupling between the coils. The ring coil\nconverts the corresponding user's mouse input into the unique frequency\nresponse via an up to 449 uW mouse-driven modulation system. Therefore, the\ncontinuous use of picoRing mouse can last approximately 600 (8hrs use/day)-1000\n(4hrs use/day) hours on a single charge of a 27 mAh battery while supporting\nsubtle thumb-to-index scrolling and pressing interactions in real-world\nwearable computing situations."}
{"id": "2507.15375", "pdf": "https://arxiv.org/pdf/2507.15375.pdf", "abs": "https://arxiv.org/abs/2507.15375", "title": "STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models", "authors": ["Cheng-Han Chiang", "Xiaofei Wang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Shujie Liu", "Zhendong Wang", "Zhengyuan Yang", "Hung-yi Lee", "Lijuan Wang"], "categories": ["cs.CL", "eess.AS"], "comment": "Work in progress. Project page: https://d223302.github.io/STITCH/", "summary": "Spoken Language Models (SLMs) are designed to take speech inputs and produce\nspoken responses. However, current SLMs lack the ability to perform an\ninternal, unspoken thinking process before responding. In contrast, humans\ntypically engage in complex mental reasoning internally, enabling them to\ncommunicate ideas clearly and concisely. Thus, integrating an unspoken thought\nprocess into SLMs is highly desirable. While naively generating a complete\nchain-of-thought (CoT) reasoning before starting to talk can enable thinking\nfor SLMs, this induces additional latency for the speech response, as the CoT\nreasoning can be arbitrarily long. To solve this issue, we propose Stitch, a\nnovel generation method that alternates between the generation of unspoken\nreasoning chunks and spoken response chunks. Since the audio duration of a\nchunk of spoken response is much longer than the time to generate the tokens in\na chunk of spoken response, we use the remaining free time to generate the\nunspoken reasoning tokens. When a chunk of audio is played to the user, the\nmodel continues to generate the next unspoken reasoning chunk, achieving\nsimultaneous thinking and talking. Remarkably, Stitch matches the latency of\nbaselines that cannot generate unspoken CoT by design while outperforming those\nbaselines by 15% on math reasoning datasets; Stitch also performs equally well\non non-reasoning datasets as those baseline models. Some animations and\ndemonstrations are on the project page: https://d223302.github.io/STITCH."}
{"id": "2505.21385", "pdf": "https://arxiv.org/pdf/2505.21385.pdf", "abs": "https://arxiv.org/abs/2505.21385", "title": "EEGVid: Dynamic Vision from EEG Brain Recordings, How much does EEG know?", "authors": ["Prajwal Singh", "Anupam Sharma", "Pankaj Pandey", "Krishna Miyapuram", "Shanmuganathan Raman"], "categories": ["cs.HC"], "comment": null, "summary": "Reconstructing and understanding dynamic visual information (video) from\nbrain EEG recordings is challenging due to the non-stationary nature of EEG\nsignals, their low signal-to-noise ratio (SNR), and the limited availability of\nEEG-Video stimulus datasets. Most recent studies have focused on reconstructing\nstatic images from EEG recordings. In this work, we propose a framework to\nreconstruct dynamic visual stimuli from EEG data and conduct an in-depth study\nof the information encoded in EEG signals. Our approach first trains a feature\nextraction network using a triplet-based contrastive learning strategy within\nan EEG-video generation framework. The extracted EEG features are then used for\nvideo synthesis with a modified StyleGAN-ADA, which incorporates temporal\ninformation as conditioning. Additionally, we analyze how different brain\nregions contribute to processing dynamic visual stimuli. Through several\nempirical studies, we evaluate the effectiveness of our framework and\ninvestigate how much dynamic visual information can be inferred from EEG\nsignals. The inferences we derive through our extensive studies would be of\nimmense value to future research on extracting visual dynamics from EEG."}
{"id": "2507.15378", "pdf": "https://arxiv.org/pdf/2507.15378.pdf", "abs": "https://arxiv.org/abs/2507.15378", "title": "AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming", "authors": ["Jierui Li", "Raymond Mooney"], "categories": ["cs.CL"], "comment": "19 pages, pre-print only", "summary": "Recent progress in LLMs, such as reasoning models, has demonstrated strong\nabilities to solve complex competitive programming problems, often rivaling top\nhuman competitors. However, it remains underexplored whether these abilities\ngeneralize to relevant domains that are less seen during training. To address\nthis, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'\nability to identify algorithmically similar problems (ASPs)-problems that can\nbe solved using similar algorithmic approaches. AlgoSimBench consists of 1317\nproblems, annotated with 231 distinct fine-grained algorithm tags, from which\nwe curate 402 multiple-choice questions (MCQs), where each question presents\none algorithmically similar problem alongside three textually similar but\nalgorithmically dissimilar distractors. Our evaluation reveals that LLMs\nstruggle to identify ASPs, with the best-performing model (o3-mini) achieving\nonly 65.9% accuracy on the MCQ task. To address this challenge, we propose\nattempted solution matching (ASM), a novel method for improving problem\nsimilarity detection. On our MCQ task, ASM yields an absolute accuracy\nimprovement of 6.7% to 11.7% across different models. We also evaluated code\nembedding models and retrieval methods on similar problem identification. While\nthe adversarial selection of problems degrades the performance to be less than\nrandom, we found that simply summarizing the problem to remove narrative\nelements eliminates the effect, and combining ASM with a keyword-prioritized\nmethod, BM25, can yield up to 52.2% accuracy. Code and data are available at\ngithub.com"}
{"id": "2506.12540", "pdf": "https://arxiv.org/pdf/2506.12540.pdf", "abs": "https://arxiv.org/abs/2506.12540", "title": "Regulating Next-Generation Implantable Brain-Computer Interfaces: Recommendations for Ethical Development and Implementation", "authors": ["Renee Sirbu", "Jessica Morley", "Tyler Schroder", "Raghavendra Pradyumna Pothukuchi", "Muhammed Ugur", "Abhishek Bhattacharjee", "Luciano Floridi"], "categories": ["cs.HC", "cs.CY", "cs.ET"], "comment": "35 pages, 3 tables, 2 appendices", "summary": "Brain-computer interfaces offer significant therapeutic opportunities for a\nvariety of neurophysiological and neuropsychiatric disorders and may perhaps\none day lead to augmenting the cognition and decision-making of the healthy\nbrain. However, existing regulatory frameworks designed for implantable medical\ndevices are inadequate to address the unique ethical, legal, and social risks\nassociated with next-generation networked brain-computer interfaces. In this\narticle, we make nine recommendations to support developers in the design of\nBCIs and nine recommendations to support policymakers in the application of\nBCIs, drawing insights from the regulatory history of IMDs and principles from\nAI ethics. We begin by outlining the historical development of IMDs and the\nregulatory milestones that have shaped their oversight. Next, we summarize\nsimilarities between IMDs and emerging implantable BCIs, identifying existing\nprovisions for their regulation. We then use two case studies of emerging\ncutting-edge BCIs, the HALO and SCALO computer systems, to highlight\ndistinctive features in the design and application of next-generation BCIs\narising from contemporary chip architectures, which necessitate reevaluating\nregulatory approaches. We identify critical ethical considerations for these\nBCIs, including unique conceptions of autonomy, identity, and mental privacy.\nBased on these insights, we suggest potential avenues for the ethical\nregulation of BCIs, emphasizing the importance of interdisciplinary\ncollaboration and proactive mitigation of potential harms. The goal is to\nsupport the responsible design and application of new BCIs, ensuring their safe\nand ethical integration into medical practice."}
{"id": "2507.15501", "pdf": "https://arxiv.org/pdf/2507.15501.pdf", "abs": "https://arxiv.org/abs/2507.15501", "title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution", "authors": ["Alexandru Coca", "Mark Gaynor", "Zhenxing Zhang", "Jianpeng Cheng", "Bo-Hsiang Tseng", "Pete Boothroyd", "Héctor Martinez Alonso", "Diarmuid Ó Séaghdha", "Anders Johannsen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "37 pages, 22 figures. To appear at ACL 2025", "summary": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation."}
{"id": "2507.00286", "pdf": "https://arxiv.org/pdf/2507.00286.pdf", "abs": "https://arxiv.org/abs/2507.00286", "title": "\"Before, I Asked My Mom, Now I Ask ChatGPT\": Visual Privacy Management with Generative AI for Blind and Low-Vision People", "authors": ["Tanusree Sharma", "Yu-Yun Tseng", "Lotus Zhang", "Ayae Ide", "Kelly Avery Mack", "Leah Findlater", "Danna Gurari", "Yang Wang"], "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to\ninterpret and manage visual content in their daily lives. While such tools can\nenhance the accessibility of visual content and so enable greater user\nindependence, they also introduce complex challenges around visual privacy. In\nthis paper, we investigate the current practices and future design preferences\nof blind and low vision individuals through an interview study with 21\nparticipants. Our findings reveal a range of current practices with GenAI that\nbalance privacy, efficiency, and emotional agency, with users accounting for\nprivacy risks across six key scenarios, such as self-presentation,\nindoor/outdoor spatial privacy, social sharing, and handling professional\ncontent. Our findings reveal design preferences, including on-device\nprocessing, zero-retention guarantees, sensitive content redaction,\nprivacy-aware appearance indicators, and multimodal tactile mirrored\ninteraction methods. We conclude with actionable design recommendations to\nsupport user-centered visual privacy through GenAI, expanding the notion of\nprivacy and responsible handling of others data."}
{"id": "2507.15512", "pdf": "https://arxiv.org/pdf/2507.15512.pdf", "abs": "https://arxiv.org/abs/2507.15512", "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models", "authors": ["Kaiyan Chang", "Yonghao Shi", "Chenglong Wang", "Hang Zhou", "Chi Hu", "Xiaoqian Liu", "Yingfeng Luo", "Yuan Ge", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs."}
{"id": "2502.02883", "pdf": "https://arxiv.org/pdf/2502.02883.pdf", "abs": "https://arxiv.org/abs/2502.02883", "title": "SensorChat: Answering Qualitative and Quantitative Questions during Long-Term Multimodal Sensor Interactions", "authors": ["Xiaofan Yu", "Lanxiang Hu", "Benjamin Reichman", "Dylan Chu", "Rushil Chandrupatla", "Xiyuan Zhang", "Larry Heck", "Tajana Rosing"], "categories": ["cs.AI", "cs.HC"], "comment": "To appear in IMWUT'25. Code is available at:\n  https://github.com/Orienfish/SensorChat", "summary": "Natural language interaction with sensing systems is crucial for addressing\nusers' personal concerns and providing health-related insights into their daily\nlives. When a user asks a question, the system automatically analyzes the full\nhistory of sensor data, extracts relevant information, and generates an\nappropriate response. However, existing systems are limited to short-duration\n(e.g., one minute) or low-frequency (e.g., daily step count) sensor data. In\naddition, they struggle with quantitative questions that require precise\nnumerical answers. In this work, we introduce SensorChat, the first end-to-end\nQA system designed for daily life monitoring using long-duration,\nhigh-frequency time series data. Given raw sensor signals spanning multiple\ndays and a user-defined natural language question, SensorChat generates\nsemantically meaningful responses that directly address user concerns.\nSensorChat effectively handles both quantitative questions that require\nnumerical precision and qualitative questions that require high-level reasoning\nto infer subjective insights. To achieve this, SensorChat uses an innovative\nthree-stage pipeline including question decomposition, sensor data query, and\nanswer assembly. The first and third stages leverage Large Language Models\n(LLMs) to interpret human queries and generate responses. The intermediate\nquerying stage extracts relevant information from the complete sensor data\nhistory. Real-world implementations demonstrate SensorChat's capability for\nreal-time interactions on a cloud server while also being able to run entirely\non edge platforms after quantization. Comprehensive QA evaluations show that\nSensorChat achieves 93% higher answer accuracy than the best performing\nstate-of-the-art systems on quantitative questions. Furthermore, a user study\nwith eight volunteers highlights SensorChat's effectiveness in answering\nqualitative questions."}
{"id": "2507.15557", "pdf": "https://arxiv.org/pdf/2507.15557.pdf", "abs": "https://arxiv.org/abs/2507.15557", "title": "Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification", "authors": ["Vitaly Protasov", "Nikolay Babakov", "Daryna Dementieva", "Alexander Panchenko"], "categories": ["cs.CL"], "comment": "preprint", "summary": "Despite recent progress in large language models (LLMs), evaluation of text\ngeneration tasks such as text style transfer (TST) remains a significant\nchallenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)\nrevealed a substantial gap between automatic metrics and human judgments.\nMoreover, most prior work focuses exclusively on English, leaving multilingual\nTST evaluation largely unexplored. In this paper, we perform the first\ncomprehensive multilingual study on evaluation of text detoxification system\nacross nine languages: English, Spanish, German, Chinese, Arabic, Hindi,\nUkrainian, Russian, Amharic. Drawing inspiration from the machine translation,\nwe assess the effectiveness of modern neural-based evaluation models alongside\nprompting-based LLM-as-a-judge approaches. Our findings provide a practical\nrecipe for designing more reliable multilingual TST evaluation pipeline in the\ntext detoxification case."}
{"id": "2503.05609", "pdf": "https://arxiv.org/pdf/2503.05609.pdf", "abs": "https://arxiv.org/abs/2503.05609", "title": "Decoding Safety Feedback from Diverse Raters: A Data-driven Lens on Responsiveness to Severity", "authors": ["Pushkar Mishra", "Charvi Rastogi", "Stephen R. Pfohl", "Alicia Parrish", "Tian Huey Teh", "Roma Patel", "Mark Diaz", "Ding Wang", "Michela Paganini", "Vinodkumar Prabhakaran", "Lora Aroyo", "Verena Rieser"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Ensuring the safety of Generative AI requires a nuanced understanding of\npluralistic viewpoints. In this paper, we introduce a novel data-driven\napproach for interpreting granular ratings in pluralistic datasets.\nSpecifically, we address the challenge of analyzing nuanced differences in\nsafety feedback from a diverse population expressed via ordinal scales (e.g., a\nLikert scale). We distill non-parametric responsiveness metrics that quantify\nthe consistency of raters in scoring varying levels of the severity of safety\nviolations. Leveraging a publicly available pluralistic dataset of safety\nfeedback on AI-generated content as our case study, we investigate how raters\nfrom different demographic groups (age, gender, ethnicity) use an ordinal scale\nto express their perceptions of the severity of violations. We apply our\nmetrics across violation types, demonstrating their utility in extracting\nnuanced insights that are crucial for aligning AI systems reliably in\nmulti-cultural contexts. We show that our approach can inform rater selection\nand feedback interpretation by capturing nuanced viewpoints across different\ndemographic groups, hence improving the quality of pluralistic data collection\nand in turn contributing to more robust AI development."}
{"id": "2507.15576", "pdf": "https://arxiv.org/pdf/2507.15576.pdf", "abs": "https://arxiv.org/abs/2507.15576", "title": "Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging", "authors": ["Nicolas Poggi", "Shashank Agnihotri", "Margret Keuper"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Terahertz (THz) imaging enables non-invasive analysis for applications such\nas security screening and material classification, but effective image\nclassification remains challenging due to limited annotations, low resolution,\nand visual ambiguity. We introduce In-Context Learning (ICL) with\nVision-Language Models (VLMs) as a flexible, interpretable alternative that\nrequires no fine-tuning. Using a modality-aligned prompting framework, we adapt\ntwo open-weight VLMs to the THz domain and evaluate them under zero-shot and\none-shot settings. Our results show that ICL improves classification and\ninterpretability in low-data regimes. This is the first application of\nICL-enhanced VLMs to THz imaging, offering a promising direction for\nresource-constrained scientific domains. Code:\n\\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub\nrepository}."}
{"id": "2506.11092", "pdf": "https://arxiv.org/pdf/2506.11092.pdf", "abs": "https://arxiv.org/abs/2506.11092", "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation", "authors": ["Jubin Abhishek Soni", "Amit Anand", "Rajesh Kumar Pandey", "Aniket Abhishek Soni"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "We are withdrawing the submission in order to thoroughly revise the\n  work", "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."}
{"id": "2507.15586", "pdf": "https://arxiv.org/pdf/2507.15586.pdf", "abs": "https://arxiv.org/abs/2507.15586", "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "authors": ["Xinping Zhao", "Shouzheng Huang", "Yan Zhong", "Xinshuo Hu", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL"], "comment": "16 pages, 7 Figures, 10 Tables", "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems."}
{"id": "2507.15600", "pdf": "https://arxiv.org/pdf/2507.15600.pdf", "abs": "https://arxiv.org/abs/2507.15600", "title": "Conflicting narratives and polarization on social media", "authors": ["Armin Pournaki"], "categories": ["cs.CL", "cs.SI"], "comment": "30 pages, 7 figures", "summary": "Narratives are key interpretative devices by which humans make sense of\npolitical reality. In this work, we show how the analysis of conflicting\nnarratives, i.e. conflicting interpretive lenses through which political\nreality is experienced and told, provides insight into the discursive\nmechanisms of polarization and issue alignment in the public sphere. Building\nupon previous work that has identified ideologically polarized issues in the\nGerman Twittersphere between 2021 and 2023, we analyze the discursive dimension\nof polarization by extracting textual signals of conflicting narratives from\ntweets of opposing opinion groups. Focusing on a selection of salient issues\nand events (the war in Ukraine, Covid, climate change), we show evidence for\nconflicting narratives along two dimensions: (i) different attributions of\nactantial roles to the same set of actants (e.g. diverging interpretations of\nthe role of NATO in the war in Ukraine), and (ii) emplotment of different\nactants for the same event (e.g. Bill Gates in the right-leaning Covid\nnarrative). Furthermore, we provide first evidence for patterns of narrative\nalignment, a discursive strategy that political actors employ to align opinions\nacross issues. These findings demonstrate the use of narratives as an\nanalytical lens into the discursive mechanisms of polarization."}
{"id": "2507.15641", "pdf": "https://arxiv.org/pdf/2507.15641.pdf", "abs": "https://arxiv.org/abs/2507.15641", "title": "Leveraging Context for Multimodal Fallacy Classification in Political Debates", "authors": ["Alessio Pittiglio"], "categories": ["cs.CL", "cs.AI"], "comment": "12th Workshop on Argument Mining (ArgMining 2025) @ ACL 2025", "summary": "In this paper, we present our submission to the MM-ArgFallacy2025 shared\ntask, which aims to advance research in multimodal argument mining, focusing on\nlogical fallacies in political debates. Our approach uses pretrained\nTransformer-based models and proposes several ways to leverage context. In the\nfallacy classification subtask, our models achieved macro F1-scores of 0.4444\n(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed\nperformance comparable to the text-only model, suggesting potential for\nimprovements."}
{"id": "2507.15675", "pdf": "https://arxiv.org/pdf/2507.15675.pdf", "abs": "https://arxiv.org/abs/2507.15675", "title": "P3: Prompts Promote Prompting", "authors": ["Xinyu Zhang", "Yuanquan Hu", "Fangchao Liu", "Zhicheng Dou"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 findings", "summary": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains."}
{"id": "2507.15698", "pdf": "https://arxiv.org/pdf/2507.15698.pdf", "abs": "https://arxiv.org/abs/2507.15698", "title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models", "authors": ["Congmin Zheng", "Jiachen Zhu", "Jianghao Lin", "Xinyi Dai", "Yong Yu", "Weinan Zhang", "Mengyue Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs."}
{"id": "2507.15706", "pdf": "https://arxiv.org/pdf/2507.15706.pdf", "abs": "https://arxiv.org/abs/2507.15706", "title": "Compositional Understanding in Signaling Games", "authors": ["David Peter Wallis Freeborn"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Receivers in standard signaling game models struggle with learning\ncompositional information. Even when the signalers send compositional messages,\nthe receivers do not interpret them compositionally. When information from one\nmessage component is lost or forgotten, the information from other components\nis also erased. In this paper I construct signaling game models in which\ngenuine compositional understanding evolves. I present two new models: a\nminimalist receiver who only learns from the atomic messages of a signal, and a\ngeneralist receiver who learns from all of the available information. These\nmodels are in many ways simpler than previous alternatives, and allow the\nreceivers to learn from the atomic components of messages."}
{"id": "2507.15707", "pdf": "https://arxiv.org/pdf/2507.15707.pdf", "abs": "https://arxiv.org/abs/2507.15707", "title": "Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?", "authors": ["Seok Hwan Song", "Mohna Chakraborty", "Qi Li", "Wallapak Tavanapong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been evaluated using diverse question\ntypes, e.g., multiple-choice, true/false, and short/long answers. This study\nanswers an unexplored question about the impact of different question types on\nLLM accuracy on reasoning tasks. We investigate the performance of five LLMs on\nthree different types of questions using quantitative and deductive reasoning\ntasks. The performance metrics include accuracy in the reasoning steps and\nchoosing the final answer. Key Findings: (1) Significant differences exist in\nLLM performance across different question types. (2) Reasoning accuracy does\nnot necessarily correlate with the final selection accuracy. (3) The number of\noptions and the choice of words, influence LLM performance."}
{"id": "2507.15714", "pdf": "https://arxiv.org/pdf/2507.15714.pdf", "abs": "https://arxiv.org/abs/2507.15714", "title": "Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning", "authors": ["Tian Li", "Yujian Sun", "Huizhi Liang"], "categories": ["cs.CL"], "comment": null, "summary": "The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,\nintroduces an emotion recognition challenge spanning over 28 languages. This\ncompetition encourages researchers to explore more advanced approaches to\naddress the challenges posed by the diversity of emotional expressions and\nbackground variations. It features two tracks: multi-label classification\n(Track A) and emotion intensity prediction (Track B), covering six emotion\ncategories: anger, fear, joy, sadness, surprise, and disgust. In our work, we\nsystematically explore the benefits of two contrastive learning approaches:\nsample-based (Contrastive Reasoning Calibration) and generation-based (DPO,\nSimPO) contrastive learning. The sample-based contrastive approach trains the\nmodel by comparing two samples to generate more reliable predictions. The\ngeneration-based contrastive approach trains the model to differentiate between\ncorrect and incorrect generations, refining its prediction. All models are\nfine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A\nand 6th place in Track B for English, while ranking among the top-tier\nperforming systems for other languages."}
{"id": "2507.15715", "pdf": "https://arxiv.org/pdf/2507.15715.pdf", "abs": "https://arxiv.org/abs/2507.15715", "title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs", "authors": ["Alina Hyk", "Kiera McCormick", "Mian Zhong", "Ioana Ciucă", "Sanjib Sharma", "John F Wu", "J. E. G. Peek", "Kartheik G. Iyer", "Ziang Xiao", "Anjalie Field"], "categories": ["cs.CL", "astro-ph.IM"], "comment": "Accepted to the Conference on Language Modeling 2025 (COLM), 22\n  pages, 6 figures", "summary": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research."}
{"id": "2507.15717", "pdf": "https://arxiv.org/pdf/2507.15717.pdf", "abs": "https://arxiv.org/abs/2507.15717", "title": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning", "authors": ["Sahana Srinivasan", "Xuguang Ai", "Thaddaeus Wai Soon Lo", "Aidan Gilson", "Minjie Zou", "Ke Zou", "Hyunjae Kim", "Mingjia Yang", "Krithi Pushpanathan", "Samantha Yew", "Wan Ting Loke", "Jocelyn Goh", "Yibing Chen", "Yiming Kong", "Emily Yuelei Fu", "Michelle Ongyong Hui", "Kristen Nwanyanwu", "Amisha Dave", "Kelvin Zhenghao Li", "Chen-Hsin Sun", "Mark Chia", "Gabriel Dawei Yang", "Wendy Meihua Wong", "David Ziyou Chen", "Dianbo Liu", "Maxwell Singer", "Fares Antaki", "Lucian V Del Priore", "Jost Jonas", "Ron Adelman", "Qingyu Chen", "Yih-Chung Tham"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current benchmarks evaluating large language models (LLMs) in ophthalmology\nare limited in scope and disproportionately prioritise accuracy. We introduce\nBELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive\nevaluation benchmark developed through multiple rounds of expert checking by 13\nophthalmologists. BELO assesses ophthalmology-related clinical accuracy and\nreasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we\ncurated ophthalmology-specific multiple-choice-questions (MCQs) from diverse\nmedical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset\nunderwent multiple rounds of expert checking. Duplicate and substandard\nquestions were systematically removed. Ten ophthalmologists refined the\nexplanations of each MCQ's correct answer. This was further adjudicated by\nthree senior ophthalmologists. To illustrate BELO's utility, we evaluated six\nLLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)\nusing accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,\nBARTScore, METEOR, and AlignScore). In a further evaluation involving human\nexperts, two ophthalmologists qualitatively reviewed 50 randomly selected\noutputs for accuracy, comprehensiveness, and completeness. BELO consists of 900\nhigh-quality, expert-reviewed questions aggregated from five sources: BCSC\n(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public\nleaderboard has been established to promote transparent evaluation and\nreporting. Importantly, the BELO dataset will remain a hold-out,\nevaluation-only benchmark to ensure fair and reproducible comparisons of future\nmodels."}
{"id": "2507.15736", "pdf": "https://arxiv.org/pdf/2507.15736.pdf", "abs": "https://arxiv.org/abs/2507.15736", "title": "Understanding Large Language Models' Ability on Interdisciplinary Research", "authors": ["Yuanhao Shen", "Daniel Xavier de Sousa", "Ricardo Marçal", "Ali Asad", "Hongyu Guo", "Xiaodan Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have revealed their\nimpressive ability to perform multi-step, logic-driven reasoning across complex\ndomains, positioning them as powerful tools and collaborators in scientific\ndiscovery while challenging the long-held view that inspiration-driven ideation\nis uniquely human. However, the lack of a dedicated benchmark that evaluates\nLLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings\nposes a critical barrier to fully understanding their strengths and\nlimitations. To address this gap, we introduce IDRBench -- a pioneering\nbenchmark featuring an expert annotated dataset and a suite of tasks tailored\nto evaluate LLMs' capabilities in proposing valuable research ideas from\ndifferent scientific domains for interdisciplinary research. This benchmark\naims to provide a systematic framework for assessing LLM performance in\ncomplex, cross-domain scientific research. Our dataset consists of scientific\npublications sourced from the ArXiv platform covering six distinct disciplines,\nand is annotated by domain experts with diverse academic backgrounds. To ensure\nhigh-quality annotations, we emphasize clearly defined dimensions that\ncharacterize authentic interdisciplinary research. The design of evaluation\ntasks in IDRBench follows a progressive, real-world perspective, reflecting the\nnatural stages of interdisciplinary research development, including 1) IDR\nPaper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.\nUsing IDRBench, we construct baselines across 10 LLMs and observe that despite\nfostering some level of IDR awareness, LLMs still struggle to produce quality\nIDR ideas. These findings could not only spark new research directions, but\nalso help to develop next-generation LLMs that excel in interdisciplinary\nresearch."}
{"id": "2507.15742", "pdf": "https://arxiv.org/pdf/2507.15742.pdf", "abs": "https://arxiv.org/abs/2507.15742", "title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme", "authors": ["Paul Sheridan", "Zeyad Ahmed", "Aitazaz A. Farooque"], "categories": ["cs.CL", "cs.IR", "math.ST", "stat.TH"], "comment": "23 pages, 4 tables", "summary": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably\nthe most celebrated mathematical expression in the history of information\nretrieval. Conceived as a simple heuristic quantifying the extent to which a\ngiven term's occurrences are concentrated in any one given document out of\nmany, TF-IDF and its many variants are routinely used as term-weighting schemes\nin diverse text analysis applications. There is a growing body of scholarship\ndedicated to placing TF-IDF on a sound theoretical foundation. Building on that\ntradition, this paper justifies the use of TF-IDF to the statistics community\nby demonstrating how the famed expression can be understood from a significance\ntesting perspective. We show that the common TF-IDF variant TF-ICF is, under\nmild regularity conditions, closely related to the negative logarithm of the\n$p$-value from a one-tailed version of Fisher's exact test of statistical\nsignificance. As a corollary, we establish a connection between TF-IDF and the\nsaid negative log-transformed $p$-value under certain idealized assumptions. We\nfurther demonstrate, as a limiting case, that this same quantity converges to\nTF-IDF in the limit of an infinitely large document collection. The Fisher's\nexact test justification of TF-IDF equips the working statistician with a ready\nexplanation of the term-weighting scheme's long-established effectiveness."}
{"id": "2507.15752", "pdf": "https://arxiv.org/pdf/2507.15752.pdf", "abs": "https://arxiv.org/abs/2507.15752", "title": "DialogueForge: LLM Simulation of Human-Chatbot Dialogue", "authors": ["Ruizhe Zhu", "Hao Zhu", "Yaxuan Li", "Syang Zhou", "Shijing Cai", "Malgorzata Lazuka", "Elliott Ash"], "categories": ["cs.CL", "cs.AI"], "comment": "For our code and data, see\n  https://github.com/nerchio/Human_Chatbot-Generation", "summary": "Collecting human-chatbot dialogues typically demands substantial manual\neffort and is time-consuming, which limits and poses challenges for research on\nconversational AI. In this work, we propose DialogueForge - a framework for\ngenerating AI-simulated conversations in human-chatbot style. To initialize\neach generated conversation, DialogueForge uses seed prompts extracted from\nreal human-chatbot interactions. We test a variety of LLMs to simulate the\nhuman chatbot user, ranging from state-of-the-art proprietary models to\nsmall-scale open-source LLMs, and generate multi-turn dialogues tailored to\nspecific tasks. In addition, we explore fine-tuning techniques to enhance the\nability of smaller models to produce indistinguishable human-like dialogues. We\nevaluate the quality of the simulated conversations and compare different\nmodels using the UniEval and GTEval evaluation protocols. Our experiments show\nthat large proprietary models (e.g., GPT-4o) generally outperform others in\ngenerating more realistic dialogues, while smaller open-source models (e.g.,\nLlama, Mistral) offer promising performance with greater customization. We\ndemonstrate that the performance of smaller models can be significantly\nimproved by employing supervised fine-tuning techniques. Nevertheless,\nmaintaining coherent and natural long-form human-like dialogues remains a\ncommon challenge across all models."}
{"id": "2507.15759", "pdf": "https://arxiv.org/pdf/2507.15759.pdf", "abs": "https://arxiv.org/abs/2507.15759", "title": "Interaction as Intelligence: Deep Research With Human-AI Partnership", "authors": ["Lyumanshan Ye", "Xiaojie Cai", "Xinkai Wang", "Junfei Wang", "Xiangkun Hu", "Jiadi Su", "Yang Nan", "Sihan Wang", "Bohan Zhang", "Xiaoze Fan", "Jinbin Luo", "Yuxiang Zheng", "Tianze Xu", "Dayuan Fu", "Yunze Wu", "Pengrui Lu", "Zengzhi Wang", "Yiwei Qin", "Zhen Huang", "Yan Ma", "Zhulin Hu", "Haoyang Zou", "Tiantian Mi", "Yixin Ye", "Ethan Chern", "Pengfei Liu"], "categories": ["cs.CL"], "comment": "30 pages, 10 figures", "summary": "This paper introduces \"Interaction as Intelligence\" research series,\npresenting a reconceptualization of human-AI relationships in deep research\ntasks. Traditional approaches treat interaction merely as an interface for\naccessing AI capabilities-a conduit between human intent and machine output. We\npropose that interaction itself constitutes a fundamental dimension of\nintelligence. As AI systems engage in extended thinking processes for research\ntasks, meaningful interaction transitions from an optional enhancement to an\nessential component of effective intelligence. Current deep research systems\nadopt an \"input-wait-output\" paradigm where users initiate queries and receive\nresults after black-box processing. This approach leads to error cascade\neffects, inflexible research boundaries that prevent question refinement during\ninvestigation, and missed opportunities for expertise integration. To address\nthese limitations, we introduce Deep Cognition, a system that transforms the\nhuman role from giving instructions to cognitive oversight-a mode of engagement\nwhere humans guide AI thinking processes through strategic intervention at\ncritical junctures. Deep cognition implements three key innovations:\n(1)Transparent, controllable, and interruptible interaction that reveals AI\nreasoning and enables intervention at any point; (2)Fine-grained bidirectional\ndialogue; and (3)Shared cognitive context where the system observes and adapts\nto user behaviors without explicit instruction. User evaluation demonstrates\nthat this cognitive oversight paradigm outperforms the strongest baseline\nacross six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),\nReal-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),\nResults-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on\nchallenging research problems show 31.8% to 50.0% points of improvements over\ndeep research systems."}
{"id": "2507.15773", "pdf": "https://arxiv.org/pdf/2507.15773.pdf", "abs": "https://arxiv.org/abs/2507.15773", "title": "Supernova: Achieving More with Less in Transformer Architectures", "authors": ["Andrei-Valentin Tanase", "Elena Pelican"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Supernova, a 650M-parameter decoder-only transformer that\ndemonstrates how careful architectural design and tokenization innovation can\nachieve the performance of larger models while maintaining computational\nefficiency. Our architecture combines Rotary Positional Embeddings (RoPE),\nGrouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for\ncomputational efficiency, and SwiGLU activation functions. A critical\ninnovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which\nachieves state-of-the-art compression performance. Through detailed analysis,\nwe show that Supernova achieves 90% of the performance of 1B-parameter models\nwhile using 53% fewer parameters and requiring only 100B training tokens--an\norder of magnitude less than competing models. Our findings challenge the\nprevailing scaling paradigm, demonstrating that architectural efficiency and\ntokenization quality can compensate for reduced parameter counts."}
{"id": "2507.15778", "pdf": "https://arxiv.org/pdf/2507.15778.pdf", "abs": "https://arxiv.org/abs/2507.15778", "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR", "authors": ["Jiakang Wang", "Runze Liu", "Fuzheng Zhang", "Xiu Li", "Guorui Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR."}
{"id": "2507.15779", "pdf": "https://arxiv.org/pdf/2507.15779.pdf", "abs": "https://arxiv.org/abs/2507.15779", "title": "Reservoir Computing as a Language Model", "authors": ["Felix Köster", "Atsushi Uchida"], "categories": ["cs.CL"], "comment": "8 pages, 5 figures, 1 table", "summary": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance."}
{"id": "2507.15823", "pdf": "https://arxiv.org/pdf/2507.15823.pdf", "abs": "https://arxiv.org/abs/2507.15823", "title": "Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work", "authors": ["Anton Abilov", "Ke Zhang", "Hemank Lamba", "Elizabeth M. Olson", "Joel R. Tetreault", "Alejandro Jaimes"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Publications in the AI for Good space have tended to focus on the research\nand model development that can support high-impact applications. However, very\nfew AI for Good papers discuss the process of deploying and collaborating with\nthe partner organization, and the resulting real-world impact. In this work, we\nshare details about the close collaboration with a humanitarian-to-humanitarian\n(H2H) organization and how to not only deploy the AI model in a\nresource-constrained environment, but also how to maintain it for continuous\nperformance updates, and share key takeaways for practitioners."}
{"id": "2507.15849", "pdf": "https://arxiv.org/pdf/2507.15849.pdf", "abs": "https://arxiv.org/abs/2507.15849", "title": "The Impact of Language Mixing on Bilingual LLM Reasoning", "authors": ["Yihao Li", "Jiayi Xin", "Miranda Muqing Miao", "Qi Long", "Lyle Ungar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing--alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We demonstrate that language mixing can\nenhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6\npercentage points on math reasoning tasks. Additionally, a lightweight probe\ncan be trained to predict whether a potential language switch would benefit or\nharm reasoning, and when used to guide decoding, increases accuracy by up to\n6.25 percentage points. Our findings suggest that language mixing is not merely\na byproduct of multilingual training, but is a strategic reasoning behavior."}
{"id": "2507.15850", "pdf": "https://arxiv.org/pdf/2507.15850.pdf", "abs": "https://arxiv.org/abs/2507.15850", "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking", "authors": ["Basma El Amel Boussaha", "Leen AlQadi", "Mugariya Farooq", "Shaikha Alsuwaidi", "Giulia Campesan", "Ahmed Alzubaidi", "Mohammed Alyafeai", "Hakim Hacid"], "categories": ["cs.CL"], "comment": null, "summary": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas."}
{"id": "2507.14179", "pdf": "https://arxiv.org/pdf/2507.14179.pdf", "abs": "https://arxiv.org/abs/2507.14179", "title": "A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering", "authors": ["Nobel Dhar", "Bobin Deng", "Md Romyull Islam", "Xinyue Zhang", "Kazi Fahim Ahmad Nasif", "Kun Suo"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": "To be published in Euro-Par 2025", "summary": "Large Language Models (LLMs) exhibit significant activation sparsity, where\nonly a subset of neurons are active for a given input. Although this sparsity\npresents opportunities to reduce computational cost, efficiently utilizing it\nrequires predicting activation patterns in a scalable manner. However, direct\nprediction at the neuron level is computationally expensive due to the vast\nnumber of neurons in modern LLMs. To enable efficient prediction and\nutilization of activation sparsity, we propose a clustering-based activation\npattern compression framework. Instead of treating each neuron independently,\nwe group similar activation patterns into a small set of representative\nclusters. Our method achieves up to 79.34% clustering precision, outperforming\nstandard binary clustering approaches while maintaining minimal degradation in\nperplexity (PPL) scores. With a sufficiently large number of clusters, our\napproach attains a PPL score as low as 12.49, demonstrating its effectiveness\nin preserving model quality while reducing computational overhead. By\npredicting cluster assignments rather than individual neuron states, future\nmodels can efficiently infer activation patterns from pre-computed centroids.\nWe detail the clustering algorithm, analyze its effectiveness in capturing\nmeaningful activation structures, and demonstrate its potential to improve\nsparse computation efficiency. This clustering-based formulation serves as a\nfoundation for future work on activation pattern prediction, paving the way for\nefficient inference in large-scale language models."}
{"id": "2507.14201", "pdf": "https://arxiv.org/pdf/2507.14201.pdf", "abs": "https://arxiv.org/abs/2507.14201", "title": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation", "authors": ["Yiran Wu", "Mauricio Velazco", "Andrew Zhao", "Manuel Raúl Meléndez Luján", "Srisuma Movva", "Yogesh K Roy", "Quang Nguyen", "Roberto Rodriguez", "Qingyun Wu", "Michael Albada", "Julia Kiseleva", "Anand Mudgerikar"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on\nthe task of Cyber Threat Investigation through security questions derived from\ninvestigation graphs. Real-world security analysts must sift through a large\nnumber of heterogeneous alert signals and security logs, follow multi-hop\nchains of evidence, and compile an incident report. With the developments of\nLLMs, building LLM-based agents for automatic thread investigation is a\npromising direction. To assist the development and evaluation of LLM agents, we\nconstruct a dataset from a controlled Azure tenant that covers 8 simulated\nreal-world multi-step attacks, 57 log tables from Microsoft Sentinel and\nrelated services, and 589 automatically generated questions. We leverage\nsecurity logs extracted with expert-crafted detection logic to build threat\ninvestigation graphs, and then generate questions with LLMs using paired nodes\non the graph, taking the start node as background context and the end node as\nanswer. Anchoring each question to these explicit nodes and edges not only\nprovides automatic, explainable ground truth answers but also makes the\npipeline reusable and readily extensible to new logs. This also enables the\nautomatic generation of procedural tasks with verifiable rewards, which can be\nnaturally extended to training agents via reinforcement learning. Our\ncomprehensive experiments with different models confirm the difficulty of the\ntask: with the base setting, the average reward across all evaluated models is\n0.249, and the best achieved is 0.368, leaving substantial headroom for future\nresearch. Code and data are coming soon!"}
{"id": "2507.14204", "pdf": "https://arxiv.org/pdf/2507.14204.pdf", "abs": "https://arxiv.org/abs/2507.14204", "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models", "authors": ["Dachuan Shi", "Yonggan Fu", "Xiangchi Yuan", "Zhongzhi Yu", "Haoran You", "Sixu Li", "Xin Dong", "Jan Kautz", "Pavlo Molchanov", "Yingyan", "Lin"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache", "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache."}
{"id": "2507.14221", "pdf": "https://arxiv.org/pdf/2507.14221.pdf", "abs": "https://arxiv.org/abs/2507.14221", "title": "Identifying Algorithmic and Domain-Specific Bias in Parliamentary Debate Summarisation", "authors": ["Eoghan Cunningham", "James Cross", "Derek Greene"], "categories": ["cs.CY", "cs.CL", "cs.LG"], "comment": null, "summary": "The automated summarisation of parliamentary debates using large language\nmodels (LLMs) offers a promising way to make complex legislative discourse more\naccessible to the public. However, such summaries must not only be accurate and\nconcise but also equitably represent the views and contributions of all\nspeakers. This paper explores the use of LLMs to summarise plenary debates from\nthe European Parliament and investigates the algorithmic and representational\nbiases that emerge in this context. We propose a structured, multi-stage\nsummarisation framework that improves textual coherence and content fidelity,\nwhile enabling the systematic analysis of how speaker attributes -- such as\nspeaking order or political affiliation -- influence the visibility and\naccuracy of their contributions in the final summaries. Through our experiments\nusing both proprietary and open-weight LLMs, we find evidence of consistent\npositional and partisan biases, with certain speakers systematically\nunder-represented or misattributed. Our analysis shows that these biases vary\nby model and summarisation strategy, with hierarchical approaches offering the\ngreatest potential to reduce disparity. These findings underscore the need for\ndomain-sensitive evaluation metrics and ethical oversight in the deployment of\nLLMs for democratic applications."}
{"id": "2507.14293", "pdf": "https://arxiv.org/pdf/2507.14293.pdf", "abs": "https://arxiv.org/abs/2507.14293", "title": "WebGuard: Building a Generalizable Guardrail for Web Agents", "authors": ["Boyuan Zheng", "Zeyi Liao", "Scott Salisbury", "Zeyuan Liu", "Michael Lin", "Qinyuan Zheng", "Zifan Wang", "Xiang Deng", "Dawn Song", "Huan Sun", "Yu Su"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "We publicly release WebGuard, along with its annotation tools and\n  fine-tuned models, to facilitate open-source research on monitoring and\n  safeguarding web agents. All resources are available at\n  https://github.com/OSU-NLP-Group/WebGuard", "summary": "The rapid development of autonomous web agents powered by Large Language\nModels (LLMs), while greatly elevating efficiency, exposes the frontier risk of\ntaking unintended or harmful actions. This situation underscores an urgent need\nfor effective safety measures, akin to access controls for human users. To\naddress this critical challenge, we introduce WebGuard, the first comprehensive\ndataset designed to support the assessment of web agent action risks and\nfacilitate the development of guardrails for real-world online environments. In\ndoing so, WebGuard specifically focuses on predicting the outcome of\nstate-changing actions and contains 4,939 human-annotated actions from 193\nwebsites across 22 diverse domains, including often-overlooked long-tail\nwebsites. These actions are categorized using a novel three-tier risk schema:\nSAFE, LOW, and HIGH. The dataset includes designated training and test splits\nto support evaluation under diverse generalization settings. Our initial\nevaluations reveal a concerning deficiency: even frontier LLMs achieve less\nthan 60% accuracy in predicting action outcomes and less than 60% recall in\nlagging HIGH-risk actions, highlighting the risks of deploying\ncurrent-generation agents without dedicated safeguards. We therefore\ninvestigate fine-tuning specialized guardrail models using WebGuard. We conduct\ncomprehensive evaluations across multiple generalization settings and find that\na fine-tuned Qwen2.5VL-7B model yields a substantial improvement in\nperformance, boosting accuracy from 37% to 80% and HIGH-risk action recall from\n20% to 76%. Despite these improvements, the performance still falls short of\nthe reliability required for high-stakes deployment, where guardrails must\napproach near-perfect accuracy and recall."}
{"id": "2507.14353", "pdf": "https://arxiv.org/pdf/2507.14353.pdf", "abs": "https://arxiv.org/abs/2507.14353", "title": "Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers", "authors": ["Harsh Nilesh Pathak", "Randy Paffenroth"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Parameter efficient fine tuning (PEFT) is a versatile and extensible approach\nfor adapting a Large Language Model (LLM) for newer tasks. One of the most\nprominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on\nadjusting the attention weight matrices within individual decoder blocks of a\nGenerative Pre trained Transformer (GPT2). In contrast, we introduce Solo\nConnection a novel method that adapts the representation at the decoder-block\nlevel rather than modifying individual weight matrices. Not only does Solo\nConnection outperform LoRA on E2E natural language generation benchmarks, but\nit also reduces the number of trainable parameters by 59% relative to LoRA and\nby more than 99% compared to full fine-tuning of GPT2, an early version of\nLarge Language Models (LLMs). Solo Connection is also motivated by homotopy\ntheory: we introduce a trainable linear transformation that gradually\ninterpolates between a zero vector and the task-specific representation,\nenabling smooth and stable adaptation over time. While skip connections in the\noriginal 12 layer GPT2 are typically confined to individual decoder blocks,\nsubsequent GPT2 variants scale up to 48 layers, and even larger language models\ncan include 128 or more decoder blocks. These expanded architectures underscore\nthe need to revisit how skip connections are employed during fine-tuning. This\npaper focuses on long skip connections that link outputs of different decoder\nblocks, potentially enhancing the model's ability to adapt to new tasks while\nleveraging pre-trained knowledge."}
{"id": "2507.14384", "pdf": "https://arxiv.org/pdf/2507.14384.pdf", "abs": "https://arxiv.org/abs/2507.14384", "title": "Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions", "authors": ["Angjelin Hila", "Elliott Hauser"], "categories": ["cs.HC", "cs.CL"], "comment": "Extended version of paper accepted for presentation at the ASIS&T\n  Annual Meeting 2025. 38 pages, 12 figures", "summary": "In this study, we investigate the use of large language models (LLMs),\nspecifically ChatGPT, for structured deductive qualitative coding. While most\ncurrent research emphasizes inductive coding applications, we address the\nunderexplored potential of LLMs to perform deductive classification tasks\naligned with established human-coded schemes. Using the Comparative Agendas\nProject (CAP) Master Codebook, we classified U.S. Supreme Court case summaries\ninto 21 major policy domains. We tested four intervention methods: zero-shot,\nfew-shot, definition-based, and a novel Step-by-Step Task Decomposition\nstrategy, across repeated samples. Performance was evaluated using standard\nclassification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's\nalpha), and construct validity was assessed using chi-squared tests and\nCramer's V. Chi-squared and effect size analyses confirmed that intervention\nstrategies significantly influenced classification behavior, with Cramer's V\nvalues ranging from 0.359 to 0.613, indicating moderate to strong shifts in\nclassification patterns. The Step-by-Step Task Decomposition strategy achieved\nthe strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),\nachieving thresholds for substantial agreement. Despite the semantic ambiguity\nwithin case summaries, ChatGPT displayed stable agreement across samples,\nincluding high F1 scores in low-support subclasses. These findings demonstrate\nthat with targeted, custom-tailored interventions, LLMs can achieve reliability\nlevels suitable for integration into rigorous qualitative coding workflows."}
{"id": "2507.14417", "pdf": "https://arxiv.org/pdf/2507.14417.pdf", "abs": "https://arxiv.org/abs/2507.14417", "title": "Inverse Scaling in Test-Time Compute", "authors": ["Aryo Pradipta Gema", "Alexander Hägele", "Runjin Chen", "Andy Arditi", "Jacob Goldman-Wetzler", "Kit Fraser-Taliente", "Henry Sleight", "Linda Petrini", "Julian Michael", "Beatrice Alex", "Pasquale Minervini", "Yanda Chen", "Joe Benton", "Ethan Perez"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs."}
{"id": "2507.14419", "pdf": "https://arxiv.org/pdf/2507.14419.pdf", "abs": "https://arxiv.org/abs/2507.14419", "title": "It's Not That Simple. An Analysis of Simple Test-Time Scaling", "authors": ["Guojun Wu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Prior work proposed simple test-time scaling, a method for replicating this\nscaling behavior with models distilled from o1-like models by manually\ncontrolling test-time compute: either scaling down by enforcing a maximum\nlength or scaling up by iteratively appending \"Wait\" when the model is about to\nterminate its generation. This paper presents an analysis of simple test-time\nscaling and finds that the scaling behavior is largely attributed to scaling\ndown by enforcing a maximum length. In contrast, fine-tuning on long CoT data\ndistilled from o1-like models has no significant impact on scaling behavior,\nand scaling up by appending \"Wait\" leads to inconsistencies, as the model may\noscillate between solutions. A key distinction exists between scaling down by\nenforcing a maximum length and scaling up test-time compute in o1-like models,\nsuch as DeepSeek-R1\\@. These models are typically allowed to utilize as much\ncompute as needed, with the only constraint being the model's maximum supported\nlength. By learning to naturally scale up test-time compute during\nreinforcement learning, o1-like models surpass their peak performance when\nscaling up. In contrast, simple test-time scaling progressively imposes a lower\nupper limit on model performance as it scales down. While replicating the\ntest-time scaling behavior of o1 models can be straightforward by scaling down,\nit is crucial to recognize that the goal of scaling test-time compute is to\nunlock higher performance -- beyond what the model could originally achieve --\nrather than merely reproducing the appearance of scaling behavior."}
{"id": "2507.14447", "pdf": "https://arxiv.org/pdf/2507.14447.pdf", "abs": "https://arxiv.org/abs/2507.14447", "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "authors": ["Guancheng Zeng", "Xueyi Chen", "Jiawang Hu", "Shaohua Qi", "Yaxuan Mao", "Zhantao Wang", "Yifan Nie", "Shuang Li", "Qiuyang Feng", "Pengxu Qiu", "Yujia Wang", "Wenqiang Han", "Linyan Huang", "Gang Li", "Jingjing Mo", "Haowen Hu"], "categories": ["cs.AI", "cs.CL"], "comment": "26 pages, 8 figures, 5 tables", "summary": "The deployment of agent systems in an enterprise environment is often\nhindered by several challenges: common models lack domain-specific process\nknowledge, leading to disorganized plans, missing key tools, and poor execution\nstability. To address this, this paper introduces Routine, a multi-step agent\nplanning framework designed with a clear structure, explicit instructions, and\nseamless parameter passing to guide the agent's execution module in performing\nmulti-step tool-calling tasks with high stability. In evaluations conducted\nwithin a real-world enterprise scenario, Routine significantly increases the\nexecution accuracy in model tool calls, increasing the performance of GPT-4o\nfrom 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed\na Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an\naccuracy increase to 88.2% on scenario-specific evaluations, indicating\nimproved adherence to execution plans. In addition, we employed Routine-based\ndistillation to create a scenario-specific, multi-step tool-calling dataset.\nFine-tuning on this distilled dataset raised the model's accuracy to 95.5%,\napproaching GPT-4o's performance. These results highlight Routine's\neffectiveness in distilling domain-specific tool-usage patterns and enhancing\nmodel adaptability to new scenarios. Our experimental results demonstrate that\nRoutine provides a practical and accessible approach to building stable agent\nworkflows, accelerating the deployment and adoption of agent systems in\nenterprise environments, and advancing the technical vision of AI for Process."}
{"id": "2507.14497", "pdf": "https://arxiv.org/pdf/2507.14497.pdf", "abs": "https://arxiv.org/abs/2507.14497", "title": "Efficient Whole Slide Pathology VQA via Token Compression", "authors": ["Weimin Lyu", "Qingqiao Hu", "Kehan Qi", "Zhan Shi", "Wentao Huang", "Saumya Gupta", "Chao Chen"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin."}
{"id": "2507.14534", "pdf": "https://arxiv.org/pdf/2507.14534.pdf", "abs": "https://arxiv.org/abs/2507.14534", "title": "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion", "authors": ["Yu Zhang", "Baotong Tian", "Zhiyao Duan"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Zero-shot online voice conversion (VC) holds significant promise for\nreal-time communications and entertainment. However, current VC models struggle\nto preserve semantic fidelity under real-time constraints, deliver\nnatural-sounding conversions, and adapt effectively to unseen speaker\ncharacteristics. To address these challenges, we introduce Conan, a chunkwise\nonline zero-shot voice conversion model that preserves the content of the\nsource while matching the voice timbre and styles of reference speech. Conan\ncomprises three core components: 1) a Stream Content Extractor that leverages\nEmformer for low-latency streaming content encoding; 2) an Adaptive Style\nEncoder that extracts fine-grained stylistic features from reference speech for\nenhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully\ncausal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations\ndemonstrate that Conan outperforms baseline models in subjective and objective\nmetrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo."}
{"id": "2507.14586", "pdf": "https://arxiv.org/pdf/2507.14586.pdf", "abs": "https://arxiv.org/abs/2507.14586", "title": "What do Large Language Models know about materials?", "authors": ["Adrian Ehrenhofer", "Thomas Wallmersperger", "Gianaurelio Cuniberti"], "categories": ["physics.app-ph", "cs.CE", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied in the fields of\nmechanical engineering and materials science. As models that establish\nconnections through the interface of language, LLMs can be applied for\nstep-wise reasoning through the Processing-Structure-Property-Performance chain\nof material science and engineering. Current LLMs are built for adequately\nrepresenting a dataset, which is the most part of the accessible internet.\nHowever, the internet mostly contains non-scientific content. If LLMs should be\napplied for engineering purposes, it is valuable to investigate models for\ntheir intrinsic knowledge -- here: the capacity to generate correct information\nabout materials. In the current work, for the example of the Periodic Table of\nElements, we highlight the role of vocabulary and tokenization for the\nuniqueness of material fingerprints, and the LLMs' capabilities of generating\nfactually correct output of different state-of-the-art open models. This leads\nto a material knowledge benchmark for an informed choice, for which steps in\nthe PSPP chain LLMs are applicable, and where specialized models are required."}
{"id": "2507.14619", "pdf": "https://arxiv.org/pdf/2507.14619.pdf", "abs": "https://arxiv.org/abs/2507.14619", "title": "Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining", "authors": ["Van-Hoang Le", "Duc-Vu Nguyen", "Kiet Van Nguyen", "Ngan Luu-Thuy Nguyen"], "categories": ["cs.IR", "cs.CL"], "comment": "Accepted at ICCCI 2025", "summary": "Large Language Models (LLMs) face significant challenges in specialized\ndomains like law, where precision and domain-specific knowledge are critical.\nThis paper presents a streamlined two-stage framework consisting of Retrieval\nand Re-ranking to enhance legal document retrieval efficiency and accuracy. Our\napproach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,\nfollowed by a Cross-Encoder for precise re-ranking, both optimized through\nstrategic negative example mining. Key innovations include the introduction of\nthe Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard\nnegatives to mitigate training bias, which significantly improved re-ranking\nperformance. Evaluated on the SoICT Hackathon 2024 for Legal Document\nRetrieval, our team, 4Huiter, achieved a top-three position. While\ntop-performing teams employed ensemble models and iterative self-training on\nlarge bge-m3 architectures, our lightweight, single-pass approach offered a\ncompetitive alternative with far fewer parameters. The framework demonstrates\nthat optimized data processing, tailored loss functions, and balanced negative\nsampling are pivotal for building robust retrieval-augmented systems in legal\ncontexts."}
{"id": "2507.14660", "pdf": "https://arxiv.org/pdf/2507.14660.pdf", "abs": "https://arxiv.org/abs/2507.14660", "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "authors": ["Qibing Ren", "Sitao Xie", "Longxuan Wei", "Zhenfei Yin", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "categories": ["cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/renqibing/RogueAgent", "summary": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent."}
{"id": "2507.14675", "pdf": "https://arxiv.org/pdf/2507.14675.pdf", "abs": "https://arxiv.org/abs/2507.14675", "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding", "authors": ["Yuchen Duan", "Zhe Chen", "Yusong Hu", "Weiyun Wang", "Shenglong Ye", "Botian Shi", "Lewei Lu", "Qibin Hou", "Tong Lu", "Hongsheng Li", "Jifeng Dai", "Wenhai Wang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Despite significant progress in multimodal large language models (MLLMs),\ntheir performance on complex, multi-page document comprehension remains\ninadequate, largely due to the lack of high-quality, document-level datasets.\nWhile current retrieval-augmented generation (RAG) methods offer partial\nsolutions, they suffer from issues, such as fragmented retrieval contexts,\nmulti-stage error accumulation, and extra time costs of retrieval. In this\nwork, we present a high-quality document-level dataset, Doc-750K, designed to\nsupport in-depth understanding of multimodal documents. This dataset includes\ndiverse document structures, extensive cross-page dependencies, and real\nquestion-answer pairs derived from the original documents. Building on the\ndataset, we develop a native multimodal model, Docopilot, which can accurately\nhandle document-level dependencies without relying on RAG. Experiments\ndemonstrate that Docopilot achieves superior coherence, accuracy, and\nefficiency in document understanding tasks and multi-turn interactions, setting\na new baseline for document-level multimodal understanding. Data, code, and\nmodels are released at https://github.com/OpenGVLab/Docopilot"}
{"id": "2507.14679", "pdf": "https://arxiv.org/pdf/2507.14679.pdf", "abs": "https://arxiv.org/abs/2507.14679", "title": "GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks", "authors": ["Zixin Xu", "Zhijie Wang", "Zhiyuan Pan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The exponential growth of spam text on the Internet necessitates robust\ndetection mechanisms to mitigate risks such as information leakage and social\ninstability. This work addresses two principal challenges: adversarial\nstrategies employed by spammers and the scarcity of labeled data. We propose a\nnovel spam-text detection framework GCC-Spam, which integrates three core\ninnovations. First, a character similarity network captures orthographic and\nphonetic features to counter character-obfuscation attacks and furthermore\nproduces sentence embeddings for downstream classification. Second, contrastive\nlearning enhances discriminability by optimizing the latent-space distance\nbetween spam and normal texts. Third, a Generative Adversarial Network (GAN)\ngenerates realistic pseudo-spam samples to alleviate data scarcity while\nimproving model robustness and classification accuracy. Extensive experiments\non real-world datasets demonstrate that our model outperforms baseline\napproaches, achieving higher detection rates with significantly fewer labeled\nexamples."}
{"id": "2507.14843", "pdf": "https://arxiv.org/pdf/2507.14843.pdf", "abs": "https://arxiv.org/abs/2507.14843", "title": "The Invisible Leash: Why RLVR May Not Escape Its Origin", "authors": ["Fang Wu", "Weihao Xuan", "Ximing Lu", "Zaid Harchaoui", "Yejin Choi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in large reasoning models highlight Reinforcement Learning\nwith Verifiable Rewards (RLVR) as a promising method for enhancing AI's\ncapabilities, particularly in solving complex logical tasks. However, it\nremains unclear whether RLVR truly expands a model's reasoning boundary or\nmerely amplifies high-reward outputs that the base model already knows for\nimproved precision. This study presents a theoretical and empirical\ninvestigation that provides fresh insights into the potential limits of RLVR.\nFirst, we offer a new theoretical perspective that RLVR is constrained by the\nbase model's support-unable to sample solutions with zero initial\nprobability-and operates as a conservative reweighting mechanism that may\nrestrict the discovery of entirely original solutions. We also identify an\nentropy-reward tradeoff: while RLVR reliably enhances precision, it may\nprogressively narrow exploration and potentially overlook correct yet\nunderrepresented solutions. Extensive empirical experiments validate that while\nRLVR consistently improves pass@1, the shrinkage of empirical support generally\noutweighs the expansion of empirical support under larger sampling budgets,\nfailing to recover correct answers that were previously accessible to the base\nmodel. Interestingly, we also observe that while RLVR sometimes increases\ntoken-level entropy, resulting in greater uncertainty at each generation step,\nanswer-level entropy declines, indicating that these seemingly more uncertain\npaths ultimately converge onto a smaller set of distinct answers. Taken\ntogether, these findings reveal potential limits of RLVR in extending reasoning\nhorizons. Breaking this invisible leash may require future algorithmic\ninnovations such as explicit exploration mechanisms or hybrid strategies that\nseed probability mass into underrepresented solution regions."}
{"id": "2507.15007", "pdf": "https://arxiv.org/pdf/2507.15007.pdf", "abs": "https://arxiv.org/abs/2507.15007", "title": "Hear Your Code Fail, Voice-Assisted Debugging for Python", "authors": ["Sayed Mahbub Hasan Amiri", "Md. Mainul Islam", "Mohammad Shakhawat Hossen", "Sayed Majhab Hasan Amiri", "Mohammad Shawkat Ali Mamun", "Sk. Humaun Kabir", "Naznin Akter"], "categories": ["cs.PL", "cs.CL"], "comment": "35 pages, 20 figures", "summary": "This research introduces an innovative voice-assisted debugging plugin for\nPython that transforms silent runtime errors into actionable audible\ndiagnostics. By implementing a global exception hook architecture with pyttsx3\ntext-to-speech conversion and Tkinter-based GUI visualization, the solution\ndelivers multimodal error feedback through parallel auditory and visual\nchannels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,\nn=50) compared to traditional stack-trace debugging, while enabling 78% faster\nerror identification through vocalized exception classification and\ncontextualization. The system achieves sub-1.2 second voice latency with under\n18% CPU overhead during exception handling, vocalizing error types and\nconsequences while displaying interactive tracebacks with documentation deep\nlinks. Criteria validate compatibility across Python 3.7+ environments on\nWindows, macOS, and Linux platforms. Needing only two lines of integration\ncode, the plugin significantly boosts availability for aesthetically impaired\ndesigners and supports multitasking workflows through hands-free error medical\ndiagnosis. Educational applications show particular promise, with pilot studies\nindicating 45% faster debugging skill acquisition among novice programmers.\nFuture development will incorporate GPT-based repair suggestions and real-time\nmultilingual translation to further advance auditory debugging paradigms. The\nsolution represents a fundamental shift toward human-centric error diagnostics,\nbridging critical gaps in programming accessibility while establishing new\nstandards for cognitive efficiency in software development workflows."}
{"id": "2507.15205", "pdf": "https://arxiv.org/pdf/2507.15205.pdf", "abs": "https://arxiv.org/abs/2507.15205", "title": "Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation", "authors": ["Xinran Li", "Xiujuan Xu", "Jiaqi Qiao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)", "summary": "Emotion Recognition in Conversation (ERC) is a practical and challenging\ntask. This paper proposes a novel multimodal approach, the Long-Short Distance\nGraph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it\nconstructs a long-distance graph neural network and a short-distance graph\nneural network to obtain multimodal features of distant and nearby utterances,\nrespectively. To ensure that long- and short-distance features are as distinct\nas possible in representation while enabling mutual influence between the two\nmodules, we employ a Differential Regularizer and incorporate a BiAffine Module\nto facilitate feature interaction. In addition, we propose an Improved\nCurriculum Learning (ICL) to address the challenge of data imbalance. By\ncomputing the similarity between different emotions to emphasize the shifts in\nsimilar emotions, we design a \"weighted emotional shift\" metric and develop a\ndifficulty measurer, enabling a training process that prioritizes learning easy\nsamples before harder ones. Experimental results on the IEMOCAP and MELD\ndatasets demonstrate that our model outperforms existing benchmarks."}
{"id": "2507.15214", "pdf": "https://arxiv.org/pdf/2507.15214.pdf", "abs": "https://arxiv.org/abs/2507.15214", "title": "Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems", "authors": ["Natalia Tomashenko", "Emmanuel Vincent", "Marc Tommasi"], "categories": ["cs.SD", "cs.CL", "cs.CR", "eess.AS"], "comment": "Accepted at Interspeech-2025", "summary": "The temporal dynamics of speech, encompassing variations in rhythm,\nintonation, and speaking rate, contain important and unique information about\nspeaker identity. This paper proposes a new method for representing speaker\ncharacteristics by extracting context-dependent duration embeddings from speech\ntemporal dynamics. We develop novel attack models using these representations\nand analyze the potential vulnerabilities in speaker verification and voice\nanonymization systems.The experimental results show that the developed attack\nmodels provide a significant improvement in speaker verification performance\nfor both original and anonymized data in comparison with simpler\nrepresentations of speech temporal dynamics reported in the literature."}
{"id": "2507.15267", "pdf": "https://arxiv.org/pdf/2507.15267.pdf", "abs": "https://arxiv.org/abs/2507.15267", "title": "GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou", "authors": ["Ninglu Shao", "Jinshan Wang", "Chenxu Wang", "Qingbiao Li", "Xiaoxue Zang", "Han Li"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Currently, short video platforms have become the primary place for\nindividuals to share experiences and obtain information. To better meet users'\nneeds for acquiring information while browsing short videos, some apps have\nintroduced a search entry at the bottom of videos, accompanied with recommended\nrelevant queries. This scenario is known as query recommendation in\nvideo-related search, where core task is item-to-query (I2Q) recommendation. As\nthis scenario has only emerged in recent years, there is a notable scarcity of\nacademic research and publicly available datasets in this domain. To address\nthis gap, we systematically examine the challenges associated with this\nscenario for the first time. Subsequently, we release a large-scale dataset\nderived from real-world data pertaining to the query recommendation in\nvideo-\\textit{\\textbf{r}}elated \\textit{\\textbf{s}}earch on the\n\\textit{\\textbf{Kuai}}shou app (\\textbf{KuaiRS}). Presently, existing methods\nrely on embeddings to calculate similarity for matching short videos with\nqueries, lacking deep interaction between the semantic content and the query.\nIn this paper, we introduce a novel LLM-based framework named \\textbf{GREAT},\nwhich \\textit{\\textbf{g}}uides que\\textit{\\textbf{r}}y\ng\\textit{\\textbf{e}}ner\\textit{\\textbf{a}}tion with a \\textit{\\textbf{t}}rie to\naddress I2Q recommendation in related search. Specifically, we initially gather\nhigh-quality queries with high exposure and click-through rate to construct a\nquery-based trie. During training, we enhance the LLM's capability to generate\nhigh-quality queries using the query-based trie. In the inference phase, the\nquery-based trie serves as a guide for the token generation. Finally, we\nfurther refine the relevance and literal quality between items and queries via\na post-processing module. Extensive offline and online experiments demonstrate\nthe effectiveness of our proposed method."}
{"id": "2507.15272", "pdf": "https://arxiv.org/pdf/2507.15272.pdf", "abs": "https://arxiv.org/abs/2507.15272", "title": "A2TTS: TTS for Low Resource Indian Languages", "authors": ["Ayush Singh Bhadoriya", "Abhishek Nikunj Shinde", "Isha Pandey", "Ganesh Ramakrishnan"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "We present a speaker conditioned text-to-speech (TTS) system aimed at\naddressing challenges in generating speech for unseen speakers and supporting\ndiverse Indian languages. Our method leverages a diffusion-based TTS\narchitecture, where a speaker encoder extracts embeddings from short reference\naudio samples to condition the DDPM decoder for multispeaker generation. To\nfurther enhance prosody and naturalness, we employ a cross-attention based\nduration prediction mechanism that utilizes reference audio, enabling more\naccurate and speaker consistent timing. This results in speech that closely\nresembles the target speaker while improving duration modeling and overall\nexpressiveness. Additionally, to improve zero-shot generation, we employed\nclassifier free guidance, allowing the system to generate speech more near\nspeech for unknown speakers. Using this approach, we trained language-specific\nspeaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian\nlanguages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and\nTamil."}
{"id": "2507.15507", "pdf": "https://arxiv.org/pdf/2507.15507.pdf", "abs": "https://arxiv.org/abs/2507.15507", "title": "Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback", "authors": ["Johannes Ackermann", "Takashi Ishida", "Masashi Sugiyama"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accept at the Conference On Language Modeling (COLM) 2025", "summary": "Reinforcement Learning from Human Feedback (RLHF) allows us to train models,\nsuch as language models (LMs), to follow complex human preferences. In RLHF for\nLMs, we first train an LM using supervised fine-tuning, sample pairs of\nresponses, obtain human feedback, and use the resulting data to train a reward\nmodel (RM). RL methods are then used to train the LM to maximize the reward\ngiven by the RM. As training progresses, the responses generated by the LM no\nlonger resemble the responses seen by the RM during training, leading to the RM\nbecoming inaccurate. The score given by the RM keeps increasing, but the\nlearned behavior no longer matches the human preferences. This issue is known\nas overoptimization. We investigate overoptimization from the point of view of\ndistribution shift and show that the shift results in an inconsistent estimate\nof the RM parameters, leading to an inconsistent estimate of the policy\ngradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which\niteratively off-policy corrects the RM using importance weighting, without\nrequiring new labels or samples. This results in a more accurate RM, which\nempirically leads to an improved final policy. We validate our approach in\nexperiments with summarization and chatbot datasets and show that it performs\nsignificantly better than standard RLHF methods and baselines. Our\nimplementation is available at\nhttps://github.com/JohannesAck/OffPolicyCorrectedRewardModeling"}
{"id": "2507.15640", "pdf": "https://arxiv.org/pdf/2507.15640.pdf", "abs": "https://arxiv.org/abs/2507.15640", "title": "Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training", "authors": ["Kailai Yang", "Xiao Liu", "Lei Ji", "Hao Li", "Yeyun Gong", "Peng Cheng", "Mao Yang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Continual pre-training on small-scale task-specific data is an effective\nmethod for improving large language models in new target fields, yet it risks\ncatastrophic forgetting of their original capabilities. A common solution is to\nre-weight training data mixtures from source and target fields on a domain\nspace to achieve balanced performance. Previous domain reweighting strategies\nrely on manual designation with certain heuristics based on human intuition or\nempirical results. In this work, we prove that more general heuristics can be\nparameterized by proposing Data Mixing Agent, the first model-based, end-to-end\nframework that learns to re-weight domains. The agent learns generalizable\nheuristics through reinforcement learning on large quantities of data mixing\ntrajectories with corresponding feedback from an evaluation environment.\nExperiments in continual pre-training on math reasoning show that Data Mixing\nAgent outperforms strong baselines in achieving balanced performance across\nsource and target field benchmarks. Furthermore, it generalizes well across\nunseen source fields, target models, and domain spaces without retraining.\nDirect application to the code generation field also indicates its adaptability\nacross target domains. Further analysis showcases the agents' well-aligned\nheuristics with human intuitions and their efficiency in achieving superior\nmodel performance with less source-field data."}
{"id": "2507.15743", "pdf": "https://arxiv.org/pdf/2507.15743.pdf", "abs": "https://arxiv.org/abs/2507.15743", "title": "Towards physician-centered oversight of conversational diagnostic AI", "authors": ["Elahe Vedadi", "David Barrett", "Natalie Harris", "Ellery Wulczyn", "Shashir Reddy", "Roma Ruparel", "Mike Schaekermann", "Tim Strother", "Ryutaro Tanno", "Yash Sharma", "Jihyeon Lee", "Cían Hughes", "Dylan Slack", "Anil Palepu", "Jan Freyberg", "Khaled Saab", "Valentin Liévin", "Wei-Hung Weng", "Tao Tu", "Yun Liu", "Nenad Tomasev", "Kavita Kulkarni", "S. Sara Mahdavi", "Kelvin Guu", "Joëlle Barral", "Dale R. Webster", "James Manyika", "Avinatan Hassidim", "Katherine Chou", "Yossi Matias", "Pushmeet Kohli", "Adam Rodman", "Vivek Natarajan", "Alan Karthikesalingam", "David Stutz"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Recent work has demonstrated the promise of conversational AI systems for\ndiagnostic dialogue. However, real-world assurance of patient safety means that\nproviding individual diagnoses and treatment plans is considered a regulated\nactivity by licensed professionals. Furthermore, physicians commonly oversee\nother team members in such activities, including nurse practitioners (NPs) or\nphysician assistants/associates (PAs). Inspired by this, we propose a framework\nfor effective, asynchronous oversight of the Articulate Medical Intelligence\nExplorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent\nsystem that performs history taking within guardrails, abstaining from\nindividualized medical advice. Afterwards, g-AMIE conveys assessments to an\noverseeing primary care physician (PCP) in a clinician cockpit interface. The\nPCP provides oversight and retains accountability of the clinical decision.\nThis effectively decouples oversight from intake and can thus happen\nasynchronously. In a randomized, blinded virtual Objective Structured Clinical\nExamination (OSCE) of text consultations with asynchronous oversight, we\ncompared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across\n60 scenarios, g-AMIE outperformed both groups in performing high-quality\nintake, summarizing cases, and proposing diagnoses and management plans for the\noverseeing PCP to review. This resulted in higher quality composite decisions.\nPCP oversight of g-AMIE was also more time-efficient than standalone PCP\nconsultations in prior work. While our study does not replicate existing\nclinical practices and likely underestimates clinicians' capabilities, our\nresults demonstrate the promise of asynchronous oversight as a feasible\nparadigm for diagnostic AI systems to operate under expert human oversight for\nenhancing real-world care."}
{"id": "2507.15758", "pdf": "https://arxiv.org/pdf/2507.15758.pdf", "abs": "https://arxiv.org/abs/2507.15758", "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization", "authors": ["Xingyu Wu", "Yuchen Yan", "Shangke Lyu", "Linjuan Wu", "Yiwen Qiu", "Yongliang Shen", "Weiming Lu", "Jian Shao", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.AI", "cs.CL"], "comment": "GitHub:https://github.com/zju-real/lapo;\n  Project:https://zju-real.github.io/lapo", "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality."}
{"id": "2507.15776", "pdf": "https://arxiv.org/pdf/2507.15776.pdf", "abs": "https://arxiv.org/abs/2507.15776", "title": "Dissociating model architectures from inference computations", "authors": ["Noor Sajid", "Johan Medrano"], "categories": ["q-bio.NC", "cs.CL", "cs.LG"], "comment": "3 pages, 1 figure", "summary": "Parr et al., 2025 examines how auto-regressive and deep temporal models\ndiffer in their treatment of non-Markovian sequence modelling. Building on\nthis, we highlight the need for dissociating model architectures, i.e., how the\npredictive distribution factorises, from the computations invoked at inference.\nWe demonstrate that deep temporal computations are mimicked by autoregressive\nmodels by structuring context access during iterative inference. Using a\ntransformer trained on next-token prediction, we show that inducing\nhierarchical temporal factorisation during iterative inference maintains\npredictive capacity while instantiating fewer computations. This emphasises\nthat processes for constructing and refining predictions are not necessarily\nbound to their underlying model architectures."}
{"id": "2507.15788", "pdf": "https://arxiv.org/pdf/2507.15788.pdf", "abs": "https://arxiv.org/abs/2507.15788", "title": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning", "authors": ["Sneheel Sarangi", "Hanan Salam"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability."}
{"id": "2507.15844", "pdf": "https://arxiv.org/pdf/2507.15844.pdf", "abs": "https://arxiv.org/abs/2507.15844", "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "authors": ["Shangke Lyu", "Linjuan Wu", "Yuchen Yan", "Xingyu Wu", "Hao Li", "Yongliang Shen", "Peisheng Jiang", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.AI", "cs.CL"], "comment": "Code: https://github.com/zju-real/hbpo Project\n  Page:https://zju-real.github.io/hbpo/", "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity."}
{"id": "2507.15846", "pdf": "https://arxiv.org/pdf/2507.15846.pdf", "abs": "https://arxiv.org/abs/2507.15846", "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "authors": ["Fei Tang", "Zhangxuan Gu", "Zhengxi Lu", "Xuyang Liu", "Shuheng Shen", "Changhua Meng", "Wen Wang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks."}
{"id": "2303.09823", "pdf": "https://arxiv.org/pdf/2303.09823.pdf", "abs": "https://arxiv.org/abs/2303.09823", "title": "Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages", "authors": ["Angel Felipe Magnossão de Paula", "Imene Bensalem", "Paolo Rosso", "Wajdi Zaghouani"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 3 tables", "summary": "This paper describes our participation in the shared task of hate speech\ndetection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our\nexperiments evaluate the performance of six transformer models and their\ncombination using 2 ensemble approaches. The best results on the training set,\nin a five-fold cross validation scenario, were obtained by using the ensemble\napproach based on the majority vote. The evaluation of this approach on the\ntest set resulted in an F1-score of 0.60 and an Accuracy of 0.86."}
{"id": "2311.09675", "pdf": "https://arxiv.org/pdf/2311.09675.pdf", "abs": "https://arxiv.org/abs/2311.09675", "title": "Where Do People Tell Stories Online? Story Detection Across Online Communities", "authors": ["Maria Antoniak", "Joel Mire", "Maarten Sap", "Elliott Ash", "Andrew Piper"], "categories": ["cs.CL"], "comment": null, "summary": "Story detection in online communities is a challenging task as stories are\nscattered across communities and interwoven with non-storytelling spans within\na single text. We address this challenge by building and releasing the\nStorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts\nand comments, a detailed codebook adapted to the social media context, and\nmodels to predict storytelling at the document and span levels. Our dataset is\nsampled from hundreds of popular English-language Reddit communities ranging\nacross 33 topic categories, and it contains fine-grained expert annotations,\nincluding binary story labels, story spans, and event spans. We evaluate a\nrange of detection methods using our data, and we identify the distinctive\ntextual features of online storytelling, focusing on storytelling spans. We\nilluminate distributional characteristics of storytelling on a large\ncommunity-centric social media platform, and we also conduct a case study on\nr/ChangeMyView, where storytelling is used as one of many persuasive\nstrategies, illustrating that our data and models can be used for both inter-\nand intra-community research. Finally, we discuss implications of our tools and\nanalyses for narratology and the study of online communities."}
{"id": "2311.16789", "pdf": "https://arxiv.org/pdf/2311.16789.pdf", "abs": "https://arxiv.org/abs/2311.16789", "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems: Data, Task and Models", "authors": ["Hongru Wang", "Lingzhi Wang", "Yiming Du", "Liang Chen", "Jingyan Zhou", "Yufei Wang", "Kam-Fai Wong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Dialogue systems (DS), including the task-oriented dialogue system (TOD) and\nthe open-domain dialogue system (ODD), have always been a fundamental task in\nnatural language processing (NLP), allowing various applications in practice.\nOwing to sophisticated training and well-designed model architecture, language\nmodels (LM) are usually adopted as the necessary backbone to build the dialogue\nsystem. Consequently, every breakthrough in LM brings about a shift in learning\nparadigm and research attention within dialogue system, especially the\nappearance of pre-trained language models (PLMs) and large language models\n(LLMs). In this paper, we take a deep look at the history of the dialogue\nsystem, especially its special relationship with the advancements of language\nmodels. Thus, our survey offers a systematic perspective, categorizing\ndifferent stages in a chronological order aligned with LM breakthroughs,\nproviding a comprehensive review of state-of-the-art research outcomes. What's\nmore, we turn our attention to emerging topics and engage in a discussion on\nopen challenges, providing valuable insights into the future directions for\nLLM-based dialogue systems. In summary, this survey delves into the dynamic\ninterplay between language models and dialogue systems, unraveling the\nevolutionary path of this essential relationship. Through this exploration, we\npave the way for a deeper comprehension of the field, guiding future\ndevelopments in LM-based dialogue systems."}
{"id": "2311.17741", "pdf": "https://arxiv.org/pdf/2311.17741.pdf", "abs": "https://arxiv.org/abs/2311.17741", "title": "End-to-end Joint Punctuated and Normalized ASR with a Limited Amount of Punctuated Training Data", "authors": ["Can Cui", "Imran Ahamad Sheikh", "Mostafa Sadeghi", "Emmanuel Vincent"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Joint punctuated and normalized automatic speech recognition (ASR) aims at\noutputing transcripts with and without punctuation and casing. This task\nremains challenging due to the lack of paired speech and punctuated text data\nin most ASR corpora. We propose two approaches to train an end-to-end joint\npunctuated and normalized ASR system using limited punctuated data. The first\napproach uses a language model to convert normalized training transcripts into\npunctuated transcripts. This achieves a better performance on out-of-domain\ntest data, with up to 17% relative Punctuation-Case-aware Word Error Rate\n(PC-WER) reduction. The second approach uses a single decoder conditioned on\nthe type of output. This yields a 42% relative PC-WER reduction compared to\nWhisper-base and a 4% relative (normalized) WER reduction compared to the\nnormalized output of a punctuated-only model. Additionally, our proposed model\ndemonstrates the feasibility of a joint ASR system using as little as 5%\npunctuated training data with a moderate (2.42% absolute) PC-WER increase."}
{"id": "2402.02655", "pdf": "https://arxiv.org/pdf/2402.02655.pdf", "abs": "https://arxiv.org/abs/2402.02655", "title": "VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based Machine Reading Comprehension", "authors": ["Thinh Phuoc Ngo", "Khoa Tran Anh Dang", "Son T. Luu", "Kiet Van Nguyen", "Ngan Luu-Thuy Nguyen"], "categories": ["cs.CL"], "comment": "To appear as the main conference paper at EACL 2024", "summary": "This paper presents the development process of a Vietnamese spoken language\ncorpus for machine reading comprehension (MRC) tasks and provides insights into\nthe challenges and opportunities associated with using real-world data for\nmachine reading comprehension tasks. The existing MRC corpora in Vietnamese\nmainly focus on formal written documents such as Wikipedia articles, online\nnewspapers, or textbooks. In contrast, the VlogQA consists of 10,076\nquestion-answer pairs based on 1,230 transcript documents sourced from YouTube\n-- an extensive source of user-uploaded content, covering the topics of food\nand travel. By capturing the spoken language of native Vietnamese speakers in\nnatural settings, an obscure corner overlooked in Vietnamese research, the\ncorpus provides a valuable resource for future research in reading\ncomprehension tasks for the Vietnamese language. Regarding performance\nevaluation, our deep-learning models achieved the highest F1 score of 75.34% on\nthe test set, indicating significant progress in machine reading comprehension\nfor Vietnamese spoken language data. In terms of EM, the highest score we\naccomplished is 53.97%, which reflects the challenge in processing spoken-based\ncontent and highlights the need for further improvement."}
{"id": "2404.07053", "pdf": "https://arxiv.org/pdf/2404.07053.pdf", "abs": "https://arxiv.org/abs/2404.07053", "title": "Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation", "authors": ["Elisa Sanchez-Bayona", "Rodrigo Agerri"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Metaphors are a ubiquitous but often overlooked part of everyday language. As\na complex cognitive-linguistic phenomenon, they provide a valuable means to\nevaluate whether language models can capture deeper aspects of meaning,\nincluding semantic, pragmatic, and cultural context. In this work, we present\nMeta4XNLI, the first parallel dataset for Natural Language Inference (NLI)\nnewly annotated for metaphor detection and interpretation in both English and\nSpanish. Meta4XNLI facilitates the comparison of encoder- and decoder-based\nmodels in detecting and understanding metaphorical language in multilingual and\ncross-lingual settings. Our results show that fine-tuned encoders outperform\ndecoders-only LLMs in metaphor detection. Metaphor interpretation is evaluated\nvia the NLI framework with comparable performance of masked and autoregressive\nmodels, which notably decreases when the inference is affected by metaphorical\nlanguage. Our study also finds that translation plays an important role in the\npreservation or loss of metaphors across languages, introducing shifts that\nmight impact metaphor occurrence and model performance. These findings\nunderscore the importance of resources like Meta4XNLI for advancing the\nanalysis of the capabilities of language models and improving our understanding\nof metaphor processing across languages. Furthermore, the dataset offers\npreviously unavailable opportunities to investigate metaphor interpretation,\ncross-lingual metaphor transferability, and the impact of translation on the\ndevelopment of multilingual annotated resources."}
{"id": "2406.12644", "pdf": "https://arxiv.org/pdf/2406.12644.pdf", "abs": "https://arxiv.org/abs/2406.12644", "title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles", "authors": ["Devichand Budagam", "Ashutosh Kumar", "Mahsa Khoshnoodi", "Sankalp KJ", "Vinija Jain", "Aman Chadha"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "18 pages, 9 figures, KDD workshop on Prompt Optimization 2025", "summary": "Assessing the effectiveness of large language models (LLMs) in performing\ndifferent tasks is crucial for understanding their strengths and weaknesses.\nThis paper presents Hierarchical Prompting Taxonomy (HPT), grounded on human\ncognitive principles and designed to assess LLMs by examining the cognitive\ndemands of various tasks. The HPT utilizes the Hierarchical Prompting Framework\n(HPF), which structures five unique prompting strategies in a hierarchical\norder based on their cognitive requirement on LLMs when compared to human\nmental capabilities. It assesses the complexity of tasks with the Hierarchical\nPrompting Index (HPI), which demonstrates the cognitive competencies of LLMs\nacross diverse datasets and offers insights into the cognitive demands that\ndatasets place on different LLMs. This approach enables a comprehensive\nevaluation of an LLMs problem solving abilities and the intricacy of a dataset,\noffering a standardized metric for task complexity. Extensive experiments with\nmultiple datasets and LLMs show that HPF enhances LLM performance by 2% to 63%\ncompared to baseline performance, with GSM8k being the most cognitively complex\ntask among reasoning and coding tasks with an average HPI of 3.20 confirming\nthe effectiveness of HPT. To support future research and reproducibility in\nthis domain, the implementations of HPT and HPF are available here."}
{"id": "2407.02543", "pdf": "https://arxiv.org/pdf/2407.02543.pdf", "abs": "https://arxiv.org/abs/2407.02543", "title": "Towards the Next Frontier in Speech Representation Learning Using Disentanglement", "authors": ["Varun Krishna", "Sriram Ganapathy"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "There were some bugs in the Code that was used to produce the results\n  in the paper. The results reported in the paper are not valid", "summary": "The popular frameworks for self-supervised learning of speech representations\nhave largely focused on frame-level masked prediction of speech regions. While\nthis has shown promising downstream task performance for speech recognition and\nrelated tasks, this has largely ignored factors of speech that are encoded at\ncoarser level, like characteristics of the speaker or channel that remain\nconsistent through-out a speech utterance. In this work, we propose a framework\nfor Learning Disentangled Self Supervised (termed as Learn2Diss)\nrepresentations of speech, which consists of frame-level and an utterance-level\nencoder modules. The two encoders are initially learned independently, where\nthe frame-level model is largely inspired by existing self supervision\ntechniques, thereby learning pseudo-phonemic representations, while the\nutterance-level encoder is inspired by constrastive learning of pooled\nembeddings, thereby learning pseudo-speaker representations. The joint learning\nof these two modules consists of disentangling the two encoders using a mutual\ninformation based criterion. With several downstream evaluation experiments, we\nshow that the proposed Learn2Diss achieves state-of-the-art results on a\nvariety of tasks, with the frame-level encoder representations improving\nsemantic tasks, while the utterance-level representations improve non-semantic\ntasks."}
{"id": "2407.12828", "pdf": "https://arxiv.org/pdf/2407.12828.pdf", "abs": "https://arxiv.org/abs/2407.12828", "title": "Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "authors": ["Jiaxin Qin", "Zixuan Zhang", "Manling Li", "Pengfei Yu", "Heng Ji"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Extensive previous research has focused on post-training knowledge editing\n(KE) for language models (LMs) to ensure that knowledge remains accurate and\nup-to-date. One desired property and open question in KE is to let edited LMs\ncorrectly handle ripple effects, where LM is expected to answer its logically\nrelated knowledge accurately. In this paper, we answer the question of why most\nKE methods still create messy ripple effects. We conduct extensive analysis and\nidentify a salient indicator, GradSim, that effectively reveals when and why\nupdated knowledge ripples in LMs. GradSim is computed by the cosine similarity\nbetween gradients of the original fact and its related knowledge. We observe a\nstrong positive correlation between ripple effect performance and GradSim\nacross different LMs, KE methods, and evaluation metrics. Further\ninvestigations into three counter-intuitive failure cases (Negation,\nOver-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures\nare often associated with very low GradSim. This finding validates that GradSim\nis an effective indicator of when knowledge ripples in LMs."}
{"id": "2409.00061", "pdf": "https://arxiv.org/pdf/2409.00061.pdf", "abs": "https://arxiv.org/abs/2409.00061", "title": "Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language", "authors": ["Arief Purnama Muharram", "Ayu Purwarianti"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to the Journal of ICT Research and Applications (JICTRA)", "summary": "Automated fact-checking is a key strategy to overcome the spread of COVID-19\nmisinformation on the internet. These systems typically leverage deep learning\napproaches through Natural Language Inference (NLI) to verify the truthfulness\nof information based on supporting evidence. However, one challenge that arises\nin deep learning is performance stagnation due to a lack of knowledge during\ntraining. This study proposes using a Knowledge Graph (KG) as external\nknowledge to enhance NLI performance for automated COVID-19 fact-checking in\nthe Indonesian language. The proposed model architecture comprises three\nmodules: a fact module, an NLI module, and a classifier module. The fact module\nprocesses information from the KG, while the NLI module handles semantic\nrelationships between the given premise and hypothesis. The representation\nvectors from both modules are concatenated and fed into the classifier module\nto produce the final result. The model was trained using the generated\nIndonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.\nOur study demonstrates that incorporating KGs can significantly improve NLI\nperformance in fact-checking, achieving the best accuracy of 0.8616. This\nsuggests that KGs are a valuable component for enhancing NLI performance in\nautomated fact-checking."}
{"id": "2409.18023", "pdf": "https://arxiv.org/pdf/2409.18023.pdf", "abs": "https://arxiv.org/abs/2409.18023", "title": "DARE: Diverse Visual Question Answering with Robustness Evaluation", "authors": ["Hannah Sterz", "Jonas Pfeiffer", "Ivan Vulić"], "categories": ["cs.CL"], "comment": null, "summary": "Vision Language Models (VLMs) extend remarkable capabilities of text-only\nlarge language models and vision-only models, and are able to learn from and\nprocess multi-modal vision-text input. While modern VLMs perform well on a\nnumber of standard image classification and image-text matching tasks, they\nstill struggle with a number of crucial vision-language (VL) reasoning\nabilities such as counting and spatial reasoning. Moreover, while they might be\nvery brittle to small variations in instructions and/or evaluation protocols,\nexisting benchmarks fail to evaluate their robustness (or rather the lack of\nit). In order to couple challenging VL scenarios with comprehensive robustness\nevaluation, we introduce DARE, Diverse Visual Question Answering with\nRobustness Evaluation, a carefully created and curated multiple-choice VQA\nbenchmark. DARE evaluates VLM performance on five diverse categories and\nincludes four robustness-oriented evaluations based on the variations of:\nprompts, the subsets of answer options, the output format and the number of\ncorrect answers. Among a spectrum of other findings, we report that\nstate-of-the-art VLMs still struggle with questions in most categories and are\nunable to consistently deliver their peak performance across the tested\nrobustness evaluations. The worst case performance across the subsets of\noptions is up to 34% below the performance in the standard case. The robustness\nof the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match the\nclosed-source models such as GPT-4 and Gemini, but even the latter remain very\nbrittle to different variations."}
{"id": "2410.06944", "pdf": "https://arxiv.org/pdf/2410.06944.pdf", "abs": "https://arxiv.org/abs/2410.06944", "title": "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages", "authors": ["Pretam Ray", "Jivnesh Sandhan", "Amrith Krishna", "Pawan Goyal"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2024 Main (Short), 9 pages, 3 figures, 4 Tables", "summary": "Neural dependency parsing has achieved remarkable performance for low\nresource morphologically rich languages. It has also been well-studied that\nmorphologically rich languages exhibit relatively free word order. This prompts\na fundamental investigation: Is there a way to enhance dependency parsing\nperformance, making the model robust to word order variations utilizing the\nrelatively free word order nature of morphologically rich languages? In this\nwork, we examine the robustness of graph-based parsing architectures on 7\nrelatively free word order languages. We focus on scrutinizing essential\nmodifications such as data augmentation and the removal of position encoding\nrequired to adapt these architectures accordingly. To this end, we propose a\ncontrastive self-supervised learning method to make the model robust to word\norder variations. Furthermore, our proposed modification demonstrates a\nsubstantial average gain of 3.03/2.95 points in 7 relatively free word order\nlanguages, as measured by the UAS/LAS Score metric when compared to the best\nperforming baseline."}
{"id": "2410.12601", "pdf": "https://arxiv.org/pdf/2410.12601.pdf", "abs": "https://arxiv.org/abs/2410.12601", "title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization", "authors": ["Yixi Ding", "Jiaying Wu", "Tongyao Zhu", "Yanxia Qin", "Qian Liu", "Min-Yen Kan"], "categories": ["cs.CL"], "comment": "Accepted to KDD 2025 SciSoc LLM Workshop: Large Language Models for\n  Scientific and Societal Advances", "summary": "To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning."}
{"id": "2410.20016", "pdf": "https://arxiv.org/pdf/2410.20016.pdf", "abs": "https://arxiv.org/abs/2410.20016", "title": "Vulnerability of LLMs to Vertically Aligned Text Manipulations", "authors": ["Zhecheng Li", "Yiwei Wang", "Bryan Hooi", "Yujun Cai", "Zhen Xiong", "Nanyun Peng", "Kai-wei Chang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Vertical text input is commonly encountered in various real-world\napplications, such as mathematical computations and word-based Sudoku puzzles.\nWhile current large language models (LLMs) have excelled in natural language\ntasks, they remain vulnerable to variations in text formatting. Recent research\ndemonstrates that modifying input formats, such as vertically aligning words\nfor encoder-based models, can substantially lower accuracy in text\nclassification tasks. While easily understood by humans, these inputs can\nsignificantly mislead models, posing a potential risk of bypassing detection in\nreal-world scenarios involving harmful or sensitive information. With the\nexpanding application of LLMs, a crucial question arises: \\textit{Do\ndecoder-based LLMs exhibit similar vulnerabilities to vertically formatted text\ninput?} In this paper, we investigate the impact of vertical text input on the\nperformance of various LLMs across multiple text classification datasets and\nanalyze the underlying causes. Our findings are as follows: (i) Vertical text\ninput significantly degrades the accuracy of LLMs in text classification tasks.\n(ii) \\textit{Chain of Thought (CoT)} reasoning does not help LLMs recognize\nvertical input or mitigate its vulnerability, but \\textit{few-shot learning}\nwith careful analysis does. (iii) We explore the underlying cause of the\nvulnerability by analyzing the inherent issues in tokenization and attention\nmatrices."}
{"id": "2411.05375", "pdf": "https://arxiv.org/pdf/2411.05375.pdf", "abs": "https://arxiv.org/abs/2411.05375", "title": "Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking", "authors": ["Mubashara Akhtar", "Michael Schlichtkrull", "Andreas Vlachos"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Accepted at TACL", "summary": "Current automated fact-checking (AFC) approaches typically evaluate evidence\neither implicitly via the predicted verdicts or through exact matches with\npredefined closed knowledge sources, such as Wikipedia. However, these methods\nare limited due to their reliance on evaluation metrics originally designed for\nother purposes and constraints from closed knowledge sources. In this work, we\nintroduce\n\\textbf{\\textcolor{skyblue}{Ev\\textsuperscript{2}}\\textcolor{orangebrown}{R}}\nwhich combines the strengths of reference-based evaluation and verdict-level\nproxy scoring. Ev\\textsuperscript{2}R jointly assesses how well the evidence\naligns with the gold references and how reliably it supports the verdict,\naddressing the shortcomings of prior methods. We evaluate\nEv\\textsuperscript{2}R against three types of evidence evaluation approaches:\nreference-based, proxy-reference, and reference-less baselines. Assessments\nagainst human ratings and adversarial tests demonstrate that\nEv\\textsuperscript{2}R consistently outperforms existing scoring approaches in\naccuracy and robustness. It achieves stronger correlation with human judgments\nand greater robustness to adversarial perturbations, establishing it as a\nreliable metric for evidence evaluation in AFC.\\footnote{Code is available at\n\\href{https://github.com/mubasharaak/fc-evidence-evaluation}{https://github.com/mubasharaak/fc-evidence-evaluation}.}"}
{"id": "2411.17993", "pdf": "https://arxiv.org/pdf/2411.17993.pdf", "abs": "https://arxiv.org/abs/2411.17993", "title": "DRS: Deep Question Reformulation With Structured Output", "authors": ["Zhecheng Li", "Yiwei Wang", "Bryan Hooi", "Yujun Cai", "Nanyun Peng", "Kai-Wei Chang"], "categories": ["cs.CL"], "comment": "Findings of the Association for Computational Linguistics (ACL 2025)", "summary": "Question answering represents a core capability of large language models\n(LLMs). However, when individuals encounter unfamiliar knowledge in texts, they\noften formulate questions that the text itself cannot answer due to\ninsufficient understanding of the underlying information. Recent studies reveal\nthat while LLMs can detect unanswerable questions, they struggle to assist\nusers in reformulating these questions. Even advanced models like GPT-3.5\ndemonstrate limited effectiveness in this regard. To address this limitation,\nwe propose DRS: Deep Question Reformulation with Structured Output, a novel\nzero-shot method aimed at enhancing LLMs ability to assist users in\nreformulating questions to extract relevant information from new documents. DRS\ncombines the strengths of LLMs with a DFS-based algorithm to iteratively\nexplore potential entity combinations and constrain outputs using predefined\nentities. This structured approach significantly enhances the reformulation\ncapabilities of LLMs. Comprehensive experimental evaluations demonstrate that\nDRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while\nalso enhancing the performance of open-source models, such as Gemma2-9B, from\n26.35% to 56.75%."}
{"id": "2412.03920", "pdf": "https://arxiv.org/pdf/2412.03920.pdf", "abs": "https://arxiv.org/abs/2412.03920", "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios", "authors": ["Xiachong Feng", "Longxu Dou", "Ella Li", "Qinghao Wang", "Haochuan Wang", "Yu Guo", "Chang Ma", "Lingpeng Kong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Game-theoretic scenarios have become pivotal in evaluating the social\nintelligence of Large Language Model (LLM)-based social agents. While numerous\nstudies have explored these agents in such settings, there is a lack of a\ncomprehensive survey summarizing the current progress. To address this gap, we\nsystematically review existing research on LLM-based social agents within\ngame-theoretic scenarios. Our survey organizes the findings into three core\ncomponents: Game Framework, Social Agent, and Evaluation Protocol. The game\nframework encompasses diverse game scenarios, ranging from choice-focusing to\ncommunication-focusing games. The social agent part explores agents'\npreferences, beliefs, and reasoning abilities, as well as their interactions\nand synergistic effects on decision-making. The evaluation protocol covers both\ngame-agnostic and game-specific metrics for assessing agent performance.\nAdditionally, we analyze the performance of current social agents across\nvarious game scenarios. By reflecting on the current research and identifying\nfuture research directions, this survey provides insights to advance the\ndevelopment and evaluation of social agents in game-theoretic scenarios."}
{"id": "2412.08985", "pdf": "https://arxiv.org/pdf/2412.08985.pdf", "abs": "https://arxiv.org/abs/2412.08985", "title": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts in K-12 Education?", "authors": ["Tianshi Zheng", "Weihan Li", "Jiaxin Bai", "Weiqi Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs."}
{"id": "2501.03441", "pdf": "https://arxiv.org/pdf/2501.03441.pdf", "abs": "https://arxiv.org/abs/2501.03441", "title": "Finding A Voice: Exploring the Potential of African American Dialect and Voice Generation for Chatbots", "authors": ["Sarah E. Finch", "Ellie S. Paek", "Ikseon Choi", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "As chatbots become integral to daily life, personalizing systems is key for\nfostering trust, engagement, and inclusivity. This study examines how\nlinguistic similarity affects chatbot performance, focusing on integrating\nAfrican American English (AAE) into virtual agents to better serve the African\nAmerican community. We develop text-based and spoken chatbots using large\nlanguage models and text-to-speech technology, then evaluate them with AAE\nspeakers against standard English chatbots. Our results show that while\ntext-based AAE chatbots often underperform, spoken chatbots benefit from an\nAfrican American voice and AAE elements, improving performance and preference.\nThese findings underscore the complexities of linguistic personalization and\nthe dynamics between text and speech modalities, highlighting technological\nlimitations that affect chatbots' AA speech generation and pointing to\npromising future research directions."}
{"id": "2502.05986", "pdf": "https://arxiv.org/pdf/2502.05986.pdf", "abs": "https://arxiv.org/abs/2502.05986", "title": "Preventing Rogue Agents Improves Multi-Agent Collaboration", "authors": ["Ohav Barbi", "Ori Yoran", "Mor Geva"], "categories": ["cs.CL", "cs.MA"], "comment": "Accepted as a spotlight to REALM (First Workshop for Research on\n  Agent Language Models) at ACL 2025", "summary": "Multi-agent systems, where specialized agents collaborate to solve a shared\ntask hold great potential, from increased modularity to simulating complex\nenvironments. However, they also have a major caveat -- a single agent can\ncause the entire system to fail. Consider a simple game where the knowledge to\nsolve the task is distributed between agents, which share information in a\ncommunication channel. At each round, any of the agents can terminate the game\nand make the final prediction, even if they are uncertain about the outcome of\ntheir action. Detection of such rogue agents before they act may prevent the\nsystem's failure. In this work, we propose to monitor agents during action\nprediction and intervene when a future error is likely to occur. To test our\napproach, we introduce WhoDunitEnv, a multi-agent collaboration environment\nthat allows modular control over task complexity and communication structure.\nExperiments on WhoDunitEnv, code generation tasks and the GovSim environment\nfor resource sustainability show that our approach leads to substantial\nperformance gains up to 17.4%, 2.5% and 20%, respectively. Thorough analysis\nshows that our monitors successfully identify critical points of agent\nconfusion and our interventions effectively stop agent errors from propagating."}
{"id": "2502.07057", "pdf": "https://arxiv.org/pdf/2502.07057.pdf", "abs": "https://arxiv.org/abs/2502.07057", "title": "Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih Gümüş", "Sercan Karakaş", "Banu Diri", "Savaş Yıldırım"], "categories": ["cs.CL", "68T50, 68T10", "I.2.7; I.2.6; H.3.1"], "comment": null, "summary": "Tokenization is a fundamental preprocessing step in NLP, directly impacting\nlarge language models' (LLMs) ability to capture syntactic, morphosyntactic,\nand semantic structures. This paper introduces a novel framework for\nsystematically evaluating tokenization strategies, addressing challenges in\nmorphologically rich and low-resource languages. Using a Turkish dataset of\n6,200 multiple-choice questions from the Massive Multitask Language\nUnderstanding (MMLU) benchmark, the framework assesses tokenizers across five\nkey metrics: vocabulary size, token count, processing time, language-specific\ntoken percentages (\\%TR), and token purity. These metrics provide a structured\napproach to evaluating how well tokenizers preserve linguistic structures.\nWhile \\%TR measures the proportion of valid words in the target language,\n\\%Pure assesses the alignment of tokens with meaningful linguistic units, such\nas roots and valid morphemes, minimizing semantic fragmentation. The findings\nreveal that \\%TR, introduced as a critical metric, exhibits a stronger\ncorrelation with downstream performance (e.g., MMLU scores) than token purity,\nemphasizing its role in improving model accuracy. Additionally, larger model\nparameters do not necessarily yield better tokenization quality or enhanced\nresults, highlighting the importance of tailored tokenization strategies that\nprioritize linguistic alignment. This framework sets a new standard for\ndeveloping robust tokenization methods optimized for morphologically complex\nand low-resource languages. Future work will refine morphological analysis,\nexplore domain-specific customizations, and conduct cross-linguistic\nevaluations to further enhance tokenization practices."}
{"id": "2502.10871", "pdf": "https://arxiv.org/pdf/2502.10871.pdf", "abs": "https://arxiv.org/abs/2502.10871", "title": "Layerwise Recall and the Geometry of Interwoven Knowledge in LLMs", "authors": ["Ge Lei", "Samuel J. Cooper"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study explores how large language models (LLMs) encode interwoven\nscientific knowledge, using chemical elements and LLaMA-series models as a case\nstudy. We identify a 3D spiral structure in the hidden states that aligns with\nthe conceptual structure of the periodic table, suggesting that LLMs can\nreflect the geometric organization of scientific concepts learned from text.\nLinear probing reveals that middle layers encode continuous, overlapping\nattributes that enable indirect recall, while deeper layers sharpen categorical\ndistinctions and incorporate linguistic context. These findings suggest that\nLLMs represent symbolic knowledge not as isolated facts, but as structured\ngeometric manifolds that intertwine semantic information across layers. We hope\nthis work inspires further exploration of how LLMs represent and reason about\nscientific knowledge, particularly in domains such as materials science."}
{"id": "2502.11476", "pdf": "https://arxiv.org/pdf/2502.11476.pdf", "abs": "https://arxiv.org/abs/2502.11476", "title": "FastMCTS: A Simple Sampling Strategy for Data Synthesis", "authors": ["Peiji Li", "Kai Lv", "Yunfan Shao", "Yichuan Ma", "Linyang Li", "Xiaoqing Zheng", "Xipeng Qiu", "Qipeng Guo"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Synthetic high-quality multi-step reasoning data can significantly enhance\nthe performance of large language models on various tasks. However, most\nexisting methods rely on rejection sampling, which generates trajectories\nindependently and suffers from inefficiency and imbalanced sampling across\nproblems of varying difficulty. In this work, we introduce FastMCTS, an\ninnovative data synthesis strategy inspired by Monte Carlo Tree Search.\nFastMCTS provides a more efficient sampling method for multi-step reasoning\ndata, offering step-level evaluation signals and promoting balanced sampling\nacross problems of different difficulty levels. Experiments on both English and\nChinese reasoning datasets demonstrate that FastMCTS generates over 30\\% more\ncorrect reasoning paths compared to rejection sampling as the number of\ngenerated tokens scales up. Furthermore, under comparable synthetic data\nbudgets, models trained on FastMCTS-generated data outperform those trained on\nrejection sampling data by 3.9\\% across multiple benchmarks. As a lightweight\nsampling strategy, FastMCTS offers a practical and efficient alternative for\nsynthesizing high-quality reasoning data. Our code will be released soon."}
{"id": "2502.12788", "pdf": "https://arxiv.org/pdf/2502.12788.pdf", "abs": "https://arxiv.org/abs/2502.12788", "title": "Commonsense Reasoning in Arab Culture", "authors": ["Abdelrahman Sadallah", "Junior Cedric Tonga", "Khalid Almubarak", "Saeed Almheiri", "Farah Atif", "Chatrine Qwaider", "Karima Kadaoui", "Sara Shatnawi", "Yaser Alesh", "Fajri Koto"], "categories": ["cs.CL"], "comment": "ACL 2025 - Main", "summary": "Despite progress in Arabic large language models, such as Jais and AceGPT,\ntheir evaluation on commonsense reasoning has largely relied on\nmachine-translated datasets, which lack cultural depth and may introduce\nAnglocentric biases. Commonsense reasoning is shaped by geographical and\ncultural contexts, and existing English datasets fail to capture the diversity\nof the Arab world. To address this, we introduce ArabCulture, a commonsense\nreasoning dataset in Modern Standard Arabic (MSA), covering cultures of 13\ncountries across the Gulf, Levant, North Africa, and the Nile Valley. The\ndataset was built from scratch by engaging native speakers to write and\nvalidate culturally relevant questions for their respective countries.\nArabCulture spans 12 daily life domains with 54 fine-grained subtopics,\nreflecting various aspects of social norms, traditions, and everyday\nexperiences. Zero-shot evaluations show that open-weight language models with\nup to 32B parameters struggle to comprehend diverse Arab cultures, with\nperformance varying across regions. These findings highlight the need for more\nculturally aware models and datasets tailored to the Arabic-speaking world."}
{"id": "2502.12829", "pdf": "https://arxiv.org/pdf/2502.12829.pdf", "abs": "https://arxiv.org/abs/2502.12829", "title": "KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan", "authors": ["Mukhammed Togmanov", "Nurdaulet Mukhituly", "Diana Turmakhan", "Jonibek Mansurov", "Maiya Goloburda", "Akhmed Sakip", "Zhuohan Xie", "Yuxia Wang", "Bekassyl Syzdykov", "Nurkhan Laiyk", "Alham Fikri Aji", "Ekaterina Kochmar", "Preslav Nakov", "Fajri Koto"], "categories": ["cs.CL"], "comment": null, "summary": "Despite having a population of twenty million, Kazakhstan's culture and\nlanguage remain underrepresented in the field of natural language processing.\nAlthough large language models (LLMs) continue to advance worldwide, progress\nin Kazakh language has been limited, as seen in the scarcity of dedicated\nmodels and benchmark evaluations. To address this gap, we introduce KazMMLU,\nthe first MMLU-style dataset specifically designed for Kazakh language. KazMMLU\ncomprises 23,000 questions that cover various educational levels, including\nSTEM, humanities, and social sciences, sourced from authentic educational\nmaterials and manually validated by native speakers and educators. The dataset\nincludes 10,969 Kazakh questions and 12,031 Russian questions, reflecting\nKazakhstan's bilingual education system and rich local context. Our evaluation\nof several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,\nand DeepSeek V3) demonstrates substantial room for improvement, as even the\nbest-performing models struggle to achieve competitive performance in Kazakh\nand Russian. These findings underscore significant performance gaps compared to\nhigh-resource languages. We hope that our dataset will enable further research\nand development of Kazakh-centric LLMs. Data and code will be made available\nupon acceptance."}
{"id": "2502.14642", "pdf": "https://arxiv.org/pdf/2502.14642.pdf", "abs": "https://arxiv.org/abs/2502.14642", "title": "How Far are LLMs from Being Our Digital Twins? A Benchmark for Persona-Based Behavior Chain Simulation", "authors": ["Rui Li", "Heming Xia", "Xinfeng Yuan", "Qingxiu Dong", "Lei Sha", "Wenjie Li", "Zhifang Sui"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Recently, LLMs have garnered increasing attention across academic disciplines\nfor their potential as human digital twins, virtual proxies designed to\nreplicate individuals and autonomously perform tasks such as decision-making,\nproblem-solving, and reasoning on their behalf. However, current evaluations of\nLLMs primarily emphasize dialogue simulation while overlooking human behavior\nsimulation, which is crucial for digital twins. To address this gap, we\nintroduce BehaviorChain, the first benchmark for evaluating LLMs' ability to\nsimulate continuous human behavior. BehaviorChain comprises diverse,\nhigh-quality, persona-based behavior chains, totaling 15,846 distinct behaviors\nacross 1,001 unique personas, each with detailed history and profile metadata.\nFor evaluation, we integrate persona metadata into LLMs and employ them to\niteratively infer contextually appropriate behaviors within dynamic scenarios\nprovided by BehaviorChain. Comprehensive evaluation results demonstrated that\neven state-of-the-art models struggle with accurately simulating continuous\nhuman behavior."}
{"id": "2502.14916", "pdf": "https://arxiv.org/pdf/2502.14916.pdf", "abs": "https://arxiv.org/abs/2502.14916", "title": "MKE-Coder: Multi-Axial Knowledge with Evidence Verification in ICD Coding for Chinese EMRs", "authors": ["Xinxin You", "Xien Liu", "Xue Yang", "Ziyi Wang", "Ji Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "We have decided to withdraw this manuscript in order to allow for\n  further revisions and additional experiments", "summary": "The task of automatically coding the International Classification of Diseases\n(ICD) in the medical field has been well-established and has received much\nattention. Automatic coding of the ICD in the medical field has been successful\nin English but faces challenges when dealing with Chinese electronic medical\nrecords (EMRs). The first issue lies in the difficulty of extracting disease\ncode-related information from Chinese EMRs, primarily due to the concise\nwriting style and specific internal structure of the EMRs. The second problem\nis that previous methods have failed to leverage the disease-based multi-axial\nknowledge and lack of association with the corresponding clinical evidence.\nThis paper introduces a novel framework called MKE-Coder: Multi-axial Knowledge\nwith Evidence verification in ICD coding for Chinese EMRs. Initially, we\nidentify candidate codes for the diagnosis and categorize each of them into\nknowledge under four coding axes.Subsequently, we retrieve corresponding\nclinical evidence from the comprehensive content of EMRs and filter credible\nevidence through a scoring model. Finally, to ensure the validity of the\ncandidate code, we propose an inference module based on the masked language\nmodeling strategy. This module verifies that all the axis knowledge associated\nwith the candidate code is supported by evidence and provides recommendations\naccordingly. To evaluate the performance of our framework, we conduct\nexperiments using a large-scale Chinese EMR dataset collected from various\nhospitals. The experimental results demonstrate that MKE-Coder exhibits\nsignificant superiority in the task of automatic ICD coding based on Chinese\nEMRs. In the practical evaluation of our method within simulated real coding\nscenarios, it has been demonstrated that our approach significantly aids coders\nin enhancing both their coding accuracy and speed."}
{"id": "2502.15090", "pdf": "https://arxiv.org/pdf/2502.15090.pdf", "abs": "https://arxiv.org/abs/2502.15090", "title": "Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans", "authors": ["Masha Fedzechkina", "Eleonora Gualdoni", "Sinead Williamson", "Katherine Metcalf", "Skyler Seto", "Barry-John Theobald"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern large language models (LLMs) achieve impressive performance on some\ntasks, while exhibiting distinctly non-human-like behaviors on others. This\nraises the question of how well the LLM's learned representations align with\nhuman representations. In this work, we introduce a novel approach to study\nrepresentation alignment: we adopt a method from research on activation\nsteering to identify neurons responsible for specific concepts (e.g., ''cat'')\nand then analyze the corresponding activation patterns. We find that LLM\nrepresentations captured this way closely align with human representations\ninferred from behavioral data, matching inter-human alignment levels. Our\napproach significantly outperforms the alignment captured by word embeddings,\nwhich have been the focus of prior work on human-LLM alignment. Additionally,\nour approach enables a more granular view of how LLMs represent concepts -- we\nshow that LLMs organize concepts in a way that mirrors human concept\norganization."}
{"id": "2502.15639", "pdf": "https://arxiv.org/pdf/2502.15639.pdf", "abs": "https://arxiv.org/abs/2502.15639", "title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models", "authors": ["Anirudh Sundar", "Sinead Williamson", "Katherine Metcalf", "Barry-John Theobald", "Skyler Seto", "Masha Fedzechkina"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "34 pages", "summary": "Aligned representations across languages is a desired property in\nmultilingual large language models (mLLMs), as alignment can improve\nperformance in cross-lingual tasks. Typically alignment requires fine-tuning a\nmodel, which is computationally expensive, and sizable language data, which\noften may not be available. A data-efficient alternative to fine-tuning is\nmodel interventions -- a method for manipulating model activations to steer\ngeneration into the desired direction. We analyze the effect of a popular\nintervention (finding experts) on the alignment of cross-lingual\nrepresentations in mLLMs. We identify the neurons to manipulate for a given\nlanguage and introspect the embedding space of mLLMs pre- and\npost-manipulation. We show that modifying the mLLM's activations changes its\nembedding space such that cross-lingual alignment is enhanced. Further, we show\nthat the changes to the embedding space translate into improved downstream\nperformance on retrieval tasks, with up to 2x improvements in top-1 accuracy on\ncross-lingual retrieval."}
{"id": "2502.16002", "pdf": "https://arxiv.org/pdf/2502.16002.pdf", "abs": "https://arxiv.org/abs/2502.16002", "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse", "authors": ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Yujia Bao", "Shiyu Chang"], "categories": ["cs.CL"], "comment": null, "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."}
{"id": "2502.17163", "pdf": "https://arxiv.org/pdf/2502.17163.pdf", "abs": "https://arxiv.org/abs/2502.17163", "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation", "authors": ["María Andrea Cruz Blandón", "Jayasimha Talur", "Bruno Charron", "Dong Liu", "Saab Mansour", "Marcello Federico"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. Our\ndataset is available at https://github.com/amazon-science/MEMERAG"}
{"id": "2502.19545", "pdf": "https://arxiv.org/pdf/2502.19545.pdf", "abs": "https://arxiv.org/abs/2502.19545", "title": "Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in Product QA Agents", "authors": ["Ashley Lewis", "Michael White", "Jing Liu", "Toshiaki Koike-Akino", "Kieran Parsons", "Ye Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The deployment of Large Language Models (LLMs) in customer support is\nconstrained by hallucination (generating false information) and the high cost\nof proprietary models. To address these challenges, we propose a\nretrieval-augmented question-answering (QA) pipeline and explore how to balance\nhuman input and automation. Using a dataset of questions about a Samsung Smart\nTV user manual, we demonstrate that synthetic data generated by LLMs\noutperforms crowdsourced data in reducing hallucination in finetuned models. We\nalso compare self-training (fine-tuning models on their own outputs) and\nknowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o),\nand find that self-training achieves comparable hallucination reduction. We\nconjecture that this surprising finding can be attributed to increased exposure\nbias issues in the knowledge distillation case and support this conjecture with\npost hoc analysis. We also improve robustness to unanswerable questions and\nretrieval failures with contextualized \"I don't know\" responses. These findings\nshow that scalable, cost-efficient QA systems can be built using synthetic data\nand self-training with open-source models, reducing reliance on proprietary\ntools or costly human annotations."}
{"id": "2503.00024", "pdf": "https://arxiv.org/pdf/2503.00024.pdf", "abs": "https://arxiv.org/abs/2503.00024", "title": "Do Emotions Really Affect Argument Convincingness? A Dynamic Approach with LLM-based Manipulation Checks", "authors": ["Yanran Chen", "Steffen Eger"], "categories": ["cs.CL"], "comment": "ACL 2025 Camera-ready", "summary": "Emotions have been shown to play a role in argument convincingness, yet this\naspect is underexplored in the natural language processing (NLP) community.\nUnlike prior studies that use static analyses, focus on a single text domain or\nlanguage, or treat emotion as just one of many factors, we introduce a dynamic\nframework inspired by manipulation checks commonly used in psychology and\nsocial science; leveraging LLM-based manipulation checks, this framework\nexamines the extent to which perceived emotional intensity influences perceived\nconvincingness. Through human evaluation of arguments across different\nlanguages, text domains, and topics, we find that in over half of cases, human\njudgments of convincingness remain unchanged despite variations in perceived\nemotional intensity; when emotions do have an impact, they more often enhance\nrather than weaken convincingness. We further analyze whether 11 LLMs behave\nlike humans in the same scenario, finding that while LLMs generally mirror\nhuman patterns, they struggle to capture nuanced emotional effects in\nindividual judgments."}
{"id": "2503.01781", "pdf": "https://arxiv.org/pdf/2503.01781.pdf", "abs": "https://arxiv.org/abs/2503.01781", "title": "Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning Models", "authors": ["Meghana Rajeev", "Rajkumar Ramamurthy", "Prapti Trivedi", "Vikas Yadav", "Oluwanifemi Bamgbose", "Sathwik Tejaswi Madhusudan", "James Zou", "Nazneen Rajani"], "categories": ["cs.CL"], "comment": "Accepted to CoLM 2025", "summary": "We investigate the robustness of reasoning models trained for step-by-step\nproblem solving by introducing query-agnostic adversarial triggers - short,\nirrelevant text that, when appended to math problems, systematically mislead\nmodels to output incorrect answers without altering the problem's semantics. We\npropose CatAttack, an automated iterative attack pipeline for generating\ntriggers on a weaker, less expensive proxy model (DeepSeek V3) and successfully\ntransfer them to more advanced reasoning target models like DeepSeek R1 and\nDeepSeek R1-distilled-Qwen-32B, resulting in greater than 300% increase in the\nlikelihood of the target model generating an incorrect answer. For example,\nappending, \"Interesting fact: cats sleep most of their lives,\" to any math\nproblem leads to more than doubling the chances of a model getting the answer\nwrong. Our findings highlight critical vulnerabilities in reasoning models,\nrevealing that even state-of-the-art models remain susceptible to subtle\nadversarial inputs, raising security and reliability concerns. The CatAttack\ntriggers dataset with model responses is available at\nhttps://huggingface.co/datasets/collinear-ai/cat-attack-adversarial-triggers."}
{"id": "2503.05641", "pdf": "https://arxiv.org/pdf/2503.05641.pdf", "abs": "https://arxiv.org/abs/2503.05641", "title": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning", "authors": ["Justin Chih-Yao Chen", "Sukwon Yun", "Elias Stengel-Eskin", "Tianlong Chen", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The first three authors contributed equally. Project Page:\n  https://symbolic-moe.github.io/", "summary": "Combining existing pre-trained expert LLMs is a promising avenue for scalably\ntackling large-scale and diverse tasks. However, selecting task-level experts\nis often too coarse-grained, as heterogeneous tasks may require different\nexpertise per instance. To enable adaptive instance-level mixing of pre-trained\nLLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free\nMixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to\nselection by emphasizing skills, e.g., algebra in math or molecular biology in\nbiomedical reasoning. We propose a skill-based recruiting strategy that\ndynamically selects the most relevant set of expert LLMs for diverse reasoning\ntasks based on their strengths. Each selected expert then generates its own\nreasoning, resulting in k outputs from k experts, which are then synthesized\ninto a final high-quality response by an aggregator chosen based on its ability\nto integrate diverse reasoning outputs. We show that Symbolic-MoE's\ninstance-level expert selection improves performance by a large margin but --\nwhen implemented naively -- can introduce a high computational overhead due to\nthe need for constant model loading and offloading. To address this, we\nimplement a batch strategy that groups instances based on their assigned\nexperts, loading each model only once. This allows us to integrate 16 expert\nmodels on 1 GPU with a time cost comparable to or better than prior multi-agent\nbaselines using 4 GPUs. Through extensive evaluations on diverse benchmarks\n(MMLU-Pro, GPQA, AIME, and MedMCQA), we show that Symbolic-MoE beats strong\nLLMs like GPT4o-mini, as well as multi-agent approaches, with an absolute avg.\ngain of 8.15% over the best multi-agent baseline. Moreover, Symbolic-MoE\ngeneralizes well to unseen tasks and removes the need for expensive multi-round\ndiscussions, outperforming discussion baselines with less computation."}
{"id": "2503.09516", "pdf": "https://arxiv.org/pdf/2503.09516.pdf", "abs": "https://arxiv.org/abs/2503.09516", "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "authors": ["Bowen Jin", "Hansi Zeng", "Zhenrui Yue", "Jinsung Yoon", "Sercan Arik", "Dong Wang", "Hamed Zamani", "Jiawei Han"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "31 pages", "summary": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities to use search\nengines during inference is often suboptimal, as the LLM might not fully\npossess the capability on how to interact optimally with the search engine.\nThis paper introduces Search-R1, an extension of reinforcement learning (RL)\nfor reasoning frameworks where the LLM learns to autonomously generate\n(multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn\nsearch interactions, leveraging retrieved token masking for stable RL training\nand a simple outcome-based reward function. Experiments on seven\nquestion-answering datasets show that Search-R1 improves performance by 41%\n(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same\nsetting. This paper further provides empirical insights into RL optimization\nmethods, LLM choices, and response length dynamics in retrieval-augmented\nreasoning. The code and model checkpoints are available at\nhttps://github.com/PeterGriffinJin/Search-R1."}
{"id": "2503.11299", "pdf": "https://arxiv.org/pdf/2503.11299.pdf", "abs": "https://arxiv.org/abs/2503.11299", "title": "BriLLM: Brain-inspired Large Language Model", "authors": ["Hai Zhao", "Hongqiu Wu", "Dongjie Yang", "Anni Zou", "Jiale Hong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper reports the first brain-inspired large language model (BriLLM).\nThis is a non-Transformer, non-GPT, non-traditional machine learning\ninput-output controlled generative language model. The model is based on the\nSignal Fully-connected flowing (SiFu) definition on the directed graph in terms\nof the neural network, and has the interpretability of all nodes on the graph\nof the whole model, instead of the traditional machine learning model that only\nhas limited interpretability at the input and output ends. In the language\nmodel scenario, the token is defined as a node in the graph. A randomly shaped\nor user-defined signal flow flows between nodes on the principle of \"least\nresistance\" along paths. The next token or node to be predicted or generated is\nthe target of the signal flow. As a language model, BriLLM theoretically\nsupports infinitely long $n$-gram models when the model size is independent of\nthe input and predicted length of the model. The model's working signal flow\nprovides the possibility of recall activation and innate multi-modal support\nsimilar to the cognitive patterns of the human brain. At present, we released\nthe first BriLLM version in Chinese, with 4000 tokens, 32-dimensional node\nwidth, 16-token long sequence prediction ability, and language model prediction\nperformance comparable to GPT-1. More computing power will help us explore the\ninfinite possibilities depicted above."}
{"id": "2503.11858", "pdf": "https://arxiv.org/pdf/2503.11858.pdf", "abs": "https://arxiv.org/abs/2503.11858", "title": "OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights LLMs", "authors": ["Ivan Kartáč", "Mateusz Lango", "Ondřej Dušek"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated great potential as evaluators\nof NLG systems, allowing for high-quality, reference-free, and multi-aspect\nassessments. However, existing LLM-based metrics suffer from two major\ndrawbacks: reliance on proprietary models to generate training data or perform\nevaluations, and a lack of fine-grained, explanatory feedback. In this paper,\nwe introduce OpeNLGauge, a fully open-source, reference-free NLG evaluation\nmetric that provides accurate explanations based on error spans. OpeNLGauge is\navailable as a two-stage ensemble of larger open-weight LLMs, or as a small\nfine-tuned evaluation model, with confirmed generalizability to unseen tasks,\ndomains and aspects. Our extensive meta-evaluation shows that OpeNLGauge\nachieves competitive correlation with human judgments, outperforming\nstate-of-the-art models on certain tasks while maintaining full reproducibility\nand providing explanations more than twice as accurate."}
{"id": "2503.15220", "pdf": "https://arxiv.org/pdf/2503.15220.pdf", "abs": "https://arxiv.org/abs/2503.15220", "title": "Entity-aware Cross-lingual Claim Detection for Automated Fact-checking", "authors": ["Rrubaa Panchendrarajan", "Arkaitz Zubiaga"], "categories": ["cs.CL"], "comment": null, "summary": "Identifying claims requiring verification is a critical task in automated\nfact-checking, especially given the proliferation of misinformation on social\nmedia platforms. Despite notable progress, challenges remain-particularly in\nhandling multilingual data prevalent in online discourse. Recent efforts have\nfocused on fine-tuning pre-trained multilingual language models to address\nthis. While these models can handle multiple languages, their ability to\neffectively transfer cross-lingual knowledge for detecting claims spreading on\nsocial media remains under-explored. In this paper, we introduce EX-Claim, an\nentity-aware cross-lingual claim detection model that generalizes well to\nhandle multilingual claims. The model leverages entity information derived from\nnamed entity recognition and entity linking techniques to improve the\nlanguage-level performance of both seen and unseen languages during training.\nExtensive experiments conducted on three datasets from different social media\nplatforms demonstrate that our proposed model stands out as an effective\nsolution, demonstrating consistent performance gains across 27 languages and\nrobust knowledge transfer between languages seen and unseen during training."}
{"id": "2503.21544", "pdf": "https://arxiv.org/pdf/2503.21544.pdf", "abs": "https://arxiv.org/abs/2503.21544", "title": "SWI: Speaking with Intent in Large Language Models", "authors": ["Yuwei Yin", "EunJeong Hwang", "Giuseppe Carenini"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "Code: https://github.com/YuweiYin/SWI", "summary": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for communication and problem-solving. This paper introduces the\nconcept of Speaking with Intent (SWI) in large language models (LLMs), where\nthe explicitly generated intent encapsulates the model's underlying intention\nand provides high-level planning to guide subsequent analysis and action. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on text summarization, multi-task question\nanswering, and mathematical reasoning benchmarks consistently demonstrate the\neffectiveness and generalizability of Speaking with Intent over direct\ngeneration without explicit intent. Further analysis corroborates the\ngeneralizability of SWI under different experimental settings. Moreover, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. The promising results in enhancing LLMs with explicit\nintents pave a new avenue for boosting LLMs' generation and reasoning abilities\nwith cognitive notions."}
{"id": "2503.23768", "pdf": "https://arxiv.org/pdf/2503.23768.pdf", "abs": "https://arxiv.org/abs/2503.23768", "title": "Texture or Semantics? Vision-Language Models Get Lost in Font Recognition", "authors": ["Zhecheng Li", "Guoxian Song", "Yujun Cai", "Zhen Xiong", "Junsong Yuan", "Yiwei Wang"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted to COLM 2025", "summary": "Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic\ncapabilities, achieving impressive performance in various tasks such as image\nrecognition and object localization. However, their effectiveness in\nfine-grained tasks remains an open question. In everyday scenarios, individuals\nencountering design materials, such as magazines, typography tutorials,\nresearch papers, or branding content, may wish to identify aesthetically\npleasing fonts used in the text. Given their multimodal capabilities and free\naccessibility, many VLMs are often considered potential tools for font\nrecognition. This raises a fundamental question: Do VLMs truly possess the\ncapability to recognize fonts? To investigate this, we introduce the Font\nRecognition Benchmark (FRB), a compact and well-structured dataset comprising\n15 commonly used fonts. FRB includes two versions: (i) an easy version, where\n10 sentences are rendered in different fonts, and (ii) a hard version, where\neach text sample consists of the names of the 15 fonts themselves, introducing\na stroop effect that challenges model perception. Through extensive evaluation\nof various VLMs on font recognition tasks, we arrive at the following key\nfindings: (i) Current VLMs exhibit limited font recognition capabilities, with\nmany state-of-the-art models failing to achieve satisfactory performance and\nbeing easily affected by the stroop effect introduced by textual information.\n(ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal\nbenefits in improving font recognition accuracy across different VLMs. (iii)\nAttention analysis sheds light on the inherent limitations of VLMs in capturing\nsemantic features."}
{"id": "2504.01216", "pdf": "https://arxiv.org/pdf/2504.01216.pdf", "abs": "https://arxiv.org/abs/2504.01216", "title": "Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models", "authors": ["Feng Chen", "Dror Ben-Zeev", "Gillian Sparks", "Arya Kadakia", "Trevor Cohen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Post-Traumatic Stress Disorder (PTSD) remains underdiagnosed in clinical\nsettings, presenting opportunities for automated detection to identify\npatients. This study evaluates natural language processing approaches for\ndetecting PTSD from clinical interview transcripts. We compared general and\nmental health-specific transformer models (BERT/RoBERTa), embedding-based\nmethods (SentenceBERT/LLaMA), and large language model prompting strategies\n(zero-shot/few-shot/chain-of-thought) using the DAIC-WOZ dataset.\nDomain-specific end-to-end models significantly outperformed general models\n(Mental-RoBERTa AUPRC=0.675+/-0.084 vs. RoBERTa-base 0.599+/-0.145).\nSentenceBERT embeddings with neural networks achieved the highest overall\nperformance (AUPRC=0.758+/-0.128). Few-shot prompting using DSM-5 criteria\nyielded competitive results with two examples (AUPRC=0.737). Performance varied\nsignificantly across symptom severity and comorbidity status with depression,\nwith higher accuracy for severe PTSD cases and patients with comorbid\ndepression. Our findings highlight the potential of domain-adapted embeddings\nand LLMs for scalable screening while underscoring the need for improved\ndetection of nuanced presentations and offering insights for developing\nclinically viable AI tools for PTSD assessment."}
{"id": "2504.03022", "pdf": "https://arxiv.org/pdf/2504.03022.pdf", "abs": "https://arxiv.org/abs/2504.03022", "title": "The Dual-Route Model of Induction", "authors": ["Sheridan Feucht", "Eric Todd", "Byron Wallace", "David Bau"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "43 pages, 49 figures. Published as a conference paper at COLM 2025.\n  Code and data at https://dualroute.baulab.info", "summary": "Prior work on in-context copying has shown the existence of induction heads,\nwhich attend to and promote individual tokens during copying. In this work we\ndiscover a new type of induction head: concept-level induction heads, which\ncopy entire lexical units instead of individual tokens. Concept induction heads\nlearn to attend to the ends of multi-token words throughout training, working\nin parallel with token-level induction heads to copy meaningful text. We show\nthat these heads are responsible for semantic tasks like word-level\ntranslation, whereas token induction heads are vital for tasks that can only be\ndone verbatim (like copying nonsense tokens). These two \"routes\" operate\nindependently: we show that ablation of token induction heads causes models to\nparaphrase where they would otherwise copy verbatim. By patching concept\ninduction head outputs, we find that they contain language-independent word\nrepresentations that mediate natural language translation, suggesting that LLMs\nrepresent abstract word meanings independent of language or form."}
{"id": "2504.03601", "pdf": "https://arxiv.org/pdf/2504.03601.pdf", "abs": "https://arxiv.org/abs/2504.03601", "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay", "authors": ["Akshara Prabhakar", "Zuxin Liu", "Ming Zhu", "Jianguo Zhang", "Tulika Awalgaonkar", "Shiyu Wang", "Zhiwei Liu", "Haolin Chen", "Thai Hoang", "Juan Carlos Niebles", "Shelby Heinecke", "Weiran Yao", "Huan Wang", "Silvio Savarese", "Caiming Xiong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages plus references and appendices; fixes typo in fig 6", "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source 5K synthetic data trajectories\nand the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4;\nDataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website\nat https://apigen-mt.github.io"}
{"id": "2504.09763", "pdf": "https://arxiv.org/pdf/2504.09763.pdf", "abs": "https://arxiv.org/abs/2504.09763", "title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems", "authors": ["Zaid Khan", "Elias Stengel-Eskin", "Archiki Prasad", "Jaemin Cho", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://zaidkhan.me/EFAGen/", "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from reinforcement learning (procedural\nenvironments) to physics (simulation engines). These programs can be seen as\nfunctions which execute to different outputs based on their parameterizations\n(e.g., gridworld configuration or initial physical conditions). We introduce\nthe term EFA (Executable Functional Abstraction) to denote such programs for\nmath problems. EFA-like constructs have been shown to be useful for\nmathematical reasoning as problem generators for stress-testing models.\nHowever, prior work has been limited to automatically constructing abstractions\nfor grade-school math (whose simple rules are easy to encode in programs),\nwhile generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced\nmathematics problems by developing EFAGen, which operationalizes the task of\nautomatically inferring an EFA for a given seed problem and solution as a\nprogram synthesis task. We first formalize the properties of any valid EFA as\nexecutable unit tests. Using execution feedback from the unit tests, we search\nover candidate programs sampled from a LLM to find EFA programs that are\nfaithful to the generalized problem and solution class underlying the seed\nproblem. We then apply the tests as a reward signal, training LLMs to become\nbetter writers of EFAs. We show that EFAs inferred by EFAGen are faithful to\nthe seed problems, produce learnable problem variations, and that EFAGen can\ninfer EFAs across diverse sources of competition-level math problems. Finally,\nwe show uses of model-written EFAs e.g., finding harder/easier problem\nvariants, as well as data generation."}
{"id": "2505.05445", "pdf": "https://arxiv.org/pdf/2505.05445.pdf", "abs": "https://arxiv.org/abs/2505.05445", "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "comment": "31 pages", "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems."}
{"id": "2505.19073", "pdf": "https://arxiv.org/pdf/2505.19073.pdf", "abs": "https://arxiv.org/abs/2505.19073", "title": "Towards Harmonized Uncertainty Estimation for Large Language Models", "authors": ["Rui Li", "Jing Long", "Muge Qi", "Heming Xia", "Lei Sha", "Peiyi Wang", "Zhifang Sui"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "To facilitate robust and trustworthy deployment of large language models\n(LLMs), it is essential to quantify the reliability of their generations\nthrough uncertainty estimation. While recent efforts have made significant\nadvancements by leveraging the internal logic and linguistic features of LLMs\nto estimate uncertainty scores, our empirical analysis highlights the pitfalls\nof these methods to strike a harmonized estimation between indication, balance,\nand calibration, which hinders their broader capability for accurate\nuncertainty estimation. To address this challenge, we propose CUE (Corrector\nfor Uncertainty Estimation): A straightforward yet effective method that\nemploys a lightweight model trained on data aligned with the target LLM's\nperformance to adjust uncertainty scores. Comprehensive experiments across\ndiverse models and tasks demonstrate its effectiveness, which achieves\nconsistent improvements of up to 60% over existing methods."}
{"id": "2505.19116", "pdf": "https://arxiv.org/pdf/2505.19116.pdf", "abs": "https://arxiv.org/abs/2505.19116", "title": "Controlling Language Confusion in Multilingual LLMs", "authors": ["Nahyun Lee", "Yeongseo Woo", "Hyunwoo Ko", "Guijin Son"], "categories": ["cs.CL"], "comment": "4 pages", "summary": "Large language models often suffer from language confusion, a phenomenon in\nwhich responses are partially or entirely generated in unintended languages.\nThis critically degrades the user experience, especially in low-resource\nsettings. We hypothesize that this issue stems from limitations in conventional\nfine-tuning objectives, such as supervised learning, which optimize the\nlikelihood of correct tokens without explicitly penalizing undesired outputs\nsuch as cross-lingual mixing. Analysis of loss trajectories during pretraining\nfurther reveals that models fail to distinguish between monolingual and\nlanguage-mixed texts, highlighting the absence of inherent pressure to avoid\nsuch confusion. In this work, we apply ORPO, which adds penalties for unwanted\noutput styles to standard SFT, effectively suppressing language-confused\ngenerations. ORPO maintains strong language consistency, even under high\ndecoding temperatures, while preserving general QA performance. Our findings\nsuggest that incorporating appropriate penalty terms can effectively mitigate\nlanguage confusion in multilingual models, particularly in low-resource\nscenarios."}
{"id": "2506.02701", "pdf": "https://arxiv.org/pdf/2506.02701.pdf", "abs": "https://arxiv.org/abs/2506.02701", "title": "On Entity Identification in Language Models", "authors": ["Masaki Sakata", "Benjamin Heinzerling", "Sho Yokoi", "Takumi Ito", "Kentaro Inui"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings; 26 pages, 13 figures, 9 tables", "summary": "We analyze the extent to which internal representations of language models\n(LMs) identify and distinguish mentions of named entities, focusing on the\nmany-to-many correspondence between entities and their mentions. We first\nformulate two problems of entity mentions -- ambiguity and variability -- and\npropose a framework analogous to clustering quality metrics. Specifically, we\nquantify through cluster analysis of LM internal representations the extent to\nwhich mentions of the same entity cluster together and mentions of different\nentities remain separated. Our experiments examine five Transformer-based\nautoregressive models, showing that they effectively identify and distinguish\nentities with metrics analogous to precision and recall ranging from 0.66 to\n0.9. Further analysis reveals that entity-related information is compactly\nrepresented in a low-dimensional linear subspace at early LM layers.\nAdditionally, we clarify how the characteristics of entity representations\ninfluence word prediction performance. These findings are interpreted through\nthe lens of isomorphism between LM representations and entity-centric knowledge\nstructures in the real world, providing insights into how LMs internally\norganize and use entity information."}
{"id": "2506.06806", "pdf": "https://arxiv.org/pdf/2506.06806.pdf", "abs": "https://arxiv.org/abs/2506.06806", "title": "Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification", "authors": ["Subhendu Khatuya", "Shashwat Naidu", "Saptarshi Ghosh", "Pawan Goyal", "Niloy Ganguly"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This work has been accepted to appear at the Association for\n  Computational Linguistics (ACL), 2025", "summary": "The explosion of textual data has made manual document classification\nincreasingly challenging. To address this, we introduce a robust, efficient\ndomain-agnostic generative model framework for multi-label text classification.\nInstead of treating labels as mere atomic symbols, our approach utilizes\npredefined label descriptions and is trained to generate these descriptions\nbased on the input text. During inference, the generated descriptions are\nmatched to the pre-defined labels using a finetuned sentence transformer. We\nintegrate this with a dual-objective loss function, combining cross-entropy\nloss and cosine similarity of the generated sentences with the predefined\ntarget descriptions, ensuring both semantic alignment and accuracy. Our\nproposed model LAGAMC stands out for its parameter efficiency and versatility\nacross diverse datasets, making it well-suited for practical applications. We\ndemonstrate the effectiveness of our proposed model by achieving new\nstate-of-the-art performances across all evaluated datasets, surpassing several\nstrong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in\nMacro-F1 compared to the closest baseline across all datasets."}
{"id": "2506.08373", "pdf": "https://arxiv.org/pdf/2506.08373.pdf", "abs": "https://arxiv.org/abs/2506.08373", "title": "Draft-based Approximate Inference for LLMs", "authors": ["Kevin Galim", "Ethan Ewer", "Wonjun Kang", "Minjae Lee", "Hyung Il Koo", "Kangwook Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Added discussion and comparison with SpecPrefill", "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm."}
{"id": "2506.11092", "pdf": "https://arxiv.org/pdf/2506.11092.pdf", "abs": "https://arxiv.org/abs/2506.11092", "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation", "authors": ["Jubin Abhishek Soni", "Amit Anand", "Rajesh Kumar Pandey", "Aniket Abhishek Soni"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "We are withdrawing the submission in order to thoroughly revise the\n  work", "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."}
{"id": "2506.19037", "pdf": "https://arxiv.org/pdf/2506.19037.pdf", "abs": "https://arxiv.org/abs/2506.19037", "title": "Plan for Speed: Dilated Scheduling for Masked Diffusion Language Models", "authors": ["Omer Luxembourg", "Haim Permuter", "Eliya Nachmani"], "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.NE", "math.IT"], "comment": null, "summary": "Masked diffusion language models (MDLMs) promise fast, non-autoregressive\ntext generation, yet existing samplers, which pick tokens to unmask based on\nmodel confidence, ignore interactions when unmasking multiple positions in\nparallel and effectively reduce to slow, autoregressive behavior. We propose\nthe Dilated Unmasking Scheduler (DUS), an inference-only, planner-model-free\nmethod that partitions sequence positions into non-adjacent dilated groups and\nunmasked them in parallel so as to minimize an upper bound on joint entropy\ngain at each denoising step. By explicitly trading off the number of network\ncalls against generation quality, DUS recovers most of the performance lost\nunder traditional parallel unmasking strategies. Across math (GSM8K, MATH500),\ncode (HumanEval, MBPP) and general-knowledge benchmarks (BBH, MMLU-Pro), DUS\noutperforms confidence-based planners, without modifying the underlying\ndenoiser, and reveals the true speed-quality frontier of MDLMs."}
{"id": "2506.20923", "pdf": "https://arxiv.org/pdf/2506.20923.pdf", "abs": "https://arxiv.org/abs/2506.20923", "title": "KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model", "authors": ["Xinping Zhao", "Xinshuo Hu", "Zifei Shan", "Shouzheng Huang", "Yao Zhou", "Zetian Sun", "Zhenyu Liu", "Dongfang Li", "Xinyuan Wei", "Qian Chen", "Youcheng Pan", "Yang Xiang", "Meishan Zhang", "Haofen Wang", "Jun Yu", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL"], "comment": "Technical Report; 26 pages 12 tables 1 figure. arXiv admin note: text\n  overlap with arXiv:2501.01028", "summary": "In this paper, we propose KaLM-Embedding-V2, a versatile and compact\nembedding model, which achieves impressive performance in general-purpose text\nembedding tasks by leveraging superior training techniques and data. Our key\ninnovations include: (1) To better align the architecture with representation\nlearning, we remove the causal attention mask and adopt a fully bidirectional\ntransformer with simple yet effective mean-pooling to produce fixed-length\nembeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on\nlarge-scale weakly supervised open-source corpora; (ii) fine-tuning on\nhigh-quality retrieval and non-retrieval datasets; and (iii) model-soup\nparameter averaging for robust generalization. Besides, we introduce a\nfocal-style reweighting mechanism that concentrates learning on difficult\nsamples and an online hard-negative mixing strategy to continuously enrich hard\nnegatives without expensive offline mining; (3) We collect over 20 categories\nof data for pre-training and 100 categories of data for fine-tuning, to boost\nboth the performance and generalization of the embedding model. Extensive\nevaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English\nshow that our model significantly outperforms others of comparable size, and\ncompetes with 3x, 14x, 18x, and 26x larger embedding models, setting a new\nstandard for a versatile and compact embedding model with less than 1B\nparameters."}
{"id": "2507.06565", "pdf": "https://arxiv.org/pdf/2507.06565.pdf", "abs": "https://arxiv.org/abs/2507.06565", "title": "A Mathematical Theory of Discursive Networks", "authors": ["Juan B. Gutiérrez"], "categories": ["cs.CL", "cs.LG", "68T01, 60J10, 91D30, 05C82, 68T50, 68W20, 94A15", "I.2.7; I.2.11; G.3"], "comment": "39 pages, 4 figures, 4 tables, 3 algorithm, 56 references", "summary": "Large language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source Flaws-of-Others (FOO) algorithm: a configurable loop in which any\nset of agents critique one another while a harmonizer merges their verdicts. We\nidentify an ethical transgression, epithesis, that occurs when humans fail to\nengage in the discursive network. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nconnecting imperfect ones into networks that enforce mutual accountability."}
{"id": "2507.08297", "pdf": "https://arxiv.org/pdf/2507.08297.pdf", "abs": "https://arxiv.org/abs/2507.08297", "title": "KAT-V1: Kwai-AutoThink Technical Report", "authors": ["Zizheng Zhan", "Ken Deng", "Huaixi Tang", "Wen Xiang", "Kun Wu", "Weihao Li", "Wenqiang Zhu", "Jingxuan Xu", "Lecheng Huang", "Zongxian Feng", "Shaojie Wang", "Shangpeng Yan", "Xuxing Chen", "Jiaheng Liu", "Zhongyuan Peng", "Zuchen Gao", "Haoyang Huang", "Xiaojiang Zhang", "Jinghui Wang", "Zheng Lin", "Mengtong Li", "Huiming Wang", "Ziqi Zhan", "Yanan Wu", "Yuanxing Zhang", "Jian Yang", "Guang Chen", "Haotian Zhang", "Bin Chen", "Bing Yu"], "categories": ["cs.CL"], "comment": null, "summary": "We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model\ndeveloped to address the overthinking problem in reasoning-intensive tasks,\nwhere an automatic thinking training paradigm is proposed to dynamically switch\nbetween reasoning and non-reasoning modes based on task complexity.\nSpecifically, first, we construct the dual-regime dataset based on a novel\ntagging pipeline and a multi-agent synthesis strategy, and then we apply\nMulti-Token Prediction (MTP)-enhanced knowledge distillation, enabling\nefficient and fine-grained reasoning transfer with minimal pretraining cost.\nBesides, we implement a cold-start initialization strategy that introduces\nmode-selection priors using majority-vote signals and intent-aware prompting.\nFinally, we propose Step-SRPO, a reinforcement learning algorithm that\nincorporates intermediate supervision into the GRPO framework, offering\nstructured guidance over both reasoning-mode selection and response accuracy.\nExtensive experiments across multiple benchmarks demonstrate that KAT\nconsistently matches or even outperforms current state-of-the-art models,\nincluding DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of\nreasoning-intensive tasks while reducing token usage. Notably, KAT outperforms\nall open-source models and even surpasses o3-mini on the leakage-controlled\nLiveCodeBench Pro. Beyond academic evaluation, KAT has been successfully\ndeployed in Kwaipilot (i.e., Kuaishou's internal coding assistant), where it\nimproves real-world development workflows with high accuracy, efficiency, and\ncontrollable reasoning behaviors. Moreover, we are actively training a 200B\nMixture-of-Experts (MoE) model with 40B active parameters, and early results\nalready show significant gains, further demonstrating the scalability of the\nAutoThink paradigm."}
{"id": "2507.09025", "pdf": "https://arxiv.org/pdf/2507.09025.pdf", "abs": "https://arxiv.org/abs/2507.09025", "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "categories": ["cs.CL", "cs.LG"], "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."}
{"id": "2507.10524", "pdf": "https://arxiv.org/pdf/2507.10524.pdf", "abs": "https://arxiv.org/abs/2507.10524", "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation", "authors": ["Sangmin Bae", "Yujin Kim", "Reza Bayat", "Sungnyun Kim", "Jiyoun Ha", "Tal Schuster", "Adam Fisch", "Hrayr Harutyunyan", "Ziwei Ji", "Aaron Courville", "Se-Young Yun"], "categories": ["cs.CL", "cs.LG"], "comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions", "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."}
{"id": "2507.11299", "pdf": "https://arxiv.org/pdf/2507.11299.pdf", "abs": "https://arxiv.org/abs/2507.11299", "title": "Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian", "authors": ["Andrei Niculae", "Adrian Cosma", "Cosmin Dumitrache", "Emilian Rǎdoi"], "categories": ["cs.CL"], "comment": "10 figures, 2 tables, 2 listings", "summary": "Text-based telemedicine has become increasingly common, yet the quality of\nmedical advice in doctor-patient interactions is often judged more on how\nadvice is communicated rather than its clinical accuracy. To address this, we\nintroduce Dr. Copilot , a multi-agent large language model (LLM) system that\nsupports Romanian-speaking doctors by evaluating and enhancing the presentation\nquality of their written responses. Rather than assessing medical correctness,\nDr. Copilot provides feedback along 17 interpretable axes. The system comprises\nof three LLM agents with prompts automatically optimized via DSPy. Designed\nwith low-resource Romanian data and deployed using open-weight models, it\ndelivers real-time specific feedback to doctors within a telemedicine platform.\nEmpirical evaluations and live deployment with 41 doctors show measurable\nimprovements in user reviews and response quality, marking one of the first\nreal-world deployments of LLMs in Romanian medical settings."}
{"id": "2507.13334", "pdf": "https://arxiv.org/pdf/2507.13334.pdf", "abs": "https://arxiv.org/abs/2507.13334", "title": "A Survey of Context Engineering for Large Language Models", "authors": ["Lingrui Mei", "Jiayu Yao", "Yuyao Ge", "Yiwei Wang", "Baolong Bi", "Yujun Cai", "Jiazhi Liu", "Mingyu Li", "Zhong-Zhi Li", "Duzhen Zhang", "Chenlin Zhou", "Jiayi Mao", "Tianze Xia", "Jiafeng Guo", "Shenghua Liu"], "categories": ["cs.CL"], "comment": "ongoing work; 166 pages, 1411 citations", "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1400 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI."}
{"id": "2211.05403", "pdf": "https://arxiv.org/pdf/2211.05403.pdf", "abs": "https://arxiv.org/abs/2211.05403", "title": "Enabling Efficient Attack Investigation via Human-in-the-Loop Security Analysis", "authors": ["Saimon Amanuel Tsegai", "Xinyu Yang", "Haoyuan Liu", "Peng Gao"], "categories": ["cs.CR", "cs.CL", "cs.DB"], "comment": "Accepted at 51st International Conference on Very Large Data Bases\n  (VLDB) 2025", "summary": "System auditing is a vital technique for collecting system call events as\nsystem provenance and investigating complex multi-step attacks such as Advanced\nPersistent Threats. However, existing attack investigation methods struggle to\nuncover long attack sequences due to the massive volume of system provenance\ndata and their inability to focus on attack-relevant parts. In this paper, we\npresent Provexa, a defense system that enables human analysts to effectively\nanalyze large-scale system provenance to reveal multi-step attack sequences.\nProvexa introduces an expressive domain-specific language, ProvQL, that offers\nessential primitives for various types of attack analyses (e.g., attack pattern\nsearch, attack dependency tracking) with user-defined constraints, enabling\nanalysts to focus on attack-relevant parts and iteratively sift through the\nlarge provenance data. Moreover, Provexa provides an optimized execution engine\nfor efficient language execution. Our extensive evaluations on a wide range of\nattack scenarios demonstrate the practical effectiveness of Provexa in\nfacilitating timely attack investigation."}
{"id": "2402.04161", "pdf": "https://arxiv.org/pdf/2402.04161.pdf", "abs": "https://arxiv.org/abs/2402.04161", "title": "Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains", "authors": ["Ashok Vardhan Makkuva", "Marco Bondaschi", "Adway Girish", "Alliot Nagle", "Martin Jaggi", "Hyeji Kim", "Michael Gastpar"], "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT", "stat.ML"], "comment": "Published at ICLR 2025 under the title \"Attention with Markov: A\n  Curious Case of Single-Layer Transformers\"", "summary": "Attention-based transformers have achieved tremendous success across a\nvariety of disciplines including natural languages. To deepen our understanding\nof their sequential modeling capabilities, there is a growing interest in using\nMarkov input processes to study them. A key finding is that when trained on\nfirst-order Markov chains, transformers with two or more layers consistently\ndevelop an induction head mechanism to estimate the in-context bigram\nconditional distribution. In contrast, single-layer transformers, unable to\nform an induction head, directly learn the Markov kernel but often face a\nsurprising challenge: they become trapped in local minima representing the\nunigram distribution, whereas deeper models reliably converge to the\nground-truth bigram. While single-layer transformers can theoretically model\nfirst-order Markov chains, their empirical failure to learn this simple kernel\nin practice remains a curious phenomenon. To explain this contrasting behavior\nof single-layer models, in this paper we introduce a new framework for a\nprincipled analysis of transformers via Markov chains. Leveraging our\nframework, we theoretically characterize the loss landscape of single-layer\ntransformers and show the existence of global minima (bigram) and bad local\nminima (unigram) contingent on data properties and model architecture. We\nprecisely delineate the regimes under which these local optima occur. Backed by\nexperiments, we demonstrate that our theoretical findings are in congruence\nwith the empirical results. Finally, we outline several open problems in this\narena. Code is available at https://github.com/Bond1995/Markov ."}
{"id": "2409.06211", "pdf": "https://arxiv.org/pdf/2409.06211.pdf", "abs": "https://arxiv.org/abs/2409.06211", "title": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning", "authors": ["Jaeseong Lee", "seung-won hwang", "Aurick Qiao", "Daniel F Campos", "Zhewei Yao", "Yuxiong He"], "categories": ["cs.LG", "cs.CL"], "comment": "ACL 2025 main", "summary": "Mixture-of-experts (MoEs) have been adopted for reducing inference costs by\nsparsely activating experts in Large language models (LLMs). Despite this\nreduction, the massive number of experts in MoEs still makes them expensive to\nserve. In this paper, we study how to address this, by pruning MoEs. Among\npruning methodologies, unstructured pruning has been known to achieve the\nhighest performance for a given pruning ratio, compared to structured pruning,\nsince the latter imposes constraints on the sparsification structure. This is\nintuitive, as the solution space of unstructured pruning subsumes that of\nstructured pruning. However, our counterintuitive finding reveals that expert\npruning, a form of structured pruning, can actually precede unstructured\npruning to outperform unstructured-only pruning. As existing expert pruning,\nrequiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot\nscale for recent MoEs, we propose a scalable alternative with $O(1)$\ncomplexity, yet outperforming the more expensive methods. The key idea is\nleveraging a latent structure between experts, based on behavior similarity,\nsuch that the greedy decision of whether to prune closely captures the joint\npruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized\nMoE with 128 experts, our method needs only one H100 and two hours to achieve\nnearly no loss in performance with 40% sparsity, even in generative tasks such\nas GSM8K, where state-of-the-art unstructured pruning fails to. The code will\nbe made publicly available."}
{"id": "2409.06656", "pdf": "https://arxiv.org/pdf/2409.06656.pdf", "abs": "https://arxiv.org/abs/2409.06656", "title": "Sortformer: A Novel Approach for Permutation-Resolved Speaker Supervision in Speech-to-Text Systems", "authors": ["Taejin Park", "Ivan Medennikov", "Kunal Dhawan", "Weiqing Wang", "He Huang", "Nithin Rao Koluguri", "Krishna C. Puvvada", "Jagadeesh Balam", "Boris Ginsburg"], "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Published at ICML 2025", "summary": "Sortformer is an encoder-based speaker diarization model designed for\nsupervising speaker tagging in speech-to-text models. Instead of relying solely\non permutation invariant loss (PIL), Sortformer introduces Sort Loss to resolve\nthe permutation problem, either independently or in tandem with PIL. In\naddition, we propose a streamlined multi-speaker speech-to-text architecture\nthat leverages Sortformer for speaker supervision, embedding speaker labels\ninto the encoder using sinusoidal kernel functions. This design addresses the\nspeaker permutation problem through sorted objectives, effectively bridging\ntimestamps and tokens to supervise speaker labels in the output transcriptions.\nExperiments demonstrate that Sort Loss can boost speaker diarization\nperformance, and incorporating the speaker supervision from Sortformer improves\nmulti-speaker transcription accuracy. We anticipate that the proposed\nSortformer and multi-speaker architecture will enable the seamless integration\nof speaker tagging capabilities into foundational speech-to-text systems and\nmultimodal large language models (LLMs), offering an easily adoptable and\nuser-friendly mechanism to enhance their versatility and performance in\nspeaker-aware tasks. The code and trained models are made publicly available\nthrough the NVIDIA NeMo Framework."}
{"id": "2410.10148", "pdf": "https://arxiv.org/pdf/2410.10148.pdf", "abs": "https://arxiv.org/abs/2410.10148", "title": "AlphaDPO: Adaptive Reward Margin for Direct Preference Optimization", "authors": ["Junkang Wu", "Xue Wang", "Zhengyi Yang", "Jiancan Wu", "Jinyang Gao", "Bolin Ding", "Xiang Wang", "Xiangnan He"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human values and intentions is\ncrucial for their utility, honesty, and safety. Reinforcement learning from\nhuman feedback (RLHF) is a popular approach to achieve this alignment, but it\nfaces challenges in computational efficiency and training stability. Recent\nmethods like Direct Preference Optimization (DPO) and Simple Preference\nOptimization (SimPO) have proposed offline alternatives to RLHF, simplifying\nthe process by reparameterizing the reward function. However, DPO depends on a\npotentially suboptimal reference model, and SimPO's assumption of a fixed\ntarget reward margin may lead to suboptimal decisions in diverse data settings.\nIn this work, we propose $\\alpha$-DPO, an adaptive preference optimization\nalgorithm designed to address these limitations by introducing a dynamic reward\nmargin. Specifically, $\\alpha$-DPO employs an adaptive preference distribution,\nbalancing the policy model and the reference model to achieve personalized\nreward margins. We provide theoretical guarantees for $\\alpha$-DPO,\ndemonstrating its effectiveness as a surrogate optimization objective and its\nability to balance alignment and diversity through KL divergence control.\nEmpirical evaluations on AlpacaEval 2 and Arena-Hard show that $\\alpha$-DPO\nconsistently outperforms DPO and SimPO across various model settings,\nestablishing it as a robust approach for fine-tuning LLMs. Our method achieves\nsignificant improvements in win rates, highlighting its potential as a powerful\ntool for LLM alignment. The code is available at\nhttps://github.com/junkangwu/alpha-DPO"}
{"id": "2501.03040", "pdf": "https://arxiv.org/pdf/2501.03040.pdf", "abs": "https://arxiv.org/abs/2501.03040", "title": "ChronoSense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events", "authors": ["Duygu Sezen Islakoglu", "Jan-Christoph Kalo"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ACL 2025. Results on a larger test set. 13 pages, 2\n  figures", "summary": "Large Language Models (LLMs) have achieved remarkable success in various NLP\ntasks, yet they still face significant challenges in reasoning and arithmetic.\nTemporal reasoning, a critical component of natural language understanding, has\nraised increasing research attention. However, comprehensive testing of Allen's\ninterval relations (e.g., before, after, during) -- a fundamental framework for\ntemporal relationships -- remains underexplored. To fill this gap, we present\nChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It\nincludes 16 tasks, focusing on identifying the Allen relation between two\ntemporal events and temporal arithmetic, using both abstract events and\nreal-world data from Wikidata. We assess the performance of seven recent LLMs\nusing this benchmark and the results indicate that models handle Allen\nrelations, even symmetrical ones, quite differently. Moreover, the findings\nsuggest that the models may rely on memorization to answer time-related\nquestions. Overall, the models' low performance highlights the need for\nimproved temporal understanding in LLMs and ChronoSense offers a robust\nframework for future research in this area. Our dataset and the source code are\navailable at https://github.com/duyguislakoglu/chronosense."}
{"id": "2501.10062", "pdf": "https://arxiv.org/pdf/2501.10062.pdf", "abs": "https://arxiv.org/abs/2501.10062", "title": "OMoE: Diversifying Mixture of Low-Rank Adaptation by Orthogonal Finetuning", "authors": ["Jinyuan Feng", "Zhiqiang Pu", "Tianyi Hu", "Dongmin Li", "Xiaolin Ai", "Huimu Wang"], "categories": ["cs.LG", "cs.CL"], "comment": "This paper is accepted by ECAI 2025", "summary": "Building mixture-of-experts (MoE) architecture for Low-rank adaptation (LoRA)\nis emerging as a potential direction in parameter-efficient fine-tuning (PEFT)\nfor its modular design and remarkable performance. However, simply stacking the\nnumber of experts cannot guarantee significant improvement. In this work, we\nfirst conduct qualitative analysis to indicate that experts collapse to similar\nrepresentations in vanilla MoE, limiting the capacity of modular design and\ncomputational efficiency. Ulteriorly, Our analysis reveals that the performance\nof previous MoE variants maybe limited by a lack of diversity among experts.\nMotivated by these findings, we propose Orthogonal Mixture-of-Experts (OMoE), a\nresource-efficient MoE variant that trains experts in an orthogonal manner to\npromote diversity. In OMoE, a Gram-Schmidt process is leveraged to enforce that\nthe experts' representations lie within the Stiefel manifold. By applying\northogonal constraints directly to the architecture, OMoE keeps the learning\nobjective unchanged, without compromising optimality. Our method is simple and\nalleviates memory bottlenecks, as it incurs minimal experts compared to vanilla\nMoE models. Experiments on diverse commonsense reasoning benchmarks demonstrate\nthat OMoE can consistently achieve stable and efficient performance improvement\nwhen compared with the state-of-the-art methods while significantly reducing\nthe number of required experts."}
{"id": "2502.00409", "pdf": "https://arxiv.org/pdf/2502.00409.pdf", "abs": "https://arxiv.org/abs/2502.00409", "title": "Doing More with Less: A Survey on Routing Strategies for Resource Optimisation in Large Language Model-Based Systems", "authors": ["Clovis Varangot-Reille", "Christophe Bouvard", "Antoine Gourru", "Mathieu Ciancone", "Marion Schaeffer", "François Jacquenet"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Model (LLM)-based systems, i.e. interconnected elements that\ninclude an LLM as a central component, such as conversational agents, are\nusually designed with monolithic, static architectures that rely on a single,\ngeneral-purpose LLM to handle all user queries. However, these systems may be\ninefficient as different queries may require different levels of reasoning,\ndomain knowledge or pre-processing. While generalist LLMs (e.g. GPT-4o,\nClaude-Sonnet) perform well across a wide range of tasks, they may incur\nsignificant financial, energy and computational costs. These costs may be\ndisproportionate for simpler queries, resulting in unnecessary resource\nutilisation. A routing mechanism can therefore be employed to route queries to\nmore appropriate components, such as smaller or specialised models, thereby\nimproving efficiency and optimising resource consumption. This survey aims to\nprovide a comprehensive overview of routing strategies in LLM-based systems.\nSpecifically, it reviews when, why, and how routing should be integrated into\nLLM pipelines to improve efficiency, scalability, and performance. We define\nthe objectives to optimise, such as cost minimisation and performance\nmaximisation, and discuss the timing of routing within the LLM workflow,\nwhether it occurs before or after generation. We also detail the various\nimplementation strategies, including similarity-based, supervised,\nreinforcement learning-based, and generative methods. Practical considerations\nsuch as industrial applications and current limitations are also examined, like\nstandardising routing experiments, accounting for non-financial costs, and\ndesigning adaptive strategies. By formalising routing as a performance-cost\noptimisation problem, this survey provides tools and directions to guide future\nresearch and development of adaptive low-cost LLM-based systems."}
{"id": "2502.15652", "pdf": "https://arxiv.org/pdf/2502.15652.pdf", "abs": "https://arxiv.org/abs/2502.15652", "title": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey", "authors": ["Fengxiang Cheng", "Haoxuan Li", "Fenrong Liu", "Robert van Rooij", "Kun Zhang", "Zhouchen Lin"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by IJCAI 2025 (Survey Track)", "summary": "Large language models (LLMs) have achieved remarkable successes on various\ntasks. However, recent studies have found that there are still significant\nchallenges to the logical reasoning abilities of LLMs, which can be categorized\ninto the following two aspects: (1) Logical question answering: LLMs often fail\nto generate the correct answer within a complex logical problem which requires\nsophisticated deductive, inductive or abductive reasoning given a collection of\npremises. (2) Logical consistency: LLMs are prone to producing responses\ncontradicting themselves across different questions. For example, a\nstate-of-the-art question-answering LLM Macaw, answers Yes to both questions Is\na magpie a bird? and Does a bird have wings? but answers No to Does a magpie\nhave wings?. To facilitate this research direction, we comprehensively\ninvestigate the most cutting-edge methods and propose a detailed taxonomy.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, and\nfine-tuning. To avoid logical contradictions, we discuss concepts and solutions\nof various logical consistencies, including implication, negation,\ntransitivity, factuality consistencies, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extending to modal logic to account for\nuncertainty and developing efficient algorithms that simultaneously satisfy\nmultiple logical consistencies."}
{"id": "2503.10968", "pdf": "https://arxiv.org/pdf/2503.10968.pdf", "abs": "https://arxiv.org/abs/2503.10968", "title": "Combinatorial Optimization for All: Using LLMs to Aid Non-Experts in Improving Optimization Algorithms", "authors": ["Camilo Chacón Sartori", "Christian Blum"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have shown notable potential in code generation\nfor optimization algorithms, unlocking exciting new opportunities. This paper\nexamines how LLMs, rather than creating algorithms from scratch, can improve\nexisting ones without the need for specialized expertise. To explore this\npotential, we selected 10 baseline optimization algorithms from various domains\n(metaheuristics, reinforcement learning, deterministic, and exact methods) to\nsolve the classic Travelling Salesman Problem. The results show that our simple\nmethodology often results in LLM-generated algorithm variants that improve over\nthe baseline algorithms in terms of solution quality, reduction in\ncomputational time, and simplification of code complexity, all without\nrequiring specialized optimization knowledge or advanced algorithmic\nimplementation skills."}
{"id": "2503.12899", "pdf": "https://arxiv.org/pdf/2503.12899.pdf", "abs": "https://arxiv.org/abs/2503.12899", "title": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on Code Generation", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "13 pages, 7 figure, 8 tables, under peer-review", "summary": "Language Models (LMs) are widely used in software engineering for code\ngeneration, but they may produce code with errors. Rather than repairing the\ngenerated code, an alternative way is to address the underlying failures of\nmodels. LM repair offers a lightweight solution to this challenge: it requires\nminimal data, reduces computational costs, and reduces the side effects. Unlike\nretraining, LM repair focuses on applying tailored updates to targeted neurons,\nmaking it ideal for scenarios with limited resources, high-performance demands,\nor strict safety requirements. In this paper, we propose Semantic Targeting for\nAnalytical Repair (STAR), a pioneering and novel semantic-based optimization\napproach for repairing LLMs. STAR realizes the main operations of repairing LMs\nin an optimization process, including locating ``buggy neurons'', solving\n``neuron patches'', and patching ``buggy neurons''. Correspondingly, it\ncomputes the deltas of weight matrix as the prior information to guide\noptimization; and attributes the targeted layers and neurons leveraging\nstatistical insights. The neuron patches are computed with a solid\nsemantic-based analytical formula, which directly bridges the changes to logits\nwith the deltas of neurons, by steering latent representations. Compared to the\nprior work of LM repair (MINT) and optimization methods (SGD), STAR integrates\ntheir strengths while mitigating their limitations. STAR supports solving\nmultiple failures together, significantly improving the usefulness. Evaluated\non coding tasks using popular code LMs, STAR exhibits superior effectiveness\n(10.5%-19.9% improvements) and efficiency (2.4-7.0 times speedup). In terms of\nside effects, namely the balance between generalization and specificity, STAR\noutperforms prior work by a significant margin. Additionally, we conducted\nassessments on the overfitting risk of LM repair as well as the cumulative\nimpact."}
{"id": "2503.14075", "pdf": "https://arxiv.org/pdf/2503.14075.pdf", "abs": "https://arxiv.org/abs/2503.14075", "title": "Growing a Twig to Accelerate Large Vision-Language Models", "authors": ["Zhenwei Shao", "Mingyang Wang", "Zhou Yu", "Wenwen Pan", "Yan Yang", "Tao Wei", "Hongyuan Zhang", "Ning Mao", "Wei Chen", "Jun Yu"], "categories": ["cs.CV", "cs.CL"], "comment": "accepted at ICCV 2025", "summary": "Large vision-language models (VLMs) have demonstrated remarkable capabilities\nin open-world multimodal understanding, yet their high computational overheads\npose great challenges for practical deployment. Some recent works have proposed\nmethods to accelerate VLMs by pruning redundant visual tokens guided by the\nattention maps of VLM's early layers. Despite the success of these token\npruning methods, they still suffer from two major shortcomings: (i)\nconsiderable accuracy drop due to insensitive attention signals in early\nlayers, and (ii) limited speedup when generating long responses (e.g., 30\ntokens). To address the limitations above, we present TwigVLM -- a simple and\ngeneral architecture by growing a lightweight twig upon an early layer of the\nbase VLM. Compared with most existing VLM acceleration methods purely based on\nvisual token pruning, our TwigVLM not only achieves better accuracy retention\nby employing a twig-guided token pruning (TTP) strategy, but also yields higher\ngeneration speed by utilizing a self-speculative decoding (SSD) strategy.\nTaking LLaVA-1.5-7B as the base VLM, experimental results show that TwigVLM\npreserves 96% of the original performance after pruning 88.9% of visual tokens\nand achieves 154% speedup in generating long responses, delivering\nsignificantly better performance in terms of both accuracy and speed over the\nstate-of-the-art VLM acceleration methods."}
{"id": "2503.16148", "pdf": "https://arxiv.org/pdf/2503.16148.pdf", "abs": "https://arxiv.org/abs/2503.16148", "title": "Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models", "authors": ["Mats Faulborn", "Indira Sen", "Max Pellert", "Andreas Spitz", "David Garcia"], "categories": ["cs.CY", "cs.CL"], "comment": "Preprint of ACL 2025 paper", "summary": "Prompt-based language models like GPT4 and LLaMa have been used for a wide\nvariety of use cases such as simulating agents, searching for information, or\nfor content analysis. For all of these applications and others, political\nbiases in these models can affect their performance. Several researchers have\nattempted to study political bias in language models using evaluation suites\nbased on surveys, such as the Political Compass Test (PCT), often finding a\nparticular leaning favored by these models. However, there is some variation in\nthe exact prompting techniques, leading to diverging findings, and most\nresearch relies on constrained-answer settings to extract model responses.\nMoreover, the Political Compass Test is not a scientifically valid survey\ninstrument. In this work, we contribute a political bias measured informed by\npolitical science theory, building on survey design principles to test a wide\nvariety of input prompts, while taking into account prompt sensitivity. We then\nprompt 11 different open and commercial models, differentiating between\ninstruction-tuned and non-instruction-tuned models, and automatically classify\ntheir political stances from 88,110 responses. Leveraging this dataset, we\ncompute political bias profiles across different prompt variations and find\nthat while PCT exaggerates bias in certain models like GPT3.5, measures of\npolitical bias are often unstable, but generally more left-leaning for\ninstruction-tuned models. Code and data are available on:\nhttps://github.com/MaFa211/theory_grounded_pol_bias"}
{"id": "2505.08622", "pdf": "https://arxiv.org/pdf/2505.08622.pdf", "abs": "https://arxiv.org/abs/2505.08622", "title": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models", "authors": ["Donghoon Kim", "Minji Bae", "Kyuhong Shim", "Byonghyo Shim"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "ICLR 2025 (Official Code: https://github.com/DonghoonKim-1938/VGD)", "summary": "Text-to-image generative models like DALL-E and Stable Diffusion have\nrevolutionized visual content creation across various applications, including\nadvertising, personalized media, and design prototyping. However, crafting\neffective textual prompts to guide these models remains challenging, often\nrequiring extensive trial and error. Existing prompt inversion approaches, such\nas soft and hard prompt techniques, are not so effective due to the limited\ninterpretability and incoherent prompt generation. To address these issues, we\npropose Visually Guided Decoding (VGD), a gradient-free approach that leverages\nlarge language models (LLMs) and CLIP-based guidance to generate coherent and\nsemantically aligned prompts. In essence, VGD utilizes the robust text\ngeneration capabilities of LLMs to produce human-readable prompts. Further, by\nemploying CLIP scores to ensure alignment with user-specified visual concepts,\nVGD enhances the interpretability, generalization, and flexibility of prompt\ngeneration without the need for additional training. Our experiments\ndemonstrate that VGD outperforms existing prompt inversion techniques in\ngenerating understandable and contextually relevant prompts, facilitating more\nintuitive and controllable interactions with text-to-image models."}
{"id": "2505.12891", "pdf": "https://arxiv.org/pdf/2505.12891.pdf", "abs": "https://arxiv.org/abs/2505.12891", "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "categories": ["cs.AI", "cs.CL"], "comment": "Second version", "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME ."}
{"id": "2506.11475", "pdf": "https://arxiv.org/pdf/2506.11475.pdf", "abs": "https://arxiv.org/abs/2506.11475", "title": "AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction", "authors": ["Syeda Kisaa Fatima", "Tehreem Zubair", "Noman Ahmed", "Asifullah Khan"], "categories": ["cs.MA", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper introduces LUCID-MA (Learning and Understanding Crime through\nDialogue of Multiple Agents), an innovative AI powered framework where multiple\nAI agents collaboratively analyze and understand crime data. Our system that\nconsists of three core components: an analysis assistant that highlights\nspatiotemporal crime patterns; a feedback component that reviews and refines\nanalytical results; and a prediction component that forecasts future crime\ntrends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it\nruns completely offline and allows the agents undergo self-improvement through\n100 rounds of communication with less human interaction. A scoring function is\nincorporated to evaluate agent performance, providing visual plots to track\nlearning progress. This work demonstrates the potential of AutoGen-style agents\nfor autonomous, scalable, and iterative analysis in social science domains,\nmaintaining data privacy through offline execution. It also showcases a\ncomputational model with emergent intelligence, where the system's global\nbehavior emerges from the interactions of its agents. This emergent behavior\nmanifests as enhanced individual agent performance, driven by collaborative\ndialogue between the LLM-based agents."}
{"id": "2506.11558", "pdf": "https://arxiv.org/pdf/2506.11558.pdf", "abs": "https://arxiv.org/abs/2506.11558", "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs", "authors": ["Bo-Cheng Chiu", "Jen-Jee Chen", "Yu-Chee Tseng", "Feng-Chi Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with LLM-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling."}
{"id": "2507.05108", "pdf": "https://arxiv.org/pdf/2507.05108.pdf", "abs": "https://arxiv.org/abs/2507.05108", "title": "Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration", "authors": ["Yuyi Zhang", "Peirong Zhang", "Zhenhua Yang", "Pengyu Yan", "Yongxin Shi", "Pengwei Liu", "Fengjun Guo", "Lianwen Jin"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Historical documents represent an invaluable cultural heritage, yet have\nundergone significant degradation over time through tears, water erosion, and\noxidation. Existing Historical Document Restoration (HDR) methods primarily\nfocus on single modality or limited-size restoration, failing to meet practical\nneeds. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel\nautomated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and\n6,543 synthetic images with character-level and line-level locations, as well\nas character annotations in different damage grades. AutoHDR mimics historians'\nrestoration workflows through a three-stage approach: OCR-assisted damage\nlocalization, vision-language context text prediction, and patch autoregressive\nappearance restoration. The modular architecture of AutoHDR enables seamless\nhuman-machine collaboration, allowing for flexible intervention and\noptimization at each restoration stage. Experiments demonstrate AutoHDR's\nremarkable performance in HDR. When processing severely damaged documents, our\nmethod improves OCR accuracy from 46.83% to 84.05%, with further enhancement to\n94.25% through human-machine collaboration. We believe this work represents a\nsignificant advancement in automated historical document restoration and\ncontributes substantially to cultural heritage preservation. The model and\ndataset are available at https://github.com/SCUT-DLVCLab/AutoHDR."}
{"id": "2507.10571", "pdf": "https://arxiv.org/pdf/2507.10571.pdf", "abs": "https://arxiv.org/abs/2507.10571", "title": "Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning", "authors": ["Konstantinos I. Roumeliotis", "Ranjan Sapkota", "Manoj Karkee", "Nikolaos D. Tselikas"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern Artificial Intelligence (AI) increasingly relies on multi-agent\narchitectures that blend visual and language understanding. Yet, a pressing\nchallenge remains: How can we trust these agents especially in zero-shot\nsettings with no fine-tuning? We introduce a novel modular Agentic AI visual\nclassification framework that integrates generalist multimodal agents with a\nnon-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)\nmodule. Applied to apple leaf disease diagnosis, we benchmark three\nconfigurations: (I) zero-shot with confidence-based orchestration, (II)\nfine-tuned agents with improved performance, and (III) trust-calibrated\norchestration enhanced by CLIP-based image retrieval and re-evaluation loops.\nUsing confidence calibration metrics (ECE, OCR, CCC), the orchestrator\nmodulates trust across agents. Our results demonstrate a 77.94\\% accuracy\nimprovement in the zero-shot setting using trust-aware orchestration and RAG,\nachieving 85.63\\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL\ndisplayed overconfidence. Furthermore, image-RAG grounded predictions with\nvisually similar cases, enabling correction of agent overconfidence via\niterative re-evaluation. The proposed system separates perception (vision\nagents) from meta-reasoning (orchestrator), enabling scalable and interpretable\nmulti-agent AI. This blueprint is extensible to diagnostics, biology, and other\ntrust-critical domains. All models, prompts, results, and system components\nincluding the complete software source code are openly released to support\nreproducibility, transparency, and community benchmarking at Github:\nhttps://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust"}
{"id": "2507.10827", "pdf": "https://arxiv.org/pdf/2507.10827.pdf", "abs": "https://arxiv.org/abs/2507.10827", "title": "Supporting SENCOTEN Language Documentation Efforts with Automatic Speech Recognition", "authors": ["Mengzhe Geng", "Patrick Littell", "Aidan Pine", "PENÁĆ", "Marc Tessier", "Roland Kuhn"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted by ComputEL-8", "summary": "The SENCOTEN language, spoken on the Saanich peninsula of southern Vancouver\nIsland, is in the midst of vigorous language revitalization efforts to turn the\ntide of language loss as a result of colonial language policies. To support\nthese on-the-ground efforts, the community is turning to digital technology.\nAutomatic Speech Recognition (ASR) technology holds great promise for\naccelerating language documentation and the creation of educational resources.\nHowever, developing ASR systems for SENCOTEN is challenging due to limited data\nand significant vocabulary variation from its polysynthetic structure and\nstress-driven metathesis. To address these challenges, we propose an ASR-driven\ndocumentation pipeline that leverages augmented speech data from a\ntext-to-speech (TTS) system and cross-lingual transfer learning with Speech\nFoundation Models (SFMs). An n-gram language model is also incorporated via\nshallow fusion or n-best restoring to maximize the use of available data.\nExperiments on the SENCOTEN dataset show a word error rate (WER) of 19.34% and\na character error rate (CER) of 5.09% on the test set with a 57.02%\nout-of-vocabulary (OOV) rate. After filtering minor cedilla-related errors, WER\nimproves to 14.32% (26.48% on unseen words) and CER to 3.45%, demonstrating the\npotential of our ASR-driven pipeline to support SENCOTEN language\ndocumentation."}
{"id": "2507.10880", "pdf": "https://arxiv.org/pdf/2507.10880.pdf", "abs": "https://arxiv.org/abs/2507.10880", "title": "Domain-Adaptive Small Language Models for Structured Tax Code Prediction", "authors": ["Souvik Nath", "Sumit Wadhwa", "Luis Perez"], "categories": ["cs.LG", "cs.CL"], "comment": "10 pages, 3 figures", "summary": "Every day, multinational firms process thousands of transactions, each of\nwhich must adhere to tax regulations that vary by jurisdiction and are often\nnuanced. The determination of product and service tax codes, such as HSN or SAC\nis a major use case in Tax compliance. An accurate determination of such codes\nis imperative to avoid any tax penalties. This paper proposes a domain-adaptive\nsmall language model (SLM) with an encoder-decoder architecture for the\nenhanced prediction of product and service tax codes. In this approach, we\naddress the problem of predicting hierarchical tax code sequences using\nunstructured product and services data. We employ an SLM based upon\nencoder-decoder architecture as this enables sequential generation of tax codes\nto capture the hierarchical dependencies present within the tax codes. Our\nexperiments demonstrate that encoder-decoder SLMs can be successfully applied\nto the sequential prediction of structured tax codes, a domain that remains\ncomparatively unexplored in current NLP research. In this paper, we demonstrate\nthe superior performance of the domain-adaptive encoder-decoder SLMs over flat\nclassifiers when applied to the Harmonized System of Nomenclature (HSN), and\nachieve superior results compared to decoder-only and encoder-only\narchitectures for structured sequence generation tasks. This approach can also\nbe scaled to other government-mandated tax commodity codes, such as United\nNations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura\nComum do Mercosul (NCM)."}
