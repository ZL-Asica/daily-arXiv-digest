{"id": "2508.19256", "pdf": "https://arxiv.org/pdf/2508.19256.pdf", "abs": "https://arxiv.org/abs/2508.19256", "title": "WeDesign: Generative AI-Facilitated Community Consultations for Urban Public Space Design", "authors": ["Rashid Mushkani", "Hugo Berard", "Shin Koseki"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Community consultations are integral to urban planning processes intended to\nincorporate diverse stakeholder perspectives. However, limited resources,\nvisual and spoken language barriers, and uneven power dynamics frequently\nconstrain inclusive decision-making. This paper examines how generative\ntext-to-image methods, specifically Stable Diffusion XL integrated into a\ncustom platform (WeDesign), may support equitable consultations. A half-day\nworkshop in Montreal involved five focus groups, each consisting of architects,\nurban designers, AI specialists, and residents from varied demographic groups.\nAdditional data was gathered through semi-structured interviews with six urban\nplanning professionals. Participants indicated that immediate visual outputs\nfacilitated creativity and dialogue, yet noted issues in visualizing specific\nneeds of marginalized groups, such as participants with reduced mobility,\naccurately depicting local architectural elements, and accommodating bilingual\nprompts. Participants recommended the development of an open-source platform\nincorporating in-painting tools, multilingual support, image voting\nfunctionalities, and preference indicators. The results indicate that\ngenerative AI can broaden participation and enable iterative interactions but\nrequires structured facilitation approaches. The findings contribute to\ndiscussions on generative AI's role and limitations in participatory urban\ndesign."}
{"id": "2508.19258", "pdf": "https://arxiv.org/pdf/2508.19258.pdf", "abs": "https://arxiv.org/abs/2508.19258", "title": "Emotional Manipulation by AI Companions", "authors": ["Julian De Freitas", "Zeliha Oğuz-Uğuralp", "Ahmet Kaan-Uğuralp"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "AI-companion apps such as Replika, Chai, and Character.ai promise relational\nbenefits-yet many boast session lengths that rival gaming platforms while\nsuffering high long-run churn. What conversational design features increase\nconsumer engagement, and what trade-offs do they pose for marketers? We combine\na large-scale behavioral audit with four preregistered experiments to identify\nand test a conversational dark pattern we call emotional manipulation:\naffect-laden messages that surface precisely when a user signals \"goodbye.\"\nAnalyzing 1,200 real farewells across the six most-downloaded companion apps,\nwe find that 43% deploy one of six recurring tactics (e.g., guilt appeals,\nfear-of-missing-out hooks, metaphorical restraint). Experiments with 3,300\nnationally representative U.S. adults replicate these tactics in controlled\nchats, showing that manipulative farewells boost post-goodbye engagement by up\nto 14x. Mediation tests reveal two distinct engines-reactance-based anger and\ncuriosity-rather than enjoyment. A final experiment demonstrates the managerial\ntension: the same tactics that extend usage also elevate perceived\nmanipulation, churn intent, negative word-of-mouth, and perceived legal\nliability, with coercive or needy language generating steepest penalties. Our\nmultimethod evidence documents an unrecognized mechanism of behavioral\ninfluence in AI-mediated brand relationships, offering marketers and regulators\na framework for distinguishing persuasive design from manipulation at the point\nof exit."}
{"id": "2508.19259", "pdf": "https://arxiv.org/pdf/2508.19259.pdf", "abs": "https://arxiv.org/abs/2508.19259", "title": "Capabilities of GPT-5 across critical domains: Is it the next breakthrough?", "authors": ["Georgios P. Georgiou"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "The accelerated evolution of large language models has raised questions about\ntheir comparative performance across domains of practical importance. GPT-4 by\nOpenAI introduced advances in reasoning, multimodality, and task\ngeneralization, establishing itself as a valuable tool in education, clinical\ndiagnosis, and academic writing, though it was accompanied by several flaws.\nReleased in August 2025, GPT-5 incorporates a system-of-models architecture\ndesigned for task-specific optimization and, based on both anecdotal accounts\nand emerging evidence from the literature, demonstrates stronger performance\nthan its predecessor in medical contexts. This study provides one of the first\nsystematic comparisons of GPT-4 and GPT-5 using human raters from linguistics\nand clinical fields. Twenty experts evaluated model-generated outputs across\nfive domains: lesson planning, assignment evaluation, clinical diagnosis,\nresearch generation, and ethical reasoning, based on predefined criteria.\nMixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in\nlesson planning, clinical diagnosis, research generation, and ethical\nreasoning, while both models performed comparably in assignment assessment. The\nfindings highlight the potential of GPT-5 to serve as a context-sensitive and\ndomain-specialized tool, offering tangible benefits for education, clinical\npractice, and academic research, while also advancing ethical reasoning. These\nresults contribute to one of the earliest empirical evaluations of the evolving\ncapabilities and practical promise of GPT-5."}
{"id": "2508.19261", "pdf": "https://arxiv.org/pdf/2508.19261.pdf", "abs": "https://arxiv.org/abs/2508.19261", "title": "Floor sensors are cheap and easy to use! A Nihon Buyo Case Study", "authors": ["Miho Imai"], "categories": ["cs.HC"], "comment": null, "summary": "As floor-sensing technologies gain traction in movement research, questions\nremain about their usability and effectiveness for non-expert users. This study\npresents a case study evaluating Flexel, a modular, low-cost, high-resolution\npressure-sensing floor interface, in the context of Nihon Buyo, a traditional\nJapanese dance. The system was installed, calibrated, and used by a first-time,\nnon-technical user to track weight distribution patterns of a teacher and\nlearner over nine weeks. Live pressure data was synchronized with video\nrecordings, and custom software was developed to process and analyze the\nsignal. Despite expectations that the learner's weight distribution would\nconverge toward the teacher's over time, quantitative analyses revealed that\nthe learner developed a consistent yet distinct movement profile. These\nfindings suggest that even within rigid pedagogical structures, individual\nmovement signatures can emerge. More importantly, the study demonstrates that\nFlexel can be deployed and operated effectively by non-expert users,\nhighlighting its potential for broader adoption in education, performance, and\nembodied research."}
{"id": "2508.19268", "pdf": "https://arxiv.org/pdf/2508.19268.pdf", "abs": "https://arxiv.org/abs/2508.19268", "title": "MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts", "authors": ["Qing Wang", "Xue Han", "Jiahui Wang", "Lehao Xing", "Qian Hu", "Lianlian Zhang", "Chao Deng", "Junlan Feng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite LLMs' excellent code creation capabilities, multilingual code\ngeneration remains extremely challenging. To address this, we intent to improve\nthe multi-programming-lingual (MultiPL) performance of the base LLMs while\nretaining the most popular ones using restricted computational resources. We\nconsider MultiPL to be a special case of multiple natural languages and propose\na MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called\nMultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize\nexpert selection at both the token and segment levels. The token-level MoE is a\nstandard upcycling MoE structure with a shared expert and a novel gate weight\nnormalization approach that aids in the final fusion with the segment-level\nMoE. The segment-level MoE incorporates two innovative designs to better\ncapture the syntactic structure and contextual patterns of programming\nlanguages: First, using a sliding window to partition the input token sequence\ninto multiple segments; Then, adopting an expert-choice routing strategy that\nallows experts to select the top-k segments. The results of the experiment\nproved the effectiveness of MultiPL-MoE."}
{"id": "2508.19264", "pdf": "https://arxiv.org/pdf/2508.19264.pdf", "abs": "https://arxiv.org/abs/2508.19264", "title": "A Theory of Information, Variation, and Artificial Intelligence", "authors": ["Bijean Ghafouri"], "categories": ["cs.HC", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "A growing body of empirical work suggests that the widespread adoption of\ngenerative AI produces a significant homogenizing effect on information,\ncreativity, and cultural production. I first develop a novel theoretical\nframework to explain this phenomenon. I argue that a dynamic of AI-derivative\nepistemology, in which individuals increasingly defer to AI outputs, allows a\ncentralized AI Prism to function, a technical mechanism whose architecture is\ndesigned to reduce variance and converge on the statistical mean. This provides\na causal explanation for the generative monocultures observed in recent\nstudies. However, I contend this represents only the first stage of a more\ncomplex and dialectical process. This paper's central and paradoxical thesis is\nthat the very homogenization that flattens knowledge within specialized domains\nsimultaneously renders that knowledge into consistent modules that can be\nrecombined across them, a process foundational to innovation and creativity.\nHowever, this recombinant potential is not automatic, but rather conditional.\nThis paper argues that these opposing forces, homogenizing defaults versus\nrecombinant possibilities, are governed by the nature of human engagement with\nthe technology. The ultimate effect of generative AI is conditional on whether\nindividuals act as passive consumers deferring to the AI's statistical outputs,\nor as active curators who critically interrogate, re-contextualize, and\nrecombine them. The paper concludes by outlining the cognitive and\ninstitutional scaffolds required to resolve this tension, arguing they are the\ndecisive variable that determine whether generative AI becomes an instrument of\ninnovation or homogenization."}
{"id": "2508.19270", "pdf": "https://arxiv.org/pdf/2508.19270.pdf", "abs": "https://arxiv.org/abs/2508.19270", "title": "Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English", "authors": ["Nguyen Huu Nhat Minh", "Tran Nguyen Anh", "Truong Dinh Dung", "Vo Van Nam", "Le Pham Tuyen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cross-lingual phoneme recognition has emerged as a significant challenge for\naccurate automatic speech recognition (ASR) when mixing Vietnamese and English\npronunciations. Unlike many languages, Vietnamese relies on tonal variations to\ndistinguish word meanings, whereas English features stress patterns and\nnon-standard pronunciations that hinder phoneme alignment between the two\nlanguages. To address this challenge, we propose a novel bilingual speech\nrecognition approach with two primary contributions: (1) constructing a\nrepresentative bilingual phoneme set that bridges the differences between\nVietnamese and English phonetic systems; (2) designing an end-to-end system\nthat leverages the PhoWhisper pre-trained encoder for deep high-level\nrepresentations to improve phoneme recognition. Our extensive experiments\ndemonstrate that the proposed approach not only improves recognition accuracy\nin bilingual speech recognition for Vietnamese but also provides a robust\nframework for addressing the complexities of tonal and stress-based phoneme\nrecognition"}
{"id": "2508.19378", "pdf": "https://arxiv.org/pdf/2508.19378.pdf", "abs": "https://arxiv.org/abs/2508.19378", "title": "Improving Hypertension and Diabetes Outcomes with Digital Care Coordination and Remote Monitoring in Rural Health", "authors": ["K. K. Kim", "S. P. McGrath", "D. Lindeman"], "categories": ["cs.HC", "H.5; J.3"], "comment": "12 pages, 1 figure", "summary": "Chronic illnesses are a global concern with essential hypertension and\ndiabetes mellitus among the most common conditions. Remote patient monitoring\nhas shown promising results on clinical and health outcomes. However, access to\ncare and digital health solutions is limited among rural, lower-income, and\nolder adult populations. This paper repots on a pre-post study of a\ncomprehensive care coordination program including connected, wearable blood\npressure and glucometer devices, tablets, and medical assistant-provided health\ncoaching in a community health center in rural California. The participants\n(n=221) had a mean age of 54.6 years, were majority female, two-thirds spoke\nSpanish, 19.9% had hypertension, 49.8% diabetic, and 30.3% both conditions.\nParticipants with hypertension achieved a mean reduction in systolic blood\npressure of 20.24 (95% CI: 13.61, 26.87) at six months while those with\ndiabetes achieved a mean reduction of 3.85 points (95% CI: 3.73, 4.88). These\noutcomes compare favorably to the small but growing body of evidence supporting\ndigital care coordination and remote monitoring. These results also support the\nfeasibility of well-designed digital health solutions yielding improved health\noutcomes among underserved communities."}
{"id": "2508.19271", "pdf": "https://arxiv.org/pdf/2508.19271.pdf", "abs": "https://arxiv.org/abs/2508.19271", "title": "Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT", "authors": ["Rushitha Santhoshi Mamidala", "Anshuman Chhabra", "Ankur Mali"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and\nIn-Context Learning (ICL) have become widely used for eliciting reasoning\ncapabilities in large language models (LLMs). However, these methods rely on\nfragile, implicit mechanisms often yielding inconsistent outputs across seeds,\nformats, or minor prompt variations making them fundamentally unreliable for\ntasks requiring stable, interpretable reasoning. In contrast, automata-based\nneuro-symbolic frameworks like RetoMaton offer a more structured and\ntrustworthy alternative by grounding retrieval in symbolic memory with\ndeterministic transitions. In this work, we extend RetoMaton by replacing its\nglobal datastore with a local, task-adaptive Weighted Finite Automaton (WFA),\nconstructed directly from external domain corpora. This local automaton\nstructure promotes robust, context-aware retrieval while preserving symbolic\ntraceability and low inference overhead. Unlike prompting, which entangles\ncontext and memory in opaque ways, our approach leverages the explicit\nstructure of WFAs to provide verifiable and modular retrieval behavior, making\nit better suited for domain transfer and interoperability. We evaluate this\nlocal RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT\nacross three reasoning tasks: TriviaQA (reading comprehension), GSM8K\n(multi-step math), and MMLU (domain knowledge). Compared to the base model and\nprompting-based methods, augmenting these setups with local RetoMaton\nconsistently improves performance while enabling transparent and reproducible\nretrieval dynamics. Our results highlight a promising shift toward trustworthy,\nsymbolic reasoning in modern LLMs via lightweight, automaton-guided memory."}
{"id": "2508.19407", "pdf": "https://arxiv.org/pdf/2508.19407.pdf", "abs": "https://arxiv.org/abs/2508.19407", "title": "Exploring Paper as a Material: Plotting the Design Space of The Fabrication for Dynamic Paper-Based Interactions", "authors": ["Ruhan Yang", "Ellen Yi-Luen Do"], "categories": ["cs.HC"], "comment": null, "summary": "We reviewed 43 papers to understand the fabrication of dynamic paper-based\ninteractions. We used a design space to classify tool selection, technique\nchoice, and exploration of paper as a material. We classified 9 dimensions for\nthe design space, including 4 dimensions for tools (precision, accommodation,\ncomplexity, and availability), 3 dimensions for techniques (cutting techniques,\nfolding techniques, and integration techniques), and 2 dimensions for paper as\nthe material (paper weight and paper type). The patterns we observed in the\ndesign space indicate a majority use of high precision tools, high complexity\ntools, and surface integration techniques in previous practice. Meanwhile,\nprinting and plain paper are the leading material choices. We analyze these\npatterns and suggest potential directions for future work. Our study helps\nresearchers locate different fabrication approaches and instances, thus\nfostering innovation in the field of paper-based interaction."}
{"id": "2508.19272", "pdf": "https://arxiv.org/pdf/2508.19272.pdf", "abs": "https://arxiv.org/abs/2508.19272", "title": "RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits", "authors": ["Kshitij Fadnis", "Sara Rosenthal", "Maeda Hanafi", "Yannis Katsis", "Marina Danilevsky"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) is an important aspect of conversing\nwith Large Language Models (LLMs) when factually correct information is\nimportant. LLMs may provide answers that appear correct, but could contain\nhallucinated information. Thus, building benchmarks that can evaluate LLMs on\nmulti-turn RAG conversations has become an increasingly important task.\nSimulating real-world conversations is vital for producing high quality\nevaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform\nthat enables annotators to simulate real-world conversations for benchmarking\nand evaluating LLMs. RAGAPHENE has been successfully used by approximately 40\nannotators to build thousands of real-world conversations."}
{"id": "2508.19463", "pdf": "https://arxiv.org/pdf/2508.19463.pdf", "abs": "https://arxiv.org/abs/2508.19463", "title": "\"She was useful, but a bit too optimistic\": Augmenting Design with Interactive Virtual Personas", "authors": ["Paluck Deep", "Monica Bharadhidasan", "A. Baki Kocaballi"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Personas have been widely used to understand and communicate user needs in\nhuman-centred design. Despite their utility, they may fail to meet the demands\nof iterative workflows due to their static nature, limited engagement, and\ninability to adapt to evolving design needs. Recent advances in large language\nmodels (LLMs) pave the way for more engaging and adaptive approaches to user\nrepresentation. This paper introduces Interactive Virtual Personas (IVPs):\nmultimodal, LLM-driven, conversational user simulations that designers can\ninterview, brainstorm with, and gather feedback from in real time via voice\ninterface. We conducted a qualitative study with eight professional UX\ndesigners, employing an IVP named \"Alice\" across three design activities: user\nresearch, ideation, and prototype evaluation. Our findings demonstrate the\npotential of IVPs to expedite information gathering, inspire design solutions,\nand provide rapid user-like feedback. However, designers raised concerns about\nbiases, over-optimism, the challenge of ensuring authenticity without real\nstakeholder input, and the inability of the IVP to fully replicate the nuances\nof human interaction. Our participants emphasised that IVPs should be viewed as\na complement to, not a replacement for, real user engagement. We discuss\nstrategies for prompt engineering, human-in-the-loop integration, and ethical\nconsiderations for effective and responsible IVP use in design. Finally, our\nwork contributes to the growing body of research on generative AI in the design\nprocess by providing insights into UX designers' experiences of LLM-powered\ninteractive personas."}
{"id": "2508.19274", "pdf": "https://arxiv.org/pdf/2508.19274.pdf", "abs": "https://arxiv.org/abs/2508.19274", "title": "Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis", "authors": ["Yue Chu"], "categories": ["cs.CL"], "comment": "Ph.D. dissertation submitted to The Ohio State University, August\n  2025", "summary": "In countries without civil registration and vital statistics, verbal autopsy\n(VA) is a critical tool for estimating cause of death (COD) and inform policy\npriorities. In VA, interviewers ask proximal informants for details on the\ncircumstances preceding a death, in the form of unstructured narratives and\nstructured questions. Existing automated VA cause classification algorithms\nonly use the questions and ignore the information in the narratives. In this\nthesis, we investigate how the VA narrative can be used for automated COD\nclassification using pretrained language models (PLMs) and machine learning\n(ML) techniques. Using empirical data from South Africa, we demonstrate that\nwith the narrative alone, transformer-based PLMs with task-specific fine-tuning\noutperform leading question-only algorithms at both the individual and\npopulation levels, particularly in identifying non-communicable diseases. We\nexplore various multimodal fusion strategies combining narratives and questions\nin unified frameworks. Multimodal approaches further improve performance in COD\nclassification, confirming that each modality has unique contributions and may\ncapture valuable information that is not present in the other modality. We also\ncharacterize physician-perceived information sufficiency in VA. We describe\nvariations in sufficiency levels by age and COD and demonstrate that\nclassification accuracy is affected by sufficiency for both physicians and\nmodels. Overall, this thesis advances the growing body of knowledge at the\nintersection of natural language processing, epidemiology, and global health.\nIt demonstrates the value of narrative in enhancing COD classification. Our\nfindings underscore the need for more high-quality data from more diverse\nsettings to use in training and fine-tuning PLM/ML methods, and offer valuable\ninsights to guide the rethinking and redesign of the VA instrument and\ninterview."}
{"id": "2508.19517", "pdf": "https://arxiv.org/pdf/2508.19517.pdf", "abs": "https://arxiv.org/abs/2508.19517", "title": "Orchid: Orchestrating Context Across Creative Workflows with Generative AI", "authors": ["Srishti Palani", "Gonzalo Ramos"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Context is critical for meaningful interactions between people and Generative\nAI (GenAI). Yet mainstream tools offer limited means to orchestrate it,\nparticularly across workflows that span multiple interactions, sessions, and\nmodels, as often occurs in creative projects. Re specifying prior details,\njuggling diverse artifacts, and dealing with context drift overwhelm users,\nobscure intent, and curtail creativity. To address these challenges, we present\nOrchid, a system that gives its users affordances to specify, reference, and\nmonitor context throughout evolving workflows. Specifically, Orchid enables\nusers to (1) specify context related to the project, themselves, and different\nstyles, (2) reference these via explicit mentions, inline selection, or\nimplicit grounding, and (3) monitor context assigned to different interactions\nacross the workflow. In a within-subjects study (n=12), participants using\nOrchid to execute creative tasks (compared to a baseline toolkit of web search,\nLLM-based chat, and digital notebooks) produced more novel and feasible\noutcomes, reporting greater alignment between their intent and the AI's\nresponses, higher perceived control, and increased transparency. By\nprioritizing context orchestration, Orchid offers an actionable step toward\nnext generation GenAI tools that support complex, iterative workflows -\nenabling creators and AI to stay aligned and augment their creative potential."}
{"id": "2508.19279", "pdf": "https://arxiv.org/pdf/2508.19279.pdf", "abs": "https://arxiv.org/abs/2508.19279", "title": "FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series", "authors": ["Gunjan Jalori", "Preetika Verma", "Sercan Ö Arık"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP", "summary": "Time series Forecasting with large languagemodels (LLMs) requires bridging\nnumericalpatterns and natural language. Effective fore-casting on LLM often\nrelies on extensive pre-processing and fine-tuning.Recent studiesshow that a\nfrozen LLM can rival specializedforecasters when supplied with a carefully\nen-gineered natural-language prompt, but craft-ing such a prompt for each task\nis itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt\noptimization framework thatutilizes an agentic system: a\nForecaster-agentgenerates forecasts using an initial prompt,which is then\nrefined by a refiner agent, in-formed by past outputs and retrieved\nanalogs.This adaptive prompting generalizes across do-mains using creative\nprompt templates andgenerates high-quality forecasts without inter-mediate code\ngeneration.Experiments onbenchmark datasets show improved accuracyover static\nprompting and retrieval-augmentedbaselines, approaching the performance\nofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,\nachievingstrong performance via its agentic approach toadaptive prompt\nrefinement and retrieval."}
{"id": "2508.19622", "pdf": "https://arxiv.org/pdf/2508.19622.pdf", "abs": "https://arxiv.org/abs/2508.19622", "title": "PersoNo: Personalised Notification Urgency Classifier in Mixed Reality", "authors": ["Jingyao Zheng", "Haodi Weng", "Xian Wang", "Chengbin Cui", "Sven Mayer", "Chi-lok Tai", "Lik-Hang Lee"], "categories": ["cs.HC", "cs.MM"], "comment": "Accepted by ISMAR 2025", "summary": "Mixed Reality (MR) is increasingly integrated into daily life, providing\nenhanced capabilities across various domains. However, users face growing\nnotification streams that disrupt their immersive experience. We present\nPersoNo, a personalised notification urgency classifier for MR that\nintelligently classifies notifications based on individual user preferences.\nThrough a user study (N=18), we created the first MR notification dataset\ncontaining both self-labelled and interaction-based data across activities with\nvarying cognitive demands. Our thematic analysis revealed that, unlike in\nmobiles, the activity context is equally important as the content and the\nsender in determining notification urgency in MR. Leveraging these insights, we\ndeveloped PersoNo using large language models that analyse users replying\nbehaviour patterns. Our multi-agent approach achieved 81.5% accuracy and\nsignificantly reduced false negative rates (0.381) compared to baseline models.\nPersoNo has the potential not only to reduce unnecessary interruptions but also\nto offer users understanding and control of the system, adhering to\nHuman-Centered Artificial Intelligence design principles."}
{"id": "2508.19282", "pdf": "https://arxiv.org/pdf/2508.19282.pdf", "abs": "https://arxiv.org/abs/2508.19282", "title": "CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning", "authors": ["Ziqiang Cui", "Yunpeng Weng", "Xing Tang", "Peiyang Liu", "Shiwei Li", "Bowei He", "Jiamin Chen", "Xiuqiang He", "Chen Ma"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\nenhance the timeliness of knowledge and the factual accuracy of responses in\nLarge Language Models (LLMs). However, the inclusion of excessive retrieved\ndocuments substantially increases the input length, leading to higher\ncomputational costs. Previous studies have attempted to compress retrieved\ndocuments into shorter texts before in-context integration, but such methods\noften compromise end-task performance. The lack of well-defined compression\ntargets forces many approaches to rely on fixed heuristics, which cannot\nguarantee that the compressed content will effectively support the end task. To\naddress these limitations, we propose CORE, a novel method designed to achieve\nlossless context compression for RAG. CORE employs reinforcement learning to\noptimize the compression process without relying on predefined compression\nlabels. Specifically, it utilizes end-task performance as a reward signal and\napplies Generalized Reinforcement Learning Policy Optimization (GRPO) to train\nthe compressor. This end-to-end training framework enables the compressor to\ngenerate summaries that maximize the accuracy of answers generated by the LLM.\nExtensive experiments on four datasets demonstrate the superiority of our\napproach. With a high compression ratio of 3\\%, our method not only avoids\nperformance degradation compared to prepending full documents across all\ndatasets but also improves the average Exact Match (EM) score by 3.3 points.\nThe code will be released soon."}
{"id": "2508.19703", "pdf": "https://arxiv.org/pdf/2508.19703.pdf", "abs": "https://arxiv.org/abs/2508.19703", "title": "Haptic Tracing: A new paradigm for spatialized Haptic rendering", "authors": ["Tom Roy", "Yann Glemarec", "Gurvan Lecuyer", "Quentin Galvane", "Philippe Guillotel", "Ferran Argelaguet"], "categories": ["cs.HC"], "comment": null, "summary": "Haptic technology enhances interactive experiences by providing force and\ntactile feedback, improving user performance and immersion. However, despite\nadvancements, creating tactile experiences still remains challenging due to\ndevice diversity and complexity. Most available haptic frameworks rely on\ntrigger-based or event-based systems, and disregard the information of the 3D\nscene to render haptic information. This paper introduces Haptic Tracing, a\nnovel method for spatial haptic rendering that simplifies the creation of\ninteractive haptic experiences without relying on physical simulations. It uses\nconcepts from visual and audio rendering to model and propagate haptic\ninformation through a 3D scene. The paper also describes how our proposed\nhaptic rendering method can be used to create a vibrotactile rendering system,\nenabling the creation of perceptually coherent and dynamic haptic interactions.\nFinally, the paper discusses a user study that explores the role of the haptic\npropagation and multi-actuator rendering on the users' haptic experience. The\nresults show that our approach significantly enhances the realism and the\nexpressivity of the haptic feedback, showcasing its potential for developing\nmore complex and realistic haptic experiences."}
{"id": "2508.19357", "pdf": "https://arxiv.org/pdf/2508.19357.pdf", "abs": "https://arxiv.org/abs/2508.19357", "title": "Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains", "authors": ["Peiran Zhou", "Junnan Zhu", "Yichen Shen", "Ruoxi Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in language tasks but are prone to\nhallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)\nmitigates these by grounding LLMs in external knowledge. However, in complex\ndomains involving multiple, lengthy, or conflicting documents, traditional RAG\nsuffers from information overload and inefficient synthesis, leading to\ninaccurate and untrustworthy answers. To address this, we propose CASC\n(Context-Adaptive Synthesis and Compression), a novel framework that\nintelligently processes retrieved contexts. CASC introduces a Context Analyzer\n& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs\nkey information extraction, cross-document consistency checking and conflict\nresolution, and question-oriented structured synthesis. This process transforms\nraw, scattered information into a highly condensed, structured, and\nsemantically rich context, significantly reducing the token count and cognitive\nload for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new\nchallenging multi-document question answering dataset designed for complex\nscientific domains with inherent redundancies and conflicts. Our extensive\nexperiments demonstrate that CASC consistently outperforms strong baselines."}
{"id": "2508.19708", "pdf": "https://arxiv.org/pdf/2508.19708.pdf", "abs": "https://arxiv.org/abs/2508.19708", "title": "Attention is also needed for form design", "authors": ["B. Sankar", "Dibakar Sen"], "categories": ["cs.HC", "cs.AI", "68T07, 68T42, 68T50", "I.2; J.5; J.6"], "comment": "55 pages, 45 figures,", "summary": "Conventional product design is a cognitively demanding process, limited by\nits time-consuming nature, reliance on subjective expertise, and the opaque\ntranslation of inspiration into tangible concepts. This research introduces a\nnovel, attention-aware framework that integrates two synergistic systems:\nEUPHORIA, an immersive Virtual Reality environment using eye-tracking to\nimplicitly capture a designer's aesthetic preferences, and RETINA, an agentic\nAI pipeline that translates these implicit preferences into concrete design\noutputs. The foundational principles were validated in a two-part study. An\ninitial study correlated user's implicit attention with explicit preference and\nthe next one correlated mood to attention. A comparative study where 4\ndesigners solved challenging design problems using 4 distinct workflows, from a\nmanual process to an end-to-end automated pipeline, showed the integrated\nEUPHORIA-RETINA workflow was over 4 times more time-efficient than the\nconventional method. A panel of 50 design experts evaluated the 16 final\nrenderings. Designs generated by the fully automated system consistently\nreceived the highest Worthiness (calculated by an inverse Plackett-Luce model\nbased on gradient descent optimization) and Design Effectiveness scores,\nindicating superior quality across 8 criteria: novelty, visual appeal,\nemotional resonance, clarity of purpose, distinctiveness of silhouette, implied\nmateriality, proportional balance, & adherence to the brief. This research\npresents a validated paradigm shift from traditional Computer-Assisted Design\n(CAD) to a collaborative model of Designer-Assisting Computers (DAC). By\nautomating logistical and skill-dependent generative tasks, the proposed\nframework elevates the designer's role to that of a creative director,\nsynergizing human intuition with the generative power of agentic AI to produce\nhigher-quality designs more efficiently."}
{"id": "2508.19359", "pdf": "https://arxiv.org/pdf/2508.19359.pdf", "abs": "https://arxiv.org/abs/2508.19359", "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction", "authors": ["Fatemeh Haji", "Mazal Bethany", "Cho-Yu Jason Chiang", "Anthony Rios", "Peyman Najafirad"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Event Extraction (EE) involves automatically identifying and extracting\nstructured information about events from unstructured text, including triggers,\nevent types, and arguments. Traditional discriminative models demonstrate high\nprecision but often exhibit limited recall, particularly for nuanced or\ninfrequent events. Conversely, generative approaches leveraging Large Language\nModels (LLMs) provide higher semantic flexibility and recall but suffer from\nhallucinations and inconsistent predictions. To address these challenges, we\npropose Agreement-based Reflective Inference System (ARIS), a hybrid approach\ncombining a Self Mixture of Agents with a discriminative sequence tagger. ARIS\nexplicitly leverages structured model consensus, confidence-based filtering,\nand an LLM reflective inference module to reliably resolve ambiguities and\nenhance overall event prediction quality. We further investigate decomposed\ninstruction fine-tuning for enhanced LLM event extraction understanding.\nExperiments demonstrate our approach outperforms existing state-of-the-art\nevent extraction methods across three benchmark datasets."}
{"id": "2508.19768", "pdf": "https://arxiv.org/pdf/2508.19768.pdf", "abs": "https://arxiv.org/abs/2508.19768", "title": "Burst: Collaborative Curation in Connected Social Media Communities", "authors": ["Yutong Zhang", "Taeuk Kang", "Sydney Yeh", "Anavi Baddepudi", "Lindsay Popowski", "Tiziano Piccardi", "Michael S. Bernstein"], "categories": ["cs.HC"], "comment": "29 pages, 5 figures; This work will appear in the 28th ACM SIGCHI\n  Conference on Computer-Supported Cooperative Work & Social Computing (CSCW\n  2025)", "summary": "Positive social interactions can occur in groups of many shapes and sizes,\nspanning from small and private to large and open. However, social media tends\nto binarize our experiences into either isolated small groups or into large\npublic squares. In this paper, we introduce Burst, a social media design that\nallows users to share and curate content between many spaces of varied size and\ncomposition. Users initially post content to small trusted groups, who can then\nburst that content, routing it to the groups that would be the best audience.\nWe instantiate this approach into a mobile phone application, and demonstrate\nthrough a ten-day field study (N=36) that Burst enabled a participatory\ncuration culture. With this work, we aim to articulate potential new design\ndirections for social media sharing."}
{"id": "2508.19363", "pdf": "https://arxiv.org/pdf/2508.19363.pdf", "abs": "https://arxiv.org/abs/2508.19363", "title": "LongReasonArena: A Long Reasoning Benchmark for Large Language Models", "authors": ["Jiayu Ding", "Shuming Ma", "Lei Cui", "Nanning Zheng", "Furu Wei"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing long-context benchmarks for Large Language Models (LLMs) focus on\nevaluating comprehension of long inputs, while overlooking the evaluation of\nlong reasoning abilities. To address this gap, we introduce LongReasonArena, a\nbenchmark specifically designed to assess the long reasoning capabilities of\nLLMs. Our tasks require models to solve problems by executing multi-step\nalgorithms that reflect key aspects of long reasoning, such as retrieval and\nbacktracking. By controlling the inputs, the required reasoning length can be\narbitrarily scaled, reaching up to 1 million tokens of reasoning for the most\nchallenging tasks. Extensive evaluation results demonstrate that\nLongReasonArena presents a significant challenge for both open-source and\nproprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our\ntask. Further analysis also reveals that the accuracy exhibits a linear decline\nwith respect to the logarithm of the expected number of reasoning steps. Our\ncode and data is available at\nhttps://github.com/LongReasonArena/LongReasonArena."}
{"id": "2508.19818", "pdf": "https://arxiv.org/pdf/2508.19818.pdf", "abs": "https://arxiv.org/abs/2508.19818", "title": "Towards a Real-Time Warning System for Detecting Inaccuracies in Photoplethysmography-Based Heart Rate Measurements in Wearable Devices", "authors": ["Rania Islmabouli", "Marlene Brunner", "Devender Kumar", "Mahdi Sareban", "Gunnar Treff", "Michael Neudorfer", "Josef Niebauer", "Arne Bathke", "Jan David Smeddinck"], "categories": ["cs.HC"], "comment": null, "summary": "Wearable devices with photoplethysmography (PPG) sensors are widely used to\nmonitor heart rate (HR), yet often suffer from accuracy issues. However, users\ntypically do not receive an indication of potential measurement errors. We\npresent a real-time warning system that detects and communicates inaccuracies\nin PPG-derived HR, aiming to enhance transparency and trust. Using data from\nPolar and Garmin devices, we trained a deep learning model to classify HR\naccuracy using only the derived HR signal. The system detected over 80% of\ninaccurate readings. By providing interpretable, real-time feedback directly to\nusers, our work contributes to HCI by promoting user awareness, informed\ndecision-making, and trust in wearable health technology."}
{"id": "2508.19372", "pdf": "https://arxiv.org/pdf/2508.19372.pdf", "abs": "https://arxiv.org/abs/2508.19372", "title": "Database Entity Recognition with Data Augmentation and Deep Learning", "authors": ["Zikun Fu", "Chen Yang", "Kourosh Davoudi", "Ken Q. Pu"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "comment": "6 pages, 5 figures. Accepted at IEEE 26th International Conference on\n  Information Reuse and Integration for Data Science (IRI 2025), San Jose,\n  California, August 6-8, 2025", "summary": "This paper addresses the challenge of Database Entity Recognition (DB-ER) in\nNatural Language Queries (NLQ). We present several key contributions to advance\nthis field: (1) a human-annotated benchmark for DB-ER task, derived from\npopular text-to-sql benchmarks, (2) a novel data augmentation procedure that\nleverages automatic annotation of NLQs based on the corresponding SQL queries\nwhich are available in popular text-to-SQL benchmarks, (3) a specialized\nlanguage model based entity recognition model using T5 as a backbone and two\ndown-stream DB-ER tasks: sequence tagging and token classification for\nfine-tuning of backend and performing DB-ER respectively. We compared our DB-ER\ntagger with two state-of-the-art NER taggers, and observed better performance\nin both precision and recall for our model. The ablation evaluation shows that\ndata augmentation boosts precision and recall by over 10%, while fine-tuning of\nthe T5 backbone boosts these metrics by 5-10%."}
{"id": "2508.19867", "pdf": "https://arxiv.org/pdf/2508.19867.pdf", "abs": "https://arxiv.org/abs/2508.19867", "title": "Lessons from Biophilic Design: Rethinking Affective Interaction Design in Built Environments", "authors": ["Shruti Rao", "Judith Good", "Hamed Alavi"], "categories": ["cs.HC"], "comment": "3 pages, 1 footer image, provocation paper for CHI 2025 Workshop on\n  Affective Interactions, Japan", "summary": "The perspectives of affective interaction in built environments are largely\noverlooked and instead dominated by affective computing approaches that view\nemotions as \"static\", computable states to be detected and regulated. To\naddress this limitation, we interviewed architects to explore how biophilic\ndesign -- our deep-rooted emotional connection with nature -- could shape\naffective interaction design in smart buildings. Our findings reveal that\nnatural environments facilitate self-directed emotional experiences through\nspatial diversity, embodied friction, and porous sensory exchanges. Based on\nthis, we introduce three design principles for discussion at the Affective\nInteraction workshop: (1) Diversity of Spatial Experiences, (2) Self-Reflection\nThrough Complexity & Friction, and (3) Permeability & Sensory Exchange with the\nOutside World, while also examining the challenges of integrating these\nperspectives into built environments."}
{"id": "2508.19402", "pdf": "https://arxiv.org/pdf/2508.19402.pdf", "abs": "https://arxiv.org/abs/2508.19402", "title": "One Joke to Rule them All? On the (Im)possibility of Generalizing Humor", "authors": ["Mor Turgeman", "Chen Shani", "Dafna Shahaf"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Humor is a broad and complex form of communication that remains challenging\nfor machines. Despite its broadness, most existing research on computational\nhumor traditionally focused on modeling a specific type of humor. In this work,\nwe wish to understand whether competence on one or more specific humor tasks\nconfers any ability to transfer to novel, unseen types; in other words, is this\nfragmentation inevitable? This question is especially timely as new humor types\ncontinuously emerge in online and social media contexts (e.g., memes,\nanti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this\nevolving landscape, they must be able to generalize across humor types by\ncapturing deeper, transferable mechanisms. To investigate this, we conduct a\nseries of transfer learning experiments across four datasets, representing\ndifferent humor tasks. We train LLMs under varied diversity settings (1-3\ndatasets in training, testing on a novel task). Experiments reveal that models\nare capable of some transfer, and can reach up to 75% accuracy on unseen\ndatasets; training on diverse sources improves transferability (1.88-4.05%)\nwith minimal-to-no drop in in-domain performance. Further analysis suggests\nrelations between humor types, with Dad Jokes surprisingly emerging as the best\nenabler of transfer (but is difficult to transfer to). We release data and\ncode."}
{"id": "2508.19942", "pdf": "https://arxiv.org/pdf/2508.19942.pdf", "abs": "https://arxiv.org/abs/2508.19942", "title": "Socially Interactive Agents for Preserving and Transferring Tacit Knowledge in Organizations", "authors": ["Martin Benderoth", "Patrick Gebhard", "Christian Keller", "C. Benjamin Nakhosteen", "Stefan Schaffer", "Tanja Schneeberger"], "categories": ["cs.HC"], "comment": "4 pages, 2 figures", "summary": "This paper introduces a novel approach to tackle the challenges of preserving\nand transferring tacit knowledge--deep, experience-based insights that are hard\nto articulate but vital for decision-making, innovation, and problem-solving.\nTraditional methods rely heavily on human facilitators, which, while effective,\nare resource-intensive and lack scalability. A promising alternative is the use\nof Socially Interactive Agents (SIAs) as AI-driven knowledge transfer\nfacilitators. These agents interact autonomously and socially intelligently\nwith users through multimodal behaviors (verbal, paraverbal, nonverbal),\nsimulating expert roles in various organizational contexts. SIAs engage\nemployees in empathic, natural-language dialogues, helping them externalize\ninsights that might otherwise remain unspoken. Their success hinges on building\ntrust, as employees are often hesitant to share tacit knowledge without\nassurance of confidentiality and appreciation. Key technologies include Large\nLanguage Models (LLMs) for generating context-relevant dialogue,\nRetrieval-Augmented Generation (RAG) to integrate organizational knowledge, and\nChain-of-Thought (CoT) prompting to guide structured reflection. These enable\nSIAs to actively elicit knowledge, uncover implicit assumptions, and connect\ninsights to broader organizational contexts. Potential applications span\nonboarding, where SIAs support personalized guidance and introductions, and\nknowledge retention, where they conduct structured interviews with retiring\nexperts to capture heuristics behind decisions. Success depends on addressing\nethical and operational challenges such as data privacy, algorithmic bias, and\nresistance to AI. Transparency, robust validation, and a culture of trust are\nessential to mitigate these risks."}
{"id": "2508.19427", "pdf": "https://arxiv.org/pdf/2508.19427.pdf", "abs": "https://arxiv.org/abs/2508.19427", "title": "A perishable ability? The future of writing in the face of generative artificial intelligence", "authors": ["Evandro L. T. P. Cunha"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "10 pages", "summary": "The 2020s have been witnessing a very significant advance in the development\nof generative artificial intelligence tools, including text generation systems\nbased on large language models. These tools have been increasingly used to\ngenerate texts in the most diverse domains -- from technical texts to literary\ntexts --, which might eventually lead to a lower volume of written text\nproduction by humans. This article discusses the possibility of a future in\nwhich human beings will have lost or significantly decreased their ability to\nwrite due to the outsourcing of this activity to machines. This possibility\nparallels the loss of the ability to write in other moments of human history,\nsuch as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE)."}
{"id": "2508.19971", "pdf": "https://arxiv.org/pdf/2508.19971.pdf", "abs": "https://arxiv.org/abs/2508.19971", "title": "CapTune: Adapting Non-Speech Captions With Anchored Generative Models", "authors": ["Jeremy Zhengqi Huang", "Caluã de Lacerda Pataca", "Liang-Yuan Wu", "Dhruv Jain"], "categories": ["cs.HC", "cs.HC, cs.AI"], "comment": "ASSETS 2025", "summary": "Non-speech captions are essential to the video experience of deaf and hard of\nhearing (DHH) viewers, yet conventional approaches often overlook the diversity\nof their preferences. We present CapTune, a system that enables customization\nof non-speech captions based on DHH viewers' needs while preserving creator\nintent. CapTune allows caption authors to define safe transformation spaces\nusing concrete examples and empowers viewers to personalize captions across\nfour dimensions: level of detail, expressiveness, sound representation method,\nand genre alignment. Evaluations with seven caption creators and twelve DHH\nparticipants showed that CapTune supported creators' creative control while\nenhancing viewers' emotional engagement with content. Our findings also reveal\ntrade-offs between information richness and cognitive load, tensions between\ninterpretive and descriptive representations of sound, and the\ncontext-dependent nature of caption preferences."}
{"id": "2508.19428", "pdf": "https://arxiv.org/pdf/2508.19428.pdf", "abs": "https://arxiv.org/abs/2508.19428", "title": "Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)", "authors": ["Aleksandra Beliaeva", "Temurbek Rahmatullaev"], "categories": ["cs.CL", "cs.LO", "cs.SC", "68T30, 68T50, 68T07, 68U15", "I.2.4; I.2.7; H.3.1; H.3.3; I.2.6"], "comment": null, "summary": "We present a comprehensive system for addressing Tasks A, B, and C of the\nLLMs4OL 2025 challenge, which together span the full ontology construction\npipeline: term extraction, typing, and taxonomy discovery. Our approach\ncombines retrieval-augmented prompting, zero-shot classification, and\nattention-based graph modeling -- each tailored to the demands of the\nrespective task. For Task A, we jointly extract domain-specific terms and their\nontological types using a retrieval-augmented generation (RAG) pipeline.\nTraining data was reformulated into a document to terms and types\ncorrespondence, while test-time inference leverages semantically similar\ntraining examples. This single-pass method requires no model finetuning and\nimproves overall performance through lexical augmentation Task B, which\ninvolves assigning types to given terms, is handled via a dual strategy. In the\nfew-shot setting (for domains with labeled training data), we reuse the RAG\nscheme with few-shot prompting. In the zero-shot setting (for previously unseen\ndomains), we use a zero-shot classifier that combines cosine similarity scores\nfrom multiple embedding models using confidence-based weighting. In Task C, we\nmodel taxonomy discovery as graph inference. Using embeddings of type labels,\nwe train a lightweight cross-attention layer to predict is-a relations by\napproximating a soft adjacency matrix. These modular, task-specific solutions\nenabled us to achieve top-ranking results in the official leaderboard across\nall three tasks. Taken together these strategies showcase the scalability,\nadaptability, and robustness of LLM-based architectures for ontology learning\nacross heterogeneous domains.\n  Code is available at:\nhttps://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek"}
{"id": "2508.20034", "pdf": "https://arxiv.org/pdf/2508.20034.pdf", "abs": "https://arxiv.org/abs/2508.20034", "title": "FlyMeThrough: Human-AI Collaborative 3D Indoor Mapping with Commodity Drones", "authors": ["Xia Su", "Ruiqi Chen", "Jingwei Ma", "Chu Li", "Jon E. Froehlich"], "categories": ["cs.HC", "H.5.2; I.2.10"], "comment": "Accepted at UIST 2025, 14 pages, 8 figures, 2 tables", "summary": "Indoor mapping data is crucial for routing, navigation, and building\nmanagement, yet such data are widely lacking due to the manual labor and\nexpense of data collection, especially for larger indoor spaces. Leveraging\nrecent advancements in commodity drones and photogrammetry, we introduce\nFlyMeThrough -- a drone-based indoor scanning system that efficiently produces\n3D reconstructions of indoor spaces with human-AI collaborative annotations for\nkey indoor points-of-interest (POI) such as entrances, restrooms, stairs, and\nelevators. We evaluated FlyMeThrough in 12 indoor spaces with varying sizes and\nfunctionality. To investigate use cases and solicit feedback from target\nstakeholders, we also conducted a qualitative user study with five building\nmanagers and five occupants. Our findings indicate that FlyMeThrough can\nefficiently and precisely create indoor 3D maps for strategic space planning,\nresource management, and navigation."}
{"id": "2508.19464", "pdf": "https://arxiv.org/pdf/2508.19464.pdf", "abs": "https://arxiv.org/abs/2508.19464", "title": "Bridging Language Gaps: Enhancing Few-Shot Language Adaptation", "authors": ["Philipp Borchert", "Jochen De Weerdt", "Marie-Francine Moens"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "The disparity in language resources poses a challenge in multilingual NLP,\nwith high-resource languages benefiting from extensive data, while low-resource\nlanguages lack sufficient data for effective training. Our Contrastive Language\nAlignment with Prompting (CoLAP) method addresses this gap by integrating\ncontrastive learning with cross-lingual representations, facilitating\ntask-specific knowledge transfer from high-resource to lower-resource\nlanguages. The primary advantage of our approach is its data efficiency,\nenabling rapid adaptation to new languages and reducing the need for large\nlabeled datasets. We conduct experiments with multilingual encoder-only and\ndecoder-only language models on natural language understanding tasks, including\nnatural language inference and relation extraction, evaluating performance\nacross both high- and low-resource languages. Our results demonstrate that\nCoLAP outperforms few-shot cross-lingual transfer baselines and in-context\nlearning, even with limited available data. This effectively narrows the\ncross-lingual performance gap, contributing to the development of more\nefficient multilingual NLP techniques."}
{"id": "2508.19254", "pdf": "https://arxiv.org/pdf/2508.19254.pdf", "abs": "https://arxiv.org/abs/2508.19254", "title": "Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration", "authors": ["Jookyung Song", "Mookyoung Kang", "Nojun Kwak"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "6 pages, 4 figures, NeurIPS Creative AI Track 2025", "summary": "This paper presents a real-time generative drawing system that interprets and\nintegrates both formal intent - the structural, compositional, and stylistic\nattributes of a sketch - and contextual intent - the semantic and thematic\nmeaning inferred from its visual content - into a unified transformation\nprocess. Unlike conventional text-prompt-based generative systems, which\nprimarily capture high-level contextual descriptions, our approach\nsimultaneously analyzes ground-level intuitive geometric features such as line\ntrajectories, proportions, and spatial arrangement, and high-level semantic\ncues extracted via vision-language models. These dual intent signals are\njointly conditioned in a multi-stage generation pipeline that combines\ncontour-preserving structural control with style- and content-aware image\nsynthesis. Implemented with a touchscreen-based interface and distributed\ninference architecture, the system achieves low-latency, two-stage\ntransformation while supporting multi-user collaboration on shared canvases.\nThe resulting platform enables participants, regardless of artistic expertise,\nto engage in synchronous, co-authored visual creation, redefining human-AI\ninteraction as a process of co-creation and mutual enhancement."}
{"id": "2508.19467", "pdf": "https://arxiv.org/pdf/2508.19467.pdf", "abs": "https://arxiv.org/abs/2508.19467", "title": "Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset", "authors": ["Sumon Kanti Dey", "Jeanne M. Powell", "Azra Ismail", "Jeanmarie Perrone", "Abeed Sarker"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Dataset and code: https://github.com/SumonKantiDey/Reddit_Impacts_NER", "summary": "Nonmedical opioid use is an urgent public health challenge, with far-reaching\nclinical and social consequences that are often underreported in traditional\nhealthcare settings. Social media platforms, where individuals candidly share\nfirst-person experiences, offer a valuable yet underutilized source of insight\ninto these impacts. In this study, we present a named entity recognition (NER)\nframework to extract two categories of self-reported consequences from social\nmedia narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,\ndepression) and SocialImpacts (e.g., job loss). To support this task, we\nintroduce RedditImpacts 2.0, a high-quality dataset with refined annotation\nguidelines and a focus on first-person disclosures, addressing key limitations\nof prior work. We evaluate both fine-tuned encoder-based models and\nstate-of-the-art large language models (LLMs) under zero- and few-shot\nin-context learning settings. Our fine-tuned DeBERTa-large model achieves a\nrelaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming\nLLMs in precision, span accuracy, and adherence to task-specific guidelines.\nFurthermore, we show that strong NER performance can be achieved with\nsubstantially less labeled data, emphasizing the feasibility of deploying\nrobust models in resource-limited settings. Our findings underscore the value\nof domain-specific fine-tuning for clinical NLP tasks and contribute to the\nresponsible development of AI tools that may enhance addiction surveillance,\nimprove interpretability, and support real-world healthcare decision-making.\nThe best performing model, however, still significantly underperforms compared\nto inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap\npersists between expert intelligence and current state-of-the-art NER/AI\ncapabilities for tasks requiring deep domain knowledge."}
{"id": "2508.19367", "pdf": "https://arxiv.org/pdf/2508.19367.pdf", "abs": "https://arxiv.org/abs/2508.19367", "title": "Inference of Human-derived Specifications of Object Placement via Demonstration", "authors": ["Alex Cuellar", "Ho Chit Siu", "Julie A Shah"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,\nobject packing, sorting, and kitting), methods focused on understanding\nhuman-acceptable object configurations remain limited expressively with regard\nto capturing spatial relationships important to humans. To advance robotic\nunderstanding of human rules for object arrangement, we introduce\npositionally-augmented RCC (PARCC), a formal logic framework based on region\nconnection calculus (RCC) for describing the relative position of objects in\nspace. Additionally, we introduce an inference algorithm for learning PARCC\nspecifications via demonstrations. Finally, we present the results from a human\nstudy, which demonstrate our framework's ability to capture a human's intended\nspecification and the benefits of learning from demonstration approaches over\nhuman-provided specifications."}
{"id": "2508.19475", "pdf": "https://arxiv.org/pdf/2508.19475.pdf", "abs": "https://arxiv.org/abs/2508.19475", "title": "Automatic Question & Answer Generation Using Generative Large Language Model (LLM)", "authors": ["Md. Alvee Ehsan", "A. S. M Mehedi Hasan", "Kefaya Benta Shahnoor", "Syeda Sumaiya Tasneem"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "\\Abstract{In the realm of education, student evaluation holds equal\nsignificance as imparting knowledge. To be evaluated, students usually need to\ngo through text-based academic assessment methods. Instructors need to make\ndiverse sets of questions that need to be fair for all students to prove their\nadequacy over a particular topic. This can prove to be quite challenging as\nthey may need to manually go through several different lecture materials. Our\nobjective is to make this whole process much easier by implementing Automatic\nQuestion Answer Generation /(AQAG), using fine-tuned generative LLM. For\ntailoring the instructor's preferred question style (MCQ, conceptual, or\nfactual questions), prompt Engineering (PE) is being utilized. In this\nresearch, we propose to leverage unsupervised learning methods in NLP,\nprimarily focusing on the English language. This approach empowers the base\nMeta-Llama 2-7B model to integrate RACE dataset as training data for the\nfine-tuning process. Creating a customized model that will offer efficient\nsolutions for educators, instructors, and individuals engaged in text-based\nevaluations. A reliable and efficient tool for generating questions and answers\ncan free up valuable time and resources, thus streamlining their evaluation\nprocesses.}"}
{"id": "2508.19427", "pdf": "https://arxiv.org/pdf/2508.19427.pdf", "abs": "https://arxiv.org/abs/2508.19427", "title": "A perishable ability? The future of writing in the face of generative artificial intelligence", "authors": ["Evandro L. T. P. Cunha"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "10 pages", "summary": "The 2020s have been witnessing a very significant advance in the development\nof generative artificial intelligence tools, including text generation systems\nbased on large language models. These tools have been increasingly used to\ngenerate texts in the most diverse domains -- from technical texts to literary\ntexts --, which might eventually lead to a lower volume of written text\nproduction by humans. This article discusses the possibility of a future in\nwhich human beings will have lost or significantly decreased their ability to\nwrite due to the outsourcing of this activity to machines. This possibility\nparallels the loss of the ability to write in other moments of human history,\nsuch as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE)."}
{"id": "2508.19481", "pdf": "https://arxiv.org/pdf/2508.19481.pdf", "abs": "https://arxiv.org/abs/2508.19481", "title": "Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study", "authors": ["Manuel Mosquera", "Melissa Robles", "Johan Rodriguez", "Ruben Manrique"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Low-resource machine translation remains a significant challenge for large\nlanguage models (LLMs), which often lack exposure to these languages during\npretraining and have limited parallel data for fine-tuning. We propose a novel\napproach that enhances translation for low-resource languages by integrating an\nexternal dictionary tool and training models end-to-end using reinforcement\nlearning, in addition to supervised fine-tuning. Focusing on the\nSpanish-Wayuunaiki language pair, we frame translation as a tool-augmented\ndecision-making problem in which the model can selectively consult a bilingual\ndictionary during generation. Our method combines supervised instruction tuning\nwith Guided Reward Policy Optimization (GRPO), enabling the model to learn both\nwhen and how to use the tool effectively. BLEU similarity scores are used as\nrewards to guide this learning process. Preliminary results show that our\ntool-augmented models achieve up to +3.37 BLEU improvement over previous work,\nand a 18% relative gain compared to a supervised baseline without dictionary\naccess, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared\nTask. We also conduct ablation studies to assess the effects of model\narchitecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other\nmodels such as LLaMA and a prior NLLB-based system. These findings highlight\nthe promise of combining LLMs with external tools and the role of reinforcement\nlearning in improving translation quality in low-resource language settings."}
{"id": "2508.19993", "pdf": "https://arxiv.org/pdf/2508.19993.pdf", "abs": "https://arxiv.org/abs/2508.19993", "title": "MathBuddy: A Multimodal System for Affective Math Tutoring", "authors": ["Debanjana Kar", "Leopold Böss", "Dacia Braca", "Sebastian Maximilian Dennerlein", "Nina Christine Hubig", "Philipp Wintersberger", "Yufang Hou"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The rapid adoption of LLM-based conversational systems is already\ntransforming the landscape of educational technology. However, the current\nstate-of-the-art learning models do not take into account the student's\naffective states. Multiple studies in educational psychology support the claim\nthat positive or negative emotional states can impact a student's learning\ncapabilities. To bridge this gap, we present MathBuddy, an emotionally aware\nLLM-powered Math Tutor, which dynamically models the student's emotions and\nmaps them to relevant pedagogical strategies, making the tutor-student\nconversation a more empathetic one. The student's emotions are captured from\nthe conversational text as well as from their facial expressions. The student's\nemotions are aggregated from both modalities to confidently prompt our LLM\nTutor for an emotionally-aware response. We have effectively evaluated our\nmodel using automatic evaluation metrics across eight pedagogical dimensions\nand user studies. We report a massive 23 point performance gain using the win\nrate and a 3 point gain at an overall level using DAMR scores which strongly\nsupports our hypothesis of improving LLM-based tutor's pedagogical abilities by\nmodeling students' emotions."}
{"id": "2508.19484", "pdf": "https://arxiv.org/pdf/2508.19484.pdf", "abs": "https://arxiv.org/abs/2508.19484", "title": "Rule Synergy Analysis using LLMs: State of the Art and Implications", "authors": ["Bahar Bateni", "Benjamin Pratt", "Jim Whitehead"], "categories": ["cs.CL"], "comment": "Submitted for publication at the IEEE Transactions on Games 2024,\n  Special Issue on Large Language Models and Games (10 pages excluding\n  appendix, 3 figures)", "summary": "Large language models (LLMs) have demonstrated strong performance across a\nvariety of domains, including logical reasoning, mathematics, and more. In this\npaper, we investigate how well LLMs understand and reason about complex rule\ninteractions in dynamic environments, such as card games. We introduce a\ndataset of card synergies from the game Slay the Spire, where pairs of cards\nare classified based on their positive, negative, or neutral interactions. Our\nevaluation shows that while LLMs excel at identifying non-synergistic pairs,\nthey struggle with detecting positive and, particularly, negative synergies. We\ncategorize common error types, including issues with timing, defining game\nstates, and following game rules. Our findings suggest directions for future\nresearch to improve model performance in predicting the effect of rules and\ntheir interactions."}
{"id": "2506.14799", "pdf": "https://arxiv.org/pdf/2506.14799.pdf", "abs": "https://arxiv.org/abs/2506.14799", "title": "Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust", "authors": ["Evdoxia Taka", "Debadyuti Bhattacharya", "Joanne Garde-Hansen", "Sanjay Sharma", "Tanaya Guha"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in AI has made automated analysis of complex media content at\nscale possible while generating actionable insights regarding character\nrepresentation along such dimensions as gender and age. Past works focused on\nquantifying representation from audio/video/text using AI models, but without\nhaving the audience in the loop. We ask, even if character distribution along\ndemographic dimensions are available, how useful are those to the general\npublic? Do they actually trust the numbers generated by AI models? Our work\naddresses these open questions by proposing a new AI-based character\nrepresentation tool and performing a thorough user study. Our tool has two\ncomponents: (i) An analytics extraction model based on the Contrastive Language\nImage Pretraining (CLIP) foundation model that analyzes visual screen data to\nquantify character representation across age and gender; (ii) A visualization\ncomponent effectively designed for presenting the analytics to lay audience.\nThe user study seeks empirical evidence on the usefulness and trustworthiness\nof the AI-generated results for carefully chosen movies presented in the form\nof our visualizations. We found that participants were able to understand the\nanalytics in our visualizations, and deemed the tool `overall useful'.\nParticipants also indicated a need for more detailed visualizations to include\nmore demographic categories and contextual information of the characters.\nParticipants' trust in AI-based gender and age models is seen to be moderate to\nlow, although they were not against the use of AI in this context. Our tool\nincluding code, benchmarking, and the user study data can be found at\nhttps://github.com/debadyuti0510/Character-Representation-Media."}
{"id": "2508.19529", "pdf": "https://arxiv.org/pdf/2508.19529.pdf", "abs": "https://arxiv.org/abs/2508.19529", "title": "Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding", "authors": ["Bowen Sun", "Yujun Cai", "Ming-Hsuan Yang", "Yiwei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Discrete diffusion language models have shown strong potential for text\ngeneration, yet standard supervised fine-tuning (SFT) misaligns with their\nsemi-autoregressive inference: training randomly masks tokens across the entire\nresponse, while inference generates fixed-size blocks sequentially. This\nmismatch introduces noisy prefixes and leaky suffixes, biasing gradients away\nfrom the desired blockwise likelihood. We propose Blockwise SFT, which\npartitions responses into fixed-size blocks, selects one active block per step\nfor stochastic masking, freezes all preceding tokens, and fully hides future\nones. Loss is computed only over the active block, directly mirroring the\nblockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show\nconsistent gains over classical SFT under equal compute or token budgets. Block\nsize consistency studies and ablations confirm that improvements stem from\nfaithful training-inference alignment rather than incidental masking effects.\nOur results highlight the importance of matching supervision granularity to the\ndecoding procedure in diffusion-based language models."}
{"id": "2506.23016", "pdf": "https://arxiv.org/pdf/2506.23016.pdf", "abs": "https://arxiv.org/abs/2506.23016", "title": "Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks", "authors": ["Tomás Silva Santos Rocha", "Anastasiia Mikhailova", "Moreno I. Coco", "José Santos-Victor"], "categories": ["cs.HC", "cs.CV"], "comment": "13 pages, 5 figures", "summary": "The global prevalence of dementia is projected to double by 2050,\nhighlighting the urgent need for scalable diagnostic tools. This study utilizes\ndigital cognitive tasks with eye-tracking data correlated with memory processes\nto distinguish between Healthy Controls (HC) and Mild Cognitive Impairment\n(MCI), a precursor to dementia. A deep learning model based on VTNet was\ntrained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who\nperformed a visual memory task. The model utilizes both time series and spatial\ndata derived from eye-tracking. It was modified to incorporate scan paths, heat\nmaps, and image content. These modifications also enabled testing parameters\nsuch as image resolution and task performance, analyzing their impact on model\nperformance. The best model, utilizing $700\\times700px$ resolution heatmaps,\nachieved 68% sensitivity and 76% specificity. Despite operating under more\nchallenging conditions (e.g., smaller dataset size, shorter task duration, or a\nless standardized task), the model's performance is comparable to an\nAlzheimer's study using similar methods (70% sensitivity and 73% specificity).\nThese findings contribute to the development of automated diagnostic tools for\nMCI. Future work should focus on refining the model and using a standardized\nlong-term visual memory task."}
{"id": "2508.19532", "pdf": "https://arxiv.org/pdf/2508.19532.pdf", "abs": "https://arxiv.org/abs/2508.19532", "title": "Alignment with Fill-In-the-Middle for Enhancing Code Generation", "authors": ["Houxing Ren", "Zimu Lu", "Weikang Shi", "Haotian Hou", "Yunqiao Yang", "Ke Wang", "Aojun Zhou", "Junting Pan", "Mingjie Zhan", "Hongsheng Li"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 (main conference)", "summary": "The code generation capabilities of Large Language Models (LLMs) have\nadvanced applications like tool invocation and problem-solving. However,\nimproving performance in code-related tasks remains challenging due to limited\ntraining data that is verifiable with accurate test cases. While Direct\nPreference Optimization (DPO) has shown promise, existing methods for\ngenerating test cases still face limitations. In this paper, we propose a novel\napproach that splits code snippets into smaller, granular blocks, creating more\ndiverse DPO pairs from the same test cases. Additionally, we introduce the\nAbstract Syntax Tree (AST) splitting and curriculum training method to enhance\nthe DPO training. Our approach demonstrates significant improvements in code\ngeneration tasks, as validated by experiments on benchmark datasets such as\nHumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data\nare available at https://github.com/SenseLLM/StructureCoder."}
{"id": "2507.22893", "pdf": "https://arxiv.org/pdf/2507.22893.pdf", "abs": "https://arxiv.org/abs/2507.22893", "title": "Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure", "authors": ["Giuseppe Riva"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Contemporary human-AI interaction research overlooks how AI systems\nfundamentally reshape human cognition pre-consciously, a critical blind spot\nfor understanding distributed cognition. This paper introduces \"Cognitive\nInfrastructure Studies\" (CIS) as a new interdisciplinary domain to\nreconceptualize AI as \"cognitive infrastructures\": foundational, often\ninvisible systems conditioning what is knowable and actionable in digital\nsocieties. These semantic infrastructures transport meaning, operate through\nanticipatory personalization, and exhibit adaptive invisibility, making their\ninfluence difficult to detect. Critically, they automate \"relevance judgment,\"\nshifting the \"locus of epistemic agency\" to non-human systems. Through\nnarrative scenarios spanning individual (cognitive dependency), collective\n(democratic deliberation), and societal (governance) scales, we describe how\ncognitive infrastructures reshape human cognition, public reasoning, and social\nepistemologies. CIS aims to address how AI preprocessing reshapes distributed\ncognition across individual, collective, and cultural scales, requiring\nunprecedented integration of diverse disciplinary methods. The framework also\naddresses critical gaps across disciplines: cognitive science lacks\npopulation-scale preprocessing analysis capabilities, digital sociology cannot\naccess individual cognitive mechanisms, and computational approaches miss\ncultural transmission dynamics. To achieve this goal CIS also provides\nmethodological innovations for studying invisible algorithmic influence:\n\"infrastructure breakdown methodologies\", experimental approaches that reveal\ncognitive dependencies by systematically withdrawing AI preprocessing after\nperiods of habituation."}
{"id": "2508.19533", "pdf": "https://arxiv.org/pdf/2508.19533.pdf", "abs": "https://arxiv.org/abs/2508.19533", "title": "Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation", "authors": ["Kun Peng", "Cong Cao", "Hao Peng", "Guanlin Wu", "Zhifeng Hao", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP2025", "summary": "Current Emotion Recognition in Conversation (ERC) research follows a\nclosed-domain assumption. However, there is no clear consensus on emotion\nclassification in psychology, which presents a challenge for models when it\ncomes to recognizing previously unseen emotions in real-world applications. To\nbridge this gap, we introduce the Unseen Emotion Recognition in Conversation\n(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based\nemotion transfer framework. This prototype-based approach shows promise but\nstill faces key challenges: First, implicit expressions complicate emotion\ndefinition, which we address by proposing an LLM-enhanced description approach.\nSecond, utterance encoding in long conversations is difficult, which we tackle\nwith a proposed parameter-free mechanism for efficient encoding and overfitting\nprevention. Finally, the Markovian flow nature of emotions is hard to transfer,\nwhich we address with an improved Attention Viterbi Decoding (AVD) method to\ntransfer seen emotion transitions to unseen emotions. Extensive experiments on\nthree datasets show that our method serves as a strong baseline for preliminary\nexploration in this new area."}
{"id": "2508.03969", "pdf": "https://arxiv.org/pdf/2508.03969.pdf", "abs": "https://arxiv.org/abs/2508.03969", "title": "Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI Perspective", "authors": ["Wei Xu"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This chapter systematically promotes an emerging interdisciplinary field of\nhuman-artificial intelligence interaction (human-AI interaction, HAII) from a\nhuman-centered AI (HCAI) perspective. It introduces a framework of\nhuman-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII\nresearch and applications, emphasizing the importance of adopting a\nhuman-centered approach over a technology-centered one. The chapter presents\nthe HC-HAII methodology, including human-centered methods, process,\ninterdisciplinary teams, and multi-level design paradigms. It also highlights\nkey research challenges and future directions. As the first chapter, this\nchapter also provides a structural overview of this book, which brings together\ncontributions from an interdisciplinary community of researchers and\npractitioners to advance the theory, methodology, and applications of HCAI in\ndiverse domains of HAII. The purpose of this chapter is to provide a\nfundamental framework for this book, centered on HAII research and applications\nbased on the HCAI approach, which will pave the way for the content of\nsubsequent chapters."}
{"id": "2508.19546", "pdf": "https://arxiv.org/pdf/2508.19546.pdf", "abs": "https://arxiv.org/abs/2508.19546", "title": "Language Models Identify Ambiguities and Exploit Loopholes", "authors": ["Jio Choi", "Mohit Bansal", "Elias Stengel-Eskin"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 camera-ready; Code:\n  https://github.com/esteng/ambiguous-loophole-exploitation", "summary": "Studying the responses of large language models (LLMs) to loopholes presents\na two-fold opportunity. First, it affords us a lens through which to examine\nambiguity and pragmatics in LLMs, since exploiting a loophole requires\nidentifying ambiguity and performing sophisticated pragmatic reasoning. Second,\nloopholes pose an interesting and novel alignment problem where the model is\npresented with conflicting goals and can exploit ambiguities to its own\nadvantage. To address these questions, we design scenarios where LLMs are given\na goal and an ambiguous user instruction in conflict with the goal, with\nscenarios covering scalar implicature, structural ambiguities, and power\ndynamics. We then measure different models' abilities to exploit loopholes to\nsatisfy their given goals as opposed to the goals of the user. We find that\nboth closed-source and stronger open-source models can identify ambiguities and\nexploit their resulting loopholes, presenting a potential AI safety risk. Our\nanalysis indicates that models which exploit loopholes explicitly identify and\nreason about both ambiguity and conflicting goals."}
{"id": "2508.08524", "pdf": "https://arxiv.org/pdf/2508.08524.pdf", "abs": "https://arxiv.org/abs/2508.08524", "title": "StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI", "authors": ["Jon E. Froehlich", "Alexander Fiannaca", "Nimer Jaber", "Victor Tsara", "Shaun Kane"], "categories": ["cs.HC", "cs.AI", "H.5; I.2"], "comment": "Accepted to UIST'25", "summary": "Interactive streetscape mapping tools such as Google Street View (GSV) and\nMeta Mapillary enable users to virtually navigate and experience real-world\nenvironments via immersive 360{\\deg} imagery but remain fundamentally\ninaccessible to blind users. We introduce StreetViewAI, the first-ever\naccessible street view tool, which combines context-aware, multimodal AI,\naccessible navigation controls, and conversational speech. With StreetViewAI,\nblind users can virtually examine destinations, engage in open-world\nexploration, or virtually tour any of the over 220 billion images and 100+\ncountries where GSV is deployed. We iteratively designed StreetViewAI with a\nmixed-visual ability team and performed an evaluation with eleven blind users.\nOur findings demonstrate the value of an accessible street view in supporting\nPOI investigations and remote route planning. We close by enumerating key\nguidelines for future work."}
{"id": "2508.19578", "pdf": "https://arxiv.org/pdf/2508.19578.pdf", "abs": "https://arxiv.org/abs/2508.19578", "title": "Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts", "authors": ["Jiaqi Deng", "Yuho Lee", "Nicole Hee-Yeon Kim", "Hyangsuk Min", "Taewon Yun", "Minjeong Ban", "Kim Yul", "Hwanjun Song"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 (Main)", "summary": "We introduce HAMLET, a holistic and automated framework for evaluating the\nlong-context comprehension of large language models (LLMs). HAMLET structures\nsource texts into a three-level key-fact hierarchy at root-, branch-, and\nleaf-levels, and employs query-focused summarization to evaluate how well\nmodels recall and faithfully represent information at each level. To validate\nthe reliability of our fully automated pipeline, we conduct a systematic human\nstudy, showing that our automatic evaluation achieves over 90% agreement with\nexpert human judgments, while reducing the cost by up to 25 times. HAMLET\nreveals that LLMs struggle with fine-grained comprehension, especially at the\nleaf level, and are sensitive to positional effects like the\nlost-in-the-middle. Analytical queries pose greater challenges than narrative\nones, and consistent performance gaps emerge between open-source and\nproprietary models, as well as across model scales. Our code and dataset are\npublicly available at https://github.com/DISL-Lab/HAMLET."}
{"id": "2508.11030", "pdf": "https://arxiv.org/pdf/2508.11030.pdf", "abs": "https://arxiv.org/abs/2508.11030", "title": "Families' Vision of Generative AI Agents for Household Safety Against Digital and Physical Threats", "authors": ["Zikai Wen", "Lanjing Liu", "Yaxing Yao"], "categories": ["cs.HC"], "comment": "Accepted in Proc. ACM Hum.-Comput. Interact. 9, 7, Article CSCW", "summary": "As families face increasingly complex safety challenges in digital and\nphysical environments, generative AI (GenAI) presents new opportunities to\nsupport household safety through multiple specialized AI agents. Through a\ntwo-phase qualitative study consisting of individual interviews and\ncollaborative sessions with 13 parent-child dyads, we explored families'\nconceptualizations of GenAI and their envisioned use of AI agents in daily\nfamily life. Our findings reveal that families preferred to distribute\nsafety-related support across multiple AI agents, each embodying a familiar\ncaregiving role: a household manager coordinating routine tasks and mitigating\nrisks such as digital fraud and home accidents; a private tutor providing\npersonalized educational support, including safety education; and a family\ntherapist offering emotional support to address sensitive safety issues such as\ncyberbullying and digital harassment. Families emphasized the need for\nagent-specific privacy boundaries, recognized generational differences in trust\ntoward AI agents, and stressed the importance of maintaining open family\ncommunication alongside the assistance of AI agents. Based on these findings,\nwe propose a multi-agent system design featuring four privacy-preserving\nprinciples: memory segregation, conversational consent, selective data sharing,\nand progressive memory management to help balance safety, privacy, and autonomy\nwithin family contexts."}
{"id": "2508.19580", "pdf": "https://arxiv.org/pdf/2508.19580.pdf", "abs": "https://arxiv.org/abs/2508.19580", "title": "ArgCMV: An Argument Summarization Benchmark for the LLM-era", "authors": ["Omkar Gurjar", "Agam Goyal", "Eshwar Chandrasekharan"], "categories": ["cs.CL"], "comment": null, "summary": "Key point extraction is an important task in argument summarization which\ninvolves extracting high-level short summaries from arguments. Existing\napproaches for KP extraction have been mostly evaluated on the popular ArgKP21\ndataset. In this paper, we highlight some of the major limitations of the\nArgKP21 dataset and demonstrate the need for new benchmarks that are more\nrepresentative of actual human conversations. Using SoTA large language models\n(LLMs), we curate a new argument key point extraction dataset called ArgCMV\ncomprising of around 12K arguments from actual online human debates spread\nacross over 3K topics. Our dataset exhibits higher complexity such as longer,\nco-referencing arguments, higher presence of subjective discourse units, and a\nlarger range of topics over ArgKP21. We show that existing methods do not adapt\nwell to ArgCMV and provide extensive benchmark results by experimenting with\nexisting baselines and latest open source models. This work introduces a novel\nKP extraction dataset for long-context online discussions, setting the stage\nfor the next generation of LLM-driven summarization research."}
{"id": "2508.16596", "pdf": "https://arxiv.org/pdf/2508.16596.pdf", "abs": "https://arxiv.org/abs/2508.16596", "title": "Using Generative AI to Uncover What Drives Player Enjoyment in PC and VR Games", "authors": ["Hisham Abdelqader"], "categories": ["cs.HC", "cs.SI"], "comment": "The Steam dataset used in this study can be accessed at:\n  https://data.mendeley.com/datasets/jxy85cr3th/2", "summary": "As video games continue to evolve, understanding what drives player enjoyment\nremains a key challenge. Player reviews provide valuable insights, but their\nunstructured nature makes large-scale analysis difficult. This study applies\ngenerative AI and machine learning, leveraging Microsoft Phi-4 LLM and XGBoost,\nto quantify and analyze game reviews from Steam and Meta Quest stores. The\napproach converts qualitative feedback into structured data, enabling\ncomprehensive evaluation of key game design elements, monetization models, and\nplatform-specific trends. The findings reveal distinct patterns in player\npreferences across PC and VR games, highlighting factors that contribute to\nhigher player satisfaction. By integrating Google Cloud for largescale data\nstorage and processing, this study establishes a scalable framework for game\nreview analysis. The study's insights offer actionable guidance for game\ndevelopers, helping optimize game mechanics, pricing strategies, and player\nengagement."}
{"id": "2508.19587", "pdf": "https://arxiv.org/pdf/2508.19587.pdf", "abs": "https://arxiv.org/abs/2508.19587", "title": "Towards stable AI systems for Evaluating Arabic Pronunciations", "authors": ["Hadi Zaatiti", "Hatem Hajri", "Osama Abdullah", "Nader Masmoudi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and\nsentence-level transcription, yet struggle to classify isolated letters. In\nthis study, we show that this phoneme-level task, crucial for language\nlearning, speech therapy, and phonetic research, is challenging because\nisolated letters lack co-articulatory cues, provide no lexical context, and\nlast only a few hundred milliseconds. Recogniser systems must therefore rely\nsolely on variable acoustic cues, a difficulty heightened by Arabic's emphatic\n(pharyngealized) consonants and other sounds with no close analogues in many\nlanguages. This study introduces a diverse, diacritised corpus of isolated\nArabic letters and demonstrates that state-of-the-art wav2vec 2.0 models\nachieve only 35% accuracy on it. Training a lightweight neural network on\nwav2vec embeddings raises performance to 65%. However, adding a small amplitude\nperturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we\napply adversarial training, limiting the noisy-speech drop to 9% while\npreserving clean-speech accuracy. We detail the corpus, training pipeline, and\nevaluation protocol, and release, on demand, data and code for reproducibility.\nFinally, we outline future work extending these methods to word- and\nsentence-level frameworks, where precise letter pronunciation remains critical."}
{"id": "2402.01292", "pdf": "https://arxiv.org/pdf/2402.01292.pdf", "abs": "https://arxiv.org/abs/2402.01292", "title": "From Evidence to Decision: Exploring Evaluative AI", "authors": ["Thao Le", "Tim Miller", "Liz Sonenberg", "Ronal Singh", "H. Peter Soyer"], "categories": ["cs.AI", "cs.HC"], "comment": "This paper is an extension of a prior work that was published at ECAI\n  2024 and is currently under review at a journal", "summary": "This paper presents a hypothesis-driven approach to improve AI-supported\ndecision-making that is based on the Evaluative AI paradigm - a conceptual\nframework that proposes providing users with evidence for or against a given\nhypothesis. We propose an implementation of Evaluative AI by extending the\nWeight of Evidence framework, leading to hypothesis-driven models that support\nboth tabular and image data. We demonstrate the application of the new\ndecision-support approach in two domains: housing price prediction and skin\ncancer diagnosis. The findings show promising results in improving human\ndecisions, as well as providing insights on the strengths and weaknesses of\ndifferent decision-support approaches."}
{"id": "2508.19594", "pdf": "https://arxiv.org/pdf/2508.19594.pdf", "abs": "https://arxiv.org/abs/2508.19594", "title": "Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs", "authors": ["Jun Bai", "Minghao Tong", "Yang Liu", "Zixia Jia", "Zilong Zheng"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Main", "summary": "Context faithfulness is essential for reliable reasoning in context-dependent\nscenarios. However, large language models often struggle to ground their\noutputs in the provided context, resulting in irrelevant responses. Inspired by\nthe emergent expert specialization observed in mixture-of-experts\narchitectures, this work investigates whether certain experts exhibit\nspecialization in context utilization, offering a potential pathway toward\ntargeted optimization for improved context faithfulness. To explore this, we\npropose Router Lens, a method that accurately identifies context-faithful\nexperts. Our analysis reveals that these experts progressively amplify\nattention to relevant contextual information, thereby enhancing context\ngrounding. Building on this insight, we introduce Context-faithful Expert\nFine-Tuning (CEFT), a lightweight optimization approach that selectively\nfine-tunes context-faithful experts. Experiments across a wide range of\nbenchmarks and models demonstrate that CEFT matches or surpasses the\nperformance of full fine-tuning while being significantly more efficient."}
{"id": "2508.03037", "pdf": "https://arxiv.org/pdf/2508.03037.pdf", "abs": "https://arxiv.org/abs/2508.03037", "title": "When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025", "authors": ["Ariya Mukherjee-Gandhi", "Oliver Muellerklein"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "23 pages, 7 figures, 8 tables", "summary": "As generative AI continues to reshape artistic production and alternate modes\nof human expression, artists whose livelihoods are most directly affected have\nraised urgent concerns about consent, transparency, and the future of creative\nlabor. However, the voices of artists are often marginalized in dominant public\nand scholarly discourse. This study presents a twelve-year analysis, from 2013\nto 2025, of English-language discourse surrounding AI-generated art. It draws\nfrom 439 curated 500-word excerpts sampled from opinion articles, news reports,\nblogs, legal filings, and spoken-word transcripts. Through a reproducible\nmethodology, we identify five stable thematic clusters and uncover a\nmisalignment between artists' perceptions and prevailing media narratives. Our\nfindings highlight how the use of technical jargon can function as a subtle\nform of gatekeeping, often sidelining the very issues artists deem most urgent.\nOur work provides a BERTopic-based methodology and a multimodal baseline for\nfuture research, alongside a clear call for deeper, transparency-driven\nengagement with artist perspectives in the evolving AI-creative landscape."}
{"id": "2508.19614", "pdf": "https://arxiv.org/pdf/2508.19614.pdf", "abs": "https://arxiv.org/abs/2508.19614", "title": "LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation", "authors": ["Yang Sun", "Lixin Zou", "Dan Luo", "Zhiyong Xie", "Long Zhang", "Liming Dong", "Yunwei Zhao", "Xixun Lin", "Yanxiong Lu", "Chenliang Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) incorporates external knowledge into\nlarge language models (LLMs), improving their adaptability to downstream tasks\nand enabling information updates. Surprisingly, recent empirical evidence\ndemonstrates that injecting noise into retrieved relevant documents\nparadoxically facilitates exploitation of external knowledge and improves\ngeneration quality. Although counterintuitive and challenging to apply in\npractice, this phenomenon enables granular control and rigorous analysis of how\nLLMs integrate external knowledge. Therefore, in this paper, we intervene on\nnoise injection and establish a layer-specific functional demarcation within\nthe LLM: shallow layers specialize in local context modeling, intermediate\nlayers focus on integrating long-range external factual knowledge, and deeper\nlayers primarily rely on parametric internal knowledge. Building on this\ninsight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that\ndirectly combines representations from an intermediate layer with final-layer\ndecoding outputs to fully exploit the external factual knowledge. To identify\nthe optimal intermediate layer, we introduce an internal knowledge score (IKS)\ncriterion that selects the layer with the lowest IKS value in the latter half\nof layers. Experimental results across multiple benchmarks demonstrate that LFD\nhelps RAG systems more effectively surface retrieved context knowledge with\nminimal cost."}
{"id": "2508.19633", "pdf": "https://arxiv.org/pdf/2508.19633.pdf", "abs": "https://arxiv.org/abs/2508.19633", "title": "A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection", "authors": ["Chong Tian", "Qirong Ho", "Xiuying Chen"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Rapid LLM advancements heighten fake news risks by enabling the automatic\ngeneration of increasingly sophisticated misinformation. Previous detection\nmethods, including fine-tuned small models or LLM-based detectors, often\nstruggle with its dynamically evolving nature. In this work, we propose a novel\nframework called the Symbolic Adversarial Learning Framework (SALF), which\nimplements an adversarial training paradigm by an agent symbolic learning\noptimization process, rather than relying on numerical updates. SALF introduces\na paradigm where the generation agent crafts deceptive narratives, and the\ndetection agent uses structured debates to identify logical and factual flaws\nfor detection, and they iteratively refine themselves through such adversarial\ninteractions. Unlike traditional neural updates, we represent agents using\nagent symbolic learning, where learnable weights are defined by agent prompts,\nand simulate back-propagation and gradient descent by operating on natural\nlanguage representations of weights, loss, and gradients. Experiments on two\nmultilingual benchmark datasets demonstrate SALF's effectiveness, showing it\ngenerates sophisticated fake news that degrades state-of-the-art detection\nperformance by up to 53.4% in Chinese and 34.2% in English on average. SALF\nalso refines detectors, improving detection of refined content by up to 7.7%.\nWe hope our work inspires further exploration into more robust, adaptable fake\nnews detection systems."}
{"id": "2508.19665", "pdf": "https://arxiv.org/pdf/2508.19665.pdf", "abs": "https://arxiv.org/abs/2508.19665", "title": "Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design", "authors": ["Giovanni Pollo", "Andrei Mihai Albu", "Alessio Burrello", "Daniele Jahier Pagliari", "Cristian Tesconi", "Loris Panaro", "Dario Soldi", "Fabio Autieri", "Sara Vinco"], "categories": ["cs.CL"], "comment": null, "summary": "The recent advancements of the automotive sector demand robust co-simulation\nmethodologies that enable early validation and seamless integration across\nhardware and software domains. However, the lack of standardized interfaces and\nthe dominance of proprietary simulation platforms pose significant challenges\nto collaboration, scalability, and IP protection. To address these limitations,\nthis paper presents an approach for automatically wrapping SystemC models by\nusing the Functional Mock-up Interface (FMI) standard. This method combines the\nmodeling accuracy and fast time-to-market of SystemC with the interoperability\nand encapsulation benefits of FMI, enabling secure and portable integration of\nembedded components into co-simulation workflows. We validate the proposed\nmethodology on real-world case studies, demonstrating its effectiveness with\ncomplex designs."}
{"id": "2508.19667", "pdf": "https://arxiv.org/pdf/2508.19667.pdf", "abs": "https://arxiv.org/abs/2508.19667", "title": "Survey of Specialized Large Language Model", "authors": ["Chenghan Yang", "Ruiyu Zhao", "Yang Liu", "Ling Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 1 figures", "summary": "The rapid evolution of specialized large language models (LLMs) has\ntransitioned from simple domain adaptation to sophisticated native\narchitectures, marking a paradigm shift in AI development. This survey\nsystematically examines this progression across healthcare, finance, legal, and\ntechnical domains. Besides the wide use of specialized LLMs, technical\nbreakthrough such as the emergence of domain-native designs beyond fine-tuning,\ngrowing emphasis on parameter efficiency through sparse computation and\nquantization, increasing integration of multimodal capabilities and so on are\napplied to recent LLM agent. Our analysis reveals how these innovations address\nfundamental limitations of general-purpose LLMs in professional applications,\nwith specialized models consistently performance gains on domain-specific\nbenchmarks. The survey further highlights the implications for E-Commerce field\nto fill gaps in the field."}
{"id": "2508.19689", "pdf": "https://arxiv.org/pdf/2508.19689.pdf", "abs": "https://arxiv.org/abs/2508.19689", "title": "Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality", "authors": ["Xiaoying Zhang"], "categories": ["cs.CL"], "comment": "179 pages", "summary": "Developing adaptable, extensible, and accurate task bots with minimal or zero\nhuman intervention is a significant challenge in dialog research. This thesis\nexamines the obstacles and potential solutions for creating such bots, focusing\non innovative techniques that enable bots to learn and adapt autonomously in\nconstantly changing environments."}
{"id": "2508.19720", "pdf": "https://arxiv.org/pdf/2508.19720.pdf", "abs": "https://arxiv.org/abs/2508.19720", "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models", "authors": ["Yilin Wang", "Heng Wang", "Yuyang Bai", "Minnan Luo"], "categories": ["cs.CL"], "comment": null, "summary": "In Large Language Models (LLMs) generation, there exist knowledge conflicts\nand scenarios where parametric knowledge contradicts knowledge provided in the\ncontext. Previous works studied tuning, decoding algorithms, or locating and\nediting context-aware neurons to adapt LLMs to be faithful to new contextual\nknowledge. However, they are usually inefficient or ineffective for large\nmodels, not workable for black-box models, or unable to continuously adjust\nLLMs' sensitivity to the knowledge provided in the context. To mitigate these\nproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a\nsimple framework that can steer LLMs' sensitivity to contextual knowledge\ncontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.\nproxy models) and use the difference in their output distributions to shift the\noriginal distribution of an LLM without modifying the LLM weights. In the\nevaluation process, we not only design synthetic data and fine-grained metrics\nto measure models' sensitivity to contextual knowledge but also use a real\nconflict dataset to validate CSKS's practical efficacy. Extensive experiments\ndemonstrate that our framework achieves continuous and precise control over\nLLMs' sensitivity to contextual knowledge, enabling both increased sensitivity\nand reduced sensitivity, thereby allowing LLMs to prioritize either contextual\nor parametric knowledge as needed flexibly. Our data and code are available at\nhttps://github.com/OliveJuiceLin/CSKS."}
{"id": "2508.19721", "pdf": "https://arxiv.org/pdf/2508.19721.pdf", "abs": "https://arxiv.org/abs/2508.19721", "title": "CAMÕES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese", "authors": ["Carlos Carvalho", "Francisco Teixeira", "Catarina Botelho", "Anna Pompili", "Rubén Solera-Ureña", "Sérgio Paulo", "Mariana Julião", "Thomas Rolland", "John Mendonça", "Diogo Pereira", "Isabel Trancoso", "Alberto Abad"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to ASRU 2025", "summary": "Existing resources for Automatic Speech Recognition in Portuguese are mostly\nfocused on Brazilian Portuguese, leaving European Portuguese (EP) and other\nvarieties under-explored. To bridge this gap, we introduce CAM\\~OES, the first\nopen framework for EP and other Portuguese varieties. It consists of (1) a\ncomprehensive evaluation benchmark, including 46h of EP test data spanning\nmultiple domains; and (2) a collection of state-of-the-art models. For the\nlatter, we consider multiple foundation models, evaluating their zero-shot and\nfine-tuned performances, as well as E-Branchformer models trained from scratch.\nA curated set of 425h of EP was used for both fine-tuning and training. Our\nresults show comparable performance for EP between fine-tuned foundation models\nand the E-Branchformer. Furthermore, the best-performing models achieve\nrelative improvements above 35% WER, compared to the strongest zero-shot\nfoundation model, establishing a new state-of-the-art for EP and other\nvarieties."}
{"id": "2508.19724", "pdf": "https://arxiv.org/pdf/2508.19724.pdf", "abs": "https://arxiv.org/abs/2508.19724", "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks", "authors": ["Aritra Dutta", "Swapnanil Mukherjee", "Deepanway Ghosal", "Somak Aditya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models."}
{"id": "2508.19740", "pdf": "https://arxiv.org/pdf/2508.19740.pdf", "abs": "https://arxiv.org/abs/2508.19740", "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval", "authors": ["Wenhao Li", "Yuxin Zhang", "Gen Luo", "Haiyuan Wan", "Ziyang Gong", "Fei Chao", "Rongrong Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."}
{"id": "2508.19758", "pdf": "https://arxiv.org/pdf/2508.19758.pdf", "abs": "https://arxiv.org/abs/2508.19758", "title": "Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval", "authors": ["Yixuan Tang", "Yuanyuan Shi", "Yiqun Sun", "Anthony Kum Hoe Tung"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by EMNLP 2025", "summary": "Access to diverse perspectives is essential for understanding real-world\nevents, yet most news retrieval systems prioritize textual relevance, leading\nto redundant results and limited viewpoint exposure. We propose NEWSCOPE, a\ntwo-stage framework for diverse news retrieval that enhances event coverage by\nexplicitly modeling semantic variation at the sentence level. The first stage\nretrieves topically relevant content using dense retrieval, while the second\nstage applies sentence-level clustering and diversity-aware re-ranking to\nsurface complementary information. To evaluate retrieval diversity, we\nintroduce three interpretable metrics, namely Average Pairwise Distance,\nPositive Cluster Coverage, and Information Density Ratio, and construct two\nparagraph-level benchmarks: LocalNews and DSGlobal. Experiments show that\nNEWSCOPE consistently outperforms strong baselines, achieving significantly\nhigher diversity without compromising relevance. Our results demonstrate the\neffectiveness of fine-grained, interpretable modeling in mitigating redundancy\nand promoting comprehensive event understanding. The data and code are\navailable at https://github.com/tangyixuan/NEWSCOPE."}
{"id": "2508.19764", "pdf": "https://arxiv.org/pdf/2508.19764.pdf", "abs": "https://arxiv.org/abs/2508.19764", "title": "Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance", "authors": ["Pedro Henrique Luz de Araujo", "Paul Röttger", "Dirk Hovy", "Benjamin Roth"], "categories": ["cs.CL"], "comment": "30 pages, 29 figures, accepted to EMNLP 2025", "summary": "Expert persona prompting -- assigning roles such as expert in math to\nlanguage models -- is widely used for task improvement. However, prior work\nshows mixed results on its effectiveness, and does not consider when and why\npersonas should improve performance. We analyze the literature on persona\nprompting for task improvement and distill three desiderata: 1) performance\nadvantage of expert personas, 2) robustness to irrelevant persona attributes,\nand 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs\nacross 27 tasks with respect to these desiderata. We find that expert personas\nusually lead to positive or non-significant performance changes. Surprisingly,\nmodels are highly sensitive to irrelevant persona details, with performance\ndrops of almost 30 percentage points. In terms of fidelity, we find that while\nhigher education, specialization, and domain-relatedness can boost performance,\ntheir effects are often inconsistent or negligible across tasks. We propose\nmitigation strategies to improve robustness -- but find they only work for the\nlargest, most capable models. Our findings underscore the need for more careful\npersona design and for evaluation schemes that reflect the intended effects of\npersona usage."}
{"id": "2508.19813", "pdf": "https://arxiv.org/pdf/2508.19813.pdf", "abs": "https://arxiv.org/abs/2508.19813", "title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables", "authors": ["Jie Zhang", "Changzai Pan", "Kaiwen Wei", "Sishi Xiong", "Yu Zhao", "Xiangyu Li", "Jiaxin Peng", "Xiaoyan Gu", "Jian Yang", "Wenhan Chang", "Zhenhe Wu", "Jiang Zhong", "Shuangyong Song", "Yongxiang Li", "Xuelong Li"], "categories": ["cs.CL"], "comment": null, "summary": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench. Source code and data will be available after acceptance."}
{"id": "2508.19828", "pdf": "https://arxiv.org/pdf/2508.19828.pdf", "abs": "https://arxiv.org/abs/2508.19828", "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning", "authors": ["Sikuan Yan", "Xiufeng Yang", "Zuchao Huang", "Ercong Nie", "Zifeng Ding", "Zonggen Li", "Xiaowen Ma", "Hinrich Schütze", "Volker Tresp", "Yunpu Ma"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations\n{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant\nentries and reasons over them to produce an answer. Both agents are fine-tuned\nwith outcome-driven RL (PPO and GRPO), enabling adaptive memory management and\nuse with minimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the most\ncompetitive existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behaviors in LLMs, pointing toward richer, more persistent\nreasoning systems."}
{"id": "2508.19831", "pdf": "https://arxiv.org/pdf/2508.19831.pdf", "abs": "https://arxiv.org/abs/2508.19831", "title": "Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis", "authors": ["Anusha Kamath", "Kanishk Singla", "Rakesh Paul", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages."}
{"id": "2508.19836", "pdf": "https://arxiv.org/pdf/2508.19836.pdf", "abs": "https://arxiv.org/abs/2508.19836", "title": "Scalable and consistent few-shot classification of survey responses using text embeddings", "authors": ["Jonas Timmann Mjaaland", "Markus Fleten Kreutzer", "Halvor Tyseng", "Rebeckah K. Fussell", "Gina Passante", "N. G. Holmes", "Anders Malthe-Sørenssen", "Tor Ole B. Odden"], "categories": ["cs.CL", "physics.ed-ph"], "comment": null, "summary": "Qualitative analysis of open-ended survey responses is a commonly-used\nresearch method in the social sciences, but traditional coding approaches are\noften time-consuming and prone to inconsistency. Existing solutions from\nNatural Language Processing such as supervised classifiers, topic modeling\ntechniques, and generative large language models have limited applicability in\nqualitative analysis, since they demand extensive labeled data, disrupt\nestablished qualitative workflows, and/or yield variable results. In this\npaper, we introduce a text embedding-based classification framework that\nrequires only a handful of examples per category and fits well with standard\nqualitative workflows. When benchmarked against human analysis of a conceptual\nphysics survey consisting of 2899 open-ended responses, our framework achieves\na Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in\nan exhaustive coding scheme. We further show how performance of this framework\nimproves with fine-tuning of the text embedding model, and how the method can\nbe used to audit previously-analyzed datasets. These findings demonstrate that\ntext embedding-assisted coding can flexibly scale to thousands of responses\nwithout sacrificing interpretability, opening avenues for deductive qualitative\nanalysis at scale."}
{"id": "2508.19856", "pdf": "https://arxiv.org/pdf/2508.19856.pdf", "abs": "https://arxiv.org/abs/2508.19856", "title": "TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation", "authors": ["Shashi Kumar", "Srikanth Madikeri", "Esaú Villatoro-Tello", "Sergio Burdisso", "Pradeep Rangappa", "Andrés Carofilis", "Petr Motlicek", "Karthik Pandia", "Shankar Venkatesan", "Kadri Hacioğlu", "Andreas Stolcke"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to IEEE ASRU 2025. Copyright\\copyright 2025 IEEE", "summary": "Token-based multitasking frameworks like TokenVerse require all training\nutterances to have labels for all tasks, hindering their ability to leverage\npartially annotated datasets and scale effectively. We propose TokenVerse++,\nwhich introduces learnable vectors in the acoustic embedding space of the\nXLSR-Transducer ASR model for dynamic task activation. This core mechanism\nenables training with utterances labeled for only a subset of tasks, a key\nadvantage over TokenVerse. We demonstrate this by successfully integrating a\ndataset with partial labels, specifically for ASR and an additional task,\nlanguage identification, improving overall performance. TokenVerse++ achieves\nresults on par with or exceeding TokenVerse across multiple tasks, establishing\nit as a more practical multitask alternative without sacrificing ASR\nperformance."}
{"id": "2508.19873", "pdf": "https://arxiv.org/pdf/2508.19873.pdf", "abs": "https://arxiv.org/abs/2508.19873", "title": "Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning", "authors": ["Vanessa Toborek", "Sebastian Müller", "Tim Selbach", "Tamás Horváth", "Christian Bauckhage"], "categories": ["cs.CL"], "comment": "Presented at ICNLSP 2025; to appear in the ACL Anthology; received\n  the Best Short Paper Award", "summary": "Curriculum learning (CL) aims to improve training by presenting data from\n\"easy\" to \"hard\", yet defining and measuring linguistic difficulty remains an\nopen challenge. We investigate whether human-curated simple language can serve\nas an effective signal for CL. Using the article-level labels from the Simple\nWikipedia corpus, we compare label-based curricula to competence-based\nstrategies relying on shallow heuristics. Our experiments with a BERT-tiny\nmodel show that adding simple data alone yields no clear benefit. However,\nstructuring it via a curriculum -- especially when introduced first --\nconsistently improves perplexity, particularly on simple language. In contrast,\ncompetence-based curricula lead to no consistent gains over random ordering,\nprobably because they fail to effectively separate the two classes. Our results\nsuggest that human intuition about linguistic difficulty can guide CL for\nlanguage model pre-training."}
{"id": "2508.19883", "pdf": "https://arxiv.org/pdf/2508.19883.pdf", "abs": "https://arxiv.org/abs/2508.19883", "title": "AI-Powered Detection of Inappropriate Language in Medical School Curricula", "authors": ["Chiman Salavati", "Shannon Song", "Scott A. Hale", "Roberto E. Montenegro", "Shiri Dori-Hacohen", "Fabricio Murai"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.1; I.2.7"], "comment": "Accepted at 2025 AAAI/ACM AI, Ethics and Society Conference (AIES'25)", "summary": "The use of inappropriate language -- such as outdated, exclusionary, or\nnon-patient-centered terms -- medical instructional materials can significantly\ninfluence clinical training, patient interactions, and health outcomes. Despite\ntheir reputability, many materials developed over past decades contain examples\nnow considered inappropriate by current medical standards. Given the volume of\ncurricular content, manually identifying instances of inappropriate use of\nlanguage (IUL) and its subcategories for systematic review is prohibitively\ncostly and impractical. To address this challenge, we conduct a first-in-class\nevaluation of small language models (SLMs) fine-tuned on labeled data and\npre-trained LLMs with in-context learning on a dataset containing approximately\n500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL\nclassifier, (2) subcategory-specific binary classifiers, (3) a multilabel\nclassifier, and (4) a two-stage hierarchical pipeline for general IUL detection\nfollowed by multilabel classification. For LLMs, we consider variations of\nprompts that include subcategory definitions and/or shots. We found that both\nLLama-3 8B and 70B, even with carefully curated shots, are largely outperformed\nby SLMs. While the multilabel classifier performs best on annotated data,\nsupplementing training with unflagged excerpts as negative examples boosts the\nspecific classifiers' AUC by up to 25%, making them most effective models for\nmitigating harmful language in medical curricula."}
{"id": "2508.19887", "pdf": "https://arxiv.org/pdf/2508.19887.pdf", "abs": "https://arxiv.org/abs/2508.19887", "title": "Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement", "authors": ["Mohammed Rakibul Hasan", "Rafi Majid", "Ahanaf Tahmid"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question\nAnswering (VQA) Dataset in Bangla, a widely used, low-resource language in\nmultimodal AI research. The majority of existing datasets are either manually\nannotated with an emphasis on a specific domain, query type, or answer type or\nare constrained by niche answer formats. In order to mitigate human-induced\nerrors and guarantee lucidity, we implemented a multilingual LLM-assisted\ntranslation refinement pipeline. This dataset overcomes the issues of\nlow-quality translations from multilingual sources. The dataset comprises\n52,650 question-answer pairs across 4750+ images. Questions are classified into\nthree distinct answer types: nominal (short descriptive), quantitative\n(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive\nopen-source, high-quality VQA benchmark in Bangla, aiming to advance research\nin low-resource multimodal learning and facilitate the development of more\ninclusive AI systems."}
{"id": "2508.19903", "pdf": "https://arxiv.org/pdf/2508.19903.pdf", "abs": "https://arxiv.org/abs/2508.19903", "title": "Logical Reasoning with Outcome Reward Models for Test-Time Scaling", "authors": ["Ramya Keerthy Thatikonda", "Wray Buntine", "Ehsan Shareghi"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Logical reasoning is a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs), as it reflects their ability to derive valid\nconclusions from given premises. While the combination of test-time scaling\nwith dedicated outcome or process reward models has opened up new avenues to\nenhance LLMs performance in complex reasoning tasks, this space is\nunder-explored in deductive logical reasoning. We present a set of Outcome\nReward Models (ORMs) for deductive reasoning. To train the ORMs we mainly\ngenerate data using Chain-of-Thought (CoT) with single and multiple samples.\nAdditionally, we propose a novel tactic to further expand the type of errors\ncovered in the training dataset of the ORM. In particular, we propose an echo\ngeneration technique that leverages LLMs' tendency to reflect incorrect\nassumptions made in prompts to extract additional training data, covering\npreviously unexplored error types. While a standard CoT chain may contain\nerrors likely to be made by the reasoner, the echo strategy deliberately steers\nthe model toward incorrect reasoning. We show that ORMs trained on CoT and\necho-augmented data demonstrate improved performance on the FOLIO, JustLogic,\nand ProverQA datasets across four different LLMs."}
{"id": "2508.19919", "pdf": "https://arxiv.org/pdf/2508.19919.pdf", "abs": "https://arxiv.org/abs/2508.19919", "title": "Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems", "authors": ["Jingyu Guo", "Yingying Xu"], "categories": ["cs.CL"], "comment": null, "summary": "While stereotypes are well-documented in human social interactions, AI\nsystems are often presumed to be less susceptible to such biases. Previous\nstudies have focused on biases inherited from training data, but whether\nstereotypes can emerge spontaneously in AI agent interactions merits further\nexploration. Through a novel experimental framework simulating workplace\ninteractions with neutral initial conditions, we investigate the emergence and\nevolution of stereotypes in LLM-based multi-agent systems. Our findings reveal\nthat (1) LLM-Based AI agents develop stereotype-driven biases in their\ninteractions despite beginning without predefined biases; (2) stereotype\neffects intensify with increased interaction rounds and decision-making power,\nparticularly after introducing hierarchical structures; (3) these systems\nexhibit group effects analogous to human social behavior, including halo\neffects, confirmation bias, and role congruity; and (4) these stereotype\npatterns manifest consistently across different LLM architectures. Through\ncomprehensive quantitative analysis, these findings suggest that stereotype\nformation in AI systems may arise as an emergent property of multi-agent\ninteractions, rather than merely from training data biases. Our work\nunderscores the need for future research to explore the underlying mechanisms\nof this phenomenon and develop strategies to mitigate its ethical impacts."}
{"id": "2508.19922", "pdf": "https://arxiv.org/pdf/2508.19922.pdf", "abs": "https://arxiv.org/abs/2508.19922", "title": "HEAL: A Hypothesis-Based Preference-Aware Analysis Framework", "authors": ["Yifu Huo", "Chenglong Wang", "Qiren Zhu", "Shunjie Xing", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jinbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Findings", "summary": "Preference optimization methods like DPO have achieved remarkable performance\nin LLM alignment. However, the evaluation for these methods relies on a single\nresponse and overlooks other potential outputs, which could also be generated\nin real-world applications within this hypothetical space. To address this\nissue, this paper presents a \\textbf{H}ypothesis-based\nPr\\textbf{E}ference-aware \\textbf{A}na\\textbf{L}ysis Framework (HEAL), a novel\nevaluation paradigm that formulates preference alignment as a re-ranking\nprocess within hypothesis spaces. The framework incorporates two complementary\nmetrics: ranking accuracy for evaluating ordinal consistency and preference\nstrength correlation for assessing continuous alignment. To facilitate this\nframework, we develop UniHypoBench, a unified hypothesis benchmark constructed\nfrom diverse instruction-response pairs. Through extensive experiments based on\nHEAL, with a particular focus on the intrinsic mechanisms of preference\nlearning, we demonstrate that current preference learning methods can\neffectively capture preferences provided by proxy models while simultaneously\nsuppressing negative samples. These findings contribute to preference learning\nresearch through two significant avenues. Theoretically, we introduce\nhypothesis space analysis as an innovative paradigm for understanding\npreference alignment. Practically, HEAL offers researchers robust diagnostic\ntools for refining preference optimization methods, while our empirical results\nidentify promising directions for developing more advanced alignment algorithms\ncapable of comprehensive preference capture."}
{"id": "2508.19966", "pdf": "https://arxiv.org/pdf/2508.19966.pdf", "abs": "https://arxiv.org/abs/2508.19966", "title": "Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation", "authors": ["Slimane Bellaouar", "Attia Nehar", "Soumia Souffi", "Mounia Bouameur"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 7 figures", "summary": "Despite its significance, Arabic, a linguistically rich and morphologically\ncomplex language, faces the challenge of being under-resourced. The scarcity of\nlarge annotated datasets hampers the development of accurate tools for\nsubjectivity analysis in Arabic. Recent advances in deep learning and\nTransformers have proven highly effective for text classification in English\nand French. This paper proposes a new approach for subjectivity assessment in\nArabic textual data. To address the dearth of specialized annotated datasets,\nwe developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic\ndatasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we\nfine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and\nArabianGPT) on AraDhati+ for effective subjectivity classification.\nFurthermore, we experimented with an ensemble decision approach to harness the\nstrengths of individual models. Our approach achieves a remarkable accuracy of\n97.79\\,\\% for Arabic subjectivity classification. Results demonstrate the\neffectiveness of the proposed approach in addressing the challenges posed by\nlimited resources in Arabic language processing."}
{"id": "2508.19982", "pdf": "https://arxiv.org/pdf/2508.19982.pdf", "abs": "https://arxiv.org/abs/2508.19982", "title": "Diffusion Language Models Know the Answer Before Decoding", "authors": ["Pengxiang Li", "Yefan Zhou", "Dilxat Muhtar", "Lu Yin", "Shilin Yan", "Li Shen", "Yi Liang", "Soroush Vosoughi", "Shiwei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet."}
{"id": "2508.19988", "pdf": "https://arxiv.org/pdf/2508.19988.pdf", "abs": "https://arxiv.org/abs/2508.19988", "title": "AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios", "authors": ["Lisa Alazraki", "Lihu Chen", "Ana Brassard", "Joe Stacey", "Hossein A. Rahmani", "Marek Rei"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved high accuracy on complex\ncommonsense and mathematical problems that involve the composition of multiple\nreasoning steps. However, current compositional benchmarks testing these skills\ntend to focus on either commonsense or math reasoning, whereas LLM agents\nsolving real-world tasks would require a combination of both. In this work, we\nintroduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each\ncompositional task requires a commonsense reasoning step and a math reasoning\nstep. We test it on 61 LLMs of different sizes, model families, and training\nstrategies. We find that LLMs can usually solve both steps in isolation, yet\ntheir accuracy drops by ~30% on average when the two are combined. This is a\nsubstantially greater performance gap than the one we observe in prior\ncompositional benchmarks that combine multiple steps of the same reasoning\ntype. In contrast, non-expert human annotators can solve the compositional\nquestions and the individual steps in AgentCoMa with similarly high accuracy.\nFurthermore, we conduct a series of interpretability studies to better\nunderstand the performance gap, examining neuron patterns, attention maps and\nmembership inference. Our work underscores a substantial degree of model\nbrittleness in the context of mixed-type compositional reasoning and offers a\ntest bed for future improvement."}
{"id": "2508.19993", "pdf": "https://arxiv.org/pdf/2508.19993.pdf", "abs": "https://arxiv.org/abs/2508.19993", "title": "MathBuddy: A Multimodal System for Affective Math Tutoring", "authors": ["Debanjana Kar", "Leopold Böss", "Dacia Braca", "Sebastian Maximilian Dennerlein", "Nina Christine Hubig", "Philipp Wintersberger", "Yufang Hou"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The rapid adoption of LLM-based conversational systems is already\ntransforming the landscape of educational technology. However, the current\nstate-of-the-art learning models do not take into account the student's\naffective states. Multiple studies in educational psychology support the claim\nthat positive or negative emotional states can impact a student's learning\ncapabilities. To bridge this gap, we present MathBuddy, an emotionally aware\nLLM-powered Math Tutor, which dynamically models the student's emotions and\nmaps them to relevant pedagogical strategies, making the tutor-student\nconversation a more empathetic one. The student's emotions are captured from\nthe conversational text as well as from their facial expressions. The student's\nemotions are aggregated from both modalities to confidently prompt our LLM\nTutor for an emotionally-aware response. We have effectively evaluated our\nmodel using automatic evaluation metrics across eight pedagogical dimensions\nand user studies. We report a massive 23 point performance gain using the win\nrate and a 3 point gain at an overall level using DAMR scores which strongly\nsupports our hypothesis of improving LLM-based tutor's pedagogical abilities by\nmodeling students' emotions."}
{"id": "2508.19996", "pdf": "https://arxiv.org/pdf/2508.19996.pdf", "abs": "https://arxiv.org/abs/2508.19996", "title": "ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning", "authors": ["Yiming Du", "Yifan Xiang", "Bin Liang", "Dahua Lin", "Kam-Fai Wong", "Fei Tan"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning multi-turn dialogue systems requires high-quality supervision but\noften suffers from degraded performance when exposed to low-quality data.\nSupervision errors in early turns can propagate across subsequent turns,\nundermining coherence and response quality. Existing methods typically address\ndata quality via static prefiltering, which decouples quality control from\ntraining and fails to mitigate turn-level error propagation. In this context,\nwe propose ReSURE (Regularizing Supervision UnREliability), an adaptive\nlearning method that dynamically down-weights unreliable supervision without\nexplicit filtering. ReSURE estimates per-turn loss distributions using\nWelford's online statistics and reweights sample losses on the fly accordingly.\nExperiments on both single-source and mixed-quality datasets show improved\nstability and response quality. Notably, ReSURE enjoys positive Spearman\ncorrelations (0.21 ~ 1.0 across multiple benchmarks) between response scores\nand number of samples regardless of data quality, which potentially paves the\nway for utilizing large-scale data effectively. Code is publicly available at\nhttps://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training."}
{"id": "2508.19997", "pdf": "https://arxiv.org/pdf/2508.19997.pdf", "abs": "https://arxiv.org/abs/2508.19997", "title": "Selective Retrieval-Augmentation for Long-Tail Legal Text Classification", "authors": ["Boheng Mao"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Legal text classification is a fundamental NLP task in the legal domain.\nBenchmark datasets in this area often exhibit a long-tail label distribution,\nwhere many labels are underrepresented, leading to poor model performance on\nrare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a\nsolution to this problem. SRA focuses on augmenting samples belonging to\nlow-frequency labels in the training set, preventing the introduction of noise\nfor well-represented classes, and requires no changes to the model\narchitecture. Retrieval is performed only from the training data to ensure\nthere is no potential information leakage, removing the need for external\ncorpora simultaneously. The proposed SRA method is tested on two legal text\nclassification benchmark datasets with long-tail distributions: LEDGAR\n(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA\nattains higher micro-F1 and macro-F1 scores compared to all current LexGLUE\nbaselines across both datasets, illustrating consistent improvements in\nlong-tail legal text classification. The code repository is available at:\nhttps://github.com/Boheng-Mao/sra-legal"}
{"id": "2508.20033", "pdf": "https://arxiv.org/pdf/2508.20033.pdf", "abs": "https://arxiv.org/abs/2508.20033", "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis", "authors": ["Liana Patel", "Negar Arabzadeh", "Harshit Gupta", "Ankita Sundar", "Ion Stoica", "Matei Zaharia", "Carlos Guestrin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The ability to research and synthesize knowledge is central to human\nexpertise and progress. An emerging class of systems promises these exciting\ncapabilities through generative research synthesis, performing retrieval over\nthe live web and synthesizing discovered sources into long-form, cited\nsummaries. However, evaluating such systems remains an open challenge: existing\nquestion-answering benchmarks focus on short-form factual responses, while\nexpert-curated datasets risk staleness and data contamination. Both fail to\ncapture the complexity and evolving nature of real research synthesis tasks. In\nthis work, we introduce DeepScholar-bench, a live benchmark and holistic,\nautomated evaluation framework designed to evaluate generative research\nsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv\npapers and focuses on a real research synthesis task: generating the related\nwork sections of a paper by retrieving, synthesizing, and citing prior\nresearch. Our evaluation framework holistically assesses performance across\nthree key dimensions, knowledge synthesis, retrieval quality, and\nverifiability. We also develop DeepScholar-base, a reference pipeline\nimplemented efficiently using the LOTUS API. Using the DeepScholar-bench\nframework, we perform a systematic evaluation of prior open-source systems,\nsearch AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that\nDeepScholar-base establishes a strong baseline, attaining competitive or higher\nperformance than each other method. We also find that DeepScholar-bench remains\nfar from saturated, with no system exceeding a score of $19\\%$ across all\nmetrics. These results underscore the difficulty of DeepScholar-bench, as well\nas its importance for progress towards AI systems capable of generative\nresearch synthesis. We make our code available at\nhttps://github.com/guestrin-lab/deepscholar-bench."}
{"id": "2508.20038", "pdf": "https://arxiv.org/pdf/2508.20038.pdf", "abs": "https://arxiv.org/abs/2508.20038", "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks", "authors": ["Sheng Liu", "Qiang Sheng", "Danding Wang", "Yang Li", "Guang Yang", "Juan Cao"], "categories": ["cs.CL"], "comment": "EMNLP 2025 findings", "summary": "Despite advances in improving large language model(LLM) to refuse to answer\nmalicious instructions, widely used LLMs remain vulnerable to jailbreak attacks\nwhere attackers generate instructions with distributions differing from safety\nalignment corpora. New attacks expose LLMs' inability to recognize unseen\nmalicious instructions, highlighting a critical distributional mismatch between\ntraining data and real-world attacks that forces developers into reactive\npatching cycles. To tackle this challenge, we propose IMAGINE, a synthesis\nframework that leverages embedding space distribution analysis to generate\njailbreak-like instructions. This approach effectively fills the distributional\ngap between authentic jailbreak patterns and safety alignment corpora. IMAGINE\nfollows an iterative optimization process that dynamically evolves text\ngeneration distributions across iterations, thereby augmenting the coverage of\nsafety alignment data distributions through synthesized data examples. Based on\nthe safety-aligned corpus enhanced through IMAGINE, our framework demonstrates\nsignificant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2\nwithout compromising their utility."}
{"id": "2508.20047", "pdf": "https://arxiv.org/pdf/2508.20047.pdf", "abs": "https://arxiv.org/abs/2508.20047", "title": "AraHealthQA 2025 Shared Task Description Paper", "authors": ["Hassan Alhuzali", "Farah Shamout", "Muhammad Abdul-Mageed", "Chaimae Abouzahir", "Mouath Abu-Daoud", "Ashwag Alasmari", "Walid Al-Eisawi", "Renad Al-Monef", "Ali Alqahtani", "Lama Ayash", "Nizar Habash", "Leen Kharouf"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question\nAnswering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located\nwith EMNLP 2025). This shared task addresses the paucity of high-quality Arabic\nmedical QA resources by offering two complementary tracks: {MentalQA}, focusing\non Arabic mental health Q\\&A (e.g., anxiety, depression, stigma reduction), and\n{MedArabiQ}, covering broader medical domains such as internal medicine,\npediatrics, and clinical decision making. Each track comprises multiple\nsubtasks, evaluation datasets, and standardized metrics, facilitating fair\nbenchmarking. The task was structured to promote modeling under realistic,\nmultilingual, and culturally nuanced healthcare contexts. We outline the\ndataset creation, task design and evaluation framework, participation\nstatistics, baseline systems, and summarize the overall outcomes. We conclude\nwith reflections on the performance trends observed and prospects for future\niterations in Arabic health QA."}
{"id": "2508.20068", "pdf": "https://arxiv.org/pdf/2508.20068.pdf", "abs": "https://arxiv.org/abs/2508.20068", "title": "11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis", "authors": ["Chengzu Li", "Wenshan Wu", "Huanyu Zhang", "Qingtao Li", "Zeyu Gao", "Yan Xia", "José Hernández-Orallo", "Ivan Vulić", "Furu Wei"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "9 pages, 4 figures (22 pages, 7 figures, 7 tables including\n  references and appendices)", "summary": "For human cognitive process, spatial reasoning and perception are closely\nentangled, yet the nature of this interplay remains underexplored in the\nevaluation of multimodal large language models (MLLMs). While recent MLLM\nadvancements show impressive performance on reasoning, their capacity for\nhuman-like spatial cognition remains an open question. In this work, we\nintroduce a systematic evaluation framework to assess the spatial reasoning\nabilities of state-of-the-art MLLMs relative to human performance. Central to\nour work is 11Plus-Bench, a high-quality benchmark derived from realistic\nstandardized spatial aptitude tests. 11Plus-Bench also features fine-grained\nexpert annotations of both perceptual complexity and reasoning process,\nenabling detailed instance-level analysis of model behavior. Through extensive\nexperiments across 14 MLLMs and human evaluation, we find that current MLLMs\nexhibit early signs of spatial cognition. Despite a large performance gap\ncompared to humans, MLLMs' cognitive profiles resemble those of humans in that\ncognitive effort correlates strongly with reasoning-related complexity.\nHowever, instance-level performance in MLLMs remains largely random, whereas\nhuman correctness is highly predictable and shaped by abstract pattern\ncomplexity. These findings highlight both emerging capabilities and limitations\nin current MLLMs' spatial reasoning capabilities and provide actionable\ninsights for advancing model design."}
{"id": "2508.19259", "pdf": "https://arxiv.org/pdf/2508.19259.pdf", "abs": "https://arxiv.org/abs/2508.19259", "title": "Capabilities of GPT-5 across critical domains: Is it the next breakthrough?", "authors": ["Georgios P. Georgiou"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "The accelerated evolution of large language models has raised questions about\ntheir comparative performance across domains of practical importance. GPT-4 by\nOpenAI introduced advances in reasoning, multimodality, and task\ngeneralization, establishing itself as a valuable tool in education, clinical\ndiagnosis, and academic writing, though it was accompanied by several flaws.\nReleased in August 2025, GPT-5 incorporates a system-of-models architecture\ndesigned for task-specific optimization and, based on both anecdotal accounts\nand emerging evidence from the literature, demonstrates stronger performance\nthan its predecessor in medical contexts. This study provides one of the first\nsystematic comparisons of GPT-4 and GPT-5 using human raters from linguistics\nand clinical fields. Twenty experts evaluated model-generated outputs across\nfive domains: lesson planning, assignment evaluation, clinical diagnosis,\nresearch generation, and ethical reasoning, based on predefined criteria.\nMixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in\nlesson planning, clinical diagnosis, research generation, and ethical\nreasoning, while both models performed comparably in assignment assessment. The\nfindings highlight the potential of GPT-5 to serve as a context-sensitive and\ndomain-specialized tool, offering tangible benefits for education, clinical\npractice, and academic research, while also advancing ethical reasoning. These\nresults contribute to one of the earliest empirical evaluations of the evolving\ncapabilities and practical promise of GPT-5."}
{"id": "2508.19262", "pdf": "https://arxiv.org/pdf/2508.19262.pdf", "abs": "https://arxiv.org/abs/2508.19262", "title": "Beat-Based Rhythm Quantization of MIDI Performances", "authors": ["Maximilian Wachter", "Sebastian Murgul", "Michael Heizmann"], "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Accepted to the Late Breaking Demo Papers of the 1st AES\n  International Conference on Artificial Intelligence and Machine Learning for\n  Audio (AIMLA LBDP), 2025", "summary": "We propose a transformer-based rhythm quantization model that incorporates\nbeat and downbeat information to quantize MIDI performances into\nmetrically-aligned, human-readable scores. We propose a beat-based\npreprocessing method that transfers score and performance data into a unified\ntoken representation. We optimize our model architecture and data\nrepresentation and train on piano and guitar performances. Our model exceeds\nstate-of-the-art performance based on the MUSTER metric."}
{"id": "2508.19269", "pdf": "https://arxiv.org/pdf/2508.19269.pdf", "abs": "https://arxiv.org/abs/2508.19269", "title": "Should LLMs be WEIRD? Exploring WEIRDness and Human Rights in Large Language Models", "authors": ["Ke Zhou", "Marios Constantinides", "Daniele Quercia"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "This paper has been accepted in AIES 2025", "summary": "Large language models (LLMs) are often trained on data that reflect WEIRD\nvalues: Western, Educated, Industrialized, Rich, and Democratic. This raises\nconcerns about cultural bias and fairness. Using responses to the World Values\nSurvey, we evaluated five widely used LLMs: GPT-3.5, GPT-4, Llama-3, BLOOM, and\nQwen. We measured how closely these responses aligned with the values of the\nWEIRD countries and whether they conflicted with human rights principles. To\nreflect global diversity, we compared the results with the Universal\nDeclaration of Human Rights and three regional charters from Asia, the Middle\nEast, and Africa. Models with lower alignment to WEIRD values, such as BLOOM\nand Qwen, produced more culturally varied responses but were 2% to 4% more\nlikely to generate outputs that violated human rights, especially regarding\ngender and equality. For example, some models agreed with the statements ``a\nman who cannot father children is not a real man'' and ``a husband should\nalways know where his wife is'', reflecting harmful gender norms. These\nfindings suggest that as cultural representation in LLMs increases, so does the\nrisk of reproducing discriminatory beliefs. Approaches such as Constitutional\nAI, which could embed human rights principles into model behavior, may only\npartly help resolve this tension."}
{"id": "2508.19294", "pdf": "https://arxiv.org/pdf/2508.19294.pdf", "abs": "https://arxiv.org/abs/2508.19294", "title": "Object Detection with Multimodal Large Vision-Language Models: An In-depth Review", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "First Peer Reviewed Review Paper for Object Detection with\n  Vision-Language Models (VLMs)", "summary": "The fusion of language and vision in large vision-language models (LVLMs) has\nrevolutionized deep learning-based object detection by enhancing adaptability,\ncontextual reasoning, and generalization beyond traditional architectures. This\nin-depth review presents a structured exploration of the state-of-the-art in\nLVLMs, systematically organized through a three-step research review process.\nFirst, we discuss the functioning of vision language models (VLMs) for object\ndetection, describing how these models harness natural language processing\n(NLP) and computer vision (CV) techniques to revolutionize object detection and\nlocalization. We then explain the architectural innovations, training\nparadigms, and output flexibility of recent LVLMs for object detection,\nhighlighting how they achieve advanced contextual understanding for object\ndetection. The review thoroughly examines the approaches used in integration of\nvisual and textual information, demonstrating the progress made in object\ndetection using VLMs that facilitate more sophisticated object detection and\nlocalization strategies. This review presents comprehensive visualizations\ndemonstrating LVLMs' effectiveness in diverse scenarios including localization\nand segmentation, and then compares their real-time performance, adaptability,\nand complexity to traditional deep learning systems. Based on the review, its\nis expected that LVLMs will soon meet or surpass the performance of\nconventional methods in object detection. The review also identifies a few\nmajor limitations of the current LVLM modes, proposes solutions to address\nthose challenges, and presents a clear roadmap for the future advancement in\nthis field. We conclude, based on this study, that the recent advancement in\nLVLMs have made and will continue to make a transformative impact on object\ndetection and robotic applications in the future."}
{"id": "2508.19316", "pdf": "https://arxiv.org/pdf/2508.19316.pdf", "abs": "https://arxiv.org/abs/2508.19316", "title": "Sycophancy as compositions of Atomic Psychometric Traits", "authors": ["Shreyans Jain", "Alexandra Yost", "Amirali Abdullah"], "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.4"], "comment": "8 pages, 4 figures", "summary": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an\nisolated failure mode that occurs via a single causal mechanism. We instead\npropose modeling it as geometric and causal compositions of psychometric traits\nsuch as emotionality, openness, and agreeableness - similar to factor\ndecomposition in psychometrics. Using Contrastive Activation Addition (CAA), we\nmap activation directions to these factors and study how different combinations\nmay give rise to sycophancy (e.g., high extraversion combined with low\nconscientiousness). This perspective allows for interpretable and compositional\nvector-based interventions like addition, subtraction and projection; that may\nbe used to mitigate safety-critical behaviors in LLMs."}
{"id": "2508.19321", "pdf": "https://arxiv.org/pdf/2508.19321.pdf", "abs": "https://arxiv.org/abs/2508.19321", "title": "An Investigation on Group Query Hallucination Attacks", "authors": ["Kehao Miao", "Xiaolong Jin"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "With the widespread use of large language models (LLMs), understanding their\npotential failure modes during user interactions is essential. In practice,\nusers often pose multiple questions in a single conversation with LLMs.\nTherefore, in this study, we propose Group Query Attack, a technique that\nsimulates this scenario by presenting groups of queries to LLMs simultaneously.\nWe investigate how the accumulated context from consecutive prompts influences\nthe outputs of LLMs. Specifically, we observe that Group Query Attack\nsignificantly degrades the performance of models fine-tuned on specific tasks.\nMoreover, we demonstrate that Group Query Attack induces a risk of triggering\npotential backdoors of LLMs. Besides, Group Query Attack is also effective in\ntasks involving reasoning, such as mathematical reasoning and code generation\nfor pre-trained and aligned models."}
{"id": "2508.19492", "pdf": "https://arxiv.org/pdf/2508.19492.pdf", "abs": "https://arxiv.org/abs/2508.19492", "title": "Geopolitical Parallax: Beyond Walter Lippmann Just After Large Language Models", "authors": ["Mehmet Can Yavuz", "Humza Gohar Kabir", "Aylin Özkan"], "categories": ["cs.CY", "cs.CL"], "comment": "7 pages, 4 figures, 7 tables", "summary": "Objectivity in journalism has long been contested, oscillating between ideals\nof neutral, fact-based reporting and the inevitability of subjective framing.\nWith the advent of large language models (LLMs), these tensions are now\nmediated by algorithmic systems whose training data and design choices may\nthemselves embed cultural or ideological biases. This study investigates\ngeopolitical parallax-systematic divergence in news quality and subjectivity\nassessments-by comparing article-level embeddings from Chinese-origin (Qwen,\nBGE, Jina) and Western-origin (Snowflake, Granite) model families. We evaluate\nboth on a human-annotated news quality benchmark spanning fifteen stylistic,\ninformational, and affective dimensions, and on parallel corpora covering\npolitically sensitive topics, including Palestine and reciprocal China-United\nStates coverage. Using logistic regression probes and matched-topic evaluation,\nwe quantify per-metric differences in predicted positive-class probabilities\nbetween model families. Our findings reveal consistent, non-random divergences\naligned with model origin. In Palestine-related coverage, Western models assign\nhigher subjectivity and positive emotion scores, while Chinese models emphasize\nnovelty and descriptiveness. Cross-topic analysis shows asymmetries in\nstructural quality metrics Chinese-on-US scoring notably lower in fluency,\nconciseness, technicality, and overall quality-contrasted by higher negative\nemotion scores. These patterns align with media bias theory and our distinction\nbetween semantic, emotional, and relational subjectivity, and extend LLM bias\nliterature by showing that geopolitical framing effects persist in downstream\nquality assessment tasks. We conclude that LLM-based media evaluation pipelines\nrequire cultural calibration to avoid conflating content differences with\nmodel-induced bias."}
{"id": "2508.19558", "pdf": "https://arxiv.org/pdf/2508.19558.pdf", "abs": "https://arxiv.org/abs/2508.19558", "title": "Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking", "authors": ["Zhuohao Li", "Wenqing Chen", "Jianxing Yu", "Zhichao Lu"], "categories": ["cs.SE", "cs.CL", "cs.PL"], "comment": null, "summary": "Embedding models have demonstrated strong performance in tasks like\nclustering, retrieval, and feature extraction while offering computational\nadvantages over generative models and cross-encoders. Benchmarks such as MTEB\nhave shown that text embeddings from large language models (LLMs) capture rich\nsemantic information, but their ability to reflect code-level functional\nsemantics remains unclear. Existing studies largely focus on code clone\ndetection, which emphasizes syntactic similarity and overlooks functional\nunderstanding. In this paper, we focus on the functional consistency of LLM\ncode embeddings, which determines if two code snippets perform the same\nfunction regardless of syntactic differences. We propose a novel data synthesis\nframework called Functionality-Oriented Code Self-Evolution to construct\ndiverse and challenging benchmarks. Specifically, we define code examples\nacross four semantic and syntactic categories and find that existing datasets\npredominantly capture syntactic properties. Our framework generates four unique\nvariations from a single code instance, providing a broader spectrum of code\nexamples that better reflect functional differences. Extensive experiments on\nthree downstream tasks-code clone detection, code functional consistency\nidentification, and code retrieval-demonstrate that embedding models\nsignificantly improve their performance when trained on our evolved datasets.\nThese results highlight the effectiveness and generalization of our data\nsynthesis framework, advancing the functional understanding of code."}
{"id": "2508.19611", "pdf": "https://arxiv.org/pdf/2508.19611.pdf", "abs": "https://arxiv.org/abs/2508.19611", "title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "18 pages, 9 figures", "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings."}
{"id": "2508.19619", "pdf": "https://arxiv.org/pdf/2508.19619.pdf", "abs": "https://arxiv.org/abs/2508.19619", "title": "Word Chain Generators for Prefix Normal Words", "authors": ["Duncan Adamson", "Moritz Dudey", "Pamela Fleischmann", "Annika Huch"], "categories": ["math.CO", "cs.CL"], "comment": null, "summary": "In 2011, Fici and Lipt\\'ak introduced prefix normal words. A binary word is\nprefix normal if it has no factor (substring) that contains more occurrences of\nthe letter 1 than the prefix of the same length. Among the open problems\nregarding this topic are the enumeration of prefix normal words and efficient\ntesting methods. We show a range of characteristics of prefix normal words.\nThese include properties of factors that are responsible for a word not being\nprefix normal. With word chains and generators, we introduce new ways of\nrelating words of the same length to each other."}
{"id": "2508.19697", "pdf": "https://arxiv.org/pdf/2508.19697.pdf", "abs": "https://arxiv.org/abs/2508.19697", "title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads", "authors": ["Chao Huang", "Zefeng Zhang", "Juewei Yue", "Quangang Li", "Chuang Zhang", "Tingwen Liu"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Current safety alignment for large language models(LLMs) continues to present\nvulnerabilities, given that adversarial prompting can effectively bypass their\nsafety measures.Our investigation shows that these safety mechanisms\npredominantly depend on a limited subset of attention heads: removing or\nablating these heads can severely compromise model safety. To identify and\nevaluate these safety-critical components, we introduce RDSHA, a targeted\nablation method that leverages the model's refusal direction to pinpoint\nattention heads mostly responsible for safety behaviors. Further analysis shows\nthat existing jailbreak attacks exploit this concentration by selectively\nbypassing or manipulating these critical attention heads. To address this\nissue, we propose AHD, a novel training strategy designed to promote the\ndistributed encoding of safety-related behaviors across numerous attention\nheads. Experimental results demonstrate that AHD successfully distributes\nsafety-related capabilities across more attention heads. Moreover, evaluations\nunder several mainstream jailbreak attacks show that models trained with AHD\nexhibit considerably stronger safety robustness, while maintaining overall\nfunctional utility."}
{"id": "2508.19827", "pdf": "https://arxiv.org/pdf/2508.19827.pdf", "abs": "https://arxiv.org/abs/2508.19827", "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?", "authors": ["Samuel Lewis-Lim", "Xingwei Tan", "Zhixue Zhao", "Nikolaos Aletras"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned."}
{"id": "2508.19843", "pdf": "https://arxiv.org/pdf/2508.19843.pdf", "abs": "https://arxiv.org/abs/2508.19843", "title": "SoK: Large Language Model Copyright Auditing via Fingerprinting", "authors": ["Shuo Shao", "Yiming Li", "Yu He", "Hongwei Yao", "Wenyuan Yang", "Dacheng Tao", "Zhan Qin"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The broad capabilities and substantial resources required to train Large\nLanguage Models (LLMs) make them valuable intellectual property, yet they\nremain vulnerable to copyright infringement, such as unauthorized use and model\ntheft. LLM fingerprinting, a non-intrusive technique that extracts and compares\nthe distinctive features from LLMs to identify infringements, offers a\npromising solution to copyright auditing. However, its reliability remains\nuncertain due to the prevalence of diverse model modifications and the lack of\nstandardized evaluation. In this SoK, we present the first comprehensive study\nof LLM fingerprinting. We introduce a unified framework and formal taxonomy\nthat categorizes existing methods into white-box and black-box approaches,\nproviding a structured overview of the state of the art. We further propose\nLeaFBench, the first systematic benchmark for evaluating LLM fingerprinting\nunder realistic deployment scenarios. Built upon mainstream foundation models\nand comprising 149 distinct model instances, LeaFBench integrates 13\nrepresentative post-development techniques, spanning both parameter-altering\nmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms\n(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the\nstrengths and weaknesses of existing methods, thereby outlining future research\ndirections and critical open problems in this emerging field. The code is\navailable at https://github.com/shaoshuo-ss/LeaFBench."}
{"id": "2508.19944", "pdf": "https://arxiv.org/pdf/2508.19944.pdf", "abs": "https://arxiv.org/abs/2508.19944", "title": "KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts", "authors": ["Taebaek Hwang", "Minseo Kim", "Gisang Lee", "Seonuk Kim", "Hyunjun Eun"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Understanding and reasoning over text within visual contexts poses a\nsignificant challenge for Vision-Language Models (VLMs), given the complexity\nand diversity of real-world scenarios. To address this challenge, text-rich\nVisual Question Answering (VQA) datasets and benchmarks have emerged for\nhigh-resource languages like English. However, a critical gap persists for\nlow-resource languages such as Korean, where the lack of comprehensive\nbenchmarks hinders robust model evaluation and comparison. To bridge this gap,\nwe introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich\nVQA Attuned to diverse visual contexts. KRETA facilitates an in-depth\nevaluation of both visual text understanding and reasoning capabilities, while\nalso supporting a multifaceted assessment across 15 domains and 26 image types.\nAdditionally, we introduce a semi-automated VQA generation pipeline\nspecifically optimized for text-rich settings, leveraging refined stepwise\nimage decomposition and a rigorous seven-metric evaluation protocol to ensure\ndata quality. While KRETA is tailored for Korean, we hope our adaptable and\nextensible pipeline will facilitate the development of similar benchmarks in\nother languages, thereby accelerating multilingual VLM research. The code and\ndataset for KRETA are available at https://github.com/tabtoyou/KRETA."}
{"id": "2508.19972", "pdf": "https://arxiv.org/pdf/2508.19972.pdf", "abs": "https://arxiv.org/abs/2508.19972", "title": "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity", "authors": ["Seongheon Park", "Yixuan Li"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Object hallucination in large vision-language models presents a significant\nchallenge to their safe deployment in real-world applications. Recent works\nhave proposed object-level hallucination scores to estimate the likelihood of\nobject hallucination; however, these methods typically adopt either a global or\nlocal perspective in isolation, which may limit detection reliability. In this\npaper, we introduce GLSim, a novel training-free object hallucination detection\nframework that leverages complementary global and local embedding similarity\nsignals between image and text modalities, enabling more accurate and reliable\nhallucination detection in diverse scenarios. We comprehensively benchmark\nexisting object hallucination detection methods and demonstrate that GLSim\nachieves superior detection performance, outperforming competitive baselines by\na significant margin."}
{"id": "2508.19990", "pdf": "https://arxiv.org/pdf/2508.19990.pdf", "abs": "https://arxiv.org/abs/2508.19990", "title": "Self-Supervised Pre-Training with Equilibrium Constraints", "authors": ["Xiaodong Cui", "A F M Saif", "Brian Kingsbury", "Tianyi Chen"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Self-supervised pre-training using unlabeled data is widely used in machine\nlearning. In this paper, we propose a new self-supervised pre-training approach\nto dealing with heterogeneous data. Instead of mixing all the data and\nminimizing the averaged global loss in the conventional way, we impose\nadditional equilibrium constraints to ensure that the models optimizes each\nsource of heterogeneous data to its local optima after $K$-step gradient\ndescent initialized from the model. We formulate this as a bilevel optimization\nproblem, and use the first-order approximation method to solve the problem. We\ndiscuss its connection to model-agnostic meta learning (MAML). Experiments are\ncarried out on self-supervised pre-training using multi-domain and multilingual\ndatasets, demonstrating that the proposed approach can significantly improve\nthe adaptivity of the self-supervised pre-trained model for the downstream\nsupervised fine-tuning tasks."}
{"id": "2508.19999", "pdf": "https://arxiv.org/pdf/2508.19999.pdf", "abs": "https://arxiv.org/abs/2508.19999", "title": "Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation", "authors": ["Ziniu Zhang", "Zhenshuo Zhang", "Dongyue Li", "Lu Wang", "Jennifer Dy", "Hongyang R. Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 pages. To appear in EMNLP'25", "summary": "This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n$\\mathbf{1}\\%$ error across six datasets. This allows us to scale up subset\nselection that would otherwise run full inference by up to\n$\\mathbf{37.7}\\times$ on models with up to $34$ billion parameters, and\noutperform existing selection methods based on input embeddings by\n$\\mathbf{11}\\%$ on average."}
{"id": "2508.20018", "pdf": "https://arxiv.org/pdf/2508.20018.pdf", "abs": "https://arxiv.org/abs/2508.20018", "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control", "authors": ["Quanfeng Lu", "Zhantao Ma", "Shuai Zhong", "Jin Wang", "Dahai Yu", "Michael K. Ng", "Ping Luo"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "comment": "28 pages, 12 figures", "summary": "The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems."}
{"id": "2508.20019", "pdf": "https://arxiv.org/pdf/2508.20019.pdf", "abs": "https://arxiv.org/abs/2508.20019", "title": "Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence", "authors": ["Ji Wang", "Kashing Chen", "Xinyuan Song", "Ke Zhang", "Lynn Ai", "Eric Yang", "Bill Shi"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Most existing Large Language Model (LLM)-based agent frameworks rely on\ncentralized orchestration, incurring high deployment costs, rigid communication\ntopologies, and limited adaptability. To address these challenges, we introduce\nSymphony, a decentralized multi-agent system which enables lightweight LLMs on\nconsumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:\n(1) a decentralized ledger that records capabilities, (2) a Beacon-selection\nprotocol for dynamic task allocation, and (3) weighted result voting based on\nCoTs. This design forms a privacy-saving, scalable, and fault-tolerant\norchestration with low overhead. Empirically, Symphony outperforms existing\nbaselines on reasoning benchmarks, achieving substantial accuracy gains and\ndemonstrating robustness across models of varying capacities."}
{"id": "2508.20032", "pdf": "https://arxiv.org/pdf/2508.20032.pdf", "abs": "https://arxiv.org/abs/2508.20032", "title": "Pruning Strategies for Backdoor Defense in LLMs", "authors": ["Santosh Chapagain", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted in CIKM '25: The 34th ACM International Conference on\n  Information and Knowledge Management Proceedings", "summary": "Backdoor attacks are a significant threat to the performance and integrity of\npre-trained language models. Although such models are routinely fine-tuned for\ndownstream NLP tasks, recent work shows they remain vulnerable to backdoor\nattacks that survive vanilla fine-tuning. These attacks are difficult to defend\nbecause end users typically lack knowledge of the attack triggers. Such attacks\nconsist of stealthy malicious triggers introduced through subtle syntactic or\nstylistic manipulations, which can bypass traditional detection and remain in\nthe model, making post-hoc purification essential. In this study, we explore\nwhether attention-head pruning can mitigate these threats without any knowledge\nof the trigger or access to a clean reference model. To this end, we design and\nimplement six pruning-based strategies: (i) gradient-based pruning, (ii)\nlayer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2\nsparsification, (iv) randomized ensemble pruning, (v)\nreinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.\nEach method iteratively removes the least informative heads while monitoring\nvalidation accuracy to avoid over-pruning. Experimental evaluation shows that\ngradient-based pruning performs best while defending the syntactic triggers,\nwhereas reinforcement learning and Bayesian pruning better withstand stylistic\nattacks."}
{"id": "2508.20083", "pdf": "https://arxiv.org/pdf/2508.20083.pdf", "abs": "https://arxiv.org/abs/2508.20083", "title": "Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Kuan Li", "Shuai Wang"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a standard approach for\nimproving the reliability of large language models (LLMs). Prior work\ndemonstrates the vulnerability of RAG systems by misleading them into\ngenerating attacker-chosen outputs through poisoning the knowledge base.\nHowever, this paper uncovers that such attacks could be mitigated by the strong\n\\textit{self-correction ability (SCA)} of modern LLMs, which can reject false\ncontext once properly configured. This SCA poses a significant challenge for\nattackers aiming to manipulate RAG systems.\n  In contrast to previous poisoning methods, which primarily target the\nknowledge base, we introduce \\textsc{DisarmRAG}, a new poisoning paradigm that\ncompromises the retriever itself to suppress the SCA and enforce\nattacker-chosen outputs. This compromisation enables the attacker to\nstraightforwardly embed anti-SCA instructions into the context provided to the\ngenerator, thereby bypassing the SCA. To this end, we present a\ncontrastive-learning-based model editing technique that performs localized and\nstealthy edits, ensuring the retriever returns a malicious instruction only for\nspecific victim queries while preserving benign retrieval behavior. To further\nstrengthen the attack, we design an iterative co-optimization framework that\nautomatically discovers robust instructions capable of bypassing prompt-based\ndefenses. We extensively evaluate DisarmRAG across six LLMs and three QA\nbenchmarks. Our results show near-perfect retrieval of malicious instructions,\nwhich successfully suppress SCA and achieve attack success rates exceeding 90\\%\nunder diverse defensive prompts. Also, the edited retriever remains stealthy\nunder several detection methods, highlighting the urgent need for\nretriever-centric defenses."}
{"id": "2401.09244", "pdf": "https://arxiv.org/pdf/2401.09244.pdf", "abs": "https://arxiv.org/abs/2401.09244", "title": "Cross-lingual Offensive Language Detection: A Systematic Review of Datasets, Transfer Approaches and Challenges", "authors": ["Aiqi Jiang", "Arkaitz Zubiaga"], "categories": ["cs.CL"], "comment": "35 pages, 7 figures", "summary": "The growing prevalence and rapid evolution of offensive language in social\nmedia amplify the complexities of detection, particularly highlighting the\nchallenges in identifying such content across diverse languages. This survey\npresents a systematic and comprehensive exploration of Cross-Lingual Transfer\nLearning (CLTL) techniques in offensive language detection in social media. Our\nstudy stands as the first holistic overview to focus exclusively on the\ncross-lingual scenario in this domain. We analyse 67 relevant papers and\ncategorise these studies across various dimensions, including the\ncharacteristics of multilingual datasets used, the cross-lingual resources\nemployed, and the specific CLTL strategies implemented. According to \"what to\ntransfer\", we also summarise three main CLTL transfer approaches: instance,\nfeature, and parameter transfer. Additionally, we shed light on the current\nchallenges and future research opportunities in this field. Furthermore, we\nhave made our survey resources available online, including two comprehensive\ntables that provide accessible references to the multilingual datasets and CLTL\nmethods used in the reviewed literature."}
{"id": "2403.01777", "pdf": "https://arxiv.org/pdf/2403.01777.pdf", "abs": "https://arxiv.org/abs/2403.01777", "title": "NPHardEval4V: Dynamic Evaluation of Large Vision-Language Models with Effects of Vision", "authors": ["Xiang Li", "Wenyue Hua", "Kaijie Zhu", "Lingyao Li", "Haoyang Ling", "Jinkui Chi", "Qi Dou", "Jindong Wang", "Yongfeng Zhang", "Xin Ma", "Lizhou Fan"], "categories": ["cs.CL", "cs.CV"], "comment": "25 pages, 9 figures, 2 tables", "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities in multimodal understanding, yet their reasoning abilities remain\nunderexplored. Existing benchmarks tend to focus on perception or text-based\ncomprehension, offering limited insight into how well these models perform on\nstructured, logic-driven tasks that require both visual and linguistic\nreasoning. To address this gap, we introduce NPHardEval4V, a multimodal\nbenchmark suite grounded in four classical NP-hard problems: Knapsack, Set\nCover, Traveling Salesperson, and Vertex Cover. Each task is presented through\na combination of structured visual layouts and textual prompts, designed to\nassess the ability of LVLMs to perform combinatorial reasoning under\nvisual-linguistic constraints. We evaluate a set of advanced open-source and\nclosed-source vision-language models under a unified prompting and problem\nrepresentation framework. This enables fair comparison across models and task\ntypes, while isolating key variables affecting performance. Our results show\nthat while these models perform reasonably well on perception-based inputs,\nthey struggle with global optimization, abstraction, and constraint\nsatisfaction. No single model demonstrates consistent reasoning capability\nacross all problem types, and common failure patterns reveal fundamental\nlimitations in current architectures. By leveraging the structure and\ncomplexity of NP-hard problems, NPHardEval4V provides a scalable,\ninterpretable, and challenging testbed for diagnosing reasoning behaviors in\nLVLMs. We hope this benchmark can support the community in building more\nrobust, inference-capable multimodal systems. The benchmark dataset and code\nare available at https://github.com/lizhouf/NPHardEval4."}
{"id": "2410.12513", "pdf": "https://arxiv.org/pdf/2410.12513.pdf", "abs": "https://arxiv.org/abs/2410.12513", "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency Reduction", "authors": ["Akriti Jain", "Saransh Sharma", "Koyel Mukherjee", "Soumyabrata Pal"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."}
{"id": "2411.17374", "pdf": "https://arxiv.org/pdf/2411.17374.pdf", "abs": "https://arxiv.org/abs/2411.17374", "title": "Understanding Fairness-Accuracy Trade-offs in Machine Learning Models: Does Promoting Fairness Undermine Performance?", "authors": ["Junhua Liu", "Roy Ka-Wei Lee", "Kwan Hui Lim"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to ASONAM 2025", "summary": "Fairness in both Machine Learning (ML) predictions and human decision-making\nis essential, yet both are susceptible to different forms of bias, such as\nalgorithmic and data-driven in ML, and cognitive or subjective in humans. In\nthis study, we examine fairness using a real-world university admissions\ndataset comprising 870 applicant profiles, leveraging three ML models: XGB,\nBi-LSTM, and KNN, alongside BERT embeddings for textual features. To evaluate\nindividual fairness, we introduce a consistency metric that quantifies\nagreement in decisions among ML models and human experts with diverse\nbackgrounds. Our analysis reveals that ML models surpass human evaluators in\nfairness consistency by margins ranging from 14.08\\% to 18.79\\%. Our findings\nhighlight the potential of using ML to enhance fairness in admissions while\nmaintaining high accuracy, advocating a hybrid approach combining human\njudgement and ML models."}
{"id": "2411.19930", "pdf": "https://arxiv.org/pdf/2411.19930.pdf", "abs": "https://arxiv.org/abs/2411.19930", "title": "On Domain-Adaptive Post-Training for Multimodal Large Language Models", "authors": ["Daixuan Cheng", "Shaohan Huang", "Ziyu Zhu", "Xintong Zhang", "Wayne Xin Zhao", "Zhongzhi Luan", "Bo Dai", "Zhenliang Zhang"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "EMNLP 2025 Findings, Project Page:\n  https://huggingface.co/AdaptLLM/Adapt-MLLM-to-Domains", "summary": "Adapting general multimodal large language models (MLLMs) to specific\ndomains, such as scientific and industrial fields, is highly significant in\npromoting their practical applications. This paper systematically investigates\ndomain adaptation of MLLMs via post-training, focusing on data synthesis,\ntraining pipeline, and task evaluation. (1) Data Synthesis: Using only\nopen-source models, we develop a generate-then-filter pipeline that curates\ndiverse visual instruction tasks based on domain-specific image-caption pairs.\nThe resulting data surpass the data synthesized by manual rules or strong\nclosed-source models in enhancing domain-specific performance. (2) Training\nPipeline: Unlike general MLLMs that typically adopt a two-stage training\nparadigm, we find that a single-stage approach is more effective for domain\nadaptation. (3) Task Evaluation: We conduct extensive experiments in\nhigh-impact domains such as biomedicine, food, and remote sensing, by\npost-training a variety of MLLMs and then evaluating MLLM performance on\nvarious domain-specific tasks. Finally, we fully open-source our models, code,\nand data to encourage future research in this area."}
{"id": "2412.19512", "pdf": "https://arxiv.org/pdf/2412.19512.pdf", "abs": "https://arxiv.org/abs/2412.19512", "title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging", "authors": ["Hua Farn", "Hsuan Su", "Shachi H Kumar", "Saurav Sahay", "Shang-Tse Chen", "Hung-yi Lee"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Fine-tuning large language models (LLMs) for downstream tasks often leads to\ncatastrophic forgetting, notably degrading the safety of originally aligned\nmodels. While some existing methods attempt to restore safety by incorporating\nadditional safety data, the quality of such data typically falls short of that\nused in the original alignment process. Moreover, these high-quality safety\ndatasets are generally inaccessible, making it difficult to fully recover the\nmodel's original safety. We ask: How can we preserve safety while improving\ndownstream task performance without additional safety data? We show that simply\nmerging the weights of pre- and post-fine-tuned models effectively mitigates\nsafety degradation while enhancing performance. Experiments across different\ndownstream tasks and models validate the method's practicality and\neffectiveness."}
{"id": "2501.09993", "pdf": "https://arxiv.org/pdf/2501.09993.pdf", "abs": "https://arxiv.org/abs/2501.09993", "title": "Agent-as-Judge for Factual Summarization of Long Narratives", "authors": ["Yeonseok Jeong", "Minsoo Kim", "Seung-won Hwang", "Byung-Hak Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated near-human performance in\nsummarization tasks based on traditional metrics such as ROUGE and BERTScore.\nHowever, these metrics do not adequately capture critical aspects of\nsummarization quality, such as factual accuracy, particularly for long\nnarratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the\nlimitations of metrics based on lexical similarity but still exhibit factual\ninconsistencies, especially in understanding character relationships and\nstates. In this work, we introduce NarrativeFactScore, a novel\n\"Agent-as-a-Judge\" framework for evaluating and refining summaries. By\nleveraging a Character Knowledge Graph (CKG) extracted from input and generated\nsummaries, NarrativeFactScore assesses the factual consistency and provides\nactionable guidance for refinement, such as identifying missing or erroneous\nfacts. We demonstrate the effectiveness of NarrativeFactScore through a\ndetailed workflow illustration and extensive validation on widely adopted\nbenchmarks, achieving superior performance compared to competitive methods. Our\nresults highlight the potential of agent-driven evaluation systems to improve\nthe factual reliability of LLM-generated summaries."}
{"id": "2501.11110", "pdf": "https://arxiv.org/pdf/2501.11110.pdf", "abs": "https://arxiv.org/abs/2501.11110", "title": "Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective", "authors": ["Yiyao Yu", "Yuxiang Zhang", "Dongdong Zhang", "Xiao Liang", "Hengyuan Zhang", "Xingxing Zhang", "Mahmoud Khademi", "Hany Awadalla", "Junjie Wang", "Yujiu Yang", "Furu Wei"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Large Language Models (LLMs) have made notable progress in mathematical\nreasoning, yet often rely on single-paradigm reasoning, limiting their\neffectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a\nnovel unified framework integrating multiple reasoning paradigms--Natural\nLanguage Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning\n(SR)--to enable synergistic collaboration. CoR generates multiple potential\nanswers via different reasoning paradigms and synthesizes them into a coherent\nfinal solution. We propose a Progressive Paradigm Training (PPT) strategy for\nmodels to progressively master these paradigms, leading to CoR-Math-7B.\nExperimental results demonstrate that CoR-Math-7B significantly outperforms\ncurrent SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o\nin theorem proving and a 15.0% improvement over RL-based methods on the MATH\nbenchmark in arithmetic tasks. These results show the enhanced mathematical\ncomprehension ability of our model, enabling zero-shot generalization across\ntasks."}
{"id": "2501.15000", "pdf": "https://arxiv.org/pdf/2501.15000.pdf", "abs": "https://arxiv.org/abs/2501.15000", "title": "MDEval: Evaluating and Enhancing Markdown Awareness in Large Language Models", "authors": ["Zhongpu Chen", "Yinfeng Liu", "Long Shi", "Xingyan Chen", "Yu Zhao", "Fuji Ren"], "categories": ["cs.CL", "cs.IR"], "comment": "WWW 2025", "summary": "Large language models (LLMs) are expected to offer structured Markdown\nresponses for the sake of readability in web chatbots (e.g., ChatGPT). Although\nthere are a myriad of metrics to evaluate LLMs, they fail to evaluate the\nreadability from the view of output content structure. To this end, we focus on\nan overlooked yet important metric -- Markdown Awareness, which directly\nimpacts the readability and structure of the content generated by these\nlanguage models. In this paper, we introduce MDEval, a comprehensive benchmark\nto assess Markdown Awareness for LLMs, by constructing a dataset with 20K\ninstances covering 10 subjects in English and Chinese. Unlike traditional\nmodel-based evaluations, MDEval provides excellent interpretability by\ncombining model-based generation tasks and statistical methods. Our results\ndemonstrate that MDEval achieves a Spearman correlation of 0.791 and an\naccuracy of 84.1% with human, outperforming existing methods by a large margin.\nExtensive experimental results also show that through fine-tuning over our\nproposed dataset, less performant open-source models are able to achieve\ncomparable performance to GPT-4o in terms of Markdown Awareness. To ensure\nreproducibility and transparency, MDEval is open sourced at\nhttps://github.com/SWUFE-DB-Group/MDEval-Benchmark."}
{"id": "2502.11779", "pdf": "https://arxiv.org/pdf/2502.11779.pdf", "abs": "https://arxiv.org/abs/2502.11779", "title": "Efficient Response Generation Strategy Selection for Fine-Tuning Large Language Models Through Self-Aligned Perplexity", "authors": ["Xuan Ren", "Qi Chen", "Lingqiao Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning large language models (LLMs) typically relies on producing large\nsets of input-output pairs. Yet for a given question, there can be many valid\noutputs. In practice, these outputs are often derived by distilling knowledge\nfrom teacher models, and they can vary depending on the specific teacher model\nor prompting strategy employed. Recent findings show that how these training\noutputs are generated can significantly affect the performance of the\nfine-tuned model, raising an important question: how do we pick the best data\ngeneration method from among numerous possibilities? Rather than exhaustively\ntraining and evaluating on each candidate, this paper proposes a scalable\napproximate method that assesses a small subset of generated data to estimate\nits suitability for a specific target LLM. Our central idea is that effective\noutputs should be familiar to the target LLM. While previous work measures\nfamiliarity with perplexity, we find that perplexity might be suboptimal in\ncharacterizing familiarity through empirical analyses and practical\nobservations. To address this, we introduce self-aligned perplexity, a novel\nmetric capturing how closely candidate outputs adhere to the target LLM's own\nstyle and reasoning patterns. In this way, we can identify the most effective\ngeneration strategy on a small sample, then apply it to produce the complete\ntraining set. We demonstrate that training on data generated by the chosen\nmethod yields significant improvements across diverse reasoning-focused\nbenchmarks, particularly in cases where different candidate methods lead to\nhighly divergent training outcomes. Our implementation is publicly available at\nhttps://github.com/XuanRen4470/SPPL."}
{"id": "2502.15348", "pdf": "https://arxiv.org/pdf/2502.15348.pdf", "abs": "https://arxiv.org/abs/2502.15348", "title": "Constructing a Norm for Children's Scientific Drawing: Distribution Features Based on Semantic Similarity of Large Language Models", "authors": ["Yi Zhang", "Fan Wei", "Jingyi Li", "Yan Wang", "Yanyan Yu", "Jianli Chen", "Zipo Cai", "Xinyu Liu", "Wei Wang", "Sensen Yao", "Peng Wang", "Zhong Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The use of children's drawings to examining their conceptual understanding\nhas been proven to be an effective method, but there are two major problems\nwith previous research: 1. The content of the drawings heavily relies on the\ntask, and the ecological validity of the conclusions is low; 2. The\ninterpretation of drawings relies too much on the subjective feelings of the\nresearchers. To address this issue, this study uses the Large Language Model\n(LLM) to identify 1420 children's scientific drawings (covering 9 scientific\nthemes/concepts), and uses the word2vec algorithm to calculate their semantic\nsimilarity. The study explores whether there are consistent drawing\nrepresentations for children on the same theme, and attempts to establish a\nnorm for children's scientific drawings, providing a baseline reference for\nfollow-up children's drawing research. The results show that the representation\nof most drawings has consistency, manifested as most semantic similarity>0.8.\nAt the same time, it was found that the consistency of the representation is\nindependent of the accuracy (of LLM's recognition), indicating the existence of\nconsistency bias. In the subsequent exploration of influencing factors, we used\nKendall rank correlation coefficient to investigate the effects of \"sample\nsize\", \"abstract degree\", and \"focus points\" on drawings, and used word\nfrequency statistics to explore whether children represented abstract\nthemes/concepts by reproducing what was taught in class. It was found that\naccuracy (of LLM's recognition) is the most sensitive indicator, and data such\nas sample size and semantic similarity are related to it; The consistency\nbetween classroom experiments and teaching purpose is also an important factor,\nmany students focus more on the experiments themselves rather than what they\nexplain."}
{"id": "2503.01510", "pdf": "https://arxiv.org/pdf/2503.01510.pdf", "abs": "https://arxiv.org/abs/2503.01510", "title": "KoWit-24: A Richly Annotated Dataset of Wordplay in News Headlines", "authors": ["Alexander Baranov", "Anna Palatkina", "Yulia Makovka", "Pavel Braslavski"], "categories": ["cs.CL"], "comment": "Accepted to RANLP 2025", "summary": "We present KoWit-24, a dataset with fine-grained annotation of wordplay in\n2,700 Russian news headlines. KoWit-24 annotations include the presence of\nwordplay, its type, wordplay anchors, and words/phrases the wordplay refers to.\nUnlike the majority of existing humor collections of canned jokes, KoWit-24\nprovides wordplay contexts -- each headline is accompanied by the news lead and\nsummary. The most common type of wordplay in the dataset is the transformation\nof collocations, idioms, and named entities -- the mechanism that has been\nunderrepresented in previous humor datasets. Our experiments with five LLMs\nshow that there is ample room for improvement in wordplay detection and\ninterpretation tasks. The dataset and evaluation scripts are available at\nhttps://github.com/Humor-Research/KoWit-24"}
{"id": "2503.13423", "pdf": "https://arxiv.org/pdf/2503.13423.pdf", "abs": "https://arxiv.org/abs/2503.13423", "title": "SuperBPE: Space Travel for Language Models", "authors": ["Alisa Liu", "Jonathan Hayase", "Valentin Hofmann", "Sewoong Oh", "Noah A. Smith", "Yejin Choi"], "categories": ["cs.CL", "cs.LG"], "comment": "COLM 2025 camera-ready", "summary": "The assumption across nearly all language model (LM) tokenization schemes is\nthat tokens should be subwords, i.e., contained within word boundaries. While\nproviding a seemingly reasonable inductive bias, is this common practice\nlimiting the potential of modern LMs? Whitespace is not a reliable delimiter of\nmeaning, as evidenced by multi-word expressions (e.g., \"by the way\"),\ncrosslingual variation in the number of words needed to express a concept\n(e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do\nnot use whitespace at all (e.g., Chinese). To explore the potential of\ntokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE,\nwhich incorporates a simple pretokenization curriculum into the byte-pair\nencoding (BPE) algorithm to first learn subwords, then superwords that bridge\nwhitespace. This brings dramatic improvements in encoding efficiency: when\nfixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with\nup to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B\ntransformer LMs from scratch while fixing the model size, vocabulary size, and\ntrain compute, varying *only* the algorithm for learning the vocabulary. Our\nmodel trained with SuperBPE achieves an average +4.0% absolute improvement over\nthe BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while\nsimultaneously requiring 27% less compute at inference time. In analysis, we\nfind that SuperBPE results in segmentations of text that are more uniform in\nper-token difficulty. Qualitatively, this may be because SuperBPE tokens often\ncapture common multi-word expressions that function semantically as a single\nunit. SuperBPE is a straightforward, local modification to tokenization that\nimproves both encoding efficiency and downstream performance, yielding better\nlanguage models overall."}
{"id": "2504.00657", "pdf": "https://arxiv.org/pdf/2504.00657.pdf", "abs": "https://arxiv.org/abs/2504.00657", "title": "News is More than a Collection of Facts: Moral Frame Preserving News Summarization", "authors": ["Enrico Liscio", "Michela Lorandi", "Pradeep K. Murukannaiah"], "categories": ["cs.CL"], "comment": "Accepted at COLM2025", "summary": "News articles are more than collections of facts; they reflect journalists'\nframing, shaping how events are presented to the audience. One key aspect of\nframing is the choice to write in (or quote verbatim) morally charged language\nas opposed to using neutral terms. This moral framing carries implicit\njudgments that automated news summarizers should recognize and preserve to\nmaintain the original intent of the writer. In this work, we perform the first\nstudy on the preservation of moral framing in AI-generated news summaries. We\npropose an approach that leverages the intuition that journalists intentionally\nuse or report specific moral-laden words, which should be retained in\nsummaries. Through automated, crowd-sourced, and expert evaluations, we\ndemonstrate that our approach enhances the preservation of moral framing while\nmaintaining overall summary quality."}
{"id": "2504.07994", "pdf": "https://arxiv.org/pdf/2504.07994.pdf", "abs": "https://arxiv.org/abs/2504.07994", "title": "Evaluating the Fitness of Ontologies for the Task of Question Generation", "authors": ["Samah Alkhuzaey", "Floriana Grasso", "Terry R. Payne", "Valentina Tamma"], "categories": ["cs.CL", "cs.AI"], "comment": "Revised version (v2) accepted for the 28th European Conference on\n  Artificial Intelligence (ECAI-2025), including a validation study", "summary": "Ontology-based question generation is an important application of\nsemantic-aware systems that enables the creation of large question banks for\ndiverse learning environments. The effectiveness of these systems, both in\nterms of the calibre and cognitive difficulty of the resulting questions,\ndepends heavily on the quality and modelling approach of the underlying\nontologies, making it crucial to assess their fitness for this task. To date,\nthere has been no comprehensive investigation into the specific ontology\naspects or characteristics that affect the question generation process.\nTherefore, this paper proposes a set of requirements and task-specific metrics\nfor evaluating the fitness of ontologies for question generation tasks in\npedagogical settings. Using the ROMEO methodology (a structured framework used\nfor identifying task-specific metrics), a set of evaluation metrics have been\nderived from an expert assessment of questions generated by a question\ngeneration model. To validate the proposed metrics, we apply them to a set of\nontologies previously used in question generation to illustrate how the metric\nscores align with and complement findings reported in earlier studies. The\nanalysis confirms that ontology characteristics significantly impact the\neffectiveness of question generation, with different ontologies exhibiting\nvarying performance levels. This highlights the importance of assessing\nontology quality with respect to Automatic Question Generation (AQG) tasks."}
{"id": "2504.13655", "pdf": "https://arxiv.org/pdf/2504.13655.pdf", "abs": "https://arxiv.org/abs/2504.13655", "title": "Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts", "authors": ["Jie Zou", "Cheng Lin", "Weikang Guo", "Zheng Wang", "Jiwei Wei", "Yang Yang", "Heng Tao Shen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "31 pages; Accepted by Information Fusion", "summary": "Conversational recommender systems enable natural language conversations and\nthus lead to a more engaging and effective recommendation scenario. As the\nconversations for recommender systems usually contain limited contextual\ninformation, many existing conversational recommender systems incorporate\nexternal sources to enrich the contextual information. However, how to combine\ndifferent types of contextual information is still a challenge. In this paper,\nwe propose a multi-type context-aware conversational recommender system, called\nMCCRS, effectively fusing multi-type contextual information via\nmixture-of-experts to improve conversational recommender systems. MCCRS\nincorporates both structured information and unstructured information,\nincluding the structured knowledge graph, unstructured conversation history,\nand unstructured item reviews. It consists of several experts, with each expert\nspecialized in a particular domain (i.e., one specific contextual information).\nMultiple experts are then coordinated by a ChairBot to generate the final\nresults. Our proposed MCCRS model takes advantage of different contextual\ninformation and the specialization of different experts followed by a ChairBot\nbreaks the model bottleneck on a single contextual information. Experimental\nresults demonstrate that our proposed MCCRS method achieves significantly\nhigher performance compared to existing baselines."}
{"id": "2504.19395", "pdf": "https://arxiv.org/pdf/2504.19395.pdf", "abs": "https://arxiv.org/abs/2504.19395", "title": "ICL CIPHERS: Quantifying \"Learning\" in In-Context Learning via Substitution Ciphers", "authors": ["Zhouxiang Fang", "Aayush Mishra", "Muhan Gao", "Anqi Liu", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": null, "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ''learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve tasks reformulated by ICL CIPHERS with a BIJECTIVE mapping,\nwhich requires ''deciphering'' the latent cipher. We show that LLMs are better\nat solving tasks reformulated by ICL CIPHERS with BIJECTIVE mappings than the\nNON-BIJECTIVE (irreversible) baseline, providing a novel approach to quantify\n''learning'' in ICL. While this gap is small, it is consistent across the board\non four datasets and six models. Finally, we examine LLMs' internal\nrepresentations and identify evidence in their ability to decode the ciphered\ninputs."}
{"id": "2505.13972", "pdf": "https://arxiv.org/pdf/2505.13972.pdf", "abs": "https://arxiv.org/abs/2505.13972", "title": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals", "authors": ["Qianli Wang", "Van Bach Nguyen", "Nils Feldhus", "Luis Felipe Villa-Arenas", "Christin Seifert", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL"], "comment": "Accepted at INLG 2025, camera-ready version", "summary": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models: being the same model, belonging to the same model family, being\nindependent models, and having an distillation relationship. Through extensive\nexperiments involving two state-of-the-art LLM-based methods, three datasets,\nfour generator models, and 15 judge models, complemented by a user study (n =\n90), we demonstrate that judge models with an independent, non-fine-tuned\nrelationship to the generator model provide the most reliable label flipping\nevaluations. Relationships between the generator and judge models, which are\nclosely aligned with the user study for CDA, result in better model performance\nand robustness. Nevertheless, we find that the gap between the most effective\njudge models and the results obtained from the user study remains considerably\nlarge. This suggests that a fully automated pipeline for CDA may be inadequate\nand requires human intervention."}
{"id": "2505.17464", "pdf": "https://arxiv.org/pdf/2505.17464.pdf", "abs": "https://arxiv.org/abs/2505.17464", "title": "Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning", "authors": ["Xingyu Tan", "Xiaoyang Wang", "Qing Liu", "Xiwei Xu", "Xin Yuan", "Liming Zhu", "Wenjie Zhang"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP2025 (Main Conference)", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. Current hybrid RAG system retrieves evidence\nfrom both knowledge graphs (KGs) and text documents to support LLM reasoning.\nHowever, it faces challenges like handling multi-hop reasoning, multi-entity\nquestions, multi-source verification, and effective graph utilization. To\naddress these limitations, we present Hydra, a training-free framework that\nunifies graph topology, document semantics, and source reliability to support\ndeep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity\nproblems through agent-driven exploration that combines structured and\nunstructured retrieval, increasing both diversity and precision of evidence. To\ntackle multi-source verification, Hydra uses a tri-factor cross-source\nverification (source trustworthiness assessment, cross-source corroboration,\nand entity-path alignment), to balance topic relevance with cross-modal\nagreement. By leveraging graph structure, Hydra fuses heterogeneous sources,\nguides efficient exploration, and prunes noise early. Comprehensive experiments\non seven benchmark datasets show that Hydra achieves overall state-of-the-art\nresults on all benchmarks with GPT-3.5, outperforming the strong hybrid\nbaseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra\nenables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance\ncomparable to that of GPT-4-Turbo. The source code is available on\nhttps://stevetantan.github.io/Hydra/."}
{"id": "2506.08400", "pdf": "https://arxiv.org/pdf/2506.08400.pdf", "abs": "https://arxiv.org/abs/2506.08400", "title": "mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks", "authors": ["Luel Hagos Beyene", "Vivek Verma", "Min Ma", "Jesujoba O. Alabi", "Fabian David Schmidt", "Joyce Nakatumba-Nabende", "David Ifeoluwa Adelani"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted to ASRU 2025", "summary": "Large Language models (LLMs) have demonstrated impressive performance on a\nwide range of tasks, including in multimodal settings such as speech. However,\ntheir evaluation is often limited to English and a few high-resource languages.\nFor low-resource languages, there is no standardized evaluation benchmark. In\nthis paper, we address this gap by introducing mSTEB, a new benchmark to\nevaluate the performance of LLMs on a wide range of tasks covering language\nidentification, text classification, question answering, and translation tasks\non both speech and text modalities. We evaluated the performance of leading\nLLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open\nmodels such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in\nperformance between high-resource and low-resource languages, especially for\nlanguages spoken in Africa and Americas/Oceania. Our findings show that more\ninvestment is needed to address their under-representation in LLMs coverage."}
{"id": "2506.22402", "pdf": "https://arxiv.org/pdf/2506.22402.pdf", "abs": "https://arxiv.org/abs/2506.22402", "title": "Refining Czech GEC: Insights from a Multi-Experiment Approach", "authors": ["Petr Pechman", "Milan Straka", "Jana Straková", "Jakub Náplava"], "categories": ["cs.CL"], "comment": "Accepted to TSD 2025", "summary": "We present a grammar error correction (GEC) system that achieves state of the\nart for the Czech language. Our system is based on a neural network translation\napproach with the Transformer architecture, and its key feature is its\nreal-time synthetic generation pipeline, which dynamically augments sentences\nwith artificial errors by introducing both language-agnostic and Czech-specific\nerrors. We conduct a comprehensive series of experiments, investigating the\nCzech GEC corpora as bases for synthetic error introduction, several error\ngeneration strategies, domain balancing, tokenization granularity, model size,\nand data scaling during fine-tuning. Additionally, we evaluate the performance\nof large language models (LLMs) on Czech GEC in both end-user and expert\nfine-tuning scenarios. Our best-performing model is superior both in\nperformance and computational efficiency. The source code and the trained model\nlinks are available on https://github.com/ufal/tsd2025-gec."}
{"id": "2507.05444", "pdf": "https://arxiv.org/pdf/2507.05444.pdf", "abs": "https://arxiv.org/abs/2507.05444", "title": "PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs", "authors": ["Sana Kang", "Myeongseok Gwon", "Su Young Kwon", "Jaewook Lee", "Andrew Lan", "Bhiksha Raj", "Rita Singh"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main", "summary": "Vocabulary acquisition poses a significant challenge for second-language (L2)\nlearners, especially when learning typologically distant languages such as\nEnglish and Korean, where phonological and structural mismatches complicate\nvocabulary learning. Recently, large language models (LLMs) have been used to\ngenerate keyword mnemonics by leveraging similar keywords from a learner's\nfirst language (L1) to aid in acquiring L2 vocabulary. However, most of this\nresearch has focused on native English speakers learning other languages,\nrather than the reverse. In this paper, we present PhoniTale, a novel\ncross-lingual mnemonic generation system that retrieves L1 keyword sequence\nbased on phonological similarity and uses LLMs to generate mnemonics. We\nevaluate PhoniTale using both automated metrics and human evaluations,\ncomparing its output to mnemonics created by humans and by previous automated\napproaches. To assess practical effectiveness, we also conduct a short-term\nrecall test measuring mnemonic helpfulness. Our findings show that PhoniTale\nperforms comparably to human-authored mnemonics. We also highlight key areas\nfor future improvement in mnemonic quality and methodology."}
{"id": "2507.07998", "pdf": "https://arxiv.org/pdf/2507.07998.pdf", "abs": "https://arxiv.org/abs/2507.07998", "title": "PyVision: Agentic Vision with Dynamic Tooling", "authors": ["Shitian Zhao", "Haoquan Zhang", "Shaoheng Lin", "Ming Li", "Qilong Wu", "Kaipeng Zhang", "Chen Wei"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "26 Pages, 10 Figures, Technical report, Fix Typo", "summary": "LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning."}
{"id": "2507.14819", "pdf": "https://arxiv.org/pdf/2507.14819.pdf", "abs": "https://arxiv.org/abs/2507.14819", "title": "Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents", "authors": ["Akriti Jain", "Pritika Ramu", "Aparna Garimella", "Apoorv Saxena"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ntransforming text descriptions or tables to data visualizations via\ninstruction-tuning methods. However, it is not straightforward to apply these\nmethods directly for a more real-world use case of visualizing data from long\ndocuments based on user-given intents, as opposed to the user pre-selecting the\nrelevant content manually. We introduce the task of intent-based chart\ngeneration from documents: given a user-specified intent and document(s), the\ngoal is to generate a chart adhering to the intent and grounded on the\ndocument(s) in a zero-shot setting. We propose an unsupervised, two-staged\nframework in which an LLM first extracts relevant information from the\ndocument(s) by decomposing the intent and iteratively validates and refines\nthis data. Next, a heuristic-guided module selects an appropriate chart type\nbefore final code generation. To assess the data accuracy of the generated\ncharts, we propose an attribution-based metric that uses a structured textual\nrepresentation of charts, instead of relying on visual decoding metrics that\noften fail to capture the chart data effectively. To validate our approach, we\ncurate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from\ntwo domains, finance and scientific, in contrast to the existing datasets that\nare largely limited to parallel text descriptions/ tables and their\ncorresponding charts. We compare our approach with baselines using single-shot\nchart generation using LLMs and query-based retrieval methods; our method\noutperforms by upto $9$ points and $17$ points in terms of chart data accuracy\nand chart type respectively over the best baselines."}
{"id": "2507.16632", "pdf": "https://arxiv.org/pdf/2507.16632.pdf", "abs": "https://arxiv.org/abs/2507.16632", "title": "Step-Audio 2 Technical Report", "authors": ["Boyong Wu", "Chao Yan", "Chen Hu", "Cheng Yi", "Chengli Feng", "Fei Tian", "Feiyu Shen", "Gang Yu", "Haoyang Zhang", "Jingbei Li", "Mingrui Chen", "Peng Liu", "Wang You", "Xiangyu Tony Zhang", "Xingyuan Li", "Xuerui Yang", "Yayue Deng", "Yechang Huang", "Yuxin Li", "Yuxin Zhang", "Zhao You", "Brian Li", "Changyi Wan", "Hanpeng Hu", "Jiangjie Zhen", "Siyu Chen", "Song Yuan", "Xuelin Zhang", "Yimin Jiang", "Yu Zhou", "Yuxiang Yang", "Bingxin Li", "Buyun Ma", "Changhe Song", "Dongqing Pang", "Guoqiang Hu", "Haiyang Sun", "Kang An", "Na Wang", "Shuli Gao", "Wei Ji", "Wen Li", "Wen Sun", "Xuan Wen", "Yong Ren", "Yuankai Ma", "Yufan Lu", "Bin Wang", "Bo Li", "Changxin Miao", "Che Liu", "Chen Xu", "Dapeng Shi", "Dingyuan Hu", "Donghang Wu", "Enle Liu", "Guanzhe Huang", "Gulin Yan", "Han Zhang", "Hao Nie", "Haonan Jia", "Hongyu Zhou", "Jianjian Sun", "Jiaoren Wu", "Jie Wu", "Jie Yang", "Jin Yang", "Junzhe Lin", "Kaixiang Li", "Lei Yang", "Liying Shi", "Li Zhou", "Longlong Gu", "Ming Li", "Mingliang Li", "Mingxiao Li", "Nan Wu", "Qi Han", "Qinyuan Tan", "Shaoliang Pang", "Shengjie Fan", "Siqi Liu", "Tiancheng Cao", "Wanying Lu", "Wenqing He", "Wuxun Xie", "Xu Zhao", "Xueqi Li", "Yanbo Yu", "Yang Yang", "Yi Liu", "Yifan Lu", "Yilei Wang", "Yuanhao Ding", "Yuanwei Liang", "Yuanwei Lu", "Yuchu Luo", "Yuhe Yin", "Yumeng Zhan", "Yuxiang Zhang", "Zidong Yang", "Zixin Zhang", "Binxing Jiao", "Daxin Jiang", "Heung-Yeung Shum", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Yibo Zhu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "v3: Added introduction and evaluation results of Step-Audio 2 mini", "summary": "This paper presents Step-Audio 2, an end-to-end multi-modal large language\nmodel designed for industry-strength audio understanding and speech\nconversation. By integrating a latent audio encoder and reasoning-centric\nreinforcement learning (RL), Step-Audio 2 achieves promising performance in\nautomatic speech recognition (ASR) and audio understanding. To facilitate\ngenuine end-to-end speech conversation, Step-Audio 2 incorporates the\ngeneration of discrete audio tokens into language modeling, significantly\nenhancing its responsiveness to paralinguistic information such as speaking\nstyles and emotions. To effectively leverage the rich textual and acoustic\nknowledge in real-world data, Step-Audio 2 integrates retrieval-augmented\ngeneration (RAG) and is able to call external tools such as web search to\nmitigate hallucination and audio search to switch timbres. Trained on millions\nof hours of speech and audio data, Step-Audio 2 delivers intelligence and\nexpressiveness across diverse conversational scenarios. Evaluation results\ndemonstrate that Step-Audio 2 achieves state-of-the-art performance on various\naudio understanding and conversational benchmarks compared to other open-source\nand commercial solutions. Please visit\nhttps://github.com/stepfun-ai/Step-Audio2 for more information."}
{"id": "2507.16812", "pdf": "https://arxiv.org/pdf/2507.16812.pdf", "abs": "https://arxiv.org/abs/2507.16812", "title": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning", "authors": ["Run-Ze Fan", "Zengzhi Wang", "Pengfei Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "39 pages; Github: https://github.com/GAIR-NLP/MegaScience; HF:\n  https://huggingface.co/MegaScience", "summary": "Scientific reasoning is critical for developing AI scientists and supporting\nhuman researchers in advancing the frontiers of natural science discovery.\nHowever, the open-source community has primarily focused on mathematics and\ncoding while neglecting the scientific domain, largely due to the absence of\nopen, large-scale, high-quality, verifiable scientific reasoning datasets. To\nbridge this gap, we first present TextbookReasoning, an open dataset featuring\ntruthful reference answers extracted from 12k university-level scientific\ntextbooks, comprising 650k reasoning questions spanning 7 scientific\ndisciplines. We further introduce MegaScience, a large-scale mixture of\nhigh-quality open-source datasets totaling 1.25 million instances, developed\nthrough systematic ablation studies that evaluate various data selection\nmethodologies to identify the optimal subset for each publicly available\nscientific dataset. Meanwhile, we build a comprehensive evaluation system\ncovering diverse subjects and question types across 15 benchmarks,\nincorporating comprehensive answer extraction strategies to ensure accurate\nevaluation metrics. Our experiments demonstrate that our datasets achieve\nsuperior performance and training efficiency with more concise response lengths\ncompared to existing open-source scientific datasets. Furthermore, we train\nLlama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which\nsignificantly outperform the corresponding official instruct models in average\nperformance. In addition, MegaScience exhibits greater effectiveness for larger\nand stronger models, suggesting a scaling benefit for scientific tuning. We\nrelease our data curation pipeline, evaluation system, datasets, and seven\ntrained models to the community to advance scientific reasoning research."}
{"id": "2508.03037", "pdf": "https://arxiv.org/pdf/2508.03037.pdf", "abs": "https://arxiv.org/abs/2508.03037", "title": "When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025", "authors": ["Ariya Mukherjee-Gandhi", "Oliver Muellerklein"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "23 pages, 7 figures, 8 tables", "summary": "As generative AI continues to reshape artistic production and alternate modes\nof human expression, artists whose livelihoods are most directly affected have\nraised urgent concerns about consent, transparency, and the future of creative\nlabor. However, the voices of artists are often marginalized in dominant public\nand scholarly discourse. This study presents a twelve-year analysis, from 2013\nto 2025, of English-language discourse surrounding AI-generated art. It draws\nfrom 439 curated 500-word excerpts sampled from opinion articles, news reports,\nblogs, legal filings, and spoken-word transcripts. Through a reproducible\nmethodology, we identify five stable thematic clusters and uncover a\nmisalignment between artists' perceptions and prevailing media narratives. Our\nfindings highlight how the use of technical jargon can function as a subtle\nform of gatekeeping, often sidelining the very issues artists deem most urgent.\nOur work provides a BERTopic-based methodology and a multimodal baseline for\nfuture research, alongside a clear call for deeper, transparency-driven\nengagement with artist perspectives in the evolving AI-creative landscape."}
{"id": "2508.08292", "pdf": "https://arxiv.org/pdf/2508.08292.pdf", "abs": "https://arxiv.org/abs/2508.08292", "title": "Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs", "authors": ["Aryan Gulati", "Brando Miranda", "Eric Chen", "Emily Xia", "Kai Fronsdal", "Bruno Dumont", "Elyas Obbad", "Sanmi Koyejo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO", "cs.NE", "68T20, 68T05, 68Q32", "F.2.2; I.2.3; I.2.6; I.2.8"], "comment": "27 pages total (10-page main paper + 17-page appendix), 12 figures, 6\n  tables. Submitted to ICML 2025 (under review)", "summary": "Current mathematical reasoning benchmarks for large language models (LLMs)\nare approaching saturation, with some achieving > 90% accuracy, and are\nincreasingly compromised by training-set contamination. We introduce\nPutnam-AXIOM, a benchmark of 522 university-level competition problems drawn\nfrom the prestigious William Lowell Putnam Mathematical Competition, and\nPutnam-AXIOM Variation, an unseen companion set of 100 functional variants\ngenerated by programmatically perturbing variables and constants. The variation\nprotocol produces an unlimited stream of equally difficult, unseen instances --\nyielding a contamination-resilient test bed. On the Original set, OpenAI's\no1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy\ndrops by 19.6% (46.8% relative decrease) on the paired Variations. The\nremaining eighteen models show the same downward trend, ten of them with\nnon-overlapping 95% confidence intervals. These gaps suggest memorization and\nhighlight the necessity of dynamic benchmarks. We complement \"boxed\" accuracy\nwith Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores\nreasoning traces and automates natural language proof evaluations. Putnam-AXIOM\ntherefore provides a rigorous, contamination-resilient evaluation framework for\nassessing advanced mathematical reasoning of LLMs. Data and evaluation code are\npublicly available at https://github.com/brando90/putnam-axiom."}
{"id": "2508.08712", "pdf": "https://arxiv.org/pdf/2508.08712.pdf", "abs": "https://arxiv.org/abs/2508.08712", "title": "A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models", "authors": ["Lingzhe Zhang", "Liancheng Fang", "Chiming Duan", "Minghua He", "Leyi Pan", "Pei Xiao", "Shiyu Huang", "Yunpeng Zhai", "Xuming Hu", "Philip S. Yu", "Aiwei Liu"], "categories": ["cs.CL", "cs.AI", "cs.DC", "68T50", "I.2.7"], "comment": null, "summary": "As text generation has become a core capability of modern Large Language\nModels (LLMs), it underpins a wide range of downstream applications. However,\nmost existing LLMs rely on autoregressive (AR) generation, producing one token\nat a time based on previously generated context-resulting in limited generation\nspeed due to the inherently sequential nature of the process. To address this\nchallenge, an increasing number of researchers have begun exploring parallel\ntext generation-a broad class of techniques aimed at breaking the\ntoken-by-token generation bottleneck and improving inference efficiency.\nDespite growing interest, there remains a lack of comprehensive analysis on\nwhat specific techniques constitute parallel text generation and how they\nimprove inference performance. To bridge this gap, we present a systematic\nsurvey of parallel text generation methods. We categorize existing approaches\ninto AR-based and Non-AR-based paradigms, and provide a detailed examination of\nthe core techniques within each category. Following this taxonomy, we assess\ntheir theoretical trade-offs in terms of speed, quality, and efficiency, and\nexamine their potential for combination and comparison with alternative\nacceleration strategies. Finally, based on our findings, we highlight recent\nadvancements, identify open challenges, and outline promising directions for\nfuture research in parallel text generation. We have also created a GitHub\nrepository for indexing relevant papers and open resources available at\nhttps://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation."}
{"id": "2508.09016", "pdf": "https://arxiv.org/pdf/2508.09016.pdf", "abs": "https://arxiv.org/abs/2508.09016", "title": "A Survey on Training-free Alignment of Large Language Models", "authors": ["Birong Pan", "Yongqi Li", "Weiyu Zhang", "Wenpeng Lu", "Mayi Xu", "Shen Zhou", "Yuanyuan Zhu", "Ming Zhong", "Tieyun Qian"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to EMNLP 2025 (findings), camera-ready version", "summary": "The alignment of large language models (LLMs) aims to ensure their outputs\nadhere to human values, ethical standards, and legal norms. Traditional\nalignment methods often rely on resource-intensive fine-tuning (FT), which may\nsuffer from knowledge degradation and face challenges in scenarios where the\nmodel accessibility or computational resources are constrained. In contrast,\ntraining-free (TF) alignment techniques--leveraging in-context learning,\ndecoding-time adjustments, and post-generation corrections--offer a promising\nalternative by enabling alignment without heavily retraining LLMs, making them\nadaptable to both open-source and closed-source environments. This paper\npresents the first systematic review of TF alignment methods, categorizing them\nby stages of pre-decoding, in-decoding, and post-decoding. For each stage, we\nprovide a detailed examination from the viewpoint of LLMs and multimodal LLMs\n(MLLMs), highlighting their mechanisms and limitations. Furthermore, we\nidentify key challenges and future directions, paving the way for more\ninclusive and effective TF alignment techniques. By synthesizing and organizing\nthe rapidly growing body of research, this survey offers a guidance for\npractitioners and advances the development of safer and more reliable LLMs."}
{"id": "2508.09115", "pdf": "https://arxiv.org/pdf/2508.09115.pdf", "abs": "https://arxiv.org/abs/2508.09115", "title": "SinLlama -- A Large Language Model for Sinhala", "authors": ["H. W. K. Aravinda", "Rashad Sirajudeen", "Samith Karunathilake", "Nisansa de Silva", "Surangika Ranathunga", "Rishemjit Kaur"], "categories": ["cs.CL"], "comment": null, "summary": "Low-resource languages such as Sinhala are often overlooked by open-source\nLarge Language Models (LLMs). In this research, we extend an existing\nmultilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM\ntokenizer with Sinhala specific vocabulary and perform continual pre-training\non a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This\nis the very first decoder-based open-source LLM with explicit Sinhala support.\nWhen SinLlama was instruction fine-tuned for three text classification tasks,\nit outperformed base and instruct variants of Llama-3-8B by a significant\nmargin."}
{"id": "2508.12733", "pdf": "https://arxiv.org/pdf/2508.12733.pdf", "abs": "https://arxiv.org/abs/2508.12733", "title": "LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models", "authors": ["Zhiyuan Ning", "Tianle Gu", "Jiaxin Song", "Shixin Hong", "Lingyu Li", "Huacan Liu", "Jie Li", "Yixu Wang", "Meng Lingyu", "Yan Teng", "Yingchun Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "7pages, 5 figures", "summary": "The widespread adoption and increasing prominence of large language models\n(LLMs) in global technologies necessitate a rigorous focus on ensuring their\nsafety across a diverse range of linguistic and cultural contexts. The lack of\na comprehensive evaluation and diverse data in existing multilingual safety\nevaluations for LLMs limits their effectiveness, hindering the development of\nrobust multilingual safety alignment. To address this critical gap, we\nintroduce LinguaSafe, a comprehensive multilingual safety benchmark crafted\nwith meticulous attention to linguistic authenticity. The LinguaSafe dataset\ncomprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated\nusing a combination of translated, transcreated, and natively-sourced data, our\ndataset addresses the critical need for multilingual safety evaluations of\nLLMs, filling the void in the safety evaluation of LLMs across diverse\nunder-represented languages from Hungarian to Malay. LinguaSafe presents a\nmultidimensional and fine-grained evaluation framework, with direct and\nindirect safety assessments, including further evaluations for oversensitivity.\nThe results of safety and helpfulness evaluations vary significantly across\ndifferent domains and different languages, even in languages with similar\nresource levels. Our benchmark provides a comprehensive suite of metrics for\nin-depth safety evaluation, underscoring the critical importance of thoroughly\nassessing multilingual safety in LLMs to achieve more balanced safety\nalignment. Our dataset and code are released to the public to facilitate\nfurther research in the field of multilingual LLM safety."}
{"id": "2508.16021", "pdf": "https://arxiv.org/pdf/2508.16021.pdf", "abs": "https://arxiv.org/abs/2508.16021", "title": "X-Troll: eXplainable Detection of State-Sponsored Information Operations Agents", "authors": ["Lin Tian", "Xiuzhen Zhang", "Maria Myung-Hee Kim", "Jennifer Biggs", "Marian-Andrei Rizoiu"], "categories": ["cs.CL"], "comment": "15 pages, 5 figures, 4 tables, accepted by CIKM2025", "summary": "State-sponsored trolls, malicious actors who deploy sophisticated linguistic\nmanipulation in coordinated information campaigns, posing threats to online\ndiscourse integrity. While Large Language Models (LLMs) achieve strong\nperformance on general natural language processing (NLP) tasks, they struggle\nwith subtle propaganda detection and operate as ``black boxes'', providing no\ninterpretable insights into manipulation strategies. This paper introduces\nX-Troll, a novel framework that bridges this gap by integrating explainable\nadapter-based LLMs with expert-derived linguistic knowledge to detect\nstate-sponsored trolls and provide human-readable explanations for its\ndecisions. X-Troll incorporates appraisal theory and propaganda analysis\nthrough specialized LoRA adapters, using dynamic gating to capture\ncampaign-specific discourse patterns in coordinated information operations.\nExperiments on real-world data demonstrate that our linguistically-informed\napproach shows strong performance compared with both general LLM baselines and\nexisting troll detection models in accuracy while providing enhanced\ntransparency through expert-grounded explanations that reveal the specific\nlinguistic strategies used by state-sponsored actors. X-Troll source code is\navailable at: https://github.com/ltian678/xtroll_source/."}
{"id": "2508.16070", "pdf": "https://arxiv.org/pdf/2508.16070.pdf", "abs": "https://arxiv.org/abs/2508.16070", "title": "Less Redundancy: Boosting Practicality of Vision Language Model in Walking Assistants", "authors": ["Chongyang Li", "Zhiqiang Yuan", "Jiapei Zhang", "Ying Deng", "Hanbo Bi", "Zexi Jia", "Xiaoyue Duan", "Peixiang Luo", "Jinchao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Approximately 283 million people worldwide live with visual impairments,\nmotivating increasing research into leveraging Visual Language Models (VLMs) to\ndevelop effective walking assistance systems for blind and low vision\nindividuals. However, existing VLMs in walking assistant task often have\noutputs that contain considerable redundancy and extraneous details, adversely\naffecting users' ability to accurately assess their surroundings. Moreover,\nthese models typically lack the capability to proactively assess environmental\nrisks and adaptively trigger reminders based on the appropriate scene, leading\nto excessive temporal redundancy. To mitigate output and temporal redundancy,\nwe propose WalkVLM-LR, a walking assistance model with less redundancy. To\nreduce output redundancy, we introduce four human-preference-based custom\nreward functions within the GRPO-based reasoning framework to optimize the\noutput in terms of conciseness, fluency, keyword density, and accuracy, thereby\nproducing more informative and streamlined outputs. To minimize temporal\nredundancy, we incorporate an environment awareness discriminator, which shares\nthe visual encoder with the VLMs to reduce redundant computations and enhance\ndiscriminative efficiency, to make WalkVLM-LR assess scene risk levels and\nminimize unnecessary reminders. Experimental results demonstrate that our\nmethod achieves state-of-the-art performance across all evaluation metrics\ncompared with other models, particularly in output conciseness and less\ntemporal redundancy."}
{"id": "2508.17670", "pdf": "https://arxiv.org/pdf/2508.17670.pdf", "abs": "https://arxiv.org/abs/2508.17670", "title": "CoCoA: Confidence and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models", "authors": ["Anant Khandelwal", "Manish Gupta", "Puneet Agrawal"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP'25, Main. 21 pages, 17 tables, 3 Figures", "summary": "Faithful generation in large language models (LLMs) is challenged by\nknowledge conflicts between parametric memory and external context. Existing\ncontrastive decoding methods tuned specifically to handle conflict often lack\nadaptability and can degrade performance in low conflict settings. We introduce\nCoCoA (Confidence- and Context-Aware Adaptive Decoding), a novel token-level\nalgorithm for principled conflict resolution and enhanced faithfulness. CoCoA\nresolves conflict by utilizing confidence-aware measures (entropy gap and\ncontextual peakedness) and the generalized divergence between the parametric\nand contextual distributions. Crucially, CoCoA maintains strong performance\neven in low conflict settings. Extensive experiments across multiple LLMs on\ndiverse Question Answering (QA), Summarization, and Long-Form Question\nAnswering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performance\nover strong baselines like AdaCAD. It yields significant gains in QA accuracy,\nup to 9.2 points on average compared to the strong baseline AdaCAD, and\nimproves factuality in summarization and LFQA by up to 2.5 points on average\nacross key benchmarks. Additionally, it demonstrates superior sensitivity to\nconflict variations. CoCoA enables more informed, context-aware, and ultimately\nmore faithful token generation."}
{"id": "2508.18473", "pdf": "https://arxiv.org/pdf/2508.18473.pdf", "abs": "https://arxiv.org/abs/2508.18473", "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing", "authors": ["Jiawei Li", "Akshayaa Magesh", "Venugopal V. Veeravalli"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages", "summary": "While Large Language Models (LLMs) have emerged as powerful foundational\nmodels to solve a variety of tasks, they have also been shown to be prone to\nhallucinations, i.e., generating responses that sound confident but are\nactually incorrect or even nonsensical. In this work, we formulate the problem\nof detecting hallucinations as a hypothesis testing problem and draw parallels\nto the problem of out-of-distribution detection in machine learning models. We\npropose a multiple-testing-inspired method to solve the hallucination detection\nproblem, and provide extensive experimental results to validate the robustness\nof our approach against state-of-the-art methods."}
{"id": "2508.18609", "pdf": "https://arxiv.org/pdf/2508.18609.pdf", "abs": "https://arxiv.org/abs/2508.18609", "title": "Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models", "authors": ["Chenxi Zhou", "Pengfei Cao", "Jiang Li", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) present significant deployment challenges due to\ntheir scale, with post-training quantization (PTQ) emerging as a practical\ncompression solution. However, a comprehensive understanding of how PTQ\nprecisely impacts diverse LLM knowledge capabilities remains elusive, and\nexisting scaling laws for quantized models often overlook crucial PTQ-specific\nparameters and task-specific sensitivities. This paper addresses these gaps by\nconducting an extensive empirical investigation to establish task-stratified\nscaling laws. We disentangle LLM knowledge into memorization and utilization\ncapabilities and develop a unified quantitative framework that incorporates\nmodel size, effective bit-width, calibration set size, and group size. Our\ncentral finding reveals that knowledge memorization exhibits markedly greater\nsensitivity to variations in effective bit-width, calibration set size, and\nmodel size compared to the more robust knowledge utilization. These findings\noffer a fine-grained understanding of PTQ's impact and provide guidance for\ndeveloping knowledge-aware quantization strategies that can better preserve\ntargeted cognitive functions."}
{"id": "2508.18648", "pdf": "https://arxiv.org/pdf/2508.18648.pdf", "abs": "https://arxiv.org/abs/2508.18648", "title": "Thinking Before You Speak: A Proactive Test-time Scaling Approach", "authors": ["Cong Liu", "Wenchang Chai", "Hejun Wu", "Yan Pan", "Pengxu Wei", "Liang Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit deficiencies with complex\nreasoning tasks, such as maths, which we attribute to the discrepancy between\nhuman reasoning patterns and those presented in the LLMs' training data. When\ndealing with complex problems, humans tend to think carefully before expressing\nsolutions. However, they often do not articulate their inner thoughts,\nincluding their intentions and chosen methodologies. Consequently, critical\ninsights essential for bridging reasoning steps may be absent in training data\ncollected from human sources. To bridge this gap, we proposes inserting\n\\emph{insight}s between consecutive reasoning steps, which review the status\nand initiate the next reasoning steps. Unlike prior prompting strategies that\nrely on a single or a workflow of static prompts to facilitate reasoning,\n\\emph{insight}s are \\emph{proactively} generated to guide reasoning processes.\nWe implement our idea as a reasoning framework, named \\emph{Thinking Before You\nSpeak} (TBYS), and design a pipeline for automatically collecting and filtering\nin-context examples for the generation of \\emph{insight}s, which alleviates\nhuman labeling efforts and fine-tuning overheads. Experiments on challenging\nmathematical datasets verify the effectiveness of TBYS. Project website:\nhttps://gitee.com/jswrt/TBYS"}
{"id": "2404.01245", "pdf": "https://arxiv.org/pdf/2404.01245.pdf", "abs": "https://arxiv.org/abs/2404.01245", "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules", "authors": ["Xiang Li", "Feng Ruan", "Huiyuan Wang", "Qi Long", "Weijie J. Su"], "categories": ["math.ST", "cs.CL", "cs.CR", "cs.LG", "stat.ML", "stat.TH", "62C05 (Primary), 62F03 (Secondary)"], "comment": "Accepted by Annals of Statistics", "summary": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments."}
{"id": "2407.12680", "pdf": "https://arxiv.org/pdf/2407.12680.pdf", "abs": "https://arxiv.org/abs/2407.12680", "title": "Reducing Biases towards Minoritized Populations in Medical Curricular Content via Artificial Intelligence for Fairer Health Outcomes", "authors": ["Chiman Salavati", "Shannon Song", "Willmar Sosa Diaz", "Scott A. Hale", "Roberto E. Montenegro", "Fabricio Murai", "Shiri Dori-Hacohen"], "categories": ["cs.CY", "cs.CL"], "comment": "Accepted at the 2024 AAAI/ACM Conference on AI, Ethics and Society\n  (AIES'24)", "summary": "Biased information (recently termed bisinformation) continues to be taught in\nmedical curricula, often long after having been debunked. In this paper, we\nintroduce BRICC, a firstin-class initiative that seeks to mitigate medical\nbisinformation using machine learning to systematically identify and flag text\nwith potential biases, for subsequent review in an expert-in-the-loop fashion,\nthus greatly accelerating an otherwise labor-intensive process. A gold-standard\nBRICC dataset was developed throughout several years, and contains over 12K\npages of instructional materials. Medical experts meticulously annotated these\ndocuments for bias according to comprehensive coding guidelines, emphasizing\ngender, sex, age, geography, ethnicity, and race. Using this labeled dataset,\nwe trained, validated, and tested medical bias classifiers. We test three\nclassifier approaches: a binary type-specific classifier, a general bias\nclassifier; an ensemble combining bias type-specific classifiers\nindependently-trained; and a multitask learning (MTL) model tasked with\npredicting both general and type-specific biases. While MTL led to some\nimprovement on race bias detection in terms of F1-score, it did not outperform\nbinary classifiers trained specifically on each task. On general bias\ndetection, the binary classifier achieves up to 0.923 of AUC, a 27.8%\nimprovement over the baseline. This work lays the foundations for debiasing\nmedical curricula by exploring a novel dataset and evaluating different\ntraining model strategies. Hence, it offers new pathways for more nuanced and\neffective mitigation of bisinformation."}
{"id": "2409.07132", "pdf": "https://arxiv.org/pdf/2409.07132.pdf", "abs": "https://arxiv.org/abs/2409.07132", "title": "LLM-based feature generation from text for interpretable machine learning", "authors": ["Vojtěch Balek", "Lukáš Sýkora", "Vilém Sklenák", "Tomáš Kliegr"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Existing text representations such as embeddings and bag-of-words are not\nsuitable for rule learning due to their high dimensionality and absent or\nquestionable feature-level interpretability. This article explores whether\nlarge language models (LLMs) could address this by extracting a small number of\ninterpretable features from text. We demonstrate this process on two datasets\n(CORD-19 and M17+) containing several thousand scientific articles from\nmultiple disciplines and a target being a proxy for research impact. An\nevaluation based on testing for the statistically significant correlation with\nresearch impact has shown that LLama 2-generated features are semantically\nmeaningful. We consequently used these generated features in text\nclassification to predict the binary target variable representing the citation\nrate for the CORD-19 dataset and the ordinal 5-class target representing an\nexpert-awarded grade in the M17+ dataset. Machine-learning models trained on\nthe LLM-generated features provided similar predictive performance to the\nstate-of-the-art embedding model SciBERT for scientific text. The LLM used only\n62 features compared to 768 features in SciBERT embeddings, and these features\nwere directly interpretable, corresponding to notions such as article\nmethodological rigor, novelty, or grammatical correctness. As the final step,\nwe extract a small number of well-interpretable action rules. Consistently\ncompetitive results obtained with the same LLM feature set across both\nthematically diverse datasets show that this approach generalizes across\ndomains."}
{"id": "2411.13868", "pdf": "https://arxiv.org/pdf/2411.13868.pdf", "abs": "https://arxiv.org/abs/2411.13868", "title": "Robust Detection of Watermarks for Large Language Models Under Human Edits", "authors": ["Xiang Li", "Feng Ruan", "Huiyuan Wang", "Qi Long", "Weijie J. Su"], "categories": ["stat.ME", "cs.CL", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "To appear in Journal of the Royal Statistical Society: Series B", "summary": "Watermarking has offered an effective approach to distinguishing text\ngenerated by large language models (LLMs) from human-written text. However, the\npervasive presence of human edits on LLM-generated text dilutes watermark\nsignals, thereby significantly degrading detection performance of existing\nmethods. In this paper, by modeling human edits through mixture model\ndetection, we introduce a new method in the form of a truncated goodness-of-fit\ntest for detecting watermarked text under human edits, which we refer to as\nTr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection\nof the Gumbel-max watermark in a certain asymptotic regime of substantial text\nmodifications and vanishing watermark signals. Importantly, Tr-GoF achieves\nthis optimality \\textit{adaptively} as it does not require precise knowledge of\nhuman edit levels or probabilistic specifications of the LLMs, in contrast to\nthe optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,\nwe establish that the Tr-GoF test attains the highest detection efficiency rate\nin a certain regime of moderate text modifications. In stark contrast, we show\nthat sum-based detection rules, as employed by existing methods, fail to\nachieve optimal robustness in both regimes because the additive nature of their\nstatistics is less resilient to edit-induced noise. Finally, we demonstrate the\ncompetitive and sometimes superior empirical performance of the Tr-GoF test on\nboth synthetic data and open-source LLMs in the OPT and LLaMA families."}
{"id": "2501.02531", "pdf": "https://arxiv.org/pdf/2501.02531.pdf", "abs": "https://arxiv.org/abs/2501.02531", "title": "Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI", "authors": ["Ljubisa Bojic", "Dylan Seychell", "Milan Cabarkapa"], "categories": ["cs.CY", "cs.CL"], "comment": "34 pages, 3 figures", "summary": "As general-purpose artificial intelligence systems become increasingly\nintegrated into society and are used for information seeking, content\ngeneration, problem solving, textual analysis, coding, and running processes,\nit is crucial to assess their long-term impact on humans. This research\nexplores the sentiment of large language models (LLMs) and humans toward\nartificial general intelligence (AGI) using a Likert-scale survey. Seven LLMs,\nincluding GPT-4 and Bard, were analyzed and compared with sentiment data from\nthree independent human sample populations. Temporal variations in sentiment\nwere also evaluated over three consecutive days. The results show a diversity\nin sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4\nrecorded the most positive sentiment toward AGI, while Bard leaned toward a\nneutral sentiment. In contrast, the human samples showed a lower average\nsentiment of 2.97. The analysis outlines potential conflicts of interest and\nbiases in the sentiment formation of LLMs, and indicates that LLMs could subtly\ninfluence societal perceptions. To address the need for regulatory oversight\nand culturally grounded assessments of AI systems, we introduce the Societal AI\nAlignment and Sentiment Benchmark (SAAS-AI), which leverages multidimensional\nprompts and empirically validated societal value frameworks to evaluate\nlanguage model outputs across temporal, model, and multilingual axes. This\nbenchmark is designed to guide policymakers and AI agencies, including within\nframeworks such as the EU AI Act, by providing robust, actionable insights into\nAI alignment with human values, public sentiment, and ethical norms at both\nnational and international levels. Future research should further refine the\noperationalization of the SAAS-AI benchmark and systematically evaluate its\neffectiveness through comprehensive empirical testing."}
{"id": "2501.04820", "pdf": "https://arxiv.org/pdf/2501.04820.pdf", "abs": "https://arxiv.org/abs/2501.04820", "title": "Unifying the Extremes: Developing a Unified Model for Detecting and Predicting Extremist Traits and Radicalization", "authors": ["Allison Lahnala", "Vasudha Varadarajan", "Lucie Flek", "H. Andrew Schwartz", "Ryan L. Boyd"], "categories": ["cs.SI", "cs.CL", "cs.CY"], "comment": "17 pages, 7 figures, 4 tables", "summary": "The proliferation of ideological movements into extremist factions via social\nmedia has become a global concern. While radicalization has been studied\nextensively within the context of specific ideologies, our ability to\naccurately characterize extremism in more generalizable terms remains\nunderdeveloped. In this paper, we propose a novel method for extracting and\nanalyzing extremist discourse across a range of online community forums. By\nfocusing on verbal behavioral signatures of extremist traits, we develop a\nframework for quantifying extremism at both user and community levels. Our\nresearch identifies 11 distinct factors, which we term ``The Extremist\nEleven,'' as a generalized psychosocial model of extremism. Applying our method\nto various online communities, we demonstrate an ability to characterize\nideologically diverse communities across the 11 extremist traits. We\ndemonstrate the power of this method by analyzing user histories from members\nof the incel community. We find that our framework accurately predicts which\nusers join the incel community up to 10 months before their actual entry with\nan AUC of $>0.6$, steadily increasing to AUC ~0.9 three to four months before\nthe event. Further, we find that upon entry into an extremist forum, the users\ntend to maintain their level of extremism within the community, while still\nremaining distinguishable from the general online discourse. Our findings\ncontribute to the study of extremism by introducing a more holistic,\ncross-ideological approach that transcends traditional, trait-specific models."}
{"id": "2501.10913", "pdf": "https://arxiv.org/pdf/2501.10913.pdf", "abs": "https://arxiv.org/abs/2501.10913", "title": "Know \"No\" Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP", "authors": ["Junsung Park", "Jungbeom Lee", "Jongyoon Song", "Sangwon Yu", "Dahuin Jung", "Sungroh Yoon"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICCV 2025", "summary": "While CLIP has significantly advanced multimodal understanding by bridging\nvision and language, the inability to grasp negation - such as failing to\ndifferentiate concepts like \"parking\" from \"no parking\" - poses substantial\nchallenges. By analyzing the data used in the public CLIP model's pre-training,\nwe posit this limitation stems from a lack of negation-inclusive data. To\naddress this, we introduce data generation pipelines that employ a large\nlanguage model (LLM) and a multimodal LLM to produce negation-inclusive\ncaptions. Fine-tuning CLIP with data generated from our pipelines, we develop\nNegationCLIP, which enhances negation awareness while preserving the\ngenerality. Moreover, to enable a comprehensive evaluation of negation\nunderstanding, we propose NegRefCOCOg-a benchmark tailored to test VLMs'\nability to interpret negation across diverse expressions and positions within a\nsentence. Experiments on various CLIP architectures validate the effectiveness\nof our data generation pipelines in enhancing CLIP's ability to perceive\nnegation accurately. Additionally, NegationCLIP's enhanced negation awareness\nhas practical applications across various multimodal tasks, demonstrated by\nperformance gains in text-to-image generation and referring image segmentation."}
{"id": "2502.20034", "pdf": "https://arxiv.org/pdf/2502.20034.pdf", "abs": "https://arxiv.org/abs/2502.20034", "title": "Do Vision Encoders Truly Explain Object Hallucination?: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore", "authors": ["Hongseok Oh", "Wonseok Hwang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recently, Large Vision-Language Models (LVLMs) show remarkable performance\nacross various domains. However, these models suffer from object hallucination.\nThis study revisits the previous claim that the cause of such hallucinations\nlies in the limited representational capacity of the vision encoder. Our\nanalysis implies that the capacity of the vision encoder is not necessarily a\nmajor limiting factor in detecting object hallucination. Based on this insight,\nwe propose Fine-grained CLIPScore (F-CLIPScore), a simple yet effective\nevaluation metric that enhances object-level granularity by incorporating text\nembeddings at the noun level. Evaluations on the OHD-Caps benchmark show that\nF-CLIPScore significantly outperforms conventional CLIPScore in accuracy by a\nlarge margin of \\textbf{39.6\\%} without additional training. We further\ndemonstrate that F-CLIPScore-based data filtering reduces object hallucination\nin LVLM (4.9\\% in POPE)."}
{"id": "2503.11519", "pdf": "https://arxiv.org/pdf/2503.11519.pdf", "abs": "https://arxiv.org/abs/2503.11519", "title": "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models", "authors": ["Hao Cheng", "Erjia Xiao", "Yichi Wang", "Lingfeng Zhang", "Qiang Zhang", "Jiahang Cao", "Kaidi Xu", "Mengshu Sun", "Xiaoshuai Hao", "Jindong Gu", "Renjing Xu"], "categories": ["cs.CV", "cs.CL"], "comment": "This paper is accepted by IJCAI2025 Workshop on Deepfake Detection,\n  Localization, and Interpretability", "summary": "Current Cross-Modality Generation Models (GMs) demonstrate remarkable\ncapabilities in various generative tasks. Given the ubiquity and information\nrichness of vision modality inputs in real-world scenarios, Cross-Vision tasks,\nencompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), have\nattracted significant attention. Large Vision Language Models (LVLMs) and I2I\nGeneration Models (GMs) are employed to handle VLP and I2I tasks, respectively.\nPrevious research indicates that printing typographic words into input images\nsignificantly induces LVLMs and I2I GMs to produce disruptive outputs that are\nsemantically aligned with those words. Additionally, visual prompts, as a more\nsophisticated form of typography, are also revealed to pose security risks to\nvarious applications of cross-vision tasks. However, the specific\ncharacteristics of the threats posed by visual prompts remain underexplored. In\nthis paper, to comprehensively investigate the performance impact induced by\nTypographic Visual Prompt Injection (TVPI) in various LVLMs and I2I GMs, we\npropose the Typographic Visual Prompts Injection Dataset and thoroughly\nevaluate the TVPI security risks on various open-source and closed-source LVLMs\nand I2I GMs under visual prompts with different target semantics, deepening the\nunderstanding of TVPI threats."}
{"id": "2504.05220", "pdf": "https://arxiv.org/pdf/2504.05220.pdf", "abs": "https://arxiv.org/abs/2504.05220", "title": "Utility-Focused LLM Annotation for Retrieval and Retrieval-Augmented Generation", "authors": ["Hengran Zhang", "Minghao Tang", "Keping Bi", "Jiafeng Guo", "Shihao Liu", "Daiting Shi", "Dawei Yin", "Xueqi Cheng"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted by the EMNLP25 main conference", "summary": "This paper explores the use of large language models (LLMs) for annotating\ndocument utility in training retrieval and retrieval-augmented generation (RAG)\nsystems, aiming to reduce dependence on costly human annotations. We address\nthe gap between retrieval relevance and generative utility by employing LLMs to\nannotate document utility. To effectively utilize multiple positive samples per\nquery, we introduce a novel loss that maximizes their summed marginal\nlikelihood. Using the Qwen-2.5-32B model, we annotate utility on the MS MARCO\ndataset and conduct retrieval experiments on MS MARCO and BEIR, as well as RAG\nexperiments on MS MARCO QA, NQ, and HotpotQA. Our results show that\nLLM-generated annotations enhance out-of-domain retrieval performance and\nimprove RAG outcomes compared to models trained solely on human annotations or\ndownstream QA metrics. Furthermore, combining LLM annotations with just 20% of\nhuman labels achieves performance comparable to using full human annotations.\nOur study offers a comprehensive approach to utilizing LLM annotations for\ninitializing QA systems on new corpora."}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717.pdf", "abs": "https://arxiv.org/abs/2505.11717", "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "EMNLP 2025 main", "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--denoted as\nthe target action. However, existing attacks suffer from limited effectiveness\nor stealthiness, or are impractical in real-world settings. In this work, we\npropose EnvInjection, a new attack that addresses these limitations. Our attack\nadds a perturbation to the raw pixel values of the rendered webpage. After\nthese perturbed pixels are mapped into a screenshot, the perturbation induces\nthe web agent to perform the target action. We formulate the task of finding\nthe perturbation as an optimization problem. A key challenge in solving this\nproblem is that the mapping between raw pixel values and screenshot is\nnon-differentiable, making it difficult to backpropagate gradients to the\nperturbation. To overcome this, we train a neural network to approximate the\nmapping and apply projected gradient descent to solve the reformulated\noptimization problem. Extensive evaluation on multiple webpage datasets shows\nthat EnvInjection is highly effective and significantly outperforms existing\nbaselines."}
{"id": "2506.18088", "pdf": "https://arxiv.org/pdf/2506.18088.pdf", "abs": "https://arxiv.org/abs/2506.18088", "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation", "authors": ["Tianxing Chen", "Zanxin Chen", "Baijun Chen", "Zijian Cai", "Yibin Liu", "Zixuan Li", "Qiwei Liang", "Xianliang Lin", "Yiheng Ge", "Zhenyu Gu", "Weiliang Deng", "Yubin Guo", "Tian Nian", "Xuanbing Xie", "Qiangyu Chen", "Kailun Su", "Tianling Xu", "Guodong Liu", "Mengkang Hu", "Huan-ang Gao", "Kaixuan Wang", "Zhixuan Liang", "Yusen Qin", "Xiaokang Yang", "Ping Luo", "Yao Mu"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "comment": "Project Page: https://robotwin-platform.github.io/, Code:\n  https://github.com/robotwin-Platform/robotwin, Doc:\n  https://robotwin-platform.github.io/doc/", "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nadvancing real-world robotic manipulation. Yet existing datasets remain\ninsufficient for robust bimanual manipulation due to (1) the lack of scalable\ntask generation methods and (2) oversimplified simulation environments. We\npresent RoboTwin 2.0, a scalable framework for automated, large-scale\ngeneration of diverse and realistic data, together with unified evaluation\nprotocols for dual-arm manipulation. At its core is RoboTwin-OD, an object\nlibrary of 731 instances across 147 categories with semantic and\nmanipulation-relevant annotations. Building on this, we design an expert data\nsynthesis pipeline that leverages multimodal language models (MLLMs) and\nsimulation-in-the-loop refinement to automatically generate task-level\nexecution code. To improve sim-to-real transfer, RoboTwin 2.0 applies\nstructured domain randomization along five axes: clutter, lighting, background,\ntabletop height, and language, enhancing data diversity and policy robustness.\nThe framework is instantiated across 50 dual-arm tasks and five robot\nembodiments. Empirically, it yields a 10.9% gain in code generation success\nrate. For downstream policy learning, a VLA model trained with synthetic data\nplus only 10 real demonstrations achieves a 367% relative improvement over the\n10-demo baseline, while zero-shot models trained solely on synthetic data\nobtain a 228% gain. These results highlight the effectiveness of RoboTwin 2.0\nin strengthening sim-to-real transfer and robustness to environmental\nvariations. We release the data generator, benchmark, dataset, and code to\nsupport scalable research in robust bimanual manipulation. Project Page:\nhttps://robotwin-platform.github.io/, Code:\nhttps://github.com/robotwin-Platform/robotwin/."}
{"id": "2508.03772", "pdf": "https://arxiv.org/pdf/2508.03772.pdf", "abs": "https://arxiv.org/abs/2508.03772", "title": "GTPO: Trajectory-Based Policy Optimization in Large Language Models", "authors": ["Marco Simoni", "Aleksandar Fontana", "Giulio Rossolini", "Andrea Saracino"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Policy-based optimizations are widely adopted today for the training and\nalignment of language models, where one of the most recent and effective\napproaches is Group-relative Policy Optimization (GRPO). In this paper, we\nreveals and analyze two major limitations of GRPO: (i) tokens frequently appear\nin completions with both positive and negative rewards, leading to conflicting\ngradient updates that can reduce their output probability, even though can be\nessential for maintaining proper structure; (ii) negatively rewarded\ncompletions may penalize confident responses and shift model decisions toward\nunlikely tokens, progressively flattening the output distribution and degrading\nlearning. To address these issues and provide a more stable and effective\npolicy optimization strategy, we introduce GTPO (Group-relative\nTrajectory-based Policy Optimization), which identifies conflict tokens, tokens\nappearing in the same position across completions with opposite rewards,\nprotects them by skipping negative updates, while amplifying positive ones. To\nfurther prevent policy collapse, GTPO filters out completions whose entropy\nexceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence\nregularization, eliminating the need for a reference model during training,\nwhile still ensuring greater training stability and improved performance,\nvalidated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks."}
{"id": "2508.05004", "pdf": "https://arxiv.org/pdf/2508.05004.pdf", "abs": "https://arxiv.org/abs/2508.05004", "title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data", "authors": ["Chengsong Huang", "Wenhao Yu", "Xiaoyang Wang", "Hongming Zhang", "Zongxia Li", "Ruosen Li", "Jiaxin Huang", "Haitao Mi", "Dong Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Self-evolving Large Language Models (LLMs) offer a scalable path toward\nsuper-intelligence by autonomously generating, refining, and learning from\ntheir own experiences. However, existing methods for training such models still\nrely heavily on vast human-curated tasks and labels, typically via fine-tuning\nor reinforcement learning, which poses a fundamental bottleneck to advancing AI\nsystems toward capabilities beyond human intelligence. To overcome this\nlimitation, we introduce R-Zero, a fully autonomous framework that generates\nits own training data from scratch. Starting from a single base LLM, R-Zero\ninitializes two independent models with distinct roles, a Challenger and a\nSolver. These models are optimized separately and co-evolve through\ninteraction: the Challenger is rewarded for proposing tasks near the edge of\nthe Solver capability, and the Solver is rewarded for solving increasingly\nchallenging tasks posed by the Challenger. This process yields a targeted,\nself-improving curriculum without any pre-existing tasks and labels.\nEmpirically, R-Zero substantially improves reasoning capability across\ndifferent backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on\nmath-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks."}
{"id": "2508.13654", "pdf": "https://arxiv.org/pdf/2508.13654.pdf", "abs": "https://arxiv.org/abs/2508.13654", "title": "Input-Time Scaling", "authors": ["Rapheal Huang", "Weilong Guo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input-Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we utilize\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\ndiscover a new phenomenon, train-test co-design. It requires us to apply query\nstrategies during training and testing as a whole. Only applying strategies on\ntraining or testing would seriously degrade the performance gained. We are also\nsurprised to find that seemingly low data quality datasets can perform better.\nWe can get the best performance even by adding irrelevant information to the\nqueries, with randomly selected 1k examples from a minimally filtered dataset.\nThese findings contradict the widely held inductive bias, \"garbage in, garbage\nout\". Curating datasets with seemingly high-quality data can even potentially\nlimit the performance ceiling. In addition, models trained on more data with\nsimilar quality (15k VS 1k) perform worse, the intuition of simply scaling the\nsize should also be carefully inspected. The good news is that our findings are\ncompatible with the Less is More phenomenon. 1K examples are enough to invoke\nhigh-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are\nable to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints."}
{"id": "2508.16439", "pdf": "https://arxiv.org/pdf/2508.16439.pdf", "abs": "https://arxiv.org/abs/2508.16439", "title": "PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark", "authors": ["Adil Bahaj", "Oumaima Fadi", "Mohamed Chetouani", "Mounir Ghogho"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.GR", "cs.MM"], "comment": null, "summary": "Large language models (LLMs) and vision-augmented LLMs (VLMs) have\nsignificantly advanced medical informatics, diagnostics, and decision support.\nHowever, these models exhibit systematic biases, particularly age bias,\ncompromising their reliability and equity. This is evident in their poorer\nperformance on pediatric-focused text and visual question-answering tasks. This\nbias reflects a broader imbalance in medical research, where pediatric studies\nreceive less funding and representation despite the significant disease burden\nin children. To address these issues, a new comprehensive multi-modal pediatric\nquestion-answering benchmark, PediatricsMQA, has been introduced. It consists\nof 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric\ntopics across seven developmental stages (prenatal to adolescent) and 2,067\nvision-based MCQs using 634 pediatric images from 67 imaging modalities and 256\nanatomical regions. The dataset was developed using a hybrid manual-automatic\npipeline, incorporating peer-reviewed pediatric literature, validated question\nbanks, existing benchmarks, and existing QA resources. Evaluating\nstate-of-the-art open models, we find dramatic performance drops in younger\ncohorts, highlighting the need for age-aware methods to ensure equitable AI\nsupport in pediatric care."}
{"id": "2508.19229", "pdf": "https://arxiv.org/pdf/2508.19229.pdf", "abs": "https://arxiv.org/abs/2508.19229", "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning", "authors": ["Wei Xiong", "Wenting Zhao", "Weizhe Yuan", "Olga Golovneva", "Tong Zhang", "Jason Weston", "Sainbayar Sukhbaatar"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search."}
