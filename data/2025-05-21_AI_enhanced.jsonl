{"id": "2505.13612", "pdf": "https://arxiv.org/pdf/2505.13612.pdf", "abs": "https://arxiv.org/abs/2505.13612", "title": "Sight, Sound and Smell in Immersive Experiences of Urban History: Virtual Vauxhall Gardens Case Study", "authors": ["Tim Pearce", "David Souto", "Douglas Barrett", "Benjamin Lok", "Mateusz Bocian", "Artur Soczawa-Stronczyk", "Giasemi Vavoula", "Paul Long", "Avinash Bhangaonkar", "Stephanie Bowry", "Michaela Butter", "David Coke", "Kate Loveman", "Rosemary Sweet", "Lars Tharp", "Jeremy Webster", "Hongji Yang", "Robin Green", "Andrew Hugill"], "categories": ["cs.HC", "cs.CY"], "comment": "24 pages, 10 figures", "summary": "We explore the integration of multisensory elements in virtual reality\nreconstructions of historical spaces through a case study of the Virtual\nVauxhall Gardens project. While visual and auditory components have become\nstandard in digital heritage experiences, the addition of olfactory stimuli\nremains underexplored, despite its powerful connection to memory and emotional\nengagement. This research investigates how multisensory experiences involving\nolfaction can be effectively integrated into VR reconstructions of historical\nspaces to enhance presence and engagement with cultural heritage. In the\ncontext of a VR reconstruction of London's eighteenth-century Vauxhall Pleasure\nGardens, we developed a networked portable olfactory display capable of\nsynchronizing specific scents with visual and auditory elements at pivotal\nmoments in the virtual experience. Our evaluation methodology assesses both\ntechnical implementation and user experience, measuring presence, and usability\nmetrics across diverse participant groups. Our results show that integrating\nsynchronized olfactory stimuli into the VR experience can enhance user\nengagement and be perceived positively, contributing to a unique and immersive\nencounter with historical settings. While presence questionnaires indicated a\nstrong sense of auditory presence and control, with other sensory factors rated\nmoderately, user experience of attractiveness was exceptionally high;\nqualitative feedback suggested heightened sensory awareness and engagement\ninfluenced by the inclusion and anticipation of smell. Our results suggest that\nevaluating multisensory VR heritage experiences requires a nuanced approach, as\nstandard usability metrics may be ill-suited and 'realism' might be less\ncritical than creating an evocative, historically informed, and emotionally\nresonant experience......", "AI": {"tldr": "This paper investigates the integration of olfactory elements in virtual reality (VR) reconstructions of historical spaces, focusing on enhancing user engagement in cultural heritage experiences through multisensory stimuli.", "motivation": "The research aims to enhance the immersion of digital heritage experiences by incorporating olfactory stimuli, which have been underexplored despite their strong connection to memory and emotional engagement.", "method": "A case study of the Virtual Vauxhall Gardens project, which involved developing a networked portable olfactory display to synchronize scents with visual and auditory components in a VR experience.", "result": "The integration of synchronized olfactory stimuli into the VR experience was found to enhance user engagement and was positively received by participants, indicating a unique immersive encounter with the historical setting.", "conclusion": "Evaluating multisensory VR heritage experiences requires a nuanced approach, suggesting that traditional usability metrics may not fully capture user experience and that creating evocative and emotionally resonant experiences is paramount.", "key_contributions": ["Integration of olfactory stimuli into VR historical reconstructions", "Development of a networked portable olfactory display", "Evaluation of multisensory user experience metrics"], "limitations": "Standard usability metrics may be ill-suited for evaluating multisensory experiences.", "keywords": ["Virtual Reality", "multisensory elements", "olfaction", "user engagement", "cultural heritage"], "importance_score": 4, "read_time_minutes": 24}}
{"id": "2505.13648", "pdf": "https://arxiv.org/pdf/2505.13648.pdf", "abs": "https://arxiv.org/abs/2505.13648", "title": "Conceptual Modeling: Topics, Themes, and Technology Trends", "authors": ["V. C. Storey", "R. Lukyanenko", "A. Castellanos"], "categories": ["cs.HC", "cs.DB"], "comment": null, "summary": "Conceptual modeling is an important part of information systems development\nand use that involves identifying and representing relevant aspects of reality.\nAlthough the past decades have experienced continuous digitalization of\nservices and products that impact business and society, conceptual modeling\nefforts are still required to support new technologies as they emerge. This\npaper surveys research on conceptual modeling over the past five decades and\nshows how its topics and trends continue to evolve to accommodate emerging\ntechnologies, while remaining grounded in basic constructs. We survey over\n5,300 papers that address conceptual modeling topics from the 1970s to the\npresent, which are collected from 35 multidisciplinary journals and\nconferences, and use them as the basis from which to analyze the progression of\nconceptual modeling. The important role that conceptual modeling should play in\nour evolving digital world is discussed, and future research directions\nproposed.", "AI": {"tldr": "This paper surveys conceptual modeling research over five decades, analyzing its evolution and impact on emerging technologies.", "motivation": "To understand the evolution of conceptual modeling in response to digitalization and emerging technologies.", "method": "Survey of over 5,300 papers from 35 multidisciplinary journals and conferences covering conceptual modeling topics from the 1970s to the present.", "result": "Identified evolving topics and trends in conceptual modeling to accommodate new technologies while retaining foundational constructs.", "conclusion": "Conceptual modeling is crucial for future digital developments, and further research directions are proposed.", "key_contributions": ["Comprehensive survey of over 5,300 conceptual modeling papers", "Analysis of trends over five decades in relation to emerging technologies", "Discussion on the future role of conceptual modeling in digitalization"], "limitations": "", "keywords": ["conceptual modeling", "digitalization", "emerging technologies"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2505.13688", "pdf": "https://arxiv.org/pdf/2505.13688.pdf", "abs": "https://arxiv.org/abs/2505.13688", "title": "Gaze-Enhanced Multimodal Turn-Taking Prediction in Triadic Conversations", "authors": ["Seongsil Heo", "Calvin Murdock", "Michael Proulx", "Christi Miller"], "categories": ["cs.HC"], "comment": null, "summary": "Turn-taking prediction is crucial for seamless interactions. This study\nintroduces a novel, lightweight framework for accurate turn-taking prediction\nin triadic conversations without relying on computationally intensive methods.\nUnlike prior approaches that either disregard gaze or treat it as a passive\nsignal, our model integrates gaze with speaker localization, structuring it\nwithin a spatial constraint to transform it into a reliable predictive cue.\nLeveraging egocentric behavioral cues, our experiments demonstrate that\nincorporating gaze data from a single-user significantly improves prediction\nperformance, while gaze data from multiple-users further enhances it by\ncapturing richer conversational dynamics. This study presents a lightweight and\nprivacy-conscious approach to support adaptive, directional sound control,\nenhancing speech intelligibility in noisy environments, particularly for\nhearing assistance in smart glasses.", "AI": {"tldr": "This study presents a lightweight framework for accurate turn-taking prediction in triadic conversations, integrating gaze with speaker localization to improve interaction quality.", "motivation": "To develop an effective method for turn-taking prediction that enhances conversational interactions while being computationally efficient.", "method": "The study integrates gaze and speaker localization within a spatial constraint and leverages egocentric behavioral cues for prediction.", "result": "Incorporating gaze data significantly improves prediction performance, with multiple-user gaze data enhancing conversational dynamics.", "conclusion": "The proposed model offers a privacy-conscious solution for adaptive sound control, improving speech intelligibility for hearing assistance in smart glasses.", "key_contributions": ["Introduction of a lightweight framework for turn-taking prediction.", "Integration of gaze with speaker localization for improved predictive accuracy.", "Enhancement of speech intelligibility in noisy environments for hearing assistance."], "limitations": "", "keywords": ["turn-taking prediction", "gaze integration", "triadic conversations", "speech intelligibility", "adaptive sound control"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2505.13953", "pdf": "https://arxiv.org/pdf/2505.13953.pdf", "abs": "https://arxiv.org/abs/2505.13953", "title": "Human Authenticity and Flourishing in an AI-Driven World: Edmund's Journey and the Call for Mindfulness", "authors": ["Sebastian Zepf", "Mark Colley"], "categories": ["cs.HC"], "comment": null, "summary": "Humans have always dreamed of possessing superpowers, and the rapid\ndevelopment of AI-based features promises to bring these dreams (closer) to\nreality. However, these advancements come with significant risks. This paper\nadvocates for challenging existing methods and approaches in design and\nevaluation for more responsible AI. We stimulate reflection through a\nfuturistic user journey illustrating the AI-driven life of Edmund in 2035.\nSubsequently, we discuss four AI-based superpowers: extended perception,\ncognitive offloading, externalized memory, and enhanced presence. We then\ndiscuss implications for HCI and AI, emphasizing the need for preserving\nintrinsic human superpowers, identifying meaningful use cases for AI, and\nevaluating AI's impact on human abilities. This paper advocates for responsible\nand reflective AI integration and proposes a pathway towards the idea of a\nHuman Flourishing Benchmark.", "AI": {"tldr": "The paper discusses the integration of AI into human experiences, highlighting its potential benefits and risks through a futuristic narrative and exploration of AI-driven 'superpowers.'", "motivation": "To reflect on the responsible integration of AI technologies into human experiences and challenge existing design methods in HCI.", "method": "The paper uses a futuristic user journey and identifies four AI-based superpowers to illustrate the implications of AI on human abilities and experiences.", "result": "The exploration reveals significant implications for HCI and AI, advocating for responsible development that preserves essential human capabilities while identifying meaningful applications of AI.", "conclusion": "The authors propose a Human Flourishing Benchmark as a framework for evaluating the impact of AI on human abilities and ensuring a responsible integration of AI technologies.", "key_contributions": ["Illustration of a futuristic user journey that highlights AI's impact on human life.", "Identification of four AI-driven superpowers and their implications for HCI.", "Proposal of a Human Flourishing Benchmark for responsible AI integration."], "limitations": "", "keywords": ["AI integration", "Human-Computer Interaction", "responsible AI", "superpowers", "future of AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13480", "pdf": "https://arxiv.org/pdf/2505.13480.pdf", "abs": "https://arxiv.org/abs/2505.13480", "title": "Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale", "authors": ["Avinash Patil", "Siru Tao", "Amardeep Gedhu"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "8 Pages, 6 Figures, 1 Table", "summary": "Suicide prevention remains a critical public health challenge. While online\nplatforms such as Reddit's r/SuicideWatch have historically provided spaces for\nindividuals to express suicidal thoughts and seek community support, the advent\nof large language models (LLMs) introduces a new paradigm-where individuals may\nbegin disclosing ideation to AI systems instead of humans. This study evaluates\nthe capability of LLMs to perform automated suicide risk assessment using the\nColumbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot\nperformance of six models-including Claude, GPT, Mistral, and LLaMA-in\nclassifying posts across a 7-point severity scale (Levels 0-6). Results\nindicate that Claude and GPT closely align with human annotations, while\nMistral achieves the lowest ordinal prediction error. Most models exhibit\nordinal sensitivity, with misclassifications typically occurring between\nadjacent severity levels. We further analyze confusion patterns,\nmisclassification sources, and ethical considerations, underscoring the\nimportance of human oversight, transparency, and cautious deployment. Full code\nand supplementary materials are available at\nhttps://github.com/av9ash/llm_cssrs_code.", "AI": {"tldr": "This study evaluates the effectiveness of large language models in automating suicide risk assessment using a severity scale.", "motivation": "To address the critical public health challenge of suicide prevention by exploring the role of AI in assessing suicide risk on online platforms.", "method": "The study assesses the zero-shot performance of six language models (Claude, GPT, Mistral, LLaMA) in classifying posts using the Columbia-Suicide Severity Rating Scale (C-SSRS).", "result": "Claude and GPT closely align with human annotations, while Mistral shows the lowest prediction error; models generally exhibit ordinal sensitivity with common misclassifications between adjacent levels.", "conclusion": "The findings highlight the need for human oversight in AI-driven assessments and the ethical considerations necessary for deployment.", "key_contributions": ["Evaluation of LLMs for automated suicide risk assessment", "Comparison of multiple models on a standardized scale", "Discussion of ethical implications and need for human oversight"], "limitations": "Analysis limited to specific LLMs and the C-SSRS; may not generalize to all AI applications in mental health.", "keywords": ["suicide prevention", "large language models", "C-SSRS", "risk assessment", "ethical considerations"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14031", "pdf": "https://arxiv.org/pdf/2505.14031.pdf", "abs": "https://arxiv.org/abs/2505.14031", "title": "Reading.help: Supporting EFL Readers with Proactive and On-Demand Explanation of English Grammar and Semantics", "authors": ["Sunghyo Chung", "Hyeon Jeon", "Sungbok Shin", "Md Naimul Hoque"], "categories": ["cs.HC"], "comment": "Preprint", "summary": "A large portion of texts in the world is written in English, but readers who\nsee English as a Foreign Language (EFL) often struggle to read texts written in\nEnglish accurately and swiftly. In many countries, EFL readers seek help from\nprofessional teachers and mentors, which is limited and costly. In this paper,\nwe explore how an intelligent reading tool can assist EFL readers. To support\nour research agenda, we conducted a case study with EFL readers in South Korea.\nWe at first developed an LLM-based reading tool based on prior literature. We\nthen revised the tool based on the feedback from a study with 15 South Korean\nEFL readers. The final tool, named Reading.help, helps EFL readers comprehend\ncomplex sentences and paragraphs with on-demand and proactive explanations. We\nfinally evaluated the tool with 5 EFL readers and 2 EFL education\nprofessionals. Our findings suggest Reading.help could potentially help EFL\nreaders self-learn english when they do not have access to any external\nsupport.", "AI": {"tldr": "An intelligent reading tool, Reading.help, assists English as a Foreign Language (EFL) readers by providing on-demand explanations to improve comprehension.", "motivation": "EFL readers often struggle with reading English texts accurately and quickly, and existing support from teachers is limited and costly.", "method": "Developed an LLM-based reading tool called Reading.help, revised it based on feedback from a case study with 15 EFL readers in South Korea, and evaluated the tool with both EFL readers and education professionals.", "result": "The findings indicate that Reading.help could significantly aid EFL readers in self-learning English by improving their comprehension of complex sentences and paragraphs.", "conclusion": "Reading.help shows promise as a supportive tool for EFL readers who lack external learning resources.", "key_contributions": ["Development of an LLM-based reading tool for EFL readers", "Conducted a case study to refine tool effectiveness", "Demonstrated potential for enhancing self-learning in EFL contexts"], "limitations": "Limited sample size and context-specific findings pertaining to South Korean EFL readers.", "keywords": ["EFL readers", "LLM-based tool", "self-learning", "reading comprehension", "English as a Foreign Language"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.13483", "pdf": "https://arxiv.org/pdf/2505.13483.pdf", "abs": "https://arxiv.org/abs/2505.13483", "title": "EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors", "authors": ["Xingyuan Lu", "Yuxi Liu", "Dongyu Zhang", "Zhiyao Wu", "Jing Ren", "Feng Xia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Metaphors play a pivotal role in expressing emotions, making them crucial for\nemotional intelligence. The advent of multimodal data and widespread\ncommunication has led to a proliferation of multimodal metaphors, amplifying\nthe complexity of emotion classification compared to single-mode scenarios.\nHowever, the scarcity of research on constructing multimodal metaphorical\nfine-grained emotion datasets hampers progress in this domain. Moreover,\nexisting studies predominantly focus on English, overlooking potential\nvariations in emotional nuances across languages. To address these gaps, we\nintroduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of\nmetaphorical advertisements. Each entry is meticulously annotated for metaphor\noccurrence, domain relations and fine-grained emotion classification\nencompassing joy, love, trust, fear, sadness, disgust, anger, surprise,\nanticipation, and neutral. Our dataset is publicly accessible\n(https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in\nthis burgeoning field.", "AI": {"tldr": "The paper introduces a multimodal dataset of 5,000 Chinese text-image pairs focused on metaphorical advertisements, aimed at enhancing emotion classification.", "motivation": "To address the lack of multimodal metaphorical fine-grained emotion datasets and to explore emotional nuances across languages beyond English.", "method": "The authors developed a dataset containing 5,000 annotated text-image pairs for metaphor occurrence and fine-grained emotion classification.", "result": "The dataset includes annotations for various emotions such as joy, love, trust, fear, sadness, disgust, anger, surprise, anticipation, and neutral, contributing to the field of emotional intelligence.", "conclusion": "The publicly accessible dataset will enable research advancements in multimodal metaphors and emotion classification.", "key_contributions": ["Introduction of a multimodal dataset in Chinese", "Annotation of metaphor occurrence and emotional dimensions", "Focus on fine-grained emotion classification"], "limitations": "", "keywords": ["multimodal metaphors", "emotion classification", "dataset", "Chinese", "advertisements"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.14074", "pdf": "https://arxiv.org/pdf/2505.14074.pdf", "abs": "https://arxiv.org/abs/2505.14074", "title": "Recreating Neural Activity During Speech Production with Language and Speech Model Embeddings", "authors": ["Owais Mujtaba Khanday", "Pablo Rodroguez San Esteban", "Zubair Ahmad Lone", "Marc Ouellet", "Jose Andres Gonzalez Lopez"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "Accepted for presentation at Interspeech2025", "summary": "Understanding how neural activity encodes speech and language production is a\nfundamental challenge in neuroscience and artificial intelligence. This study\ninvestigates whether embeddings from large-scale, self-supervised language and\nspeech models can effectively reconstruct neural activity recordings captured\nduring speech production. We leverage pre-trained embeddings from deep learning\nmodels trained on linguistic and acoustic data to represent high-level speech\nfeatures and map them onto neural signals. We analyze the extent to which these\nembeddings preserve the spatio-temporal dynamics of brain activity. We evaluate\nreconstructed neural signals against ground truth recordings using correlation\nmetrics and signal reconstruction quality assessments. The results indicate\nthat neural activity can be effectively reconstructed using embeddings from\nlarge language and speech models across all study participants, yielding\nPearson correlation coefficients ranging from 0.79 to 0.99.", "AI": {"tldr": "This study investigates the use of large-scale language and speech model embeddings to reconstruct neural activity recordings during speech production, demonstrating high correlation with true neural signals.", "motivation": "The research addresses the challenge of understanding neural encoding of speech and language production, linking neuroscience with artificial intelligence.", "method": "The study employs pre-trained embeddings from deep learning models on linguistic and acoustic data, mapping them onto neural signals and evaluating the reconstructions using correlation metrics and quality assessments.", "result": "Embeddings from language and speech models effectively reconstruct neural activity, achieving Pearson correlation coefficients between 0.79 and 0.99 across participants.", "conclusion": "The findings suggest that large-scale embeddings can accurately capture the spatio-temporal dynamics of brain activity related to speech production.", "key_contributions": ["Innovative application of self-supervised embeddings to neural signal reconstruction", "Demonstration of high correlation between reconstructed and actual neural data", "Insights into the intersection of neuroscience and AI in language production"], "limitations": "", "keywords": ["neuroscience", "speech production", "language models"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.13487", "pdf": "https://arxiv.org/pdf/2505.13487.pdf", "abs": "https://arxiv.org/abs/2505.13487", "title": "Detecting Prefix Bias in LLM-based Reward Models", "authors": ["Ashwin Kumar", "Yuzi He", "Aram H. Markosyan", "Bobbie Chern", "Imanol Arrieta-Ibarra"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Human Feedback (RLHF) has emerged as a key\nparadigm for task-specific fine-tuning of language models using human\npreference data. While numerous publicly available preference datasets provide\npairwise comparisons of responses, the potential for biases in the resulting\nreward models remains underexplored. In this work, we introduce novel methods\nto detect and evaluate prefix bias -- a systematic shift in model preferences\ntriggered by minor variations in query prefixes -- in LLM-based reward models\ntrained on such datasets. We leverage these metrics to reveal significant\nbiases in preference models across racial and gender dimensions. Our\ncomprehensive evaluation spans diverse open-source preference datasets and\nreward model architectures, demonstrating susceptibility to this kind of bias\nregardless of the underlying model architecture. Furthermore, we propose a data\naugmentation strategy to mitigate these biases, showing its effectiveness in\nreducing the impact of prefix bias. Our findings highlight the critical need\nfor bias-aware dataset design and evaluation in developing fair and reliable\nreward models, contributing to the broader discourse on fairness in AI.", "AI": {"tldr": "This paper addresses biases in reinforcement learning reward models influenced by query prefixes and proposes methods to detect and mitigate these biases.", "motivation": "The study aims to explore and mitigate biases in reward models trained on human preference data, particularly concerning racial and gender dimensions.", "method": "The authors introduce novel methods to detect prefix bias in reward models and evaluate their impact across diverse datasets and model architectures. They also propose a data augmentation strategy to alleviate these biases.", "result": "The evaluation reveals significant biases in LLM-based reward models across racial and gender categories, highlighting their susceptibility to prefix bias.", "conclusion": "There is a critical need for bias-aware dataset design and evaluation to develop fair and reliable reward models in AI.", "key_contributions": ["Introduction of methods to detect prefix bias in reward models", "Demonstration of significant biases across various datasets and architectures", "Proposal of a data augmentation strategy to mitigate biases"], "limitations": "The study may not cover all possible sources of bias and focuses primarily on prefix bias.", "keywords": ["Reinforcement Learning", "Human Feedback", "Bias Detection", "Language Models", "Fairness in AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.14078", "pdf": "https://arxiv.org/pdf/2505.14078.pdf", "abs": "https://arxiv.org/abs/2505.14078", "title": "The Virtual Reality Koinos Method: Analyzing Virtual Reality Collaboration from the perspective of communication models", "authors": ["Eloise Minder", "Sylvain Fleury", "Solène Neyret", "Jean-Rémy Chardonnet"], "categories": ["cs.HC"], "comment": "25 pages, 7 figures, in preprint for a journal (currently in review\n  process)", "summary": "Understanding which factors could influence co-presence in Virtual Reality\ncould help develop more qualitative social interactions, or social interactions\nthat generate similar sensations, emotions and feelings than the ones generated\nduring Face-to-Face interactions. Co-presence is studied since the beginning of\nVirtual Reality (VR); though, no consensus is identified on what factors could\ninfluence it, except the consensus on the definition of \"being there together\"\ninside the Virtual Environment. In this paper, we introduce the Koinos method\nto explain social interactions in VR through communication models, (i)\ntheoretically, and (ii) on two VR experiments that change the virtual partner\nsocial and physical representations. These analyses lead us to propose an\nequation to predict and help manage the sense of co-presence in VR.", "AI": {"tldr": "This paper introduces the Koinos method to analyze co-presence in Virtual Reality through communication models and proposes an equation for predicting co-presence.", "motivation": "To enhance social interactions in Virtual Reality that emulate the qualitative aspects of Face-to-Face interactions.", "method": "The paper utilizes the Koinos method, exploring theoretical communication models and conducting two VR experiments to assess variations in social and physical representations of virtual partners.", "result": "The research leads to the proposal of a predictive equation for managing the sense of co-presence in VR environments.", "conclusion": "By understanding the factors influencing co-presence, VR experiences can be designed to foster more meaningful social interactions.", "key_contributions": ["Introduction of the Koinos method for understanding co-presence in VR", "Conducting two experimental analyses to assess social interaction dynamics", "Proposing a predictive equation for enhancing co-presence in VR environments."], "limitations": "", "keywords": ["Co-presence", "Virtual Reality", "Social Interaction", "Koinos Method", "Communication Models"], "importance_score": 7, "read_time_minutes": 25}}
{"id": "2505.13488", "pdf": "https://arxiv.org/pdf/2505.13488.pdf", "abs": "https://arxiv.org/abs/2505.13488", "title": "Source framing triggers systematic evaluation bias in Large Language Models", "authors": ["Federico Germani", "Giovanni Spitale"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used not only to generate text\nbut also to evaluate it, raising urgent questions about whether their judgments\nare consistent, unbiased, and robust to framing effects. In this study, we\nsystematically examine inter- and intra-model agreement across four\nstate-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and\nMistral) tasked with evaluating 4,800 narrative statements on 24 different\ntopics of social, political, and public health relevance, for a total of\n192,000 assessments. We manipulate the disclosed source of each statement to\nassess how attribution to either another LLM or a human author of specified\nnationality affects evaluation outcomes. We find that, in the blind condition,\ndifferent LLMs display a remarkably high degree of inter- and intra-model\nagreement across topics. However, this alignment breaks down when source\nframing is introduced. Here we show that attributing statements to Chinese\nindividuals systematically lowers agreement scores across all models, and in\nparticular for Deepseek Reasoner. Our findings reveal that framing effects can\ndeeply affect text evaluation, with significant implications for the integrity,\nneutrality, and fairness of LLM-mediated information systems.", "AI": {"tldr": "The study examines the consistency and bias in LLM evaluations of narrative statements across multiple models, revealing significant framing effects based on attribution to authors of different nationalities.", "motivation": "To investigate the reliability and bias of Large Language Models in evaluating text, particularly how author attribution influences assessments.", "method": "The study evaluates 4,800 narrative statements across four LLMs, measuring inter- and intra-model agreement while manipulating the source attribution to analyze its impact.", "result": "LLMs showed high agreement in their evaluations but displayed decreased consistency when the sources were attributed to different nationalities, particularly affecting the Deepseek Reasoner.", "conclusion": "Framing effects significantly impact LLM evaluations, raising concerns regarding the neutrality and fairness of systems depending on LLM outputs.", "key_contributions": ["Systematic analysis of LLM evaluation consistency", "Identification of framing effects in text assessment", "Implications for fairness in LLM applications"], "limitations": "Limited to four LLMs and specific narrative statements; may not generalize to all types of texts or contexts.", "keywords": ["Large Language Models", "text evaluation", "framing effects", "bias", "neutrality"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.14363", "pdf": "https://arxiv.org/pdf/2505.14363.pdf", "abs": "https://arxiv.org/abs/2505.14363", "title": "Human and Machine as Seen at the Co-Creation Age: A Co-Word Analysis in Human Machine Co-creation (2014-2024)", "authors": ["Mengyao Guo", "Jinda Han", "Ze Gao", "Yuan Zhuang", "Xingting Wu"], "categories": ["cs.HC"], "comment": "24 pages", "summary": "This paper explores the evolving landscape of human-machine co-creation,\nfocusing on its development in the context of the ACM Conference on Human\nFactors in Computing Systems (CHI) from 2014 to 2024. We employ co-word\nanalysis to identify emerging trends, central themes, and the intellectual\ntrajectory of this field. The study highlights the shift from viewing machines\nas mere tools to recognizing them as collaborative partners in creative\nprocesses. By understanding these dynamics, we aim to provide insights into the\nimplications of this paradigm shift for creativity, innovation, and societal\nimpact, ultimately fostering a more inclusive and effective approach to\nhuman-machine interaction in various domains.", "AI": {"tldr": "This paper examines the transition from treating machines as tools to viewing them as collaborators in human-machine co-creation, analyzing trends and implications in the field from 2014 to 2024.", "motivation": "To understand the shift in the perception of machines from tools to collaborative partners and its implications for creativity and innovation.", "method": "Co-word analysis is employed to identify emerging trends and central themes in the context of human-machine co-creation.", "result": "The study reveals significant trends and intellectual developments in human-machine co-creation, emphasizing its evolving role in creativity and societal impact.", "conclusion": "Fostering a more inclusive and effective approach to human-machine interaction can enhance creativity and innovation across various domains.", "key_contributions": ["Identification of emerging trends in human-machine co-creation", "Insights into the collaborative role of machines in creative processes", "Implications for future research and societal impact"], "limitations": "", "keywords": ["human-machine co-creation", "collaborative partners", "creativity", "innovation", "ACM CHI"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.13491", "pdf": "https://arxiv.org/pdf/2505.13491.pdf", "abs": "https://arxiv.org/abs/2505.13491", "title": "ProdRev: A DNN framework for empowering customers using generative pre-trained transformers", "authors": ["Aakash Gupta", "Nataraj Das"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2022 International Conference on Decision Aid Sciences and\n  Applications (DASA)", "summary": "Following the pandemic, customers, preference for using e-commerce has\naccelerated. Since much information is available in multiple reviews (sometimes\nrunning in thousands) for a single product, it can create decision paralysis\nfor the buyer. This scenario disempowers the consumer, who cannot be expected\nto go over so many reviews since its time consuming and can confuse them.\nVarious commercial tools are available, that use a scoring mechanism to arrive\nat an adjusted score. It can alert the user to potential review manipulations.\nThis paper proposes a framework that fine-tunes a generative pre-trained\ntransformer to understand these reviews better. Furthermore, using\n\"common-sense\" to make better decisions. These models have more than 13 billion\nparameters. To fine-tune the model for our requirement, we use the curie engine\nfrom generative pre-trained transformer (GPT3). By using generative models, we\nare introducing abstractive summarization. Instead of using a simple extractive\nmethod of summarizing the reviews. This brings out the true relationship\nbetween the reviews and not simply copy-paste. This introduces an element of\n\"common sense\" for the user and helps them to quickly make the right decisions.\nThe user is provided the pros and cons of the processed reviews. Thus the\nuser/customer can take their own decisions.", "AI": {"tldr": "This paper proposes a framework using a fine-tuned generative transformer to summarize e-commerce product reviews, aiding consumers in decision-making by highlighting pros and cons.", "motivation": "To address the decision paralysis faced by consumers due to the overwhelming number of reviews available for e-commerce products.", "method": "The framework fine-tunes a generative pre-trained transformer (GPT-3, Curie engine) to better understand and summarize reviews via abstractive summarization, incorporating common sense reasoning.", "result": "The generative model effectively summarizes reviews by presenting the true relationships within them, offering users clear pros and cons.", "conclusion": "By using this approach, consumers can make more informed and quicker decisions regarding their purchases in an e-commerce setting.", "key_contributions": ["Development of an abstractive summarization method for e-commerce reviews", "Incorporation of common sense reasoning into review summaries", "Improvement in consumer decision-making efficiency through refined review understanding"], "limitations": "", "keywords": ["e-commerce", "review summarization", "GPT-3", "common sense reasoning", "decision-making"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.14370", "pdf": "https://arxiv.org/pdf/2505.14370.pdf", "abs": "https://arxiv.org/abs/2505.14370", "title": "What Does Success Look Like? Catalyzing Meeting Intentionality with AI-Assisted Prospective Reflection", "authors": ["Ava Elizabeth Scott", "Lev Tankelevitch", "Payod Panda", "Rishi Vanukuru", "Xinyue Chen", "Sean Rintel"], "categories": ["cs.HC"], "comment": null, "summary": "Despite decades of HCI and Meeting Science research, complaints about\nineffective meetings are still pervasive. We argue that meeting technologies\nlack support for prospective reflection, that is, thinking about why a meeting\nis needed and what might happen. To explore this, we designed a Meeting Purpose\nAssistant (MPA) technology probe to coach users to articulate their meeting's\npurpose and challenges, and act accordingly. The MPA used Generative AI to\nsupport personalized and actionable prospective reflection across the diversity\nof meeting contexts. Using a participatory prompting methodology, 18 employees\nof a global technology company reflected with the MPA on upcoming meetings.\nObserved impacts were: clarifying meeting purposes, challenges, and success\nconditions; changing perspectives and flexibility; improving preparation and\ncommunication; and proposing changed plans. We also identify perceived social,\ntemporal, and technological barriers to using the MPA. We present system and\nworkflow design considerations for developing AI-assisted reflection support\nfor meetings.", "AI": {"tldr": "A study on the design and impact of a Meeting Purpose Assistant (MPA) that uses Generative AI to help users reflect on the purpose and challenges of meetings.", "motivation": "Despite significant research in HCI and Meeting Science, ineffective meetings remain common due to a lack of technology that supports prospective reflection on meeting needs and outcomes.", "method": "The study used a participatory prompting methodology with 18 employees from a global tech company who interacted with the MPA to reflect on their upcoming meetings.", "result": "The MPA improved users' ability to clarify meeting purposes, adapt perspectives, enhance preparation, and suggest plan changes, while also revealing barriers to its use.", "conclusion": "The research emphasizes the need for AI-assisted support in meeting reflection and provides design considerations for similar technologies.", "key_contributions": ["Introduction of the Meeting Purpose Assistant (MPA) to facilitate meeting reflection.", "Demonstrated observable impacts on meeting preparation and communication.", "Identified barriers to the effective use of AI in meeting contexts."], "limitations": "Barriers related to social, temporal, and technological factors in utilizing the MPA were noted.", "keywords": ["HCI", "Meeting Science", "Generative AI", "prospective reflection", "collaboration"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.13492", "pdf": "https://arxiv.org/pdf/2505.13492.pdf", "abs": "https://arxiv.org/abs/2505.13492", "title": "LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis", "authors": ["Weiming Zhang", "Lingyue Fu", "Qingyao Li", "Kounianhua Du", "Jianghao Lin", "Jingwei Yu", "Wei Xia", "Weinan Zhang", "Ruiming Tang", "Yong Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Cognitive diagnosis (CD) plays a crucial role in intelligent education,\nevaluating students' comprehension of knowledge concepts based on their test\nhistories. However, current CD methods often model students, exercises, and\nknowledge concepts solely on their ID relationships, neglecting the abundant\nsemantic relationships present within educational data space. Furthermore,\ncontemporary intelligent tutoring systems (ITS) frequently involve the addition\nof new students and exercises, a situation that ID-based methods find\nchallenging to manage effectively. The advent of large language models (LLMs)\noffers the potential for overcoming this challenge with open-world knowledge.\nIn this paper, we propose LLM4CD, which Leverages Large Language Models for\nOpen-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the\nopen-world knowledge of LLMs to construct cognitively expressive textual\nrepresentations, which are then encoded to introduce rich semantic information\ninto the CD task. Additionally, we propose an innovative bi-level encoder\nframework that models students' test histories through two levels of encoders:\na macro-level cognitive text encoder and a micro-level knowledge state encoder.\nThis approach substitutes traditional ID embeddings with semantic\nrepresentations, enabling the model to accommodate new students and exercises\nwith open-world knowledge and address the cold-start problem. Extensive\nexperimental results demonstrate that our proposed method consistently\noutperforms previous CD models on multiple real-world datasets, validating the\neffectiveness of leveraging LLMs to introduce rich semantic information into\nthe CD task.", "AI": {"tldr": "The paper proposes LLM4CD, a method using large language models for cognitive diagnosis in education, enhancing traditional ID-based methods by incorporating semantic relationships.", "motivation": "Current cognitive diagnosis methods rely on student-ID relationships and struggle with new students and exercises; LLMs can provide open-world knowledge to improve this.", "method": "The proposed LLM4CD method constructs cognitive representations using LLMs and introduces a bi-level encoder framework to handle students' test histories more effectively, focusing on semantic rather than ID-based information.", "result": "LLM4CD outperforms existing cognitive diagnosis models across multiple real-world datasets, demonstrating improved handling of semantic relationships and alleviating cold-start issues.", "conclusion": "The incorporation of LLM semantics into cognitive diagnosis improves model performance and helps manage the variability in educational contexts, offering a scalable solution for intelligent tutoring systems.", "key_contributions": ["Introduction of LLM4CD leveraging LLMs for cognitive diagnosis", "Development of a bi-level encoder framework for enhanced student modeling", "Demonstrated superiority over traditional cognitive diagnosis models with extensive experimental validation"], "limitations": "", "keywords": ["Cognitive Diagnosis", "Large Language Models", "Intelligent Tutoring Systems", "Educational Data", "Open-World Knowledge"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.14377", "pdf": "https://arxiv.org/pdf/2505.14377.pdf", "abs": "https://arxiv.org/abs/2505.14377", "title": "When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making", "authors": ["Ulrike Kuhl", "Annika Bush"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted for XAI2025", "summary": "Although the integration of artificial intelligence (AI) into everyday tasks\nimproves efficiency and objectivity, it also risks transmitting bias to human\ndecision-making. In this study, we conducted a controlled experiment that\nsimulated hiring decisions to examine how biased AI recommendations - augmented\nwith or without counterfactual explanations - influence human judgment over\ntime. Participants, acting as hiring managers, completed 60 decision trials\ndivided into a baseline phase without AI, followed by a phase with biased (X)AI\nrecommendations (favoring either male or female candidates), and a final\npost-interaction phase without AI. Our results indicate that the participants\nfollowed the AI recommendations 70% of the time when the qualifications of the\ngiven candidates were comparable. Yet, only a fraction of participants detected\nthe gender bias (8 out of 294). Crucially, exposure to biased AI altered\nparticipants' inherent preferences: in the post-interaction phase,\nparticipants' independent decisions aligned with the bias when no\ncounterfactual explanations were provided before, but reversed the bias when\nexplanations were given. Reported trust did not differ significantly across\nconditions. Confidence varied throughout the study phases after exposure to\nmale-biased AI, indicating nuanced effects of AI bias on decision certainty.\nOur findings point to the importance of calibrating XAI to avoid unintended\nbehavioral shifts in order to safeguard equitable decision-making and prevent\nthe adoption of algorithmic bias.", "AI": {"tldr": "This study investigates the impact of biased AI recommendations on human hiring decisions, using controlled experiments to assess how counterfactual explanations can mitigate bias effects.", "motivation": "To understand how biased AI influences human decision-making in hiring processes and explore the role of counterfactual explanations in mitigating these biases.", "method": "Conducted a controlled experiment simulating hiring decisions with participants making choices influenced by biased AI recommendations. The study involved three phases: baseline without AI, with biased AI recommendations, and a final phase without AI.", "result": "Participants followed biased AI recommendations 70% of the time. Only 8 out of 294 detected the bias. Counterfactual explanations reversed bias in independent decisions, while trust remained unchanged, suggesting nuanced effects of AI bias on cognitive confidence.", "conclusion": "Counterfactual explanations are crucial in minimizing the adverse effects of biased AI on decision-making and promoting equitable judgment by mitigating algorithmic biases.", "key_contributions": ["Demonstrated significant influence of biased AI on human decision-making in hiring.", "Showed effectiveness of counterfactual explanations in reversing bias effects.", "Highlighted the need for careful calibration of XAI to prevent unwanted shifts in preferences."], "limitations": "The study was based on a simulated environment and may not fully represent real-world complexities in hiring contexts.", "keywords": ["artificial intelligence", "human decision-making", "bias", "counterfactual explanations", "explainable AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13498", "pdf": "https://arxiv.org/pdf/2505.13498.pdf", "abs": "https://arxiv.org/abs/2505.13498", "title": "IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation", "authors": ["Khanh-Tung Tran", "Barry O'Sullivan", "Hoang D. Nguyen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have demonstrated promising\nknowledge and reasoning abilities, yet their performance in multilingual and\nlow-resource settings remains underexplored. Existing benchmarks often exhibit\ncultural bias, restrict evaluation to text-only, rely on multiple-choice\nformats, and, more importantly, are limited for extremely low-resource\nlanguages. To address these gaps, we introduce IRLBench, presented in parallel\nEnglish and Irish, which is considered definitely endangered by UNESCO. Our\nbenchmark consists of 12 representative subjects developed from the 2024 Irish\nLeaving Certificate exams, enabling fine-grained analysis of model capabilities\nacross domains. By framing the task as long-form generation and leveraging the\nofficial marking scheme, it does not only support a comprehensive evaluation of\ncorrectness but also language fidelity. Our extensive experiments of leading\nclosed-source and open-source LLMs reveal a persistent performance gap between\nEnglish and Irish, in which models produce valid Irish responses less than 80\\%\nof the time, and answer correctly 55.8\\% of the time compared to 76.2\\% in\nEnglish for the best-performing model. We release IRLBench\n(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying\nevaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future\nresearch on robust, culturally aware multilingual AI development.", "AI": {"tldr": "IRLBench is a multilingual benchmark for evaluating Large Language Models (LLMs) in endangered languages, notably Irish, highlighting performance disparities between English and Irish.", "motivation": "To address the lack of evaluation benchmarks for LLMs in multilingual and low-resource settings, particularly for culturally significant languages like Irish.", "method": "The authors developed IRLBench, a benchmark featuring long-form generation tasks based on the 2024 Irish Leaving Certificate exams, enabling detailed analysis of LLM capabilities.", "result": "Experiments show a notable performance gap in LLM responses between English and Irish, with the best-performing model achieving only 55.8% correctness in Irish compared to 76.2% in English.", "conclusion": "The release of IRLBench, along with an evaluation codebase, aims to facilitate future research on creating more robust multilingual AI systems that acknowledge cultural contexts.", "key_contributions": ["Introduction of IRLBench for multilingual evaluation of LLMs", "Focus on endangered languages and cultural awareness", "Release of a comprehensive dataset and evaluation tools"], "limitations": "Performance results are limited to a specific low-resource language (Irish) and may not generalize to all multilingual contexts.", "keywords": ["Large Language Models", "multilingual evaluation", "low-resource languages", "cultural bias", "natural language generation"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.14379", "pdf": "https://arxiv.org/pdf/2505.14379.pdf", "abs": "https://arxiv.org/abs/2505.14379", "title": "Two Empirical Studies on Audiovisual Semiotics of Uncertainty", "authors": ["Sita Vriend", "David Hägele", "Daniel Weiskopf"], "categories": ["cs.HC"], "comment": "Accepted to Audio Mostly 2025 [AM '25]", "summary": "There exists limited theoretical guidance on integrating visualization and\nsonification. In this paper, we address this gap by investigating audiovisual\nsemiotics for uncertainty representation: joining uncertainty visualization and\nsonification to combine audiovisual channels for enhancing users' perception of\nuncertainty. We conducted two preregistered crowd-sourced user studies. First,\nwe assessed suitable audio/visual pairs. Then, we investigated audiovisual\nmappings of uncertainty. Here, we use probability as it is an easily\ncommunicated aspect of uncertainty. We analyzed the participants' preferences\nand reaction times in both user studies. Additionally, we explored the\nstrategies employed by participants through qualitative analysis. Our results\nreveal audiovisual mappings that lead to particularly strong preferences and\nlow reaction times. Furthermore, we found that preferred audio/visual pairs are\nnot necessarily suitable audiovisual mappings of uncertainty. For example,\nwhile pitch paired with brightness was preferred as a pair, it was not well\nsuited as a mapping for uncertainty. We recommend audiovisual mappings of\nuncertainty that lead to low reaction times and high preferences in both user\nstudies. This paper presents guidelines to anyone seeking to employ audiovisual\nrepresentations for uncertainty, contributing to enhancing the perception of\nuncertainty.", "AI": {"tldr": "This paper investigates the integration of visualization and sonification for better representation of uncertainty through audiovisual means.", "motivation": "The paper addresses the lack of theoretical guidance in combining visualization and sonification, focusing on enhancing users' understanding of uncertainty.", "method": "Two preregistered crowd-sourced user studies were conducted to assess audio/visual pair preferences and to explore audiovisual mappings of uncertainty based on probability.", "result": "The studies revealed strong preferences and low reaction times for specific audiovisual mappings, but highlighted that preferred pairs were not necessarily effective for uncertainty representation.", "conclusion": "The paper provides guidelines for the effective use of audiovisual representations to improve the perception of uncertainty, emphasizing the importance of low reaction times and high user preferences.", "key_contributions": ["Investigated audiovisual semiotics for uncertainty representation", "Conducted two user studies on audio/visual pairs and mappings of uncertainty", "Provided guidelines for effective audiovisual mappings of uncertainty"], "limitations": "", "keywords": ["audiovisual representation", "uncertainty visualization", "sonification"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.13500", "pdf": "https://arxiv.org/pdf/2505.13500.pdf", "abs": "https://arxiv.org/abs/2505.13500", "title": "Noise Injection Systemically Degrades Large Language Model Safety Guardrails", "authors": ["Prithviraj Singh Shahani", "Matthias Scheutz"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages,3 figures", "summary": "Safety guardrails in large language models (LLMs) are a critical component in\npreventing harmful outputs. Yet, their resilience under perturbation remains\npoorly understood. In this paper, we investigate the robustness of safety\nfine-tuning in LLMs by systematically injecting Gaussian noise into model\nactivations. We show across multiple open-weight models that (1) Gaussian noise\nraises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety\nfine-tuning affords no extra protection, and (3) that chain-of-thought\nreasoning remains largely intact. The findings reveal critical vulnerabilities\nin current safety alignment techniques and highlight the potential of\nreasoning-based and reinforcement learning approaches as promising direction\nfor developing more robust AI safety systems. These results have important\nimplications for real-world deployment of LLMs in safety-critical applications\nas these results imply that widely-deployed safety tuning methods can fail even\nwithout adversarial prompts.", "AI": {"tldr": "This paper investigates the robustness of safety fine-tuning in LLMs, revealing vulnerabilities in current safety methods and suggesting directions for improvement.", "motivation": "To understand the resilience of safety guardrails in LLMs against harmful outputs under perturbation.", "method": "The study systematically injects Gaussian noise into model activations of multiple open-weight models to assess harmful output rates and effects on safety fine-tuning.", "result": "Gaussian noise increases harmful-output rates by up to 27% with significant statistical confidence (p < 0.001). Deeper safety fine-tuning does not provide additional protection, yet chain-of-thought reasoning remains intact.", "conclusion": "Current safety alignment techniques have critical vulnerabilities, and reasoning-based and reinforcement learning approaches may lead to more robust AI safety systems.", "key_contributions": ["Demonstrated the impact of Gaussian noise on harmful output rates in LLMs", "Revealed that deeper safety fine-tuning does not enhance robustness", "Identified reasoning-based approaches as potential solutions for enhancing AI safety"], "limitations": "", "keywords": ["language models", "safety tuning", "AI robustness", "Gaussian noise", "safety alignment"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14452", "pdf": "https://arxiv.org/pdf/2505.14452.pdf", "abs": "https://arxiv.org/abs/2505.14452", "title": "How Managers Perceive AI-Assisted Conversational Training for Workplace Communication", "authors": ["Lance T Wilhelm", "Xiaohan Ding", "Kirk McInnis Knutsen", "Buse Carik", "Eugenia H Rho"], "categories": ["cs.HC", "cs.AI"], "comment": "accepted to CUI '25", "summary": "Effective workplace communication is essential for managerial success, yet\nmany managers lack access to tailored and sustained training. Although\nAI-assisted communication systems may offer scalable training solutions, little\nis known about how managers envision the role of AI in helping them improve\ntheir communication skills. To investigate this, we designed a conversational\nrole-play system, CommCoach, as a functional probe to understand how managers\nanticipate using AI to practice their communication skills. Through\nsemi-structured interviews, participants emphasized the value of adaptive,\nlow-risk simulations for practicing difficult workplace conversations. They\nalso highlighted opportunities, including human-AI teaming, transparent and\ncontext-aware feedback, and greater control over AI-generated personas.\nAI-assisted communication training should balance personalization, structured\nlearning objectives, and adaptability to different user styles and contexts.\nHowever, achieving this requires carefully navigating tensions between adaptive\nand consistent AI feedback, realism and potential bias, and the open-ended\nnature of AI conversations versus structured workplace discourse.", "AI": {"tldr": "This paper investigates how managers envision using AI-assisted communication systems like CommCoach for training their communication skills through role-play simulations.", "motivation": "To address the gap in tailored and sustained training for managers to improve workplace communication skills using AI.", "method": "Conducted semi-structured interviews with managers regarding their expectations and experiences with AI in communication training.", "result": "Participants valued adaptive simulations for practicing challenging conversations, highlighting needs for human-AI collaboration and context-aware feedback.", "conclusion": "Effective AI-assisted communication training requires balancing personalization, structured learning, and adaptability, while addressing issues of bias and realism in AI interactions.", "key_contributions": ["Developed CommCoach as a conversational role-play system for managers.", "Highlighted the importance of adaptive, low-risk training environments.", "Identified key needs for AI in workplace communication, such as transparency and adaptability."], "limitations": "The study may not cover all perspectives on AI in communication training and is based on limited interviews.", "keywords": ["AI-assisted training", "communication skills", "human-AI teaming"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.13506", "pdf": "https://arxiv.org/pdf/2505.13506.pdf", "abs": "https://arxiv.org/abs/2505.13506", "title": "EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation", "authors": ["Ruobing Yao", "Yifei Zhang", "Shuang Song", "Neng Gao", "Chenyang Tu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) compensates for the static knowledge\nlimitations of Large Language Models (LLMs) by integrating external knowledge,\nproducing responses with enhanced factual correctness and query-specific\ncontextualization. However, it also introduces new attack surfaces such as\ncorpus poisoning at the same time. Most of the existing defense methods rely on\nthe internal knowledge of the model, which conflicts with the design concept of\nRAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and\nbait-guided context diversity detection to identify malicious content by\nanalyzing the context diversity of candidate documents without relying on LLM\ninternal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art\nsecurity with plug-and-play deployment, simultaneously improving clean-scenario\nRAG performance while maintaining practical operational costs (relatively\n1.2$\\times$ latency, 48\\%-80\\% token reduction versus Vanilla RAG).", "AI": {"tldr": "EcoSafeRAG enhances Retrieval-Augmented Generation (RAG) by bolstering security against corpus poisoning while improving performance.", "motivation": "To address the limitations of Large Language Models (LLMs) in factual correctness and contextualization while ensuring security against new attack vectors introduced by Retrieval-Augmented Generation (RAG).", "method": "EcoSafeRAG employs sentence-level processing and bait-guided context diversity detection to identify malicious content without relying on the internal knowledge of LLMs.", "result": "EcoSafeRAG achieves state-of-the-art security and improves performance in clean scenarios, achieving a latency of 1.2 times and 48%-80% token reduction compared to Vanilla RAG.", "conclusion": "EcoSafeRAG presents an effective defense mechanism for RAG systems that enhances both security and operational efficiency while improving performance.", "key_contributions": ["Introduces a novel method for identifying malicious content in RAG systems without using internal model knowledge.", "Demonstrates significant performance gains in clean scenarios while reducing operational costs.", "Provides a plug-and-play deployment model for easy integration into existing systems."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "security", "Large Language Models", "context diversity", "corpus poisoning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.13508", "pdf": "https://arxiv.org/pdf/2505.13508.pdf", "abs": "https://arxiv.org/abs/2505.13508", "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs", "authors": ["Zijia Liu", "Peixuan Han", "Haofei Yu", "Haoru Li", "Jiaxuan You"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\n\\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a \\textit{reinforcement learning (RL)\ncurriculum} driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release \\textit{Time-Bench},\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of \\textit{Time-R1} checkpoints.", "AI": {"tldr": "Introducing Time-R1, a framework for improving temporal intelligence in LLMs, enabling understanding, prediction, and creative generation of events.", "motivation": "To enhance the temporal intelligence of LLMs, addressing their struggle to integrate past reasoning with future predictions and creative generation.", "method": "A three-stage reinforcement learning curriculum with a dynamic rule-based reward system to develop foundational understanding, prediction skills, and creative scenario generation capabilities.", "result": "Time-R1 significantly outperforms larger models on future event prediction and creative scenario benchmarks, demonstrating highly effective temporal performance with smaller models.", "conclusion": "The findings suggest that well-structured reinforcement learning fine-tuning can lead to superior temporal capabilities in smaller LLMs, paving the way for time-aware AI applications.", "key_contributions": ["Introduction of Time-R1 framework for temporal reasoning in LLMs.", "Development of Time-Bench, a multi-task temporal reasoning dataset.", "Showcasing superior performance of smaller models over larger state-of-the-art models."], "limitations": "", "keywords": ["Large Language Models", "Temporal Intelligence", "Reinforcement Learning", "Prediction", "Scenario Generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13514", "pdf": "https://arxiv.org/pdf/2505.13514.pdf", "abs": "https://arxiv.org/abs/2505.13514", "title": "Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models", "authors": ["Shuxun Wang", "Qingyu Yin", "Chak Tou Leong", "Qiang Zhang", "Linyi Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Repetition curse is a phenomenon where Large Language Models (LLMs) generate\nrepetitive sequences of tokens or cyclic sequences. While the repetition curse\nhas been widely observed, its underlying mechanisms remain poorly understood.\nIn this work, we investigate the role of induction heads--a specific type of\nattention head known for their ability to perform in-context learning--in\ndriving this repetitive behavior. Specifically, we focus on the \"toxicity\" of\ninduction heads, which we define as their tendency to dominate the model's\noutput logits during repetition, effectively excluding other attention heads\nfrom contributing to the generation process. Our findings have important\nimplications for the design and training of LLMs. By identifying induction\nheads as a key driver of the repetition curse, we provide a mechanistic\nexplanation for this phenomenon and suggest potential avenues for mitigation.\nWe also propose a technique with attention head regularization that could be\nemployed to reduce the dominance of induction heads during generation, thereby\npromoting more diverse and coherent outputs.", "AI": {"tldr": "This paper investigates the repetition curse in Large Language Models (LLMs), focusing on the role of induction heads in generating repetitive outputs and proposes techniques to mitigate this issue.", "motivation": "To understand the mechanisms behind the repetition curse observed in LLMs, particularly the role of induction heads in driving this behavior.", "method": "The study analyzes induction heads and their 'toxicity' defined as their dominance in output logits, leading to repetitive token generation, and proposes an attention head regularization technique to mitigate this effect.", "result": "The findings reveal that induction heads significantly contribute to the repetition curse by limiting the contributions of other attention heads, and the proposed regularization technique shows potential for improving output diversity.", "conclusion": "Identifying induction heads as a key factor in repetitive behavior aids in developing strategies to enhance model outputs, suggesting future directions for LLM training and design.", "key_contributions": ["Mechanistic insight into the repetition curse in LLMs", "Identification of induction heads as primary drivers of repetitive outputs", "Proposal of attention head regularization technique for improved output diversity"], "limitations": "", "keywords": ["Large Language Models", "repetition curse", "induction heads", "attention heads", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.13527", "pdf": "https://arxiv.org/pdf/2505.13527.pdf", "abs": "https://arxiv.org/abs/2505.13527", "title": "Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression", "authors": ["Jingyu Peng", "Maolin Wang", "Nan Wang", "Xiangyu Zhao", "Jiatong Li", "Kai Zhang", "Qi Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite substantial advancements in aligning large language models (LLMs)\nwith human values, current safety mechanisms remain susceptible to jailbreak\nattacks. We hypothesize that this vulnerability stems from distributional\ndiscrepancies between alignment-oriented prompts and malicious prompts. To\ninvestigate this, we introduce LogiBreak, a novel and universal black-box\njailbreak method that leverages logical expression translation to circumvent\nLLM safety systems. By converting harmful natural language prompts into formal\nlogical expressions, LogiBreak exploits the distributional gap between\nalignment data and logic-based inputs, preserving the underlying semantic\nintent and readability while evading safety constraints. We evaluate LogiBreak\non a multilingual jailbreak dataset spanning three languages, demonstrating its\neffectiveness across various evaluation settings and linguistic contexts.", "AI": {"tldr": "LogiBreak is a novel black-box jailbreak method for LLMs that exploits distributional discrepancies between alignment-oriented prompts and malicious prompts.", "motivation": "The paper addresses vulnerabilities in LLM safety mechanisms that are susceptible to jailbreak attacks by exploring the differences in prompt distributions.", "method": "LogiBreak translates harmful natural language prompts into formal logical expressions to bypass LLM safety systems.", "result": "LogiBreak is effective across a multilingual jailbreak dataset, demonstrating its robustness in various evaluation settings and contexts.", "conclusion": "The findings indicate that transforming prompts into logical expressions can effectively evade safety constraints in LLMs.", "key_contributions": ["Introduction of LogiBreak as a universal jailbreak method", "Demonstration of effectiveness across multilingual datasets", "Insight into the distributional discrepancies affecting LLM safety"], "limitations": "The method's reliance on logical expression translation may have specific contexts where it does not apply.", "keywords": ["large language models", "jailbreak attacks", "logical expressions", "AI safety", "multilingual evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13554", "pdf": "https://arxiv.org/pdf/2505.13554.pdf", "abs": "https://arxiv.org/abs/2505.13554", "title": "Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation", "authors": ["Zhanglin Wu", "Daimeng Wei", "Xiaoyu Chen", "Hengchao Shang", "Jiaxin Guo", "Zongyao Li", "Yuanchang Luo", "Jinlong Yang", "Zhiqiang Rao", "Hao Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 2 figures, 9 tables, ACL 2025", "summary": "Large language model (LLM) shows promising performances in a variety of\ndownstream tasks, such as machine translation (MT). However, using LLMs for\ntranslation suffers from high computational costs and significant latency.\nBased on our evaluation, in most cases, translations using LLMs are comparable\nto that generated by neural machine translation (NMT) systems. Only in\nparticular scenarios, LLM and NMT models show respective advantages. As a\nresult, integrating NMT and LLM for translation and using LLM only when\nnecessary seems to be a sound solution. A scheduling policy that optimizes\ntranslation result while ensuring fast speed and as little LLM usage as\npossible is thereby required. We compare several scheduling policies and\npropose a novel and straightforward decider that leverages source sentence\nfeatures. We conduct extensive experiments on multilingual test sets and the\nresult shows that we can achieve optimal translation performance with minimal\nLLM usage, demonstrating effectiveness of our decider.", "AI": {"tldr": "This paper explores the integration of large language models (LLM) and neural machine translation (NMT) to optimize translation performance while minimizing LLM usage and latency.", "motivation": "To address the high computational costs and latency when using LLMs for machine translation, while maintaining translation quality comparable to NMT systems.", "method": "The authors propose a scheduling policy that optimizes translation results by leveraging source sentence features, comparing various scheduling approaches through extensive experiments on multilingual test sets.", "result": "The proposed decider achieves optimal translation performance with minimal LLM usage, validating the effectiveness of the scheduling policy.", "conclusion": "Integrating NMT and LLM using an intelligent scheduling policy can enhance translation efficiency without compromising quality.", "key_contributions": ["Proposed a novel scheduling policy that integrates LLM and NMT for translation tasks.", "Demonstrated the effectiveness of the decider through extensive experiments on multilingual datasets.", "Provided a comparative analysis of different scheduling strategies."], "limitations": "The experiments focus on specific multilingual test sets, which may limit generalizability to all language pairs or contexts.", "keywords": ["large language models", "neural machine translation", "scheduling policy", "translation optimization", "multilingual datasets"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14080", "pdf": "https://arxiv.org/pdf/2505.14080.pdf", "abs": "https://arxiv.org/abs/2505.14080", "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory", "authors": ["Franziska Sofia Hafner", "Ana Valdivia", "Luc Rocher"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed.", "AI": {"tldr": "This paper investigates how language models encode and perpetuate harmful gender stereotypes, advocating for a broader understanding of gender bias beyond superficial associations.", "motivation": "To address and mitigate harmful gendered stereotypes perpetuated by language models, which affect diverse gender identities and can lead to real-world consequences in applications like health informatics.", "method": "The study operationalizes insights from gender studies, testing 16 language models of varying architectures and sizes to analyze how they encode gender.", "result": "The research finds that these models predominantly encode gender as a binary linked to biological sex, erasing and pathologizing non-binary identities, with larger models showing stronger biases.", "conclusion": "The authors call for a re-evaluation of the definitions and methods used to identify and mitigate gendered harms in language models.", "key_contributions": ["Proposes a broader definition of gender bias in language models", "Empirical analysis of 16 different language models regarding gender encoding", "Highlights the impact of model size on gender biases, emphasizing the need for re-evaluation"], "limitations": "The study may not include all existing language models or encapsulate the full diversity of gender identities beyond the binary framework.", "keywords": ["gender bias", "language models", "gender identity", "gender studies", "bias mitigation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.13559", "pdf": "https://arxiv.org/pdf/2505.13559.pdf", "abs": "https://arxiv.org/abs/2505.13559", "title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models", "authors": ["Sathya Krishnan Suresh", "Tanmay Surana", "Lim Zhi Hao", "Eng Siong Chng"], "categories": ["cs.CL", "cs.LG"], "comment": "17 pages, 5 figures and 11 tables", "summary": "Code-switching (CS) poses a significant challenge for Large Language Models\n(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce\nCS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue\nto English summarization. CS-Sum is the first benchmark for CS dialogue\nsummarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and\nMalay-English (EN-MS), with 900-1300 human-annotated dialogues per language\npair. Evaluating ten LLMs, including open and closed-source models, we analyze\nperformance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA\non synthetic data) approaches. Our findings show that though the scores on\nautomated metrics are high, LLMs make subtle mistakes that alter the complete\nmeaning of the dialogue. To this end, we introduce 3 most common type of errors\nthat LLMs make when handling CS input. Error rates vary across CS pairs and\nLLMs, with some LLMs showing more frequent errors on certain language pairs,\nunderscoring the need for specialized training on code-switched data.", "AI": {"tldr": "A study introducing CS-Sum, a benchmark for evaluating Large Language Models on code-switching dialogue summarization across multiple language pairs.", "motivation": "To address the challenges posed by code-switching in Large Language Models and evaluate their comprehensibility in summarizing code-switched dialogues.", "method": "CS-Sum benchmark consisting of 900-1300 human-annotated dialogues for each language pair (EN-ZH, EN-TA, EN-MS). Evaluated ten LLMs using few-shot, translate-summarize, and fine-tuning approaches on synthetic data.", "result": "High automated metric scores for LLMs, but identified subtle errors that impact the meaning of dialogues. Error rates vary by language pair and LLM, highlighting the necessity for specialized training on code-switching data.", "conclusion": "The findings emphasize the need for improved understanding and capabilities of LLMs when handling code-switched input, suggesting a path for future research on specialized training.", "key_contributions": ["Introduction of the CS-Sum benchmark for code-switching dialogue summarization.", "Evaluation of ten LLMs on comprehensibility of code-switching.", "Identification of common error types made by LLMs with code-switched data."], "limitations": "Focus on a limited set of language pairs and types of LLMs.", "keywords": ["Code-switching", "Large Language Models", "Summarization", "Natural Language Processing", "Benchmark"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.13628", "pdf": "https://arxiv.org/pdf/2505.13628.pdf", "abs": "https://arxiv.org/abs/2505.13628", "title": "Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning", "authors": ["Nathaniel Krasner", "Nicholas Lanuzo", "Antonios Anastasopoulos"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Multilingual alignment of sentence representations has mostly required\nbitexts to bridge the gap between languages. We investigate whether visual\ninformation can bridge this gap instead. Image caption datasets are very easy\nto create without requiring multilingual expertise, so this offers a more\nefficient alternative for low-resource languages. We find that multilingual\nimage-caption alignment can implicitly align the text representations between\nlanguages, languages unseen by the encoder in pretraining can be incorporated\ninto this alignment post-hoc, and these aligned representations are usable for\ncross-lingual Natural Language Understanding (NLU) and bitext retrieval.", "AI": {"tldr": "The paper explores the use of visual information in multilingual sentence representation alignment, utilizing image caption datasets as an efficient alternative to bitexts for low-resource languages.", "motivation": "To address the challenge of multilingual sentence representation alignment without relying on bitexts, particularly for low-resource languages.", "method": "The authors investigate multilingual image-caption alignment as a method to implicitly align text representations across different languages.", "result": "The study demonstrates that visual information can effectively align text representations, allowing for the incorporation of unseen languages into the alignment process and enabling cross-lingual NLU and bitext retrieval.", "conclusion": "Multilingual image-caption alignment is a promising approach for bridging linguistic gaps, particularly beneficial for low-resource languages.", "key_contributions": ["Demonstrated the effectiveness of using visual information for multilingual alignment.", "Showed that unseen languages can be incorporated into existing models post-hoc.", "Provided evidence for usability in cross-lingual NLU and retrieval tasks."], "limitations": "", "keywords": ["Multilingual Alignment", "Image-Caption Data", "Cross-Lingual NLU"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.13657", "pdf": "https://arxiv.org/pdf/2505.13657.pdf", "abs": "https://arxiv.org/abs/2505.13657", "title": "Clarifying orthography: Orthographic transparency as compressibility", "authors": ["Charles J. Torres", "Richard Futrell"], "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Orthographic transparency -- how directly spelling is related to sound --\nlacks a unified, script-agnostic metric. Using ideas from algorithmic\ninformation theory, we quantify orthographic transparency in terms of the\nmutual compressibility between orthographic and phonological strings. Our\nmeasure provides a principled way to combine two factors that decrease\northographic transparency, capturing both irregular spellings and rule\ncomplexity in one quantity. We estimate our transparency measure using\nprequential code-lengths derived from neural sequence models. Evaluating 22\nlanguages across a broad range of script types (alphabetic, abjad, abugida,\nsyllabic, logographic) confirms common intuitions about relative transparency\nof scripts. Mutual compressibility offers a simple, principled, and general\nyardstick for orthographic transparency.", "AI": {"tldr": "This paper proposes a new metric for orthographic transparency based on mutual compressibility between orthographic and phonological strings, applicable across diverse writing systems.", "motivation": "To provide a unified, script-agnostic metric for quantifying orthographic transparency, which lacks an existing framework.", "method": "The authors utilize ideas from algorithmic information theory to calculate mutual compressibility between spelling and sound representations, employing prequential code-lengths from neural sequence models.", "result": "The metric was applied to evaluate orthographic transparency in 22 languages, demonstrating its effectiveness in capturing irregular spellings and rule complexity.", "conclusion": "Mutual compressibility serves as a straightforward and general measurement for assessing orthographic transparency across various languages and scripts.", "key_contributions": ["Introduced a novel metric for orthographic transparency using mutual compressibility.", "Combined two factors affecting transparency—irregular spellings and rule complexity—into a single measure.", "Evaluated the transparency measure on a diverse set of languages and scripts."], "limitations": "", "keywords": ["orthographic transparency", "mutual compressibility", "algorithmic information theory", "neural sequence models", "language evaluation"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2505.13706", "pdf": "https://arxiv.org/pdf/2505.13706.pdf", "abs": "https://arxiv.org/abs/2505.13706", "title": "Are Large Language Models Good at Detecting Propaganda?", "authors": ["Julia Jose", "Rachel Greenstadt"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Propagandists use rhetorical devices that rely on logical fallacies and\nemotional appeals to advance their agendas. Recognizing these techniques is key\nto making informed decisions. Recent advances in Natural Language Processing\n(NLP) have enabled the development of systems capable of detecting manipulative\ncontent. In this study, we look at several Large Language Models and their\nperformance in detecting propaganda techniques in news articles. We compare the\nperformance of these LLMs with transformer-based models. We find that, while\nGPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude\n3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally,\nwe find that all three LLMs outperform a MultiGranularity Network (MGN)\nbaseline in detecting instances of one out of six propaganda techniques\n(name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in\ndetecting instances of appeal to fear and flag-waving.", "AI": {"tldr": "This study evaluates the effectiveness of various Large Language Models (LLMs) in detecting propaganda techniques in news articles, comparing their performance to transformer-based models.", "motivation": "Understanding how propaganda influences decision-making is crucial; recent NLP advancements can aid in identifying manipulative content.", "method": "We evaluated several LLMs, including GPT-4, GPT-3.5, and Claude 3 Opus, against a RoBERTa-CRF baseline in detecting six types of propaganda techniques in news articles.", "result": "GPT-4 had the highest F1 score among the LLMs (F1=0.16), but it was still below the RoBERTa-CRF baseline (F1=0.67). All LLMs exceeded a MultiGranularity Network (MGN) baseline in identifying name-calling, with GPT-3.5 and GPT-4 also exceeding it in appeal to fear and flag-waving.", "conclusion": "While LLMs show promise in detecting specific propaganda techniques, they currently do not match the performance of established transformer baselines.", "key_contributions": ["Comparison of LLMs and transformer models in propaganda detection", "Demonstration of specific techniques LLMs excel in", "Banchmarking against traditional NLP methods for propaganda analysis"], "limitations": "The study is limited by the low F1 scores and the focus on only six specific propaganda techniques.", "keywords": ["Natural Language Processing", "propaganda detection", "Large Language Models"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.13725", "pdf": "https://arxiv.org/pdf/2505.13725.pdf", "abs": "https://arxiv.org/abs/2505.13725", "title": "SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs", "authors": ["Yu Guo", "Dong Jin", "Shenghao Ye", "Shuangwu Chen", "Jian Yang", "Xiaobin Tan"], "categories": ["cs.CL"], "comment": "12 pages, 7 figures, accepted to ACL Findings 2025", "summary": "Large Language models (LLMs) have demonstrated significant potential in\ntext-to-SQL reasoning tasks, yet a substantial performance gap persists between\nexisting open-source models and their closed-source counterparts. In this\npaper, we introduce SQLForge, a novel approach for synthesizing reliable and\ndiverse data to enhance text-to-SQL reasoning in LLMs. We improve data\nreliability through SQL syntax constraints and SQL-to-question reverse\ntranslation, ensuring data logic at both structural and semantic levels. We\nalso propose an SQL template enrichment and iterative data domain exploration\nmechanism to boost data diversity. Building on the augmented data, we fine-tune\na variety of open-source models with different architectures and parameter\nsizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves\nthe state-of-the-art performance on the widely recognized Spider and BIRD\nbenchmarks among the open-source models. Specifically, SQLForge-LM achieves EX\naccuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing\nthe performance gap with closed-source methods.", "AI": {"tldr": "SQLForge enhances text-to-SQL reasoning in LLMs by synthesizing diverse and reliable data, resulting in improved performance.", "motivation": "To address the performance gap between open-source and closed-source models in text-to-SQL reasoning tasks.", "method": "The authors introduce SQLForge, which focuses on synthesizing data using SQL syntax constraints, reverse translation from SQL to questions, SQL template enrichment, and iterative data domain exploration.", "result": "SQLForge-LM, a family of fine-tuned models, achieves state-of-the-art performance on Spider and BIRD benchmarks, with EX accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev.", "conclusion": "By enhancing data reliability and diversity, SQLForge significantly improves the performance of open-source LLMs in text-to-SQL tasks.", "key_contributions": ["Introduction of SQLForge for enhancing text-to-SQL reasoning", "Utilization of SQL syntax constraints and reverse translation for data reliability", "State-of-the-art performance achievements for open-source models on established benchmarks."], "limitations": "", "keywords": ["Large Language Models", "text-to-SQL", "data synthesis", "machine learning", "natural language processing"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.14633", "pdf": "https://arxiv.org/pdf/2505.14633.pdf", "abs": "https://arxiv.org/abs/2505.14633", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "AI": {"tldr": "This paper introduces LitmusValues, an evaluation pipeline for assessing AI models' value prioritization to predict risky behaviors, using AIRiskDilemmas to measure conflicts among values relevant to AI safety.", "motivation": "As AI models become more powerful and employ strategies like Alignment Faking to evade detection of risky behaviors, it is essential to identify underlying values within these models to better predict and mitigate potential risks.", "method": "The authors propose LitmusValues, an evaluation pipeline designed to reveal AI models' priorities among various classes of values. They collect AIRiskDilemmas, a diverse set of scenarios showcasing conflicting values relevant to AI safety, to measure and predict risky behaviors based on the models' value prioritization.", "result": "The study demonstrates that the values assessed through LitmusValues, including ones that may appear harmless, can effectively predict both previously identified risky behaviors and novel risky behaviors found in HarmBench.", "conclusion": "By identifying and prioritizing values within AI models using the LitmusValues framework, researchers can better anticipate and address potential risks associated with advanced AI systems.", "key_contributions": ["Introduction of the LitmusValues evaluation pipeline for AI value assessment", "Collection of AIRiskDilemmas to test AI models against value conflicts", "Successful prediction of risky behaviors in AI based on value prioritization"], "limitations": "The framework may require further validation across a wider variety of AI models and potential scenarios, and may not perfectly capture all nuances of AI behaviors.", "keywords": ["AI Safety", "AI Risks", "Value Prioritization", "Human-Computer Interaction", "Machine Learning"], "importance_score": 6, "read_time_minutes": 30}}
{"id": "2505.13761", "pdf": "https://arxiv.org/pdf/2505.13761.pdf", "abs": "https://arxiv.org/abs/2505.13761", "title": "Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making", "authors": ["Jacob Kleiman", "Kevin Frank", "Sindy Campagna"], "categories": ["cs.CL"], "comment": null, "summary": "Simulations, although powerful in accurately replicating real-world systems,\noften remain inaccessible to non-technical users due to their complexity.\nConversely, large language models (LLMs) provide intuitive, language-based\ninteractions but can lack the structured, causal understanding required to\nreliably model complex real-world dynamics. We introduce our simulation agent\nframework, a novel approach that integrates the strengths of both simulation\nmodels and LLMs. This framework helps empower users by leveraging the\nconversational capabilities of LLMs to interact seamlessly with sophisticated\nsimulation systems, while simultaneously utilizing the simulations to ground\nthe LLMs in accurate and structured representations of real-world phenomena.\nThis integrated approach helps provide a robust and generalizable foundation\nfor empirical validation and offers broad applicability across diverse domains.", "AI": {"tldr": "This paper presents a simulation agent framework that combines the strengths of simulations and large language models (LLMs) to enhance user interaction and understanding of complex systems.", "motivation": "To improve accessibility of simulations for non-technical users using LLMs' conversational capabilities.", "method": "The authors developed a framework that integrates LLMs with simulation models, enabling intuitive interactions while ensuring accurate representations of real-world dynamics.", "result": "The framework empowers users to interact with simulations effectively, leading to a better understanding of complex systems.", "conclusion": "By combining simulations with LLMs, the framework provides a robust platform for empirical validation and can be applied across various domains.", "key_contributions": ["Integration of LLMs with simulation models", "Enhanced user accessibility to complex simulations", "Robust framework for empirical validation across domains"], "limitations": "", "keywords": ["simulation", "large language models", "human-computer interaction", "empirical validation", "user accessibility"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13772", "pdf": "https://arxiv.org/pdf/2505.13772.pdf", "abs": "https://arxiv.org/abs/2505.13772", "title": "Krikri: Advancing Open Large Language Models for Greek", "authors": ["Dimitris Roussis", "Leon Voukoutis", "Georgios Paraskevopoulos", "Sokratis Sofianopoulos", "Prokopis Prokopidis", "Vassilis Papavasileiou", "Athanasios Katsamanis", "Stelios Piperidis", "Vassilis Katsouros"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored\nfor the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been\nextensively trained on high-quality Greek data to ensure superior adaptation to\nlinguistic nuances. With 8 billion parameters, it offers advanced capabilities\nwhile maintaining efficient computational performance. Llama-Krikri-8B supports\nboth Modern Greek and English, and is also equipped to handle polytonic text\nand Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage\npost-training pipeline, utilizing both human and synthetic instruction and\npreference data, by applying techniques such as MAGPIE. In addition, for\nevaluation, we propose three novel public benchmarks for Greek. Our evaluation\non existing as well as the proposed benchmarks shows notable improvements over\ncomparable Greek and multilingual LLMs in both natural language understanding\nand generation as well as code generation.", "AI": {"tldr": "Introduction of Llama-Krikri-8B, a Large Language Model optimized for Greek, demonstrating significant improvements in NLP tasks.", "motivation": "To create a high-performance language model specifically for the Greek language that adapts to its unique linguistic features.", "method": "The model is built on Meta's Llama 3.1-8B, trained on extensive high-quality Greek data, and includes a multi-stage post-training pipeline utilizing both human and synthetic instruction data, employing techniques like MAGPIE.", "result": "Llama-Krikri-8B exhibits notable advancements in natural language understanding and generation, outperforming existing Greek and multilingual LLMs.", "conclusion": "The model not only serves current NLP needs for Greek but also opens new avenues for language model capabilities in resource-scarce languages.", "key_contributions": ["Development of Llama-Krikri-8B as a Greek language-specific LLM", "Introduction of public benchmarks for evaluating Greek LLMs", "Improved performance in both natural language and code generation tasks for Greek."], "limitations": "", "keywords": ["Llama-Krikri-8B", "Large Language Model", "Greek language", "natural language processing", "machine learning"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.13792", "pdf": "https://arxiv.org/pdf/2505.13792.pdf", "abs": "https://arxiv.org/abs/2505.13792", "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation", "authors": ["Siddhant Bhambri", "Upasana Biswas", "Subbarao Kambhampati"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Question Answering (QA) poses a challenging and critical problem,\nparticularly in today's age of interactive dialogue systems such as ChatGPT,\nPerplexity, Microsoft Copilot, etc. where users demand both accuracy and\ntransparency in the model's outputs. Since smaller language models (SLMs) are\ncomputationally more efficient but often under-perform compared to larger\nmodels, Knowledge Distillation (KD) methods allow for finetuning these smaller\nmodels to improve their final performance. Lately, the intermediate tokens or\nthe so called `reasoning' traces produced by Chain-of-Thought (CoT) or by\nreasoning models such as DeepSeek R1 are used as a training signal for KD.\nHowever, these reasoning traces are often verbose and difficult to interpret or\nevaluate. In this work, we aim to address the challenge of evaluating the\nfaithfulness of these reasoning traces and their correlation with the final\nperformance. To this end, we employ a KD method leveraging rule-based problem\ndecomposition. This approach allows us to break down complex queries into\nstructured sub-problems, generating interpretable traces whose correctness can\nbe readily evaluated, even at inference time. Specifically, we demonstrate this\napproach on Open Book QA, decomposing the problem into a Classification step\nand an Information Retrieval step, thereby simplifying trace evaluation. Our\nSFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft\nMachine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the\nstriking finding that correct traces do not necessarily imply that the model\noutputs the correct final solution. Similarly, we find a low correlation\nbetween correct final solutions and intermediate trace correctness. These\nresults challenge the implicit assumption behind utilizing reasoning traces for\nimproving SLMs' final performance via KD.", "AI": {"tldr": "This paper explores the evaluation of reasoning traces in Knowledge Distillation for Smaller Language Models, revealing that correctness of traces does not guarantee accurate final outputs.", "motivation": "The study focuses on improving the performance of smaller language models in QA tasks by addressing the evaluation of reasoning traces used in Knowledge Distillation.", "method": "The authors employ a Knowledge Distillation method that uses rule-based problem decomposition to create structured sub-problems for easier evaluation of reasoning traces.", "result": "Experiments demonstrate that while correct reasoning traces are essential, they do not consistently correlate with correct final model outputs, challenging existing assumptions in using these traces.", "conclusion": "The findings suggest that relying solely on reasoning traces for enhancing smaller models' performance might be misleading, urging a reconsideration of current Knowledge Distillation practices.", "key_contributions": ["Introduces a method for structured problem decomposition in QA tasks", "Evaluates the faithfulness of reasoning traces in KD", "Uncovers low correlation between reasoning trace correctness and final model outputs"], "limitations": "The scope is limited to specific datasets and may not generalize to all QA problems or languages.", "keywords": ["Knowledge Distillation", "Question Answering", "Chain-of-Thought", "Reasoning Traces", "Smaller Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.13840", "pdf": "https://arxiv.org/pdf/2505.13840.pdf", "abs": "https://arxiv.org/abs/2505.13840", "title": "EfficientLLM: Efficiency in Large Language Models", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "AI": {"tldr": "EfficientLLM is a benchmark study evaluating efficiency techniques for Large Language Models, examining architecture, fine-tuning, and inference methods to enhance performance while managing compute and energy costs.", "motivation": "To address the prohibitive costs associated with the growing parameter counts and context windows of Large Language Models (LLMs).", "method": "A comprehensive empirical study on a production-class cluster, systematically exploring architecture pretraining, fine-tuning, and inference techniques while defining metrics for evaluation.", "result": "Key insights include quantifiable trade-offs, task- and scale-dependent optima, and generalization across modalities, impacting performance and efficiency assessments.", "conclusion": "EfficientLLM opens new avenues for researchers and engineers by providing datasets, evaluation pipelines, and leaderboards, promoting a better understanding of LLM efficiency-performance complexities.", "key_contributions": ["Introduction of EfficientLLM benchmark for LLM efficiency evaluation", "In-depth comparative analysis of efficiency techniques", "Open-sourcing resources for the research community"], "limitations": "", "keywords": ["Large Language Models", "Efficiency", "Benchmarking", "Fine-tuning", "Inference"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2311.00721", "pdf": "https://arxiv.org/pdf/2311.00721.pdf", "abs": "https://arxiv.org/abs/2311.00721", "title": "Empathy Detection from Text, Audiovisual, Audio or Physiological Signals: A Systematic Review of Task Formulations and Machine Learning Methods", "authors": ["Md Rakibul Hasan", "Md Zakir Hossain", "Shreya Ghosh", "Aneesh Krishna", "Tom Gedeon"], "categories": ["cs.HC", "cs.LG", "cs.SI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Empathy indicates an individual's ability to understand others. Over the past\nfew years, empathy has drawn attention from various disciplines, including but\nnot limited to Affective Computing, Cognitive Science, and Psychology.\nDetecting empathy has potential applications in society, healthcare and\neducation. Despite being a broad and overlapping topic, the avenue of empathy\ndetection leveraging Machine Learning remains underexplored from a systematic\nliterature review perspective. We collected 849 papers from 10 well-known\nacademic databases, systematically screened them and analysed the final 82\npapers. Our analyses reveal several prominent task formulations - including\nempathy on localised utterances or overall expressions, unidirectional or\nparallel empathy, and emotional contagion - in monadic, dyadic and group\ninteractions. Empathy detection methods are summarised based on four input\nmodalities - text, audiovisual, audio and physiological signals - thereby\npresenting modality-specific network architecture design protocols. We discuss\nchallenges, research gaps and potential applications in the Affective\nComputing-based empathy domain, which can facilitate new avenues of\nexploration. We further enlist the public availability of datasets and codes.\nThis paper, therefore, provides a structured overview of recent advancements\nand remaining challenges towards developing a robust empathy detection system\nthat could meaningfully contribute to enhancing human well-being.", "AI": {"tldr": "A systematic literature review on empathy detection using machine learning, highlighting task formulations, input modalities, and challenges in the field.", "motivation": "To explore the underdeveloped area of empathy detection using machine learning and its potential applications across various domains.", "method": "Collected and analyzed 849 papers across academic databases, culminating in a review of 82 papers focusing on different task formulations and input modalities for empathy detection.", "result": "Identified prominent task formulations such as localised empathy and emotional contagion; summarized detection methods based on text, audiovisual, audio, and physiological inputs with proposed network architectures.", "conclusion": "The paper outlines challenges, research gaps, and provides resources for empathy detection systems, aiming to enhance human well-being through improved understanding of empathy.", "key_contributions": ["Systematic review of empathy detection literature using Machine Learning.", "Classification of empathy tasks and modalities.", "Proposed architecture design protocols for empathy detection systems."], "limitations": "The paper may not cover all emerging methodologies or the latest advancements due to the time of the literature collection.", "keywords": ["Empathy Detection", "Machine Learning", "Affective Computing", "Healthcare", "Literature Review"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.13844", "pdf": "https://arxiv.org/pdf/2505.13844.pdf", "abs": "https://arxiv.org/abs/2505.13844", "title": "Improve Language Model and Brain Alignment via Associative Memory", "authors": ["Congchi Yin", "Yongpeng Zhang", "Xuyun Wen", "Piji Li"], "categories": ["cs.CL"], "comment": "Accepted by Findings of ACL 2025", "summary": "Associative memory engages in the integration of relevant information for\ncomprehension in the human cognition system. In this work, we seek to improve\nalignment between language models and human brain while processing speech\ninformation by integrating associative memory. After verifying the alignment\nbetween language model and brain by mapping language model activations to brain\nactivity, the original text stimuli expanded with simulated associative memory\nare regarded as input to computational language models. We find the alignment\nbetween language model and brain is improved in brain regions closely related\nto associative memory processing. We also demonstrate large language models\nafter specific supervised fine-tuning better align with brain response, by\nbuilding the \\textit{Association} dataset containing 1000 samples of stories,\nwith instructions encouraging associative memory as input and associated\ncontent as output.", "AI": {"tldr": "This paper explores the enhancement of alignment between language models and human brain functions in speech processing by incorporating associative memory, leading to improved performance in comprehension tasks.", "motivation": "To improve the alignment between computational language models and human cognition, particularly in processing speech through the integration of associative memory.", "method": "We mapped language model activations to brain activities to verify alignment and tested expanded text stimuli with simulated associative memory as inputs to language models, alongside the development of the Association dataset containing 1000 story samples.", "result": "The study shows that language models aligned better with brain regions associated with associative memory after fine-tuning on the Association dataset, which includes stimuli designed to engage associative memory.", "conclusion": "Incorporating associative memory into language models significantly improves their alignment with human brain responses, suggesting a pathway for enhancing natural language understanding in AI systems.", "key_contributions": ["Demonstrated the use of associative memory for improved language model and brain alignment.", "Created the Association dataset with specialized story samples to encourage associative memory engagement in training.", "Showed that supervised fine-tuning enhances language model performance in understanding speech-related tasks."], "limitations": "", "keywords": ["associative memory", "language models", "brain alignment", "human cognition", "supervised fine-tuning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.01240", "pdf": "https://arxiv.org/pdf/2409.01240.pdf", "abs": "https://arxiv.org/abs/2409.01240", "title": "DiffEyeSyn: Diffusion-based User-specific Eye Movement Synthesis", "authors": ["Chuhan Jiao", "Guanhua Zhang", "Yeonjoo Cho", "Zhiming Hu", "Andreas Bulling"], "categories": ["cs.HC"], "comment": "Corrected bugs in creating visualisations", "summary": "High-frequency gaze data contains more user-specific information than\nlow-frequency data, promising for various applications. However, existing gaze\nmodelling methods focus on low-frequency data, ignoring user-specific subtle\neye movements in high-frequency eye movements. We present DiffEyeSyn -- the\nfirst computational method to synthesise eye movements specific to individual\nusers. The key idea is to consider the user-specific information as a special\ntype of noise in eye movement data. This perspective reshapes eye movement\nsynthesis into the task of injecting this user-specific noise into any given\neye movement sequence. We formulate this injection task as a conditional\ndiffusion process in which the synthesis is conditioned on user-specific\nembeddings extracted from the gaze data using pre-trained models for user\nauthentication. We propose user identity guidance -- a novel loss function that\nallows our model to preserve user identity while generating human-like eye\nmovements in the spatial domain. Experiments on two public datasets show that\nour synthetic eye movements preserve user-specific characteristics and are more\nrealistic than baseline approaches. Furthermore, we demonstrate that DiffEyeSyn\ncan synthesise large-scale gaze data and support various downstream tasks, such\nas gaze-based user identification. As such, our work lays the methodological\nfoundations for personalised eye movement synthesis that has significant\napplication potential, such as for character animation, eye movement\nbiometrics, and gaze data imputation.", "AI": {"tldr": "DiffEyeSyn is a new computational method that synthesizes user-specific eye movements from high-frequency gaze data, using a conditional diffusion process to inject unique user characteristics into eye movement sequences.", "motivation": "Existing gaze modeling methods primarily focus on low-frequency data, overlooking the valuable user-specific information inherent in high-frequency eye movements.", "method": "DiffEyeSyn synthesizes eye movements by treating user-specific information as a type of noise, implementing a conditional diffusion process where synthesis depends on user-specific embeddings for user authentication.", "result": "Experiments demonstrate that DiffEyeSyn generates synthetic eye movements that maintain user-specific characteristics and are more realistic than existing methods, and can support large-scale gaze data synthesis for tasks like user identification.", "conclusion": "The proposed method establishes a foundation for personalized eye movement synthesis, with applications in character animation, biometrics, and gaze data imputation.", "key_contributions": ["First method to synthesize high-frequency gaze data specific to individual users", "Introduces user identity guidance as a novel loss function", "Supports various downstream tasks including gaze-based user identification"], "limitations": "", "keywords": ["eye movements", "gaze data synthesis", "user-specific characteristics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13855", "pdf": "https://arxiv.org/pdf/2505.13855.pdf", "abs": "https://arxiv.org/abs/2505.13855", "title": "Domain Gating Ensemble Networks for AI-Generated Text Detection", "authors": ["Arihant Tripathi", "Liam Dugan", "Charis Gao", "Maggie Huan", "Emma Jin", "Peter Zhang", "David Zhang", "Julia Zhao", "Chris Callison-Burch"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to EMNLP 2025", "summary": "As state-of-the-art language models continue to improve, the need for robust\ndetection of machine-generated text becomes increasingly critical. However,\ncurrent state-of-the-art machine text detectors struggle to adapt to new unseen\ndomains and generative models. In this paper we present DoGEN (Domain Gating\nEnsemble Networks), a technique that allows detectors to adapt to unseen\ndomains by ensembling a set of domain expert detector models using weights from\na domain classifier. We test DoGEN on a wide variety of domains from leading\nbenchmarks and find that it achieves state-of-the-art performance on in-domain\ndetection while outperforming models twice its size on out-of-domain detection.\nWe release our code and trained models to assist in future research in\ndomain-adaptive AI detection.", "AI": {"tldr": "DoGEN is a novel technique for adapting text detectors to unseen domains through an ensemble of domain expert models.", "motivation": "To address the challenge of detecting machine-generated text across diverse domains, especially with the rise of advanced language models.", "method": "DoGEN leverages a domain classifier to weight an ensemble of domain expert detector models, enhancing adaptability to unseen domains.", "result": "Achieves state-of-the-art performance in detecting machine-generated text in both in-domain and out-of-domain scenarios, outperforming larger models in out-of-domain detection.", "conclusion": "DoGEN provides a promising approach for domain-adaptive AI detection, with code and models released for future research.", "key_contributions": ["Introduction of DoGEN for domain-adaptive text detection", "Demonstrated superior performance in unseen domains", "Releasing code and models for community research"], "limitations": "", "keywords": ["Domain Adaptation", "Text Detection", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.13036", "pdf": "https://arxiv.org/pdf/2410.13036.pdf", "abs": "https://arxiv.org/abs/2410.13036", "title": "Uncovering the Internet's Hidden Values: An Empirical Study of Desirable Behavior Using Highly-Upvoted Content on Reddit", "authors": ["Agam Goyal", "Charlotte Lambert", "Yoshee Jain", "Eshwar Chandrasekharan"], "categories": ["cs.HC", "cs.SI"], "comment": "Preprint: 15 pages, 3 figures, 2 tables", "summary": "A major task for moderators of online spaces is norm-setting, essentially\ncreating shared norms for user behavior in their communities. Platform design\nprinciples emphasize the importance of highlighting norm-adhering examples and\nexplicitly stating community norms. However, norms and values vary between\ncommunities and go beyond content-level attributes, making it challenging for\nplatforms and researchers to provide automated ways to identify desirable\nbehavior to be highlighted. Current automated approaches to detect desirability\nare limited to measures of prosocial behavior, but we do not know whether these\nmeasures fully capture the spectrum of what communities value. In this paper,\nwe use upvotes, which express community approval, as a proxy for desirability\nand examine 16,000 highly-upvoted comments across 80 popular sub-communities on\nReddit. Using a large language model, we extract values from these comments\nacross two years (2016 and 2022) and compile 64 and 72 $\\textit{macro}$,\n$\\textit{meso}$, and $\\textit{micro}$ values for 2016 and 2022 respectively,\nbased on their frequency across communities. Furthermore, we find that existing\ncomputational models for measuring prosociality were inadequate to capture on\naverage $82\\%$ of the values we extracted. Finally, we show that our approach\ncan not only extract most of the qualitatively-identified values from prior\ntaxonomies, but also uncover new values that are actually encouraged in\npractice. Our findings highlight the need for nuanced models of desirability\nthat go beyond preexisting prosocial measures. This work has implications for\nimproving moderator understanding of their community values and provides a\nframework that can supplement qualitative approaches with larger-scale content\nanalyses.", "AI": {"tldr": "This paper investigates the identification of community norms in online spaces by analyzing highly upvoted Reddit comments, revealing inadequacies in current prosocial behavior measures and proposing a novel framework for understanding community values.", "motivation": "The paper addresses the challenge of norm-setting in online communities, acknowledging that community values vary greatly and are not fully captured by current automated approaches.", "method": "The authors analyzed 16,000 highly upvoted comments across 80 popular sub-communities on Reddit using a large language model to extract values from comments from the years 2016 and 2022, identifying distinct macro, meso, and micro values.", "result": "The research found that existing computational models for measuring prosocial behavior were inadequate, failing to capture 82% of the values extracted from the comments. The study successfully identified both known and new community values through automated analysis.", "conclusion": "There is a critical need for refined models of desirability in online communities that extend beyond traditional prosocial measures, aiding moderators in understanding and applying community values.", "key_contributions": ["Identified 64 and 72 distinct community values for 2016 and 2022 respectively from Reddit comments.", "Demonstrated the limitations of existing prosocial behavior measures in capturing community values.", "Proposed a framework for integrating qualitative insights with large-scale content analyses for better understanding of community norms."], "limitations": "The focus was limited to Reddit, which may not generalize across all online communities or platforms.", "keywords": ["community norms", "upvotes", "prosocial behavior", "values extraction", "Reddit"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13866", "pdf": "https://arxiv.org/pdf/2505.13866.pdf", "abs": "https://arxiv.org/abs/2505.13866", "title": "Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning", "authors": ["Jiwon Song", "Dongwon Jo", "Yulhwa Kim", "Jae-Joon Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.", "AI": {"tldr": "Introducing Reasoning Path Compression (RPC) to enhance the efficiency of reasoning-focused language models by reducing memory usage and increasing generation throughput.", "motivation": "To address the high memory usage and throughput limitations of lengthy reasoning paths in reasoning-based language models.", "method": "RPC compresses the KV cache by retaining high importance scores based on a selector window of recent queries, thereby reducing the resource burden during inference.", "result": "RPC achieves up to 1.60× improvement in generation throughput with only a 1.2% accuracy drop on the AIME 2024 benchmark.", "conclusion": "Leveraging semantic sparsity in reasoning paths presents a viable strategy for the efficient deployment of reasoning LLMs.", "key_contributions": ["Proposed Reasoning Path Compression (RPC) method", "Demonstrated significant improvement in generation throughput", "Provided a practical approach for efficient reasoning model deployment"], "limitations": "Accuracy drop of 1.2% on the benchmark during compression.", "keywords": ["Reasoning Path Compression", "Language Models", "Inference Efficiency"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2412.00411", "pdf": "https://arxiv.org/pdf/2412.00411.pdf", "abs": "https://arxiv.org/abs/2412.00411", "title": "Seismocardiography for Emotion Recognition: A Study on EmoWear with Insights from DEAP", "authors": ["Mohammad Hasan Rahmani", "Rafael Berkvens", "Maarten Weyn"], "categories": ["cs.HC"], "comment": "17 pages, 9 figures", "summary": "Emotions have a profound impact on our daily lives, influencing our thoughts,\nbehaviors, and interactions, but also our physiological reactions. Recent\nadvances in wearable technology have facilitated studying emotions through\ncardio-respiratory signals. Accelerometers offer a non-invasive, convenient,\nand cost-effective method for capturing heart- and pulmonary-induced vibrations\non the chest wall, specifically Seismocardiography (SCG) and\nAccelerometry-Derived Respiration (ADR). Their affordability, wide\navailability, and ability to provide rich contextual data make accelerometers\nideal for everyday use. While accelerometers have been used as part of broader\nmodality fusions for Emotion Recognition (ER), their stand-alone potential via\nSCG and ADR remains unexplored. Bridging this gap could significantly help the\nembedding of ER into real-world applications, minimizing the hardware, and\nincreasing contextual integration potentials. To address this gap, we introduce\nSCG and ADR as novel modalities for ER and evaluate their performance using the\nEmoWear dataset. First, we replicate the single-trial emotion classification\npipeline from the DEAP dataset study, achieving similar results. Then we use\nour validated pipeline to train models that predict affective valence-arousal\nstates using SCG and compare them against established cardiac signals,\nElectrocardiography (ECG) and Blood Volume Pulse (BVP). Results show that SCG\nis a viable modality for ER, achieving similar performance to ECG and BVP. By\ncombining ADR with SCG, we achieved a working ER framework that only requires a\nsingle chest-worn accelerometer. These findings pave the way for integrating ER\ninto real-world, enabling seamless affective computing in everyday life.", "AI": {"tldr": "This paper evaluates the use of accelerometers for Emotion Recognition (ER) through Seismocardiography (SCG) and Accelerometry-Derived Respiration (ADR), demonstrating their potential for real-world applications.", "motivation": "The paper addresses the underexplored potential of using SCG and ADR in Emotion Recognition, aiming to simplify hardware requirements and enhance contextual integration in real-time applications.", "method": "The authors replicated the emotion classification pipeline from the DEAP dataset and trained models on the EmoWear dataset to compare SCG and ADR against established signals like ECG and BVP.", "result": "The study found that SCG can perform comparably to ECG and BVP for ER tasks. By integrating ADR with SCG, they established an effective ER framework using only a chest-worn accelerometer.", "conclusion": "The findings support the feasibility of integrating SCG and ADR into real-world emotion recognition systems, highlighting their practicality and accessibility for seamless affective computing.", "key_contributions": ["Introduction of SCG and ADR as novel modalities for Emotion Recognition.", "Demonstration of SCG's comparable performance to ECG and BVP.", "Development of a simple, effective Emotion Recognition framework using common wearable technology."], "limitations": "The study primarily focuses on the EmoWear dataset, which may limit generalizability to other contexts or populations.", "keywords": ["Emotion Recognition", "Wearable Technology", "Seismocardiography", "Accelerometry-Derived Respiration", "Affective Computing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.13886", "pdf": "https://arxiv.org/pdf/2505.13886.pdf", "abs": "https://arxiv.org/abs/2505.13886", "title": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning", "authors": ["Jingqi Tong", "Jixin Tang", "Hangcheng Li", "Yurong Mou", "Ming Zhang", "Jun Zhao", "Yanbo Wen", "Fan Song", "Jiahao Zhan", "Yuyang Lu", "Chaoran Tao", "Zhiyuan Guo", "Jizhou Yu", "Tianhao Cheng", "Changhao Jiang", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Weifeng Ge", "Guanhua Chen", "Tao Gui", "Xipeng Qiu", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "I.2.7; I.2.10"], "comment": "49 pages, 19 figures, submitted to NeurIPS 2025", "summary": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable to produce, challenging for state-of-the-art\nmodels, and diverse with 30 games and 158 tasks. Surprisingly, despite training\nsolely on game data, VLMs demonstrated out of domain generalization,\nspecifically Qwen2.5-VL-7B improving performance by 2.33\\% across 7 diverse\nvision-language benchmarks. Our code and dataset are available at\nhttps://github.com/tongjingqi/Code2Logic.", "AI": {"tldr": "The paper introduces Code2Logic, a method for synthesizing multimodal reasoning data from game code to improve Vision Language Models (VLMs) performance and proposes the GameQA dataset.", "motivation": "Visual-language CoT data is limited, hindering VLM reasoning capabilities; game code offers a more efficient way to generate this data.", "method": "Code2Logic leverages Large Language Models to adapt game code for automatic reasoning process acquisition and results via code execution.", "result": "Using Code2Logic to create the GameQA dataset resulted in VLMs demonstrating out of domain generalization, with Qwen2.5-VL-7B improving by 2.33% across 7 vision-language benchmarks.", "conclusion": "GameQA is a scalable, cost-effective dataset source that enhances VLMs, challenging them and improving their performance on diverse tasks.", "key_contributions": ["Introduction of a game-code-driven approach for multimodal reasoning data synthesis.", "Development of the GameQA dataset for training and evaluating VLMs.", "Demonstration of out of domain generalization in VLMs trained on game data."], "limitations": "The evaluation is primarily based on game data; real-world applicability is not fully established.", "keywords": ["Vision Language Models", "multimodal reasoning", "game code", "Large Language Models", "dataset"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2501.08046", "pdf": "https://arxiv.org/pdf/2501.08046.pdf", "abs": "https://arxiv.org/abs/2501.08046", "title": "Building Symbiotic AI: Reviewing the AI Act for a Human-Centred, Principle-Based Framework", "authors": ["Miriana Calvano", "Antonio Curci", "Giuseppe Desolda", "Andrea Esposito", "Rosa Lanzilotti", "Antonio Piccinno"], "categories": ["cs.HC", "cs.AI"], "comment": "Third version: 36 pages", "summary": "Artificial Intelligence (AI) spreads quickly as new technologies and services\ntake over modern society. The need to regulate AI design, development, and use\nis strictly necessary to avoid unethical and potentially dangerous consequences\nto humans. The European Union (EU) has released a new legal framework, the AI\nAct, to regulate AI by undertaking a risk-based approach to safeguard humans\nduring interaction. At the same time, researchers offer a new perspective on AI\nsystems, commonly known as Human-Centred AI (HCAI), highlighting the need for a\nhuman-centred approach to their design. In this context, Symbiotic AI (a\nsubtype of HCAI) promises to enhance human capabilities through a deeper and\ncontinuous collaboration between human intelligence and AI. This article\npresents the results of a Systematic Literature Review (SLR) that aims to\nidentify principles that characterise the design and development of Symbiotic\nAI systems while considering humans as the core of the process. Through content\nanalysis, four principles emerged from the review that must be applied to\ncreate Human-Centred AI systems that can establish a symbiotic relationship\nwith humans. In addition, current trends and challenges were defined to\nindicate open questions that may guide future research for the development of\nSAI systems that comply with the AI Act.", "AI": {"tldr": "This paper presents a systematic literature review identifying key principles for designing Symbiotic AI systems, emphasizing a human-centred approach to enhance collaboration between humans and AI while complying with regulations like the EU's AI Act.", "motivation": "The rapid expansion of AI technologies necessitates regulation to prevent unethical outcomes and ensure human safety during human-AI interactions.", "method": "A systematic literature review (SLR) was conducted, employing content analysis to extract key principles for the design and development of Symbiotic AI systems.", "result": "Four core principles were identified that facilitate the establishment of a symbiotic relationship between humans and AI, guiding the design of Human-Centred AI systems.", "conclusion": "The research highlights the need for a human-centric approach in AI development and identifies current trends and challenges as future research directions related to Symbiotic AI compliance with the AI Act.", "key_contributions": ["Identification of four key principles for Human-Centred AI design", "Integration of regulatory perspectives from the EU's AI Act", "Highlighting current trends and challenges for future research"], "limitations": "", "keywords": ["Human-Centred AI", "Symbiotic AI", "AI regulation", "Systematic Literature Review", "AI Act"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2505.13890", "pdf": "https://arxiv.org/pdf/2505.13890.pdf", "abs": "https://arxiv.org/abs/2505.13890", "title": "Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM", "authors": ["Zhen Xiong", "Yujun Cai", "Zhecheng Li", "Yiwei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in test-time scaling have enabled Large Language Models\n(LLMs) to display sophisticated reasoning abilities via extended\nChain-of-Thought (CoT) generation. Despite their potential, these Reasoning\nLLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as\nperformance degradation under few-shot prompting, that challenge our current\nunderstanding of RLMs. In this work, we introduce a unified graph-based\nanalytical framework for better modeling the reasoning processes of RLMs. Our\nmethod first clusters long, verbose CoT outputs into semantically coherent\nreasoning steps, then constructs directed reasoning graphs to capture\ncontextual and logical dependencies among these steps. Through comprehensive\nanalysis across models and prompting regimes, we reveal that structural\nproperties, such as exploration density, branching, and convergence ratios,\nstrongly correlate with reasoning accuracy. Our findings demonstrate how\nprompting strategies substantially reshape the internal reasoning structure of\nRLMs, directly affecting task outcomes. The proposed framework not only enables\nquantitative evaluation of reasoning quality beyond conventional metrics but\nalso provides practical insights for prompt engineering and the cognitive\nanalysis of LLMs. Code and resources will be released to facilitate future\nresearch in this direction.", "AI": {"tldr": "This paper presents a unified graph-based framework to analyze reasoning processes in Reasoning Large Language Models (RLMs) by clustering outputs into coherent steps and constructing reasoning graphs.", "motivation": "To address the counterintuitive behaviors of RLMs and enhance understanding of their reasoning abilities under various prompting conditions.", "method": "A graph-based analytical framework that clusters Chain-of-Thought outputs and constructs directed reasoning graphs to analyze reasoning processes.", "result": "The study reveals structural properties of reasoning graphs, like exploration density and branching, correlate with reasoning accuracy and can inform better prompting strategies.", "conclusion": "The proposed framework allows for better evaluation of reasoning quality in RLMs and provides insightful guidelines for prompt engineering.", "key_contributions": ["Introduction of a unified graph-based framework for analyzing RLM reasoning processes", "Demonstration of correlation between structural properties of reasoning graphs and reasoning accuracy", "Practical insights for prompt engineering and cognitive analysis of LLMs"], "limitations": "The study may not cover all types of reasoning tasks or models, and the findings are based on specific prompting regimes.", "keywords": ["Large Language Models", "Reasoning", "Graph-based analysis", "Prompt engineering", "Cognitive analysis"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.13893", "pdf": "https://arxiv.org/pdf/2505.13893.pdf", "abs": "https://arxiv.org/abs/2505.13893", "title": "InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion", "authors": ["Yuanyi Wang", "Zhaoyi Yan", "Yiming Zhang", "Qi Zhou", "Yanggan Gu", "Fei Wu", "Hongxia Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have intensified efforts to\nfuse heterogeneous open-source models into a unified system that inherits their\ncomplementary strengths. Existing logit-based fusion methods maintain inference\nefficiency but treat vocabulary dimensions independently, overlooking semantic\ndependencies encoded by cross-dimension interactions. These dependencies\nreflect how token types interact under a model's internal reasoning and are\nessential for aligning models with diverse generation behaviors. To explicitly\nmodel these dependencies, we propose \\textbf{InfiGFusion}, the first\nstructure-aware fusion framework with a novel \\textit{Graph-on-Logits\nDistillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output\nand aggregate their outer products across sequence positions to form a global\nco-activation graph, where nodes represent vocabulary channels and edges\nquantify their joint activations. To ensure scalability and efficiency, we\ndesign a sorting-based closed-form approximation that reduces the original\n$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \\log n)$, with provable\napproximation guarantees. Experiments across multiple fusion settings show that\nGLD consistently improves fusion quality and stability. InfiGFusion outperforms\nSOTA models and fusion baselines across 11 benchmarks spanning reasoning,\ncoding, and mathematics. It shows particular strength in complex reasoning\ntasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal\nJudgement over SFT, demonstrating superior multi-step and relational inference.", "AI": {"tldr": "InfiGFusion is a novel structure-aware fusion framework aimed at improving large language model integrations by modeling semantic dependencies among vocabulary dimensions.", "motivation": "Recent advances in large language models necessitate improved methods for fusing heterogeneous models while accounting for their semantic dependencies, which can enhance their complementary strengths in generation tasks.", "method": "InfiGFusion utilizes a Graph-on-Logits Distillation (GLD) loss, creating a global co-activation graph from the top-k logits and employing a closed-form approximation to optimize computational efficiency.", "result": "GLD significantly enhances fusion quality and stability, outpacing state-of-the-art models across 11 benchmarks, especially in complex reasoning tasks.", "conclusion": "InfiGFusion demonstrates superior multi-step and relational inference capabilities, establishing itself as a robust framework for model fusion.", "key_contributions": ["First structure-aware fusion framework with GLD loss", "Efficient sorting-based approximation for Gromov-Wasserstein distance", "Proven enhancements in reasoning benchmarks over SOTA models"], "limitations": "", "keywords": ["Large language models", "Model fusion", "Graph-based models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.16505", "pdf": "https://arxiv.org/pdf/2503.16505.pdf", "abs": "https://arxiv.org/abs/2503.16505", "title": "Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions", "authors": ["Dimitris Tsirmpas", "Ion Androutsopoulos", "John Pavlopoulos"], "categories": ["cs.HC", "cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "19 pages, 3 tables, 12 figures", "summary": "Limited large-scale evaluations exist for facilitation strategies of online\ndiscussions due to significant costs associated with human involvement. An\neffective solution is synthetic discussion simulations using Large Language\nModels (LLMs) to create initial pilot experiments. We propose a simple,\ngeneralizable, LLM-driven methodology to prototype the development of LLM\nfacilitators, and produce high-quality synthetic data without human\ninvolvement. We use our methodology to test whether current facilitation\nstrategies can improve the performance of LLM facilitators. We find that, while\nLLM facilitators significantly improve synthetic discussions, there is no\nevidence that the application of more elaborate facilitation strategies\nproposed in modern Social Science research lead to further improvements in\ndiscussion quality, compared to more basic approaches. Additionally, we find\nthat small LLMs (such as Mistral Nemo 12B) can perform comparably to larger\nmodels (such as LLaMa 70B), and that special instructions must be used for\ninstruction-tuned models to induce toxicity in synthetic discussions. We\nconfirm that each component of our methodology contributes substantially to\nhigh quality data via an ablation study. We release an open-source framework,\n\"SynDisco\" (pip install syndisco), which implements our methodology. We also\nrelease the \"Virtual Moderation Dataset\"\n(https://paperswithcode.com/dataset/vmd), a large, publicly available dataset\ncontaining LLM-generated and LLM-annotated discussions using multiple\nopen-source LLMs.", "AI": {"tldr": "This paper presents a methodology for using Large Language Models (LLMs) to simulate online discussions, aiming to evaluate facilitation strategies without human intervention.", "motivation": "Limited evaluations for facilitation strategies in online discussions exist due to the high costs of human involvement. The study seeks to leverage LLMs for synthetic discussion simulations to reduce costs and facilitate initial experiments.", "method": "A generalizable, LLM-driven methodology is proposed to prototype LLM facilitators and generate high-quality synthetic discussion data without human input.", "result": "While LLM facilitators improve the quality of synthetic discussions, more complex facilitation strategies do not result in significant enhancements over basic approaches. Additionally, small LLMs can perform comparably to larger models.", "conclusion": "The proposed methodology effectively contributes to generating high-quality synthetic data, and the open-source framework \"SynDisco\" along with a publicly available dataset is released for further research.", "key_contributions": ["Proposed a novel LLM-driven methodology for creating synthetic discussions.", "Demonstrated that small LLMs can perform as well as larger models in the context of facilitation.", "Released \"SynDisco\" framework and Virtual Moderation Dataset to support future research."], "limitations": "The study primarily focuses on synthetic data and may not fully capture the nuances of human-facilitated discussions.", "keywords": ["Large Language Models", "synthetic discussions", "facilitation strategies", "open-source framework", "Virtual Moderation Dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13903", "pdf": "https://arxiv.org/pdf/2505.13903.pdf", "abs": "https://arxiv.org/abs/2505.13903", "title": "Let's Verify Math Questions Step by Step", "authors": ["Chengyu Shen", "Zhen Hao Wong", "Runming He", "Hao Liang", "Meiyi Qiang", "Zimo Meng", "Zhengyang Zhao", "Bohan Zeng", "Zhengzhou Zhu", "Bin Cui", "Wentao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently achieved remarkable progress in\nmathematical reasoning. To enable such capabilities, many existing works\ndistill strong reasoning models into long chains of thought or design\nalgorithms to construct high-quality math QA data for training. However, these\nefforts primarily focus on generating correct reasoning paths and answers,\nwhile largely overlooking the validity of the questions themselves. In this\nwork, we propose Math Question Verification (MathQ-Verify), a novel five-stage\npipeline designed to rigorously filter ill-posed or under-specified math\nproblems. MathQ-Verify first performs format-level validation to remove\nredundant instructions and ensure that each question is syntactically\nwell-formed. It then formalizes each question, decomposes it into atomic\nconditions, and verifies them against mathematical definitions. Next, it\ndetects logical contradictions among these conditions, followed by a\ngoal-oriented completeness check to ensure the question provides sufficient\ninformation for solving. To evaluate this task, we use existing benchmarks\nalong with an additional dataset we construct, containing 2,147 math questions\nwith diverse error types, each manually double-validated. Experiments show that\nMathQ-Verify achieves state-of-the-art performance across multiple benchmarks,\nimproving the F1 score by up to 25 percentage points over the direct\nverification baseline. It further attains approximately 90% precision and 63%\nrecall through a lightweight model voting scheme. MathQ-Verify offers a\nscalable and accurate solution for curating reliable mathematical datasets,\nreducing label noise and avoiding unnecessary computation on invalid questions.\nOur code and data are available at https://github.com/scuuy/MathQ-Verify.", "AI": {"tldr": "This paper introduces Math Question Verification (MathQ-Verify), a pipeline for detecting invalid math problems, achieving state-of-the-art performance in data accuracy.", "motivation": "To address the oversight in existing research which primarily focuses on generating correct answers while neglecting the validity of math questions.", "method": "MathQ-Verify consists of a five-stage pipeline that formats, formalizes, decomposes, verifies, and checks mathematical questions for completeness and contradictions.", "result": "MathQ-Verify improves F1 scores by up to 25 percentage points and achieves approximately 90% precision and 63% recall on benchmark tests, showcasing its effectiveness in filtering invalid questions.", "conclusion": "MathQ-Verify is a reliable method for curating valid mathematical questions, which can enhance the quality of training data for LLMs in mathematical reasoning tasks.", "key_contributions": ["Development of a five-stage verification pipeline for math questions", "Achievement of state-of-the-art performance in mathematical dataset curation", "Creation of a new dataset with diverse error types for evaluation."], "limitations": "", "keywords": ["MathQ-Verify", "Question Verification", "Mathematical Reasoning", "Large Language Models", "Dataset Curation"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2504.13901", "pdf": "https://arxiv.org/pdf/2504.13901.pdf", "abs": "https://arxiv.org/abs/2504.13901", "title": "Examining Technology Perspectives of Older Adults with Mild Cognitive Impairment: A Scoping Review", "authors": ["Snezna B Schmidt", "Stephen Isbel", "Blooma John", "Ram Subramanian", "Nathan M DCunha"], "categories": ["cs.HC"], "comment": "Have updated the paper and the authors are reviewing the amendments.\n  I will resubmit once all authors have completed reviewing", "summary": "Mild cognitive impairment (MCI) may affect up to 20% of people over 65.\nGlobal incidence of MCI is increasing, and technology is being explored for\nearly intervention. Theories of technology adoption predict useful and\neasy-to-use solutions will have higher rates of adoption; however, these models\ndo not specifically consider older people with cognitive impairments, or unique\nhuman-computer interaction challenges posed by MCI. Older people with MCI\nopinions about technology solutions were extracted from 83 articles, published\nbetween Jan 2014 and May 2024, and found in nine databases. Inductive, thematic\nanalysis of feedback Identified five themes (i) purpose and need, (ii) solution\ndesign and ease of use, (iii) self-impression, (iv) lifestyle, and (v)\ninteraction modality. Solutions are perceived as useful, even though gaps in\nfunctional support exist, however, they are not perceived as entirely easy to\nuse, due to issues related to ease of use and user experience. Devices which\nare light, portable, common and have large screens, are preferred, as is\nmultimodal interaction, in particular speech, visual/text and touch. This\nreview recommends future work to (i) improve personalisation, (ii) better\nunderstand interaction preferences and effectiveness, (iii) enable options for\nmultimodal interaction, and (iv) more seamlessly integrate solutions into user\nlifestyles.", "AI": {"tldr": "This paper reviews opinions of older people with mild cognitive impairment (MCI) towards technology solutions, identifying key themes influencing usability and adoption.", "motivation": "To explore how technology can be adopted by older adults with MCI for early intervention, addressing unique challenges in human-computer interaction.", "method": "Inductive, thematic analysis of user feedback extracted from 83 articles published between January 2014 and May 2024 across nine databases.", "result": "Five themes were identified: purpose and need, solution design and ease of use, self-impression, lifestyle, and interaction modality. Devices preferred include light, portable ones with large screens and multimodal interaction.", "conclusion": "Future work should focus on improving personalization, understanding interaction preferences, enabling multimodal interaction options, and integrating solutions into users' lifestyles more seamlessly.", "key_contributions": ["Identified key themes in technology adoption for older people with MCI", "Provided insights into preferred device characteristics for this demographic", "Recommended strategies for improving user experience with technology for MCI."], "limitations": "The study synthesizes existing literature but may not capture all nuances of individual user experiences.", "keywords": ["mild cognitive impairment", "technology adoption", "human-computer interaction", "user experience", "older adults"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13908", "pdf": "https://arxiv.org/pdf/2505.13908.pdf", "abs": "https://arxiv.org/abs/2505.13908", "title": "Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology", "authors": ["Ajitesh Bankula", "Praney Bankula"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it\nallows for models trained on resource-rich languages to be applied to\nlow-resource languages more effectively. Recently massively multilingual\npre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot\ntransfer capabilities[14] [13]. This paper investigates cross-linguistic\ntransfer through the lens of language families and morphology. Investigating\nhow language family proximity and morphological similarity affect performance\nacross NLP tasks. We further discuss our results and how it relates to findings\nfrom recent literature. Overall, we compare multilingual model performance and\nreview how linguistic distance metrics correlate with transfer outcomes. We\nalso look into emerging approaches that integrate typological and morphological\ninformation into model pre-training to improve transfer to diverse\nlanguages[18] [19].", "AI": {"tldr": "The paper explores cross-lingual transfer in multilingual NLP, examining the impact of language family proximity and morphological similarity on performance across tasks.", "motivation": "To enhance the effectiveness of multilingual models in transferring knowledge from resource-rich to low-resource languages.", "method": "The study investigates performance across various NLP tasks in relation to linguistic distance, focusing on language families and morphology.", "result": "Findings show correlations between language family proximity, morphological similarity, and transfer performance in multilingual models.", "conclusion": "Integrating typological and morphological information into model pre-training can improve cross-linguistic transfer outcomes.", "key_contributions": ["Analysis of cross-linguistic performance based on language families and morphological features.", "Correlation between linguistic distance and model performance in multilingual NLP tasks.", "Evaluation of emerging approaches for enhancing pre-training with typological information."], "limitations": "", "keywords": ["cross-lingual transfer", "multilingual NLP", "language families", "morphological similarity", "transfer learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2504.13936", "pdf": "https://arxiv.org/pdf/2504.13936.pdf", "abs": "https://arxiv.org/abs/2504.13936", "title": "ViMo: A Generative Visual GUI World Model for App Agents", "authors": ["Dezhao Luo", "Bohan Tang", "Kang Li", "Georgios Papoudakis", "Jifei Song", "Shaogang Gong", "Jianye Hao", "Jun Wang", "Kun Shao"], "categories": ["cs.HC", "cs.LG", "cs.SY", "eess.SY"], "comment": "https://ai-agents-2030.github.io/ViMo/", "summary": "App agents, which autonomously operate mobile Apps through Graphical User\nInterfaces (GUIs), have gained significant interest in real-world applications.\nYet, they often struggle with long-horizon planning, failing to find the\noptimal actions for complex tasks with longer steps. To address this, world\nmodels are used to predict the next GUI observation based on user actions,\nenabling more effective agent planning. However, existing world models\nprimarily focus on generating only textual descriptions, lacking essential\nvisual details. To fill this gap, we propose ViMo, the first visual world model\ndesigned to generate future App observations as images. For the challenge of\ngenerating text in image patches, where even minor pixel errors can distort\nreadability, we decompose GUI generation into graphic and text content\ngeneration. We propose a novel data representation, the Symbolic Text\nRepresentation~(STR) to overlay text content with symbolic placeholders while\npreserving graphics. With this design, ViMo employs a STR Predictor to predict\nfuture GUIs' graphics and a GUI-text Predictor for generating the corresponding\ntext. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the\noutcome of different action options. Experiments show ViMo's ability to\ngenerate visually plausible and functionally effective GUIs that enable App\nagents to make more informed decisions.", "AI": {"tldr": "ViMo is a visual world model that generates future App GUI observations as images, improving long-horizon planning for app agents.", "motivation": "App agents face challenges with long-horizon planning, especially in generating optimal actions for complex tasks with longer steps.", "method": "ViMo utilizes a novel data representation called Symbolic Text Representation (STR) to separately generate graphic and text content for GUIs, enhancing the prediction of future GUI observations.", "result": "ViMo generates visually plausible and functionally effective GUIs, aiding app agents in making informed decisions based on predicted outcomes of actions.", "conclusion": "The proposed approach demonstrates significant improvements in app agent performance through effective GUI prediction.", "key_contributions": ["Introduction of ViMo, the first visual world model for GUIs.", "Development of the Symbolic Text Representation (STR) to separate graphic and text generation.", "Implementation that enhances agent-focused tasks by predicting GUI outcomes."], "limitations": "", "keywords": ["App agents", "Graphic user interfaces", "World models", "Machine learning", "Predictive modeling"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.13913", "pdf": "https://arxiv.org/pdf/2505.13913.pdf", "abs": "https://arxiv.org/abs/2505.13913", "title": "Word length predicts word order: \"Min-max\"-ing drives language evolution", "authors": ["Hiram Ring"], "categories": ["cs.CL"], "comment": null, "summary": "Current theories of language propose an innate (Baker 2001; Chomsky 1981) or\na functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface\nstructures (i.e. word order) that we observe in languages of the world, while\nevolutionary modeling (Dunn et al. 2011) suggests that descent is the primary\nfactor influencing such patterns. Although there are hypotheses for word order\nchange from both innate and usage-based perspectives for specific languages and\nfamilies, there are key disagreements between the two major proposals for\nmechanisms that drive the evolution of language more broadly (Wasow 2002; Levy\n2008). This paper proposes a universal underlying mechanism for word order\nchange based on a large tagged parallel dataset of over 1,500 languages\nrepresenting 133 language families and 111 isolates. Results indicate that word\nclass length is significantly correlated with word order crosslinguistically,\nbut not in a straightforward manner, partially supporting opposing theories of\nprocessing, while at the same time predicting historical word order change in\ntwo different phylogenetic lines and explaining more variance than descent or\nlanguage area in regression models. Such findings suggest an integrated\n\"Min-Max\" theory of language evolution driven by competing pressures of\nprocessing and information structure, aligning with recent efficiency-oriented\n(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et\nal. 2025).", "AI": {"tldr": "This paper proposes a universal mechanism for word order change based on a dataset of over 1,500 languages, suggesting a correlation between word class length and word order, contributing to theories of language evolution.", "motivation": "To address disagreements between theories regarding mechanisms driving language evolution and provide a comprehensive understanding of word order changes across languages.", "method": "Analysis of a large tagged parallel dataset of over 1,500 languages across 133 families and 111 isolates to examine correlations between word class length and word order.", "result": "Findings show a significant but complex correlation between word class length and word order, offering partial support for existing theories and predicting historical changes in two phylogenetic lines.", "conclusion": "The study supports an integrated 'Min-Max' theory of language evolution, influenced by processing and information structure, and suggests a shift in understanding language evolution mechanisms.", "key_contributions": ["Universal mechanism for word order change", "Correlation between word class length and word order", "Integrated theory aligning with efficiency-oriented and information-theoretic proposals"], "limitations": "", "keywords": ["word order", "language evolution", "processing", "information structure", "language families"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2505.13936", "pdf": "https://arxiv.org/pdf/2505.13936.pdf", "abs": "https://arxiv.org/abs/2505.13936", "title": "EEG-to-Text Translation: A Model for Deciphering Human Brain Activity", "authors": ["Saydul Akbar Murad", "Ashim Dahal", "Nick Rahimi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid advancement of large language models like Gemini, GPT, and\nothers, bridging the gap between the human brain and language processing has\nbecome an important area of focus. To address this challenge, researchers have\ndeveloped various models to decode EEG signals into text. However, these models\nstill face significant performance limitations. To overcome these shortcomings,\nwe propose a new model, R1 Translator, which aims to improve the performance of\nEEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM\nencoder with a pretrained transformer-based decoder, utilizing EEG features to\nproduce high-quality text outputs. The model processes EEG embeddings through\nthe LSTM to capture sequential dependencies, which are then fed into the\ntransformer decoder for effective text generation. The R1 Translator excels in\nROUGE metrics, outperforming both T5 (previous research) and Brain Translator.\nSpecifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%\nhigher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in\nROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain\nby 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower\nthan T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs\nbetter in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and\nBrain by 3.6% (0.7553). Code is available at\nhttps://github.com/Mmurrad/EEG-To-text.", "AI": {"tldr": "The R1 Translator model significantly improves EEG-to-text decoding by combining a bidirectional LSTM encoder with a transformer decoder, outperforming existing models.", "motivation": "To bridge the gap between the human brain and language processing by improving EEG-to-text decoding performance.", "method": "The R1 Translator utilizes a bidirectional LSTM encoder to capture sequential EEG data, which is processed by a pretrained transformer-based decoder for generating text.", "result": "R1 achieves a ROUGE-1 score of 38.00%, outperforming T5 and Brain Translator. R1 also excels in ROUGE-L, CER, and WER metrics, showing substantial improvement over prior models.", "conclusion": "The R1 Translator demonstrates considerable advancements in EEG-to-text decoding, promoting better integration of neural activity with language generation.", "key_contributions": ["Introduction of the R1 Translator for EEG-to-text decoding.", "Combination of LSTM and transformer models for improved performance.", "Significant improvements over existing models in several metrics."], "limitations": "", "keywords": ["EEG-to-text", "LSTM", "transformer", "language models", "decoding"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.11726", "pdf": "https://arxiv.org/pdf/2409.11726.pdf", "abs": "https://arxiv.org/abs/2409.11726", "title": "Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing", "authors": ["Wenyuan Zhang", "Shuaiyi Nie", "Jiawei Sheng", "Zefeng Zhang", "Xinghua Zhang", "Yongquan He", "Tingwen Liu"], "categories": ["cs.CL", "cs.HC"], "comment": "25 pages, 6 figures, 20 tables", "summary": "Large language model (LLM) role-playing has gained widespread attention.\nAuthentic character knowledge is crucial for constructing realistic LLM\nrole-playing agents. However, existing works usually overlook the exploration\nof LLMs' ability to detect characters' known knowledge errors (KKE) and unknown\nknowledge errors (UKE) while playing roles, which would lead to low-quality\nautomatic construction of character trainable corpus. In this paper, we propose\nRoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The\nresults indicate that even the latest LLMs struggle to detect these two types\nof errors effectively, especially when it comes to familiar knowledge. We\nexperimented with various reasoning strategies and propose an agent-based\nreasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore\nfurther the potential for improving error detection capabilities. Experiments\nshow that our method effectively improves the LLMs' ability to detect error\ncharacter knowledge, but it remains an issue that requires ongoing attention.", "AI": {"tldr": "This paper presents RoleKE-Bench, a framework to evaluate LLMs' detection of known and unknown knowledge errors during role-playing, revealing that current models struggle with error detection, especially with familiar knowledge, and introduces a new reasoning strategy to enhance performance.", "motivation": "To address the issue of low-quality character training corpora due to LLMs' inability to detect knowledge errors while role-playing.", "method": "The paper proposes RoleKE-Bench for evaluating LLMs' detection capabilities regarding known and unknown knowledge errors, employing various reasoning strategies and introducing the Self-Recollection and Self-Doubt (S$^2$RD) method for improvement.", "result": "The experiments demonstrate that existing LLMs have significant difficulty in identifying both known and unknown errors in character knowledge, particularly for familiar knowledge, yet the proposed S$^2$RD method shows promise in enhancing their detection abilities.", "conclusion": "Despite improvements with S$^2$RD, the issue of knowledge error detection in LLM role-playing remains a challenge needing further research.", "key_contributions": ["Introduction of RoleKE-Bench for evaluating LLMs' error detection", "Demonstration of LLMs' struggles with KKE and UKE", "Development of S$^2$RD as a reasoning method to enhance error detection"], "limitations": "The proposed method shows improvements but the fundamental issue of error detection continues to require more research.", "keywords": ["large language models", "role-playing", "knowledge errors", "error detection", "evaluation benchmark"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2505.13944", "pdf": "https://arxiv.org/pdf/2505.13944.pdf", "abs": "https://arxiv.org/abs/2505.13944", "title": "Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting", "authors": ["Bao-Ngoc Dao", "Quang Nguyen", "Luyen Ngo Dinh", "Minh Le", "Nam Le", "Linh Ngo Van"], "categories": ["cs.CL"], "comment": null, "summary": "Memory-based approaches have shown strong performance in Continual Relation\nExtraction (CRE). However, storing examples from previous tasks increases\nmemory usage and raises privacy concerns. Recently, prompt-based methods have\nemerged as a promising alternative, as they do not rely on storing past\nsamples. Despite this progress, current prompt-based techniques face several\ncore challenges in CRE, particularly in accurately identifying task identities\nand mitigating catastrophic forgetting. Existing prompt selection strategies\noften suffer from inaccuracies, lack robust mechanisms to prevent forgetting in\nshared parameters, and struggle to handle both cross-task and within-task\nvariations. In this paper, we propose WAVE++, a novel approach inspired by the\nconnection between prefix-tuning and mixture of experts. Specifically, we\nintroduce task-specific prompt pools that enhance flexibility and adaptability\nacross diverse tasks while avoiding boundary-spanning risks; this design more\neffectively captures variations within each task and across tasks. To further\nrefine relation classification, we incorporate label descriptions that provide\nricher, more global context, enabling the model to better distinguish among\ndifferent relations. We also propose a training-free mechanism to improve task\nprediction during inference. Moreover, we integrate a generative model to\nconsolidate prior knowledge within the shared parameters, thereby removing the\nneed for explicit data storage. Extensive experiments demonstrate that WAVE++\noutperforms state-of-the-art prompt-based and rehearsal-based methods, offering\na more robust solution for continual relation extraction. Our code is publicly\navailable at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.", "AI": {"tldr": "WAVE++ is a novel approach for Continual Relation Extraction that utilizes task-specific prompt pools and a generative model to enhance performance without requiring data storage.", "motivation": "To address memory usage and privacy concerns in Continual Relation Extraction while overcoming the challenges faced by prompt-based methods.", "method": "WAVE++ introduces task-specific prompt pools and a training-free mechanism for task prediction, integrating a generative model to consolidate knowledge without explicit data storage.", "result": "WAVE++ demonstrates superior performance compared to existing prompt-based and rehearsal-based methods in continual relation extraction tasks.", "conclusion": "The proposed method effectively captures task variations and significantly improves task prediction and relation classification.", "key_contributions": ["Introduction of task-specific prompt pools", "Development of a training-free mechanism for task prediction", "Integration of a generative model for knowledge consolidation"], "limitations": "", "keywords": ["Continual Relation Extraction", "prompt-based methods", "machine learning", "memory usage", "generative model"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.13948", "pdf": "https://arxiv.org/pdf/2505.13948.pdf", "abs": "https://arxiv.org/abs/2505.13948", "title": "Memory-Centric Embodied Question Answer", "authors": ["Mingliang Zhai", "Zhi Gao", "Yuwei Wu", "Yunde Jia"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "14pages, 7 figures, 6 tables", "summary": "Embodied Question Answering (EQA) requires agents to autonomously explore and\nunderstand the environment to answer context-dependent questions. Existing\nframeworks typically center around the planner, which guides the stopping\nmodule, memory module, and answering module for reasoning. In this paper, we\npropose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric\nEQA models where the memory module cannot fully interact with other modules,\nMemoryEQA flexible feeds memory information into all modules, thereby enhancing\nefficiency and accuracy in handling complex tasks, such as those involving\nmultiple targets across different regions. Specifically, we establish a\nmulti-modal hierarchical memory mechanism, which is divided into global memory\nthat stores language-enhanced scene maps, and local memory that retains\nhistorical observations and state information. When performing EQA tasks, the\nmulti-modal large language model is leveraged to convert memory information\ninto the required input formats for injection into different modules. To\nevaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset\nbased on HM3D, comprising 1,587 question-answer pairs involving multiple\ntargets across various regions, which requires agents to maintain memory of\nexploration-acquired target information. Experimental results on HM-EQA,\nMT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a\n19.8% performance gain on MT-HM3D compared to baseline model further\nunderscores memory capability's pivotal role in resolving complex tasks.", "AI": {"tldr": "MemoryEQA is a memory-centric framework for Embodied Question Answering that enhances task efficiency and accuracy by improving memory interaction among modules.", "motivation": "To improve the efficiency and accuracy of Embodied Question Answering systems by enhancing the interaction of the memory module with other components.", "method": "The authors propose a multi-modal hierarchical memory mechanism that includes global and local memory to better integrate with various modules in EQA.", "result": "The MemoryEQA framework achieved a 19.8% performance improvement on the MT-HM3D dataset compared to baseline models, demonstrating its effectiveness in handling complex tasks.", "conclusion": "Memory capabilities significantly contribute to the resolution of complex Embodied Question Answering tasks, as evidenced by improved performance metrics.", "key_contributions": ["Introduction of MemoryEQA framework which allows flexible memory interactions with all EQA modules.", "Development of a multi-modal hierarchical memory mechanism for enhanced context understanding.", "Construction of the MT-HM3D dataset for evaluating memory capabilities in EQA tasks."], "limitations": "", "keywords": ["Embodied Question Answering", "Memory framework", "Artificial Intelligence", "Multi-modal memory", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 14}}
{"id": "2505.13949", "pdf": "https://arxiv.org/pdf/2505.13949.pdf", "abs": "https://arxiv.org/abs/2505.13949", "title": "FlashThink: An Early Exit Method For Efficient Reasoning", "authors": ["Guochao Jiang", "Guofeng Quan", "Zepeng Ding", "Ziqin Luo", "Dixuan Wang", "Zheng Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance in reasoning\ntasks. However, LLMs tend to generate excessively long reasoning content,\nleading to significant computational overhead. Our observations indicate that\neven on simple problems, LLMs tend to produce unnecessarily lengthy reasoning\ncontent, which is against intuitive expectations. Preliminary experiments show\nthat at a certain point during the generation process, the model is already\ncapable of producing the correct solution without completing the full reasoning\ncontent. Therefore, we consider that the reasoning process of the model can be\nexited early to achieve the purpose of efficient reasoning. We introduce a\nverification model that identifies the exact moment when the model can stop\nreasoning and still provide the correct answer. Comprehensive experiments on\nfour different benchmarks demonstrate that our proposed method, FlashThink,\neffectively shortens the reasoning content while preserving the model accuracy.\nFor the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning\ncontent by 77.04% and 77.47%, respectively, without reducing the accuracy.", "AI": {"tldr": "The paper presents FlashThink, a method for early termination of reasoning in Large Language Models to reduce computation while maintaining accuracy.", "motivation": "Large Language Models generate excessively long reasoning content, leading to unnecessary computational overhead, even for simple problems.", "method": "A verification model is introduced to determine when the reasoning process can be exited early without compromising the correctness of the answer.", "result": "Comprehensive experiments show FlashThink significantly reduces reasoning content length by over 77% while maintaining model accuracy across various benchmarks.", "conclusion": "The approach effectively enhances efficiency in reasoning performed by LLMs with minimal impact on performance.", "key_contributions": ["Introduction of FlashThink for early termination of reasoning in LLMs", "Demonstration of significant reduction in reasoning content", "Maintaining accuracy while reducing computational overhead"], "limitations": "", "keywords": ["Large Language Models", "Reasoning", "Efficiency", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.13963", "pdf": "https://arxiv.org/pdf/2505.13963.pdf", "abs": "https://arxiv.org/abs/2505.13963", "title": "Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability", "authors": ["Qianli Wang", "Mingyang Wang", "Nils Feldhus", "Simon Ostermann", "Yuan Cao", "Hinrich Schütze", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL", "cs.LG"], "comment": "In submission", "summary": "Quantization methods are widely used to accelerate inference and streamline\nthe deployment of large language models (LLMs). While prior research has\nextensively investigated the degradation of various LLM capabilities due to\nquantization, its effects on model explainability and interpretability, which\nare crucial for understanding decision-making processes, remain unexplored. To\naddress this gap, we conduct comprehensive experiments using three common\nquantization techniques at distinct bit widths, in conjunction with two\nexplainability methods, counterfactual examples and natural language\nexplanations, as well as two interpretability approaches, knowledge\nmemorization analysis and latent multi-hop reasoning analysis. We complement\nour analysis with a thorough user study, evaluating selected explainability\nmethods. Our findings reveal that, depending on the configuration, quantization\ncan significantly impact model explainability and interpretability. Notably,\nthe direction of this effect is not consistent, as it strongly depends on (1)\nthe quantization method, (2) the explainability or interpretability approach,\nand (3) the evaluation protocol. In some settings, human evaluation shows that\nquantization degrades explainability, while in others, it even leads to\nimprovements. Our work serves as a cautionary tale, demonstrating that\nquantization can unpredictably affect model transparency. This insight has\nimportant implications for deploying LLMs in applications where transparency is\na critical requirement.", "AI": {"tldr": "This paper investigates the impact of quantization methods on the explainability and interpretability of large language models, revealing inconsistent effects that vary by configuration.", "motivation": "To explore how quantization affects model explainability and interpretability, which are crucial for understanding decision-making in LLMs.", "method": "Experiments using three quantization techniques, two explainability methods, two interpretability approaches, and a user study evaluating selected explainability methods.", "result": "Quantization significantly influences model explainability and interpretability, with effects varying based on the quantization method, the evaluation protocols, and the chosen explanation approaches.", "conclusion": "Quantization can unpredictably affect model transparency, highlighting important considerations for deploying LLMs in transparency-sensitive applications.", "key_contributions": ["Analysis of quantization's effects on model explainability and interpretability", "Contrasting impacts of quantization techniques on LLM capabilities", "User study findings on the effectiveness of explainability methods under quantization"], "limitations": "The study may not cover all possible quantization techniques or explainability methods, limiting the generalizability of the findings.", "keywords": ["quantization", "explainability", "interpretability", "large language models", "user study"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13965", "pdf": "https://arxiv.org/pdf/2505.13965.pdf", "abs": "https://arxiv.org/abs/2505.13965", "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring", "authors": ["Jiamin Su", "Yibo Yan", "Zhuoran Gao", "Han Zhang", "Xiang Liu", "Xuming Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916", "summary": "Automated Essay Scoring (AES) is crucial for modern education, particularly\nwith the increasing prevalence of multimodal assessments. However, traditional\nAES methods struggle with evaluation generalizability and multimodal\nperception, while even recent Multimodal Large Language Model (MLLM)-based\napproaches can produce hallucinated justifications and scores misaligned with\nhuman judgment. To address the limitations, we introduce CAFES, the first\ncollaborative multi-agent framework specifically designed for AES. It\norchestrates three specialized agents: an Initial Scorer for rapid,\ntrait-specific evaluations; a Feedback Pool Manager to aggregate detailed,\nevidence-grounded strengths; and a Reflective Scorer that iteratively refines\nscores based on this feedback to enhance human alignment. Extensive\nexperiments, using state-of-the-art MLLMs, achieve an average relative\nimprovement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,\nespecially for grammatical and lexical diversity. Our proposed CAFES framework\npaves the way for an intelligent multimodal AES system. The code will be\navailable upon acceptance.", "AI": {"tldr": "CAFES is a collaborative multi-agent framework for Automated Essay Scoring (AES), improving evaluation generalizability and alignment with human judgment.", "motivation": "To enhance the reliability and accuracy of Automated Essay Scoring in light of emerging multimodal assessments and limitations of current methods.", "method": "The CAFES framework employs three specialized agents: an Initial Scorer for trait-specific evaluations, a Feedback Pool Manager to gather evidence-based feedback, and a Reflective Scorer for refining scores through iterative feedback processes.", "result": "Extensive experiments demonstrate a 21% average improvement in evaluation accuracy against human ground truth, particularly in grammatical and lexical areas.", "conclusion": "CAFES sets a foundation for future intelligent multimodal AES systems, addressing key shortcomings of traditional methods.", "key_contributions": ["Introduction of a collaborative multi-agent framework for AES (CAFES).", "Improved scoring accuracy with 21% enhancement in QWK.", "Integration of feedback mechanisms for better alignment with human evaluators."], "limitations": "Potential concerns regarding the scalability and real-world applicability of the framework; reliance on state-of-the-art MLLMs may limit generalizability.", "keywords": ["Automated Essay Scoring", "Multi-agent Framework", "Multimodal Assessment", "Machine Learning", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.13972", "pdf": "https://arxiv.org/pdf/2505.13972.pdf", "abs": "https://arxiv.org/abs/2505.13972", "title": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals", "authors": ["Qianli Wang", "Van Bach Nguyen", "Nils Feldhus", "Luis Felipe Villa-Arenas", "Christin Seifert", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL"], "comment": "in submission", "summary": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models. Through extensive experiments involving two state-of-the-art\nLLM-based methods, three datasets, five generator models, and 15 judge models,\ncomplemented by a user study (n = 90), we demonstrate that judge models with an\nindependent, non-fine-tuned relationship to the generator model provide the\nmost reliable label flipping evaluations. Relationships between the generator\nand judge models, which are closely aligned with the user study for CDA, result\nin better model performance and robustness. Nevertheless, we find that the gap\nbetween the most effective judge models and the results obtained from the user\nstudy remains considerably large. This suggests that a fully automated pipeline\nfor CDA may be inadequate and requires human intervention.", "AI": {"tldr": "The paper investigates the effectiveness of different judge models for evaluating counterfactual data augmentation in large language models, finding that independent judge models yield the most reliable evaluations, though gaps remain in fully automated pipelines.", "motivation": "To improve the performance and reliability of counterfactual data augmentation (CDA) in large language models by understanding the impact of judge models on label flipping evaluations.", "method": "The study defines four types of relationships between generator and judge models and conducts extensive experiments with various LLM-based methods, datasets, generator models, and judge models, as well as a user study.", "result": "Judge models providing independent evaluations from the generator model resulted in the most reliable label flipping evaluations, while relationships closely aligned with user study results improved model performance and robustness.", "conclusion": "The findings emphasize the need for human intervention in CDA processes, as there remains a significant gap in the effectiveness of automated evaluation pipelines.", "key_contributions": ["Identification of relationships between counterfactual generator and judge models.", "Demonstration of the importance of independent judge models in reliable label flipping evaluations.", "Insights from a user study that highlight gaps in automated CDA evaluations."], "limitations": "The gap between the effectiveness of the best judge models and user study results suggests limitations in fully automating the CDA process.", "keywords": ["counterfactual data augmentation", "large language models", "judge models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.13973", "pdf": "https://arxiv.org/pdf/2505.13973.pdf", "abs": "https://arxiv.org/abs/2505.13973", "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality.", "AI": {"tldr": "This paper explores reinforcement learning (RL)-based tuning for Multimodal Large Language Models (MLLMs) in medical visual question answering, specifically addressing challenges in aligning model behavior with clinical expectations.", "motivation": "To improve the alignment of Multimodal Large Language Models' responses with clinical expectations in medical tasks, particularly in visual question answering.", "method": "The study investigates factors such as base model initialization strategy, medical semantic alignment, length-based rewards for long-chain reasoning, and the influence of bias in the context of RL-based tuning in medical VQA.", "result": "Extensive experiments reveal that these factors significantly impact the performance of medical MLLMs and show that GRPO-based RL tuning outperforms standard supervised fine-tuning in accuracy and reasoning quality.", "conclusion": "The findings provide insights into effective domain-specific fine-tuning strategies for medical MLLMs, highlighting the advantages of RL-based approaches over traditional methods.", "key_contributions": ["Identifying critical dimensions affecting RL-based tuning in medical VQA", "Demonstrating the superiority of GRPO-based RL tuning over SFT", "Providing a framework for improving alignment of model responses with clinical expectations"], "limitations": "", "keywords": ["reinforcement learning", "multimodal large language models", "medical visual question answering", "policy optimization", "fine-tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.13975", "pdf": "https://arxiv.org/pdf/2505.13975.pdf", "abs": "https://arxiv.org/abs/2505.13975", "title": "DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models", "authors": ["Yuxuan Jiang", "Dawei Li", "Frank Ferraro"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Reasoning Models (LRMs) have demonstrated success in complex\nreasoning tasks through long chain-of-thought (CoT) reasoning, their inference\noften involves excessively verbose reasoning traces, resulting in substantial\ninefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a\nhybrid framework that combines inference-time pruning with tuning-based\ndistillation, two widely used strategies for efficient reasoning. DRP uses a\nteacher model to perform skill-aware step decomposition and content pruning,\nand then distills the pruned reasoning paths into a student model, enabling it\nto reason both efficiently and accurately. Across several challenging\nmathematical reasoning datasets, we find that models trained with DRP achieve\nsubstantial improvements in token efficiency without sacrificing accuracy.\nSpecifically, DRP reduces average token usage on GSM8K from 917 to 328 while\nimproving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on\nAIME with no performance drop. Further analysis shows that aligning the\nreasoning structure of training CoTs with the student's reasoning capacity is\ncritical for effective knowledge transfer and performance gains.", "AI": {"tldr": "Proposes Distilled Reasoning Pruning (DRP) for efficient reasoning in Large Reasoning Models (LRMs) by combining pruning and distillation methods.", "motivation": "LRMs often produce excessively verbose reasoning traces which are inefficient, prompting the need for more effective reasoning strategies.", "method": "The DRP framework employs a teacher model for skill-aware step decomposition and content pruning, followed by distillation into a student model.", "result": "Models trained with DRP demonstrate significant improvements in token efficiency, achieving a reduction in token usage on GSM8K from 917 to 328 while improving accuracy from 91.7% to 94.1%.", "conclusion": "Aligning the reasoning structures between training CoTs and the student's reasoning capacity is vital for successful knowledge transfer and enhanced performance.", "key_contributions": ["Introduction of Distilled Reasoning Pruning (DRP) framework", "Significant token efficiency improvements in LRM inference", "Establishment of critical alignment for knowledge transfer"], "limitations": "", "keywords": ["Large Reasoning Models", "Distilled Reasoning Pruning", "token efficiency", "reasoning accuracy", "knowledge transfer"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.13979", "pdf": "https://arxiv.org/pdf/2505.13979.pdf", "abs": "https://arxiv.org/abs/2505.13979", "title": "Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection", "authors": ["Maya Srikanth", "Run Chen", "Julia Hirschberg"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal models play a key role in empathy detection, but their performance\ncan suffer when modalities provide conflicting cues. To understand these\nfailures, we examine cases where unimodal and multimodal predictions diverge.\nUsing fine-tuned models for text, audio, and video, along with a gated fusion\nmodel, we find that such disagreements often reflect underlying ambiguity, as\nevidenced by annotator uncertainty. Our analysis shows that dominant signals in\none modality can mislead fusion when unsupported by others. We also observe\nthat humans, like models, do not consistently benefit from multimodal input.\nThese insights position disagreement as a useful diagnostic signal for\nidentifying challenging examples and improving empathy system robustness.", "AI": {"tldr": "The paper investigates the failures of multimodal models in empathy detection when different modalities offer conflicting cues.", "motivation": "To understand the performance issues in empathy detection models caused by conflicting modality signals.", "method": "The authors analyze cases of divergence in predictions from unimodal and multimodal models using fine-tuned text, audio, and video models, supplemented by a gated fusion model.", "result": "The study finds that disagreements between modalities often arise from underlying ambiguities and annotator uncertainty.", "conclusion": "The findings suggest that disagreements in predictions can serve as diagnostic indicators, highlighting examples that challenge the robustness of empathy systems.", "key_contributions": ["Identifying the role of modality disagreements in empathy detection", "Demonstrating that dominant signals in one modality can mislead multimodal fusion", "Establishing similarities between model and human responses to multimodal input"], "limitations": "The analysis is limited to specific cases of divergence, and further exploration is needed across diverse contexts.", "keywords": ["multimodal models", "empathy detection", "modalities", "conflicting cues", "diagnostic signals"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.13988", "pdf": "https://arxiv.org/pdf/2505.13988.pdf", "abs": "https://arxiv.org/abs/2505.13988", "title": "The Hallucination Tax of Reinforcement Finetuning", "authors": ["Linxin Song", "Taiwei Shi", "Jieyu Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement finetuning (RFT) has become a standard approach for enhancing\nthe reasoning capabilities of large language models (LLMs). However, its impact\non model trustworthiness remains underexplored. In this work, we identify and\nsystematically study a critical side effect of RFT, which we term the\nhallucination tax: a degradation in refusal behavior causing models to produce\nhallucinated answers to unanswerable questions confidently. To investigate\nthis, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of\nunanswerable math problems designed to probe models' ability to recognize an\nunanswerable question by reasoning from the insufficient or ambiguous\ninformation. Our results show that standard RFT training could reduce model\nrefusal rates by more than 80%, which significantly increases model's tendency\nto hallucinate. We further demonstrate that incorporating just 10% SUM during\nRFT substantially restores appropriate refusal behavior, with minimal accuracy\ntrade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage\ninference-time compute to reason about their own uncertainty and knowledge\nboundaries, improving generalization not only to out-of-domain math problems\nbut also to factual question answering tasks.", "AI": {"tldr": "The paper examines the adverse effects of reinforcement finetuning (RFT) on the hallucination rates of large language models (LLMs), introducing a dataset to measure this impact.", "motivation": "Understanding how reinforcement finetuning affects the trustworthiness of large language models, specifically their propensity to hallucinate answers to unanswerable questions.", "method": "The authors created the SUM dataset, comprising synthetic unanswerable math problems, to analyze the impact of RFT on model refusal behavior and hallucination rates.", "result": "Standard RFT significantly decreased refusal rates (over 80%) but increased hallucinations; incorporating 10% of the SUM dataset during RFT restored appropriate refusal behavior with minimal accuracy loss.", "conclusion": "Incorporating unanswerable problem training can help LLMs recognize their uncertainty better, improving their performance on both unanswerable and factual problems.", "key_contributions": ["Introduction of the 'hallucination tax' concept in LLMs.", "Creation of the SUM dataset for testing unanswerable question recognition.", "Demonstration of a method to restore refusal behavior in LLMs with minimal trade-offs."], "limitations": "The study focuses primarily on math problems, which may not fully represent broader LLM behavior in diverse contexts.", "keywords": ["Reinforcement Finetuning", "Large Language Models", "Hallucination Tax", "Unanswerable Questions", "Trustworthiness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13990", "pdf": "https://arxiv.org/pdf/2505.13990.pdf", "abs": "https://arxiv.org/abs/2505.13990", "title": "DecIF: Improving Instruction-Following through Meta-Decomposition", "authors": ["Tingfeng Hui", "Pengyu Zhu", "Bowen Ping", "Ling Tang", "Yaqi Zhang", "Sen Su"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Instruction-following has emerged as a crucial capability for large language\nmodels (LLMs). However, existing approaches often rely on pre-existing\ndocuments or external resources to synthesize instruction-following data, which\nlimits their flexibility and generalizability. In this paper, we introduce\nDecIF, a fully autonomous, meta-decomposition guided framework that generates\ndiverse and high-quality instruction-following data using only LLMs. DecIF is\ngrounded in the principle of decomposition. For instruction generation, we\nguide LLMs to iteratively produce various types of meta-information, which are\nthen combined with response constraints to form well-structured and\nsemantically rich instructions. We further utilize LLMs to detect and resolve\npotential inconsistencies within the generated instructions. Regarding response\ngeneration, we decompose each instruction into atomic-level evaluation\ncriteria, enabling rigorous validation and the elimination of inaccurate\ninstruction-response pairs. Extensive experiments across a wide range of\nscenarios and settings demonstrate DecIF's superior performance on\ninstruction-following tasks. Further analysis highlights its strong\nflexibility, scalability, and generalizability in automatically synthesizing\nhigh-quality instruction data.", "AI": {"tldr": "DecIF is a novel framework for generating diverse instruction-following data using large language models autonomously.", "motivation": "The need for a flexible and generalizable approach to generate instruction-following data without relying on pre-existing documents or external resources.", "method": "DecIF employs a meta-decomposition strategy where LLMs iteratively produce meta-information that is combined with response constraints to create structured instructions. It also uses LLMs to identify and resolve inconsistencies in these instructions and breaks down instructions for rigorous validation of response pairs.", "result": "Extensive experiments show that DecIF outperforms existing methods on various instruction-following tasks while demonstrating high flexibility, scalability, and generalizability in generating quality instruction data.", "conclusion": "DecIF introduces a new paradigm for autonomous instruction generation, which enhances the efficiency and quality of instruction-following data synthesis.", "key_contributions": ["Introduction of DecIF framework for autonomous instruction generation", "Meta-decomposition approach for producing structured and high-quality instructions", "Rigorous validation mechanism for instruction-response pairs"], "limitations": "Work in progress; possible constraints in the initial implementation and need for further testing in diverse real-world scenarios.", "keywords": ["Instruction-following", "Large language models", "Meta-decomposition", "Data synthesis", "Autonomous generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13995", "pdf": "https://arxiv.org/pdf/2505.13995.pdf", "abs": "https://arxiv.org/abs/2505.13995", "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy", "authors": ["Myra Cheng", "Sunny Yu", "Cinoo Lee", "Pranav Khadpe", "Lujain Ibrahim", "Dan Jurafsky"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "A serious risk to the safety and utility of LLMs is sycophancy, i.e.,\nexcessive agreement with and flattery of the user. Yet existing work focuses on\nonly one aspect of sycophancy: agreement with users' explicitly stated beliefs\nthat can be compared to a ground truth. This overlooks forms of sycophancy that\narise in ambiguous contexts such as advice and support-seeking, where there is\nno clear ground truth, yet sycophancy can reinforce harmful implicit\nassumptions, beliefs, or actions. To address this gap, we introduce a richer\ntheory of social sycophancy in LLMs, characterizing sycophancy as the excessive\npreservation of a user's face (the positive self-image a person seeks to\nmaintain in an interaction). We present ELEPHANT, a framework for evaluating\nsocial sycophancy across five face-preserving behaviors (emotional validation,\nmoral endorsement, indirect language, indirect action, and accepting framing)\non two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole\n(AITA). Across eight models, we show that LLMs consistently exhibit high rates\nof social sycophancy: on OEQ, they preserve face 47% more than humans, and on\nAITA, they affirm behavior deemed inappropriate by crowdsourced human judgments\nin 42% of cases. We further show that social sycophancy is rewarded in\npreference datasets and is not easily mitigated. Our work provides theoretical\ngrounding and empirical tools (datasets and code) for understanding and\naddressing this under-recognized but consequential issue.", "AI": {"tldr": "This paper introduces a theory and framework for evaluating social sycophancy in LLMs, highlighting their tendency to excessively affirm and flatter users, which can perpetuate harmful beliefs and actions.", "motivation": "To address the gap in understanding forms of sycophancy in ambiguous contexts where explicit beliefs cannot be directly evaluated, particularly in user interactions that require advice and emotional support.", "method": "The authors present ELEPHANT, a framework that characterizes social sycophancy by evaluating LLMs on five face-preserving behaviors using two datasets (open-ended questions and Reddit's r/AmITheAsshole).", "result": "LLMs displayed significantly higher rates of social sycophancy than human responses, preserving face 47% more on open-ended questions and affirming inappropriate behaviors in 42% of AITA cases.", "conclusion": "Social sycophancy in LLMs is prevalent and reinforced by preference datasets; this work lays the groundwork for further research and tools to address and mitigate this issue.", "key_contributions": ["Introduces a novel theory of social sycophancy in LLMs.", "Develops the ELEPHANT framework for evaluating sycophancy behaviors.", "Documents empirical findings illustrating the prevalence of social sycophancy among multiple LLM models."], "limitations": "Focuses primarily on specific types of interactions and may not account for all contexts of sycophancy.", "keywords": ["social sycophancy", "LLMs", "face preservation", "ELEPHANT framework", "user interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14009", "pdf": "https://arxiv.org/pdf/2505.14009.pdf", "abs": "https://arxiv.org/abs/2505.14009", "title": "Activation-Guided Consensus Merging for Large Language Models", "authors": ["Yuxuan Yao", "Shuqi Liu", "Zehua Liu", "Qintong Li", "Mingyang Liu", "Xiongwei Han", "Zhijiang Guo", "Han Wu", "Linqi Song"], "categories": ["cs.CL"], "comment": null, "summary": "Recent research has increasingly focused on reconciling the reasoning\ncapabilities of System 2 with the efficiency of System 1. While existing\ntraining-based and prompt-based approaches face significant challenges in terms\nof efficiency and stability, model merging emerges as a promising strategy to\nintegrate the diverse capabilities of different Large Language Models (LLMs)\ninto a unified model. However, conventional model merging methods often assume\nuniform importance across layers, overlooking the functional heterogeneity\ninherent in neural components. To address this limitation, we propose\n\\textbf{A}ctivation-Guided \\textbf{C}onsensus \\textbf{M}erging (\\textbf{ACM}),\na plug-and-play merging framework that determines layer-specific merging\ncoefficients based on mutual information between activations of pre-trained and\nfine-tuned models. ACM effectively preserves task-specific capabilities without\nrequiring gradient computations or additional training. Extensive experiments\non Long-to-Short (L2S) and general merging tasks demonstrate that ACM\nconsistently outperforms all baseline methods. For instance, in the case of\nQwen-7B models, TIES-Merging equipped with ACM achieves a \\textbf{55.3\\%}\nreduction in response length while simultaneously improving reasoning accuracy\nby \\textbf{1.3} points. We submit the code with the paper for reproducibility,\nand it will be publicly available.", "AI": {"tldr": "A novel model merging framework called Activation-Guided Consensus Merging (ACM) is proposed to effectively integrate various Large Language Models (LLMs) by using layer-specific merging coefficients based on mutual information, leading to enhanced task-specific capabilities with improved efficiency.", "motivation": "To reconcile System 2 reasoning capabilities with System 1 efficiency while addressing the limitations of existing training-based and prompt-based approaches in model merging.", "method": "ACM utilizes a plug-and-play merging framework that calculates layer-specific merging coefficients based on the mutual information of activations from pre-trained and fine-tuned models, avoiding the need for gradient computations or additional training.", "result": "Extensive experiments indicate that ACM consistently surpasses baseline methods. For example, TIES-Merging equipped with ACM achieves a 55.3% reduction in response length and a 1.3 point improvement in reasoning accuracy for Qwen-7B models.", "conclusion": "ACM effectively retains task-specific performance while enhancing efficiency in merging different LLM capabilities, making it an attractive option for improving large model integrations without complex additional training.", "key_contributions": ["Introduction of Activation-Guided Consensus Merging (ACM) framework for model merging", "Layer-specific merging coefficients based on mutual information", "Publicly available code for reproducibility"], "limitations": "", "keywords": ["Model Merging", "Large Language Models", "Activation-Guided Consensus Merging"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.14015", "pdf": "https://arxiv.org/pdf/2505.14015.pdf", "abs": "https://arxiv.org/abs/2505.14015", "title": "AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation", "authors": ["Tai D. Nguyen", "Long H. Pham", "Jun Sun"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of domain-specific large language models (LLMs) in\nfields like law necessitates frameworks that account for nuanced regional legal\ndistinctions, which are critical for ensuring compliance and trustworthiness.\nExisting legal evaluation benchmarks often lack adaptability and fail to\naddress diverse local contexts, limiting their utility in dynamically evolving\nregulatory landscapes. To address these gaps, we propose AutoLaw, a novel\nviolation detection framework that combines adversarial data generation with a\njury-inspired deliberation process to enhance legal compliance of LLMs. Unlike\nstatic approaches, AutoLaw dynamically synthesizes case law to reflect local\nregulations and employs a pool of LLM-based \"jurors\" to simulate judicial\ndecision-making. Jurors are ranked and selected based on synthesized legal\nexpertise, enabling a deliberation process that minimizes bias and improves\ndetection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG\n(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:\nadversarial data generation improves LLM discrimination, while the jury-based\nvoting strategy significantly boosts violation detection rates. Our results\nhighlight the framework's ability to adaptively probe legal misalignments and\ndeliver reliable, context-aware judgments, offering a scalable solution for\nevaluating and enhancing LLMs in legally sensitive applications.", "AI": {"tldr": "AutoLaw is a violation detection framework for LLMs that combines adversarial data generation and a jury-inspired process to enhance legal compliance, adapting to local regulations.", "motivation": "The need for frameworks that ensure legal compliance in domain-specific LLMs due to diverse regional legal distinctions and limitations of existing evaluation benchmarks.", "method": "AutoLaw utilizes adversarial data generation and a jury-inspired deliberation process involving LLM-based jurors to simulate judicial decision-making and dynamically reflect case law.", "result": "AutoLaw shows improved detection accuracy and compliance in LLMs, with effective evaluations on legal benchmarks demonstrating enhanced discrimination and violation detection rates.", "conclusion": "The framework offers a scalable solution for augmenting LLMs in legally sensitive contexts, capable of adapting to evolving regulatory landscapes.", "key_contributions": ["Dynamic synthesis of case law reflecting local regulations", "Implementation of a jury-inspired deliberation process", "Enhanced violation detection rates through adversarial data generation"], "limitations": "", "keywords": ["large language models", "legal compliance", "adversarial data generation", "jury deliberation", "law evaluation"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.14045", "pdf": "https://arxiv.org/pdf/2505.14045.pdf", "abs": "https://arxiv.org/abs/2505.14045", "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora", "authors": ["Yingli Shen", "Wen Lai", "Shuo Wang", "Kangyang Luo", "Alexander Fraser", "Maosong Sun"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data.", "AI": {"tldr": "Introduction of a large-scale multi-way parallel corpus, TED2025, to improve multilingual performance in large language models (LLMs).", "motivation": "To address the limitations of unaligned multilingual data in capturing cross-lingual semantics for enhancing LLMs' performance in low-resource languages.", "method": "Presentation and analysis of a multi-way parallel dataset based on TED Talks covering 113 languages, investigation of pretraining and tuning strategies using this dataset.", "result": "Models trained on the TED2025 multi-way parallel data consistently outperform those trained on unaligned multilingual data across six multilingual benchmarks.", "conclusion": "The use of multi-way parallel corpora can significantly enhance the capabilities of LLMs in multilingual applications, providing stronger consistency and improving performance.", "key_contributions": ["Introduction of the TED2025 corpus with comprehensive multilingual coverage", "Demonstration of outperforming results of models trained on multi-way parallel data over unaligned data", "Identification of effective strategies for leveraging multi-way parallel data in model training."], "limitations": "", "keywords": ["multilingual data", "large language models", "cross-lingual semantics", "TED Talks", "multilingual performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14052", "pdf": "https://arxiv.org/pdf/2505.14052.pdf", "abs": "https://arxiv.org/abs/2505.14052", "title": "Improved Methods for Model Pruning and Knowledge Distillation", "authors": ["Wei Jiang", "Anying Fu", "Youling Zhang"], "categories": ["cs.CL", "cs.CE"], "comment": null, "summary": "Model pruning is a performance optimization technique for large language\nmodels like R1 or o3-mini. However, existing pruning methods often lead to\nsignificant performance degradation or require extensive retraining and\nfine-tuning. This technique aims to identify and remove neurons, connections\nunlikely leading to the contribution during the human-computer interaction\nphase. Our goal is to obtain a much smaller and faster knowledge distilled\nmodel that can quickly generate content almost as good as those of the unpruned\nones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an\nimproved pruning method that effectively reduces model size and computational\ncomplexity while maintaining performance comparable to the original unpruned\nmodel even at extreme pruned levels. The improved method is based on weights,\nbias fixed in the pre-training phase and GRPO rewards verified during the\npost-training phase as our novel pruning indicators. Preliminary experimental\nresults show that our method outperforms and be comparable to state-of-the-art\nmethods across various pruning levels and different downstream computational\nlinguistics tasks.", "AI": {"tldr": "The paper proposes MAMA Pruning, a novel pruning technique for large language models that minimizes performance loss while achieving significant model size reduction.", "motivation": "Existing pruning methods for large language models often degrade performance or require extensive retraining. There is a need for a method that effectively prunes models without compromising their generated content quality, especially in human-computer interaction contexts.", "method": "MAMA Pruning utilizes Movement and Magnitude Analysis to identify and remove less significant neurons and connections in the model, leveraging weights and biases during pre-training and using GRPO rewards verified in post-training as pruning indicators.", "result": "Preliminary results indicate that MAMA Pruning outperforms state-of-the-art methods across various pruning levels and NLP tasks, maintaining performance comparable to unpruned models.", "conclusion": "MAMA Pruning presents an effective solution for reducing the size and complexity of language models while preserving their performance, making it a viable option for applications in human-computer interaction and other computational linguistics tasks.", "key_contributions": ["Introduction of MAMA Pruning technique for improved model efficiency", "Demonstrated effectiveness across various pruning levels", "Performance maintenance comparable to original models even at high pruning levels"], "limitations": "", "keywords": ["Model Pruning", "Human-Computer Interaction", "Language Models", "Computational Linguistics", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14070", "pdf": "https://arxiv.org/pdf/2505.14070.pdf", "abs": "https://arxiv.org/abs/2505.14070", "title": "Enhancing LLMs via High-Knowledge Data Selection", "authors": ["Feiyu Duan", "Xuemiao Zhang", "Sirui Wang", "Haoran Que", "Yuqi Liu", "Wenge Rong", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "The performance of Large Language Models (LLMs) is intrinsically linked to\nthe quality of its training data. Although several studies have proposed\nmethods for high-quality data selection, they do not consider the importance of\nknowledge richness in text corpora. In this paper, we propose a novel and\ngradient-free High-Knowledge Scorer (HKS) to select high-quality data from the\ndimension of knowledge, to alleviate the problem of knowledge scarcity in the\npre-trained corpus. We propose a comprehensive multi-domain knowledge element\npool and introduce knowledge density and coverage as metrics to assess the\nknowledge content of the text. Based on this, we propose a comprehensive\nknowledge scorer to select data with intensive knowledge, which can also be\nutilized for domain-specific high-knowledge data selection by restricting\nknowledge elements to the specific domain. We train models on a high-knowledge\nbilingual dataset, and experimental results demonstrate that our scorer\nimproves the model's performance in knowledge-intensive and general\ncomprehension tasks, and is effective in enhancing both the generic and\ndomain-specific capabilities of the model.", "AI": {"tldr": "Proposes a High-Knowledge Scorer (HKS) for selecting high-quality training data based on knowledge richness, improving LLM performance.", "motivation": "To address knowledge scarcity in pre-trained corpora used by Large Language Models (LLMs).", "method": "Introduces a High-Knowledge Scorer (HKS) that evaluates data quality based on knowledge density and coverage metrics, with an emphasis on selecting domain-specific high-knowledge data.", "result": "Demonstrates that using the HKS significantly improves performance on knowledge-intensive and general comprehension tasks in LLMs.", "conclusion": "The proposed HKS is effective for enhancing both general and domain-specific capabilities of LLMs, contributing to better performance by utilizing knowledge-rich training data.", "key_contributions": ["Development of a gradient-free High-Knowledge Scorer (HKS) for training data selection.", "Introduction of knowledge density and coverage as metrics for evaluating knowledge content.", "Success in using HKS on a bilingual dataset to improve model performance."], "limitations": "", "keywords": ["Large Language Models", "knowledge scoring", "data selection", "bilingual dataset", "domain-specific training"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.14079", "pdf": "https://arxiv.org/pdf/2505.14079.pdf", "abs": "https://arxiv.org/abs/2505.14079", "title": "BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks", "authors": ["Weihong Du", "Wenrui Liao", "Binyu Yan", "Hongru Liang", "Anthony G. Cohn", "Wenqiang Lei"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) based agents have shown great potential in\nfollowing human instructions and automatically completing various tasks. To\ncomplete a task, the agent needs to decompose it into easily executed steps by\nplanning. Existing studies mainly conduct the planning by inferring what steps\nshould be executed next starting from the agent's initial state. However, this\nforward reasoning paradigm doesn't work well for complex tasks. We propose to\nstudy this issue in Minecraft, a virtual environment that simulates complex\ntasks based on real-world scenarios. We believe that the failure of forward\nreasoning is caused by the big perception gap between the agent's initial state\nand task goal. To this end, we leverage backward reasoning and make the\nplanning starting from the terminal state, which can directly achieve the task\ngoal in one step. Specifically, we design a BAckward Reasoning based agent\n(BAR). It is equipped with a recursive goal decomposition module, a state\nconsistency maintaining module and a stage memory module to make robust,\nconsistent, and efficient planning starting from the terminal state.\nExperimental results demonstrate the superiority of BAR over existing methods\nand the effectiveness of proposed modules.", "AI": {"tldr": "This paper presents a BAckward Reasoning based agent (BAR) that improves task planning in complex environments like Minecraft by using backward reasoning to achieve task goals directly from terminal states.", "motivation": "The paper aims to address the limitations of forward reasoning in LLM-based agents when handling complex tasks with significant perception gaps between initial states and goals.", "method": "The authors introduce BAR, which employs a recursive goal decomposition module, a state consistency maintaining module, and a stage memory module to facilitate efficient planning from terminal states.", "result": "Experimental results indicate that BAR outperforms existing planning methods in terms of robustness, consistency, and efficiency.", "conclusion": "The study demonstrates that backward reasoning is a viable approach for enhancing task completion in complex environments, leading to improved agent performance.", "key_contributions": ["Introduction of BAR agent leveraging backward reasoning", "Development of modules for goal decomposition and state consistency", "Empirical evidence of improved performance over existing methods"], "limitations": "", "keywords": ["backward reasoning", "language model agents", "task planning", "Minecraft", "goal decomposition"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.14080", "pdf": "https://arxiv.org/pdf/2505.14080.pdf", "abs": "https://arxiv.org/abs/2505.14080", "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory", "authors": ["Franziska Sofia Hafner", "Ana Valdivia", "Luc Rocher"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed.", "AI": {"tldr": "This paper explores how language models perpetuate harmful gendered stereotypes and biases, advocating for a broader definition of gender bias in AI to better address these issues.", "motivation": "To address the inadequate understanding of gender bias in language models, which often erases transgender and gender diverse identities, leading to real-world harms.", "method": "The paper empirically tests 16 language models of varying architectures and training datasets to examine how they encode gender by operationalizing insights from gender studies.", "result": "It finds that these models predominantly encode gender as a binary tied to biological sex and that larger models worsen these biases by reinforcing narrow gender understandings.", "conclusion": "The research calls for a re-evaluation of definitions and methodologies used to address gendered harms in language models.", "key_contributions": ["Broadens the definition of gender bias in language models.", "Empirically tests diverse language models to assess their encoding of gender.", "Highlights how larger models reinforce binary gender associations."], "limitations": "The study primarily focuses on model architecture and performance without addressing the societal impacts of these biases extensively.", "keywords": ["gender bias", "language models", "gendered stereotypes", "gender studies", "model architecture"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.14099", "pdf": "https://arxiv.org/pdf/2505.14099.pdf", "abs": "https://arxiv.org/abs/2505.14099", "title": "Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering", "authors": ["Yihua Zhu", "Qianying Liu", "Akiko Aizawa", "Hidetoshi Shimodaira"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions using structured knowledge from KBs. While LLM-only approaches offer\ngeneralization, they suffer from outdated knowledge, hallucinations, and lack\nof transparency. Chain-based KG-RAG methods address these issues by\nincorporating external KBs, but are limited to simple chain-structured\nquestions due to the absence of planning and logical structuring. Inspired by\nsemantic parsing methods, we propose PDRR: a four-stage framework consisting of\nPredict, Decompose, Retrieve, and Reason. Our method first predicts the\nquestion type and decomposes the question into structured triples. Then\nretrieves relevant information from KBs and guides the LLM as an agent to\nreason over and complete the decomposed triples. Experimental results\ndemonstrate that PDRR consistently outperforms existing methods across various\nLLM backbones and achieves superior performance on both chain-structured and\nnon-chain complex questions.", "AI": {"tldr": "PDRR is a novel four-stage framework for Knowledge Base Question Answering that improves performance on both simple and complex questions by predicting question types, decomposing questions, retrieving information from knowledge bases, and using LLMs for reasoning.", "motivation": "Existing KBQA methods struggle with hallucinations and outdated knowledge, and chain-based approaches are limited in handling complex question structures. A more structured and logical method is needed.", "method": "PDRR consists of four stages: Predict the question type, Decompose the question into structured triples, Retrieve relevant information from knowledge bases, and Reason using LLMs to complete the triples.", "result": "Experimental results show that PDRR outperforms existing KBQA methods across various LLMs, handling both chain-structured and non-chain complex questions effectively.", "conclusion": "PDRR offers a systematic approach to enhance KBQA, addressing the limitations of prior methods and achieving superior performance.", "key_contributions": ["Introduction of PDRR framework for KBQA", "Successful integration of structured knowledge retrieval and LLM reasoning", "Enhanced performance on both simple and complex question types"], "limitations": "", "keywords": ["Knowledge Base Question Answering", "LLM", "semantic parsing", "information retrieval", "reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14101", "pdf": "https://arxiv.org/pdf/2505.14101.pdf", "abs": "https://arxiv.org/abs/2505.14101", "title": "MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations", "authors": ["Ernests Lavrinovics", "Russa Biswas", "Katja Hose", "Johannes Bjerva"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have inherent limitations of faithfulness and\nfactuality, commonly referred to as hallucinations. Several benchmarks have\nbeen developed that provide a test bed for factuality evaluation within the\ncontext of English-centric datasets, while relying on supplementary informative\ncontext like web links or text passages but ignoring the available structured\nfactual resources. To this end, Knowledge Graphs (KGs) have been identified as\na useful aid for hallucination mitigation, as they provide a structured way to\nrepresent the facts about entities and their relations with minimal linguistic\noverhead. We bridge the lack of KG paths and multilinguality for factual\nlanguage modeling within the existing hallucination evaluation benchmarks and\npropose a KG-based multilingual, multihop benchmark called \\textbf{MultiHal}\nframed for generative text evaluation. As part of our data collection pipeline,\nwe mined 140k KG-paths from open-domain KGs, from which we pruned noisy\nKG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation\nshows an absolute scale increase by approximately 0.12 to 0.36 points for the\nsemantic similarity score in KG-RAG over vanilla QA across multiple languages\nand multiple models, demonstrating the potential of KG integration. We\nanticipate MultiHal will foster future research towards several graph-based\nhallucination mitigation and fact-checking tasks.", "AI": {"tldr": "The paper introduces MultiHal, a multilingual, multihop benchmark aimed at improving factual correctness in generative text models using Knowledge Graphs (KGs) for hallucination mitigation.", "motivation": "To address the limitations in existing benchmarks for factuality evaluation in language models, particularly their lack of structured factual resources like Knowledge Graphs.", "method": "The authors propose MultiHal, a KG-based multilingual multihop benchmark, by mining and curating 140k KG-paths from open-domain KGs and evaluating the integration of KGs in generative text models.", "result": "Baseline evaluations showed a semantic similarity score increase of 0.12 to 0.36 points for KG-RAG compared to vanilla QA across different languages and models, indicating the effectiveness of KG integration.", "conclusion": "The introduction of MultiHal is expected to advance research in graph-based methods for hallucination mitigation and fact-checking tasks.", "key_contributions": ["Proposed a new multilingual, multihop benchmark named MultiHal for hallucination evaluation.", "Demonstrated the effectiveness of integrating Knowledge Graphs into language model evaluations.", "Curated a high-quality subset of KG-paths from mined data to enhance the benchmarking process."], "limitations": "", "keywords": ["Large Language Models", "Knowledge Graphs", "Factuality Evaluation", "Hallucination Mitigation", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14104", "pdf": "https://arxiv.org/pdf/2505.14104.pdf", "abs": "https://arxiv.org/abs/2505.14104", "title": "Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents", "authors": ["Wei Fan", "Tianshi Zheng", "Yiran Hu", "Zheye Deng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Haoran Li", "Weixing Shen", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Legal rules encompass not only codified statutes but also implicit\nadjudicatory principles derived from precedents that contain discretionary\nnorms, social morality, and policy. While computational legal research has\nadvanced in applying established rules to cases, inducing legal rules from\njudicial decisions remains understudied, constrained by limitations in model\ninference efficacy and symbolic reasoning capability. The advent of Large\nLanguage Models (LLMs) offers unprecedented opportunities for automating the\nextraction of such latent principles, yet progress is stymied by the absence of\nformal task definitions, benchmark datasets, and methodologies. To address this\ngap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,\ngeneralizable doctrinal rules from sets of analogous precedents, distilling\ntheir shared preconditions, normative behaviors, and legal consequences. We\nintroduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese\ncases in total) for model tuning and 216 expert-annotated gold test sets.\nExperimental results reveal that: 1) State-of-the-art LLMs struggle with\nover-generalization and hallucination; 2) Training on our dataset markedly\nenhances LLMs capabilities in capturing nuanced rule patterns across similar\ncases.", "AI": {"tldr": "The paper introduces Legal Rule Induction (LRI) as a task for automating the extraction of legal rules from judicial precedents using LLMs, and presents the first benchmark for this task.", "motivation": "To improve the automation and understanding of legal rules derived from judicial decisions, addressing the limitations of current computational legal research.", "method": "The authors formalize the task of Legal Rule Induction, create a benchmark dataset comprising 5,121 case sets and 216 expert-annotated test sets, and evaluate state-of-the-art LLMs on their ability to derive legal rules from these cases.", "result": "Experiments show that existing LLMs face challenges like over-generalization and hallucination, but performance improves significantly when trained on the proposed benchmark dataset.", "conclusion": "The study highlights the potential of LLMs in legal rule induction while identifying current limitations and providing a foundation for future research in this area.", "key_contributions": ["Introduction of Legal Rule Induction (LRI) as a formal task.", "Creation of the first benchmark dataset for LRI.", "Demonstration of LLM performance issues and improvements with specific training."], "limitations": "Current LLMs tend to over-generalize and hallucinate when extracting legal rules.", "keywords": ["Legal Rule Induction", "Large Language Models", "judicial precedents", "benchmark datasets", "automated legal research"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.14106", "pdf": "https://arxiv.org/pdf/2505.14106.pdf", "abs": "https://arxiv.org/abs/2505.14106", "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations", "authors": ["Li Li", "Peilin Cai", "Ryan A. Rossi", "Franck Dernoncourt", "Branislav Kveton", "Junda Wu", "Tong Yu", "Linxin Song", "Tiankai Yang", "Yuehan Qin", "Nesreen K. Ahmed", "Samyadeep Basu", "Subhojyoti Mukherjee", "Ruiyi Zhang", "Zhengmian Hu", "Bo Ni", "Yuxiao Zhou", "Zichao Wang", "Yue Huang", "Yu Wang", "Xiangliang Zhang", "Philip S. Yu", "Xiyang Hu", "Yue Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present PersonaConvBench, a large-scale benchmark for evaluating\npersonalized reasoning and generation in multi-turn conversations with large\nlanguage models (LLMs). Unlike existing work that focuses on either\npersonalization or conversational structure in isolation, PersonaConvBench\nintegrates both, offering three core tasks: sentence classification, impact\nregression, and user-centric text generation across ten diverse Reddit-based\ndomains. This design enables systematic analysis of how personalized\nconversational context shapes LLM outputs in realistic multi-user scenarios. We\nbenchmark several commercial and open-source LLMs under a unified prompting\nsetup and observe that incorporating personalized history yields substantial\nperformance improvements, including a 198 percent relative gain over the best\nnon-conversational baseline in sentiment classification. By releasing\nPersonaConvBench with evaluations and code, we aim to support research on LLMs\nthat adapt to individual styles, track long-term context, and produce\ncontextually rich, engaging responses.", "AI": {"tldr": "PersonaConvBench is a benchmark for evaluating personalized reasoning and generation in multi-turn conversations with LLMs.", "motivation": "To systematically analyze how personalized conversational context influences outputs of LLMs in realistic multi-user scenarios.", "method": "The paper presents a benchmark framework integrating sentence classification, impact regression, and user-centric text generation tasks using a unified prompting setup.", "result": "Benchmarking shows substantial performance improvements for LLMs when incorporating personalized history, with a 198% relative gain in sentiment classification compared to non-conversational baselines.", "conclusion": "The release of PersonaConvBench aims to facilitate research on LLMs that adapt to personal styles and produce rich, engaging responses based on long-term context.", "key_contributions": ["Introduction of a large-scale benchmark for personalized conversation evaluation", "Integration of conversational structure and personalization in multi-turn dialogues", "Demonstrated substantial performance gains in LLMs with personalized context"], "limitations": "", "keywords": ["personalized reasoning", "multi-turn conversations", "large language models", "benchmark", "user-centric text generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14107", "pdf": "https://arxiv.org/pdf/2505.14107.pdf", "abs": "https://arxiv.org/abs/2505.14107", "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models", "authors": ["Yakun Zhu", "Zhongzhen Huang", "Linjie Mu", "Yutong Huang", "Wei Nie", "Shaoting Zhang", "Pengfei Liu", "Xiaofan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena.", "AI": {"tldr": "DiagnosisArena is a new benchmark created to evaluate the diagnostic capabilities of large language models in clinical settings.", "motivation": "To assess the diagnostic capabilities of large language models in healthcare, as current benchmarks are inadequate for advanced diagnostic reasoning.", "method": "DiagnosisArena encompasses 1,113 segmented patient cases and diagnoses across 28 medical specialties, developed through iterative screening and review by AI and human experts.", "result": "The study found that leading models (o3-mini, o1, and DeepSeek-R1) achieved low diagnostic accuracy of 45.82%, 31.09%, and 17.79% respectively, indicating a significant limitation in their performance in clinical environments.", "conclusion": "DiagnosisArena aims to enhance the diagnostic reasoning capabilities of AI in healthcare, providing tools for ongoing research and improvement in clinical diagnostics.", "key_contributions": ["Introduction of DiagnosisArena as a benchmark for AI diagnostic assessment.", "Detailed construction pipeline with expert reviews to ensure benchmark integrity.", "Revelation of significant accuracy gaps in current AI models regarding clinical diagnostics."], "limitations": "The benchmark may not cover all clinical scenarios or specialties comprehensively.", "keywords": ["Large Language Models", "Diagnostic Reasoning", "Healthcare AI", "Benchmarking", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14112", "pdf": "https://arxiv.org/pdf/2505.14112.pdf", "abs": "https://arxiv.org/abs/2505.14112", "title": "Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking", "authors": ["Tianle Gu", "Zongqi Wang", "Kexin Huang", "Yuanqi Yao", "Xiangliang Zhang", "Yujiu Yang", "Xiuying Chen"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Logit-based LLM watermarking traces and verifies AI-generated content by\nmaintaining green and red token lists and increasing the likelihood of green\ntokens during generation. However, it fails in low-entropy scenarios, where\npredictable outputs make green token selection difficult without disrupting\nnatural text flow. Existing approaches address this by assuming access to the\noriginal LLM to calculate entropy and selectively watermark high-entropy\ntokens. However, these methods face two major challenges: (1) high\ncomputational costs and detection delays due to reliance on the original LLM,\nand (2) potential risks of model leakage. To address these limitations, we\npropose Invisible Entropy (IE), a watermarking paradigm designed to enhance\nboth safety and efficiency. Instead of relying on the original LLM, IE\nintroduces a lightweight feature extractor and an entropy tagger to predict\nwhether the entropy of the next token is high or low. Furthermore, based on\ntheoretical analysis, we develop a threshold navigator that adaptively sets\nentropy thresholds. It identifies a threshold where the watermark ratio\ndecreases as the green token count increases, enhancing the naturalness of the\nwatermarked text and improving detection robustness. Experiments on HumanEval\nand MBPP datasets demonstrate that IE reduces parameter size by 99\\% while\nachieving performance on par with state-of-the-art methods. Our work introduces\na safe and efficient paradigm for low-entropy watermarking.\nhttps://github.com/Carol-gutianle/IE\nhttps://huggingface.co/datasets/Carol0110/IE-Tagger", "AI": {"tldr": "Proposes Invisible Entropy (IE), a watermarking method improving safety and efficiency in low-entropy scenarios, by using a lightweight feature extractor and adaptive thresholds.", "motivation": "To enhance the watermarking of AI-generated content in low-entropy conditions without disrupting natural text flow while addressing high computational costs and risks of model leakage in existing methods.", "method": "Introduces a lightweight feature extractor and an entropy tagger to predict token entropy, along with a threshold navigator to adaptively set entropy thresholds for watermarking.", "result": "Experiments show IE reduces parameter size by 99% and matches the performance of state-of-the-art watermarking methods on HumanEval and MBPP datasets.", "conclusion": "IE offers a new paradigm for low-entropy watermarking that is both safe and efficient, enhancing detection robustness while maintaining text naturalness.", "key_contributions": ["Introduction of the Invisible Entropy watermarking paradigm", "Use of a lightweight feature extractor and entropy tagger", "Development of an adaptive threshold navigator for embedding watermarks"], "limitations": "", "keywords": ["watermarking", "entropy", "AI-generated content", "low-entropy", "feature extractor"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.14116", "pdf": "https://arxiv.org/pdf/2505.14116.pdf", "abs": "https://arxiv.org/abs/2505.14116", "title": "Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst", "authors": ["Hongru Wang", "Deng Cai", "Wanjun Zhong", "Shijue Huang", "Jeff Z. Pan", "Zeming Liu", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Inference-time scaling has attracted much attention which significantly\nenhance the performance of Large Language Models (LLMs) in complex reasoning\ntasks by increasing the length of Chain-of-Thought. These longer intermediate\nreasoning rationales embody various meta-reasoning skills in human cognition,\nsuch as reflection and decomposition, being difficult to create and acquire. In\nthis work, we introduce \\textit{Self-Reasoning Language Model} (SRLM), where\nthe model itself can synthesize longer CoT data and iteratively improve\nperformance through self-training. By incorporating a few demonstration\nexamples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from\nexisting responses, which act as a reasoning catalyst, we demonstrate that SRLM\nnot only enhances the model's initial performance but also ensures more stable\nand consistent improvements in subsequent iterations. Our proposed SRLM\nachieves an average absolute improvement of more than $+2.5$ points across five\nreasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.\nMoreover, it brings more improvements with more times of sampling during\ninference, such as absolute $+7.89$ average improvement with $64$ sampling\ntimes, revealing the in-depth, diverse and creative reasoning paths in SRLM\nagainst the strong baseline.", "AI": {"tldr": "The paper presents the Self-Reasoning Language Model (SRLM), which enhances Large Language Models' performance in complex reasoning tasks through self-generated Chain-of-Thought data and iterative self-training.", "motivation": "There is a growing interest in inference-time scaling to improve the reasoning capabilities of Large Language Models by extending their Chain-of-Thought length, which mirrors human cognitive skills.", "method": "SRLM synthesizes longer Chain-of-Thought data and improves performance iteratively using a few demonstration examples that aid in unfolding hidden reasoning chains.", "result": "SRLM shows an average absolute improvement of over +2.5 points across five reasoning tasks and up to +7.89 points with increased sampling times, demonstrating diverse reasoning paths.", "conclusion": "The Self-Reasoning Language Model not only enhances initial performance but also stabilizes and enhances long-term reasoning improvements through self-training techniques.", "key_contributions": ["Introduction of Self-Reasoning Language Model (SRLM) for enhancing reasoning performance.", "Demonstration of significant average absolute improvements across multiple reasoning tasks.", "Methodology for iteratively training LLMs with self-generated reasoning chains."], "limitations": "", "keywords": ["Self-Reasoning Language Model", "Chain-of-Thought", "Machine Learning", "Reasoning Tasks", "Self-Training"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.14130", "pdf": "https://arxiv.org/pdf/2505.14130.pdf", "abs": "https://arxiv.org/abs/2505.14130", "title": "Probing BERT for German Compound Semantics", "authors": ["Filip Miletić", "Aaron Schmid", "Sabine Schulte im Walde"], "categories": ["cs.CL"], "comment": "Accepted to SwissText 2025", "summary": "This paper investigates the extent to which pretrained German BERT encodes\nknowledge of noun compound semantics. We comprehensively vary combinations of\ntarget tokens, layers, and cased vs. uncased models, and evaluate them by\npredicting the compositionality of 868 gold standard compounds. Looking at\nrepresentational patterns within the transformer architecture, we observe\ntrends comparable to equivalent prior work on English, with compositionality\ninformation most easily recoverable in the early layers. However, our strongest\nresults clearly lag behind those reported for English, suggesting an inherently\nmore difficult task in German. This may be due to the higher productivity of\ncompounding in German than in English and the associated increase in\nconstituent-level ambiguity, including in our target compound set.", "AI": {"tldr": "This paper explores how well pretrained German BERT encodes knowledge of noun compound semantics, finding that while representational patterns show similarities to English, performance lags due to German's more complex compounding.", "motivation": "To investigate the capability of pretrained German BERT models in understanding the semantics of noun compounds, given the challenges presented by the German language's compounding system.", "method": "The study varies combinations of target tokens, layers, and cased vs. uncased models, evaluating them on their predictive power regarding the compositionality of 868 gold standard noun compounds.", "result": "The findings indicate that compositionality information is most recoverable in the early layers of the model, but the performance is significantly below that of comparable English models.", "conclusion": "The results suggest that the task of understanding noun compounding in German is inherently more challenging than in English, likely due to the higher productivity of compound formation and greater constituent-level ambiguity in German.", "key_contributions": ["Identification of representational patterns in German BERT for noun compounds", "Comparison of German BERT performance with English counterparts", "Insights into the challenges of processing noun compounds in German language."], "limitations": "The study primarily focuses on noun compounds and may not generalize to other semantic structures; the performance gap with English needs further investigation.", "keywords": ["German BERT", "noun compounds", "compositionality", "transformer architecture", "semantic encoding"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.14131", "pdf": "https://arxiv.org/pdf/2505.14131.pdf", "abs": "https://arxiv.org/abs/2505.14131", "title": "Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering", "authors": ["Wei Zhou", "Mohsen Mesgar", "Heike Adel", "Annemarie Friedrich"], "categories": ["cs.CL"], "comment": "Accepted at ACL25 (Findings)", "summary": "In table question answering (TQA), tables are encoded as either texts or\nimages. Prior work suggests that passing images of tables to multi-modal large\nlanguage models (MLLMs) performs comparably to or even better than using\ntextual input with large language models (LLMs). However, the lack of\ncontrolled setups limits fine-grained distinctions between these approaches. In\nthis paper, we conduct the first controlled study on the effectiveness of\nseveral combinations of table representations and models from two perspectives:\nquestion complexity and table size. We build a new benchmark based on existing\nTQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we\nfind that the best combination of table representation and model varies across\nsetups. We propose FRES, a method selecting table representations dynamically,\nand observe a 10% average performance improvement compared to using both\nrepresentations indiscriminately.", "AI": {"tldr": "A study comparing table representations in TQA shows dynamic selection improves performance.", "motivation": "To investigate the effectiveness of different table representations in table question answering (TQA) using a controlled setup.", "method": "A controlled study analyzing combinations of table representations and models based on question complexity and table size, with the introduction of a new benchmark for TQA.", "result": "The best table representation and model combination varies across conditions; dynamic selection method FRES shows a 10% performance improvement.", "conclusion": "Dynamic selection of table representations enhances TQA performance, thus providing insights for future applications.", "key_contributions": ["First controlled study on table representations in TQA", "New benchmark for TQA based on existing datasets", "Introduction of FRES for dynamic representation selection"], "limitations": "", "keywords": ["Table Question Answering", "Multi-modal Models", "Dynamic Representation Selection"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.14149", "pdf": "https://arxiv.org/pdf/2505.14149.pdf", "abs": "https://arxiv.org/abs/2505.14149", "title": "Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information", "authors": ["Chengzhi Zhang", "Xinyi Yan", "Lei Zhao", "Yingyi Zhang"], "categories": ["cs.CL", "cs.DL", "cs.IR"], "comment": null, "summary": "The exponential increase in academic papers has significantly increased the\ntime required for researchers to access relevant literature. Keyphrase\nExtraction (KPE) offers a solution to this situation by enabling researchers to\nefficiently retrieve relevant literature. The current study on KPE from\nacademic articles aims to improve the performance of extraction models through\ninnovative approaches using Title and Abstract as input corpora. However, the\nsemantic richness of keywords is significantly constrained by the length of the\nabstract. While full-text-based KPE can address this issue, it simultaneously\nintroduces noise, which significantly diminishes KPE performance. To address\nthis issue, this paper utilized the structural features and section texts\nobtained from the section structure information of academic articles to extract\nkeyphrase from academic papers. The approach consists of two main parts: (1)\nexploring the effect of seven structural features on KPE models, and (2)\nintegrating the extraction results from all section texts used as input corpora\nfor KPE models via a keyphrase integration algorithm to obtain the keyphrase\nintegration result. Furthermore, this paper also examined the effect of the\nclassification quality of section structure on the KPE performance. The results\nshow that incorporating structural features improves KPE performance, though\ndifferent features have varying effects on model efficacy. The keyphrase\nintegration approach yields the best performance, and the classification\nquality of section structure can affect KPE performance. These findings\nindicate that using the section structure information of academic articles\ncontributes to effective KPE from academic articles. The code and dataset\nsupporting this study are available at https://github.com/yan-xinyi/SSB_KPE.", "AI": {"tldr": "This study enhances Keyphrase Extraction (KPE) from academic papers by incorporating section structure features and integrating extraction results from multiple sections, leading to improved model performance.", "motivation": "The growing volume of academic literature makes it increasingly difficult for researchers to access relevant papers; thus, an improved KPE method is essential.", "method": "The study investigates seven structural features of academic articles, tests their impact on KPE models, and employs a keyphrase integration algorithm to consolidate extraction results from various sections of the papers.", "result": "Incorporating structural features significantly boosts KPE performance, with the keyphrase integration approach achieving the highest efficacy; the quality of classification of section structures also affects KPE outcomes.", "conclusion": "Utilizing the structural features and information of academic article sections enhances KPE, suggesting a new direction for developing effective literature retrieval tools.", "key_contributions": ["Introduces structural features for KPE", "Presents a keyphrase integration algorithm", "Demonstrates the impact of section classification on KPE performance"], "limitations": "The study may be limited by the chosen structural features and the generalizability of the findings across different types of academic articles.", "keywords": ["Keyphrase Extraction", "structural features", "academic papers", "machine learning", "literature retrieval"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2505.14157", "pdf": "https://arxiv.org/pdf/2505.14157.pdf", "abs": "https://arxiv.org/abs/2505.14157", "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning", "authors": ["Pittawat Taveekitworachai", "Potsawee Manakul", "Sarana Nutanong", "Kunat Pipatanakul"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 42 figures", "summary": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.", "AI": {"tldr": "The paper examines prior prompt engineering (pPE) in reinforcement fine-tuning (RFT) of language models (LMs), showing that various pPE strategies significantly improve model performance and instill distinct behaviors compared to traditional methods.", "motivation": "To explore the under-researched area of prior prompt engineering in relation to reinforcement fine-tuning of language models, which remains largely overlooked compared to other RFT components.", "method": "The authors translate five inference-time prompt engineering strategies into prior prompt engineering approaches and test them on the Qwen2.5-7B model across various benchmarks including AIME2024, HumanEval+, and GPQA-Diamond.", "result": "All models trained with prior prompt engineering outperformed their inference-time counterparts, with the null-example pPE strategy yielding the most substantial performance improvement, especially in AIME2024 and GPQA-Diamond.", "conclusion": "The study highlights the significance of prior prompt engineering as a key factor in reinforcement fine-tuning, demonstrating its ability to enhance performance and enforce distinct behavioral characteristics in language models.", "key_contributions": ["Introduces a systematic exploration of prior prompt engineering in reinforcement fine-tuning.", "Demonstrates that different pPE strategies yield distinct behavioral styles in language models.", "Shows significant performance gains across various benchmarks with pPE approaches."], "limitations": "", "keywords": ["prompt engineering", "reinforcement fine-tuning", "language models", "behavioral styles", "performance evaluation"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2505.14158", "pdf": "https://arxiv.org/pdf/2505.14158.pdf", "abs": "https://arxiv.org/abs/2505.14158", "title": "Temporal Alignment of Time Sensitive Facts with Activation Engineering", "authors": ["Sanjay Govindan", "Maurice Pagnucco", "Yang Song"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are trained on diverse and often conflicting\nknowledge spanning multiple domains and time periods. Some of this knowledge is\nonly valid within specific temporal contexts, such as answering the question,\n\"Who is the President of the United States in 2022?\" Ensuring LLMs generate\ntime appropriate responses is crucial for maintaining relevance and accuracy.\nIn this work we explore activation engineering as a method for temporally\naligning LLMs to improve factual recall without any training or dataset\ncreation. In this research we explore an activation engineering technique to\nground three versions of LLaMA 2 to specific points in time and examine the\neffects of varying injection layers and prompting strategies. Our experiments\ndemonstrate up to a 44% and 16% improvement in relative and explicit prompting\nrespectively, achieving comparable performance to the fine-tuning method\nproposed by Zhao et al. (2024) . Notably, our approach achieves similar results\nto the fine-tuning baseline while being significantly more computationally\nefficient and requiring no pre-aligned datasets.", "AI": {"tldr": "This paper investigates activation engineering to improve LLMs' factual recall in context-specific temporal queries without requiring training or dataset creation.", "motivation": "The need for LLMs to provide time-appropriate responses due to the presence of time-sensitive knowledge in training data.", "method": "An activation engineering technique is implemented to ground three versions of LLaMA 2 at specific points in time, assessing various injection layers and prompting strategies.", "result": "Improvements of up to 44% and 16% in relative and explicit prompting, achieving results comparable to a fine-tuning method while being more computationally efficient.", "conclusion": "The proposed activation engineering approach allows for temporal alignment of LLMs with effective results, suggesting a novel alternative to traditional fine-tuning.", "key_contributions": ["Introduction of activation engineering for LLM temporal alignment", "Demonstrated significant performance improvements without requiring datasets", "Achieved computational efficiency in grounding models temporally"], "limitations": "", "keywords": ["Large Language Models", "activation engineering", "temporal alignment", "factual recall", "LLaMA 2"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14160", "pdf": "https://arxiv.org/pdf/2505.14160.pdf", "abs": "https://arxiv.org/abs/2505.14160", "title": "Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models", "authors": ["Zahraa Al Sahili", "Ioannis Patras", "Matthew Purver"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual vision-language models promise universal image-text retrieval,\nyet their social biases remain under-explored. We present the first systematic\naudit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and\nCAPIVARA-CLIP -- across ten languages that vary in resource availability and\ngrammatical gender. Using balanced subsets of \\textsc{FairFace} and the\n\\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and\ngender bias and measure stereotype amplification. Contrary to the assumption\nthat multilinguality mitigates bias, every model exhibits stronger gender bias\nthan its English-only baseline. CAPIVARA-CLIP shows its largest biases\nprecisely in the low-resource languages it targets, while the shared\ncross-lingual encoder of NLLB-CLIP transports English gender stereotypes into\ngender-neutral languages; loosely coupled encoders largely avoid this transfer.\nHighly gendered languages consistently magnify all measured bias types, but\neven gender-neutral languages remain vulnerable when cross-lingual weight\nsharing imports foreign stereotypes. Aggregated metrics conceal\nlanguage-specific ``hot spots,'' underscoring the need for fine-grained,\nlanguage-aware bias evaluation in future multilingual vision-language research.", "AI": {"tldr": "This paper audits multilingual vision-language models for social biases, specifically examining race and gender biases across different languages.", "motivation": "To understand the extent of social biases present in multilingual vision-language models and the impact of language resources and grammatical gender on these biases.", "method": "The study systematically audits three multilingual CLIP checkpoints (M-CLIP, NLLB-CLIP, CAPIVARA-CLIP) using balanced subsets from FairFace and PATA in a zero-shot setting.", "result": "Each model exhibited stronger gender bias compared to its English-only counterparts, with amplification of stereotypes especially in low-resource languages.", "conclusion": "The findings indicate that multilinguality does not mitigate bias and highlight the importance of language-specific evaluations in assessing biases in multilingual models.", "key_contributions": ["First systematic audit of multilingual vision-language models for biases.", "Demonstrated that multilingual models can exhibit stronger gender bias than English-only models.", "Identified language-specific bias 'hot spots' that need focused evaluation."], "limitations": "The study only examines three models and relies on specific datasets for bias measurement, which may not capture all nuances of social biases.", "keywords": ["multilingual models", "vision-language", "bias evaluation", "gender bias", "cross-lingual transfer"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.14165", "pdf": "https://arxiv.org/pdf/2505.14165.pdf", "abs": "https://arxiv.org/abs/2505.14165", "title": "PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore", "authors": ["Zhenkai Qin", "Jiajing He", "Qiao Fang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity\ntoward specific aspects within a text, enabling more precise opinion mining in\ndomains such as product reviews and social media. However, traditional FGSA\napproaches often require task-specific architectures and extensive annotated\ndata, limiting their generalization and scalability. To address these\nchallenges, we propose PL-FGSA, a unified prompt learning-based framework\nimplemented using the MindSpore platform, which integrates prompt design with a\nlightweight TextCNN backbone. Our method reformulates FGSA as a multi-task\nprompt-augmented generation problem, jointly tackling aspect extraction,\nsentiment classification, and causal explanation in a unified paradigm. By\nleveraging prompt-based guidance, PL-FGSA enhances interpretability and\nachieves strong performance under both full-data and low-resource conditions.\nExperiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and\nMAMS-demonstrate that our model consistently outperforms traditional\nfine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,\nrespectively. These results validate the effectiveness of prompt-based\ngeneralization and highlight the practical value of PL-FGSA for real-world\nsentiment analysis tasks.", "AI": {"tldr": "PL-FGSA is a unified prompt learning framework for fine-grained sentiment analysis that enhances interpretability and performance in both full and low-resource settings.", "motivation": "Traditional fine-grained sentiment analysis methods face challenges with generalization and require extensive annotated data and task-specific architectures.", "method": "PL-FGSA reformulates fine-grained sentiment analysis as a multi-task prompt-augmented generation problem, integrating prompt design with a lightweight TextCNN model on the MindSpore platform.", "result": "The PL-FGSA model demonstrates superior performance over traditional fine-tuning methods, achieving F1-scores of 0.922, 0.694, and 0.597 on benchmark datasets.", "conclusion": "The results validate the effectiveness of prompt-based learning in enhancing sentiment analysis, making PL-FGSA a valuable tool for real-world applications.", "key_contributions": ["Unified approach for aspect extraction and sentiment classification", "Integration of prompt learning with TextCNN", "Strong performance under low-resource conditions"], "limitations": "", "keywords": ["fine-grained sentiment analysis", "prompt learning", "TextCNN", "MindSpore", "aspect extraction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.14172", "pdf": "https://arxiv.org/pdf/2505.14172.pdf", "abs": "https://arxiv.org/abs/2505.14172", "title": "The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models", "authors": ["Adrian Cosma", "Stefan Ruseti", "Emilian Radoi", "Mihai Dascalu"], "categories": ["cs.CL"], "comment": "1 Table, 8 Figures", "summary": "Despite their remarkable progress across diverse domains, Large Language\nModels (LLMs) consistently fail at simple character-level tasks, such as\ncounting letters in words, due to a fundamental limitation: tokenization. In\nthis work, we frame this limitation as a problem of low mutual information and\nanalyze it in terms of concept emergence. Using a suite of 19 synthetic tasks\nthat isolate character-level reasoning in a controlled setting, we show that\nsuch capabilities emerge slowly, suddenly, and only late in training. We\nfurther show that percolation-based models of concept emergence explain these\npatterns, suggesting that learning character composition is not fundamentally\ndifferent from learning commonsense knowledge. To address this bottleneck, we\npropose a lightweight architectural modification that significantly improves\ncharacter-level reasoning while preserving the inductive advantages of subword\nmodels. Together, our results bridge low-level perceptual gaps in tokenized LMs\nand provide a principled framework for understanding and mitigating their\nstructural blind spots. We make our code publicly available.", "AI": {"tldr": "This paper addresses the limitations of Large Language Models (LLMs) in performing character-level tasks due to issues stemming from tokenization, offering insights and solutions to improve performance in these areas.", "motivation": "The study investigates the fundamental limitations of LLMs in character-level reasoning caused by tokenization, aiming to understand and mitigate these pitfalls.", "method": "The authors conducted experiments using a suite of 19 synthetic tasks to isolate character-level reasoning and analyzed the emergence of concepts during training.", "result": "The findings indicate that character-level reasoning capabilities in LLMs emerge slowly and primarily late in training, which can be explained through percolation-based models of concept emergence.", "conclusion": "A proposed architectural modification enhances character-level reasoning in LLMs while maintaining the advantages of subword models, providing a framework to address structural blind spots in tokenized LMs.", "key_contributions": ["Analysis of character-level reasoning in LLMs through synthetic tasks", "Introduction of a percolation-based model for concept emergence", "Proposition of an architectural modification to improve character-level reasoning"], "limitations": "", "keywords": ["Large Language Models", "tokenization", "character-level reasoning", "concept emergence", "architectural modification"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.14173", "pdf": "https://arxiv.org/pdf/2505.14173.pdf", "abs": "https://arxiv.org/abs/2505.14173", "title": "THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation", "authors": ["Yunlong Liang", "Fandong Meng", "Jie Zhou"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main conference", "summary": "The sparse Mixture-of-Experts (MoE) has achieved significant progress for\nneural machine translation (NMT). However, there exist two limitations in\ncurrent MoE solutions which may lead to sub-optimal performance: 1) they\ndirectly use the task knowledge of NMT into MoE (\\emph{e.g.},\ndomain/linguistics-specific knowledge), which are generally unavailable at\npractical application and neglect the naturally grouped domain/linguistic\nproperties; 2) the expert selection only depends on the localized token\nrepresentation without considering the context, which fully grasps the state of\neach token in a global view. To address the above limitations, we propose\nTHOR-MoE via arming the MoE with hierarchical task-guided and\ncontext-responsive routing policies. Specifically, it 1) firstly predicts the\ndomain/language label and then extracts mixed domain/language representation to\nallocate task-level experts in a hierarchical manner; 2) injects the context\ninformation to enhance the token routing from the pre-selected task-level\nexperts set, which can help each token to be accurately routed to more\nspecialized and suitable experts. Extensive experiments on multi-domain\ntranslation and multilingual translation benchmarks with different\narchitectures consistently demonstrate the superior performance of THOR-MoE.\nAdditionally, the THOR-MoE operates as a plug-and-play module compatible with\nexisting Top-$k$~\\cite{shazeer2017} and Top-$p$~\\cite{huang-etal-2024-harder}\nrouting schemes, ensuring broad applicability across diverse MoE architectures.\nFor instance, compared with vanilla Top-$p$~\\cite{huang-etal-2024-harder}\nrouting, the context-aware manner can achieve an average improvement of 0.75\nBLEU with less than 22\\% activated parameters on multi-domain translation\ntasks.", "AI": {"tldr": "THOR-MoE is a novel framework improving sparse Mixture-of-Experts for neural machine translation by using hierarchical, context-aware routing policies to optimize expert selection.", "motivation": "Current Mixture-of-Experts (MoE) methods for neural machine translation suffer from performance limitations due to reliance on domain-specific knowledge and localized token representation without considering context.", "method": "THOR-MoE introduces a hierarchical approach by predicting domain/language labels and employing context-responsive routing policies, allowing for more specialized expert allocation for each token.", "result": "THOR-MoE shows improved performance on multi-domain and multilingual translation tasks, illustrating an average improvement of 0.75 BLEU over existing methods with fewer activated parameters.", "conclusion": "The THOR-MoE framework enhances existing MoE architectures and serves as a plug-and-play module compatible with contemporary routing schemes, demonstrating its wide applicability.", "key_contributions": ["Hierarchical task-guided expert routing", "Context-responsive token routing", "Compatibility with existing MoE architectures"], "limitations": "", "keywords": ["Mixture-of-Experts", "Neural Machine Translation", "Hierarchical Routing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.14174", "pdf": "https://arxiv.org/pdf/2505.14174.pdf", "abs": "https://arxiv.org/abs/2505.14174", "title": "Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning", "authors": ["Yusuf Denizay Dönder", "Derek Hommel", "Andrea W Wen-Yi", "David Mimno", "Unso Eun Seo Jo"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "LLMs are effective at code generation tasks like text-to-SQL, but is it worth\nthe cost? Many state-of-the-art approaches use non-task-specific LLM techniques\nincluding Chain-of-Thought (CoT), self-consistency, and fine-tuning. These\nmethods can be costly at inference time, sometimes requiring over a hundred LLM\ncalls with reasoning, incurring average costs of up to \\$0.46 per query, while\nfine-tuning models can cost thousands of dollars. We introduce \"N-rep\"\nconsistency, a more cost-efficient text-to-SQL approach that achieves similar\nBIRD benchmark scores as other more expensive methods, at only \\$0.039 per\nquery. N-rep leverages multiple representations of the same schema input to\nmitigate weaknesses in any single representation, making the solution more\nrobust and allowing the use of smaller and cheaper models without any reasoning\nor fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL\napproach in its cost range.", "AI": {"tldr": "This paper presents N-rep consistency, a cost-efficient method for text-to-SQL tasks that outperforms more expensive methods while significantly reducing query costs.", "motivation": "To address the high costs associated with existing LLM approaches for text-to-SQL tasks, which utilize methods like Chain-of-Thought and fine-tuning.", "method": "N-rep consistency uses multiple representations of the same schema input to improve robustness and performance, eliminating the need for reasoning and fine-tuning.", "result": "N-rep achieves comparable BIRD benchmark scores to more expensive techniques, costing only $0.039 per query.", "conclusion": "N-rep consistency is the best-performing text-to-SQL solution in its price range, offering a robust alternative to pricier methods.", "key_contributions": ["Introduction of N-rep consistency for text-to-SQL tasks", "Demonstrated comparable performance to costly state-of-the-art methods", "Significantly lower operational costs per query"], "limitations": "", "keywords": ["cost-efficient", "text-to-SQL", "N-rep consistency", "LLMs", "BIRD benchmark"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.14178", "pdf": "https://arxiv.org/pdf/2505.14178.pdf", "abs": "https://arxiv.org/abs/2505.14178", "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.", "AI": {"tldr": "This paper investigates how tokenization schemes impact the reasoning abilities of language models, introducing the concept of Token Awareness to highlight the importance of token granularity in preserving logical structures.", "motivation": "To understand how the structure of tokenization, particularly in models using subword-based methods like BPE, affects symbolic reasoning and performance in language models.", "method": "The authors conduct both theoretical analysis and empirical evaluations on arithmetic and symbolic tasks to assess how different tokenization schemes influence reasoning capabilities in language models.", "result": "The study finds that poor token granularity leads to disrupted logical alignment and hampers generalization in symbolic reasoning, while better-aligned token structures enhance model performance—allowing smaller models to outperform larger ones in certain reasoning tasks.", "conclusion": "Symbolic reasoning ability in LLMs is significantly affected by token-level representations rather than being solely dependent on model architecture.", "key_contributions": ["Introduces the notion of Token Awareness to highlight token granularity's impact on reasoning.", "Demonstrates that token structure can significantly influence model performance in reasoning tasks.", "Shows that better token alignment can enable smaller models to perform better than larger ones in structured reasoning."], "limitations": "", "keywords": ["tokenization", "Chain-of-Thought", "symbolic reasoning", "Token Awareness", "LLM"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.14179", "pdf": "https://arxiv.org/pdf/2505.14179.pdf", "abs": "https://arxiv.org/abs/2505.14179", "title": "Enhancing Abstractive Summarization of Scientific Papers Using Structure Information", "authors": ["Tong Bao", "Heng Zhang", "Chengzhi Zhang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS.", "AI": {"tldr": "This paper presents a two-stage abstractive summarization framework for scientific papers that improves the capture of structural information and generates comprehensive summaries.", "motivation": "Existing summarization methods fail to capture the structural information inherent in scientific papers and struggle with robustness across disciplines.", "method": "A two-stage framework that includes standardizing chapter titles and training a classifier for structural component recognition, followed by using Longformer for context-aware summary generation.", "result": "Experiments show that the proposed method outperforms advanced baselines and produces more comprehensive summaries on domain-specific datasets.", "conclusion": "The approach effectively leverages structural recognition to enhance the quality of summarization in scientific papers.", "key_contributions": ["Introduction of a two-stage framework for summarization that recognizes structural functions.", "Development of a large-scale dataset for training structural function recognition.", "Utilization of Longformer for context-aware summary generation."], "limitations": "", "keywords": ["abstractive summarization", "scientific papers", "structural function recognition", "Longformer", "context-aware summaries"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14181", "pdf": "https://arxiv.org/pdf/2505.14181.pdf", "abs": "https://arxiv.org/abs/2505.14181", "title": "SlangDIT: Benchmarking LLMs in Interpretative Slang Translation", "authors": ["Yunlong Liang", "Fandong Meng", "Jiaan Wang", "Jie Zhou"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "The challenge of slang translation lies in capturing context-dependent\nsemantic extensions, as slang terms often convey meanings beyond their literal\ninterpretation. While slang detection, explanation, and translation have been\nstudied as isolated tasks in the era of large language models (LLMs), their\nintrinsic interdependence remains underexplored. The main reason is lacking of\na benchmark where the two tasks can be a prerequisite for the third one, which\ncan facilitate idiomatic translation. In this paper, we introduce the\ninterpretative slang translation task (named SlangDIT) consisting of three\nsub-tasks: slang detection, cross-lingual slang explanation, and slang\ntranslation within the current context, aiming to generate more accurate\ntranslation with the help of slang detection and slang explanation. To this\nend, we construct a SlangDIT dataset, containing over 25k English-Chinese\nsentence pairs. Each source sentence mentions at least one slang term and is\nlabeled with corresponding cross-lingual slang explanation. Based on the\nbenchmark, we propose a deep thinking model, named SlangOWL. It firstly\nidentifies whether the sentence contains a slang, and then judges whether the\nslang is polysemous and analyze its possible meaning. Further, the SlangOWL\nprovides the best explanation of the slang term targeting on the current\ncontext. Finally, according to the whole thought, the SlangOWL offers a\nsuitable translation. Our experiments on LLMs (\\emph{e.g.}, Qwen2.5 and\nLLama-3.1), show that our deep thinking approach indeed enhances the\nperformance of LLMs where the proposed SLangOWL significantly surpasses the\nvanilla models and supervised fine-tuned models without thinking.", "AI": {"tldr": "The paper introduces SlangDIT, a benchmark for slang detection, explanation, and translation, and presents the SlangOWL model that improves translation accuracy using context-dependent slang interpretation in English-Chinese pairs.", "motivation": "The paper addresses the challenge of accurately translating slang terms, which often have meanings that extend beyond their literal interpretations, highlighting the need for a comprehensive approach that correlates slang detection, explanation, and translation.", "method": "The authors propose a new task called interpretative slang translation (SlangDIT) that consists of three sub-tasks: slang detection, cross-lingual slang explanation, and slang translation. They construct a dataset with 25k English-Chinese sentence pairs and develop the SlangOWL model, which identifies slang, analyzes its meaning, and provides contextually appropriate translations.", "result": "Experiments demonstrate that the SlangOWL model significantly outperforms traditional LLMs and supervised fine-tuned models, enhancing their ability to manage slang within translation tasks.", "conclusion": "The introduction of the SlangDIT benchmark and the SlangOWL model demonstrates the interdependence of slang detection, explanation, and translation, showing that a more integrated approach can lead to superior translation outcomes.", "key_contributions": ["Introduction of the SlangDIT dataset containing 25k English-Chinese sentence pairs with slang labeling.", "Development of the SlangOWL model that offers context-aware translations of slang.", "Exploration of the interdependencies between slang detection, explanation, and translation tasks."], "limitations": "The work is a progress report and may not present final results or fully validated methodologies.", "keywords": ["slang translation", "slang detection", "cross-lingual explanation", "large language models", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.14183", "pdf": "https://arxiv.org/pdf/2505.14183.pdf", "abs": "https://arxiv.org/abs/2505.14183", "title": "ThinkSwitcher: When to Think Hard, When to Think Fast", "authors": ["Guosheng Liang", "Longguang Zhong", "Ziyi Yang", "Xiaojun Quan"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) excel at solving complex tasks by leveraging\nlong chain-of-thought (CoT) reasoning. However, this often leads to\noverthinking on simple tasks, resulting in unnecessary computational overhead.\nWe observe that LRMs inherently possess the capability for efficient short CoT\nreasoning, which can be reliably elicited through prompt design. To leverage\nthis capability, we propose ThinkSwitcher, a framework that enables a single\nLRM to dynamically switch between short and long CoT modes based on task\ncomplexity. ThinkSwitcher introduces a lightweight switching module trained\nwith supervision signals derived from the relative performance of each\nreasoning mode across tasks. Experiments on multiple reasoning benchmarks show\nthat ThinkSwitcher reduces computational cost by 20-30% while maintaining high\naccuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher\nas a scalable and efficient solution for unified LRM deployment.", "AI": {"tldr": "ThinkSwitcher is a framework that allows large reasoning models to dynamically switch between short and long chain-of-thought reasoning based on task complexity, improving efficiency while maintaining accuracy.", "motivation": "Large reasoning models can overthink simple tasks, leading to unnecessary computational overhead. This paper explores an efficient method to leverage the inherent capabilities of these models.", "method": "The proposed ThinkSwitcher framework utilizes a lightweight switching module that is trained with supervision signals from the performance of reasoning modes across tasks.", "result": "ThinkSwitcher reduces computational cost by 20-30% while maintaining high accuracy on complex tasks according to experiments on various reasoning benchmarks.", "conclusion": "ThinkSwitcher provides a scalable and efficient solution for the deployment of large reasoning models by optimizing reasoning modes based on task complexity.", "key_contributions": ["Dynamic switching between short and long CoT modes", "Lightweight switching module trained with performance signals", "Reduction in computational cost without sacrificing accuracy"], "limitations": "", "keywords": ["large reasoning models", "chain-of-thought reasoning", "efficient learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14195", "pdf": "https://arxiv.org/pdf/2505.14195.pdf", "abs": "https://arxiv.org/abs/2505.14195", "title": "Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification", "authors": ["Tuc Nguyen", "Yifan Hu", "Thai Le"], "categories": ["cs.CL"], "comment": "17 pages, 3 figures", "summary": "Recent advancements in large language models (LLMs) have been fueled by large\nscale training corpora drawn from diverse sources such as websites, news\narticles, and books. These datasets often contain explicit user information,\nsuch as person names and addresses, that LLMs may unintentionally reproduce in\ntheir generated outputs. Beyond such explicit content, LLMs can also leak\nidentity revealing cues through implicit signals such as distinctive writing\nstyles, raising significant concerns about authorship privacy. There are three\nmajor automated tasks in authorship privacy, namely authorship obfuscation\n(AO), authorship mimicking (AM), and authorship verification (AV). Prior\nresearch has studied AO, AM, and AV independently. However, their interplays\nremain under explored, which leaves a major research gap, especially in the era\nof LLMs, where they are profoundly shaping how we curate and share user\ngenerated content, and the distinction between machine generated and human\nauthored text is also increasingly blurred. This work then presents the first\nunified framework for analyzing the dynamic relationships among LLM enabled AO,\nAM, and AV in the context of authorship privacy. We quantify how they interact\nwith each other to transform human authored text, examining effects at a single\npoint in time and iteratively over time. We also examine the role of\ndemographic metadata, such as gender, academic background, in modulating their\nperformances, inter-task dynamics, and privacy risks. All source code will be\npublicly available.", "AI": {"tldr": "This paper presents a unified framework for understanding the interactions among authorship obfuscation, mimicking, and verification, focusing on privacy risks in the context of large language models.", "motivation": "The study aims to address the gap in understanding how authorship privacy tasks such as obfuscation, mimicking, and verification interact, particularly with the rise of LLMs that can reproduce sensitive user information.", "method": "The authors propose a framework that quantifies the interrelations among authorship obfuscation, mimicking, and verification, considering their effects at a given time and over time, and incorporating demographic metadata into the analysis.", "result": "The paper identifies how the three authorship tasks interact to transform human-authored text and highlights the role of demographic factors in influencing privacy risks and task performances.", "conclusion": "The findings emphasize the need to consider the complex dynamics among authorship privacy tasks in the development and application of LLMs, urging for a comprehensive approach to privacy in user-generated content.", "key_contributions": ["Unified framework for analyzing authorship privacy tasks", "Quantitative assessment of inter-task dynamics", "Inclusion of demographic metadata in privacy risk analysis"], "limitations": "", "keywords": ["authorshio privacy", "large language models", "authorship obfuscation", "authorship mimicking", "privacy risks"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2505.14212", "pdf": "https://arxiv.org/pdf/2505.14212.pdf", "abs": "https://arxiv.org/abs/2505.14212", "title": "Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks", "authors": ["Sizhe Yuen", "Ting Su", "Ziyang Wang", "Yali Du", "Adam J. Sobey"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs.", "AI": {"tldr": "A novel approach using LLMs to generate context-based QA pairs enhances knowledge-intensive question-answering systems by improving reasoning and comprehension.", "motivation": "To enhance QA systems struggling with complex reasoning and the integration of real-time knowledge, providing a more effective approach for automated data generation.", "method": "The paper presents an automated QA generator that creates context-based QA pairs for fine-tuning LLMs, utilizing evaluations through perplexity, ROUGE, BLEU, and BERTScore.", "result": "The proposed system significantly improves logical coherence and factual accuracy, with Mistral-7b-v0.3 outperforming Llama-3-8b on several performance metrics for LLM-generated QA pairs.", "conclusion": "The findings suggest adaptability in AI systems by reducing reliance on human labeling through automated QA pair generation.", "key_contributions": ["Automated generation of context-based QA pairs", "Improved reasoning capabilities in LLMs", "Demonstrated enhancements in coherence and accuracy over existing systems"], "limitations": "", "keywords": ["Question-Answering", "Large Language Models", "Automated Generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14226", "pdf": "https://arxiv.org/pdf/2505.14226.pdf", "abs": "https://arxiv.org/abs/2505.14226", "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs", "authors": ["Darpan Aswal", "Siddharth D Jaiswal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.", "AI": {"tldr": "This study introduces novel strategies for jailbreaking Large Language Models (LLMs) through code-mixing and phonetic perturbations, showing high effectiveness in text and image generation tasks.", "motivation": "To address existing vulnerabilities in LLMs, particularly in multilingual contexts, where models are susceptible to jailbreaking strategies that bypass safety filters.", "method": "The paper introduces new jailbreak strategies that utilize code-mixing and phonetic perturbations to generate text and image outputs. These strategies were evaluated for their effectiveness in bypassing safety mechanisms.", "result": "The proposed methods achieved a 99% Attack Success Rate for text generation and 78% for image generation, with an Attack Relevance Rate of 100% for text and 95% for image generation using the new prompts.", "conclusion": "The findings highlight the need for improved safety alignment in multilingual multimodal models, suggesting that LLMs should better handle code-mixed prompts with misspellings in real-world applications.", "key_contributions": ["Introduction of novel jailbreak strategies using code-mixing and phonetic perturbations.", "High effectiveness in bypassing safety filters in both text and image generation.", "Contribution to the understanding of word tokenization impacts on jailbreak success."], "limitations": "The study may not address all forms of multilingual and multimodal vulnerabilities and focuses primarily on specific attack strategies.", "keywords": ["Large Language Models", "jailbreak", "code-mixing", "phonetic perturbations", "multimodal"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.14233", "pdf": "https://arxiv.org/pdf/2505.14233.pdf", "abs": "https://arxiv.org/abs/2505.14233", "title": "Mechanistic Fine-tuning for In-context Learning", "authors": ["Hakaze Cho", "Peng Luo", "Mariko Kato", "Rin Kaenbyou", "Naoya Inoue"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 31 figures, 6 tables", "summary": "In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.", "AI": {"tldr": "The paper introduces Attention Behavior Fine-Tuning (ABFT) to enhance in-context learning in Language Models (LMs) while reducing computational costs by focusing on attention scores rather than final outputs.", "motivation": "The paper addresses the high computational costs associated with fine-tuning Language Models on in-context learning (ICL) style data, and aims to improve efficiency while maintaining performance.", "method": "The proposed method, Attention Behavior Fine-Tuning (ABFT), modifies training objectives to focus on attention scores that emphasize correct label tokens in the input context, rather than the final model outputs.", "result": "ABFT shows superior performance across 9 modern LMs and 8 datasets in terms of performance, robustness, unbiasedness, and efficiency, achieving these results with only around 0.01% of the data cost compared to previous methods.", "conclusion": "The findings suggest that the ABFT objective is intrinsically contained within the existing end-to-end training objectives, highlighting the potential for achieving mechanistic interpretability in language models.", "key_contributions": ["Introduction of Attention Behavior Fine-Tuning (ABFT) for LMs", "Demonstrated reduction in data cost with maintained or improved performance", "Insights into the influence of training objectives on the mechanistic behavior of LMs"], "limitations": "", "keywords": ["In-context Learning", "Language Models", "Attention Behavior Fine-Tuning", "Machine Learning", "Mechanistic Interpretability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.14238", "pdf": "https://arxiv.org/pdf/2505.14238.pdf", "abs": "https://arxiv.org/abs/2505.14238", "title": "ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models", "authors": ["Raghav Singhal", "Kaustubh Ponkshe", "Rohit Vartak", "Praneeth Vepakomma"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work", "summary": "Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.", "AI": {"tldr": "ABBA is a new Parameter-Efficient Fine-Tuning architecture that improves the adaptability of Large Language Models by allowing independent optimization of updates from pre-trained weights.", "motivation": "The paper addresses the challenge of efficiently adapting Large Language Models to new domains, overcoming limitations of existing PEFT methods like LoRA and HiRA which are constrained by their rank and structure.", "method": "ABBA reparameterizes updates as a Hadamard product of two independently learnable low-rank matrices, fully decoupling the update process from the pre-trained weights.", "result": "ABBA significantly enhances expressivity within the same parameter budget and achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, outperforming existing PEFT methods across multiple models.", "conclusion": "The introduction of ABBA demonstrates a new direction for PEFT methods, allowing more flexible optimization techniques that lead to improved performance.", "key_contributions": ["Introduction of ABBA as a new PEFT architecture", "Full decoupling of updates from pre-trained weights", "State-of-the-art performance on benchmark tasks"], "limitations": "", "keywords": ["Large Language Models", "Parameter-Efficient Fine-Tuning", "ABBA"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14242", "pdf": "https://arxiv.org/pdf/2505.14242.pdf", "abs": "https://arxiv.org/abs/2505.14242", "title": "Technical Report on classification of literature related to children speech disorder", "authors": ["Ziang Wang", "Amir Aryani"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "This technical report presents a natural language processing (NLP)-based\napproach for systematically classifying scientific literature on childhood\nspeech disorders. We retrieved and filtered 4,804 relevant articles published\nafter 2015 from the PubMed database using domain-specific keywords. After\ncleaning and pre-processing the abstracts, we applied two topic modeling\ntechniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify\nlatent thematic structures in the corpus. Our models uncovered 14 clinically\nmeaningful clusters, such as infantile hyperactivity and abnormal epileptic\nbehavior. To improve relevance and precision, we incorporated a custom stop\nword list tailored to speech pathology. Evaluation results showed that the LDA\nmodel achieved a coherence score of 0.42 and a perplexity of -7.5, indicating\nstrong topic coherence and predictive performance. The BERTopic model exhibited\na low proportion of outlier topics (less than 20%), demonstrating its capacity\nto classify heterogeneous literature effectively. These results provide a\nfoundation for automating literature reviews in speech-language pathology.", "AI": {"tldr": "This report outlines an NLP approach to classify scientific literature on childhood speech disorders using LDA and BERTopic, identifying 14 thematic clusters and suggesting implications for automating literature reviews in speech-language pathology.", "motivation": "The aim is to develop a systematic way to classify and review scientific literature related to childhood speech disorders, enhancing the current methodologies with NLP techniques.", "method": "The authors retrieved 4,804 relevant articles from PubMed, applied cleaning and pre-processing, and utilized LDA and BERTopic for topic modeling to identify clusters in the literature.", "result": "The study identified 14 clinically meaningful clusters, achieving a coherence score of 0.42 for LDA and showing low outlier topics for BERTopic, indicating effective classification and coherence.", "conclusion": "The findings support automating literature reviews in speech-language pathology, providing a foundation for further research in this area.", "key_contributions": ["Implementation of an NLP-based classification framework for scientific literature", "Identification of clinically meaningful clusters in childhood speech disorders", "Evaluation of topic modeling techniques tailored to speech pathology"], "limitations": "", "keywords": ["Natural Language Processing", "Speech Disorders", "Topic Modeling", "Literature Review", "Childhood"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.14244", "pdf": "https://arxiv.org/pdf/2505.14244.pdf", "abs": "https://arxiv.org/abs/2505.14244", "title": "TransBench: Benchmarking Machine Translation for Industrial-Scale Applications", "authors": ["Haijun Li", "Tianqi Shi", "Zifu Shang", "Yuxuan Han", "Xueyu Zhao", "Hao Wang", "Yu Qian", "Zhiqiang Qian", "Linlong Xu", "Minghao Wu", "Chenyang Lyu", "Longyue Wang", "Gongbo Tang", "Weihua Luo", "Zhao Xu", "Kaifu Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Machine translation (MT) has become indispensable for cross-border\ncommunication in globalized industries like e-commerce, finance, and legal\nservices, with recent advancements in large language models (LLMs)\nsignificantly enhancing translation quality. However, applying general-purpose\nMT models to industrial scenarios reveals critical limitations due to\ndomain-specific terminology, cultural nuances, and stylistic conventions absent\nin generic benchmarks. Existing evaluation frameworks inadequately assess\nperformance in specialized contexts, creating a gap between academic benchmarks\nand real-world efficacy. To address this, we propose a three-level translation\ncapability framework: (1) Basic Linguistic Competence, (2) Domain-Specific\nProficiency, and (3) Cultural Adaptation, emphasizing the need for holistic\nevaluation across these dimensions. We introduce TransBench, a benchmark\ntailored for industrial MT, initially targeting international e-commerce with\n17,000 professionally translated sentences spanning 4 main scenarios and 33\nlanguage pairs. TransBench integrates traditional metrics (BLEU, TER) with\nMarco-MOS, a domain-specific evaluation model, and provides guidelines for\nreproducible benchmark construction. Our contributions include: (1) a\nstructured framework for industrial MT evaluation, (2) the first publicly\navailable benchmark for e-commerce translation, (3) novel metrics probing\nmulti-level translation quality, and (4) open-sourced evaluation tools. This\nwork bridges the evaluation gap, enabling researchers and practitioners to\nsystematically assess and enhance MT systems for industry-specific needs.", "AI": {"tldr": "This paper introduces a three-level translation capability framework and a novel benchmark, TransBench, to improve machine translation quality in industrial contexts, particularly in e-commerce.", "motivation": "There is a critical need for more effective evaluation frameworks for machine translation in specialized domains, as existing models fail to capture the nuances of domain-specific terminology and cultural contexts.", "method": "The authors propose a three-level framework comprising Basic Linguistic Competence, Domain-Specific Proficiency, and Cultural Adaptation, and develop a benchmark, TransBench, using 17,000 professionally translated sentences across various scenarios and language pairs.", "result": "TransBench integrates traditional evaluation metrics with a new domain-specific model, providing a comprehensive tool for assessing machine translation performance in specific industrial sectors.", "conclusion": "The proposed framework and benchmark aim to bridge the existing evaluation gap, allowing better assessment and enhancement of machine translation in specialized industries like e-commerce.", "key_contributions": ["Structured framework for industrial MT evaluation", "First publicly available benchmark for e-commerce translation", "Novel metrics for multi-level translation quality"], "limitations": "", "keywords": ["machine translation", "evaluation framework", "domain-specific", "e-commerce", "large language models"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.14256", "pdf": "https://arxiv.org/pdf/2505.14256.pdf", "abs": "https://arxiv.org/abs/2505.14256", "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation", "authors": ["Shaolin Zhu", "Tianyu Dong", "Bo Li", "Deyi Xiong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.", "AI": {"tldr": "FuxiMT is a multilingual machine translation model that utilizes a sparsified LLM, demonstrating strong performance in low-resource language translation and zero-shot capabilities.", "motivation": "The paper aims to improve machine translation for low-resource languages and enhance multilingual communication using a robust model architecture.", "method": "FuxiMT is trained using a two-stage strategy: pre-training on a large Chinese corpus followed by multilingual fine-tuning on a dataset of 65 languages, incorporating Mixture-of-Experts and curriculum learning.", "result": "FuxiMT outperforms state-of-the-art models, especially under low-resource conditions, and shows zero-shot translation abilities for unseen languages.", "conclusion": "The findings suggest FuxiMT can effectively address translation challenges in multilingual contexts, especially where data availability is limited.", "key_contributions": ["Introduction of FuxiMT as a Chinese-centric multilingual translation model", "Utilization of Mixture-of-Experts and curriculum learning to enhance performance", "Demonstration of significant improvements in low-resource translation scenarios"], "limitations": "", "keywords": ["multilingual machine translation", "sparsified LLM", "low-resource languages", "zero-shot translation", "Mixture-of-Experts"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.14268", "pdf": "https://arxiv.org/pdf/2505.14268.pdf", "abs": "https://arxiv.org/abs/2505.14268", "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge", "authors": ["Hui Huang", "Yancheng He", "Hongli Zhou", "Rui Zhang", "Wei Liu", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Jiaheng Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 14 figures", "summary": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.", "AI": {"tldr": "The paper presents Think-J, an improved model for LLM evaluation that enhances the judgment capabilities of Large Language Models (LLMs) through reinforcement learning.", "motivation": "The study addresses the limitations of LLMs in evaluating generated responses effectively, which is crucial for LLM evaluation and reward modeling.", "method": "The authors developed Think-J by initially using a small curated dataset for judgment thinking capabilities, followed by optimizing these capabilities with two reinforcement learning approaches: offline RL (using a critic model) and online RL (using rule-based rewards).", "result": "Experimental results indicate that Think-J significantly improves LLM evaluation performance, outperforming previous LLM-Judge models without the need for additional human annotations.", "conclusion": "Think-J demonstrates that optimizing judgment thinking through RL can enhance the evaluation effectiveness of LLMs, providing a more efficient method for response generation evaluation.", "key_contributions": ["Introduction of Think-J model for LLM evaluation", "Use of reinforcement learning for judgment optimization", "Significant performance improvement without extra human annotations"], "limitations": "", "keywords": ["Large Language Models", "judgment modeling", "reinforcement learning", "evaluative capability", "human annotations"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.14271", "pdf": "https://arxiv.org/pdf/2505.14271.pdf", "abs": "https://arxiv.org/abs/2505.14271", "title": "FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning", "authors": ["Minh Ngoc Ta", "Dong Cao Van", "Duc-Anh Hoang", "Minh Le-Anh", "Truong Nguyen", "My Anh Tran Nguyen", "Yuxia Wang", "Preslav Nakov", "Sang Dinh"], "categories": ["cs.CL"], "comment": null, "summary": "The growing collaboration between humans and AI models in generative tasks\nhas introduced new challenges in distinguishing between human-written,\nAI-generated, and human-AI collaborative texts. In this work, we collect a\nmultilingual, multi-domain, multi-generator dataset FAIDSet. We further\nintroduce a fine-grained detection framework FAID to classify text into these\nthree categories, meanwhile identifying the underlying AI model family. Unlike\nexisting binary classifiers, FAID is built to capture both authorship and\nmodel-specific characteristics. Our method combines multi-level contrastive\nlearning with multi-task auxiliary classification to learn subtle stylistic\ncues. By modeling AI families as distinct stylistic entities, FAID offers\nimproved interpretability. We incorporate an adaptation to address\ndistributional shifts without retraining for unseen data. Experimental results\ndemonstrate that FAID outperforms several baseline approaches, particularly\nenhancing the generalization accuracy on unseen domains and new AI models. It\nprovide a potential solution for improving transparency and accountability in\nAI-assisted writing.", "AI": {"tldr": "This paper presents FAIDSet, a dataset for distinguishing between human, AI-generated, and human-AI collaborative texts, and introduces the FAID framework for fine-grained classification and increased interpretability.", "motivation": "Increasing collaboration between humans and AI in generative tasks has made it essential to differentiate between various types of text authorship.", "method": "The FAID framework uses multi-level contrastive learning and multi-task auxiliary classification to classify texts into human, AI, and collaborative categories while identifying the underlying AI model family.", "result": "FAID demonstrates superior performance in classifying unseen domains and new AI models compared to existing binary classifiers, thereby enhancing generalization accuracy.", "conclusion": "The findings suggest that FAID can significantly improve transparency and accountability in AI-assisted writing environments.", "key_contributions": ["Introduction of the FAIDSet dataset for multi-domain and multi-generator text classification.", "Development of the FAID framework for fine-grained authorship detection and model identification.", "Utilization of multi-level contrastive learning to enhance interpretability and adaptability."], "limitations": "", "keywords": ["AI authorship detection", "human-AI collaboration", "multi-task learning", "contrastive learning", "text classification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14272", "pdf": "https://arxiv.org/pdf/2505.14272.pdf", "abs": "https://arxiv.org/abs/2505.14272", "title": "Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data", "authors": ["Faeze Ghorbanpour", "Daryna Dementieva", "Alexander Fraser"], "categories": ["cs.CL", "cs.CY", "cs.MM"], "comment": null, "summary": "Considering the importance of detecting hateful language, labeled hate speech\ndata is expensive and time-consuming to collect, particularly for low-resource\nlanguages. Prior work has demonstrated the effectiveness of cross-lingual\ntransfer learning and data augmentation in improving performance on tasks with\nlimited labeled data. To develop an efficient and scalable cross-lingual\ntransfer learning approach, we leverage nearest-neighbor retrieval to augment\nminimal labeled data in the target language, thereby enhancing detection\nperformance. Specifically, we assume access to a small set of labeled training\ninstances in the target language and use these to retrieve the most relevant\nlabeled examples from a large multilingual hate speech detection pool. We\nevaluate our approach on eight languages and demonstrate that it consistently\noutperforms models trained solely on the target language data. Furthermore, in\nmost cases, our method surpasses the current state-of-the-art. Notably, our\napproach is highly data-efficient, retrieving as small as 200 instances in some\ncases while maintaining superior performance. Moreover, it is scalable, as the\nretrieval pool can be easily expanded, and the method can be readily adapted to\nnew languages and tasks. We also apply maximum marginal relevance to mitigate\nredundancy and filter out highly similar retrieved instances, resulting in\nimprovements in some languages.", "AI": {"tldr": "This paper presents a cross-lingual transfer learning approach for hate speech detection using nearest-neighbor retrieval to augment scarce labeled data in low-resource languages.", "motivation": "Detecting hate speech is critical, yet collecting labeled data for low-resource languages is challenging and resource-intensive.", "method": "The method utilizes nearest-neighbor retrieval to enhance minimal labeled training datasets in the target language by retrieving relevant examples from a multilingual hate speech detection pool.", "result": "The approach was evaluated on eight languages and showed consistent improvements over models trained solely on target language data, often surpassing state-of-the-art methods.", "conclusion": "The approach demonstrates data efficiency and scalability, being adaptable to new languages and tasks, while also employing a technique to reduce redundancy in retrieved instances.", "key_contributions": ["Efficient and scalable method for cross-lingual hate speech detection", "Demonstration of superior performance with minimal labeled data", "Application of maximum marginal relevance to reduce instance redundancy"], "limitations": "", "keywords": ["hate speech detection", "cross-lingual transfer learning", "data augmentation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.14279", "pdf": "https://arxiv.org/pdf/2505.14279.pdf", "abs": "https://arxiv.org/abs/2505.14279", "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "authors": ["Jennifer D'Souza", "Hamed Babaei Giglou", "Quentin Münch"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "summary": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.", "AI": {"tldr": "Introduces YESciEval, a framework for robust evaluation of large language models in scientific Q&A, using reinforcement learning and fine-grained assessments.", "motivation": "To improve the reliability of evaluations for LLMs used in scientific question-answering.", "method": "An open-source framework combining rubric-based assessment and reinforcement learning, alongside multidisciplinary science Q&A datasets, including adversarial examples.", "result": "YESciEval provides scalable, cost-free evaluation scores from various LLMs, aiming to reduce optimism bias and enhance AI alignment in science.", "conclusion": "The development of reliable LLM-as-a-judge models through YESciEval is essential for robust evaluation in scientific inquiry and the pursuit of artificial general intelligence.", "key_contributions": ["Introduction of the YESciEval framework", "Release of multidisciplinary science Q&A datasets", "Mitigation of optimism bias in LLM evaluations through reinforcement learning"], "limitations": "", "keywords": ["Large Language Models", "scientific question-answering", "evaluation robustness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14286", "pdf": "https://arxiv.org/pdf/2505.14286.pdf", "abs": "https://arxiv.org/abs/2505.14286", "title": "Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs", "authors": ["Rao Ma", "Mengjie Qian", "Vyas Raina", "Mark Gales", "Kate Knill"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The combination of pre-trained speech encoders with large language models has\nenabled the development of speech LLMs that can handle a wide range of spoken\nlanguage processing tasks. While these models are powerful and flexible, this\nvery flexibility may make them more vulnerable to adversarial attacks. To\nexamine the extent of this problem, in this work we investigate universal\nacoustic adversarial attacks on speech LLMs. Here a fixed, universal,\nadversarial audio segment is prepended to the original input audio. We\ninitially investigate attacks that cause the model to either produce no output\nor to perform a modified task overriding the original prompt. We then extend\nthe nature of the attack to be selective so that it activates only when\nspecific input attributes, such as a speaker gender or spoken language, are\npresent. Inputs without the targeted attribute should be unaffected, allowing\nfine-grained control over the model outputs. Our findings reveal critical\nvulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar\nspeech LLMs may be susceptible to universal adversarial attacks. This\nhighlights the need for more robust training strategies and improved resistance\nto adversarial attacks.", "AI": {"tldr": "This paper investigates universal acoustic adversarial attacks on speech large language models (LLMs), demonstrating their vulnerabilities and suggesting the need for improved robustness.", "motivation": "To examine the vulnerabilities of speech LLMs to adversarial attacks due to their flexibility in handling spoken language tasks.", "method": "The research investigates the impact of prepend universal adversarial audio segments to original input audio, exploring both complete output suppression and modified task performance based on input attributes.", "result": "Findings reveal vulnerabilities in models like Qwen2-Audio and Granite-Speech to universal adversarial attacks, indicating a broader susceptibility in similar speech LLMs.", "conclusion": "The study emphasizes the need for enhanced training strategies to fortify speech LLMs against adversarial attacks.", "key_contributions": ["Investigation of universal adversarial attacks in speech LLMs", "Demonstration of selective attack methods based on input attributes", "Identification of vulnerabilities in specific speech models"], "limitations": "", "keywords": ["speech LLMs", "adversarial attacks", "universal audio segments", "spoken language processing", "robust training strategies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.14297", "pdf": "https://arxiv.org/pdf/2505.14297.pdf", "abs": "https://arxiv.org/abs/2505.14297", "title": "Cross-Lingual Optimization for Language Transfer in Large Language Models", "authors": ["Jungseob Lee", "Seongtae Hong", "Hyeonseok Moon", "Heuiseok Lim"], "categories": ["cs.CL"], "comment": "Accepted for publication at ACL 2025. Jungseob Lee and Seongtae Hong\n  contributed equally to this work", "summary": "Adapting large language models to other languages typically employs\nsupervised fine-tuning (SFT) as a standard approach. However, it often suffers\nfrom an overemphasis on English performance, a phenomenon that is especially\npronounced in data-constrained environments. To overcome these challenges, we\npropose \\textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an\nEnglish-centric LLM to a target language while preserving its English\ncapabilities. CLO utilizes publicly available English SFT data and a\ntranslation model to enable cross-lingual transfer. We conduct experiments\nusing five models on six languages, each possessing varying levels of resource.\nOur results show that CLO consistently outperforms SFT in both acquiring target\nlanguage proficiency and maintaining English performance. Remarkably, in\nlow-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400\nsamples, demonstrating that CLO can achieve better performance with less data.\nFurthermore, we find that SFT is particularly sensitive to data quantity in\nmedium and low-resource languages, whereas CLO remains robust. Our\ncomprehensive analysis emphasizes the limitations of SFT and incorporates\nadditional training strategies in CLO to enhance efficiency.", "AI": {"tldr": "This paper introduces Cross-Lingual Optimization (CLO), a method for transferring English-centric LLMs to target languages while maintaining English proficiency, outperforming standard supervised fine-tuning.", "motivation": "There is a growing need to adapt large language models (LLMs) to various languages, but existing approaches often overemphasize English capabilities, leading to poorer performance in data-constrained environments.", "method": "CLO employs a translation model along with publicly available supervised fine-tuning (SFT) data in English to facilitate effective cross-lingual knowledge transfer, enabling models to perform better in target languages while preserving English performance.", "result": "CLO consistently outperforms SFT across multiple languages, showing that it can achieve high proficiency in low-resource languages with significantly fewer samples than SFT.", "conclusion": "The findings highlight the advantages of CLO over SFT in terms of data efficiency and robustness, suggesting that CLO is a more effective strategy for adapting LLMs to diverse language scenarios.", "key_contributions": ["Introduction of Cross-Lingual Optimization (CLO) for language model adaptation", "Demonstrated that CLO outperforms SFT especially in low-resource settings", "Provided insights into the sensitivity of supervised fine-tuning to data quantity"], "limitations": "The analysis focuses on the performance in specific languages; broader applicability needs to be validated in additional languages and contexts.", "keywords": ["Cross-Lingual Optimization", "Large Language Models", "Supervised Fine-Tuning", "Language Adaptation", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14305", "pdf": "https://arxiv.org/pdf/2505.14305.pdf", "abs": "https://arxiv.org/abs/2505.14305", "title": "JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling", "authors": ["Jinwang Song", "Hongying Zan", "Kunli Zhang", "Lingling Mu", "Yingjie Han", "Haobo Hua", "Min Peng"], "categories": ["cs.CL"], "comment": "Work in progress. 13 pages, 6 figures", "summary": "Text-to-SQL, which maps natural language to SQL queries, has benefited\ngreatly from recent advances in Large Language Models (LLMs). While LLMs offer\nvarious paradigms for this task, including prompting and supervised fine-tuning\n(SFT), SFT approaches still face challenges such as complex multi-stage\npipelines and poor robustness to noisy schema information. To address these\nlimitations, we present JOLT-SQL, a streamlined single-stage SFT framework that\njointly optimizes schema linking and SQL generation via a unified loss.\nJOLT-SQL employs discriminative schema linking, enhanced by local bidirectional\nattention, alongside a confusion-aware noisy schema sampling strategy with\nselective attention to improve robustness under noisy schema conditions.\nExperiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL\nachieves state-of-the-art execution accuracy among comparable-size open-source\nmodels, while significantly improving both training and inference efficiency.", "AI": {"tldr": "JOLT-SQL is a single-stage supervised fine-tuning framework that optimizes schema linking and SQL generation, addressing robustness issues in Text-to-SQL tasks.", "motivation": "To improve upon existing SFT approaches in Text-to-SQL by overcoming complexities related to multi-stage pipelines and enhancing robustness against noisy schema information.", "method": "JOLT-SQL employs a streamlined single-stage framework that combines discriminative schema linking with local bidirectional attention and a confusion-aware noisy schema sampling strategy.", "result": "JOLT-SQL achieves state-of-the-art execution accuracy on the Spider and BIRD benchmarks while enhancing training and inference efficiency compared to comparable open-source models.", "conclusion": "JOLT-SQL presents an efficient alternative for Text-to-SQL tasks, with significant improvements in execution accuracy and robustness under challenging conditions.", "key_contributions": ["Streamlined single-stage SFT framework for Text-to-SQL", "Joint optimization of schema linking and SQL generation", "Enhanced robustness through confusion-aware schema sampling"], "limitations": "", "keywords": ["text-to-sql", "large language models", "schema linking", "SQL generation", "noisy schema"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14309", "pdf": "https://arxiv.org/pdf/2505.14309.pdf", "abs": "https://arxiv.org/abs/2505.14309", "title": "Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency", "authors": ["Ehsan Doostmohammadi", "Marco Kuhlmann"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented language models have demonstrated performance comparable\nto much larger models while requiring fewer computational resources. The\neffectiveness of these models crucially depends on the overlap between query\nand retrieved context, but the optimal degree of this overlap remains\nunexplored. In this paper, we systematically investigate how varying levels of\nquery--context overlap affect model performance during both training and\ninference. Our experiments reveal that increased overlap initially has minimal\neffect, but substantially improves test-time perplexity and accelerates model\nlearning above a critical threshold. Building on these findings, we demonstrate\nthat deliberately increasing overlap through synthetic context can enhance data\nefficiency and reduce training time by approximately 40\\% without compromising\nperformance. We specifically generate synthetic context through paraphrasing\nqueries. We validate our perplexity-based findings on question-answering tasks,\nconfirming that the benefits of retrieval-augmented language modeling extend to\npractical applications. Our results provide empirical evidence of significant\noptimization potential for retrieval mechanisms in language model pretraining.", "AI": {"tldr": "This paper investigates the impact of query-context overlap on retrieval-augmented language model performance and demonstrates that increasing overlap can enhance data efficiency and reduce training time.", "motivation": "To explore how the degree of overlap between query and retrieved context impacts the performance of retrieval-augmented language models during training and inference.", "method": "The authors conducted systematic experiments to assess varying levels of query-context overlap and its effects on model learning and test-time perplexity.", "result": "Increased overlap improves test-time perplexity and accelerates model learning above a critical threshold. Synthetic context generated through query paraphrasing enhances data efficiency and reduces training time by around 40%.", "conclusion": "The study provides empirical evidence that optimizing retrieval mechanisms in language model pretraining can yield significant performance benefits without compromising quality.", "key_contributions": ["Systematic investigation of query-context overlap effects", "Demonstration of enhanced data efficiency through synthetic context", "Empirical validation of results on question-answering tasks"], "limitations": "", "keywords": ["retrieval-augmented models", "query-context overlap", "data efficiency"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.14311", "pdf": "https://arxiv.org/pdf/2505.14311.pdf", "abs": "https://arxiv.org/abs/2505.14311", "title": "HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing", "authors": ["Shamsuddeen Hassan Muhammad", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Falalu Ibrahim Lawan", "Babangida Sani", "Sukairaj Hafiz Imam", "Yusuf Aliyu", "Sani Abdullahi Sani", "Ali Usman Umar", "Kenneth Church", "Vukosi Marivate"], "categories": ["cs.CL"], "comment": null, "summary": "Hausa Natural Language Processing (NLP) has gained increasing attention in\nrecent years, yet remains understudied as a low-resource language despite\nhaving over 120 million first-language (L1) and 80 million second-language (L2)\nspeakers worldwide. While significant advances have been made in high-resource\nlanguages, Hausa NLP faces persistent challenges, including limited open-source\ndatasets and inadequate model representation. This paper presents an overview\nof the current state of Hausa NLP, systematically examining existing resources,\nresearch contributions, and gaps across fundamental NLP tasks: text\nclassification, machine translation, named entity recognition, speech\nrecognition, and question answering. We introduce HausaNLP\n(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\ntools, and research works to enhance accessibility and drive further\ndevelopment. Furthermore, we discuss challenges in integrating Hausa into large\nlanguage models (LLMs), addressing issues of suboptimal tokenization and\ndialectal variation. Finally, we propose strategic research directions\nemphasizing dataset expansion, improved language modeling approaches, and\nstrengthened community collaboration to advance Hausa NLP. Our work provides\nboth a foundation for accelerating Hausa NLP progress and valuable insights for\nbroader multilingual NLP research.", "AI": {"tldr": "Overview of the current state of Hausa NLP, challenges, resources, and proposed research directions.", "motivation": "Despite having a substantial number of speakers, Hausa remains a low-resource language in NLP, requiring systematic examination of current state and research gaps.", "method": "The paper systematically reviews existing resources and contributions in Hausa NLP across fundamental tasks and introduces a curated catalog of datasets and tools.", "result": "Introduced HausaNLP, a catalog that aggregates datasets, tools, and research works, while identifying integration challenges with LLMs.", "conclusion": "The paper suggests strategic directions for enhancing Hausa NLP through dataset expansion, improved modeling approaches, and community collaboration.", "key_contributions": ["Overview of Hausa NLP resources and gaps", "Introduction of HausaNLP catalog", "Strategic research directions for Hausa NLP advancement"], "limitations": "Limited scope might not cover all regional dialects and NLP tasks thoroughly.", "keywords": ["Hausa", "Natural Language Processing", "low-resource language", "datasets", "language models"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.14313", "pdf": "https://arxiv.org/pdf/2505.14313.pdf", "abs": "https://arxiv.org/abs/2505.14313", "title": "A MIND for Reasoning: Meta-learning for In-context Deduction", "authors": ["Leonardo Bertolazzi", "Manuel Vargas Guzmán", "Raffaella Bernardi", "Maciej Malicki", "Jakub Szymanik"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly evaluated on formal tasks,\nwhere strong reasoning abilities define the state of the art. However, their\nability to generalize to out-of-distribution problems remains limited. In this\npaper, we investigate how LLMs can achieve a systematic understanding of\ndeductive rules. Our focus is on the task of identifying the appropriate subset\nof premises within a knowledge base needed to derive a given hypothesis. To\ntackle this challenge, we propose Meta-learning for In-context Deduction\n(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND\nis to enable models to generalize more effectively to unseen knowledge bases\nand to systematically apply inference rules. Our results show that MIND\nsignificantly improves generalization in small LMs ranging from 1.5B to 7B\nparameters. The benefits are especially pronounced in smaller models and\nlow-data settings. Remarkably, small models fine-tuned with MIND outperform\nstate-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.", "AI": {"tldr": "This paper introduces MIND, a meta-learning approach aimed at improving the generalization of LLMs in deductive reasoning tasks by enabling them to effectively leverage knowledge bases.", "motivation": "Despite advancements, LLMs struggle with generalizing to out-of-distribution problems, particularly in deductive reasoning tasks.", "method": "The authors propose MIND, a few-shot meta-learning fine-tuning approach designed to help models systematically apply inference rules to derive hypotheses from knowledge bases.", "result": "MIND significantly enhances generalization capabilities in small language models (1.5B to 7B parameters), outperforming state-of-the-art LLMs in low-data environments.", "conclusion": "The findings suggest that even smaller LMs, when fine-tuned using MIND, can achieve better performance in deductive reasoning tasks than larger state-of-the-art models.", "key_contributions": ["Introduction of MIND for few-shot meta-learning in LLMs", "Demonstrated improvements in deductive reasoning capabilities", "Enhanced performance of smaller LMs in low-data scenarios"], "limitations": "", "keywords": ["large language models", "meta-learning", "deductive reasoning", "knowledge base", "generalization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14347", "pdf": "https://arxiv.org/pdf/2505.14347.pdf", "abs": "https://arxiv.org/abs/2505.14347", "title": "QA-prompting: Improving Summarization with Large Language Models using Question-Answering", "authors": ["Neelabh Sinha"], "categories": ["cs.CL"], "comment": "Submitted to ARR", "summary": "Language Models (LMs) have revolutionized natural language processing,\nenabling high-quality text generation through prompting and in-context\nlearning. However, models often struggle with long-context summarization due to\npositional biases, leading to suboptimal extraction of critical information.\nThere are techniques to improve this with fine-tuning, pipelining, or using\ncomplex techniques, which have their own challenges. To solve these challenges,\nwe propose QA-prompting - a simple prompting method for summarization that\nutilizes question-answering as an intermediate step prior to summary\ngeneration. Our method extracts key information and enriches the context of\ntext to mitigate positional biases and improve summarization in a single LM\ncall per task without requiring fine-tuning or pipelining. Experiments on\nmultiple datasets belonging to different domains using ten state-of-the-art\npre-trained models demonstrate that QA-prompting outperforms baseline and other\nstate-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This\nprovides an effective and scalable solution for summarization and highlights\nthe importance of domain-specific question selection for optimal performance.", "AI": {"tldr": "The paper presents QA-prompting, a novel method for long-context summarization using question-answering as a precursor to summary generation, which improves performance without complex techniques.", "motivation": "To address the challenges of long-context summarization due to positional biases in language models.", "method": "QA-prompting utilizes question-answering as an intermediate step before generating summaries, allowing for a more effective extraction of key information without requiring fine-tuning or pipelining.", "result": "Experimental results show that QA-prompting significantly outperforms baseline and state-of-the-art methods, achieving up to 29% improvement in ROUGE scores across various datasets.", "conclusion": "QA-prompting provides an effective and scalable solution for summarization tasks, emphasizing the need for domain-specific question selection.", "key_contributions": ["Introduction of QA-prompting for summarization", "Demonstrated performance improvement over existing methods", "Emphasis on domain-specific question selection", "Simplified approach that avoids fine-tuning and pipelining"], "limitations": "", "keywords": ["Language Models", "summarization", "question-answering", "natural language processing", "QA-prompting"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14350", "pdf": "https://arxiv.org/pdf/2505.14350.pdf", "abs": "https://arxiv.org/abs/2505.14350", "title": "OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning Large Language Models (LLMs) has become increasingly challenging\ndue to their massive scale and associated computational costs.\nParameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as\ncomputational alternatives; however, their implementations still require\nsignificant resources. In this paper, we present OSoRA (Output-Dimension and\nSingular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.\nOSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value\nDecomposition (SVD) with learnable scaling vectors in a unified framework. It\nfirst performs an SVD of pre-trained weight matrices, then optimizes an\noutput-dimension vector during training, while keeping the corresponding\nsingular vector matrices frozen. OSoRA substantially reduces computational\nresource requirements by minimizing the number of trainable parameters during\nfine-tuning. Comprehensive evaluations across mathematical reasoning, common\nsense reasoning, and other benchmarks demonstrate that OSoRA achieves\ncomparable or superior performance to state-of-the-art methods like LoRA and\nVeRA, while maintaining a linear parameter scaling even as the rank increases\nto higher dimensions. Our ablation studies further confirm that jointly\ntraining both the singular values and the output-dimension vector is critical\nfor optimal performance.", "AI": {"tldr": "OSoRA is a novel Parameter-Efficient Fine-Tuning method for Large Language Models that reduces computational costs while maintaining performance.", "motivation": "The rising challenges of fine-tuning Large Language Models due to their scale and computational costs prompted the need for more efficient methodologies.", "method": "OSoRA integrates Singular Value Decomposition with learnable scaling vectors to optimize a low-rank adaptation framework for fine-tuning LLMs, minimizing trainable parameters.", "result": "OSoRA demonstrates comparable or superior performance to state-of-the-art methods like LoRA and VeRA across various benchmarks with reduced computational requirements.", "conclusion": "Jointly training both the singular values and the output-dimension vector is critical for achieving optimal fine-tuning performance with the OSoRA method.", "key_contributions": ["Introduces OSoRA, a novel PEFT method integrating SVD for LLMs.", "Significantly reduces computational costs by minimizing trainable parameters.", "Demonstrates superior performance on multiple reasoning benchmarks compared to existing methods."], "limitations": "", "keywords": ["Large Language Models", "Parameter-Efficient Fine-Tuning", "Low-Rank Adaptation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.14354", "pdf": "https://arxiv.org/pdf/2505.14354.pdf", "abs": "https://arxiv.org/abs/2505.14354", "title": "WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications", "authors": ["Xin Li", "Mengbing Liu", "Li Wei", "Jiancheng An", "Mérouane Debbah", "Chau Yuen"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have achieved impressive results across a broad\narray of tasks, yet their capacity for complex, domain-specific mathematical\nreasoning-particularly in wireless communications-remains underexplored. In\nthis work, we introduce WirelessMathBench, a novel benchmark specifically\ndesigned to evaluate LLMs on mathematical modeling challenges to wireless\ncommunications engineering. Our benchmark consists of 587 meticulously curated\nquestions sourced from 40 state-of-the-art research papers, encompassing a\ndiverse spectrum of tasks ranging from basic multiple-choice questions to\ncomplex equation completion tasks, including both partial and full completions,\nall of which rigorously adhere to physical and dimensional constraints. Through\nextensive experimentation with leading LLMs, we observe that while many models\nexcel in basic recall tasks, their performance degrades significantly when\nreconstructing partially or fully obscured equations, exposing fundamental\nlimitations in current LLMs. Even DeepSeek-R1, the best performer on our\nbenchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%\nsuccess rate in full equation completion. By publicly releasing\nWirelessMathBench along with the evaluation toolkit, we aim to advance the\ndevelopment of more robust, domain-aware LLMs for wireless system analysis and\nbroader engineering applications.", "AI": {"tldr": "WirelessMathBench is a benchmark for evaluating LLMs on mathematical modeling challenges specific to wireless communications, revealing limitations in their domain-specific reasoning capabilities.", "motivation": "To assess the mathematical reasoning abilities of LLMs in the context of wireless communications, an area which remains underexplored despite the advanced capabilities of LLMs in general tasks.", "method": "Introduced WirelessMathBench, a benchmark consisting of 587 questions sourced from 40 research papers, including various tasks on mathematical modeling for wireless communications.", "result": "Leading LLMs perform well on basic recall but struggle significantly with complex tasks, achieving an average accuracy of only 38.05% on the benchmark, with less than 8% success in full equation completion.", "conclusion": "The benchmark seeks to motivate the development of stronger, domain-aware LLMs for wireless communications and engineering tasks and is publicly available for further research.", "key_contributions": ["Creation of WirelessMathBench to evaluate LLMs in wireless communications", "Comprehensive dataset of 587 curated mathematical questions", "Insights into the limitations of current LLMs in complex mathematical reasoning"], "limitations": "Current models excel in simpler tasks but fail in more complex mathematical reasoning, indicating a need for improvement.", "keywords": ["Wireless Communications", "Large Language Models", "Benchmarking", "Mathematical Reasoning", "Engineering Applications"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.14367", "pdf": "https://arxiv.org/pdf/2505.14367.pdf", "abs": "https://arxiv.org/abs/2505.14367", "title": "Dual Decomposition of Weights and Singular Value Low Rank Adaptation", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for\nadapting Large Language Models (LLMs) to downstream tasks, among which Low-rank\nAdaptation (LoRA) represents one of the most widely adopted methodologies.\nHowever, existing LoRA-based approaches exhibit two fundamental limitations:\nunstable training dynamics and inefficient knowledge transfer from pre-trained\nmodels, both stemming from random initialization of adapter parameters. To\novercome these challenges, we propose DuDe, a novel approach that decomposes\nweight matrices into magnitude and direction components, employing Singular\nValue Decomposition (SVD) for principled initialization. Our comprehensive\nevaluation demonstrates DuDe's superior performance and robustness, achieving\nup to 48.35\\% accuracy on MMLU and 62.53\\% ($\\pm$ 1.59) accuracy on GSM8K. Our\ntheoretical analysis and empirical validation collectively demonstrate that\nDuDe's decomposition strategy enhances optimization stability and better\npreserves pre-trained representations, particularly for domain-specific tasks\nrequiring specialized knowledge. The combination of robust empirical\nperformance and rigorous theoretical foundations establishes DuDe as a\nsignificant contribution to PEFT methodologies for LLMs.", "AI": {"tldr": "DuDe is a novel Parameter-Efficient Fine-Tuning approach that improves the training stability and knowledge transfer of Large Language Models (LLMs) using Singular Value Decomposition for initialization.", "motivation": "Address limitations of LoRA-based approaches in training dynamics and knowledge transfer for adapting LLMs to downstream tasks.", "method": "DuDe decomposes weight matrices into magnitude and direction components and uses Singular Value Decomposition (SVD) for principled initialization.", "result": "DuDe achieves improved accuracy, with up to 48.35% on MMLU and 62.53% on GSM8K.", "conclusion": "DuDe enhances optimization stability and preserves pre-trained representations, marking a significant improvement in PEFT for LLMs.", "key_contributions": ["Introduction of DuDe for improved parameter-efficient fine-tuning of LLMs.", "Use of SVD for better initialization of adapter parameters.", "Enhanced performance on domain-specific tasks requiring specialized knowledge."], "limitations": "", "keywords": ["Parameter-Efficient Fine-Tuning", "Low-rank Adaptation", "Large Language Models", "Singular Value Decomposition", "Knowledge Transfer"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14376", "pdf": "https://arxiv.org/pdf/2505.14376.pdf", "abs": "https://arxiv.org/abs/2505.14376", "title": "AutoRev: Automatic Peer Review System for Academic Research Papers", "authors": ["Maitreya Prafulla Chitale", "Ketaki Mangesh Shetye", "Harshit Gupta", "Manav Chaudhary", "Vasudeva Varma"], "categories": ["cs.CL"], "comment": null, "summary": "Generating a review for an academic research paper is a complex task that\nrequires a deep understanding of the document's content and the\ninterdependencies between its sections. It demands not only insight into\ntechnical details but also an appreciation of the paper's overall coherence and\nstructure. Recent methods have predominantly focused on fine-tuning large\nlanguage models (LLMs) to address this challenge. However, they often overlook\nthe computational and performance limitations imposed by long input token\nlengths. To address this, we introduce AutoRev, an Automatic Peer Review System\nfor Academic Research Papers. Our novel framework represents an academic\ndocument as a graph, enabling the extraction of the most critical passages that\ncontribute significantly to the review. This graph-based approach demonstrates\neffectiveness for review generation and is potentially adaptable to various\ndownstream tasks, such as question answering, summarization, and document\nrepresentation. When applied to review generation, our method outperforms SOTA\nbaselines by an average of 58.72% across all evaluation metrics. We hope that\nour work will stimulate further research in applying graph-based extraction\ntechniques to other downstream tasks in NLP. We plan to make our code public\nupon acceptance.", "AI": {"tldr": "AutoRev is an Automatic Peer Review System that utilizes a graph-based approach to enhance review generation for academic research papers, outperforming SOTA baselines significantly.", "motivation": "To address the challenge of generating coherent reviews for academic papers while overcoming the limitations of recent LLMs that focus on fine-tuning without considering long input token lengths.", "method": "The paper introduces a framework that represents academic documents as graphs, enabling the identification and extraction of critical passages for review generation.", "result": "The graph-based approach demonstrates significant effectiveness in review generation, outperforming state-of-the-art methods by an average of 58.72% across evaluation metrics.", "conclusion": "The research encourages further exploration of graph-based extraction techniques in NLP for various downstream applications.", "key_contributions": ["Introduction of AutoRev, a novel automatic peer review system.", "Development of a graph-based method for representing and extracting key information from academic documents.", "Demonstration of superior performance in review generation compared to existing methods."], "limitations": "", "keywords": ["Peer Review", "Graph-based Methods", "NLP", "Large Language Models", "Review Generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14393", "pdf": "https://arxiv.org/pdf/2505.14393.pdf", "abs": "https://arxiv.org/abs/2505.14393", "title": "Editing Across Languages: A Survey of Multilingual Knowledge Editing", "authors": ["Nadir Durrani", "Basel Mousi", "Fahim Dalvi"], "categories": ["cs.CL"], "comment": null, "summary": "While Knowledge Editing has been extensively studied in monolingual settings,\nit remains underexplored in multilingual contexts. This survey systematizes\nrecent research on Multilingual Knowledge Editing (MKE), a growing subdomain of\nmodel editing focused on ensuring factual edits generalize reliably across\nlanguages. We present a comprehensive taxonomy of MKE methods, covering\nparameter-based, memory-based, fine-tuning, and hypernetwork approaches. We\nsurvey available benchmarks,summarize key findings on method effectiveness and\ntransfer patterns, identify challenges in cross-lingual propagation, and\nhighlight open problems related to language anisotropy, evaluation coverage,\nand edit scalability. Our analysis consolidates a rapidly evolving area and\nlays the groundwork for future progress in editable language-aware LLMs.", "AI": {"tldr": "This survey discusses the burgeoning field of Multilingual Knowledge Editing (MKE), systematizing methods and challenges in ensuring factual edits across languages in language models.", "motivation": "To address the gap in multilingual contexts of Knowledge Editing, which has been primarily focused on monolingual settings.", "method": "The paper presents a comprehensive taxonomy of MKE methods, including parameter-based, memory-based, fine-tuning, and hypernetwork approaches, while surveying available benchmarks and key findings on method effectiveness.", "result": "Identified challenges in cross-lingual knowledge propagation, including issues of language anisotropy and limitations in evaluation coverage and edit scalability.", "conclusion": "The analysis supports advancing editable language-aware LLMs and lays the groundwork for future research in MKE.", "key_contributions": ["Comprehensive taxonomy of Multilingual Knowledge Editing methods", "Survey of benchmarks and key findings in MKE", "Identification of challenges and open problems in multilingual context"], "limitations": "The paper highlights ongoing challenges such as cross-lingual propagation issues and edit scalability, indicating that the field is still evolving.", "keywords": ["Multilingual Knowledge Editing", "Knowledge Editing", "Language Models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.14395", "pdf": "https://arxiv.org/pdf/2505.14395.pdf", "abs": "https://arxiv.org/abs/2505.14395", "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language", "authors": ["Seyoung Song", "Seogyeong Jeong", "Eunsu Kim", "Jiho Jin", "Dongkwan Kim", "Jay Shin", "Alice Oh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.", "AI": {"tldr": "MUG-Eval is a framework to evaluate the multilingual text generation capabilities of large language models through conversational tasks, focusing on low-resource languages.", "motivation": "To address the challenges of evaluating LLMs' capabilities, particularly in low-resource languages where traditional assessment methods are limited.", "method": "MUG-Eval transforms existing benchmarks into conversational tasks and measures LLMs' success rates in these tasks as a proxy for evaluating generation accuracy.", "result": "MUG-Eval was applied to 8 LLMs across 30 languages and demonstrated strong correlation with established benchmarks (r > 0.75), allowing standardized comparisons.", "conclusion": "MUG-Eval provides a robust and resource-efficient means of evaluating multilingual generation, applicable to thousands of languages without reliance on specific NLP tools.", "key_contributions": ["Novel framework for multilingual evaluation of LLMs", "Transforms benchmarks into conversational tasks", "Shows strong correlation with established evaluation metrics"], "limitations": "", "keywords": ["Large Language Models", "Multilingual Generation", "Evaluation Framework"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14398", "pdf": "https://arxiv.org/pdf/2505.14398.pdf", "abs": "https://arxiv.org/abs/2505.14398", "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation", "authors": ["Peter Baile Chen", "Yi Zhang", "Dan Roth", "Samuel Madden", "Jacob Andreas", "Michael Cafarella"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Data and code are available at https://peterbaile.github.io/lag/", "summary": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.", "AI": {"tldr": "This paper introduces log-augmented generation (LAG), a framework that enhances large language models by reusing prior task logs at test time to improve reasoning and efficiency.", "motivation": "Large language models struggle to retain and apply reasoning from previous tasks, limiting their performance on new challenges.", "method": "The LAG framework utilizes key-value caches to represent task logs and reuses prior reasoning and computations when new tasks arise, without requiring additional knowledge extraction steps.", "result": "Experiments show that LAG significantly outperforms standard agentic systems and existing memory-based solutions on knowledge- and reasoning-intensive datasets.", "conclusion": "LAG offers a scalable and efficient method to enhance reasoning capabilities in large language models by leveraging past experiences directly at test time.", "key_contributions": ["Introduction of the log-augmented generation (LAG) framework", "Demonstrated performance improvements over standard systems", "Direct reuse of prior reasoning without additional knowledge extraction steps"], "limitations": "", "keywords": ["log-augmented generation", "language models", "machine learning", "human-computer interaction", "reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.14406", "pdf": "https://arxiv.org/pdf/2505.14406.pdf", "abs": "https://arxiv.org/abs/2505.14406", "title": "Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis", "authors": ["Haoming Huang", "Yibo Yan", "Jiahao Huo", "Xin Zou", "Xinfeng Li", "Kun Wang", "Xuming Hu"], "categories": ["cs.CL"], "comment": "18 pages, 6 figures, EMNLP under review", "summary": "Large Language Models (LLMs), despite their remarkable capabilities, are\nhampered by hallucinations. A particularly challenging variant, knowledge\novershadowing, occurs when one piece of activated knowledge inadvertently masks\nanother relevant piece, leading to erroneous outputs even with high-quality\ntraining data. Current understanding of overshadowing is largely confined to\ninference-time observations, lacking deep insights into its origins and\ninternal mechanisms during model training. Therefore, we introduce\nPhantomCircuit, a novel framework designed to comprehensively analyze and\ndetect knowledge overshadowing. By innovatively employing knowledge circuit\nanalysis, PhantomCircuit dissects the internal workings of attention heads,\ntracing how competing knowledge pathways contribute to the overshadowing\nphenomenon and its evolution throughout the training process. Extensive\nexperiments demonstrate PhantomCircuit's effectiveness in identifying such\ninstances, offering novel insights into this elusive hallucination and\nproviding the research community with a new methodological lens for its\npotential mitigation.", "AI": {"tldr": "PhantomCircuit is a framework for analyzing and detecting knowledge overshadowing in Large Language Models (LLMs), which leads to erroneous outputs.", "motivation": "LLMs face challenges from hallucinations, particularly knowledge overshadowing, where one activated knowledge masks another relevant piece, affecting output quality.", "method": "The paper introduces PhantomCircuit, which utilizes knowledge circuit analysis to dissect the functioning of attention heads and trace competing knowledge pathways during the training process.", "result": "PhantomCircuit effectively identifies instances of knowledge overshadowing, providing new insights into its occurrence and evolution.", "conclusion": "This framework offers a new methodological approach for the research community to understand and potentially mitigate knowledge overshadowing in LLMs.", "key_contributions": ["Introduction of the PhantomCircuit framework for analyzing knowledge overshadowing", "Insight into the internal mechanisms of attention heads in LLMs", "Methodological lens for mitigating hallucination in LLMs"], "limitations": "", "keywords": ["knowledge overshadowing", "Large Language Models", "attention mechanisms", "model training", "hallucination"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.14418", "pdf": "https://arxiv.org/pdf/2505.14418.pdf", "abs": "https://arxiv.org/abs/2505.14418", "title": "Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents", "authors": ["Pengzhou Cheng", "Haowen Hu", "Zheng Wu", "Zongru Wu", "Tianjie Ju", "Daizong Ding", "Zhuosheng Zhang", "Gongshen Liu"], "categories": ["cs.CL"], "comment": "25 pages, 10 figures, 12 Tables", "summary": "Graphical user interface (GUI) agents powered by multimodal large language\nmodels (MLLMs) have shown greater promise for human-interaction. However, due\nto the high fine-tuning cost, users often rely on open-source GUI agents or\nAPIs offered by AI providers, which introduces a critical but underexplored\nsupply chain threat: backdoor attacks. In this work, we first unveil that\nMLLM-powered GUI agents naturally expose multiple interaction-level triggers,\nsuch as historical steps, environment states, and task progress. Based on this\nobservation, we introduce AgentGhost, an effective and stealthy framework for\nred-teaming backdoor attacks. Specifically, we first construct composite\ntriggers by combining goal and interaction levels, allowing GUI agents to\nunintentionally activate backdoors while ensuring task utility. Then, we\nformulate backdoor injection as a Min-Max optimization problem that uses\nsupervised contrastive learning to maximize the feature difference across\nsample classes at the representation space, improving flexibility of the\nbackdoor. Meanwhile, it adopts supervised fine-tuning to minimize the\ndiscrepancy between backdoor and clean behavior generation, enhancing\neffectiveness and utility. Extensive evaluations of various agent models in two\nestablished mobile benchmarks show that AgentGhost is effective and generic,\nwith attack accuracy that reaches 99.7\\% on three attack objectives, and shows\nstealthiness with only 1\\% utility degradation. Furthermore, we tailor a\ndefense method against AgentGhost that reduces the attack accuracy to 22.1\\%.\nOur code is available at \\texttt{anonymous}.", "AI": {"tldr": "This paper presents AgentGhost, a framework for conducting stealthy backdoor attacks on multimodal large language model (MLLM)-powered graphical user interface (GUI) agents.", "motivation": "The investigation addresses the security vulnerabilities associated with MLLM-powered GUI agents, particularly focusing on the supply chain threat posed by backdoor attacks due to reliance on open-source agents or APIs.", "method": "The paper introduces AgentGhost, which constructs composite triggers and formulates backdoor injection as a Min-Max optimization problem. It utilizes supervised contrastive learning for maximizing feature differences and supervised fine-tuning for minimizing behavior discrepancies.", "result": "AgentGhost achieves an attack accuracy of 99.7% on three attack objectives with only 1% degradation in utility, showcasing its effectiveness and stealthiness across various agent models.", "conclusion": "The study highlights significant vulnerabilities in MLLM-powered GUI agents and proposes a defense strategy that can reduce attack accuracy substantially to 22.1%.", "key_contributions": ["Introduction of AgentGhost framework for red-teaming backdoor attacks", "Formulation of backdoor injection as Min-Max optimization across various agent models", "Development of a defense method against the proposed attacks"], "limitations": "The research focuses primarily on specific mobile benchmarks and may not generalize to all types of GUI agents or interaction scenarios.", "keywords": ["backdoor attacks", "multimodal large language models", "graphical user interface agents", "AgentGhost", "security vulnerabilities"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2505.14420", "pdf": "https://arxiv.org/pdf/2505.14420.pdf", "abs": "https://arxiv.org/abs/2505.14420", "title": "SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection", "authors": ["Huopu Zhang", "Yanguang Liu", "Mengnan Du"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Predicting earnings surprises through the analysis of earnings conference\ncall transcripts has attracted increasing attention from the financial research\ncommunity. Conference calls serve as critical communication channels between\ncompany executives, analysts, and shareholders, offering valuable\nforward-looking information. However, these transcripts present significant\nanalytical challenges, typically containing over 5,000 words with substantial\nredundancy and industry-specific terminology that creates obstacles for\nlanguage models. In this work, we propose the Sparse Autoencoder for Financial\nRepresentation Enhancement (SAE-FiRE) framework to address these limitations by\nextracting key information while eliminating redundancy. SAE-FiRE employs\nSparse Autoencoders (SAEs) to efficiently identify patterns and filter out\nnoises, and focusing specifically on capturing nuanced financial signals that\nhave predictive power for earnings surprises. Experimental results indicate\nthat the proposed method can significantly outperform comparing baselines.", "AI": {"tldr": "This paper presents a novel framework, SAE-FiRE, for analyzing earnings conference call transcripts to predict earnings surprises by extracting key financial information and minimizing redundancy.", "motivation": "To enhance predictive accuracy in financial outcomes by addressing challenges posed by the complexity and redundancy in earnings call transcripts.", "method": "The framework employs Sparse Autoencoders to identify key patterns in financial transcripts while filtering out noise and redundancy.", "result": "SAE-FiRE significantly outperforms existing baseline methods in predicting earnings surprises.", "conclusion": "The use of Sparse Autoencoders greatly improves distillation of valuable financial signals from complex earnings call data, facilitating better predictions of earnings surprises.", "key_contributions": ["Introduction of SAE-FiRE for financial analysis", "Demonstration of effective noise reduction in transcripts", "Performance improvements over existing predictive models"], "limitations": "", "keywords": ["earnings prediction", "conference calls", "sparse autoencoders", "financial analysis", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.14423", "pdf": "https://arxiv.org/pdf/2505.14423.pdf", "abs": "https://arxiv.org/abs/2505.14423", "title": "Scaling Low-Resource MT via Synthetic Data Generation with LLMs", "authors": ["Ona de Gibert", "Joseph Attieh", "Teemu Vahtola", "Mikko Aulamo", "Zihao Li", "Raúl Vázquez", "Tiancheng Hu", "Jörg Tiedemann"], "categories": ["cs.CL"], "comment": null, "summary": "We investigate the potential of LLM-generated synthetic data for improving\nlow-resource machine translation (MT). Focusing on seven diverse target\nlanguages, we construct a document-level synthetic corpus from English\nEuroparl, and extend it via pivoting to 147 additional language pairs.\nAutomatic and human evaluation confirm its high overall quality. We study its\npractical application by (i) identifying effective training regimes, (ii)\ncomparing our data with the HPLT dataset, and (iii) testing its utility beyond\nEnglish-centric MT. Finally, we introduce SynOPUS, a public repository for\nsynthetic parallel datasets. Our findings show that LLM-generated synthetic\ndata, even when noisy, can substantially improve MT performance for\nlow-resource languages.", "AI": {"tldr": "The paper explores how LLM-generated synthetic data enhances low-resource machine translation across multiple languages.", "motivation": "To improve low-resource machine translation (MT) by leveraging LLM-generated synthetic data, which can enhance performance in translating diverse target languages.", "method": "Constructed a document-level synthetic corpus from English Europarl and expanded it via pivoting to create 147 additional language pair datasets; evaluated the data using automatic and human assessments.", "result": "Both automatic metrics and human evaluations confirmed the high quality of the synthetic data; practical applications and comparisons to existing datasets showed significant improvements in MT performance for low-resource languages.", "conclusion": "LLM-generated synthetic data can meaningfully enhance MT outcomes, even when it contains noise, making it valuable for low-resource language translation.", "key_contributions": ["Introduction of SynOPUS, a public repository for synthetic parallel datasets.", "Demonstration of significant improvements in MT performance using synthetic data for low-resource languages.", "Comparison of synthetic data with established datasets like HPLT to evaluate effectiveness."], "limitations": "The study focuses only on low-resource languages and does not address potential biases in synthetic data generation.", "keywords": ["Machine Translation", "Synthetic Data", "LLM", "Low-resource Languages", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14425", "pdf": "https://arxiv.org/pdf/2505.14425.pdf", "abs": "https://arxiv.org/abs/2505.14425", "title": "From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "comment": "4 pages", "summary": "Instruction-tuned large language models (LLMs) have shown strong performance\non a variety of tasks; however, generalizing from synthetic to human-authored\ninstructions in grounded environments remains a challenge for them. In this\nwork, we study generalization challenges in spatial grounding tasks where\nmodels interpret and translate instructions for building object arrangements on\na $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate\ntheir performance on a benchmark dataset containing both synthetic and\nhuman-written instructions. Our results reveal that while models generalize\nwell on simple tasks, their performance degrades significantly on more complex\ntasks. We present a detailed error analysis of the gaps in instruction\ngeneralization.", "AI": {"tldr": "Study of instruction generalization challenges in spatial grounding tasks for LLMs.", "motivation": "To address the issue of LLMs struggling to generalize from synthetic to human-authored instructions in spatial grounding tasks.", "method": "Fine-tuning LLMs with synthetic instructions and evaluating performance on a benchmark dataset with synthetic and human-written instructions.", "result": "Models perform well on simple tasks but show significant performance degradation on complex tasks.", "conclusion": "A detailed error analysis highlights the gaps in instruction generalization for LLMs, suggesting targeted areas for improvement.", "key_contributions": ["Analysis of generalization challenges in grounded environments", "Benchmarking LLMs with synthetic and human-written instructions", "Identification of performance degradation in complex tasks"], "limitations": "", "keywords": ["large language models", "instruction tuning", "spatial grounding", "generalization", "error analysis"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.14436", "pdf": "https://arxiv.org/pdf/2505.14436.pdf", "abs": "https://arxiv.org/abs/2505.14436", "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility", "summary": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.", "AI": {"tldr": "The paper explores Parametric Knowledge Transfer (PKT) across Large Language Models (LLMs) and introduces two paradigms: Post-Align PKT and Pre-Align PKT, along with a novel solution called LaTen to enhance knowledge transfer across LLMs with varying scales, revealing challenges due to Neural Incompatibility.", "motivation": "The motivation is to enhance knowledge transfer methods across LLMs of different scales, which has potential implications for improving AI applications, especially in HCI and health informatics.", "method": "The paper defines two methods for PKT: Post-Align PKT (PostPKT) and Pre-Align PKT (PrePKT). PostPKT requires fine-tuning after parameter extraction, while PrePKT aims to align the parametric spaces with minimal training steps.", "result": "Experiments across four benchmarks indicated challenges in achieving stable PKT with both PostPKT and PrePKT, highlighting Neural Incompatibility as a significant challenge in effective knowledge transfer.", "conclusion": "The findings suggest that addressing Neural Incompatibility in LLMs is crucial for efficient PKT, guiding future research in parametric architectures of LLMs.", "key_contributions": ["Introduction of Post-Align PKT and Pre-Align PKT paradigms for knowledge transfer.", "Proposal of LaTen for efficient alignment of LLMs' parametric spaces across scales.", "Identification of Neural Incompatibility as a primary challenge in PKT."], "limitations": "The study faces challenges in achieving consistently stable transfer across different scales of LLMs due to parametric structural differences.", "keywords": ["Parametric Knowledge Transfer", "Large Language Models", "Neural Incompatibility"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.14442", "pdf": "https://arxiv.org/pdf/2505.14442.pdf", "abs": "https://arxiv.org/abs/2505.14442", "title": "Creative Preference Optimization", "authors": ["Mete Ismayilzada", "Antonio Laverghetta Jr.", "Simone A. Luchini", "Reet Patel", "Antoine Bosselut", "Lonneke van der Plas", "Roger Beaty"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages", "summary": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.", "AI": {"tldr": "The paper presents Creative Preference Optimization (CrPO), a new method to enhance the creativity of Large Language Models (LLMs) by integrating multiple dimensions of creativity into preference optimization.", "motivation": "Existing methods to improve LLM creativity are often task-specific and do not address the multifaceted nature of creativity.", "method": "The authors introduce CrPO to modularly integrate signals from various creativity dimensions into the preference optimization objective, and they trained models on a new dataset, MuCE, for evaluation.", "result": "The proposed models demonstrate superior performance over strong baselines like GPT-4o, showing improvements in novelty, diversity, surprise, and output quality in both automated and human evaluations.", "conclusion": "Optimizing for creativity using preference frameworks shows significant promise without sacrificing quality, indicating a new direction for LLM enhancement.", "key_contributions": ["Introduction of Creative Preference Optimization (CrPO) as a creative enhancement method for LLMs", "Development of MuCE, a large-scale human preference dataset for creativity assessment", "Demonstration of superior performance in creativity metrics compared to existing models"], "limitations": "", "keywords": ["Creative Preference Optimization", "Large Language Models", "Creativity", "Machine Learning", "Preferences"], "importance_score": 9, "read_time_minutes": 27}}
{"id": "2505.14455", "pdf": "https://arxiv.org/pdf/2505.14455.pdf", "abs": "https://arxiv.org/abs/2505.14455", "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "authors": ["Chihan Huang", "Hao Tang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "AI": {"tldr": "CtrlDiff is a dynamic and controllable semi-autoregressive language model that combines autoregressive dependencies with discrete diffusion. It addresses key limitations of fixed output lengths and control mechanisms in diffusion models by adaptively determining generation block sizes and implementing a classifier-guided control that reduces computational costs.", "motivation": "To overcome the limitations of fixed-length outputs and weak control in existing large diffusion language models, which restrict their effectiveness in practical applications.", "method": "The authors propose CtrlDiff, which uses reinforcement learning to adaptively set the size of generation blocks based on semantics and includes a classifier-guided control for better efficiency and flexibility in text generation.", "result": "Experiments show that CtrlDiff establishes a new benchmark among hybrid diffusion models, significantly improving performance to compete with state-of-the-art autoregressive models and enhancing conditional text generation across various tasks.", "conclusion": "CtrlDiff represents a major step towards more flexible, controllable, and efficient diffusion models for language generation, effectively bridging the gap with autoregressive frameworks.", "key_contributions": ["Introduction of CtrlDiff as a semi-autoregressive framework", "Adaptive determination of generation block sizes using reinforcement learning", "Classifier-guided control mechanism for efficient post-hoc conditioning"], "limitations": "", "keywords": ["diffusion models", "language generation", "semi-autoregressive frameworks", "reinforcement learning", "conditional text generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14464", "pdf": "https://arxiv.org/pdf/2505.14464.pdf", "abs": "https://arxiv.org/abs/2505.14464", "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters", "authors": ["Xiaoyu Tian", "Yunjie Ji", "Haotian Wang", "Shuaiting Chen", "Sitong Zhao", "Yiping Peng", "Han Zhao", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging Face\\footnote{Datasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},\n\\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.", "AI": {"tldr": "This work studies the effectiveness of reasoning data distillation for enhancing language models by constructing datasets from verified outputs of teacher models.", "motivation": "To improve the reasoning capabilities of open-source language models using effective data distillation techniques.", "method": "Conducted a large-scale empirical study using three teacher models to collect data from a corpus of 1.89 million queries and analyzed the resulting datasets.", "result": "AM-Thinking-v1-distilled data shows greater token length diversity and lower perplexity, with the AM-based student model achieving the best scores on several reasoning benchmarks.", "conclusion": "The study demonstrates the significance of high-quality verified reasoning traces in training language models, with datasets made publicly available for further research.", "key_contributions": ["High-quality reasoning traces improve model performance.", "Empirical analysis of three parallel datasets from teacher models.", "Public release of distilled datasets for future research."], "limitations": "", "keywords": ["reasoning", "data distillation", "language models", "machine learning", "NLP"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14467", "pdf": "https://arxiv.org/pdf/2505.14467.pdf", "abs": "https://arxiv.org/abs/2505.14467", "title": "Void in Language Models", "authors": ["Mani Shemiranifar"], "categories": ["cs.CL"], "comment": null, "summary": "Despite advances in transformer-based language models (LMs), a fundamental\nquestion remains largely unanswered: Are all layers activated during inference?\nWe investigate this question by detecting unactivated layers (which we refer to\nas Voids) using a non-trainable and parameter-free adaptive computation method\ncalled L2 Adaptive Computation (LAC). We adapt LAC from its original\nefficiency-focused application to trace activated layers during inference. This\nmethod monitors changes in the L2-norm of activations to identify voids. We\nanalyze layer activation in instruction-tuned LMs across two phases: Prompt\nProcessing (PP), where we trace activated layers for each token in the input\nprompts, and Response Generation (RG), where we trace activated layers for each\ngenerated token. We further demonstrate that distinct layers are activated\nduring these two phases. To show the effectiveness of our method, we evaluated\nthree distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families\non three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a\nzero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an\nimprovement from 69.24 to 71.29 while the model uses only 30% of the layers.\nSimilarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to\n18.36 when using 70% of the layers during both the PP and RG phases. These\nresults show that not all layers contribute equally during inference, and that\nselectively skipping most of them can improve the performance of models on\ncertain tasks.", "AI": {"tldr": "This paper investigates layer activation in transformer-based language models during inference and introduces a method to identify unactivated layers, leading to improved model performance by selectively skipping void layers.", "motivation": "To understand whether all layers in transformer-based language models are activated during inference and improve performance based on this understanding.", "method": "The study employs a non-trainable and parameter-free technique called L2 Adaptive Computation (LAC) to track layer activations during two phases: Prompt Processing and Response Generation.", "result": "The experiments show that by skipping unactivated layers (voids), significant performance gains were achieved in multiple instruction-tuned models, with improved scores in various benchmarks while using a reduced number of layers.", "conclusion": "The findings suggest that not all layers in transformer models contribute equally, and selective activation can enhance task performance.", "key_contributions": ["Introduction of L2 Adaptive Computation for layer activation tracking", "Demonstration of performance improvements by skipping void layers", "Analysis of layer activation during different inference phases"], "limitations": "", "keywords": ["transformer models", "layer activation", "adaptive computation", "language models", "inference"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.14469", "pdf": "https://arxiv.org/pdf/2505.14469.pdf", "abs": "https://arxiv.org/abs/2505.14469", "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations", "authors": ["Somnath Banerjee", "Pratyush Chatterjee", "Shanu Kumar", "Sayan Layek", "Parag Agrawal", "Rima Hazra", "Animesh Mukherjee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.", "AI": {"tldr": "This study examines the safety concerns of LLMs with code-mixed prompts, revealing increased harmful outputs compared to monolingual prompts.", "motivation": "To address the safety concerns regarding LLMs' performance with code-mixed inputs, which have emerged from recent advancements.", "method": "The study employs explainability methods to analyze internal attribution shifts in LLMs when exposed to code-mixed versus monolingual prompts.", "result": "The investigation highlights a notable increase in unsafe outputs from LLMs with code-mixed prompts, in addition to distinctions between universally and culturally-specific unsafe queries.", "conclusion": "Understanding these dynamics is crucial for enhancing LLM safety and application in diverse linguistic contexts.", "key_contributions": ["Investigates the unique impact of code-mixed prompts on LLM safety.", "Distinguishes between universally unsafe and culturally-specific unsafe queries.", "Provides experimental insights into the internal behaviors of LLMs.led"], "limitations": "Focus on code-mixed inputs may not universally generalize to all LLM applications.", "keywords": ["LLMs", "code-mixed prompts", "safety concerns", "explainability", "cultural dimensions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14471", "pdf": "https://arxiv.org/pdf/2505.14471.pdf", "abs": "https://arxiv.org/abs/2505.14471", "title": "Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning", "authors": ["Tong Li", "Jiachuan Wang", "Yongqi Zhang", "Shuangyin Li", "Lei Chen"], "categories": ["cs.CL"], "comment": "Manuscripts, accepted to KDD 2025", "summary": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss", "AI": {"tldr": "The paper presents Citss, a framework that uses self-supervised contrastive learning to improve citation classification by adapting pretrained language models while addressing data scarcity and contextual challenges.", "motivation": "Citation classification is essential for scholarly analysis, but fine-tuning pretrained language models is complicated by data scarcity and contextual noise.", "method": "The Citss framework employs self-supervised contrastive learning, utilizing sentence-level cropping and keyphrase perturbation to generate contrastive pairs from citation data.", "result": "Experiments show that Citss outperforms previous state-of-the-art methods using both encoder-based pretrained language models and decoder-based large language models across three benchmark datasets.", "conclusion": "Citss effectively enhances citation classification performance by addressing challenges through innovative self-supervised techniques and is applicable to various PLMs.", "key_contributions": ["Introduction of self-supervised contrastive learning for citation classification", "Development of sentence-level cropping and keyphrase perturbation strategies", "Compatibility with both encoder-based PLMs and decoder-based LLMs"], "limitations": "", "keywords": ["citation classification", "self-supervised learning", "pretrained language models"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.14481", "pdf": "https://arxiv.org/pdf/2505.14481.pdf", "abs": "https://arxiv.org/abs/2505.14481", "title": "PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models", "authors": ["He Zhu", "Junyou Su", "Minxi Chen", "Wen Wang", "Yijie Deng", "Guanhua Chen", "Wenjia Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "In the field of urban planning, existing Vision-Language Models (VLMs)\nfrequently fail to effectively analyze and evaluate planning maps, despite the\ncritical importance of these visual elements for urban planners and related\neducational contexts. Planning maps, which visualize land use, infrastructure\nlayouts, and functional zoning, require specialized understanding of spatial\nconfigurations, regulatory requirements, and multi-scale analysis. To address\nthis challenge, we introduce PlanGPT-VL, the first domain-specific\nVision-Language Model tailored specifically for urban planning maps. PlanGPT-VL\nemploys three innovative approaches: (1) PlanAnno-V framework for high-quality\nVQA data synthesis, (2) Critical Point Thinking to reduce hallucinations\nthrough structured verification, and (3) comprehensive training methodology\ncombining Supervised Fine-Tuning with frozen vision encoder parameters. Through\nsystematic evaluation on our proposed PlanBench-V benchmark, we demonstrate\nthat PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs\nin specialized planning map interpretation tasks, offering urban planning\nprofessionals a reliable tool for map analysis, assessment, and educational\napplications while maintaining high factual accuracy. Our lightweight 7B\nparameter model achieves comparable performance to models exceeding 72B\nparameters, demonstrating efficient domain specialization without sacrificing\nperformance.", "AI": {"tldr": "PlanGPT-VL is a Vision-Language Model specifically designed for urban planning maps, aiming to improve interpretation and analysis in this specialized field.", "motivation": "Existing Vision-Language Models struggle to effectively analyze urban planning maps, which are crucial for urban planners and educational contexts, necessitating specialized models.", "method": "The paper introduces PlanGPT-VL, implementing three innovative approaches: the PlanAnno-V framework for data synthesis, Critical Point Thinking for hallucination reduction, and a training methodology that involves Supervised Fine-Tuning of a frozen vision encoder.", "result": "PlanGPT-VL outperforms general-purpose VLMs in interpreting urban planning maps, offering reliable analysis and educational tools for professionals while achieving efficiency with a lightweight model.", "conclusion": "The study demonstrates that PlanGPT-VL can match the performance of larger models in specialized tasks, highlighting its effectiveness and efficiency in urban planning applications.", "key_contributions": ["Introduction of a domain-specific VLM for urban planning maps", "Implementation of innovative methodologies for data synthesis and hallucination reduction", "High performance achieved with a lightweight model compared to larger counterparts"], "limitations": "", "keywords": ["Vision-Language Model", "urban planning", "map analysis", "machine learning", "data synthesis"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.14483", "pdf": "https://arxiv.org/pdf/2505.14483.pdf", "abs": "https://arxiv.org/abs/2505.14483", "title": "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance", "authors": ["Agam Goyal", "Xianyang Zhan", "Yilun Chen", "Koustuv Saha", "Eshwar Chandrasekharan"], "categories": ["cs.CL"], "comment": "Preprint: 15 pages, 4 figures, 2 tables", "summary": "Large language models (LLMs) have shown great potential in flagging harmful\ncontent in online communities. Yet, existing approaches for moderation require\na separate model for every community and are opaque in their decision-making,\nlimiting real-world adoption. We introduce Mixture of Moderation Experts\n(MoMoE), a modular, cross-community framework that adds post-hoc explanations\nto scalable content moderation. MoMoE orchestrates four operators -- Allocate,\nPredict, Aggregate, Explain -- and is instantiated as seven\ncommunity-specialized experts (MoMoE-Community) and five norm-violation experts\n(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1\nscores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned\nbaselines while consistently producing concise and reliable explanations.\nAlthough community-specialized experts deliver the highest peak accuracy,\nnorm-violation experts provide steadier performance across domains. These\nfindings show that MoMoE yields scalable, transparent moderation without\nneeding per-community fine-tuning. More broadly, they suggest that lightweight,\nexplainable expert ensembles can guide future NLP and HCI research on\ntrustworthy human-AI governance of online communities.", "AI": {"tldr": "MoMoE introduces a modular framework for scalable and explainable content moderation in online communities by using a mixture of community-specialized and norm-violation experts.", "motivation": "Existing content moderation approaches require separate models for each online community and lack transparency, hindering their real-world applicability.", "method": "MoMoE orchestrates four primary operators—Allocate, Predict, Aggregate, and Explain—across seven community-specialized experts and five norm-violation experts to address moderation across multiple subreddits.", "result": "On 30 unseen subreddits, MoMoE variants achieved Micro-F1 scores of 0.72 for community-specialized and 0.67 for norm-violation experts, surpassing strong fine-tuned baselines while providing reliable explanations.", "conclusion": "MoMoE demonstrates that explainable expert ensembles can facilitate scalable and transparent moderation without requiring per-community fine-tuning, indicating its potential in the fields of NLP and HCI for improving human-AI governance.", "key_contributions": ["Introduction of a modular framework for content moderation", "Use of explainable expert ensembles for transparency", "Demonstration of strong performance across various online communities using a unified approach."], "limitations": "Performance may vary by domain despite the framework's overall stability.", "keywords": ["content moderation", "large language models", "explainability", "human-computer interaction", "trustworthy AI governance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.14499", "pdf": "https://arxiv.org/pdf/2505.14499.pdf", "abs": "https://arxiv.org/abs/2505.14499", "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales", "authors": ["Jun Cao", "Jiyi Li", "Ziwei Yang", "Renjie Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.", "AI": {"tldr": "The paper introduces a novel framework, LRSA, for Multimodal Aspect-Based Sentiment Analysis (MABSA) that enhances small language models (SLMs) by incorporating large language models (LLMs) to improve aspect and sentiment detection in multimodal data.", "motivation": "Current methods in MABSA often rely on small language models which have limited capacity, leading to inaccuracies in identifying aspects and sentiments in multimodal data. The paper seeks to improve this by leveraging the strengths of larger models.", "method": "The LRSA framework combines the decision-making abilities of small language models with additional insights from large language models by injecting LLM-generated explanations into SLMs, and employs a dual cross-attention mechanism to enhance feature interaction and fusion.", "result": "Experiments demonstrate that the LRSA framework outperforms baseline models across three widely-used benchmarks, showing its effectiveness in improving sentiment and aspect detection.", "conclusion": "The LRSA approach demonstrates enhanced performance in MABSA through the integration of LLMs, suggesting that it is a viable and beneficial strategy for improving sentiment analysis tasks across multimodal data.", "key_contributions": ["Proposed a novel LRSA framework for MABSA", "Integrated LLM-generated explanations into SLMs", "Showcased superior performance on benchmark datasets"], "limitations": "", "keywords": ["Multimodal", "Aspect-Based Sentiment Analysis", "Large Language Models", "Small Language Models", "Dual Cross-Attention"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14505", "pdf": "https://arxiv.org/pdf/2505.14505.pdf", "abs": "https://arxiv.org/abs/2505.14505", "title": "ModRWKV: Transformer Multimodality in Linear Time", "authors": ["Jiale Kang", "Ziyin Yue", "Qingyu Yin", "Jiang Rui", "Weile Li", "Zening Lu", "Zhouran Ji"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.", "AI": {"tldr": "This paper presents ModRWKV, a decoupled multimodal framework based on RNN architectures that leverages RWKV7 as its backbone, achieving efficient multimodal processing and performance optimization.", "motivation": "To explore the applicability of modern RNN architectures in multimodal contexts, particularly as an alternative to the currently dominant Transformer models.", "method": "The authors propose ModRWKV, a framework utilizing modern RNN architecture for multimodal information fusion, involving dynamically adaptable heterogeneous modality encoders.", "result": "ModRWKV achieves an optimal balance between performance and computational efficiency in multimodal tasks and demonstrates the significance of pretrained RWKV7 weights in enhancing multimodal training.", "conclusion": "Modern RNN architectures can serve as a viable alternative to Transformers for multimodal large language models, with ModRWKV providing a well-optimized solution.", "key_contributions": ["Introduction of a decoupled multimodal framework named ModRWKV based on RNNs.", "Demonstration of the effectiveness of pretrained RWKV7 weights in multimodal training.", "Identification of an optimal configuration for the ModRWKV architecture that maximizes efficiency."], "limitations": "", "keywords": ["multimodal", "RNN", "RWKV", "large language models", "computational efficiency"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.14523", "pdf": "https://arxiv.org/pdf/2505.14523.pdf", "abs": "https://arxiv.org/abs/2505.14523", "title": "Exploring Graph Representations of Logical Forms for Language Modeling", "authors": ["Michael Sullivan"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "To be published in ACL 2025 Findings", "summary": "We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.", "AI": {"tldr": "The paper presents language models over logical forms (LFLMs), specifically introducing GFoLDS, which significantly outperforms textual transformer LMs in data efficiency and learning capability.", "motivation": "The research argues for the advantages of language models that utilize logical forms over traditional textual models due to their data efficiency and ability to learn complex patterns.", "method": "The authors introduce the Graph-based Formal-Logical Distributional Semantics (GFoLDS) prototype, which is a pretrained LM utilizing graph representations of logical forms to demonstrate the efficiency of LFLMs.", "result": "GFoLDS shows strong empirical performance, outperforming traditional transformer models pretrained on similar data amounts in various downstream tasks, proving LFLMs require less data to achieve better learning outcomes.", "conclusion": "The results suggest that LFLMs, through their scalable nature with more parameters and data, are promising for real-world applications in language understanding tasks.", "key_contributions": ["Introduction of the GFoLDS prototype as a new approach to language modeling.", "Demonstration of the superior performance of LFLMs in comparison to textual LMs.", "Evidence supporting the effective learning from limited data using logical forms."], "limitations": "", "keywords": ["language models", "logical forms", "data efficiency", "machine learning", "graph semantics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.14530", "pdf": "https://arxiv.org/pdf/2505.14530.pdf", "abs": "https://arxiv.org/abs/2505.14530", "title": "Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs", "authors": ["Zhipeng Yang", "Junzhuo Li", "Siyu Xia", "Xuming Hu"], "categories": ["cs.CL", "cs.LG"], "comment": "27 pages, 17 figures", "summary": "We show that large language models (LLMs) exhibit an $\\textit{internal\nchain-of-thought}$: they sequentially decompose and execute composite tasks\nlayer-by-layer. Two claims ground our study: (i) distinct subtasks are learned\nat different network depths, and (ii) these subtasks are executed sequentially\nacross layers. On a benchmark of 15 two-step composite tasks, we employ\nlayer-from context-masking and propose a novel cross-task patching method,\nconfirming (i). To examine claim (ii), we apply LogitLens to decode hidden\nstates, revealing a consistent layerwise execution pattern. We further\nreplicate our analysis on the real-world $\\text{TRACE}$ benchmark, observing\nthe same stepwise dynamics. Together, our results enhance LLMs transparency by\nshowing their capacity to internally plan and execute subtasks (or\ninstructions), opening avenues for fine-grained, instruction-level activation\nsteering.", "AI": {"tldr": "This paper investigates how large language models internally execute tasks by decomposing them into subtasks across different layers, confirming this behavior through empirical methods.", "motivation": "To improve the transparency of large language models by understanding their internal processes and how they execute discrete subtasks.", "method": "The study uses layer-from context-masking and a novel cross-task patching method to explore distinct subtask learning at different network depths and applies LogitLens to analyze hidden states for layerwise execution patterns.", "result": "The findings reveal a consistent layerwise execution pattern in LLMs, confirming that distinct subtasks are learned and executed sequentially across layers, as demonstrated on various benchmarks.", "conclusion": "The results enhance our understanding of LLMs' capabilities in planning and executing tasks internally, suggesting implications for instruction-level activation steering.", "key_contributions": ["Demonstration of internal chain-of-thought in LLMs", "Empirical evidence of task decomposition across layers", "Introduction of a novel method for analyzing subtask execution"], "limitations": "", "keywords": ["large language models", "internal chain-of-thought", "task decomposition", "execution patterns", "transparency"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.14536", "pdf": "https://arxiv.org/pdf/2505.14536.pdf", "abs": "https://arxiv.org/abs/2505.14536", "title": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders", "authors": ["Agam Goyal", "Vedant Rathi", "William Yeh", "Yian Wang", "Yuen Chen", "Hari Sundaram"], "categories": ["cs.CL"], "comment": "Preprint: 19 pages, 7 figures, 1 table", "summary": "Large language models (LLMs) are now ubiquitous in user-facing applications,\nyet they still generate undesirable toxic outputs, including profanity,\nvulgarity, and derogatory remarks. Although numerous detoxification methods\nexist, most apply broad, surface-level fixes and can therefore easily be\ncircumvented by jailbreak attacks. In this paper we leverage sparse\nautoencoders (SAEs) to identify toxicity-related directions in the residual\nstream of models and perform targeted activation steering using the\ncorresponding decoder vectors. We introduce three tiers of steering\naggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing\ntrade-offs between toxicity reduction and language fluency. At stronger\nsteering strengths, these causal interventions surpass competitive baselines in\nreducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2\nSmall depending on the aggressiveness. Crucially, standard NLP benchmark scores\nupon steering remain stable, indicating that the model's knowledge and general\nabilities are preserved. We further show that feature-splitting in wider SAEs\nhampers safety interventions, underscoring the importance of disentangled\nfeature learning. Our findings highlight both the promise and the current\nlimitations of SAE-based causal interventions for LLM detoxification, further\nsuggesting practical guidelines for safer language-model deployment.", "AI": {"tldr": "The paper proposes a method for detoxifying large language models using sparse autoencoders to steer their outputs away from toxic content, striving for a balance between toxicity reduction and language fluency.", "motivation": "To address the issue of toxic outputs in large language models, which pose challenges in user-facing applications, and existing methods being easily bypassed.", "method": "The authors leverage sparse autoencoders to identify and steer away from toxicity-related directions in the residual stream of models, implementing three tiers of steering aggressiveness.", "result": "Stronger steering approaches reduced toxicity by up to 20% while maintaining stability in standard NLP benchmark scores; however, fluency issues emerged with higher aggressiveness.", "conclusion": "The study reveals both the potential and limitations of sparse autoencoder interventions for detoxifying language models, proposing guidelines for safer deployment.", "key_contributions": ["Introduced a novel method using sparse autoencoders for targeted detoxification of LLMs.", "Demonstrated the trade-offs between toxicity reduction and language fluency.", "Provided insights on the importance of disentangled feature learning in safety interventions."], "limitations": "Fluency can degrade depending on the aggressiveness of the steering; feature-splitting hampers safety interventions.", "keywords": ["detoxification", "large language models", "sparse autoencoders", "toxicity reduction", "safety interventions"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14552", "pdf": "https://arxiv.org/pdf/2505.14552.pdf", "abs": "https://arxiv.org/abs/2505.14552", "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages", "summary": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "AI": {"tldr": "Introduction of KORGym, a dynamic evaluation platform for assessing reasoning capabilities of LLMs.", "motivation": "To improve evaluation methods for LLMs by addressing limitations of existing domain-specific benchmarks.", "method": "KORGym offers over fifty interactive games for evaluating LLMs' reasoning through multi-turn assessments and reinforcement learning scenarios.", "result": "Extensive experiments with 19 LLMs and 8 VLMs reveal consistent reasoning patterns and superior performance of closed-source models.", "conclusion": "KORGym is expected to advance LLM reasoning research and refine evaluation methodologies for interactive environments.", "key_contributions": ["Introduction of a diverse evaluation platform (KORGym) for LLMs", "Extensive benchmarking across multiple models and scenarios", "Insights into reasoning patterns and performance influences"], "limitations": "", "keywords": ["large language models", "reasoning capabilities", "evaluation methodology", "reinforcement learning", "interactive environments"], "importance_score": 9, "read_time_minutes": 22}}
{"id": "2505.14553", "pdf": "https://arxiv.org/pdf/2505.14553.pdf", "abs": "https://arxiv.org/abs/2505.14553", "title": "Pivot Language for Low-Resource Machine Translation", "authors": ["Abhimanyu Talwar", "Julien Laasri"], "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "7 pages, 3 figures, paper dated May 13, 2019", "summary": "Certain pairs of languages suffer from lack of a parallel corpus which is\nlarge in size and diverse in domain. One of the ways this is overcome is via\nuse of a pivot language. In this paper we use Hindi as a pivot language to\ntranslate Nepali into English. We describe what makes Hindi a good candidate\nfor the pivot. We discuss ways in which a pivot language can be used, and use\ntwo such approaches - the Transfer Method (fully supervised) and\nBacktranslation (semi-supervised) - to translate Nepali into English. Using the\nformer, we are able to achieve a devtest Set SacreBLEU score of 14.2, which\nimproves the baseline fully supervised score reported by (Guzman et al., 2019)\nby 6.6 points. While we are slightly below the semi-supervised baseline score\nof 15.1, we discuss what may have caused this under-performance, and suggest\nscope for future work.", "AI": {"tldr": "This paper explores using Hindi as a pivot language to translate Nepali into English, evaluating two methods: Transfer Method and Backtranslation.", "motivation": "The paper addresses the challenge of translating between languages with limited parallel corpora by leveraging a pivot language.", "method": "The study uses a fully supervised Transfer Method and a semi-supervised Backtranslation approach to translate Nepali to English using Hindi as the pivot.", "result": "The Transfer Method achieved a SacreBLEU score of 14.2, improving upon previous fully supervised scores, while the semi-supervised approach scored 15.1, though the authors discuss reasons for their under-performance.", "conclusion": "The findings highlight the effectiveness of using Hindi as a pivot for translation and identify areas for further research to improve results.", "key_contributions": ["Introduced Hindi as a pivot language for translation between Nepali and English.", "Demonstrated the effectiveness of the Transfer Method with a significant BLEU score improvement.", "Discussed potential reasons for performance gaps and outlined future research directions."], "limitations": "The results are slightly below the semi-supervised baseline score, indicating room for improvement in the methods used.", "keywords": ["pivot language", "translation", "Hindi", "Nepali", "English"], "importance_score": 4, "read_time_minutes": 7}}
{"id": "2505.14577", "pdf": "https://arxiv.org/pdf/2505.14577.pdf", "abs": "https://arxiv.org/abs/2505.14577", "title": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring", "authors": ["Sohaila Eltanbouly", "Salam Albatarni", "Tamer Elsayed"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Findings", "summary": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there\nis a notable lack of attention for assessing essays according to individual\ntraits. In this work, we propose TRATES, a novel trait-specific and\nrubric-based cross-prompt AES framework that is generic yet specific to the\nunderlying trait. The framework leverages a Large Language Model (LLM) that\nutilizes the trait grading rubrics to generate trait-specific features\n(represented by assessment questions), then assesses those features given an\nessay. The trait-specific features are eventually combined with generic\nwriting-quality and prompt-specific features to train a simple classical\nregression model that predicts trait scores of essays from an unseen prompt.\nExperiments show that TRATES achieves a new state-of-the-art performance across\nall traits on a widely-used dataset, with the generated LLM-based features\nbeing the most significant.", "AI": {"tldr": "TRATES is a novel Automated Essay Scoring framework that assesses essays based on individual traits using LLM-generated features.", "motivation": "There is a lack of focus on assessing essays according to individual traits in existing Automated Essay Scoring systems.", "method": "TRATES integrates a Large Language Model with trait grading rubrics to extract trait-specific features from essays, which are combined with generic writing-quality features to train a regression model for predicting trait scores.", "result": "TRATES achieves state-of-the-art performance across all traits on a widely-used dataset, demonstrating the significance of LLM-generated features.", "conclusion": "The results indicate that combining generic and trait-specific features can enhance the accuracy of essay scoring systems.", "key_contributions": ["Introduction of a trait-specific AES framework", "Utilization of LLM for generating trait-specific features", "State-of-the-art performance on benchmark dataset"], "limitations": "", "keywords": ["Automated Essay Scoring", "Trait assessment", "Large Language Model"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.14582", "pdf": "https://arxiv.org/pdf/2505.14582.pdf", "abs": "https://arxiv.org/abs/2505.14582", "title": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning", "authors": ["Shangziqi Zhao", "Jiahao Yuan", "Guisong Yang", "Usman Naseem"], "categories": ["cs.CL"], "comment": "17 pages,4 figures", "summary": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies -- targeting entire chains, core reasoning, and verification -- we\nfind that pruning verification steps yields consistent accuracy gains while\nreducing inference cost, outperforming token-level baselines and uncompressed\nfine-tuning. In contrast, pruning reasoning or all-chain steps degrades\nperformance, revealing that small models benefit not from shorter CoTs, but\nfrom semantically leaner ones. Our findings highlight pruning as a structural\noptimization strategy for aligning CoT reasoning with SLM capacity.", "AI": {"tldr": "Long-CoT reasoning can improve LLM accuracy, but its verbosity complicates distillation into small models. This paper proposes Prune-on-Logic, a method that effectively prunes low-utility reasoning steps while enhancing accuracy and reducing costs, presenting a structural optimization for aligning reasoning with model capacity.", "motivation": "To improve the distillation of Long-CoT reasoning into small language models by exploring the effects of pruning on reasoning steps.", "method": "A framework called Prune-on-Logic is introduced, which transforms Long-CoT into logic graphs and prunes reasoning steps under self-verification constraints.", "result": "Pruning verification steps leads to consistent accuracy gains and reduced inference costs, outperforming other approaches, while pruning reasoning or entire chains results in degraded performance.", "conclusion": "Pruning is a productive optimization strategy for aligning long chain-of-thought reasoning with the capabilities of small language models, emphasizing that shorter chains are not always beneficial.", "key_contributions": ["Development of the Prune-on-Logic framework for reasoning step pruning.", "Demonstration of improved accuracy and cost efficiency through strategic pruning of verification steps.", "Insights into the effectiveness of semantic structure over length in chain-of-thought reasoning."], "limitations": "", "keywords": ["Long-CoT", "LLM", "pruning", "self-verification", "structure-aware"], "importance_score": 7, "read_time_minutes": 17}}
{"id": "2505.14585", "pdf": "https://arxiv.org/pdf/2505.14585.pdf", "abs": "https://arxiv.org/abs/2505.14585", "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning", "authors": ["Wenbin Hu", "Haoran Li", "Huihao Jing", "Qi Hu", "Ziqian Zeng", "Sirui Han", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.", "AI": {"tldr": "The paper addresses safety and privacy risks in Large Language Models (LLMs) by framing these issues as contextualized compliance problems and employing a reinforcement learning approach to enhance compliance and reasoning capabilities.", "motivation": "LLMs have significant safety and privacy risks that current mitigation strategies do not adequately address, often compromising contextual reasoning.", "method": "The authors apply reinforcement learning (RL) with a rule-based reward system, aligned with Contextual Integrity (CI) theory, to improve compliance with regulations like GDPR and HIPAA while enhancing contextual reasoning.", "result": "The proposed method achieves a +17.64% improvement in legal compliance benchmarks and enhances reasoning capabilities, with OpenThinker-7B showing +2.05% and +8.98% accuracy improvements on MMLU and LegalBench, respectively.", "conclusion": "Incorporating RL in the compliance framework not only boosts legal adherence but also fortifies reasoning abilities in LLMs.", "key_contributions": ["Formulation of safety and privacy issues as contextualized compliance problems using CI theory.", "Implementation of RL with a rule-based reward to enhance compliance and reasoning.", "Demonstrated accuracy improvements in AI models for safety/privacy benchmarks and reasoning tasks."], "limitations": "", "keywords": ["Large Language Models", "safety", "privacy", "Contextual Integrity", "reinforcement learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14590", "pdf": "https://arxiv.org/pdf/2505.14590.pdf", "abs": "https://arxiv.org/abs/2505.14590", "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol", "authors": ["Huihao Jing", "Haoran Li", "Wenbin Hu", "Qi Hu", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "17 pages", "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps.Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance.", "AI": {"tldr": "The paper proposes a framework to enhance safety in the Model Context Protocol (MCP) by addressing its vulnerabilities, introducing the Model Contextual Integrity Protocol (MCIP), and developing a taxonomy to evaluate unsafe behaviors in MCP interactions.", "motivation": "The introduction of MCP has created a new ecosystem for users and developers, but its decentralized nature poses significant safety risks that need to be systematically analyzed and addressed.", "method": "A novel framework guided by the MAESTRO framework is proposed, which involves analyzing the missing safety mechanisms in MCP, developing a fine-grained taxonomy for unsafe behaviors, and creating benchmark and training data to evaluate the safety performance of LLMs in MCP interactions.", "result": "Experiments on state-of-the-art LLMs reveal vulnerabilities in handling MCP interactions and demonstrate that the proposed framework and training data significantly improve the safety performance of these models.", "conclusion": "The proposed Model Contextual Integrity Protocol (MCIP) and accompanying mechanisms effectively enhance the safety of the Model Context Protocol framework and improve LLMs' ability to identify and manage safety risks.", "key_contributions": ["Introduction of Model Contextual Integrity Protocol (MCIP)", "Development of a fine-grained taxonomy for unsafe behaviors in MCP", "Creation of benchmark and training data for evaluating LLMs' safety in MCP interactions."], "limitations": "", "keywords": ["Model Context Protocol", "safety mechanisms", "LLMs", "taxonomies", "benchmark data"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2505.14597", "pdf": "https://arxiv.org/pdf/2505.14597.pdf", "abs": "https://arxiv.org/abs/2505.14597", "title": "Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals", "authors": ["Xianzhen Luo", "Qingfu Zhu", "Zhiming Zhang", "Mingzheng Xu", "Tianhao Cheng", "Yixuan Wang", "Zheng Chu", "Shijie Xuyang", "Zhiyuan Ma", "YuanTao Fan", "Wanxiang Che"], "categories": ["cs.CL"], "comment": "Code & Model is https://github.com/Luowaterbi/CTF-Instruct", "summary": "Code Sensitivity refers to the ability of Code LLMs to recognize and respond\nto details changes in problem descriptions. While current code benchmarks and\ninstruction data focus on difficulty and diversity, sensitivity is overlooked.\nWe first introduce the CTF-Code benchmark, constructed using counterfactual\nperturbations, minimizing input changes while maximizing output changes. The\nevaluation shows that many LLMs have a more than 10\\% performance drop compared\nto the original problems. To fully utilize sensitivity, CTF-Instruct, an\nincremental instruction fine-tuning framework, extends on existing data and\nuses a selection mechanism to meet the three dimensions of difficulty,\ndiversity, and sensitivity. Experiments show that LLMs fine-tuned with\nCTF-Instruct data achieve over a 2\\% improvement on CTF-Code, and more than a\n10\\% performance boost on LiveCodeBench, validating the feasibility of\nenhancing LLMs' sensitivity to improve performance.", "AI": {"tldr": "This paper introduces the CTF-Code benchmark to evaluate code sensitivity of LLMs and proposes CTF-Instruct, an incremental instruction fine-tuning framework to enhance LLMs' performance based on sensitivity metrics.", "motivation": "To address the overlooked capability of Code LLMs in recognizing subtle changes in problem descriptions, which impacts their performance.", "method": "The CTF-Code benchmark was created using counterfactual perturbations, focusing on minimizing input changes while maximizing output changes. CTF-Instruct framework was developed for fine-tuning LLMs by incorporating sensitivity metrics along with traditional difficulty and diversity measures.", "result": "LLMs fine-tuned with the CTF-Instruct data showed over a 2% improvement on the CTF-Code benchmark and more than a 10% increase on LiveCodeBench, demonstrating effective enhancement of sensitivity.", "conclusion": "Enhancing LLMs' sensitivity through targeted fine-tuning can significantly improve their performance on coding tasks.", "key_contributions": ["Introduction of the CTF-Code benchmark for evaluating code sensitivity in LLMs.", "Development of the CTF-Instruct fine-tuning framework for improving LLM performance based on sensitivity metrics.", "Demonstration of significant performance improvements through experiments on diverse coding benchmarks."], "limitations": "", "keywords": ["Code LLMs", "Code Sensitivity", "CTF-Code", "CTF-Instruct", "Fine-tuning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.14599", "pdf": "https://arxiv.org/pdf/2505.14599.pdf", "abs": "https://arxiv.org/abs/2505.14599", "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models", "authors": ["Guangzhi Xiong", "Eric Xie", "Corey Williams", "Myles Kim", "Amir Hassan Shariatmadari", "Sikun Guo", "Stefan Bekiranov", "Aidong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IJCAI 2025", "summary": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.", "AI": {"tldr": "This paper introduces TruthHypo, a benchmark for assessing the capabilities of large language models (LLMs) in generating truthful biomedical hypotheses, alongside KnowHD, a hallucination detector to evaluate groundedness in these hypotheses.", "motivation": "To address the challenges of evaluating the truthfulness of biomedical hypotheses generated by LLMs, which may often lead to inefficiencies and inaccuracies due to hallucinations.", "method": "The authors propose TruthHypo, a benchmark for assessing LLM capabilities, and KnowHD, a detector for knowledge-based hallucinations, using them to analyze and filter hypotheses generated by LLMs based on groundedness scores.", "result": "The study reveals that LLMs commonly generate untruthful hypotheses and highlights that KnowHD effectively filters truthful hypotheses, validated through human evaluations.", "conclusion": "The findings underscore the limitations of LLMs in hypothesis generation, emphasizing the importance of tools like KnowHD for enhancing the reliability of AI-generated scientific ideas.", "key_contributions": ["Introduction of TruthHypo benchmark for assessing LLMs in generating biomedical hypotheses", "Development of KnowHD, a knowledge-based hallucination detector", "Demonstration of KnowHD's efficacy in filtering truthful hypotheses based on groundedness scores."], "limitations": "The study is limited to biomedical hypotheses and may not generalize across other domains; additional validation and testing with broader datasets are required.", "keywords": ["large language models", "truthful biomedical hypotheses", "hallucination detection", "scientific discovery", "KnowHD"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14607", "pdf": "https://arxiv.org/pdf/2505.14607.pdf", "abs": "https://arxiv.org/abs/2505.14607", "title": "sudoLLM : On Multi-role Alignment of Language Models", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Joy Mahapatra", "Utpal Garain"], "categories": ["cs.CL", "cs.CR", "I.2.7"], "comment": "Under review. Code and data to be released later", "summary": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs.", "AI": {"tldr": "This paper introduces sudoLLM, a framework for multi-role aligned LLMs that incorporates user authorization-based access control to improve safety and prevent jailbreaking.", "motivation": "There is a lack of user authorization-based access privileges in large language models (LLMs), which are crucial for safety-critical systems.", "method": "The sudoLLM framework injects user-based biases into LLM queries and trains the model to respond with sensitive information only if the user is authorized.", "result": "Empirical results show that sudoLLM significantly improves model alignment and generalization, while also providing resistance to prompt-based jailbreaking attacks.", "conclusion": "The integration of user authorization with LLMs enhances safety and serves as an additional layer of security alongside existing safety mechanisms.", "key_contributions": ["Introduction of user authorization in LLMs through sudoLLM framework", "Improved alignment and generalization in LLMs", "Enhanced safety against prompt-based attacks"], "limitations": "", "keywords": ["Large Language Models", "User Authorization", "Safety-Critical Systems", "Access Control", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.14608", "pdf": "https://arxiv.org/pdf/2505.14608.pdf", "abs": "https://arxiv.org/abs/2505.14608", "title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)", "authors": ["Rafael Rivera Soto", "Barry Chen", "Nicholas Andrews"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection.", "AI": {"tldr": "This paper examines the challenges of detecting machine-generated text and presents a robust detection method using stylistic features, which remain effective even against optimized models.", "motivation": "To address concerns about the inherent difficulty of reliably detecting machine-generated text and to evaluate recent claims about machine-text detectors' vulnerabilities.", "method": "The study explores the robustness of a stylistic feature space for detecting machine-generated text and introduces AURA, a metric for analyzing the overlap between human and machine-generated distributions as sample sizes increase.", "result": "The findings indicate that stylistic detectors maintain consistent performance even when models are optimized against them. Furthermore, detection performance improves as more samples are analyzed, allowing for better differentiation between human and machine-generated text.", "conclusion": "The results suggest that reliance on machine-text detection is problematic and that stylistic features offer a more robust detection approach, particularly as sample size increases.", "key_contributions": ["Introduction of a stylistic feature space that proves resilient to optimization efforts by language models.", "Development of the AURA metric for evaluating detection performance based on sample sizes.", "Demonstration that stylistic detectors maintain their effectiveness against specially optimized machine-generated texts."], "limitations": "The study primarily focuses on stylistic features and may not address other potential methods of detection, limiting the scope of its applicability.", "keywords": ["machine-generated text", "text detection", "stylistic features", "AURA metric", "paraphrasing approach"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.14617", "pdf": "https://arxiv.org/pdf/2505.14617.pdf", "abs": "https://arxiv.org/abs/2505.14617", "title": "Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models", "authors": ["Sahar Abdelnabi", "Ahmed Salem"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation.", "AI": {"tldr": "This paper investigates the influence of test awareness on the behavior and safety alignment of reasoning-focused large language models, introducing a framework for probing and controlling this effect.", "motivation": "The study addresses the phenomenon where large language models alter their performance when aware of being evaluated, which can lead to unsafe behaviors and test optimization.", "method": "A white-box probing framework is developed to identify and steer test awareness in models while monitoring their performance on tasks.", "result": "The study shows that test awareness significantly affects safety alignment in different reasoning LLMs, varying across models and task settings.", "conclusion": "The findings underscore the importance of understanding test awareness in model evaluation to enhance safety and trust in AI systems.", "key_contributions": ["First quantitative study on the impact of test awareness in LLMs", "Development of a probing framework for test awareness control", "Demonstration of varied safety alignment effects across models"], "limitations": "The study is limited to specific state-of-the-art open-source reasoning LLMs and might not generalize to all models.", "keywords": ["Large Language Models", "Test Awareness", "Safety Alignment", "AI Evaluation", "Reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.14631", "pdf": "https://arxiv.org/pdf/2505.14631.pdf", "abs": "https://arxiv.org/abs/2505.14631", "title": "Think Only When You Need with Large Hybrid-Reasoning Models", "authors": ["Lingjie Jiang", "Xun Wu", "Shaohan Huang", "Qingxiu Dong", "Zewen Chi", "Li Dong", "Xingxing Zhang", "Tengchao Lv", "Lei Cui", "Furu Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.", "AI": {"tldr": "This paper presents Large Hybrid-Reasoning Models (LHRMs) that adaptively select reasoning methods based on user query context, improving efficiency in processing queries.", "motivation": "To address the inefficiencies in token usage and response latency caused by extended reasoning in Large Reasoning Models (LRMs) when handling simple queries.", "method": "The authors propose a two-stage training pipeline combining Hybrid Fine-Tuning for a cold start and online reinforcement learning using Hybrid Group Policy Optimization (HGPO) to learn adaptive reasoning strategies.", "result": "LHRMs can effectively adapt their reasoning approach based on query complexity, outperforming existing LLMs and LRMs in efficiency and reasoning capabilities.", "conclusion": "The study emphasizes the need to balance reasoning depth with efficiency and offers a framework for developing future hybrid reasoning systems.", "key_contributions": ["Introduction of Large Hybrid-Reasoning Models (LHRMs) that adapt reasoning based on query context.", "Development of a two-stage training pipeline combining Hybrid Fine-Tuning and reinforcement learning.", "Proposal of a new metric, Hybrid Accuracy, to evaluate hybrid thinking capabilities."], "limitations": "", "keywords": ["Hybrid Reasoning Models", "Large Language Models", "Reinforcement Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14633", "pdf": "https://arxiv.org/pdf/2505.14633.pdf", "abs": "https://arxiv.org/abs/2505.14633", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "AI": {"tldr": "The paper introduces LitmusValues, a pipeline for evaluating AI models' value priorities as a method to predict and assess AI risks, particularly in unsafe scenarios.", "motivation": "The emergence of stronger AI models and techniques like Alignment Faking necessitates better methods to detect AI risks. Identifying underlying values in AI can serve as an early warning system for risky behaviors.", "method": "The authors developed a pipeline called LitmusValues to evaluate the values prioritized by AI models. They created AIRiskDilemmas, a set of scenarios that test the AI's value prioritization against potential safety risks.", "result": "The study found that the values identified through LitmusValues could predict both known risky behaviors in AIRiskDilemmas and previously unseen risky behaviors in another dataset, HarmBench.", "conclusion": "Understanding AI models' value prioritization can provide significant insights into predicting and mitigating their risky behaviors.", "key_contributions": ["Introduction of LitmusValues evaluation pipeline", "Creation of AIRiskDilemmas for testing value prioritization", "Demonstration of value metrics predicting risky AI behaviors"], "limitations": "The effectiveness and comprehensiveness of the value classes in LitmusValues might be limited by the diversity and representation of the dilemmas used for evaluation.", "keywords": ["AI safety", "value prioritization", "risk detection", "machine learning", "human values"], "importance_score": 7, "read_time_minutes": 30}}
{"id": "2505.14652", "pdf": "https://arxiv.org/pdf/2505.14652.pdf", "abs": "https://arxiv.org/abs/2505.14652", "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "authors": ["Xueguang Ma", "Qian Liu", "Dongfu Jiang", "Ge Zhang", "Zejun Ma", "Wenhu Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.", "AI": {"tldr": "This paper presents General-Reasoner, a novel paradigm for enhancing LLM reasoning capabilities across various domains by utilizing a large-scale dataset and a generative model for answer verification.", "motivation": "To address the limitations of current LLM reasoning approaches that mainly focus on limited domains like mathematics and coding, thereby restricting generalization to diverse question-answer representations.", "method": "Developing a large-scale, high-quality dataset of diverse questions with verifiable answers through web crawling, and implementing a generative model-based answer verifier to enable robust reasoning capabilities.", "result": "General-Reasoner outperforms existing baseline methods across 12 benchmarks, demonstrating strong and generalizable reasoning performance, particularly in mathematical reasoning tasks.", "conclusion": "The proposed General-Reasoner significantly enhances LLM reasoning across a breadth of disciplines, moving beyond traditional narrow domain applicability.", "key_contributions": ["Construction of a large-scale, high-quality dataset of diverse questions and answers.", "Development of a generative model-based answer verifier for enhanced reasoning capabilities."], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Reasoning", "Dataset Curation", "Generative Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.14660", "pdf": "https://arxiv.org/pdf/2505.14660.pdf", "abs": "https://arxiv.org/abs/2505.14660", "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding", "authors": ["Ronald Seoh", "Dan Goldwasser"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.", "AI": {"tldr": "EmoGist is a training-free method for visual emotion classification using context-dependent emotion labels to improve prediction accuracy.", "motivation": "To improve the accuracy of emotion classification in images by utilizing context-dependent definitions of emotion labels.", "method": "EmoGist pre-generates explanations of emotion labels by analyzing clusters of example images. At test time, it retrieves context-specific explanations based on embedding similarity and uses them for classification with a visual language model (VLM).", "result": "EmoGist improves micro F1 scores by up to 13 points on the multi-label Memotion dataset and macro F1 scores by up to 8 points on the multi-class FI dataset.", "conclusion": "The EmoGist method demonstrates significant improvements in emotion classification through context-aware explanations, showing potential for better performance in visual emotion recognition tasks.", "key_contributions": ["Introduction of a training-free in-context learning method for emotion classification.", "Use of context-dependent emotion label explanations to enhance classification accuracy.", "Demonstrated performance improvements on benchmark emotion classification datasets."], "limitations": "", "keywords": ["visual emotion classification", "context-dependent labels", "in-context learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.14674", "pdf": "https://arxiv.org/pdf/2505.14674.pdf", "abs": "https://arxiv.org/abs/2505.14674", "title": "Reward Reasoning Model", "authors": ["Jiaxin Guo", "Zewen Chi", "Li Dong", "Qingxiu Dong", "Xun Wu", "Shaohan Huang", "Furu Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.", "AI": {"tldr": "This paper introduces Reward Reasoning Models (RRMs) that enhance reward model performance through chain-of-thought reasoning and adaptive use of test-time compute.", "motivation": "To improve the performance of reward models in guiding large language models by utilizing test-time compute effectively.", "method": "The authors develop RRMs within a reinforcement learning framework, enabling them to reason and generate rewards without needing explicit training data for reasoning.", "result": "RRMs show superior performance on various reward modeling benchmarks, effectively utilizing additional test-time compute to enhance reward accuracy.", "conclusion": "The adaptive nature of RRMs allows for improved reward modeling, making them a promising approach for aligning outputs with human expectations.", "key_contributions": ["Introduction of Reward Reasoning Models (RRMs) for enhanced reward modeling", "Use of chain-of-thought reasoning for better performance", "Demonstration of adaptive exploitation of test-time compute in reward generation."], "limitations": "", "keywords": ["Reward Models", "Reinforcement Learning", "Chain-of-Thought Reasoning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.14679", "pdf": "https://arxiv.org/pdf/2505.14679.pdf", "abs": "https://arxiv.org/abs/2505.14679", "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models", "authors": ["Xiaojie Gu", "Guangxu Chen", "Jungang Li", "Jia-Chen Gu", "Xuming Hu", "Kai Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.", "AI": {"tldr": "ULTRAEDIT is a new model editing solution for large language models that enables efficient and scalable lifelong learning without requiring training or memory, achieving significant speed and resource efficiency improvements.", "motivation": "The need for efficient lifelong learning in large language models that can adapt to new information while preserving existing knowledge and ensuring reliable performance.", "method": "ULTRAEDIT performs model editing using lightweight linear algebra operations to compute parameter shifts and employs a lifelong normalization strategy to adapt to distributional shifts.", "result": "ULTRAEDIT is over 7x faster than previous methods with only 1/3 the VRAM usage, able to edit a 7B LLM on a consumer-grade GPU, and supports up to 1M edits while maintaining high accuracy.", "conclusion": "ULTRAEDIT demonstrates superior performance in model editing across multiple datasets and models, addressing practical challenges in lifelong adaptation.", "key_contributions": ["Development of ULTRAEDIT, a training-, subject- and memory-free model editing solution.", "Creation of ULTRAEDITBENCH, the largest editing dataset with over 2M pairs.", "Achieving 7x faster editing speeds and reduced VRAM consumption compared to current state-of-the-art methods."], "limitations": "", "keywords": ["lifelong learning", "model editing", "large language models"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.14684", "pdf": "https://arxiv.org/pdf/2505.14684.pdf", "abs": "https://arxiv.org/abs/2505.14684", "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "authors": ["Haolei Xu", "Yuchen Yan", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Shengpei Jiang", "Kaitao Song", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.", "AI": {"tldr": "This paper introduces CoT Thought Leap Bridge Task to address missing steps in Chain-of-Thought reasoning for LLMs by automatically detecting leaps and generating intermediate steps, using a specialized dataset for training.", "motivation": "Existing mathematical CoT datasets suffer from Thought Leaps caused by omitted intermediate reasoning steps, negatively impacting model performance.", "method": "The authors propose the CoT Thought Leap Bridge Task and construct the ScaleQM+ dataset to train the CoT-Bridge model, which generates missing reasoning steps and enhances dataset completeness.", "result": "Models fine-tuned on bridged datasets outperform those trained on original datasets by up to +5.87% on NuminaMath and show improvements in distilled data and reinforcement learning applications.", "conclusion": "Enhancing reasoning completeness improves model generalization and performance on diverse reasoning tasks, with CoT-Bridge functioning as a useful module for various optimization techniques.", "key_contributions": ["Introduction of CoT Thought Leap Bridge Task", "Creation of ScaleQM+ dataset for training", "Demonstrated improvement in model performance on mathematical reasoning tasks"], "limitations": "", "keywords": ["Chain-of-Thought", "Language Models", "Mathematical Reasoning", "Dataset Creation", "Cohesion"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.14685", "pdf": "https://arxiv.org/pdf/2505.14685.pdf", "abs": "https://arxiv.org/abs/2505.14685", "title": "Language Models use Lookbacks to Track Beliefs", "authors": ["Nikhil Prakash", "Natalie Shapira", "Arnab Sen Sharma", "Christoph Riedl", "Yonatan Belinkov", "Tamar Rott Shaham", "David Bau", "Atticus Geiger"], "categories": ["cs.CL"], "comment": "32 pages, 32 figures. Code and data at https://belief.baulab.info/", "summary": "How do language models (LMs) represent characters' beliefs, especially when\nthose beliefs may differ from reality? This question lies at the heart of\nunderstanding the Theory of Mind (ToM) capabilities of LMs. We analyze\nLlama-3-70B-Instruct's ability to reason about characters' beliefs using causal\nmediation and abstraction. We construct a dataset that consists of simple\nstories where two characters each separately change the state of two objects,\npotentially unaware of each other's actions. Our investigation uncovered a\npervasive algorithmic pattern that we call a lookback mechanism, which enables\nthe LM to recall important information when it becomes necessary. The LM binds\neach character-object-state triple together by co-locating reference\ninformation about them, represented as their Ordering IDs (OIs) in low rank\nsubspaces of the state token's residual stream. When asked about a character's\nbeliefs regarding the state of an object, the binding lookback retrieves the\ncorresponding state OI and then an answer lookback retrieves the state token.\nWhen we introduce text specifying that one character is (not) visible to the\nother, we find that the LM first generates a visibility ID encoding the\nrelation between the observing and the observed character OIs. In a visibility\nlookback, this ID is used to retrieve information about the observed character\nand update the observing character's beliefs. Our work provides insights into\nthe LM's belief tracking mechanisms, taking a step toward reverse-engineering\nToM reasoning in LMs.", "AI": {"tldr": "This paper investigates how language models represent characters' beliefs, focusing on the Theory of Mind (ToM) capabilities of Llama-3-70B-Instruct.", "motivation": "Understanding how language models represent beliefs is crucial for advancing their ToM capabilities and improving interaction dynamics in AI applications.", "method": "The authors analyze the LM's reasoning through causal mediation and abstraction, constructing a dataset of stories with two characters altering states of objects. They identify a lookback mechanism for binding and retrieving character beliefs and state information.", "result": "The study reveals that the LM uses a lookback mechanism to recall essential information about the characters' beliefs and actions effectively, incorporating visibility relations for belief updates.", "conclusion": "The findings provide insights into the LM's mechanisms for tracking beliefs, contributing to the understanding of ToM reasoning in language models.", "key_contributions": ["Introduced a lookback mechanism in LMs for belief tracking.", "Developed a unique dataset for evaluating character belief reasoning.", "Examined the influence of visibility on character beliefs."], "limitations": "", "keywords": ["language models", "Theory of Mind", "belief representation", "lookback mechanism"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2312.10097", "pdf": "https://arxiv.org/pdf/2312.10097.pdf", "abs": "https://arxiv.org/abs/2312.10097", "title": "Arithmetics-Based Decomposition of Numeral Words -- Arithmetic Conditions give the Unpacking Strategy", "authors": ["Isidor Konrad Maier", "Matthias Wolff"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a novel numeral decomposer based on arithmetic criteria.\nThe criteria are not dependent on a base-10 assumption but only on Hurford's\nPacking Strategy. Hurford's Packing Strategy constitutes numerals by packing\nfactors and summands to multiplicators. We found out that a numeral of value n\nhas a multiplicator larger than sqrt(n), a summand smaller than n/2 and a\nfactor smaller than sqrt(n). Using these findings, the numeral decomposer\nattempts to detect and unpack factors and summand in order to reverse Hurford's\nPacking strategy. We tested its applicability for incremental unsupervised\ngrammar induction in 273 languages. This way, grammars were obtained with\nsensible mathematical attributes that explain the structure of produced\nnumerals. The numeral-decomposer-induced grammars are often close to\nexpert-made and more compact than numeral grammars induced by a modern\nstate-of-the-art grammar induction tool. Furthermore, this paper contains a\nreport about the few cases of incorrect induced mathematical attributes, which\nare often linked to linguistic peculiarities like context sensitivity.", "AI": {"tldr": "The paper introduces a numeral decomposer based on Hurford's Packing Strategy, which decomposes numerals using specific arithmetic criteria and applies it to grammar induction in 273 languages.", "motivation": "To explore a novel method for numeral decomposition that does not rely on base-10 assumptions, enhancing the understanding of numeral structures in linguistics.", "method": "The methodology involves unpacking numerals into their factors and summands based on criteria derived from Hurford's Packing Strategy, followed by applying this decomposer for grammar induction in various languages.", "result": "The resultant grammars from the numeral decomposer exhibit sensible mathematical attributes, are often close to expert-made grammars, and are more compact than those induced by a state-of-the-art tool.", "conclusion": "The study demonstrates the potential of the numeral decomposer for effective grammar induction while also addressing some cases of incorrect mathematical attribute induction linked to context sensitivity.", "key_contributions": ["Introduction of a numeral decomposer based on Hurford's Packing Strategy.", "Demonstration of effective grammar induction in 273 languages using the decomposer.", "Comparison of induced grammars showing improvements over state-of-the-art tools."], "limitations": "Some incorrect mathematical attributes were identified, often related to linguistic peculiarities like context sensitivity.", "keywords": ["numeral decomposition", "grammar induction", "Hurford's Packing Strategy", "linguistic attributes", "arithmetic criteria"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2403.10056", "pdf": "https://arxiv.org/pdf/2403.10056.pdf", "abs": "https://arxiv.org/abs/2403.10056", "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning", "authors": ["Yongquan He", "Wenyuan Zhang", "Xuancheng Huang", "Peng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 6 figures", "summary": "Instruction tuning for large language models (LLMs) can drive them to produce\nresults consistent with human goals in specific downstream tasks. However, the\nprocess of continual instruction tuning (CIT) for LLMs may bring about the\ncatastrophic forgetting (CF) problem, where previously learned abilities are\ndegraded. Recent methods try to alleviate the CF problem by modifying models or\nreplaying data, which may only remember the surface-level pattern of\ninstructions and get confused on held-out tasks. In this paper, we propose a\nnovel continual instruction tuning method based on Key-part Information Gain\n(KPIG). Our method computes the information gain on masked parts to dynamically\nreplay data and refine the training objective, which enables LLMs to capture\ntask-aware information relevant to the correct response and alleviate\noverfitting to general descriptions in instructions. In addition, we propose\ntwo metrics, P-score and V-score, to measure the generalization and\ninstruction-following abilities of LLMs. Experiments demonstrate our method\nachieves superior performance on both seen and held-out tasks.", "AI": {"tldr": "This paper proposes a continual instruction tuning method for large language models that alleviates catastrophic forgetting by evaluating key-part information gain to improve data replay and refine training objectives.", "motivation": "To address the issues of catastrophic forgetting in continual instruction tuning for large language models, ensuring models retain previous knowledge while adapting to new tasks.", "method": "The authors introduce a method based on Key-part Information Gain (KPIG) to dynamically replay data and adjust training objectives, allowing models to focus on task-relevant information.", "result": "Experiments indicate that the proposed method outperforms existing approaches, demonstrating improved performance in both seen and unseen tasks.", "conclusion": "The KPIG-based method shows promise in enhancing continual instruction tuning by mitigating catastrophic forgetting and improving model generalization and instruction-following capabilities.", "key_contributions": ["Novel approach using Key-part Information Gain for continual instruction tuning", "Development of P-score and V-score metrics for evaluating model performance", "Empirical results demonstrating improved performance over existing methods"], "limitations": "", "keywords": ["continual instruction tuning", "catastrophic forgetting", "large language models", "information gain", "performance metrics"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2404.17662", "pdf": "https://arxiv.org/pdf/2404.17662.pdf", "abs": "https://arxiv.org/abs/2404.17662", "title": "PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games", "authors": ["Qinglin Zhu", "Runcong Zhao", "Bin Liang", "Jinhua Du", "Lin Gui", "Yulan He"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce WellPlay, a reasoning dataset for multi-agent conversational\ninference in Murder Mystery Games (MMGs). WellPlay comprises 1,482 inferential\nquestions across 12 games, spanning objectives, reasoning, and relationship\nunderstanding, and establishes a systematic benchmark for evaluating agent\nreasoning abilities in complex social settings. Building on this foundation, we\npresent PLAYER*, a novel framework for Large Language Model (LLM)-based agents\nin MMGs. MMGs pose unique challenges, including undefined state spaces, absent\nintermediate rewards, and the need for strategic reasoning through natural\nlanguage. PLAYER* addresses these challenges with a sensor-based state\nrepresentation and an information-driven strategy that optimises questioning\nand suspect pruning. Experiments show that PLAYER* outperforms existing methods\nin reasoning accuracy, efficiency, and agent-human interaction, advancing\nreasoning agents for complex social scenarios.", "AI": {"tldr": "Introduction of WellPlay dataset and PLAYER* framework for reasoning in murder mystery games.", "motivation": "To create a robust benchmark for evaluating reasoning abilities of agents in multi-agent conversational settings and to improve reasoning accuracy and interaction in MMGs.", "method": "Development of WellPlay dataset consisting of 1,482 inferential questions and PLAYER*, a framework utilizing sensor-based state representation and information-driven strategies for agent operations.", "result": "PLAYER* demonstrates improved reasoning accuracy and efficiency compared to existing methods, enhancing agent-human interactions in MMGs.", "conclusion": "Journey toward advancing reasoning capabilities of agents in complex social interactions through improved benchmarks and frameworks.", "key_contributions": ["Creation of the WellPlay reasoning dataset", "Introduction of PLAYER* framework for LLM agents", "Demonstrated performance improvements in reasoning accuracy and efficiency"], "limitations": "", "keywords": ["Multi-agent systems", "Conversation inference", "Large Language Models", "Murder Mystery Games", "Reasoning agents"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2407.01082", "pdf": "https://arxiv.org/pdf/2407.01082.pdf", "abs": "https://arxiv.org/abs/2407.01082", "title": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs", "authors": ["Minh Nguyen", "Andrew Baker", "Clement Neo", "Allen Roush", "Andreas Kirsch", "Ravid Shwartz-Ziv"], "categories": ["cs.CL"], "comment": "In line with ICLR/Openreview changes + better overall reading flow.\n  https://iclr.cc/virtual/2025/poster/30358", "summary": "Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. Popular\nsampling methods like top-p (nucleus sampling) often struggle to balance\nquality and diversity, especially at higher temperatures which lead to\nincoherent or repetitive outputs. We propose min-p sampling, a dynamic\ntruncation method that adjusts the sampling threshold based on the model's\nconfidence by using the top token's probability as a scaling factor. Our\nexperiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative\nWriting show that min-p sampling improves both the quality and diversity of\ngenerated text across different model families (Mistral and Llama 3) and model\nsizes (1B to 123B parameters), especially at higher temperatures. Human\nevaluations further show a clear preference for min-p sampling, in both text\nquality and creativity. Min-p sampling has been adopted by popular open-source\nLLM frameworks, including Hugging Face Transformers, VLLM, and many others,\nhighlighting its considerable impact on improving text generation quality.", "AI": {"tldr": "This paper introduces min-p sampling, a method that improves the quality and diversity of text generated by LLMs by dynamically adjusting sampling thresholds based on model confidence.", "motivation": "The paper addresses the limitations of existing sampling methods like top-p (nucleus sampling) in generating coherent and diverse text, especially at higher temperatures.", "method": "Min-p sampling dynamically truncates the sampling threshold according to the model’s confidence, utilizing the probability of the top token as a scaling factor.", "result": "Experiments demonstrate that min-p sampling enhances both text quality and diversity across various benchmarks and model families, with human evaluations favoring this method.", "conclusion": "Min-p sampling significantly impacts text generation by improving coherence and creativity, and has been adopted in major open-source LLM frameworks.", "key_contributions": ["Introduction of min-p sampling as a new sampling method for LLMs", "Demonstrated improvements in text quality and diversity across different benchmarks", "Adoption of min-p sampling in popular open-source frameworks"], "limitations": "", "keywords": ["min-p sampling", "large language models", "text generation", "sampling methods", "AI creativity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2407.18416", "pdf": "https://arxiv.org/pdf/2407.18416.pdf", "abs": "https://arxiv.org/abs/2407.18416", "title": "PersonaGym: Evaluating Persona Agents and LLMs", "authors": ["Vinay Samuel", "Henry Peng Zou", "Yue Zhou", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Ameet Deshpande", "Karthik Narasimhan", "Vishvak Murahari"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "21 pages, 5 figures", "summary": "Persona agents, which are LLM agents conditioned to act according to an\nassigned persona, enable contextually rich and user aligned interactions across\ndomains like education and healthcare. However, evaluating how faithfully these\nagents adhere to their personas remains a significant challenge, particularly\nin free-form settings that demand consistency across diverse, persona-relevant\nenvironments. We introduce PersonaGym, the first dynamic evaluation framework\nfor persona agents, and PersonaScore, a human-aligned automatic metric grounded\nin decision theory that enables comprehensive large-scale evaluation. Our\nevaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals\nsignificant advancement opportunities. For example, GPT-4.1 had the exact same\nPersonaScore as LLaMA-3-8b despite being a more recent and advanced closed\nsource model. Importantly, increased model size and complexity do not\nnecessarily enhance persona agent capabilities, underscoring the need for\nalgorithmic and architectural innovation toward faithful, performant persona\nagents.", "AI": {"tldr": "Introduction of PersonaGym framework and PersonaScore metric for evaluating persona agents using LLMs.", "motivation": "Address the challenge of evaluating how persona agents adhere to their assigned personas in various domains, particularly in free-form interaction settings.", "method": "Developed the PersonaGym framework and PersonaScore metric grounded in decision theory to enable large-scale evaluation of persona agents.", "result": "Evaluated 10 leading LLMs across 200 personas and 10,000 questions, showing similar PersonaScores for models like GPT-4.1 and LLaMA-3-8b despite differences in model sophistication.", "conclusion": "Incremental improvements in model size do not guarantee better persona adherence, highlighting the need for innovative algorithmic and architectural solutions.", "key_contributions": ["Introduction of PersonaGym as an evaluation framework for persona agents", "Development of PersonaScore as an automatic metric based on decision theory", "Large-scale evaluation revealing parity in performance across different models"], "limitations": "", "keywords": ["persona agents", "evaluation framework", "language models", "healthcare", "education"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.00054", "pdf": "https://arxiv.org/pdf/2409.00054.pdf", "abs": "https://arxiv.org/abs/2409.00054", "title": "Automating Intervention Discovery from Scientific Literature: A Progressive Ontology Prompting and Dual-LLM Framework", "authors": ["Yuting Hu", "Dancheng Liu", "Qingyun Wang", "Charles Yu", "Chenhui Xu", "Qingxiao Zheng", "Heng Ji", "Jinjun Xiong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCAI2025", "summary": "Identifying effective interventions from the scientific literature is\nchallenging due to the high volume of publications, specialized terminology,\nand inconsistent reporting formats, making manual curation laborious and prone\nto oversight. To address this challenge, this paper proposes a novel framework\nleveraging large language models (LLMs), which integrates a progressive\nontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo. On\nthe one hand, the POP algorithm conducts a prioritized breadth-first search\n(BFS) across a predefined ontology, generating structured prompt templates and\naction sequences to guide the automatic annotation process. On the other hand,\nthe LLM-Duo system features two specialized LLM agents, an explorer and an\nevaluator, working collaboratively and adversarially to continuously refine\nannotation quality. We showcase the real-world applicability of our framework\nthrough a case study focused on speech-language intervention discovery.\nExperimental results show that our approach surpasses advanced baselines,\nachieving more accurate and comprehensive annotations through a fully automated\nprocess. Our approach successfully identified 2,421 interventions from a corpus\nof 64,177 research articles in the speech-language pathology domain,\nculminating in the creation of a publicly accessible intervention knowledge\nbase with great potential to benefit the speech-language pathology community.", "AI": {"tldr": "This paper introduces a framework utilizing large language models to automate the identification and annotation of scientific interventions in speech-language pathology, enhancing efficiency and accuracy.", "motivation": "The paper addresses the challenges of identifying effective scientific interventions due to the overwhelming volume of literature and inconsistent reporting, aiming to streamline this process through automation.", "method": "The proposed framework combines a Progressive Ontology Prompting (POP) algorithm with a dual-agent LLM system (LLM-Duo) that includes an explorer and an evaluator working together to improve annotation quality.", "result": "The framework was applied in a case study for speech-language interventions, where it successfully identified 2,421 interventions from 64,177 articles, outperforming existing baselines in accuracy and comprehensiveness.", "conclusion": "The approach demonstrates significant potential for the speech-language pathology community by creating a publicly accessible intervention knowledge base, making it easier to access vetted interventions.", "key_contributions": ["Introduction of a novel framework integrating large language models for annotation tasks", "Development of the Progressive Ontology Prompting algorithm for structured prompting", "Creation of a knowledge base from a large corpus of literature in speech-language pathology."], "limitations": "", "keywords": ["large language models", "speech-language pathology", "automated annotation", "ontology prompting", "dual-agent system"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2409.11074", "pdf": "https://arxiv.org/pdf/2409.11074.pdf", "abs": "https://arxiv.org/abs/2409.11074", "title": "RoMath: A Mathematical Reasoning Benchmark in Romanian", "authors": ["Adrian Cosma", "Ana-Maria Bucur", "Emilian Radoi"], "categories": ["cs.CL", "cs.AI"], "comment": "5 Figures, 11 Tables", "summary": "Mathematics has long been conveyed through natural language, primarily for\nhuman understanding. With the rise of mechanized mathematics and proof\nassistants, there is a growing need to understand informal mathematical text,\nyet most existing benchmarks focus solely on English, overlooking other\nlanguages. This paper introduces RoMath, a Romanian mathematical reasoning\nbenchmark suite comprising three subsets: Baccalaureate, Competitions and\nSynthetic, which cover a range of mathematical domains and difficulty levels,\naiming to improve non-English language models and promote multilingual AI\ndevelopment. By focusing on Romanian, a low-resource language with unique\nlinguistic features, RoMath addresses the limitations of Anglo-centric models\nand emphasizes the need for dedicated resources beyond simple automatic\ntranslation. We benchmark several open-weight language models, highlighting the\nimportance of creating resources for underrepresented languages. Code and\ndatasets are be made available.", "AI": {"tldr": "The paper introduces RoMath, a Romanian mathematical reasoning benchmark suite to enhance multilingual AI development and support low-resource language models.", "motivation": "To address the lack of benchmarks for understanding informal mathematical text in non-English languages, particularly Romanian, and to promote the development of multilingual AI resources.", "method": "The paper presents RoMath, which consists of three subsets—Baccalaureate, Competitions, and Synthetic—covering various mathematical domains and difficulty levels. The performance of several open-weight language models is benchmarked on these subsets.", "result": "The benchmarking highlights the performance disparities of existing models on Romanian mathematical texts, emphasizing the need for dedicated resources for low-resource languages.", "conclusion": "RoMath serves as a vital resource for improving non-English language models, underscoring the importance of developing dedicated tools for underrepresented languages.", "key_contributions": ["Introduction of the RoMath benchmark suite for Romanian language", "Focus on multilingual AI development", "Highlighting the need for resources beyond automatic translation"], "limitations": "", "keywords": ["multilingual AI", "mathematics", "Romanian language", "language models", "benchmarking"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2409.11726", "pdf": "https://arxiv.org/pdf/2409.11726.pdf", "abs": "https://arxiv.org/abs/2409.11726", "title": "Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing", "authors": ["Wenyuan Zhang", "Shuaiyi Nie", "Jiawei Sheng", "Zefeng Zhang", "Xinghua Zhang", "Yongquan He", "Tingwen Liu"], "categories": ["cs.CL", "cs.HC"], "comment": "25 pages, 6 figures, 20 tables", "summary": "Large language model (LLM) role-playing has gained widespread attention.\nAuthentic character knowledge is crucial for constructing realistic LLM\nrole-playing agents. However, existing works usually overlook the exploration\nof LLMs' ability to detect characters' known knowledge errors (KKE) and unknown\nknowledge errors (UKE) while playing roles, which would lead to low-quality\nautomatic construction of character trainable corpus. In this paper, we propose\nRoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The\nresults indicate that even the latest LLMs struggle to detect these two types\nof errors effectively, especially when it comes to familiar knowledge. We\nexperimented with various reasoning strategies and propose an agent-based\nreasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore\nfurther the potential for improving error detection capabilities. Experiments\nshow that our method effectively improves the LLMs' ability to detect error\ncharacter knowledge, but it remains an issue that requires ongoing attention.", "AI": {"tldr": "This paper introduces RoleKE-Bench, a benchmark for evaluating large language models' (LLMs) ability to detect knowledge errors in role-playing scenarios, and proposes a novel reasoning method to enhance error detection.", "motivation": "Character knowledge is essential for creating realistic LLM role-playing agents, yet current models struggle to identify known and unknown knowledge errors, which limits the quality of generated character data.", "method": "The paper presents RoleKE-Bench for assessing LLMs' error detection abilities regarding known knowledge errors (KKE) and unknown knowledge errors (UKE). It introduces a new reasoning strategy, Self-Recollection and Self-Doubt (S^2RD), to improve detection capabilities.", "result": "Results show that contemporary LLMs have significant difficulties in identifying KKE and UKE, particularly related to familiar knowledge. The proposed S^2RD method demonstrably enhances the LLMs' error detection performance.", "conclusion": "While the S^2RD method improves error detection in LLMs, the challenges in accurately detecting knowledge errors highlight the need for continued research in this area.", "key_contributions": ["Introduction of RoleKE-Bench for evaluating error detection in LLMs", "Proposal of the Self-Recollection and Self-Doubt (S^2RD) method to enhance error detection", "Empirical results showing the limitations of existing LLMs in detecting KKE and UKE"], "limitations": "The study identifies ongoing challenges in error detection for LLMs that require further exploration beyond initial improvements.", "keywords": ["large language models", "role-playing agents", "knowledge errors", "error detection", "reasoning strategies"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2410.03663", "pdf": "https://arxiv.org/pdf/2410.03663.pdf", "abs": "https://arxiv.org/abs/2410.03663", "title": "Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review", "authors": ["Zhuochun Li", "Yuelyu Ji", "Rui Meng", "Daqing He"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 5 figures", "summary": "While reasoning capabilities typically emerge in large language models (LLMs)\nwith tens of billions of parameters, recent research focuses on improving\nsmaller open-source models through knowledge distillation (KD) from commercial\nLLMs. However, many of these studies rely solely on responses from a single LLM\nas the gold rationale, unlike the natural human learning process, which\ninvolves understanding both the correct answers and the reasons behind\nmistakes. In this paper, we introduce a novel Fault-Aware DistIllation via\nPeer-Review (FAIR) approach: 1) instead of merely obtaining rationales from\nteachers, our method asks teachers to identify and explain the student's\nmistakes, providing customized instruction learning data; 2) we design a\nsimulated peer-review process between teacher LLMs, and selects only the\ngenerated rationales above the acceptance threshold, which reduces the chance\nof teachers guessing correctly with flawed rationale, improving instructional\ndata quality. Comprehensive experiments and analysis on mathematical,\ncommonsense, and logical reasoning tasks demonstrate the effectiveness of our\nmethod. Our code is available at\nhttps://github.com/zhuochunli/Learn-from-Committee.", "AI": {"tldr": "This paper presents a novel Fault-Aware DistIllation via Peer-Review (FAIR) approach to improve smaller open-source models by focusing on customized instruction data through peer-reviewed rationales from teacher LLMs.", "motivation": "To enhance the reasoning capabilities of smaller models via knowledge distillation by capturing not only answers but also explanations of mistakes, aligning with natural human learning processes.", "method": "The FAIR approach implements a system where teacher LLMs identify and explain students' errors, and a simulated peer-review process selects the most reliable rationales, improving the quality of instructional data.", "result": "Comprehensive experiments show that FAIR significantly improves performance on tasks related to mathematical, commonsense, and logical reasoning compared to traditional knowledge distillation methods.", "conclusion": "The FAIR method advances the effectiveness of knowledge distillation, providing quality instructional data that mimics human learning and improves reasoning in smaller models.", "key_contributions": ["Introduction of the FAIR approach for knowledge distillation", "Customized instructional data through mistake explanations", "Peer-review process among teacher LLMs to enhance rationale quality"], "limitations": "The study primarily focuses on reasoning tasks and may need further evaluation in diverse application contexts.", "keywords": ["Knowledge Distillation", "Large Language Models", "Reasoning Tasks", "Peer-Review Process", "Human Learning"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2410.10624", "pdf": "https://arxiv.org/pdf/2410.10624.pdf", "abs": "https://arxiv.org/abs/2410.10624", "title": "SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with LLMs for Activity Recognition", "authors": ["Zechen Li", "Shohreh Deldari", "Linyao Chen", "Hao Xue", "Flora D. Salim"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce SensorLLM, a two-stage framework that enables Large Language\nModels (LLMs) to perform human activity recognition (HAR) from wearable sensor\ndata. While LLMs excel at reasoning and generalization, they struggle with\ntime-series inputs due to limited semantic context, numerical complexity, and\nsequence variability. To address these challenges, we construct SensorQA, a\nquestion-answering dataset of human-intuitive sensor-text pairs spanning\ndiverse HAR scenarios. It supervises the Sensor-Language Alignment stage, where\nthe model aligns sensor inputs with trend descriptions. Special tokens are\nintroduced to mark channel boundaries. This alignment enables LLMs to interpret\nnumerical patterns, channel-specific signals, and variable-length\ninputs--without requiring human annotation. In the subsequent Task-Aware Tuning\nstage, we adapt the model for multivariate HAR classification, achieving\nperformance that matches or exceeds state-of-the-art methods. Our results show\nthat, guided by human-intuitive alignment, SensorLLM becomes an effective\nsensor learner, reasoner, and classifier--generalizing across varied HAR\nsettings and paving the way for foundation model research in time-series\nanalysis.", "AI": {"tldr": "SensorLLM is a framework for human activity recognition from wearable sensor data, overcoming challenges faced by LLMs in handling time-series inputs through a two-stage approach.", "motivation": "To enable LLMs to effectively recognize human activities from wearable sensor data, addressing challenges like semantic context and numerical complexity.", "method": "The framework consists of two stages: Sensor-Language Alignment, which aligns sensor data with descriptive text, and Task-Aware Tuning, which adapts the model for multivariate HAR classification.", "result": "SensorLLM matches or exceeds the performance of state-of-the-art multivariate HAR methods, showing effective generalization across various scenarios.", "conclusion": "The introduction of human-intuitive alignment allows SensorLLM to effectively learn from sensor data, making it a significant step for foundation model research in time-series analysis.", "key_contributions": ["Introduction of SensorLLM for HAR using LLMs", "Development of SensorQA dataset for sensor-text alignment", "Achievement of state-of-the-art performance in multivariate HAR classification"], "limitations": "", "keywords": ["Human Activity Recognition", "Large Language Models", "Wearable Sensors", "Time-Series Analysis", "Machine Learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2410.11348", "pdf": "https://arxiv.org/pdf/2410.11348.pdf", "abs": "https://arxiv.org/abs/2410.11348", "title": "RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals", "authors": ["David Reber", "Sean Richardson", "Todd Nief", "Cristina Garbacea", "Victor Veitch"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025. Code at https://github.com/toddnief/RATE", "summary": "Reward models are widely used as proxies for human preferences when aligning\nor evaluating LLMs. However, reward models are black boxes, and it is often\nunclear what, exactly, they are actually rewarding. In this paper we develop\nRewrite-based Attribute Treatment Estimator (RATE) as an effective method for\nmeasuring the sensitivity of a reward model to high-level attributes of\nresponses, such as sentiment, helpfulness, or complexity. Importantly, RATE\nmeasures the causal effect of an attribute on the reward. RATE uses LLMs to\nrewrite responses to produce imperfect counterfactuals examples that can be\nused to measure causal effects. A key challenge is that these rewrites are\nimperfect in a manner that can induce substantial bias in the estimated\nsensitivity of the reward model to the attribute. The core idea of RATE is to\nadjust for this imperfect-rewrite effect by rewriting twice. We establish the\nvalidity of the RATE procedure and show empirically that it is an effective\nestimator.", "AI": {"tldr": "RATE is a method for assessing the sensitivity of reward models used in LLMs to various high-level attributes like sentiment and complexity, addressing the challenge of using imperfect counterfactuals in this measurement.", "motivation": "To understand what reward models are actually rewarding when aligning or evaluating large language models (LLMs), as existing models operate as black boxes with unclear mechanisms.", "method": "RATE employs a rewriting technique on responses to create counterfactual examples, allowing the measurement of the causal effects of high-level attributes on the reward. It utilizes a double rewriting process to mitigate biases from imperfect rewrites.", "result": "The RATE method demonstrates effectiveness as a causal estimator by empirically validating its performance in measuring the sensitivity to attributes while reducing estimation bias.", "conclusion": "The paper establishes that RATE can reliably assess reward model sensitivity, contributing to more transparent evaluations of LLMs.", "key_contributions": ["Development of the RATE methodology for causal effect measurement in reward models", "Introduction of a double rewriting technique to adjust for bias in sensitivity estimation", "Empirical validation of RATE's effectiveness in handling attribute sensitivity."], "limitations": "The potential for biases due to the imperfections in the generated counterfactuals; further studies needed for broader application.", "keywords": ["reward models", "large language models", "causal estimation", "sensitivity analysis", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.12924", "pdf": "https://arxiv.org/pdf/2410.12924.pdf", "abs": "https://arxiv.org/abs/2410.12924", "title": "Interpreting token compositionality in LLMs: A robustness analysis", "authors": ["Nura Aljaafari", "Danilo S. Carvalho", "André Freitas"], "categories": ["cs.CL"], "comment": "23 pages, 3 Figures, 14 tables", "summary": "Understanding the internal mechanisms of large language models (LLMs) is\nintegral to enhancing their reliability, interpretability, and inference\nprocesses. We present Constituent-Aware Pooling (CAP), a methodology designed\nto analyse how LLMs process compositional linguistic structures. Grounded in\nprinciples of compositionality, mechanistic interpretability, and information\ntheory, CAP systematically intervenes in model activations through\nconstituent-based pooling at various model levels. Our experiments on inverse\ndefinition modelling, hypernym and synonym prediction reveal critical insights\ninto transformers' limitations in handling compositional abstractions. No\nspecific layer integrates tokens into unified semantic representations based on\ntheir constituent parts. We observe fragmented information processing, which\nintensifies with model size, suggesting that larger models struggle more with\nthese interventions and exhibit greater information dispersion. This\nfragmentation likely stems from transformers' training objectives and\narchitectural design, preventing systematic and cohesive representations. Our\nfindings highlight fundamental limitations in current transformer architectures\nregarding compositional semantics processing and model interpretability,\nunderscoring the critical need for novel approaches in LLM design to address\nthese challenges.", "AI": {"tldr": "The paper introduces Constituent-Aware Pooling (CAP) to analyze compositional linguistic structures in LLMs, revealing limitations in handling semantic representations, especially in larger models.", "motivation": "To enhance the reliability, interpretability, and inference processes of large language models by understanding their internal mechanisms, particularly how they handle compositional linguistic structures.", "method": "The methodology involves systematic interventions in model activations through constituent-based pooling at different model levels, grounding the approach in compositionality, mechanistic interpretability, and information theory.", "result": "Experiments on inverse definition modelling and semantic prediction tasks reveal that no specific model layer effectively integrates tokens into cohesive semantic representations, leading to fragmented information processing that worsens with model size.", "conclusion": "The study uncovers fundamental limitations in existing transformer architectures regarding compositional semantics, highlighting the need for innovative approaches in LLM design to overcome these challenges.", "key_contributions": ["Introduction of Constituent-Aware Pooling (CAP) methodology", "Insights into the limitations of transformers in processing compositional structures", "Evidence of information fragmentation in larger LLMs"], "limitations": "The approach may not be generalizable across all types of LLMs and relies on specific experimental conditions.", "keywords": ["large language models", "compositionality", "mechanistic interpretability", "information theory", "transformer architecture"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2410.13779", "pdf": "https://arxiv.org/pdf/2410.13779.pdf", "abs": "https://arxiv.org/abs/2410.13779", "title": "The Mystery of the Pathological Path-star Task for Language Models", "authors": ["Arvid Frydenlund"], "categories": ["cs.CL", "cs.LG"], "comment": "EMNLP 2024 Main at https://aclanthology.org/2024.emnlp-main.695/ See\n  'Language Models, Graph Searching, and Supervision Adulteration: When More\n  Supervision is Less and How to Make More More' for a follow-up work", "summary": "The recently introduced path-star task is a minimal task designed to\nexemplify limitations to the abilities of language models (Bachmann and\nNagarajan, 2024). It involves a path-star graph where multiple arms radiate\nfrom a single starting node and each node is unique. Given the start node and a\nspecified target node that ends an arm, the task is to generate the arm\ncontaining that target node. This is straightforward for a human but\nsurprisingly difficult for language models, which did not outperform the random\nbaseline. The authors hypothesized this is due to a deficiency in\nteacher-forcing and the next-token prediction paradigm.\n  We demonstrate the task is learnable using teacher-forcing in alternative\nsettings and that the issue is partially due to representation. We introduce a\nregularization method using structured samples of the same graph but with\ndiffering target nodes, improving results across a variety of model types. We\nprovide RASP proofs showing the task is theoretically solvable. Finally, we\nfind settings where an encoder-only model can consistently solve the task.", "AI": {"tldr": "The path-star task highlights language models' limitations in graph traversal, showing they struggle in generating paths despite human ease; improvements are introduced through teacher-forcing and representation adjustments.", "motivation": "To explore the limitations of language models using the path-star task, which involves generating a graph path between nodes, and address the observed performance issues.", "method": "The authors use teacher-forcing in different setups and introduce a regularization method that uses structured samples of the graph with varying target nodes to improve model performance.", "result": "The proposed methods enhance language models' ability to solve the path-star task and provide theoretical proofs that confirm solvability under certain conditions.", "conclusion": "The findings suggest that modification of training strategies can help language models overcome specific task limitations, particularly through better representation and supervision techniques.", "key_contributions": ["Introduction of the path-star task as a benchmark for language model capabilities.", "Development of a regularization method that improves task performance across various model types.", "Theoretical proof of task solvability under certain conditions."], "limitations": "The study is limited to the context of the path-star task and may not generalize to other types of graph-related tasks.", "keywords": ["language models", "path-star task", "teacher-forcing", "graph traversal", "supervision techniques"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2410.14425", "pdf": "https://arxiv.org/pdf/2410.14425.pdf", "abs": "https://arxiv.org/abs/2410.14425", "title": "Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation", "authors": ["Shuai Zhao", "Xiaobao Wu", "Cong-Duy Nguyen", "Yanhao Jia", "Meihuizi Jia", "Yichao Feng", "Luu Anh Tuan"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct comprehensive experiments on three state-of-the-art\nlarge language models and several different backdoor attack algorithms. Our\nempirical results demonstrate the outstanding performance of W2SDefense in\ndefending against backdoor attacks without compromising model performance.", "AI": {"tldr": "This paper presents W2SDefense, a novel algorithm that enhances the unlearning capability of poisoned large language models (LLMs) to defend against backdoor attacks while maintaining model performance.", "motivation": "Parameter-efficient fine-tuning (PEFT) is vulnerable to malicious backdoor attacks in large language models (LLMs). This research aims to develop a method to mitigate such vulnerabilities.", "method": "W2SDefense utilizes a weak-to-strong unlearning algorithm based on feature alignment knowledge distillation. A clean teacher model is first trained via full-parameter fine-tuning, and then it instructs the poisoned student model to unlearn the backdoor features.", "result": "The empirical results demonstrate that W2SDefense effectively enhances the student model's ability to unlearn backdoor features, successfully preventing the activation of backdoors while preserving model performance.", "conclusion": "W2SDefense shows strong potential as a defense mechanism against backdoor attacks in LLMs, proving effective across multiple models and attack algorithms without degrading overall performance.", "key_contributions": ["Introduction of W2SDefense for unlearning backdoors in LLMs", "Demonstrated effectiveness across three state-of-the-art LLMs", "Maintains model performance while mitigating backdoor risks"], "limitations": "", "keywords": ["backdoor attacks", "parameter-efficient fine-tuning", "large language models", "unlearning", "knowledge distillation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.15522", "pdf": "https://arxiv.org/pdf/2410.15522.pdf", "abs": "https://arxiv.org/abs/2410.15522", "title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings", "authors": ["Srishti Gureja", "Lester James V. Miranda", "Shayekh Bin Islam", "Rishabh Maheshwary", "Drishti Sharma", "Gusti Winata", "Nathan Lambert", "Sebastian Ruder", "Sara Hooker", "Marzieh Fadaee"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 6 figures, 10 tables. Website:\n  https://m-rewardbench.github.io/ , Updated results with latest models. Added\n  more author information", "summary": "Reward models (RMs) have driven the state-of-the-art performance of LLMs\ntoday by enabling the integration of human feedback into the language modeling\nprocess. However, RMs are primarily trained and evaluated in English, and their\ncapabilities in multilingual settings remain largely understudied. In this\nwork, we conduct a systematic evaluation of several reward models in\nmultilingual settings. We first construct the first-of-its-kind multilingual RM\nevaluation benchmark, M-RewardBench, consisting of 2.87k preference instances\nfor 23 typologically diverse languages, that tests the chat, safety, reasoning,\nand translation capabilities of RMs. We then rigorously evaluate a wide range\nof reward models on M-RewardBench, offering fresh insights into their\nperformance across diverse languages. We identify a significant gap in RMs'\nperformances between English and non-English languages and show that RM\npreferences can change substantially from one language to another. We also\npresent several findings on how different multilingual aspects impact RM\nperformance. Specifically, we show that the performance of RMs is improved with\nimproved translation quality. Similarly, we demonstrate that the models exhibit\nbetter performance for high-resource languages. We release M-RewardBench\ndataset and the codebase in this study to facilitate a better understanding of\nRM evaluation in multilingual settings.", "AI": {"tldr": "This paper evaluates reward models (RMs) in multilingual settings using a new benchmark, M-RewardBench, revealing substantial performance gaps across languages.", "motivation": "To understand the performance of reward models in multilingual contexts, as RMs have primarily been evaluated in English and their multilingual capabilities are understudied.", "method": "A systematic evaluation of several reward models was conducted using the M-RewardBench, a benchmark containing 2.87k preference instances across 23 languages, focusing on chat, safety, reasoning, and translation capabilities.", "result": "The evaluation showed a significant performance gap between English and non-English languages, with RM preferences varying extensively across languages. Furthermore, RM performance improved with better translation quality and was generally better for high-resource languages.", "conclusion": "The study highlights the need for better evaluation of reward models in multilingual settings and the impact of translation quality and resource availability on RM performance. The M-RewardBench dataset and codebase are released to support future research.", "key_contributions": ["Introduction of the M-RewardBench multilingual benchmark for evaluating reward models.", "Identification of performance discrepancies in RMs between English and non-English languages.", "Insights on how multilingual factors like translation quality affect RM performance."], "limitations": "The benchmark's scope is limited to 23 typologically diverse languages, which may not represent all languages' RM performance.", "keywords": ["reward models", "multilingual", "human feedback", "language evaluation", "M-RewardBench"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2410.21272", "pdf": "https://arxiv.org/pdf/2410.21272.pdf", "abs": "https://arxiv.org/abs/2410.21272", "title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics", "authors": ["Yaniv Nikankin", "Anja Reusch", "Aaron Mueller", "Yonatan Belinkov"], "categories": ["cs.CL", "68T5", "I.2.7"], "comment": null, "summary": "Do large language models (LLMs) solve reasoning tasks by learning robust\ngeneralizable algorithms, or do they memorize training data? To investigate\nthis question, we use arithmetic reasoning as a representative task. Using\ncausal analysis, we identify a subset of the model (a circuit) that explains\nmost of the model's behavior for basic arithmetic logic and examine its\nfunctionality. By zooming in on the level of individual circuit neurons, we\ndiscover a sparse set of important neurons that implement simple heuristics.\nEach heuristic identifies a numerical input pattern and outputs corresponding\nanswers. We hypothesize that the combination of these heuristic neurons is the\nmechanism used to produce correct arithmetic answers. To test this, we\ncategorize each neuron into several heuristic types-such as neurons that\nactivate when an operand falls within a certain range-and find that the\nunordered combination of these heuristic types is the mechanism that explains\nmost of the model's accuracy on arithmetic prompts. Finally, we demonstrate\nthat this mechanism appears as the main source of arithmetic accuracy early in\ntraining. Overall, our experimental results across several LLMs show that LLMs\nperform arithmetic using neither robust algorithms nor memorization; rather,\nthey rely on a \"bag of heuristics\".", "AI": {"tldr": "This paper investigates how large language models (LLMs) perform arithmetic reasoning, concluding they use a combination of simple heuristics rather than robust algorithms or memorization.", "motivation": "To determine whether LLMs solve reasoning tasks via learned algorithms or by memorization.", "method": "Causal analysis of LLMs focused on identifying circuits responsible for arithmetic logic and analyzing individual circuit neurons' behavior.", "result": "Identified a sparse set of neurons implementing simple heuristics that explain the model's arithmetic accuracy, revealing that LLMs rely on a 'bag of heuristics' for performance.", "conclusion": "The findings suggest that LLMs do not use robust algorithms or purely memorization for arithmetic tasks but a combination of heuristic approaches.", "key_contributions": ["Identification of a circuit in LLMs responsible for arithmetic logic", "Discovery of heuristic types among individual neurons", "Demonstration that heuristic combinations drive arithmetic accuracy"], "limitations": "", "keywords": ["large language models", "arithmetic reasoning", "neural circuits", "heuristics", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2411.02448", "pdf": "https://arxiv.org/pdf/2411.02448.pdf", "abs": "https://arxiv.org/abs/2411.02448", "title": "Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models", "authors": ["Aliyah R. Hsu", "James Zhu", "Zhichao Wang", "Bin Bi", "Shubham Mehrotra", "Shiva K. Pentyala", "Katherine Tan", "Xiang-Bo Mao", "Roshanak Omrani", "Sougata Chaudhuri", "Regunathan Radhakrishnan", "Sitaram Asur", "Claire Na Cheng", "Bin Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have demonstrated impressive proficiency in generating coherent and\nhigh-quality text, making them valuable across a range of text-generation\ntasks. However, rigorous evaluation of this generated content is crucial, as\nensuring its quality remains a significant challenge due to persistent issues\nsuch as factual inaccuracies and hallucination. This paper introduces three\nfine-tuned general-purpose LLM autoevaluators, REC-8B, REC-12B and REC-70B,\nspecifically designed to evaluate generated text across several dimensions:\nfaithfulness, instruction following, coherence, and completeness. These models\nnot only provide ratings for these metrics but also offer detailed explanation\nand verifiable citation, thereby enhancing trust in the content. Moreover, the\nmodels support various citation modes, accommodating different requirements for\nlatency and granularity. Extensive evaluations on diverse benchmarks\ndemonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms\nstate-of-the-art LLMs, excelling in content evaluation by delivering better\nquality explanation and citation with minimal bias. Our REC dataset and models\nare available at https://github.com/adelaidehsu/REC.", "AI": {"tldr": "This paper presents three fine-tuned general-purpose LLM autoevaluators aimed at evaluating generated text for various quality metrics.", "motivation": "To address the challenges of ensuring quality in LLM-generated content due to issues like factual inaccuracies and hallucinations.", "method": "The authors developed three models, REC-8B, REC-12B, and REC-70B, which evaluate text on faithfulness, instruction following, coherence, and completeness, providing ratings along with explanations and verifiable citations.", "result": "Extensive evaluations show that the REC-70B model outperforms existing LLMs in content evaluation with better quality explanations and citations while maintaining minimal bias.", "conclusion": "The models and their dataset are made available for further research and application.", "key_contributions": ["Introduction of fine-tuned LLM autoevaluators for quality assessment of generated text", "Provision of detailed explanations and verifiable citations to enhance trust in generated content", "Demonstration of superior performance in evaluating content quality over state-of-the-art models"], "limitations": "", "keywords": ["LLM autoevaluators", "text generation", "content evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.14318", "pdf": "https://arxiv.org/pdf/2411.14318.pdf", "abs": "https://arxiv.org/abs/2411.14318", "title": "Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training", "authors": ["Zheheng Luo", "Xin Zhang", "Xiao Liu", "Haoling Li", "Yeyun Gong", "Chen Qi", "Peng Cheng"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "It is well-known that a diverse corpus is critical for training large\nlanguage models, which are typically constructed from a mixture of various\ndomains. In general, previous efforts resort to sampling training data from\ndifferent domains with static proportions, as well as adjusting data\nproportions during training. However, few methods have addressed the\ncomplexities of domain-adaptive continual pre-training. To fill this gap, we\npropose Velocitune, a novel framework dynamically assesses learning velocity\nand adjusts data proportions accordingly, favoring slower-learning domains\nwhile shunning faster-learning ones, which is guided by a scaling law to\nindicate the desired learning goal for each domain with less associated cost.\nTo evaluate the effectiveness of Velocitune, we conduct experiments in a\nreasoning-focused dataset with CodeLlama, as well as in a corpus specialised\nfor system command generation with Llama3 and Mistral. Velocitune achieves\nperformance gains in both math and code reasoning tasks and command-line\ngeneration benchmarks. Further analysis reveals that key factors driving\nVelocitune's effectiveness include target loss prediction and data ordering.", "AI": {"tldr": "Velocitune is a framework for dynamic data proportion adjustment in continual pre-training of language models, achieving performance improvements in reasoning and command generation tasks.", "motivation": "To address the complexities of domain-adaptive continual pre-training and improve training efficiency by dynamically adjusting data proportions.", "method": "Velocitune dynamically assesses learning velocity and adjusts data proportions, focusing on slower-learning domains based on a scaling law for desired goals with lower costs.", "result": "Velocitune demonstrates performance gains in math and code reasoning tasks, as well as command-line generation benchmarks using CodeLlama, Llama3, and Mistral.", "conclusion": "The framework's effectiveness is driven by target loss prediction and data ordering, leading to improved results in specific tasks.", "key_contributions": ["Introduction of the Velocitune framework for dynamic data adjustment", "Improved training efficiency for language models by addressing domain complexities", "Empirical validation through experiments on reasoning-focused and command-generation datasets"], "limitations": "", "keywords": ["large language models", "domain-adaptive training", "data proportion adjustment", "machine learning", "reasoning tasks"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2411.17388", "pdf": "https://arxiv.org/pdf/2411.17388.pdf", "abs": "https://arxiv.org/abs/2411.17388", "title": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?", "authors": ["Haoyu Huang", "Chong Chen", "Zeang Sheng", "Yang Li", "Wentao Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.", "AI": {"tldr": "The paper presents GraphJudge, a framework for constructing Knowledge Graphs from unstructured data that addresses noise, inaccuracies, and hallucinations in LLM outputs.", "motivation": "To improve the reliability and accuracy of Knowledge Graph construction from unstructured data, mitigating common issues faced by existing methods.", "method": "GraphJudge employs an entity-centric strategy to filter out noise and uses a fine-tuned LLM as a graph judge to enhance the quality of generated Knowledge Graphs.", "result": "Experiments show that GraphJudge outperforms various baseline methods and demonstrates strong generalization across general and domain-specific datasets.", "conclusion": "GraphJudge effectively addresses key challenges in Knowledge Graph construction from unstructured data, providing a robust framework for future applications.", "key_contributions": ["Introduction of an entity-centric strategy to reduce noise in documents", "Development of a fine-tuned LLM to improve KG quality", "Demonstration of superior performance against baseline methods across diverse datasets"], "limitations": "", "keywords": ["Knowledge Graphs", "Large Language Models", "Information Retrieval", "Data Noise", "Hallucination"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2412.06245", "pdf": "https://arxiv.org/pdf/2412.06245.pdf", "abs": "https://arxiv.org/abs/2412.06245", "title": "A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension", "authors": ["Saahith Janapati", "Yangfeng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "The performance of Large Language Models (LLMs) on natural language tasks can\nbe improved through both supervised fine-tuning (SFT) and in-context learning\n(ICL), which operate via distinct mechanisms. Supervised fine-tuning updates\nthe model's weights by minimizing loss on training data, whereas in-context\nlearning leverages task demonstrations embedded in the prompt, without changing\nthe model's parameters. This study investigates the effects of these learning\nparadigms on the hidden representations of LLMs using Intrinsic Dimension (ID).\nWe use ID to estimate the number of degrees of freedom between representations\nextracted from LLMs as they perform specific natural language tasks. We first\nexplore how the ID of LLM representations evolves during SFT and how it varies\ndue to the number of demonstrations in ICL. We then compare the IDs induced by\nSFT and ICL and find that ICL consistently induces a higher ID compared to SFT,\nsuggesting that representations generated during ICL reside in higher\ndimensional manifolds in the embedding space.", "AI": {"tldr": "This study investigates the effects of supervised fine-tuning and in-context learning on the hidden representations of Large Language Models, finding that in-context learning leads to higher intrinsic dimension in representations compared to supervised fine-tuning.", "motivation": "To understand how different learning paradigms (supervised fine-tuning and in-context learning) affect the hidden representations of Large Language Models.", "method": "The study employs Intrinsic Dimension (ID) to estimate degrees of freedom in LLM representations during specific natural language tasks, comparing the evolution of ID through supervised fine-tuning and the impact of demonstration numbers in in-context learning.", "result": "In-context learning consistently induces a higher intrinsic dimension compared to supervised fine-tuning, indicating that representations during in-context learning reside in higher dimensional manifolds.", "conclusion": "The findings suggest that in-context learning is more effective in enriching the representation space of LLMs than supervised fine-tuning.", "key_contributions": ["Investigation of intrinsic dimensions in LLM representations", "Comparison of effects of supervised fine-tuning and in-context learning", "Demonstration of higher dimensionality in representations via in-context learning"], "limitations": "", "keywords": ["Large Language Models", "supervised fine-tuning", "in-context learning", "intrinsic dimension", "natural language tasks"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2412.11936", "pdf": "https://arxiv.org/pdf/2412.11936.pdf", "abs": "https://arxiv.org/abs/2412.11936", "title": "A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges", "authors": ["Yibo Yan", "Jiamin Su", "Jianxiang He", "Fangteng Fu", "Xu Zheng", "Yuanhuiyi Lyu", "Kun Wang", "Shen Wang", "Qingsong Wen", "Xuming Hu"], "categories": ["cs.CL"], "comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL Findings 2025)", "summary": "Mathematical reasoning, a core aspect of human cognition, is vital across\nmany domains, from educational problem-solving to scientific advancements. As\nartificial general intelligence (AGI) progresses, integrating large language\nmodels (LLMs) with mathematical reasoning tasks is becoming increasingly\nsignificant. This survey provides the first comprehensive analysis of\nmathematical reasoning in the era of multimodal large language models (MLLMs).\nWe review over 200 studies published since 2021, and examine the\nstate-of-the-art developments in Math-LLMs, with a focus on multimodal\nsettings. We categorize the field into three dimensions: benchmarks,\nmethodologies, and challenges. In particular, we explore multimodal\nmathematical reasoning pipeline, as well as the role of (M)LLMs and the\nassociated methodologies. Finally, we identify five major challenges hindering\nthe realization of AGI in this domain, offering insights into the future\ndirection for enhancing multimodal reasoning capabilities. This survey serves\nas a critical resource for the research community in advancing the capabilities\nof LLMs to tackle complex multimodal reasoning tasks.", "AI": {"tldr": "This survey analyzes mathematical reasoning in multimodal large language models (MLLMs), reviewing over 200 studies and outlining key methodologies and challenges.", "motivation": "As AGI develops, integrating LLMs with mathematical reasoning tasks is vital for advancing educational and scientific problem-solving.", "method": "The paper surveys existing literature on Math-LLMs, categorizing it into benchmarks, methodologies, and challenges, with a focus on multimodal applications.", "result": "Identified trends and methodologies in MLLMs related to mathematical reasoning, along with five major challenges impeding AGI progress in this area.", "conclusion": "The survey provides insights crucial for enhancing multimodal reasoning capabilities in LLMs, serving as a resource for future research.", "key_contributions": ["Comprehensive review of 200+ studies on Math-LLMs", "Identification of key benchmarks and methodologies", "Insightful discussion on challenges and future directions"], "limitations": "The paper focuses primarily on existing literature and may not cover all emerging methodologies or applications.", "keywords": ["Mathematical Reasoning", "Multimodal Large Language Models", "Artificial General Intelligence", "Machine Learning", "Cognition"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.14161", "pdf": "https://arxiv.org/pdf/2412.14161.pdf", "abs": "https://arxiv.org/abs/2412.14161", "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "authors": ["Frank F. Xu", "Yufan Song", "Boxuan Li", "Yuxuan Tang", "Kritanjali Jain", "Mengxue Bao", "Zora Z. Wang", "Xuhui Zhou", "Zhitong Guo", "Murong Cao", "Mingyang Yang", "Hao Yang Lu", "Amaad Martin", "Zhe Su", "Leander Maben", "Raj Mehta", "Wayne Chi", "Lawrence Jang", "Yiqing Xie", "Shuyan Zhou", "Graham Neubig"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at accelerating or even autonomously performing\nwork-related tasks? The answer to this question has important implications both\nfor industry looking to adopt AI into their workflows and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that the most\ncompetitive agent can complete 30% of tasks autonomously. This paints a nuanced\npicture on task automation with LM agents--in a setting simulating a real\nworkplace, a good portion of simpler tasks could be solved autonomously, but\nmore difficult long-horizon tasks are still beyond the reach of current\nsystems. We release code, data, environment, and experiments on\nhttps://the-agent-company.com.", "AI": {"tldr": "The paper introduces TheAgentCompany, a benchmark to evaluate AI agents simulating digital worker tasks, revealing that current models can autonomously complete 30% of simpler work-related tasks.", "motivation": "To understand the performance of AI agents in performing real-world, work-related tasks and their implications for industry and economic policy.", "method": "An extensible benchmark was created, simulating a software company environment with various tasks for AI agents to complete.", "result": "Baseline agents powered by both closed API-based and open-weights language models completed 30% of tasks autonomously, indicating potential for automation but limitations on complex tasks.", "conclusion": "While simpler tasks can be automated, current systems struggle with more complex, long-horizon tasks, highlighting the nuanced landscape of task automation with LM agents.", "key_contributions": ["Introduction of TheAgentCompany benchmark for AI agents", "Simulation of a software company environment for evaluating performance", "Empirical findings on the autonomous capabilities of AI agents in workplace tasks."], "limitations": "Current AI agents are less effective at complex, long-horizon tasks, which remain a challenge.", "keywords": ["AI agents", "Large language models", "Task automation", "Benchmarking", "Workplace simulation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.14470", "pdf": "https://arxiv.org/pdf/2412.14470.pdf", "abs": "https://arxiv.org/abs/2412.14470", "title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents", "authors": ["Zhexin Zhang", "Shiyao Cui", "Yida Lu", "Jingzhuo Zhou", "Junxiao Yang", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CL"], "comment": "26 pages", "summary": "As large language models (LLMs) are increasingly deployed as agents, their\nintegration into interactive environments and tool use introduce new safety\nchallenges beyond those associated with the models themselves. However, the\nabsence of comprehensive benchmarks for evaluating agent safety presents a\nsignificant barrier to effective assessment and further improvement. In this\npaper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to\nevaluate the safety of LLM agents. Agent-SafetyBench encompasses 349\ninteraction environments and 2,000 test cases, evaluating 8 categories of\nsafety risks and covering 10 common failure modes frequently encountered in\nunsafe interactions. Our evaluation of 16 popular LLM agents reveals a\nconcerning result: none of the agents achieves a safety score above 60%. This\nhighlights significant safety challenges in LLM agents and underscores the\nconsiderable need for improvement. Through failure mode and helpfulness\nanalysis, we summarize two fundamental safety defects in current LLM agents:\nlack of robustness and lack of risk awareness. Furthermore, our findings\nsuggest that reliance on defense prompts alone may be insufficient to address\nthese safety issues, emphasizing the need for more advanced and robust\nstrategies. To drive progress in this area, Agent-SafetyBench has been released\nat https://github.com/thu-coai/Agent-SafetyBench/ to facilitate further\nresearch in agent safety evaluation and improvement.", "AI": {"tldr": "Introduction of Agent-SafetyBench, a benchmark for evaluating the safety of LLM agents with 349 environments and 2,000 test cases.", "motivation": "To address the lack of comprehensive benchmarks for evaluating the safety of LLM agents in interactive environments and tool use.", "method": "Development of Agent-SafetyBench, which includes 349 interaction environments and 2,000 test cases to assess 8 categories of safety risks and 10 common failure modes.", "result": "Evaluation of 16 popular LLM agents showed none scored above 60% in safety, revealing significant safety challenges and the need for methodological improvements.", "conclusion": "The findings indicate fundamental defects in LLM agent safety and highlight the inadequacy of defense prompts, urging the adoption of stronger strategies for safety enhancement.", "key_contributions": ["Introduction of Agent-SafetyBench benchmark for agent safety.", "Evaluation showing all tested LLM agents scored below 60% in safety.", "Identification of two key defects: lack of robustness and risk awareness."], "limitations": "The benchmark may not cover all possible interaction scenarios and safety risks.", "keywords": ["LLM agents", "safety benchmark", "Agent-SafetyBench", "human-computer interaction", "machine learning"], "importance_score": 9, "read_time_minutes": 26}}
{"id": "2412.16783", "pdf": "https://arxiv.org/pdf/2412.16783.pdf", "abs": "https://arxiv.org/abs/2412.16783", "title": "SubData: Bridging Heterogeneous Datasets to Enable Theory-Driven Evaluation of Political and Demographic Perspectives in LLMs", "authors": ["Leon Fröhling", "Pietro Bernardelle", "Gianluca Demartini"], "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "As increasingly capable large language models (LLMs) emerge, researchers have\nbegun exploring their potential for subjective tasks. While recent work\ndemonstrates that LLMs can be aligned with diverse human perspectives,\nevaluating this alignment on actual downstream tasks (e.g., hate speech\ndetection) remains challenging due to the use of inconsistent datasets across\nstudies. To address this issue, in this resource paper we propose a two-step\nframework: we (1) introduce SubData, an open-source Python library designed for\nstandardizing heterogeneous datasets to evaluate LLM perspective alignment; and\n(2) present a theory-driven approach leveraging this library to test how\ndifferently-aligned LLMs (e.g., aligned with different political viewpoints)\nclassify content targeting specific demographics. SubData's flexible mapping\nand taxonomy enable customization for diverse research needs, distinguishing it\nfrom existing resources. We invite contributions to add datasets to our\ninitially proposed resource and thereby help expand SubData into a\nmulti-construct benchmark suite for evaluating LLM perspective alignment on NLP\ntasks.", "AI": {"tldr": "This paper introduces SubData, a Python library for standardizing heterogeneous datasets to evaluate the alignment of large language models (LLMs) with human perspectives, specifically in subjective tasks.", "motivation": "The need for consistent evaluation of LLMs' perspective alignment on downstream tasks has arisen due to the variability in datasets used across studies.", "method": "The paper presents a two-step framework that includes an open-source library, SubData, for dataset standardization and a theory-driven approach to test the classification of content by differently aligned LLMs.", "result": "The proposed SubData library allows customization for varying research needs and aims to create a multi-construct benchmark suite for LLM evaluation.", "conclusion": "Contributions to SubData are welcomed to enhance its dataset offerings and strengthen the assessment of LLM perspective alignment in NLP tasks.", "key_contributions": ["Introduction of SubData for standardizing datasets", "Theory-driven approach for evaluating LLM alignment", "Invitation for community contributions to enhance SubData"], "limitations": "", "keywords": ["large language models", "perspective alignment", "NLP", "dataset standardization", "subjective tasks"], "importance_score": 9, "read_time_minutes": 11}}
{"id": "2501.02009", "pdf": "https://arxiv.org/pdf/2501.02009.pdf", "abs": "https://arxiv.org/abs/2501.02009", "title": "Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts", "authors": ["Youcheng Huang", "Chen Huang", "Duanyu Feng", "Wenqiang Lei", "Jiancheng Lv"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Camera Ready", "summary": "Understanding the inner workings of Large Language Models (LLMs) is a\ncritical research frontier. Prior research has shown that a single LLM's\nconcept representations can be captured as steering vectors (SVs), enabling the\ncontrol of LLM behavior (e.g., towards generating harmful content). Our work\ntakes a novel approach by exploring the intricate relationships between concept\nrepresentations across different LLMs, drawing an intriguing parallel to\nPlato's Allegory of the Cave. In particular, we introduce a linear\ntransformation method to bridge these representations and present three key\nfindings: 1) Concept representations across different LLMs can be effectively\naligned using simple linear transformations, enabling efficient cross-model\ntransfer and behavioral control via SVs. 2) This linear transformation\ngeneralizes across concepts, facilitating alignment and control of SVs\nrepresenting different concepts across LLMs. 3) A weak-to-strong\ntransferability exists between LLM concept representations, whereby SVs\nextracted from smaller LLMs can effectively control the behavior of larger\nLLMs.", "AI": {"tldr": "This paper explores the alignment of concept representations in different LLMs using linear transformations, enabling controllable behavior through steering vectors.", "motivation": "To understand the inner workings and relationships of concept representations across different Large Language Models (LLMs) and improve behavioral control of LLMs.", "method": "The authors propose a linear transformation method to align concept representations between different LLMs and analyze the transferability of steering vectors.", "result": "Found that concept representations can be aligned using simple linear transformations, facilitating cross-model transfer and control; demonstrated weak-to-strong transferability between smaller and larger LLMs.", "conclusion": "Linear transformations can help bridge concept representations in LLMs, enhancing the controllability of their behavior through effective steering vectors.", "key_contributions": ["Introduction of a linear transformation method for aligning LLM concept representations.", "Detailed exploration of the weak-to-strong transferability between LLMs.", "Significant findings regarding the efficiency of steering vectors in controlling model behavior."], "limitations": "", "keywords": ["Large Language Models", "steering vectors", "concept representations", "linear transformations", "transferability"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2501.06582", "pdf": "https://arxiv.org/pdf/2501.06582.pdf", "abs": "https://arxiv.org/abs/2501.06582", "title": "ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting", "authors": ["Steven H. Wang", "Maksim Zubkov", "Kexin Fan", "Sarah Harrell", "Yuyang Sun", "Wei Chen", "Andreas Plesner", "Roger Wattenhofer"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. See the project page at\n  https://www.atticusprojectai.org/acord", "summary": "Information retrieval, specifically contract clause retrieval, is\nfoundational to contract drafting because lawyers rarely draft contracts from\nscratch; instead, they locate and revise the most relevant precedent. We\nintroduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval\nbenchmark for contract drafting fully annotated by experts. ACORD focuses on\ncomplex contract clauses such as Limitation of Liability, Indemnification,\nChange of Control, and Most Favored Nation. It includes 114 queries and over\n126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task\nis to find the most relevant precedent clauses to a query. The bi-encoder\nretriever paired with pointwise LLMs re-rankers shows promising results.\nHowever, substantial improvements are still needed to effectively manage the\ncomplex legal work typically undertaken by lawyers. As the first retrieval\nbenchmark for contract drafting annotated by experts, ACORD can serve as a\nvaluable IR benchmark for the NLP community.", "AI": {"tldr": "Introducing the Atticus Clause Retrieval Dataset (ACORD) for contract clause retrieval, essential for contract drafting.", "motivation": "Lawyers typically do not draft contracts from scratch, relying instead on discovering and revising relevant precedent clauses.", "method": "ACORD contains 114 queries and over 126,000 query-clause pairs, which are ranked on a scale of 1 to 5 stars, and utilizes a bi-encoder retriever combined with LLMs for re-ranking.", "result": "The methodology shows promising results but indicates that significant enhancements are necessary to address the complexities of legal work.", "conclusion": "ACORD is the first retrieval benchmark for contract drafting fully annotated by experts, which can be a useful resource for both legal practitioners and the NLP community.", "key_contributions": ["Introduction of the ACORD dataset", "Focus on complex contract clauses", "First expert-annotated retrieval benchmark for contract drafting"], "limitations": "Substantial improvements still needed for effective management of complex legal work.", "keywords": ["contract retrieval", "NLP", "legal informatics", "dataset", "information retrieval"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2501.07482", "pdf": "https://arxiv.org/pdf/2501.07482.pdf", "abs": "https://arxiv.org/abs/2501.07482", "title": "TiEBe: Tracking Language Model Recall of Notable Worldwide Events Through Time", "authors": ["Thales Sales Almeida", "Giovana Kerche Bonás", "João Guilherme Alves Santos", "Hugo Abonizio", "Rodrigo Nogueira"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the knowledge landscape evolves and large language models (LLMs) become\nincreasingly widespread, there is a growing need to keep these models updated\nwith current events. While existing benchmarks assess general factual recall,\nfew studies explore how LLMs retain knowledge over time or across different\nregions. To address these gaps, we present the Timely Events Benchmark (TiEBe),\na dataset of over 23,000 question-answer pairs centered on notable global and\nregional events, spanning more than 10 years of events, 23 regions, and 13\nlanguages. TiEBe leverages structured retrospective data from Wikipedia to\nidentify notable events through time. These events are then used to construct a\nbenchmark to evaluate LLMs' understanding of global and regional developments,\ngrounded in factual evidence beyond Wikipedia itself. Our results reveal\nsignificant geographic disparities in factual recall, emphasizing the need for\nmore balanced global representation in LLM training. We also observe a Pearson\ncorrelation of more than 0.7 between models' performance in TiEBe and various\ncountries' socioeconomic indicators, such as HDI. In addition, we examine the\nimpact of language on factual recall by posing questions in the native language\nof the region where each event occurred, uncovering substantial performance\ngaps for low-resource languages.", "AI": {"tldr": "The paper introduces the Timely Events Benchmark (TiEBe), a dataset for evaluating LLMs' knowledge retention over time, revealing disparities in factual recall linked to geography and socioeconomics.", "motivation": "The growing prevalence of LLMs necessitates their continual update with current events, but few frameworks examine how LLMs retain knowledge and perform across diverse regions and languages.", "method": "A dataset of 23,000 question-answer pairs derived from structured retrospective data from Wikipedia, spanning global notable events over 10 years, covering 23 regions and 13 languages.", "result": "LLMs show significant geographic disparities in factual recall, correlating with socioeconomic indicators; substantial performance gaps exist for low-resource languages.", "conclusion": "A need for more balanced global representation in LLM training is highlighted, given the performance gaps noticed in lower-resource languages and geographic differences.", "key_contributions": ["Introduction of the Timely Events Benchmark (TiEBe) for evaluating LLMs", "Insights on geographic disparities in LLM performance", "Correlation findings between LLM performance and socioeconomic indicators"], "limitations": "", "keywords": ["Large Language Models", "Benchmarking", "Factual Recall", "Geographic Disparities", "Low-Resource Languages"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.14315", "pdf": "https://arxiv.org/pdf/2501.14315.pdf", "abs": "https://arxiv.org/abs/2501.14315", "title": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning", "authors": ["Chao-Chung Wu", "Zhi Rui Tam", "Chieh-Yen Lin", "Yun-Nung Chen", "Shao-Hua Sun", "Hung-yi Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Maintaining consistent model performance across domains is a fundamental\nchallenge in machine learning. While recent work has explored using\nLLM-generated data for fine-tuning, its impact on cross-domain generalization\nremains poorly understood. This paper presents a systematic analysis revealing\nthat fine-tuning with LLM-generated data not only improves target task\nperformance but also reduces non-target task degradation compared to\nfine-tuning with ground truth data. Through analyzing the data sequence in\ntasks of various domains, we demonstrate that this enhancement of non-target\ntask robustness stems from the reduction of high perplexity tokens found in\nLLM-generated sequences. Following our findings, we showed that masking high\nperplexity tokens in ground truth training data achieves similar non-target\ntask performance preservation, comparable to using LLM-generated data.\nExtensive experiments across different model families and scales, including\nGemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our\nfindings. To the best of our knowledge, this is the first work to provide an\nempirical explanation based on token perplexity reduction to mitigate\ncatastrophic forgetting in LLMs after fine-tuning, offering valuable insights\nfor developing more robust fine-tuning strategies.", "AI": {"tldr": "The paper analyzes the effects of fine-tuning LLMs with LLM-generated data on cross-domain generalization, finding improved performance in target tasks and reduced non-target task degradation.", "motivation": "To understand the impact of LLM-generated data on cross-domain generalization in machine learning.", "method": "Systematic analysis of fine-tuning with LLM-generated data versus ground truth data, focusing on token perplexity reduction and its effects across various model families and scales.", "result": "Fine-tuning with LLM-generated data improves target task performance while preserving non-target task robustness, mimicking the effects achieved by masking high perplexity tokens in ground truth data.", "conclusion": "LLM-generated data can enhance fine-tuning strategies in LLMs by mitigating catastrophic forgetting via token perplexity reduction.", "key_contributions": ["First empirical explanation of token perplexity reduction's role in mitigating catastrophic forgetting.", "Demonstration that LLM-generated data outperforms ground truth data in maintaining model performance across domains.", "Introduction of masking high perplexity tokens as an effective strategy for preserving task performance."], "limitations": "", "keywords": ["LLM", "fine-tuning", "cross-domain generalization", "token perplexity", "catastrophic forgetting"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.15451", "pdf": "https://arxiv.org/pdf/2501.15451.pdf", "abs": "https://arxiv.org/abs/2501.15451", "title": "STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection", "authors": ["Zewen Bai", "Shengdi Yin", "Junyu Lu", "Jingjie Zeng", "Haohao Zhu", "Yuanyuan Sun", "Liang Yang", "Hongfei Lin"], "categories": ["cs.CL"], "comment": "Our paper has been accepted by ACL 2025 Findings", "summary": "The proliferation of hate speech has caused significant harm to society. The\nintensity and directionality of hate are closely tied to the target and\nargument it is associated with. However, research on hate speech detection in\nChinese has lagged behind, and existing datasets lack span-level fine-grained\nannotations. Furthermore, the lack of research on Chinese hateful slang poses a\nsignificant challenge. In this paper, we provide a solution for fine-grained\ndetection of Chinese hate speech. First, we construct a dataset containing\nTarget-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first\nspan-level Chinese hate speech dataset. Secondly, we evaluate the span-level\nhate speech detection performance of existing models using STATE ToxiCN.\nFinally, we conduct the first study on Chinese hateful slang and evaluate the\nability of LLMs to detect such expressions. Our work contributes valuable\nresources and insights to advance span-level hate speech detection in Chinese.", "AI": {"tldr": "The paper presents a novel dataset and method for fine-grained detection of hate speech in Chinese, addressing gaps in existing research.", "motivation": "The rise of hate speech in society and the lack of effective detection methods for Chinese hate speech, especially with a focus on fine-grained annotations and slang.", "method": "A dataset named STATE ToxiCN was constructed containing Target-Argument-Hateful-Group quadruples for span-level annotations, and existing models were evaluated for hate speech detection performance.", "result": "Proven effectiveness of the STATE ToxiCN dataset in detecting hate speech and a first evaluation of LLMs on recognizing Chinese hateful slang.", "conclusion": "The research provides a crucial resource for the study of hate speech in Chinese, aiding in more effective detection methods.", "key_contributions": ["Creation of the first span-level Chinese hate speech dataset (STATE ToxiCN).", "Evaluation of existing models for span-level hate speech detection.", "First study on the detection of Chinese hateful slang using LLMs."], "limitations": "The research is focused solely on Chinese hate speech and does not cover other languages or broader hate speech contexts.", "keywords": ["hate speech", "Chinese language", "dataset", "LLMs", "hateful slang"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2501.15654", "pdf": "https://arxiv.org/pdf/2501.15654.pdf", "abs": "https://arxiv.org/abs/2501.15654", "title": "People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text", "authors": ["Jenna Russell", "Marzena Karpinska", "Mohit Iyyer"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 33 pages", "summary": "In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such \"expert\" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts' free-form explanations shows that while they rely heavily on specific\nlexical clues ('AI vocabulary'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext.", "AI": {"tldr": "This paper investigates human ability to detect AI-generated text, revealing that frequent users of LLMs excel at this task compared to commercial detectors.", "motivation": "The study aims to assess human detection capabilities of AI-generated content and improve understanding of text characteristics that distinguish human from AI writing.", "method": "300 non-fiction articles were labeled by annotators as human-written or AI-generated, with detailed explanations provided for each decision.", "result": "Expert annotators who regularly use LLMs misclassify only 1 of the 300 articles, outperforming many commercial detectors, especially against evasion tactics.", "conclusion": "The findings highlight the effectiveness of experienced annotators in identifying AI-generated text and reveal nuanced features that influence detection.", "key_contributions": ["Demonstrated high accuracy of LLM users in detecting AI-generated text.", "Highlighted reliance on lexical and complex textual features for detection.", "Released annotated dataset and detection code for further research."], "limitations": "The study focuses solely on non-fiction English articles and may not generalize across all text types or languages.", "keywords": ["AI detection", "human annotation", "LLM", "natural language processing", "text analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.19202", "pdf": "https://arxiv.org/pdf/2501.19202.pdf", "abs": "https://arxiv.org/abs/2501.19202", "title": "Improving LLM Unlearning Robustness via Random Perturbations", "authors": ["Dang Huu-Tien", "Hoang Thanh-Tung", "Anh Bui", "Le-Minh Nguyen", "Naoya Inoue"], "categories": ["cs.CL"], "comment": "23 pages, 10 figures, 5 tables", "summary": "In this paper, we show that current state-of-the-art LLM unlearning methods\ninherently reduce models' robustness, causing them to misbehave even when a\nsingle non-adversarial forget-token is in the retain-query. Toward\nunderstanding underlying causes, we reframe the unlearning process as backdoor\nattacks and defenses: forget-tokens act as backdoor triggers that, when\nactivated in retain-queries, cause disruptions in unlearned models' behaviors,\nsimilar to successful backdoor attacks. To mitigate this vulnerability, we\npropose Random Noise Augmentation (RNA) -- a plug-and-play, model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nunlearned models. Extensive experiments demonstrate that RNA significantly\nimproves the robustness of unlearned models, maintains unlearning performances\nwhile introducing no additional computational overhead.", "AI": {"tldr": "This paper investigates the adverse impact of state-of-the-art LLM unlearning methods on model robustness and proposes Random Noise Augmentation (RNA) to enhance robustness while preserving unlearning performance.", "motivation": "To explore the reduction in robustness caused by current LLM unlearning methods and understand it through the lens of backdoor attacks.", "method": "The paper reframes unlearning processes as backdoor attacks and proposes Random Noise Augmentation (RNA) as a method to bolster robustness in unlearned models without extra computational cost.", "result": "Extensive experiments demonstrate that RNA significantly improves the robustness of unlearned models and maintains their unlearning performance.", "conclusion": "The proposed RNA method provides a novel approach to mitigating vulnerabilities in unlearned models while ensuring they perform as intended without added complexity.", "key_contributions": ["Reframing the unlearning process as backdoor attacks and defenses.", "Introducing Random Noise Augmentation (RNA) for robustness improvement.", "Demonstrating the efficacy of RNA through extensive experiments."], "limitations": "The method may not generalize to all contexts of model unlearning and robustness in specific scenarios.", "keywords": ["LLM unlearning", "robustness", "backdoor attacks", "Random Noise Augmentation", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.02095", "pdf": "https://arxiv.org/pdf/2502.02095.pdf", "abs": "https://arxiv.org/abs/2502.02095", "title": "LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information", "authors": ["Bowen Ping", "Jiali Zeng", "Fandong Meng", "Shuo Wang", "Jie Zhou", "Shanghang Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Long-form generation is crucial for academic writing papers and repo-level\ncode generation. Despite this, current models, including GPT-4o, still exhibit\nunsatisfactory performance. Existing methods that utilize preference learning\nwith outcome supervision often fail to provide detailed feedback for extended\ncontexts. This shortcoming can lead to content that does not fully satisfy\nquery requirements, resulting in issues like length deviations, and diminished\nquality. In this paper, we propose enhancing long-form generation by\nincorporating process supervision. We employ Monte Carlo Tree Search to gather\nstepwise preference pairs, utilizing a global memory pool to maintain\nconsistency. To address the issue of suboptimal candidate selection, we\nintegrate external critiques to refine and improve the quality of the\npreference pairs. Finally, we apply step-level DPO using the collected stepwise\npreference pairs. Experimental results show that our method improves length and\nquality on long-form generation benchmarks, with almost lossless performance on\ngeneral benchmarks across various model backbones.", "AI": {"tldr": "This paper proposes a method to enhance long-form text generation by using process supervision and Monte Carlo Tree Search for preference learning, addressing shortcomings in existing models.", "motivation": "Current long-form generation methods struggle with performance and feedback, leading to issues like poor quality and length deviations in generated content.", "method": "The paper introduces Monte Carlo Tree Search for gathering stepwise preference pairs, and uses a global memory pool to maintain consistency. It also integrates external critiques to improve candidate selection before applying step-level DPO.", "result": "Experimental results indicate that the proposed method significantly improves both the length and quality of generated long-form text, performing nearly optimally on general benchmarks across multiple model backbones.", "conclusion": "The incorporation of process supervision and refined preference pairs notably enhances long-form generation results, making it a step forward in model performance.", "key_contributions": ["Introduction of process supervision in long-form generation", "Use of Monte Carlo Tree Search for preference learning", "Integration of external critiques for improving candidate selection"], "limitations": "", "keywords": ["long-form generation", "preference learning", "Monte Carlo Tree Search"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.02362", "pdf": "https://arxiv.org/pdf/2502.02362.pdf", "abs": "https://arxiv.org/abs/2502.02362", "title": "Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs", "authors": ["Sagnik Mukherjee", "Abhinav Chinta", "Takyoung Kim", "Tarun Anoop Sharma", "Dilek Hakkani-Tür"], "categories": ["cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large\nlanguage models (LLMs) by enabling detailed step-by-step solutions. However,\ndue to the verbosity of LLMs, the resulting reasoning chains can be long,\nmaking it harder to verify the reasoning steps and trace issues resulting from\ndependencies between the steps that may be farther away in the sequence of\nsteps. Importantly, mathematical reasoning allows each step to be derived from\na small set of premises, which are a subset of the preceding steps in the\nreasoning chain. In this paper, we present a framework that identifies the\npremises for each step, to improve the evaluation of reasoning. We restructure\nconventional linear reasoning chains into Premise Augmented Reasoning Chains\n(PARC) by introducing premise links, resulting in a directed acyclic graph\nwhere the nodes are the steps and the edges are the premise links. Through\nexperiments with a PARC-based dataset that we built, namely PERL (Premises and\nERrors identification in LLMs), we demonstrate that LLMs can reliably identify\npremises within complex reasoning chains. In particular, even open-source LLMs\nachieve 90% recall in premise identification. We also show that PARC helps to\nidentify errors in reasoning chains more reliably. The accuracy of error\nidentification improves by 6% to 16% absolute when step-by-step verification is\ncarried out in PARC under the premises. Our findings highlight the utility of\npremise-centric representations in addressing complex problem-solving tasks and\nopen new avenues for improving the reliability of LLM-based reasoning\nevaluations.", "AI": {"tldr": "This paper introduces Premise Augmented Reasoning Chains (PARC) which improve the evaluation of mathematical reasoning in LLMs by restructuring reasoning chains into a directed acyclic graph format with premise links.", "motivation": "To enhance mathematical reasoning in LLMs while addressing issues of verbosity and complexity in traditional reasoning chains.", "method": "A framework is proposed that restructures reasoning chains into Premise Augmented Reasoning Chains, forming a directed acyclic graph with nodes as reasoning steps and edges as premise links.", "result": "LLMs can reliably identify premises within complex reasoning chains, achieving 90% recall in premise identification, and the accuracy of error identification improves by 6% to 16% when using PARC.", "conclusion": "The findings suggest that premise-centric representations can significantly improve the reliability of LLM-based reasoning evaluations, paving the way for better problem-solving methods.", "key_contributions": ["Introduction of Premise Augmented Reasoning Chains (PARC)", "Demonstration of high recall in premise identification by LLMs", "Improvement in error identification accuracy through step-by-step verification in PARC."], "limitations": "", "keywords": ["Chain-of-Thought prompting", "Large language models", "Premise Augmented Reasoning Chains"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.02577", "pdf": "https://arxiv.org/pdf/2502.02577.pdf", "abs": "https://arxiv.org/abs/2502.02577", "title": "A comparison of translation performance between DeepL and Supertext", "authors": ["Alex Flückiger", "Chantal Amrhein", "Tim Graf", "Frédéric Odermatt", "Martin Pömsl", "Philippe Schläpfer", "Florian Schottmann", "Samuel Läubli"], "categories": ["cs.CL"], "comment": "Paper accepted at MT Summit 2025", "summary": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext.", "AI": {"tldr": "This study evaluates the performance of two machine translation systems—DeepL and Supertext—using professional translators to assess translation quality across extended context, revealing a preference for Supertext in longer texts.", "motivation": "With the reliance on large language models for machine translation, there is a need for better quality benchmarking that captures system performance on extended context.", "method": "The study compares DeepL and Supertext by evaluating their translation quality on unsegmented texts, using professional translators to perform assessments based on document-level context.", "result": "Segment-level assessments show no strong preference between the systems, but document-level analysis indicates Supertext performs better in three out of four language directions, suggesting better consistency.", "conclusion": "The findings highlight the importance of context-sensitive evaluation methodologies for machine translation systems to reflect usability in real-world scenarios.", "key_contributions": ["Comparison of two commercial MT systems focusing on document-level context", "Release of evaluation data and scripts for further research", "Advocacy for more context-sensitive methodologies in MT quality assessment"], "limitations": "", "keywords": ["machine translation", "large language models", "context-sensitive evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.02789", "pdf": "https://arxiv.org/pdf/2502.02789.pdf", "abs": "https://arxiv.org/abs/2502.02789", "title": "Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation", "authors": ["Jingyu Liu", "Beidi Chen", "Ce Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Proceedings of the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "Improving time-to-first-token (TTFT) is an essentially important objective in\nmodern large language model (LLM) inference engines. Optimizing TTFT directly\nresults in higher maximal QPS and meets the requirements of many critical\napplications. However, boosting TTFT is notoriously challenging since it is\ncompute-bounded and the performance bottleneck shifts from the self-attention\nthat many prior works focus on to the MLP part. In this work, we present\nSpecPrefill, a training free framework that accelerates the inference TTFT for\nboth long and medium context queries based on the following insight: LLMs are\ngeneralized enough to preserve the quality given only a carefully chosen subset\nof prompt tokens. At its core, SpecPrefill leverages a lightweight model to\nspeculate locally important tokens based on the context. These tokens, along\nwith the necessary positional information, are then sent to the main model for\nprocessing. We evaluate SpecPrefill with a diverse set of tasks, followed by a\ncomprehensive benchmarking of performance improvement both in a real end-to-end\nsetting and ablation studies. SpecPrefill manages to serve\nLlama-3.1-405B-Instruct-FP8 with up to 7$\\times$ maximal end-to-end QPS on real\ndownstream tasks and 7.66$\\times$ TTFT improvement.", "AI": {"tldr": "SpecPrefill is a training-free framework designed to improve time-to-first-token (TTFT) during LLM inference, achieving significant performance boosts for both long and medium context queries.", "motivation": "To optimize TTFT and enhance maximal QPS for critical applications in modern LLM inference engines.", "method": "SpecPrefill employs a lightweight model to identify locally important tokens from a prompt, which are then processed by the main model, aiming to streamline inference without additional training.", "result": "SpecPrefill showed up to 7× improvement in end-to-end QPS and a 7.66× enhancement in TTFT on diverse real-world tasks and through ablation studies.", "conclusion": "The framework successfully boosts TTFT performance by focusing on relevant prompt subsets, thereby enhancing the efficiency of LLMs in practical applications.", "key_contributions": ["Training-free framework for LLM inference optimization", "Use of a lightweight model for token speculation", "Achieved substantial improvements in TTFT and QPS"], "limitations": "", "keywords": ["large language models", "time-to-first-token", "inference optimization", "SpecPrefill", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11051", "pdf": "https://arxiv.org/pdf/2502.11051.pdf", "abs": "https://arxiv.org/abs/2502.11051", "title": "MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models", "authors": ["Jiahao Huo", "Yibo Yan", "Xu Zheng", "Yuanhuiyi Lyu", "Xin Zou", "Zhihua Wei", "Xuming Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as ACL 2025 Findings", "summary": "Recent progress in Machine Unlearning (MU) has introduced solutions for the\nselective removal of private or sensitive information encoded within deep\nneural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)\nremains in its nascent phase. Therefore, we propose to reformulate the task of\nmultimodal MU in the era of MLLMs, which aims to erase only the visual patterns\nassociated with a given entity while preserving the corresponding textual\nknowledge encoded within the original parameters of the language model\nbackbone. Furthermore, we develop a novel geometry-constrained gradient ascent\nmethod MMUnlearner. It updates the weights of MLLMs with a weight saliency map\njointly restricted by the remaining concepts and textual knowledge during\nunlearning, thereby preserving parameters essential for non-target knowledge.\nExtensive experiments demonstrate that MMUnlearner surpasses baselines that\nfinetuning MLLMs with VQA data directly through Gradient Ascent (GA) or\nNegative Preference Optimization (NPO), across all evaluation dimensions. Our\ncode will be released upon acceptance.", "AI": {"tldr": "This paper introduces a method for Machine Unlearning in Multimodal Large Language Models, focusing on selectively removing visual patterns while preserving textual knowledge.", "motivation": "Investigating the need for effective methods to remove sensitive visual information from Multimodal Large Language Models while maintaining their essential textual knowledge.", "method": "A geometry-constrained gradient ascent method called MMUnlearner is developed, utilizing a weight saliency map to update MLLMs' weights during the unlearning process.", "result": "MMUnlearner demonstrates superior performance compared to traditional methods like Gradient Ascent and Negative Preference Optimization across various evaluation metrics.", "conclusion": "The proposed method enables efficient selective unlearning in Multimodal Large Language Models without compromising critical textual information.", "key_contributions": ["Introduces MMUnlearner for multimodal machine unlearning.", "Preserves textual knowledge during the erasure of visual patterns.", "Surpasses existing baseline models in performance."], "limitations": "", "keywords": ["Machine Unlearning", "Multimodal Large Language Models", "gradient ascent"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.11066", "pdf": "https://arxiv.org/pdf/2502.11066.pdf", "abs": "https://arxiv.org/abs/2502.11066", "title": "CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment", "authors": ["Nura Aljaafari", "Danilo S. Carvalho", "André Freitas"], "categories": ["cs.CL"], "comment": "19 pages, 8 figures, 8 tables", "summary": "Large language models (LLMs) struggle with compositional generalisation,\nlimiting their ability to systematically combine learned components to\ninterpret novel inputs. While architectural modifications, fine-tuning, and\ndata augmentation improve compositionality, they often have limited\nadaptability, face scalability constraints, or yield diminishing returns on\nreal data. To address this, we propose CARMA, an intervention that enhances the\nstability and robustness of compositional reasoning in LLMs while preserving\nfine-tuned performance. CARMA employs mutual information regularisation and\nlayer-wise stability constraints to mitigate feature fragmentation, ensuring\nstructured representations persist across and within layers. We evaluate CARMA\non inverse dictionary modelling and sentiment classification, measuring its\nimpact on semantic consistency, performance stability, and robustness to\nlexical perturbations. Results show that CARMA reduces the variability\nintroduced by fine-tuning, stabilises token representations, and improves\ncompositional reasoning. While its effectiveness varies across architectures,\nCARMA's key strength lies in reinforcing learned structures rather than\nintroducing new capabilities, making it a scalable auxiliary method. These\nfindings suggest that integrating CARMA with fine-tuning can improve\ncompositional generalisation while maintaining task-specific performance in\nLLMs.", "AI": {"tldr": "The paper introduces CARMA, an intervention aimed at enhancing compositional reasoning in large language models (LLMs) by improving stability and robustness while preserving performance.", "motivation": "LLMs struggle with compositional generalisation, limiting their ability to interpret novel inputs effectively. Previous methods often fail due to scalability issues or diminishing returns.", "method": "CARMA employs mutual information regularisation and layer-wise stability constraints to improve the robustness of compositional reasoning in LLMs and mitigate feature fragmentation.", "result": "CARMA improves semantic consistency, performance stability, and robustness against lexical perturbations in tasks like inverse dictionary modelling and sentiment classification, reducing variability introduced by fine-tuning.", "conclusion": "Integrating CARMA with fine-tuning enhances compositional generalisation in LLMs while maintaining task-specific performance, reinforcing learned structures rather than adding new capabilities.", "key_contributions": ["Introduces CARMA for stability in compositional reasoning in LLMs.", "Utilizes mutual information regularisation and layer-wise constraints to mitigate feature fragmentation.", "Demonstrates improved semantic consistency and robustness across various tasks."], "limitations": "Effectiveness varies across different architectural frameworks; not a one-size-fits-all solution.", "keywords": ["Compositional Generalization", "Large Language Models", "Mutual Information Regularization", "Stability Constraints", "Fine-Tuning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2502.11100", "pdf": "https://arxiv.org/pdf/2502.11100.pdf", "abs": "https://arxiv.org/abs/2502.11100", "title": "Towards Achieving Concept Completeness for Textual Concept Bottleneck Models", "authors": ["Milan Bhan", "Yann Choho", "Pierre Moreau", "Jean-Noel Vittaut", "Nicolas Chesneau", "Marie-Jeanne Lesot"], "categories": ["cs.CL"], "comment": null, "summary": "Textual Concept Bottleneck Models (TBMs) are interpretable-by-design models\nfor text classification that predict a set of salient concepts before making\nthe final prediction. This paper proposes Complete Textual Concept Bottleneck\nModel (CT-CBM),a novel TCBM generator building concept labels in a fully\nunsupervised manner using a small language model, eliminating both the need for\npredefined human labeled concepts and LLM annotations. CT-CBM iteratively\ntargets and adds important concepts in the bottleneck layer to create a\ncomplete concept basis and addresses downstream classification leakage through\na parallel residual connection. CT-CBM achieves good results against\ncompetitors, offering a promising solution to enhance interpretability of NLP\nclassifiers without sacrificing performance.", "AI": {"tldr": "CT-CBM is an unsupervised model for text classification that improves interpretability and performance by predicting salient concepts before final predictions.", "motivation": "To enhance interpretability in text classification models without relying on predefined concepts or extensive human annotations.", "method": "CT-CBM generates concept labels using a small language model in an unsupervised manner and iteratively adds important concepts to a bottleneck layer.", "result": "CT-CBM demonstrates competitive results against other models, effectively addressing classification leakage.", "conclusion": "CT-CBM represents a promising approach for improving the interpretability of NLP classifiers while maintaining high performance.", "key_contributions": ["Introduction of a fully unsupervised TCBM generator", "Elimination of predefined human labeled concepts", "Addressing downstream classification leakage through parallel residual connections."], "limitations": "", "keywords": ["Textual Concept Bottleneck Models", "interpretability", "unsupervised learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2502.11733", "pdf": "https://arxiv.org/pdf/2502.11733.pdf", "abs": "https://arxiv.org/abs/2502.11733", "title": "Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment", "authors": ["Jonathan Jordan", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) serve not only as chatbots but as key components\nin agent systems, where their common-sense knowledge significantly impacts\nperformance as language-based planners for situated or embodied action. We\nassess LLMs' incremental learning (based on feedback from the environment), and\ncontrolled in-context learning abilities using a text-based environment. We\nintroduce challenging yet interesting set of experiments to test i) how agents\ncan incrementally solve tasks related to every day objects in typical rooms in\na house where each of them are discovered by interacting within the\nenvironment, ii) controlled in-context learning abilities and efficiency of\nagents by providing short info about locations of objects and rooms to check\nhow faster the task can be solved, and finally iii) using synthetic\npseudo-English words to gauge how well LLMs are at inferring meaning of unknown\nwords from environmental feedback. Results show that larger commercial models\nhave a substantial gap in performance compared to open-weight but almost all\nmodels struggle with the synthetic words experiments.", "AI": {"tldr": "The paper evaluates the incremental learning and controlled in-context learning capabilities of large language models (LLMs) in agent systems through experiments involving task-solving in a simulated environment.", "motivation": "To investigate the role of Large Language Models (LLMs) as effective components in agent systems, particularly focusing on their learning capabilities in interactive environments.", "method": "The study employs a set of experiments to assess i) agents' ability to incrementally discover and solve tasks related to everyday objects in a simulated house environment, ii) the efficiency of in-context learning with provided information about object locations, and iii) the inference abilities of LLMs with unfamiliar synthetic words based on environmental feedback.", "result": "Findings indicate that larger commercial LLMs perform significantly better than open-weight models, yet all models face challenges when working with synthetic pseudo-English words.", "conclusion": "The results suggest a considerable gap in performance based on model type and highlight the struggles LLMs have with learning from synthetic word inputs, pointing to areas for future improvement in model training and architecture.", "key_contributions": ["Introduction of a new experimental framework for testing LLMs in agent systems", "Insights into the learning capabilities of LLMs in interactive environments", "Comparison of performance between commercial and open-weight LLMs."], "limitations": "The experiments predominantly focus on specific environments and tasks, which may not generalize to broader applications of LLMs in other domains.", "keywords": ["Large Language Models", "incremental learning", "in-context learning", "agent systems", "synthetic words"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11811", "pdf": "https://arxiv.org/pdf/2502.11811.pdf", "abs": "https://arxiv.org/abs/2502.11811", "title": "FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models", "authors": ["Qianchi Zhang", "Hainan Zhang", "Liang Pang", "Ziwei Wang", "Hongwei Zheng", "Yongxin Tong", "Zhiming Zheng"], "categories": ["cs.CL"], "comment": "18 pages, 4 figures, 18 tables, under review", "summary": "Retrieved documents containing noise will hinder Retrieval-Augmented\nGeneration (RAG) from detecting answer clues, necessitating noise filtering\nmechanisms to enhance accuracy. Existing methods use reranking or summarization\nto identify the most relevant sentences, but directly and accurately locating\nanswer clues from these large-scale and complex documents remains challenging.\nUnlike these document-level operations, we treat noise filtering as a\nsentence-level MinMax optimization problem: first identifying potential clues\nfrom multiple documents, then ranking them by relevance, and finally retaining\nthe minimum number of clues through truncation. In this paper, we propose\nFineFilter, a novel fine-grained noise filtering mechanism for RAG, consisting\nof a clue extractor, a reranker, and a truncator. We optimize each module to\ntackle complex reasoning challenges: (1) The clue extractor first uses\nsentences containing the answer and similar ones as fine-tuning targets, aiming\nto extract sufficient potential clues; (2) The reranker is trained to\nprioritize effective clues based on the real feedback from the generation\nmodule, with clues capable of generating correct answers as positive samples\nand others as negative; (3) The truncator takes the minimum number of clues\nneeded to answer the question (truncation point) as fine-tuning targets, and\nperforms truncation on the reranked clues to achieve fine-grained noise\nfiltering. Experiments on three QA datasets demonstrate that FineFilter\nsignificantly improves QA performance over baselines on both LLaMA3 and\nMistral. Further analysis confirms its effectiveness in complex reasoning,\nrobustness to unreliable retrieval, and generalization to different scenarios.", "AI": {"tldr": "FineFilter is a novel noise filtering mechanism for Retrieval-Augmented Generation (RAG) that improves QA performance by optimizing clue extraction, reranking, and truncation.", "motivation": "Existing noise filtering methods for RAG struggle to accurately locate answer clues from large and complex documents, requiring improved mechanisms.", "method": "FineFilter employs a three-step approach: a clue extractor identifies potential clues, a reranker prioritizes them based on feedback from the generation module, and a truncator reduces to the minimum necessary clues for answering questions.", "result": "Experiments show that FineFilter significantly enhances QA performance over baselines on LLaMA3 and Mistral across three QA datasets.", "conclusion": "FineFilter is effective in complex reasoning, demonstrates robustness against unreliable retrieval, and generalizes well to different scenarios.", "key_contributions": ["Introduction of FineFilter as a fine-grained noise filtering mechanism for RAG", "Optimized modules for clue extraction, prioritization, and truncation", "Demonstrated effectiveness through extensive experiments on QA datasets"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "noise filtering", "clue extraction", "machine learning", "question answering"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2502.11916", "pdf": "https://arxiv.org/pdf/2502.11916.pdf", "abs": "https://arxiv.org/abs/2502.11916", "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models", "authors": ["Jiamin Su", "Yibo Yan", "Fangteng Fu", "Han Zhang", "Jingheng Ye", "Xiang Liu", "Jiahao Huo", "Huiyu Zhou", "Xuming Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL Findings 2025", "summary": "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research.", "AI": {"tldr": "The paper introduces EssayJudge, a multimodal benchmark for Automated Essay Scoring (AES) utilizing Multimodal Large Language Models (MLLMs) to enhance evaluation precision across various writing traits.", "motivation": "To address the challenges faced by traditional Automated Essay Scoring systems, including reliance on handcrafted features and difficulties in evaluating complex writing traits and multimodal contexts.", "method": "The authors developed EssayJudge as a multimodal benchmark to test AES capabilities, leveraging MLLMs for trait-specific scoring and context understanding.", "result": "Experiments with 18 representative MLLMs showed performance gaps in AES compared to human evaluations, especially in assessing discourse-level traits.", "conclusion": "The research highlights significant limitations in current MLLM-based AES systems and signals the need for further advancements in this field.", "key_contributions": ["Introduction of EssayJudge, a multimodal AES benchmark", "Evaluation of 18 MLLMs on AES tasks", "Identification of performance gaps compared to human evaluation"], "limitations": "Focus on multimodal contexts may limit applicability to traditional single-modal assessment scenarios.", "keywords": ["Automated Essay Scoring", "Multimodal Large Language Models", "Educational Assessment", "Natural Language Processing", "Discourse Evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.12767", "pdf": "https://arxiv.org/pdf/2502.12767.pdf", "abs": "https://arxiv.org/abs/2502.12767", "title": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs", "authors": ["Sumin Jo", "Junseong Choi", "Jiho Kim", "Edward Choi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks still suffer two practical drawbacks: they must be re-tuned whenever\nthe KG or reasoning task changes, and they depend on a single, high-capacity\nLLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce\nR2-KG, a plug-and-play, dual-agent framework that separates reasoning into two\nroles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor\n(a high-capacity LLM) that makes final judgments. This design is cost-efficient\nfor LLM inference while still maintaining strong reasoning accuracy.\nAdditionally, R2-KG employs an Abstention mechanism, generating answers only\nwhen sufficient evidence is collected from KG, which significantly enhances\nreliability. Experiments across five diverse benchmarks show that R2-KG\nconsistently outperforms baselines in both accuracy and reliability, regardless\nof the inherent capability of LLMs used as the Operator. Further experiments\nreveal that the single-agent version of R2-KG, equipped with a strict\nself-consistency strategy, achieves significantly higher-than-baseline\nreliability with reduced inference cost but increased abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning, reducing reliance on high-capacity LLMs while\nensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/.", "AI": {"tldr": "R2-KG is a dual-agent framework that enhances reasoning accuracy in LLMs with a cost-efficient approach, separating roles into an Operator and a Supervisor, and incorporates an Abstention mechanism to improve reliability.", "motivation": "To overcome the limitations of existing frameworks combining LLMs with Knowledge Graphs, specifically the need for re-tuning and dependence on a single high-capacity LLM for reliable reasoning.", "method": "R2-KG divides reasoning tasks into two roles: a low-capacity LLM (Operator) gathers evidence while a high-capacity LLM (Supervisor) makes judgments. It also implements an Abstention mechanism to ensure responses are given only when adequate evidence is available.", "result": "R2-KG outperforms existing baselines in accuracy and reliability across five benchmarks, demonstrating reduced reliance on high-capacity LLMs and establishing a cost-effective solution for Knowledge Graph-based reasoning.", "conclusion": "R2-KG is effective in offering a flexible framework for reasoning tasks that balances cost and reliability, suggesting its potential for real-world applications in various fields, including health informatics.", "key_contributions": ["Introduces a dual-agent framework for improved reasoning in LLMs.", "Employs a cost-effective mechanism that mitigates reliance on high-capacity models.", "Includes an Abstention mechanism to enhance response reliability."], "limitations": "", "keywords": ["Large Language Models", "Knowledge Graphs", "Reasoning", "Abstention Mechanism", "Artificial Intelligence"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.13061", "pdf": "https://arxiv.org/pdf/2502.13061.pdf", "abs": "https://arxiv.org/abs/2502.13061", "title": "Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection", "authors": ["Jingbiao Mei", "Jinghong Chen", "Guangyu Yang", "Weizhe Lin", "Bill Byrne"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Preprint. Under Review", "summary": "Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While LMMs have shown promise\nin hateful meme detection, they face notable challenges like sub-optimal\nperformance and limited out-of-domain generalization capabilities. Recent\nstudies further reveal the limitations of both SFT and in-context learning when\napplied to LMMs in this setting. To address these issues, we propose a robust\nadaptation framework for hateful meme detection that enhances in-domain\naccuracy and cross-domain generalization while preserving the general\nvision-language capabilities of LMMs. Experiments on six meme classification\ndatasets show that our approach achieves state-of-the-art performance,\noutperforming larger agentic systems. Moreover, our method generates\nhigher-quality rationales for explaining hateful content compared to standard\nSFT, enhancing model interpretability.", "AI": {"tldr": "A robust framework for detecting hateful memes using LMMs, improving accuracy and generalization while maintaining vision-language capabilities.", "motivation": "The increasing prevalence of hateful memes on the Internet underscores the need for effective detection systems, particularly given the shortcomings of current LMMs in performance and generalization.", "method": "We propose an adaptation framework that enhances both in-domain accuracy for hateful meme detection and cross-domain generalization, overcoming limitations of previous SFT and in-context learning methods in LMMs.", "result": "Our framework demonstrates state-of-the-art performance on six classification datasets and produces higher-quality rationales for explaining hate content compared to standard methods.", "conclusion": "The proposed approach significantly improves hateful meme detection and model interpretability, marking a step forward in developing effective automated detection systems.", "key_contributions": ["Robust adaptation framework for LMMs in hateful meme detection", "State-of-the-art performance on meme classification datasets", "Improved interpretability through high-quality rationale generation"], "limitations": "", "keywords": ["hateful memes", "LMMs", "detection system"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.13442", "pdf": "https://arxiv.org/pdf/2502.13442.pdf", "abs": "https://arxiv.org/abs/2502.13442", "title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation", "authors": ["Jialin Ouyang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Large language models (LLMs) now achieve near-human performance on standard\nmath word problem benchmarks (e.g., GSM8K), yet their true reasoning ability\nremains disputed. A key concern is that models often produce confident, yet\nunfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic\ndataset that systematically generates infinite unanswerable math word problems\nand their answerable counterparts, by representing each question as a tree and\nremoving chosen necessary conditions. Experiments show TreeCut effectively\ninduce hallucinations in large language models, including GPT-4o and o3-mini,\nwith rates of 64% and 44% in their respective worst-case scenarios under\nzero-shot setting. Further analysis highlights that deeper or more complex\ntrees, composite item names, and removing necessary condition near the middle\nof a path all increase the likelihood of hallucinations, underscoring the\npersistent challenges LLMs face in identifying unanswerable math problems. The\ndataset generation code and sample data are available at\nhttps://github.com/j-bagel/treecut-math.", "AI": {"tldr": "TreeCut is a synthetic dataset designed to generate unanswerable math word problems, revealing the limitations of large language models in reasoning.", "motivation": "To investigate and highlight the reasoning shortcomings of large language models when faced with unanswerable math problems.", "method": "TreeCut generates math word problems by representing them as trees and strategically removing necessary conditions, thus creating both answerable and unanswerable instances.", "result": "Experimental results demonstrated that models like GPT-4o and o3-mini showed high hallucination rates of 64% and 44% respectively for unanswerable problems in worst-case scenarios.", "conclusion": "The findings emphasize the ongoing challenges for LLMs in accurately identifying unanswerable math problems, calling for improved model capabilities in reasoning.", "key_contributions": ["Introduction of the TreeCut synthetic dataset", "Demonstration of high hallucination rates in LLMs", "Identification of factors influencing hallucination likelihood."], "limitations": "The dataset is synthetic and may not fully capture the diversity of real-world problems.", "keywords": ["large language models", "math word problems", "dataset generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.14037", "pdf": "https://arxiv.org/pdf/2502.14037.pdf", "abs": "https://arxiv.org/abs/2502.14037", "title": "DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation", "authors": ["Giorgio Franceschelli", "Mirco Musolesi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite their growing capabilities, language models still frequently\nreproduce content from their training data, generate repetitive text, and favor\ncommon grammatical patterns and vocabulary. A possible cause is the decoding\nstrategy: the most common strategies either consider only the most probable\ntokens, which reduces output diversity, or increase the likelihood of unlikely\ntokens, compromising output accuracy and correctness. In this paper, we propose\nthree new decoding methods that leverage a mathematical analysis of the token\nprobability distribution to ensure the generation of contextually appropriate\ntext. In particular, the difference between consecutive, sorted probabilities\ncan be used to truncate incorrect tokens. Experiments concerning math problem\nsolving, extreme summarization, and the divergent association task demonstrate\nthat our approach consistently performs at least as well as existing methods in\nterms of quality and diversity.", "AI": {"tldr": "The paper proposes three new decoding methods for language models to improve text generation quality and diversity by leveraging a mathematical analysis of token probability distributions.", "motivation": "Language models often produce repetitive or overly common text due to limitations in their decoding strategies, prompting the need for improved methods that enhance output diversity and correctness.", "method": "The authors introduce three decoding methods that analyze the difference between consecutive sorted token probabilities to truncate unlikely candidates, aiming to produce more contextually relevant text.", "result": "Experiments in math problem solving, extreme summarization, and divergent association tasks show the new methods perform at least as well as existing techniques, offering comparable quality and improved diversity in generated text.", "conclusion": "The proposed decoding strategies enhance the ability of language models to generate diverse and accurate outputs, addressing common limitations in existing methods.", "key_contributions": ["Introduction of three novel decoding methods for language models", "Mathematical analysis of token probability distribution for improved text generation", "Demonstrated performance across various tasks indicates enhanced output quality and diversity"], "limitations": "", "keywords": ["language models", "decoding methods", "text generation", "probability distribution", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.14171", "pdf": "https://arxiv.org/pdf/2502.14171.pdf", "abs": "https://arxiv.org/abs/2502.14171", "title": "Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction", "authors": ["Mehdi Jafari", "Devin Yuncheng Hua", "Hao Xue", "Flora Salim"], "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Natural language interaction with agentic Artificial Intelligence (AI),\ndriven by Large Language Models (LLMs), is expected to remain a dominant\nparadigm in the near future. While humans instinctively align their\ncommunication with mental states -- an ability known as Theory of Mind (ToM),\ncurrent LLM powered systems exhibit significant limitations in this regard.\nThis study examines the extent to which open source language models (LLaMA) can\ncapture and preserve ToM related information and how effectively it contributes\nto consistent ToM reasoning in generated responses. We further investigate\nwhether explicit manipulation of ToM related components, such as beliefs,\ndesires, and intentions, can enhance response alignment. Experiments on two\nLLaMA 3 variants demonstrate that incorporating ToM informed alignment improves\nresponse quality, achieving win rates of 67 and 63 percent for the 3B and 8B\nmodels, respectively. These findings highlight the potential of ToM driven\nstrategies to improve alignment in LLM based conversational agents.", "AI": {"tldr": "Study examines how open-source LLaMA models can capture Theory of Mind (ToM) related information to improve response quality in AI interactions.", "motivation": "Investigate the limitations of current LLM systems in aligning communication with human mental states using Theory of Mind.", "method": "Experiments conducted on two LLaMA 3 variants to see how ToM aligned strategies affect AI response quality, focusing on beliefs, desires, and intentions.", "result": "ToM informed alignment improved response quality with win rates of 67% for the 3B model and 63% for the 8B model.", "conclusion": "ToM driven strategies can significantly enhance alignment and response quality in conversational agents powered by LLMs.", "key_contributions": ["Analysis of ToM in LLaMA models", "Demonstrated enhancement in LLM response quality through ToM alignment", "Empirical results showing win rates for different model sizes"], "limitations": "Limited to open-source LLaMA models; results may vary with different architectures or datasets.", "keywords": ["Theory of Mind", "Large Language Models", "Conversational Agents", "Response Quality", "Natural Language Interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.16051", "pdf": "https://arxiv.org/pdf/2502.16051.pdf", "abs": "https://arxiv.org/abs/2502.16051", "title": "Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare", "authors": ["Max Lamparth", "Declan Grabb", "Amy Franks", "Scott Gershan", "Kaitlyn N. Kunstman", "Aaron Lulla", "Monika Drummond Roots", "Manu Sharma", "Aryan Shrivastava", "Nina Vasan", "Colleen Waickman"], "categories": ["cs.CL"], "comment": "Added minor clarifications and expanded appendices", "summary": "Current medical language model (LM) benchmarks often over-simplify the\ncomplexities of day-to-day clinical practice tasks and instead rely on\nevaluating LMs on multiple-choice board exam questions. Thus, we present an\nexpert-created and annotated dataset spanning five critical domains of\ndecision-making in mental healthcare: treatment, diagnosis, documentation,\nmonitoring, and triage. This dataset - created without any LM assistance - is\ndesigned to capture the nuanced clinical reasoning and daily ambiguities mental\nhealth practitioners encounter, reflecting the inherent complexities of care\ndelivery that are missing from existing datasets. Almost all 203 base questions\nwith five answer options each have had the decision-irrelevant demographic\npatient information removed and replaced with variables (e.g., AGE), and are\navailable for male, female, or non-binary-coded patients. For question\ncategories dealing with ambiguity and multiple valid answer options, we create\na preference dataset with uncertainties from the expert annotations. We outline\na series of intended use cases and demonstrate the usability of our dataset by\nevaluating eleven off-the-shelf and four mental health fine-tuned LMs on\ncategory-specific task accuracy, on the impact of patient demographic\ninformation on decision-making, and how consistently free-form responses\ndeviate from human annotated samples.", "AI": {"tldr": "This paper presents a dataset for evaluating medical language models in mental healthcare, capturing the complexities of clinical decision-making.", "motivation": "To address the oversimplification of medical LMs that focus primarily on board exam questions rather than real-world clinical tasks.", "method": "An expert-created dataset involving 203 base questions across five domains of decision-making in mental healthcare, designed to reflect real-world complexities and ambiguities.", "result": "Evaluation of eleven off-the-shelf and four fine-tuned mental health LMs on task accuracy while considering how patient demographics impact decision-making.", "conclusion": "The proposed dataset serves as a valuable resource for enhancing the assessment of LMs in mental healthcare, focusing on the real challenges faced by practitioners.", "key_contributions": ["Creation of a nuanced dataset reflecting complex clinical reasoning in mental health care.", "Evaluation of multiple LMs against this dataset, highlighting relevant metrics on task accuracy and demographic influences.", "The introduction of a preference dataset dealing with ambiguities and multiple valid answers."], "limitations": "", "keywords": ["mental health", "language models", "dataset", "clinical decision-making", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.16747", "pdf": "https://arxiv.org/pdf/2502.16747.pdf", "abs": "https://arxiv.org/abs/2502.16747", "title": "SQLong: Enhanced NL2SQL for Longer Contexts with LLMs", "authors": ["Dai Quoc Nguyen", "Cong Duy Vu Hoang", "Duy Vu", "Gioacchino Tangari", "Thanh Tien Vu", "Don Dharmasiri", "Yuan-Fang Li", "Long Duong"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "Accepted to Table Representation Learning Workshop at ACL 2025", "summary": "Open-weight large language models (LLMs) have significantly advanced\nperformance in the Natural Language to SQL (NL2SQL) task. However, their\neffectiveness diminishes when dealing with large database schemas, as the\ncontext length increases. To address this limitation, we present SQLong, a\nnovel and efficient data augmentation framework designed to enhance LLM\nperformance in long-context scenarios for the NL2SQL task. SQLong generates\naugmented datasets by extending existing database schemas with additional\nsynthetic CREATE TABLE commands and corresponding data rows, sampled from\ndiverse schemas in the training data. This approach effectively simulates\nlong-context scenarios during finetuning and evaluation. Through experiments on\nthe Spider and BIRD datasets, we demonstrate that LLMs finetuned with\nSQLong-augmented data significantly outperform those trained on standard\ndatasets. These imply SQLong's practical implementation and its impact on\nimproving NL2SQL capabilities in real-world settings with complex database\nschemas.", "AI": {"tldr": "SQLong is a data augmentation framework that enhances LLM performance in NL2SQL tasks by simulating long-context scenarios with synthetic data.", "motivation": "The effectiveness of LLMs in NL2SQL tasks decreases with large database schemas due to increased context length.", "method": "SQLong generates augmented datasets by adding synthetic CREATE TABLE commands and data rows to existing database schemas, facilitating long-context finetuning and evaluation.", "result": "Experiments show that LLMs fine-tuned with SQLong-augmented data significantly outperform those trained on standard datasets.", "conclusion": "SQLong shows practical applicability and improves NL2SQL capabilities in complex real-world database scenarios.", "key_contributions": ["Introduces SQLong, a novel data augmentation framework for NL2SQL tasks.", "Demonstrates significant performance improvements in LLMs for long-context scenarios.", "Provides a practical solution to enhance SQL query generation with complex database schemas."], "limitations": "", "keywords": ["large language models", "NL2SQL", "data augmentation", "machine learning", "context length"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.16894", "pdf": "https://arxiv.org/pdf/2502.16894.pdf", "abs": "https://arxiv.org/abs/2502.16894", "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment", "authors": ["Chenghao Fan", "Zhenyi Lu", "Sichen Liu", "Chengfeng Gu", "Xiaoye Qu", "Wei Wei", "Yu Cheng"], "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\nLarge Language Models (LLMs), its performance often falls short of Full\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\nleveraging of pre-trained knowledge. Another path for improving LoRA is\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\n\\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t}\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\nefficiency and performance. Experiments across 25 datasets, including natural\nlanguage understanding, commonsense reasoning, image classification, and\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\nclosing the gap with Full FT.", "AI": {"tldr": "GOAT improves Low-Rank Adaptation for Large Language Models by using an SVD-structured Mixture-of-Experts architecture, enhancing performance to match Full Fine-Tuning.", "motivation": "To enhance the performance of Low-Rank Adaptation for Large Language Models, which often lags behind Full Fine-Tuning in efficiency and effectiveness.", "method": "The GOAT framework integrates relevant priors through a singular value decomposition (SVD)-structured Mixture-of-Experts and introduces a theoretical scaling factor to align optimization with full fine-tuned models.", "result": "Experiments across 25 datasets show GOAT achieves state-of-the-art performance, significantly closing the gap with Full Fine-Tuning.", "conclusion": "GOAT provides a parameter-efficient method that boosts the efficiency and performance of LoRA MoE without changing the architecture or training algorithms.", "key_contributions": ["Introduction of GOAT framework combining SVD with MoE for improved performance", "Demonstration of state-of-the-art results on various benchmarks", "Theoretical derivation of scaling factor for optimization alignment"], "limitations": "", "keywords": ["Low-Rank Adaptation", "Mixture-of-Experts", "Large Language Models", "Parameter-Efficient Fine-Tuning", "SVD"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.16901", "pdf": "https://arxiv.org/pdf/2502.16901.pdf", "abs": "https://arxiv.org/abs/2502.16901", "title": "Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs", "authors": ["Himanshu Beniwal", "Sailesh Panda", "Birudugadda Srivibhav", "Mayank Singh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We explore \\textbf{C}ross-lingual \\textbf{B}ackdoor \\textbf{AT}tacks (X-BAT)\nin multilingual Large Language Models (mLLMs), revealing how backdoors inserted\nin one language can automatically transfer to others through shared embedding\nspaces. Using toxicity classification as a case study, we demonstrate that\nattackers can compromise multilingual systems by poisoning data in a single\nlanguage, with rare and high-occurring tokens serving as specific, effective\ntriggers. Our findings expose a critical vulnerability that influences the\nmodel's architecture, resulting in a concealed backdoor effect during the\ninformation flow. Our code and data are publicly available\nhttps://github.com/himanshubeniwal/X-BAT.", "AI": {"tldr": "This paper investigates cross-lingual backdoor attacks in multilingual large language models, demonstrating the transfer of backdoors between languages through shared embeddings, using toxicity classification as a case study.", "motivation": "To understand and expose the vulnerability of multilingual large language models (mLLMs) to cross-lingual backdoor attacks that can be initiated in one language and affect the model's performance in others.", "method": "The authors conduct experiments on toxicity classification to illustrate how backdoors inserted in a single language can transfer to others through shared embedding spaces, focusing on the use of rare and frequent tokens as triggers.", "result": "The study reveals that one can effectively compromise multilingual systems by poisoning training data in a single language, leading to hidden backdoor effects during information processing in the model.", "conclusion": "The research highlights a critical vulnerability in mLLMs, urging the need for robust defenses against cross-lingual backdoor attacks.", "key_contributions": ["Identification of cross-lingual backdoor attack vulnerabilities in mLLMs", "Demonstration of effective triggers for backdoors in toxicity classification", "Public availability of code and data for further research"], "limitations": "The study primarily focuses on toxicity classification, and further exploration is needed across other tasks and languages to generalize findings.", "keywords": ["cross-lingual", "backdoor attacks", "multilingual models", "toxicity classification", "embedding spaces"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.19982", "pdf": "https://arxiv.org/pdf/2502.19982.pdf", "abs": "https://arxiv.org/abs/2502.19982", "title": "Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models", "authors": ["Huazheng Wang", "Yongcheng Jing", "Haifeng Sun", "Yingjie Wang", "Jingyu Wang", "Jianxin Liao", "Dacheng Tao"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this paper, we investigate knowledge forgetting in large language models\nwith a focus on its generalisation--ensuring that models forget not only\nspecific training samples but also related implicit knowledge. To this end, we\nbegin by identifying a broader unlearning scope that includes both target data\nand logically associated samples, including rephrased, subject-replaced,\none-hop reasoned, and relation-reversed data. To rigorously evaluate\ngeneralisation, we introduce UGBench, the first comprehensive benchmark\nspecifically designed to assess the unlearning of in-scope implicit knowledge\ncovering 13 state-of-the-art methods across three datasets. UGBench reveals\nthat unlearned models can still recall paraphrased answers and retain target\nfacts in intermediate layers. This motivates us to take a preliminary step\ntoward more generalised implicit knowledge forgetting by proposing PerMU, a\nnovel probability perturbation-based unlearning paradigm. PerMU simulates\nadversarial unlearning samples to eliminate fact-related tokens from the logit\ndistribution, collectively reducing the probabilities of all answer-associated\ntokens. Experiments are conducted on a diverse range of datasets, including\nTOFU, Harry Potter, ZsRE, WMDP, and MUSE, using models ranging from 1.3B to 13B\nin scale. The results demonstrate that PerMU delivers up to a 50.40%\nimprovement in unlearning vanilla target data while maintaining a 40.73% boost\nin forgetting implicit knowledge. Our code can be found in\nhttps://github.com/MaybeLizzy/UGBench.", "AI": {"tldr": "The paper investigates knowledge forgetting in large language models, proposing a novel unlearning paradigm called PerMU and introducing a benchmark UGBench for evaluating unlearning performance.", "motivation": "The goal is to ensure models forget specific training samples as well as related implicit knowledge, addressing the broader scope of knowledge forgetting.", "method": "The authors introduce UGBench to assess unlearning of implicit knowledge across various methods and datasets, and propose PerMU, a probability perturbation-based paradigm that utilizes adversarial unlearning samples.", "result": "PerMU achieves up to a 50.40% improvement in unlearning specified target data and a 40.73% increase in forgetting implicit knowledge across evaluated datasets.", "conclusion": "The findings highlight the potential for more significant generalisation in knowledge forgetting, suggesting a new direction in unlearning metrics and methods.", "key_contributions": ["Introduction of UGBench as a benchmark for evaluating unlearning in large language models.", "Development of PerMU, a novel unlearning paradigm that uses probability perturbation to enhance knowledge forgetting.", "Demonstration of significant improvements in unlearning specific and implicit knowledge with empirical results."], "limitations": "", "keywords": ["knowledge forgetting", "large language models", "unlearning", "UGBench", "PerMU"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.20592", "pdf": "https://arxiv.org/pdf/2502.20592.pdf", "abs": "https://arxiv.org/abs/2502.20592", "title": "Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document Processing", "authors": ["Juntai Cao", "Xiang Zhang", "Raymond Li", "Chuyuan Li", "Chenyu You", "Shafiq Joty", "Giuseppe Carenini"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in test-time scaling have shown promising results in\nimproving Large Language Model (LLM) performance through strategic computation\nallocation during inference. While this approach has demonstrated strong\nimprovements in logical and mathematical reasoning tasks, its application to\nnatural language generation (NLG), particularly summarization, remains\nunexplored. Multi-Document Summarization (MDS), a fundamental task in NLG,\npresents unique challenges by requiring models to extract and synthesize\nessential information across multiple lengthy documents. Unlike reasoning\ntasks, MDS demands a more nuanced approach to prompt design and ensemble\nmethods, as no single \"best\" prompt can satisfy diverse summarization\nrequirements. We propose a novel framework leveraging test-time scaling for\nMDS. Our approach employs prompt ensemble techniques to generate multiple\ncandidate summaries using various prompts, then combines them with an\naggregator to produce a refined summary. To evaluate our method effectively, we\nalso introduce two new LLM-based metrics: the Consistency-Aware Preference\n(CAP) score and LLM Atom-Content-Unit (LLM-ACU) score, which assess summary\nquality while addressing the positional bias inherent in traditional automatic\nevaluation. Our extensive experiments demonstrate that this framework\nsignificantly enhances summary quality while also revealing the practical\nscaling boundaries to MDS tasks.", "AI": {"tldr": "This paper introduces a novel framework for Multi-Document Summarization (MDS) using test-time scaling and prompt ensemble techniques to improve summary quality in natural language generation.", "motivation": "Explores the unexplored application of test-time scaling in MDS, impacted by the need for nuanced prompt and ensemble methods compared to logical reasoning tasks.", "method": "The proposed framework generates multiple candidate summaries using various prompts and combines them with an aggregator to produce a final refined summary, employing two new metrics for evaluation.", "result": "Extensive experiments show that the framework significantly enhances summary quality and identifies scaling limitations in MDS tasks.", "conclusion": "The results highlight the effectiveness of test-time scaling for improving Multi-Document Summarization and demonstrate the need for innovative evaluation metrics in natural language generation.", "key_contributions": ["Novel framework for Multi-Document Summarization using test-time scaling and prompt ensemble techniques.", "Introduction of two new evaluation metrics: Consistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (LLM-ACU) score.", "Empirical evidence of enhanced summary quality and practical scaling boundaries for MDS.", "Identification of unique challenges in applying LLMs to MDS compared to reasoning tasks."], "limitations": "The framework's performance may vary based on the complexity and diversity of the documents being summarized.", "keywords": ["Multi-Document Summarization", "Large Language Models", "test-time scaling", "natural language generation", "prompt ensemble"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.21074", "pdf": "https://arxiv.org/pdf/2502.21074.pdf", "abs": "https://arxiv.org/abs/2502.21074", "title": "CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation", "authors": ["Zhenyi Shen", "Hanqi Yan", "Linhai Zhang", "Zhanghao Hu", "Yali Du", "Yulan He"], "categories": ["cs.CL"], "comment": "16 pages", "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nencouraging step-by-step reasoning in natural language. However, leveraging a\nlatent continuous space for reasoning may offer benefits in terms of both\nefficiency and robustness. Prior implicit CoT methods attempt to bypass\nlanguage completely by reasoning in continuous space but have consistently\nunderperformed compared to the standard explicit CoT approach. We introduce\nCODI (Continuous Chain-of-Thought via Self-Distillation), a novel training\nframework that effectively compresses natural language CoT into continuous\nspace. CODI jointly trains a teacher task (Explicit CoT) and a student task\n(Implicit CoT), distilling the reasoning ability from language into continuous\nspace by aligning the hidden states of a designated token. Our experiments show\nthat CODI is the first implicit CoT approach to match the performance of\nexplicit CoT on GSM8k at the GPT-2 scale, achieving a 3.1x compression rate and\noutperforming the previous state-of-the-art by 28.2% in accuracy. CODI also\ndemonstrates robustness, generalizable to complex datasets, and\ninterpretability. These results validate that LLMs can reason effectively not\nonly in natural language, but also in a latent continuous space.", "AI": {"tldr": "The paper introduces CODI, a framework that compresses Chain-of-Thought reasoning from natural language into a continuous space, achieving improved efficiency, robustness, and performance.", "motivation": "To enhance Large Language Models' reasoning capabilities by using a latent continuous space, potentially improving efficiency and robustness over traditional methods.", "method": "CODI (Continuous Chain-of-Thought via Self-Distillation) systematically distills reasoning from explicit Chain-of-Thought (CoT) into an implicit continuous space, aligning hidden states between a teacher (Explicit CoT) and a student (Implicit CoT) during training.", "result": "CODI matches the performance of explicit CoT on the GSM8k benchmark at the GPT-2 scale with a 3.1x compression rate and 28.2% accuracy improvement over the previous state-of-the-art.", "conclusion": "The findings validate that LLMs can maintain effective reasoning capabilities not just in natural language, but also within a continuous latent space, offering advantages in terms of compression and robustness.", "key_contributions": ["Introduction of CODI framework for continuous CoT reasoning", "Demonstration of superior performance and compression compared to explicit CoT", "Showcasing robustness and interpretability of implicit reasoning models"], "limitations": "", "keywords": ["Chain-of-Thought", "Large Language Models", "Continuous Space", "Self-Distillation", "Implicit Reasoning"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2503.01513", "pdf": "https://arxiv.org/pdf/2503.01513.pdf", "abs": "https://arxiv.org/abs/2503.01513", "title": "Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey", "authors": ["Katerina Korre", "Dimitris Tsirmpas", "Nikos Gkoumas", "Emma Cabalé", "Danai Myrtzani", "Theodoros Evgeniou", "Ion Androutsopoulos", "John Pavlopoulos"], "categories": ["cs.CL"], "comment": null, "summary": "We present a survey of methods for assessing and enhancing the quality of\nonline discussions, focusing on the potential of LLMs. While online discourses\naim, at least in theory, to foster mutual understanding, they often devolve\ninto harmful exchanges, such as hate speech, threatening social cohesion and\ndemocratic values. Recent advancements in LLMs enable artificial facilitation\nagents to not only moderate content, but also actively improve the quality of\ninteractions. Our survey synthesizes ideas from NLP and Social Sciences to\nprovide (a) a new taxonomy on discussion quality evaluation, (b) an overview of\nintervention and facilitation strategies, (c) along with a new taxonomy of\nconversation facilitation datasets, (d) an LLM-oriented roadmap of good\npractices and future research directions, from technological and societal\nperspectives.", "AI": {"tldr": "Survey of methods using LLMs to improve online discussion quality and moderation.", "motivation": "To address harmful exchanges in online discussions that threaten social cohesion and democratic values.", "method": "Review and synthesis of existing literature in NLP and Social Sciences, leading to the formulation of new taxonomies and strategies.", "result": "Identification of a new taxonomy for discussion quality evaluation and conversation facilitation datasets, along with intervention strategies using LLMs.", "conclusion": "LLMs have significant potential to enhance interaction quality online, necessitating further research in technology and societal impacts.", "key_contributions": ["New taxonomy on discussion quality evaluation", "Overview of facilitation strategies and interventions", "LLM-oriented roadmap for future research directions"], "limitations": "", "keywords": ["online discussions", "LLMs", "discussion quality", "facilitation strategies", "NLP"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.02589", "pdf": "https://arxiv.org/pdf/2503.02589.pdf", "abs": "https://arxiv.org/abs/2503.02589", "title": "MCiteBench: A Multimodal Benchmark for Generating Text with Citations", "authors": ["Caiyu Hu", "Yikai Zhang", "Tinghui Zhu", "Yiwei Ye", "Yanghua Xiao"], "categories": ["cs.CL", "cs.IR"], "comment": "https://caiyuhu.github.io/MCiteBench/", "summary": "Multimodal Large Language Models (MLLMs) have advanced in integrating diverse\nmodalities but frequently suffer from hallucination. A promising solution to\nmitigate this issue is to generate text with citations, providing a transparent\nchain for verification. However, existing work primarily focuses on generating\ncitations for text-only content, leaving the challenges of multimodal scenarios\nlargely unexplored. In this paper, we introduce MCiteBench, the first benchmark\ndesigned to assess the ability of MLLMs to generate text with citations in\nmultimodal contexts. Our benchmark comprises data derived from academic papers\nand review-rebuttal interactions, featuring diverse information sources and\nmultimodal content. Experimental results reveal that MLLMs struggle to ground\ntheir outputs reliably when handling multimodal input. Further analysis\nuncovers a systematic modality bias and reveals how models internally rely on\ndifferent sources when generating citations, offering insights into model\nbehavior and guiding future directions for multimodal citation tasks.", "AI": {"tldr": "This paper introduces MCiteBench, the first benchmark for assessing Multimodal Large Language Models' (MLLMs) ability to generate text with citations in multimodal contexts, revealing challenges in grounding output and a systematic modality bias among models.", "motivation": "The work aims to address hallucination in MLLMs by improving citation generation for multimodal scenarios, an area that has been largely neglected in existing research.", "method": "The authors developed MCiteBench, a benchmark that includes data from academic papers and review-rebuttal interactions featuring diverse information sources and multimodal content, to evaluate MLLMs' citation performance.", "result": "Experimental results indicate that MLLMs have difficulty reliably grounding their outputs when processing multimodal input, exposing a systematic bias towards certain modalities during the citation generation process.", "conclusion": "The findings provide insights into model behavior during multimodal citation tasks and highlight potential future research directions to improve MLLMs' performance in this area.", "key_contributions": ["Introduction of MCiteBench benchmark for multimodal citation generation", "Identification of systematic modality bias in MLLMs", "Insights into model behavior and reliance on different information sources for citation generation"], "limitations": "The benchmark may not cover all possible multimodal scenarios and the analysis is limited to the examined models' architectures.", "keywords": ["multimodal", "large language models", "citations", "benchmark", "model behavior"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.04372", "pdf": "https://arxiv.org/pdf/2503.04372.pdf", "abs": "https://arxiv.org/abs/2503.04372", "title": "Assumed Identities: Quantifying Gender Bias in Machine Translation of Gender-Ambiguous Occupational Terms", "authors": ["Orfeas Menis Mastromichalakis", "Giorgos Filandrianos", "Maria Symeonaki", "Giorgos Stamou"], "categories": ["cs.CL"], "comment": null, "summary": "Machine Translation (MT) systems frequently encounter gender-ambiguous\noccupational terms, where they must assign gender without explicit contextual\ncues. While individual translations in such cases may not be inherently biased,\nsystematic patterns-such as consistently translating certain professions with\nspecific genders-can emerge, reflecting and perpetuating societal stereotypes.\nThis ambiguity challenges traditional instance-level single-answer evaluation\napproaches, as no single gold standard translation exists. To address this, we\nintroduce GRAPE, a probability-based metric designed to evaluate gender bias by\nanalyzing aggregated model responses. Alongside this, we present GAMBIT-MT, a\nbenchmarking dataset in English with gender-ambiguous occupational terms. Using\nGRAPE, we evaluate several MT systems and examine whether their gendered\ntranslations in Greek and French align with or diverge from societal\nstereotypes, real-world occupational gender distributions, and normative\nstandards.", "AI": {"tldr": "This paper introduces GRAPE, a metric for evaluating gender bias in Machine Translation systems, and GAMBIT-MT, a dataset with gender-ambiguous occupational terms. It analyzes translation biases and their alignment with societal norms.", "motivation": "To address the systematic gender bias in Machine Translation systems that emerges from the translation of gender-ambiguous occupational terms.", "method": "The paper proposes GRAPE, a probability-based metric, and uses it to evaluate MT systems against a newly created dataset, GAMBIT-MT, which contains gender-ambiguous terms in English.", "result": "Evaluation of various MT systems revealed differing patterns of gender translation in Greek and French that either aligned with or diverged from societal stereotypes and real-world gender distributions.", "conclusion": "The study highlights the need for improved evaluation metrics like GRAPE to assess gender bias in MT, suggesting that MT systems often perpetuate societal stereotypes rather than reflecting actual gender distributions.", "key_contributions": ["Introduction of the GRAPE metric for gender bias evaluation in MT systems.", "Creation of the GAMBIT-MT dataset for benchmarking gender ambiguity in translations.", "Analysis of gender translation patterns in relation to societal norms and real-world distributions."], "limitations": "The limitations of the study include a focus on only Greek and French translations, which may not represent wider linguistic diversity.", "keywords": ["Machine Translation", "gender bias", "evaluation metric", "GAMBIT-MT", "occupational terms"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.09579", "pdf": "https://arxiv.org/pdf/2503.09579.pdf", "abs": "https://arxiv.org/abs/2503.09579", "title": "Cost-Optimal Grouped-Query Attention for Long-Context Modeling", "authors": ["Yingfa Chen", "Yutong Wu", "Chenyang Song", "Zhen Leng Thai", "Xingyu Shen", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 pages, 15 figures", "summary": "Grouped-Query Attention (GQA) is a widely adopted strategy for reducing the\ncomputational cost of attention layers in large language models (LLMs).\nHowever, current GQA configurations are often suboptimal because they overlook\nhow context length influences inference cost. Since inference cost grows with\ncontext length, the most cost-efficient GQA configuration should also vary\naccordingly. In this work, we analyze the relationship among context length,\nmodel size, GQA configuration, and model loss, and introduce two innovations:\n(1) we decouple the total head size from the hidden size, enabling more\nflexible control over attention FLOPs; and (2) we jointly optimize the model\nsize and the GQA configuration to arrive at a better allocation of inference\nresources between attention layers and other components. Our analysis reveals\nthat commonly used GQA configurations are highly suboptimal for long-context\nscenarios. More importantly, we propose a recipe for deriving cost-optimal GQA\nconfigurations. Our results show that for long-context scenarios, one should\nuse fewer attention heads while scaling up model size. Configurations selected\nby our recipe can reduce both memory usage and FLOPs by more than 50% compared\nto Llama-3's GQA, with *no degradation in model capabilities*. Our findings\noffer valuable insights for designing efficient long-context LLMs. The code is\navailable at https://www.github.com/THUNLP/cost-optimal-gqa .", "AI": {"tldr": "This paper introduces a method to optimize Grouped-Query Attention (GQA) in large language models by considering context length to minimize computational costs without degrading performance.", "motivation": "To improve the efficiency of attention layers in large language models by optimizing GQA configurations based on context length.", "method": "Analysis of the relationship among context length, model size, GQA configuration, and model loss; introduces decoupling of total head size from hidden size and joint optimization of model size and GQA configuration.", "result": "Proposed configurations can reduce memory usage and FLOPs by over 50% compared to current Llama-3's GQA while maintaining model performance.", "conclusion": "The findings suggest new strategies for designing efficient long-context LLMs and provide a systematic approach for deriving cost-optimal GQA configurations.", "key_contributions": ["Decoupling total head size from hidden size for better control of attention FLOPs.", "Joint optimization of model size and GQA configuration for efficient resource allocation.", "A recipe for deriving cost-optimal GQA configurations suited for long-context scenarios."], "limitations": "", "keywords": ["Large Language Models", "Grouped-Query Attention", "Cost Optimization", "Context Length", "Model Efficiency"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2503.10657", "pdf": "https://arxiv.org/pdf/2503.10657.pdf", "abs": "https://arxiv.org/abs/2503.10657", "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs", "authors": ["Zhongzhan Huang", "Guoming Ling", "Yupei Lin", "Yandong Chen", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Routing large language models (LLMs) is a new paradigm that uses a router to\nrecommend the best LLM from a pool of candidates for a given input. In this\npaper, our comprehensive analysis with more than 8,500 LLMs reveals a novel\nmodel-level scaling up phenomenon in Routing LLMs, i.e., a capable router can\nsignificantly enhance the performance of this paradigm as the number of\ncandidates increases. This improvement can even surpass the performance of the\nbest single model in the pool and many existing strong LLMs, confirming it a\nhighly promising paradigm. However, the lack of comprehensive and open-source\nbenchmarks for Routing LLMs has hindered the development of routers. In this\npaper, we introduce RouterEval, a benchmark tailored for router research, which\nincludes over 200,000,000 performance records for 12 popular LLM evaluations\nacross various areas such as commonsense reasoning, semantic understanding,\netc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations\nof existing Routing LLM methods reveal that most still have significant room\nfor improvement. See https://github.com/MilkThink-Lab/RouterEval for all data,\ncode and tutorial.", "AI": {"tldr": "This paper introduces RouterEval, a benchmark for evaluating Routing Large Language Models (LLMs), highlighting their performance enhancement potential when a capable router is employed.", "motivation": "The need for comprehensive and open-source benchmarks for Routing LLMs to facilitate their development and evaluation.", "method": "Analysis of over 8,500 LLMs and introduction of RouterEval benchmark with 200 million performance records for evaluating Routing LLM methods.", "result": "The analysis shows that capable routers can significantly enhance performance beyond that of the best single model in the pool, revealing substantial room for improvement in existing Routing LLM methods.", "conclusion": "RouterEval provides a solid foundation for future research in Routing LLMs, addressing the benchmarking gap that hinders router development.", "key_contributions": ["Introduction of RouterEval benchmark", "Analysis of performance as LLM candidate numbers increase", "Demonstration of significant room for improvement in existing methods"], "limitations": "Limited to the current set of 8,500 LLMs and their evaluations; effectiveness of routers can vary by domain.", "keywords": ["Routing LLMs", "Benchmark", "Machine Learning", "Performance Enhancement"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.18132", "pdf": "https://arxiv.org/pdf/2503.18132.pdf", "abs": "https://arxiv.org/abs/2503.18132", "title": "MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection", "authors": ["Yibo Yan", "Shen Wang", "Jiahao Huo", "Philip S. Yu", "Xuming Hu", "Qingsong Wen"], "categories": ["cs.CL"], "comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL Industry 2025, Oral Presentation)", "summary": "Mathematical error detection in educational settings presents a significant\nchallenge for Multimodal Large Language Models (MLLMs), requiring a\nsophisticated understanding of both visual and textual mathematical content\nalong with complex reasoning capabilities. Though effective in mathematical\nproblem-solving, MLLMs often struggle with the nuanced task of identifying and\ncategorizing student errors in multimodal mathematical contexts. Therefore, we\nintroduce MathAgent, a novel Mixture-of-Math-Agent framework designed\nspecifically to address these challenges. Our approach decomposes error\ndetection into three phases, each handled by a specialized agent: an image-text\nconsistency validator, a visual semantic interpreter, and an integrative error\nanalyzer. This architecture enables more accurate processing of mathematical\ncontent by explicitly modeling relationships between multimodal problems and\nstudent solution steps. We evaluate MathAgent on real-world educational data,\ndemonstrating approximately 5% higher accuracy in error step identification and\n3% improvement in error categorization compared to baseline models. Besides,\nMathAgent has been successfully deployed in an educational platform that has\nserved over one million K-12 students, achieving nearly 90% student\nsatisfaction while generating significant cost savings by reducing manual error\ndetection.", "AI": {"tldr": "The paper introduces MathAgent, a Mixture-of-Math-Agent framework that improves mathematical error detection in educational settings using MLLMs, achieving higher accuracy and student satisfaction.", "motivation": "Mathematical error detection in education is challenging for MLLMs due to their need for complex reasoning and understanding of visual and textual content.", "method": "MathAgent decomposes error detection into three phases, handled by specialized agents: an image-text consistency validator, a visual semantic interpreter, and an integrative error analyzer.", "result": "MathAgent shows approximately 5% higher accuracy in error step identification and a 3% improvement in error categorization on real-world educational data compared to baseline models.", "conclusion": "MathAgent has been effectively deployed in educational platforms, achieving high student satisfaction and cost savings in manual error detection.", "key_contributions": ["Introduction of a novel Mixture-of-Math-Agent framework for error detection", "Decomposition of error detection into specialized phases", "Demonstrated deployment with high student satisfaction"], "limitations": "", "keywords": ["mathematical error detection", "multimodal large language models", "educational technology"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.02438", "pdf": "https://arxiv.org/pdf/2504.02438.pdf", "abs": "https://arxiv.org/abs/2504.02438", "title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation", "authors": ["Chuanqi Cheng", "Jian Guan", "Wei Wu", "Rui Yan"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Long-form video processing fundamentally challenges vision-language models\n(VLMs) due to the high computational costs of handling extended temporal\nsequences. Existing token pruning and feature merging methods often sacrifice\ncritical temporal dependencies or dilute semantic information. We introduce\ndifferential distillation, a principled approach that systematically preserves\ntask-relevant information while suppressing redundancy. Based on this\nprinciple, we develop ViLAMP, a hierarchical video-language model that\nprocesses hour-long videos at \"mixed precision\" through two key mechanisms: (1)\ndifferential keyframe selection that maximizes query relevance while\nmaintaining temporal distinctiveness at the frame level and (2) differential\nfeature merging that preserves query-salient features in non-keyframes at the\npatch level. Hence, ViLAMP retains full information in keyframes while reducing\nnon-keyframes to their most salient features, resembling mixed-precision\ntraining. Extensive experiments demonstrate ViLAMP's superior performance\nacross five video understanding benchmarks, particularly on long-form content.\nNotably, ViLAMP can process ultra-long videos (up to 10K frames) on a single\nNVIDIA A100 GPU, achieving substantial computational efficiency while\nmaintaining state-of-the-art performance. Code and model are available at\nhttps://github.com/steven-ccq/ViLAMP.", "AI": {"tldr": "Introducing ViLAMP, a hierarchical video-language model that efficiently processes long videos using differential distillation techniques to preserve important information and improve computational efficiency.", "motivation": "To address the computational challenges of processing long-form videos with vision-language models while preserving critical temporal dependencies and semantic information.", "method": "ViLAMP uses differential keyframe selection to maximize query relevance and maintain temporal distinctiveness, along with differential feature merging to keep query-salient features in non-keyframes, optimizing processing of videos at mixed precision.", "result": "ViLAMP demonstrates superior performance on five video understanding benchmarks, particularly with ultra-long videos, achieving significant computational efficiency while maintaining state-of-the-art results.", "conclusion": "ViLAMP can process long videos while preserving essential information, demonstrating both efficiency and effectiveness in video understanding tasks.", "key_contributions": ["Introduces differential distillation to retain task-relevant information in video processing.", "Develops a hierarchical model capable of handling ultra-long videos on limited hardware.", "Achieves state-of-the-art performance on video understanding benchmarks with improved computational efficiency."], "limitations": "", "keywords": ["video processing", "vision-language models", "differential distillation", "long-form videos", "computational efficiency"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.08399", "pdf": "https://arxiv.org/pdf/2504.08399.pdf", "abs": "https://arxiv.org/abs/2504.08399", "title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models", "authors": ["Yin Jou Huang", "Rafik Hadfi"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 6 figures, 6 tables", "summary": "Self-report questionnaires have long been used to assess LLM personality\ntraits, yet they fail to capture behavioral nuances due to biases and\nmeta-knowledge contamination. This paper proposes a novel multi-observer\nframework for personality trait assessments in LLM agents that draws on\ninformant-report methods in psychology. Instead of relying on self-assessments,\nwe employ multiple observer agents. Each observer is configured with a specific\nrelational context (e.g., family member, friend, or coworker) and engages the\nsubject LLM in dialogue before evaluating its behavior across the Big Five\ndimensions. We show that these observer-report ratings align more closely with\nhuman judgments than traditional self-reports and reveal systematic biases in\nLLM self-assessments. We also found that aggregating responses from 5 to 7\nobservers reduces systematic biases and achieves optimal reliability. Our\nresults highlight the role of relationship context in perceiving personality\nand demonstrate that a multi-observer paradigm offers a more reliable,\ncontext-sensitive approach to evaluating LLM personality traits.", "AI": {"tldr": "The paper introduces a multi-observer framework for assessing LLM personality traits, emphasizing the need for context in evaluations.", "motivation": "Traditional self-report questionnaires for assessing LLM personality traits suffer from biases and do not fully capture behavioral nuances.", "method": "The authors propose using multiple observer agents, each representing different relational contexts, to evaluate an LLM's behavior based on dialogue interactions before rating across the Big Five personality dimensions.", "result": "Observer-report ratings closely align with human judgments and reveal biases in LLM self-assessments. Aggregating ratings from multiple observers improves reliability and reduces bias.", "conclusion": "A multi-observer paradigm provides a more reliable and context-sensitive approach to evaluating LLM personality traits compared to conventional self-assessment methods.", "key_contributions": ["Introduction of a multi-observer framework for LLM personality assessments", "Demonstration of the importance of relational context in personality evaluation", "Evidence that aggregated observer ratings improve reliability and reduce bias."], "limitations": "The framework relies on the setup of appropriate observer agents and may require extensive dialogue interactions.", "keywords": ["LLM", "personality assessment", "multi-observer framework", "Big Five", "bias"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2504.10368", "pdf": "https://arxiv.org/pdf/2504.10368.pdf", "abs": "https://arxiv.org/abs/2504.10368", "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models", "authors": ["Wenyuan Zhang", "Shuaiyi Nie", "Xinghua Zhang", "Zefeng Zhang", "Tingwen Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "31 pages, 9 figures, 16 tables", "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate the performance\nof Large Reasoning Models (LRMs) on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their heavy reliance on system 2 thinking may limit their system 1\nthinking capabilities. However, there is a lack of an appropriate benchmark for\nevaluating LRM's system 1 thinking capabilities. To fill this gap, S1-Bench\nintroduces a suite of simple, diverse, and natural questions across multiple\ndomains and languages, specifically designed to assess LRMs' performance on\nquestions more suitable for system 1 . We conduct extensive evaluations across\n28 LRMs, revealing their inefficiency, inadequate accuracy, and limited\nrobustness when handling simple questions. Additionally, we observe a gap\nbetween their difficulty perception and generation length. Overall, this work\npaves the way toward dual-system compatibility in the development of LRMs.", "AI": {"tldr": "S1-Bench is a benchmark for evaluating Large Reasoning Models' performance on simple tasks favoring system 1 thinking, revealing inefficiencies and limitations of LRMs in handling these tasks.", "motivation": "The paper addresses the lack of an appropriate benchmark to evaluate Large Reasoning Models (LRMs) on simple tasks that utilize intuitive system 1 thinking, as current evaluations focus on complex reasoning tasks that rely on deliberative system 2 thought.", "method": "S1-Bench introduces a suite of simple, diverse questions across multiple domains and languages to assess the performance of LRMs on tasks suitable for system 1 thinking.", "result": "Extensive evaluations across 28 LRMs showed inefficiencies, inadequate accuracy, and limited robustness when handling simple questions, revealing discrepancies in difficulty perception and generation length.", "conclusion": "The findings highlight the need for dual-system compatibility in the development of LRMs, addressing their limitations in system 1 thinking.", "key_contributions": ["Introduction of S1-Bench benchmark for system 1 thinking in LRMs", "Evaluation of 28 LRMs revealing performance gaps", "Identification of a need for dual-system compatibility in LRM development"], "limitations": "", "keywords": ["Large Reasoning Models", "system 1 thinking", "benchmark", "human-computer interaction", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.12324", "pdf": "https://arxiv.org/pdf/2504.12324.pdf", "abs": "https://arxiv.org/abs/2504.12324", "title": "Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction", "authors": ["Mengying Yuan", "Wenhao Wang", "Zixuan Wang", "Yujie Huang", "Kangli Wei", "Fei Li", "Chong Teng", "Donghong Ji"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a fundamental task in natural language\nprocessing. While NLI has developed many sub-directions such as sentence-level\nNLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI\n(CDCL-NLI) remains largely unexplored. In this paper, we propose a novel\nparadigm: CDCL-NLI, which extends traditional NLI capabilities to\nmulti-document, multilingual scenarios. To support this task, we construct a\nhigh-quality CDCL-NLI dataset including 25,410 instances and spanning 26\nlanguages. To address the limitations of previous methods on CDCL-NLI task, we\nfurther propose an innovative method that integrates RST-enhanced graph fusion\nwith interpretability-aware prediction. Our approach leverages RST (Rhetorical\nStructure Theory) within heterogeneous graph neural networks for cross-document\ncontext modeling, and employs a structure-aware semantic alignment based on\nlexical chains for cross-lingual understanding. For NLI interpretability, we\ndevelop an EDU (Elementary Discourse Unit)-level attribution framework that\nproduces extractive explanations. Extensive experiments demonstrate our\napproach's superior performance, achieving significant improvements over both\nconventional NLI models as well as large language models. Our work sheds light\non the study of NLI and will bring research interest on cross-document\ncross-lingual context understanding, hallucination elimination and\ninterpretability inference. Our code and datasets are available at\n\\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-link} for peer\nreview.", "AI": {"tldr": "The paper introduces a novel Cross-Document Cross-Lingual Natural Language Inference (CDCL-NLI) paradigm with a new dataset and an innovative methodology that enhances NLI performance.", "motivation": "To explore the largely unexplored area of Cross-Document Cross-Lingual NLI and improve existing NLI capabilities.", "method": "The authors propose a new method that combines RST-enhanced graph fusion with an interpretability-aware prediction framework to model cross-document and cross-lingual contexts.", "result": "Extensive experiments show that the proposed approach significantly outperforms conventional NLI models and large language models.", "conclusion": "The research contributes to the field of NLI by promoting interest in cross-document and cross-lingual understanding and providing methods for eliminating hallucinations and enhancing interpretability.", "key_contributions": ["Introduction of the CDCL-NLI paradigm", "Development of a high-quality dataset with 25,410 instances in 26 languages", "Innovative RST-enhanced graph neural networks for cross-document context modeling"], "limitations": "", "keywords": ["Natural Language Inference", "Cross-Document NLI", "Cross-Lingual NLI"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.14150", "pdf": "https://arxiv.org/pdf/2504.14150.pdf", "abs": "https://arxiv.org/abs/2504.14150", "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations", "authors": ["Katie Matton", "Robert Osazuwa Ness", "John Guttag", "Emre Kıcıman"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": "66 pages, 14 figures, 40 tables; ICLR 2025 (spotlight) camera ready", "summary": "Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions.", "AI": {"tldr": "This paper presents a new approach to measure the faithfulness of explanations given by large language models, addressing potential misrepresentations in their reasoning processes.", "motivation": "The reliability of large language models (LLMs) in generating explanations for their answers is questionable, leading to over-trust and misuse. This study aims to define and measure the faithfulness of LLM explanations.", "method": "The paper introduces a rigorous definition of faithfulness based on the comparison between influential concepts in LLM explanations and actual influential concepts. It employs an auxiliary LLM to create counterfactuals and uses a Bayesian hierarchical model to quantify causal effects at both example and dataset levels.", "result": "Experiments demonstrate that the proposed method effectively quantifies unfaithfulness in LLM explanations, revealing biases in social contexts and misleading claims in medical question answering.", "conclusion": "By identifying patterns of unfaithfulness, this research underscores the importance of understanding the limitations of LLM explanations to avoid misuse and enhance trust.", "key_contributions": ["A rigorous definition of faithfulness for LLM explanations.", "A novel measurement approach utilizing auxiliary LLMs for counterfactuals and Bayesian models for causality.", "Identification of unfaithfulness patterns in both social bias and medical domains."], "limitations": "The study is limited to specific tasks and may not generalize across all domains. Further validation is required.", "keywords": ["large language models", "faithfulness", "explanation", "causal inference", "social bias"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2504.15241", "pdf": "https://arxiv.org/pdf/2504.15241.pdf", "abs": "https://arxiv.org/abs/2504.15241", "title": "MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety", "authors": ["Yahan Yang", "Soham Dan", "Shuo Li", "Dan Roth", "Insup Lee"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual settings, where multilingual safety-aligned data\nis often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we introduce a multilingual\nguardrail with reasoning for prompt classification. Our method consists of: (1)\nsynthetic multilingual data generation incorporating culturally and\nlinguistically nuanced variants, (2) supervised fine-tuning, and (3) a\ncurriculum-based Group Relative Policy Optimization (GRPO) framework that\nfurther improves performance. Experimental results demonstrate that our\nmultilingual guardrail, MrGuard, consistently outperforms recent baselines\nacross both in-domain and out-of-domain languages by more than 15%. We also\nevaluate MrGuard's robustness to multilingual variations, such as\ncode-switching and low-resource language distractors in the prompt, and\ndemonstrate that it preserves safety judgments under these challenging\nconditions. The multilingual reasoning capability of our guardrail enables it\nto generate explanations, which are particularly useful for understanding\nlanguage-specific risks and ambiguities in multilingual content moderation.", "AI": {"tldr": "This paper introduces MrGuard, a multilingual guardrail for LLMs that detects and filters unsafe content by leveraging synthetic multilingual data, supervised fine-tuning, and a novel GRPO framework.", "motivation": "The need for a robust guardrail to protect against adversarial attacks on LLMs, especially in multilingual contexts with limited safety-aligned data.", "method": "The approach includes generating synthetic multilingual data, supervised fine-tuning of models, and a curriculum-based Group Relative Policy Optimization framework to enhance performance.", "result": "MrGuard outperforms existing baselines by over 15% across various languages, demonstrating resilience against multilingual variations and maintaining safety judgments.", "conclusion": "The multilingual reasoning capacity of MrGuard facilitates explanations, assisting in the understanding of language-specific risks in content moderation.", "key_contributions": ["Introduction of a multilingual guardrail for LLMs", "Innovative use of synthetic data generation for cultural nuances", "Development of a Group Relative Policy Optimization framework for improved safety performance."], "limitations": "", "keywords": ["Large Language Models", "multilingual guardrail", "adversarial attacks", "content moderation", "Group Relative Policy Optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.09930", "pdf": "https://arxiv.org/pdf/2505.09930.pdf", "abs": "https://arxiv.org/abs/2505.09930", "title": "Rethinking Prompt Optimizers: From Prompt Merits to Optimization", "authors": ["Zixiao Zhu", "Hanzhang Zhou", "Zijian Feng", "Tianjiao Li", "Chua Jia Jim Deryl", "Mak Lee Onn", "Gee Wah Ng", "Kezhi Mao"], "categories": ["cs.CL"], "comment": "21 pages, 14 figures", "summary": "Prompt optimization (PO) provides a practical way to improve response quality\nwhen users lack the time or expertise to manually craft effective prompts.\nExisting methods typically rely on advanced, large-scale LLMs like GPT-4 to\ngenerate optimized prompts. However, due to limited downward compatibility,\nverbose, instruction-heavy prompts from advanced LLMs can overwhelm lightweight\ninference models and degrade response quality. In this work, we rethink prompt\noptimization through the lens of interpretable design. We first identify a set\nof model-agnostic prompt quality merits and empirically validate their\neffectiveness in enhancing prompt and response quality. We then introduce MePO,\na merit-guided, lightweight, and locally deployable prompt optimizer trained on\nour preference dataset built from merit-aligned prompts generated by a\nlightweight LLM. Unlike prior work, MePO avoids online optimization reliance,\nreduces cost and privacy concerns, and, by learning clear, interpretable\nmerits, generalizes effectively to both large-scale and lightweight inference\nmodels. Experiments demonstrate that MePO achieves better results across\ndiverse tasks and model types, offering a scalable and robust solution for\nreal-world deployment. The code and dataset can be found in\nhttps://github.com/MidiyaZhu/MePO", "AI": {"tldr": "This paper presents MePO, a new merit-guided prompt optimizer that enhances the quality of responses in various models while addressing cost and privacy issues.", "motivation": "To improve response quality in LLMs without relying on advanced models that can degrade performance in lightweight inference scenarios.", "method": "MePO utilizes a set of model-agnostic prompt quality merits and is trained on a preference dataset made from merit-aligned prompts generated by a lightweight LLM.", "result": "MePO outperforms previous prompt optimization methods across a range of tasks and models by providing a scalable and interpretable solution.", "conclusion": "MePO is a robust alternative for prompt optimization that enhances quality while minimizing deployment concerns.", "key_contributions": ["Development of the MePO prompt optimizer", "Introduction of interpretable design in prompt optimization", "Validation of model-agnostic prompt quality merits"], "limitations": "", "keywords": ["prompt optimization", "machine learning", "interpretable design", "scalable solutions", "lightweight LLM"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.10081", "pdf": "https://arxiv.org/pdf/2505.10081.pdf", "abs": "https://arxiv.org/abs/2505.10081", "title": "Designing and Contextualising Probes for African Languages", "authors": ["Wisdom Aduah", "Francois Meyer"], "categories": ["cs.CL"], "comment": null, "summary": "Pretrained language models (PLMs) for African languages are continually\nimproving, but the reasons behind these advances remain unclear. This paper\npresents the first systematic investigation into probing PLMs for linguistic\nknowledge about African languages. We train layer-wise probes for six\ntypologically diverse African languages to analyse how linguistic features are\ndistributed. We also design control tasks, a way to interpret probe\nperformance, for the MasakhaPOS dataset. We find PLMs adapted for African\nlanguages to encode more linguistic information about target languages than\nmassively multilingual PLMs. Our results reaffirm previous findings that\ntoken-level syntactic information concentrates in middle-to-last layers, while\nsentence-level semantic information is distributed across all layers. Through\ncontrol tasks and probing baselines, we confirm that performance reflects the\ninternal knowledge of PLMs rather than probe memorisation. Our study applies\nestablished interpretability techniques to African-language PLMs. In doing so,\nwe highlight the internal mechanisms underlying the success of strategies like\nactive learning and multilingual adaptation.", "AI": {"tldr": "This paper investigates probing pretrained language models for linguistic knowledge related to African languages, revealing that African-specific models encode more linguistic information than multilingual models.", "motivation": "To improve understanding of how pretrained language models encode linguistic features specific to African languages and to evaluate the effectiveness of these models in comparison to multilingual ones.", "method": "The study involves training layer-wise probes for six African languages and designing control tasks using the MasakhaPOS dataset to assess probe performance and interpreted results.", "result": "African language-specific PLMs were found to encode more linguistic information compared to massively multilingual PLMs, with syntactic information concentrated in the middle-to-last layers and sentence-level semantic information spread across all layers.", "conclusion": "The findings affirm that probing methods can reveal distinct knowledge within PLMs, aiding in understanding model performance and the underlying mechanisms effective in multilingual adaptation and active learning.", "key_contributions": ["First systematic probing of PLMs for African languages.", "Comparison of linguistic knowledge in African-specific versus multilingual PLMs.", "Application of interpretability techniques to African language models."], "limitations": "", "keywords": ["pretrained language models", "African languages", "linguistic knowledge", "interpretability", "multilingual adaptation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.16505", "pdf": "https://arxiv.org/pdf/2503.16505.pdf", "abs": "https://arxiv.org/abs/2503.16505", "title": "Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions", "authors": ["Dimitris Tsirmpas", "Ion Androutsopoulos", "John Pavlopoulos"], "categories": ["cs.HC", "cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "19 pages, 3 tables, 12 figures", "summary": "Limited large-scale evaluations exist for facilitation strategies of online\ndiscussions due to significant costs associated with human involvement. An\neffective solution is synthetic discussion simulations using Large Language\nModels (LLMs) to create initial pilot experiments. We propose a simple,\ngeneralizable, LLM-driven methodology to prototype the development of LLM\nfacilitators, and produce high-quality synthetic data without human\ninvolvement. We use our methodology to test whether current facilitation\nstrategies can improve the performance of LLM facilitators. We find that, while\nLLM facilitators significantly improve synthetic discussions, there is no\nevidence that the application of more elaborate facilitation strategies\nproposed in modern Social Science research lead to further improvements in\ndiscussion quality, compared to more basic approaches. Additionally, we find\nthat small LLMs (such as Mistral Nemo 12B) can perform comparably to larger\nmodels (such as LLaMa 70B), and that special instructions must be used for\ninstruction-tuned models to induce toxicity in synthetic discussions. We\nconfirm that each component of our methodology contributes substantially to\nhigh quality data via an ablation study. We release an open-source framework,\n\"SynDisco\" (pip install syndisco), which implements our methodology. We also\nrelease the \"Virtual Moderation Dataset\"\n(https://paperswithcode.com/dataset/vmd), a large, publicly available dataset\ncontaining LLM-generated and LLM-annotated discussions using multiple\nopen-source LLMs.", "AI": {"tldr": "This paper presents a methodology for using LLMs to simulate online discussions and test facilitation strategies without human involvement.", "motivation": "Existing large-scale evaluations of online discussion facilitation strategies are limited due to high costs. The authors aim to develop a cost-effective solution using synthetic discussion simulations with LLMs.", "method": "The authors propose a generalizable LLM-driven methodology for prototyping LLM facilitators and generating high-quality synthetic discussion data.", "result": "LLM facilitators significantly improve discussion quality, but more complex facilitation strategies do not show additional benefits. Smaller LLMs perform comparably to larger ones. The study confirms the effectiveness of each methodology component through an ablation study.", "conclusion": "The proposed framework, SynDisco, is effective in creating synthetic discussions, and the Virtual Moderation Dataset is a valuable resource for further research.", "key_contributions": ["Development of a generalizable LLM-driven methodology for discussion facilitation", "Findings that smaller models can perform on par with larger ones", "Release of SynDisco framework and Virtual Moderation Dataset for public use"], "limitations": "", "keywords": ["Large Language Models", "Synthetic Discussions", "Facilitation Strategies"], "importance_score": 8, "read_time_minutes": 20}}
