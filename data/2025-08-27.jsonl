{"id": "2508.18283", "pdf": "https://arxiv.org/pdf/2508.18283.pdf", "abs": "https://arxiv.org/abs/2508.18283", "title": "Technology-assisted Personalized Yoga for Better Health -- Challenges and Outlook", "authors": ["Vivek Kumar", "Himanshu Sahu", "Hari Prabhat Gupta", "Biplav Srivastava"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "10 Pages, 11 figures, 2 tables", "summary": "Yoga is a discipline of physical postures, breathing techniques, and\nmeditative practices rooted in ancient Indian traditions, now embraced\nworldwide for promoting overall well-being and inner balance. The practices are\na large set of items, our term for executable actions like physical poses or\nbreath exercises, to offer for a person's well-being. However, to get benefits\nof Yoga tailored to a person's unique needs, a person needs to (a) discover\ntheir subset from the large and seemingly complex set with inter-dependencies,\n(b) continue to follow them with interest adjusted to their changing abilities\nand near-term objectives, and (c) as appropriate, adapt to alternative items\nbased on changing environment and the person's health conditions. In this\nvision paper, we describe the challenges for the Yoga personalization problem.\nNext, we sketch a preliminary approach and use the experience to provide an\noutlook on solving the challenging problem using existing and novel techniques\nfrom a multidisciplinary computing perspective. To the best of our knowledge,\nthis is the first paper that comprehensively examines decision support issues\naround Yoga personalization, from pose sensing to recommendation of corrections\nfor a complete regimen, and illustrates with a case study of Surya Namaskar --\na set of 12 choreographed poses."}
{"id": "2508.18317", "pdf": "https://arxiv.org/pdf/2508.18317.pdf", "abs": "https://arxiv.org/abs/2508.18317", "title": "Does Calibration Affect Human Actions?", "authors": ["Meir Nizri", "Amos Azaria", "Chirag Gupta", "Noam Hazon"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Calibration has been proposed as a way to enhance the reliability and\nadoption of machine learning classifiers. We study a particular aspect of this\nproposal: how does calibrating a classification model affect the decisions made\nby non-expert humans consuming the model's predictions? We perform a\nHuman-Computer-Interaction (HCI) experiment to ascertain the effect of\ncalibration on (i) trust in the model, and (ii) the correlation between\ndecisions and predictions. We also propose further corrections to the reported\ncalibrated scores based on Kahneman and Tversky's prospect theory from\nbehavioral economics, and study the effect of these corrections on trust and\ndecision-making. We find that calibration is not sufficient on its own; the\nprospect theory correction is crucial for increasing the correlation between\nhuman decisions and the model's predictions. While this increased correlation\nsuggests higher trust in the model, responses to ``Do you trust the model\nmore?\" are unaffected by the method used."}
{"id": "2508.18481", "pdf": "https://arxiv.org/pdf/2508.18481.pdf", "abs": "https://arxiv.org/abs/2508.18481", "title": "Impact of Target and Tool Visualization on Depth Perception and Usability in Optical See-Through AR", "authors": ["Yue Yang", "Xue Xie", "Xinkai Wang", "Hui Zhang", "Chiming Yu", "Xiaoxian Xiong", "Lifeng Zhu", "Yuanyi Zheng", "Jue Cen", "Bruce Daniel", "Fred Baik"], "categories": ["cs.HC", "cs.CV", "cs.GR"], "comment": null, "summary": "Optical see-through augmented reality (OST-AR) systems like Microsoft\nHoloLens 2 hold promise for arm's distance guidance (e.g., surgery), but depth\nperception of the hologram and occlusion of real instruments remain\nchallenging. We present an evaluation of how visualizing the target object with\ndifferent transparencies and visualizing a tracked tool (virtual proxy vs. real\ntool vs. no tool tracking) affects depth perception and system usability. Ten\nparticipants performed two experiments on HoloLens 2. In Experiment 1, we\ncompared high-transparency vs. low-transparency target rendering in a depth\nmatching task at arm's length. In Experiment 2, participants performed a\nsimulated surgical pinpoint task on a frontal bone target under six\nvisualization conditions ($2 \\times 3$: two target transparencies and three\ntool visualization modes: virtual tool hologram, real tool, or no tool\ntracking). We collected data on depth matching error, target localization\nerror, system usability, task workload, and qualitative feedback. Results show\nthat a more opaque target yields significantly lower depth estimation error\nthan a highly transparent target at arm's distance. Moreover, showing the real\ntool (occluding the virtual target) led to the highest accuracy and usability\nwith the lowest workload, while not tracking the tool yielded the worst\nperformance and user ratings. However, making the target highly transparent,\nwhile allowing the real tool to remain visible, slightly impaired depth cues\nand did not improve usability. Our findings underscore that correct occlusion\ncues, rendering virtual content opaque and occluding it with real tools in real\ntime, are critical for depth perception and precision in OST-AR. Designers of\narm-distance AR systems should prioritize robust tool tracking and occlusion\nhandling; if unavailable, cautiously use transparency to balance depth\nperception and tool visibility."}
{"id": "2508.18499", "pdf": "https://arxiv.org/pdf/2508.18499.pdf", "abs": "https://arxiv.org/abs/2508.18499", "title": "Skeptik: A Hybrid Framework for Combating Potential Misinformation in Journalism", "authors": ["Arlen Fan", "Fan Lei", "Steven R. Corman", "Ross Maciejewski"], "categories": ["cs.HC"], "comment": "Arlen Fan and Fan Lei contributed equally to this research. Accepted\n  by ACM Transactions on Interactive Intelligent Systems (TiiS)", "summary": "The proliferation of misinformation in journalism, often stemming from flawed\nreasoning and logical fallacies, poses significant challenges to public\nunderstanding and trust in news media. Traditional fact-checking methods, while\nvaluable, are insufficient for detecting the subtle logical inconsistencies\nthat can mislead readers within seemingly factual content. To address this gap,\nwe introduce Skeptik, a hybrid framework that integrates Large Language Models\n(LLMs) with heuristic approaches to analyze and annotate potential logical\nfallacies and reasoning errors in online news articles. Operating as a web\nbrowser extension, Skeptik automatically highlights sentences that may contain\nlogical fallacies, provides detailed explanations, and offers multi-layered\ninterventions to help readers critically assess the information presented. The\nsystem is designed to be extensible, accommodating a wide range of fallacy\ntypes and adapting to evolving misinformation tactics. Through comprehensive\ncase studies, quantitative analyses, usability experiments, and expert\nevaluations, we demonstrate the effectiveness of Skeptik in enhancing readers'\ncritical examination of news content and promoting media literacy. Our\ncontributions include the development of an expandable classification system\nfor logical fallacies, the innovative integration of LLMs for real-time\nanalysis and annotation, and the creation of an interactive user interface that\nfosters user engagement and close reading. By emphasizing the logical integrity\nof textual content rather than relying solely on factual accuracy, Skeptik\noffers a comprehensive solution to combat potential misinformation in\njournalism. Ultimately, our framework aims to improve critical reading and\nprotect the public from deceptive information online and enhance the overall\ncredibility of news media."}
{"id": "2508.18290", "pdf": "https://arxiv.org/pdf/2508.18290.pdf", "abs": "https://arxiv.org/abs/2508.18290", "title": "Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI", "authors": ["Hans-Joachim Rudolph"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "This essay develops a theoretical framework for a semantic Artificial General\nIntelligence (AGI) based on the notion of semantic attractors in complex-valued\nmeaning spaces. Departing from current transformer-based language models, which\noperate on statistical next-token prediction, we explore a model in which\nmeaning is not inferred probabilistically but formed through recursive\ntensorial transformation. Using cyclic operations involving the imaginary unit\n\\emph{i}, we describe a rotational semantic structure capable of modeling\nirony, homonymy, and ambiguity. At the center of this model, however, is a\nsemantic attractor -- a teleological operator that, unlike statistical\ncomputation, acts as an intentional agent (Microvitum), guiding meaning toward\nstability, clarity, and expressive depth. Conceived in terms of gradient flows,\ntensor deformations, and iterative matrix dynamics, the attractor offers a\nmodel of semantic transformation that is not only mathematically suggestive,\nbut also philosophically significant. We argue that true meaning emerges not\nfrom simulation, but from recursive convergence toward semantic coherence, and\nthat this requires a fundamentally new kind of cognitive architecture -- one\ndesigned to shape language, not just predict it."}
{"id": "2508.18545", "pdf": "https://arxiv.org/pdf/2508.18545.pdf", "abs": "https://arxiv.org/abs/2508.18545", "title": "Beyond prior knowledge: The predictive role of knowledge-building in Tutor Learning", "authors": ["Tasmia Shahriar", "Mia Ameen", "Aditi Mallavarapu", "Shiyan Jiang", "Noboru Matsuda"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "When adopting the role of a teacher in learning-by-teaching environments,\nstudents often struggle to engage in knowledge-building activities, such as\nproviding explanations and addressing misconceptions. Instead, they frequently\ndefault to knowledge-telling behaviors, where they simply dictate what they\nalready know or what to do without deeper reflection, thereby limiting\nlearning. Teachable agents, particularly those capable of posing persistent\nfollow-up questions, have been shown to encourage students (tutors) to shift\nfrom knowledge-telling to knowledge-building and enhance tutor learning. Tutor\nlearning encompasses two interrelated types of knowledge: conceptual and\nprocedural knowledge. Research has established a bidirectional relationship\nbetween these knowledge types, where improvements in one reinforce the other.\nThis study investigates the role of knowledge-building in mediating the\nbidirectional relationship between procedural and conceptual learning. Our\nfindings revealed a stable bidirectional relationship between procedural and\nconceptual knowledge, with higher post-test scores observed among students who\nengaged in knowledge-building, regardless of their procedural and conceptual\npre-test performance. This suggests that knowledge-building serves as a crucial\nmechanism bridging the gap between students with low prior knowledge and higher\nconceptual and procedural learning gain."}
{"id": "2508.18321", "pdf": "https://arxiv.org/pdf/2508.18321.pdf", "abs": "https://arxiv.org/abs/2508.18321", "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions", "authors": ["Maojia Song", "Tej Deep Pala", "Weisheng Jin", "Amir Zadeh", "Chuan Li", "Dorien Herremans", "Soujanya Poria"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS."}
{"id": "2508.18580", "pdf": "https://arxiv.org/pdf/2508.18580.pdf", "abs": "https://arxiv.org/abs/2508.18580", "title": "Gamification of Immersive Cervical Rehabilitation Exercises in VR: An Exploratory Study on Chin Tuck and Range of Motion Exercises", "authors": ["Haitham Abdelsalam", "Chanelle Montpetit", "Arash Harirpoush", "Maryse Fortin", "Yiming Xiao"], "categories": ["cs.HC"], "comment": "8 pages, 7 figures, Accepted in the IEEE ISMAR 2025 XRehab Workshop", "summary": "Chronic neck pain is a prevalent condition that affects millions of\nindividuals worldwide, causing significant individual suffering and\nsocioeconomic burdens. Although exercise rehabilitation is a staple in\nrelieving pain and improving muscle function for the condition, traditional\none-on-one rehabilitation sessions are costly and suffer from poor adherence\nand accessibility for the patients. Thanks to the increasing accessibility and\nrecent advancements in sensing and display technology, virtual reality (VR)\noffers the potential to tackle the challenges in traditional exercise\nrehabilitation, particularly through gamification. However, still in its\ninfancy, VR-based neck exercise rehabilitation lacks exploration in effective\ngamification strategies and existing prototypes. To address the knowledge gap,\nwe conduct an exploratory study on the gamification strategies for VR-based\ncervical rehabilitation exercises by using chin tuck and neck range of motion\nexercises as examples. Specifically, with different game themes, we investigate\na survival and level progression strategy for muscle strengthening (chin tuck)\nexercise for the first time, and the suitability of ambient reward for a neck\nrange of motion exercise. Through a preliminary user study, we assess the\nproposed novel VR neck rehabilitation games and they demonstrate excellent\nusability, engagement, and perceived health value."}
{"id": "2508.18328", "pdf": "https://arxiv.org/pdf/2508.18328.pdf", "abs": "https://arxiv.org/abs/2508.18328", "title": "Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective", "authors": ["Masudul Hasan Masud Bhuiyan", "Matteo Varvello", "Yasir Zaki", "Cristian-Alexandru Staicu"], "categories": ["cs.CL", "cs.CY", "cs.NI"], "comment": "6 pages, 6 figures", "summary": "English is the predominant language on the web, powering nearly half of the\nworld's top ten million websites. Support for multilingual content is\nnevertheless growing, with many websites increasingly combining English with\nregional or native languages in both visible content and hidden metadata. This\nmultilingualism introduces significant barriers for users with visual\nimpairments, as assistive technologies like screen readers frequently lack\nrobust support for non-Latin scripts and misrender or mispronounce non-English\ntext, compounding accessibility challenges across diverse linguistic contexts.\nYet, large-scale studies of this issue have been limited by the lack of\ncomprehensive datasets on multilingual web content. To address this gap, we\nintroduce LangCrUX, the first large-scale dataset of 120,000 popular websites\nacross 12 languages that primarily use non-Latin scripts. Leveraging this\ndataset, we conduct a systematic analysis of multilingual web accessibility and\nuncover widespread neglect of accessibility hints. We find that these hints\noften fail to reflect the language diversity of visible content, reducing the\neffectiveness of screen readers and limiting web accessibility. We finally\npropose Kizuki, a language-aware automated accessibility testing extension to\naccount for the limited utility of language-inconsistent accessibility hints."}
{"id": "2508.18591", "pdf": "https://arxiv.org/pdf/2508.18591.pdf", "abs": "https://arxiv.org/abs/2508.18591", "title": "Portable Silent Room: Exploring VR Design for Anxiety and Emotion Regulation for Neurodivergent Women and Non-Binary Individuals", "authors": ["Kinga Skiers", "Yun Suen Pai", "Marina Nakagawa", "Kouta Minamizawa", "Giulia Barbareschi"], "categories": ["cs.HC"], "comment": null, "summary": "Neurodivergent individuals, particularly those with Autism and Attention\nDeficit Hyperactivity Disorder (ADHD), frequently experience anxiety, panic\nattacks, meltdowns, and emotional dysregulation due to societal pressures and\ninadequate accommodations. These challenges are especially pronounced for\nneurodivergent women and non-binary individuals navigating intersecting\nbarriers of neurological differences and gender expectations. This research\ninvestigates virtual reality (VR) as a portable safe space for emotional\nregulation, addressing challenges of sensory overload and motion sickness while\nenhancing relaxation capabilities. Our mixed-methods approach included an\nonline survey (N=223) and an ideation workshop (N=32), which provided key\ndesign elements for creating effective calming VR environments. Based on these\nfindings, we developed and iteratively tested VR prototypes with neurodivergent\nwomen and non-binary participants (N=12), leading to a final version offering\nenhanced adaptability to individual sensory needs. This final prototype\nunderwent a comprehensive evaluation with 25 neurodivergent participants to\nassess its effectiveness as a regulatory tool. This research contributes to the\ndevelopment of inclusive, adaptive VR environments that function as\npersonalized \"portable silent rooms\" offering neurodivergent individuals\non-demand access to sensory regulation regardless of physical location."}
{"id": "2508.18381", "pdf": "https://arxiv.org/pdf/2508.18381.pdf", "abs": "https://arxiv.org/abs/2508.18381", "title": "Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models", "authors": ["Yuchun Fan", "Yilin Wang", "Yongyu Mu", "Lei Huang", "Bei Li", "Xiaocheng Feng", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 findings", "summary": "Large vision-language models (LVLMs) have demonstrated exceptional\ncapabilities in understanding visual information with human languages but also\nexhibit an imbalance in multilingual capabilities. In this work, we delve into\nthe multilingual working pattern of LVLMs and identify a salient correlation\nbetween the multilingual understanding ability of LVLMs and language-specific\nneuron activations in shallow layers. Building on this insight, we introduce\nPLAST, a training recipe that achieves efficient multilingual enhancement for\nLVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies\nlayers involved in multilingual understanding by monitoring language-specific\nneuron activations. These layers are then precisely fine-tuned with\nquestion-translation pairs to achieve multilingual alignment. Our empirical\nresults on MM-Bench and MMMB demonstrate that PLAST effectively improves the\nmultilingual capabilities of LVLMs and achieves significant efficiency with\nonly 14% of the parameters tuned. Further analysis reveals that PLAST can be\ngeneralized to low-resource and complex visual reasoning tasks, facilitating\nthe language-specific visual information engagement in shallow layers."}
{"id": "2508.18640", "pdf": "https://arxiv.org/pdf/2508.18640.pdf", "abs": "https://arxiv.org/abs/2508.18640", "title": "Enhancing XAI Interpretation through a Reverse Mapping from Insights to Visualizations", "authors": ["Aniket Nuthalapati", "Nicholas Hinds", "Brian Y. Lim", "Qianwen Wang"], "categories": ["cs.HC"], "comment": "5 pages, 5 figures, accepted by IEEE VIS 2025", "summary": "As AI systems become increasingly integrated into high-stakes domains,\nenabling users to accurately interpret model behavior is critical. While AI\nexplanations can be provided, users often struggle to reason effectively with\nthese explanations, limiting their ability to validate or learn from AI\ndecisions. To address this gap, we introduce Reverse Mapping, a novel approach\nthat enhances visual explanations by incorporating user-derived insights back\ninto the explanation workflow. Our system extracts structured insights from\nfree-form user interpretations using a large language model and maps them back\nonto visual explanations through interactive annotations and coordinated\nmulti-view visualizations. Inspired by the verification loop in the\nvisualization knowledge generation model, this design aims to foster more\ndeliberate, reflective interaction with AI explanations. We demonstrate our\napproach in a prototype system with two use cases and qualitative user\nfeedback."}
{"id": "2508.18384", "pdf": "https://arxiv.org/pdf/2508.18384.pdf", "abs": "https://arxiv.org/abs/2508.18384", "title": "Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails", "authors": ["Kellen Tan Cheng", "Anna Lisa Gentile", "Chad DeLuca", "Guang-Jie Ren"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The pervasiveness of large language models (LLMs) in enterprise settings has\nalso brought forth a significant amount of risks associated with their usage.\nGuardrails technologies aim to mitigate this risk by filtering LLMs'\ninput/output text through various detectors. However, developing and\nmaintaining robust detectors faces many challenges, one of which is the\ndifficulty in acquiring production-quality labeled data on real LLM outputs\nprior to deployment. In this work, we propose backprompting, a simple yet\nintuitive solution to generate production-like labeled data for health advice\nguardrails development. Furthermore, we pair our backprompting method with a\nsparse human-in-the-loop clustering technique to label the generated data. Our\naim is to construct a parallel corpus roughly representative of the original\ndataset yet resembling real LLM output. We then infuse existing datasets with\nour synthetic examples to produce robust training data for our detector. We\ntest our technique in one of the most difficult and nuanced guardrails: the\nidentification of health advice in LLM output, and demonstrate improvement\nversus other solutions. Our detector is able to outperform GPT-4o by up to\n3.73%, despite having 400x less parameters."}
{"id": "2508.18670", "pdf": "https://arxiv.org/pdf/2508.18670.pdf", "abs": "https://arxiv.org/abs/2508.18670", "title": "RÉCITKIT: A Spatial Toolkit for Designing and Evaluating Human-Centered Immersive Data Narratives", "authors": ["Vidya Setlur", "Samuel Ridet"], "categories": ["cs.HC"], "comment": "5 pages, 3 figures", "summary": "Spatial computing presents new opportunities for immersive data storytelling,\nyet there is limited guidance on how to build such experiences or adapt\ntraditional narrative visualizations to this medium. We introduce a toolkit,\nR\\'ECITKIT for supporting spatial data narratives in head-mounted display (HMD)\nenvironments. The toolkit allows developers to create interactive dashboards,\ntag data attributes as spatial assets to 3D models and immersive scenes,\ngenerate text and audio narratives, enabling dynamic filtering, and\nhierarchical drill-down data discoverability. To demonstrate the utility of the\ntoolkit, we developed Charles Minard's historical flow map of Napoleon's 1812\ncampaign in Russia as an immersive experience on Apple Vision Pro. We conducted\na preliminary evaluation with 21 participants that comprised two groups:\ndevelopers, who evaluated the toolkit by authoring spatial stories and\nconsumers, who provided feedback on the Minard app's narrative clarity,\ninteraction design, and engagement. Feedback highlighted how spatial\ninteractions and guided narration enhanced insight formation, with participants\nemphasizing the benefits of physical manipulation (e.g., gaze, pinch,\nnavigation) for understanding temporal and geographic data. Participants also\nidentified opportunities for future enhancement, including improved interaction\naffordance visibility, customizable storytelling logic, and integration of\ncontextual assets to support user orientation. These findings contribute to the\nbroader discourse on toolkit-driven approaches to immersive data storytelling\nacross domains such as education, decision support, and exploratory analytics."}
{"id": "2508.18387", "pdf": "https://arxiv.org/pdf/2508.18387.pdf", "abs": "https://arxiv.org/abs/2508.18387", "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little", "authors": ["Ivan Kobyzev", "Abbas Ghaddar", "Dingtao Hu", "Boxing Chen"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Softmax self-attention often assigns disproportionate weight to semantically\nuninformative tokens such as special tokens and punctuation, a phenomenon known\nas attention noise. While recent methods like Cog Attention and the\nDifferential Transformer have addressed this by introducing negative attention\nscores, they risk discarding useful information. In this paper, we propose the\nIntegral Transformer, a novel self-attention mechanism that denoises attention\nby integrating signals sampled from the logit distribution. Our approach\nmitigates noise while preserving the contributions of special tokens critical\nfor model performance. Extensive experiments demonstrate that our model\noutperforms vanilla, Cog, and Differential attention variants on\nwell-established knowledge and reasoning language benchmarks. Moreover, our\nanalysis reveals that employing vanilla self-attention in the lower Transformer\nlayers enhances performance and that the Integral Transformer effectively\nbalances attention distributions and reduces rank collapse in upper layers."}
{"id": "2508.18782", "pdf": "https://arxiv.org/pdf/2508.18782.pdf", "abs": "https://arxiv.org/abs/2508.18782", "title": "Long-Term Variability in Physiological-Arousal Relationships for Robust Emotion Estimation", "authors": ["Hiroto Sakimura", "Takayuki Nagaya", "Tomoki Nishi", "Tetsuo Kurahashi", "Katsunori Kohda", "Nobuhiko Muramoto"], "categories": ["cs.HC", "cs.AI"], "comment": "9 pages, 5 figures, accepted at 13th International Conference on\n  Affective Computing and Intelligent Interaction (ACII 2025)", "summary": "Estimating emotional states from physiological signals is a central topic in\naffective computing and psychophysiology. While many emotion estimation systems\nimplicitly assume a stable relationship between physiological features and\nsubjective affect, this assumption has rarely been tested over long timeframes.\nThis study investigates whether such relationships remain consistent across\nseveral months within individuals. We developed a custom measurement system and\nconstructed a longitudinal dataset by collecting physiological signals --\nincluding blood volume pulse, electrodermal activity (EDA), skin temperature,\nand acceleration--along with self-reported emotional states from 24\nparticipants over two three-month periods. Data were collected in naturalistic\nworking environments, allowing analysis of the relationship between\nphysiological features and subjective arousal in everyday contexts. We examined\nhow physiological-arousal relationships evolve over time by using Explainable\nBoosting Machines (EBMs) to ensure model interpretability. A model trained on\n1st-period data showed a 5\\% decrease in accuracy when tested on 2nd-period\ndata, indicating long-term variability in physiological-arousal associations.\nEBM-based comparisons further revealed that while heart rate remained a\nrelatively stable predictor, minimum EDA exhibited substantial individual-level\nfluctuations between periods. While the number of participants is limited,\nthese findings highlight the need to account for temporal variability in\nphysiological-arousal relationships and suggest that emotion estimation models\nshould be periodically updated -- e.g., every five months -- based on observed\nshift trends to maintain robust performance over time."}
{"id": "2508.18395", "pdf": "https://arxiv.org/pdf/2508.18395.pdf", "abs": "https://arxiv.org/abs/2508.18395", "title": "Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning", "authors": ["Jeong-seok Oh", "Jay-yoon Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Probabilistic decoding in Large Language Models (LLMs) often yields\ninconsistent outputs, particularly on complex or long-form questions.\nSelf-Consistency (SC) mitigates this for short-form QA by majority voting over\nexact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram\nConsistency Score (WUCS) extend to long-form responses but lose accuracy on\nshort-form benchmarks.\n  We introduce Latent Self-Consistency (LSC), which selects the most\nsemantically consistent response using learnable token embeddings. A\nlightweight forward generation of summary tokens increases inference time by\nless than 1% and requires no changes to the model architecture.\n  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,\nTruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form\nones on average, while maintaining negligible computational overhead. These\nresults position LSC as a practical consistency-selection method that works\nreliably across answer formats. Additionally, LSC provides well-calibrated\nconfidence estimates, maintaining low Expected Calibration Error across both\nanswer formats."}
{"id": "2508.18784", "pdf": "https://arxiv.org/pdf/2508.18784.pdf", "abs": "https://arxiv.org/abs/2508.18784", "title": "Insights into User Interface Innovations from a Design Thinking Workshop at deRSE25", "authors": ["Maximilian Frank", "Simon Lund"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Large Language Models have become widely adopted tools due to their versatile\ncapabilities, yet their user interfaces remain limited, often following rigid,\nlinear interaction paradigms. In this paper, we present insights from a design\nthinking workshop held at the deRSE25 conference aiming at collaboratively\ndeveloping innovative user interface concepts for LLMs. During the workshop,\nparticipants identified common use cases, evaluated the strengths and\nshortcomings of current LLM interfaces, and created visualizations of new\ninteraction concepts emphasizing flexible context management, dynamic\nconversation branching, and enhanced mechanisms for user control. We describe\nhow these participant-generated ideas advanced our own whiteboard-based UI\napproach. The ongoing development of this interface is guided by the\nhuman-centered design process - an iterative, user-focused methodology that\nemphasizes continuous refinement through user feedback. Broader implications\nfor future LLM interface development are discussed, advocating for increased\nattention to UI innovation grounded in user-centered design principles."}
{"id": "2508.18407", "pdf": "https://arxiv.org/pdf/2508.18407.pdf", "abs": "https://arxiv.org/abs/2508.18407", "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering", "authors": ["Michal Štefánik", "Timothee Mickus", "Marek Kadlčík", "Michal Spiegel", "Josef Kuchař"], "categories": ["cs.CL", "cs.AI", "68T01, 68T07, 68T50", "I.2"], "comment": "To appear in Findings of EMNLP 2025", "summary": "A majority of recent work in AI assesses models' generalization capabilities\nthrough the lens of performance on out-of-distribution (OOD) datasets. Despite\ntheir practicality, such evaluations build upon a strong assumption: that OOD\nevaluations can capture and reflect upon possible failures in a real-world\ndeployment.\n  In this work, we challenge this assumption and confront the results obtained\nfrom OOD evaluations with a set of specific failure modes documented in\nexisting question-answering (QA) models, referred to as a reliance on spurious\nfeatures or prediction shortcuts.\n  We find that different datasets used for OOD evaluations in QA provide an\nestimate of models' robustness to shortcuts that have a vastly different\nquality, some largely under-performing even a simple, in-distribution\nevaluation. We partially attribute this to the observation that spurious\nshortcuts are shared across ID+OOD datasets, but also find cases where a\ndataset's quality for training and evaluation is largely disconnected. Our work\nunderlines limitations of commonly-used OOD-based evaluations of\ngeneralization, and provides methodology and recommendations for evaluating\ngeneralization within and beyond QA more robustly."}
{"id": "2508.18875", "pdf": "https://arxiv.org/pdf/2508.18875.pdf", "abs": "https://arxiv.org/abs/2508.18875", "title": "PRIMMDebug: A Debugging Teaching Aid For Secondary Students", "authors": ["Laurie Gale", "Sue Sentance"], "categories": ["cs.HC"], "comment": "12 pages, 8 figures", "summary": "Debugging is often a challenging and infuriating experience for secondary\nschool students learning their first text-based programming language. Many\nstudents resort to ineffective debugging strategies, making success with\nsolving errors unlikely and emotional distress common. Developing tools that\nencourage students to adopt a more systematic and reflective approach to\ndebugging is therefore an important, but lacking, area of research. This paper\npresents PRIMMDebug, a debugging teaching aid for secondary school students\nlearning text-based programming. The aid consists of an online tool that takes\nstudents through the steps of a systematic debugging process based on PRIMM, a\nframework for teaching programming. The tool promotes a reflective approach to\ndebugging by heavily encouraging students to articulate their thoughts\nthroughout the PRIMMDebug process while simultaneously limiting their ability\nto run and edit code. To evaluate the tool, a set of students from four\nsecondary schools were taught with PRIMMDebug over several lessons. Survey\nresults and log data analysis show that students were generally reluctant to\nengage with the systematicity and reflection that the tool encourages. Given\nthat related work on systematic debugging has reported similar challenges, we\nend by considering how these approaches could be refined to help more students\nbenefit from them."}
{"id": "2508.18444", "pdf": "https://arxiv.org/pdf/2508.18444.pdf", "abs": "https://arxiv.org/abs/2508.18444", "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?", "authors": ["Nafis Tanveer Islam", "Zhiming Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at FQAS Conference 2024. DOI will be provided in 3 weeks\n  after the conference has published the paper", "summary": "With the improving semantic understanding capability of Large Language Models\n(LLMs), they exhibit a greater awareness and alignment with human values, but\nthis comes at the cost of transparency. Although promising results are achieved\nvia experimental analysis, an in-depth understanding of the LLM's internal\nworkings is unavoidable to comprehend the reasoning behind the re-ranking,\nwhich provides end users with an explanation that enables them to make an\ninformed decision. Moreover, in newly developed systems with limited user\nengagement and insufficient ranking data, accurately re-ranking content remains\na significant challenge. While various training methods affect the training of\nLLMs and generate inference, our analysis has found that some training methods\nexhibit better explainability than others, implying that an accurate semantic\nunderstanding has not been learned through all training methods; instead,\nabstract knowledge has been gained to optimize evaluation, which raises\nquestions about the true reliability of LLMs. Therefore, in this work, we\nanalyze how different training methods affect the semantic understanding of the\nre-ranking task in LLMs and investigate whether these models can generate more\ninformed textual reasoning to overcome the challenges of transparency or LLMs\nand limited training data. To analyze the LLMs for re-ranking tasks, we utilize\na relatively small ranking dataset from the environment and the Earth science\ndomain to re-rank retrieved content. Furthermore, we also analyze the\nexplainable information to see if the re-ranking can be reasoned using\nexplainability."}
{"id": "2508.18918", "pdf": "https://arxiv.org/pdf/2508.18918.pdf", "abs": "https://arxiv.org/abs/2508.18918", "title": "DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM with Audio Modality", "authors": ["Youngwon Choi", "Donghyuk Jung", "Hwayeon Kim"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "2 pages, 2 figures. Accepted for presentation as a UIST 2025 Poster", "summary": "We present DESAMO, an on-device smart home system for elder-friendly use\npowered by Audio LLM, that supports natural and private interactions. While\nconventional voice assistants rely on ASR-based pipelines or ASR-LLM cascades,\noften struggling with the unclear speech common among elderly users and unable\nto handle non-speech audio, DESAMO leverages an Audio LLM to process raw audio\ninput directly, enabling a robust understanding of user intent and critical\nevents, such as falls or calls for help."}
{"id": "2508.18466", "pdf": "https://arxiv.org/pdf/2508.18466.pdf", "abs": "https://arxiv.org/abs/2508.18466", "title": "Integrating gender inclusivity into large language models via instruction tuning", "authors": ["Alina Wróblewska", "Bartosz Żuk"], "categories": ["cs.CL"], "comment": null, "summary": "Imagine a language with masculine, feminine, and neuter grammatical genders,\nyet, due to historical and political conventions, masculine forms are\npredominantly used to refer to men, women and mixed-gender groups. This is the\nreality of contemporary Polish. A social consequence of this unfair linguistic\nsystem is that large language models (LLMs) trained on Polish texts inherit and\nreinforce this masculine bias, generating gender-imbalanced outputs. This study\naddresses this issue by tuning LLMs using the IPIS dataset, a collection of\nhuman-crafted gender-inclusive proofreading in Polish and Polish-to-English\ntranslation instructions. Grounded in a theoretical linguistic framework, we\ndesign a system prompt with explicit gender-inclusive guidelines for Polish. In\nour experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and\nMistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to\nintegrate gender inclusivity as an inherent feature of these models, offering a\nsystematic solution to mitigate gender bias in Polish language generation."}
{"id": "2508.18919", "pdf": "https://arxiv.org/pdf/2508.18919.pdf", "abs": "https://arxiv.org/abs/2508.18919", "title": "Impact Assessment Card: Communicating Risks and Benefits of AI Uses", "authors": ["Edyta Bogucka", "Marios Constantinides", "Sanja Šćepanović", "Daniele Quercia"], "categories": ["cs.HC", "K.4.1, K.4.2, H.5.3, D.2.9", "K.4.1; K.4.2; H.5.3; D.2.9"], "comment": "42 pages, 14 figures", "summary": "Communicating the risks and benefits of AI is important for regulation and\npublic understanding. Yet current methods such as technical reports often\nexclude people without technical expertise. Drawing on HCI research, we\ndeveloped an Impact Assessment Card to present this information more clearly.\nWe held three focus groups with a total of 12 participants who helped identify\ndesign requirements and create early versions of the card. We then tested a\nrefined version in an online study with 235 participants, including AI\ndevelopers, compliance experts, and members of the public selected to reflect\nthe U.S. population by age, sex, and race. Participants used either the card or\na full impact assessment report to write an email supporting or opposing a\nproposed AI system. The card led to faster task completion and higher-quality\nemails across all groups. We discuss how design choices can improve\naccessibility and support AI governance. Examples of cards are available at:\nhttps://social-dynamics.net/ai-risks/impact-card/."}
{"id": "2508.18473", "pdf": "https://arxiv.org/pdf/2508.18473.pdf", "abs": "https://arxiv.org/abs/2508.18473", "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing", "authors": ["Jiawei Li", "Akshayaa Magesh", "Venugopal V. Veeravalli"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages", "summary": "While Large Language Models (LLMs) have emerged as powerful foundational\nmodels to solve a variety of tasks, they have also been shown to be prone to\nhallucinations, i.e., generating responses that sound confident but are\nactually incorrect or even nonsensical. In this work, we formulate the problem\nof detecting hallucinations as a hypothesis testing problem and draw parallels\nto the problem of out-of-distribution detection in machine learning models. We\npropose a multiple-testing-inspired method to solve the hallucination detection\nproblem, and provide extensive experimental results to validate the robustness\nof our approach against state-of-the-art methods."}
{"id": "2508.19121", "pdf": "https://arxiv.org/pdf/2508.19121.pdf", "abs": "https://arxiv.org/abs/2508.19121", "title": "Reading minds on the road: decoding perceived risk in automated vehicles through 140K+ ratings", "authors": ["Xiaolin He", "Zirui Li", "Xinwei Wang", "Riender Happee", "Meng Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Perceived risk in automated vehicles (AVs) can create the very danger that\nautomation is meant to prevent: a frightened rider may hesitate when seconds\nmatter, misjudge hazards, or disengage. However, measuring how perceived risk\nevolves in real time during driving remains challenging, leaving a gap in\ndecoding such hidden psychological states. Here, we present a novel method to\ntime-continuously measure and decode perceived risk. We conducted a controlled\nexperiment where 2,164 participants viewed high-fidelity videos of common\nhighway driving scenes and provided 141,628 discrete safety ratings. Through\ncontinuous-signal reconstruction of the discrete ratings, we obtained 236 hours\nof time-continuous perceived risk data - the largest perceived risk dataset to\ndate. Leveraging this dataset, we trained deep neural networks that predict\nmoment-by-moment perceived risk from vehicle kinematics with a mean relative\nerror below $3\\%$. Explainable AI analysis uncovers which factors determine\nperceived risk in real time. Our findings demonstrate a new paradigm for\nquantifying dynamic passenger experience and psychological constructs in real\ntime. These findings can guide the design of AVs and other machines that\noperate in close proximity to people, adjusting behaviour before trust erodes,\nand help realise automation's benefits in transport, healthcare, and service\nrobotics."}
{"id": "2508.18549", "pdf": "https://arxiv.org/pdf/2508.18549.pdf", "abs": "https://arxiv.org/abs/2508.18549", "title": "COMET-poly: Machine Translation Metric Grounded in Other Candidates", "authors": ["Maike Züfle", "Vilém Zouhar", "Tu Anh Dinh", "Felipe Maia Polo", "Jan Niehues", "Mrinmaya Sachan"], "categories": ["cs.CL", "I.2.7"], "comment": "Maike Z\\\"ufle, Vil\\'em Zouhar, and Tu Anh Dinh contributed equally", "summary": "Automated metrics for machine translation attempt to replicate human\njudgment. Unlike humans, who often assess a translation in the context of\nmultiple alternatives, these metrics typically consider only the source\nsentence and a single translation. This discrepancy in the evaluation setup may\nnegatively impact the performance of automated metrics. We propose two\nautomated metrics that incorporate additional information beyond the single\ntranslation. COMET-polycand uses alternative translations of the same source\nsentence to compare and contrast with the translation at hand, thereby\nproviding a more informed assessment of its quality. COMET-polyic, inspired by\nretrieval-based in-context learning, takes in translations of similar source\ntexts along with their human-labeled quality scores to guide the evaluation. We\nfind that including a single additional translation in COMET-polycand improves\nthe segment-level metric performance (0.079 to 0.118 Kendall's tau-b\ncorrelation), with further gains when more translations are added.\nIncorporating retrieved examples in COMET-polyic yields similar improvements\n(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly."}
{"id": "2508.19230", "pdf": "https://arxiv.org/pdf/2508.19230.pdf", "abs": "https://arxiv.org/abs/2508.19230", "title": "Beyond Competitive Gaming: How Casual Players Evaluate and Respond to Teammate Performance", "authors": ["Kaushall Senthil Nathan", "Jieun Lee", "Derrick M. Wang", "Geneva M. Smith", "Eugene Kukshinov", "Daniel Harley", "Lennart E. Nacke"], "categories": ["cs.HC"], "comment": "7 pages, 1 figure, CHI Play 2025 Conference", "summary": "Teammate performance evaluation fundamentally shapes intervention design in\nvideo games. However, our current understanding stems primarily from\ncompetitive E-Sports contexts where individual performance directly impacts\noutcomes. This research addresses whether performance evaluation mechanisms and\nbehavioural responses identified in competitive games generalize to casual\ncooperative games. We investigated how casual players evaluate teammate\ncompetence and respond behaviourally in a controlled between-subjects\nexperiment (N=23). We manipulated confederate performance in Overcooked 2,\ncombining observations, NASA TLX self-reports, and interviews. We present two\nkey findings. (1) Observations revealed frustration behaviours completely\nabsent in self-report data. Thus, these instruments assess fundamentally\ndistinct constructs. (2) Participants consistently evaluated teammate\nperformance through relative comparison rather than absolute metrics. This\ncontradicts task-performance operationalizations dominant in competitive gaming\nresearch. Hence, performance evaluation frameworks from competitive contexts\ncannot be directly applied to casual cooperative games. We provide empirical\nevidence that performance evaluation in casual games requires a comparative\noperationalization."}
{"id": "2508.18569", "pdf": "https://arxiv.org/pdf/2508.18569.pdf", "abs": "https://arxiv.org/abs/2508.18569", "title": "The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation", "authors": ["Girish A. Koushik", "Fatemeh Nazarieh", "Katherine Birch", "Shenbin Qian", "Diptesh Kanojia"], "categories": ["cs.CL", "cs.CV"], "comment": "Under Review", "summary": "Visual metaphor generation is a challenging task that aims to generate an\nimage given an input text metaphor. Inherently, it needs language understanding\nto bind a source concept with a target concept, in a way that preserves meaning\nwhile ensuring visual coherence. We propose a self-evaluating visual metaphor\ngeneration framework that focuses on metaphor alignment. Our self-evaluation\napproach combines existing metrics with our newly proposed metaphor\ndecomposition score and a meaning alignment (MA) metric. Within this setup, we\nexplore two novel approaches: a training-free pipeline that explicitly\ndecomposes prompts into source-target-meaning (S-T-M) mapping for image\nsynthesis, and a complementary training-based pipeline that improves alignment\nusing our proposed self-evaluation reward schema, without any large-scale\nretraining. On the held-out test set, the training-free approach surpasses\nstrong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,\nwith the training-based approach close behind. We evaluate our framework output\nusing a user-facing study, and observed that participants preferred GPT-4o\noverall, while our training-free pipeline led open-source methods and edged\nImagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or\nmore abstract metaphors, with closed models excelling on short, concrete cases;\nwe also observe sensitivity to sampler settings. Overall, structured prompting\nand lightweight RL perform metaphor alignment well under modest compute, and\nremaining gaps to human preference appear driven by aesthetics and sampling."}
{"id": "2508.18288", "pdf": "https://arxiv.org/pdf/2508.18288.pdf", "abs": "https://arxiv.org/abs/2508.18288", "title": "Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology", "authors": ["Jay L. Cunningham", "Adinawa Adjagbodjou", "Jeffrey Basoah", "Jainaba Jawara", "Kowe Kadoma", "Aaleyah Lewis"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "10 pages, 9 Pages (References and Appendices). The archival version\n  has been accepted to AAAI (AIES 2025) without the extended Appendices. This\n  extended version includes Appendices", "summary": "This scoping literature review examines how fairness, bias, and equity are\nconceptualized and operationalized in Automatic Speech Recognition (ASR) and\nadjacent speech and language technologies (SLT) for African American English\n(AAE) speakers and other linguistically diverse communities. Drawing from 44\npeer-reviewed publications across Human-Computer Interaction (HCI), Machine\nLearning/Natural Language Processing (ML/NLP), and Sociolinguistics, we\nidentify four major areas of inquiry: (1) how researchers understand\nASR-related harms; (2) inclusive data practices spanning collection, curation,\nannotation, and model training; (3) methodological and theoretical approaches\nto linguistic inclusion; and (4) emerging practices and design recommendations\nfor more equitable systems. While technical fairness interventions are growing,\nour review highlights a critical gap in governance-centered approaches that\nforeground community agency, linguistic justice, and participatory\naccountability. We propose a governance-centered ASR lifecycle as an emergent\ninterdisciplinary framework for responsible ASR development and offer\nimplications for researchers, practitioners, and policymakers seeking to\naddress language marginalization in speech AI systems."}
{"id": "2508.18598", "pdf": "https://arxiv.org/pdf/2508.18598.pdf", "abs": "https://arxiv.org/abs/2508.18598", "title": "What do language models model? Transformers, automata, and the format of thought", "authors": ["Colin Klein"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "What do large language models actually model? Do they tell us something about\nhuman capacities, or are they models of the corpus we've trained them on? I\ngive a non-deflationary defence of the latter position. Cognitive science tells\nus that linguistic capabilities in humans rely supralinear formats for\ncomputation. The transformer architecture, by contrast, supports at best a\nlinear formats for processing. This argument will rely primarily on certain\ninvariants of the computational architecture of transformers. I then suggest a\npositive story about what transformers are doing, focusing on Liu et al.\n(2022)'s intriguing speculations about shortcut automata. I conclude with why I\ndon't think this is a terribly deflationary story. Language is not (just) a\nmeans for expressing inner state but also a kind of 'discourse machine' that\nlets us make new language given appropriate context. We have learned to use\nthis technology in one way; LLMs have also learned to use it too, but via very\ndifferent means."}
{"id": "2508.18301", "pdf": "https://arxiv.org/pdf/2508.18301.pdf", "abs": "https://arxiv.org/abs/2508.18301", "title": "A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach", "authors": ["Md Sabbir Ahmed", "Nova Ahmed"], "categories": ["cs.LG", "cs.CY", "cs.HC"], "comment": null, "summary": "Background: Existing robust, pervasive device-based systems developed in\nrecent years to detect depression require data collected over a long period and\nmay not be effective in cases where early detection is crucial.\n  Objective: Our main objective was to develop a minimalistic system to\nidentify depression using data retrieved in the fastest possible time.\n  Methods: We developed a fast tool that retrieves the past 7 days' app usage\ndata in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from\nBangladesh participated in our study, and our tool collected their app usage\ndata. To identify depressed and nondepressed students, we developed a diverse\nset of ML models. We selected important features using the stable approach,\nalong with 3 main types of feature selection (FS) approaches.\n  Results: Leveraging only the app usage data retrieved in 1 second, our light\ngradient boosting machine model used the important features selected by the\nstable FS approach and correctly identified 82.4% (n=42) of depressed students\n(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we\npresented a parsimonious stacking model where around 5 features selected by the\nall-relevant FS approach Boruta were used in each iteration of validation and\nshowed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis\nof our best models presented behavioral markers that were related to\ndepression.\n  Conclusions: Due to our system's fast and minimalistic nature, it may make a\nworthwhile contribution to identifying depression in underdeveloped and\ndeveloping regions. In addition, our detailed discussion about the implication\nof our findings can facilitate the development of less resource-intensive\nsystems to better understand students who are depressed."}
{"id": "2508.18607", "pdf": "https://arxiv.org/pdf/2508.18607.pdf", "abs": "https://arxiv.org/abs/2508.18607", "title": "A New NMT Model for Translating Clinical Texts from English to Spanish", "authors": ["Rumeng Li", "Xun Wang", "Hong Yu"], "categories": ["cs.CL"], "comment": "This work was accepted by the Machine Learning for Health (ML4H)\n  Workshop at NeurIPS 2018", "summary": "Translating electronic health record (EHR) narratives from English to Spanish\nis a clinically important yet challenging task due to the lack of a\nparallel-aligned corpus and the abundant unknown words contained. To address\nsuch challenges, we propose \\textbf{NOOV} (for No OOV), a new neural machine\ntranslation (NMT) system that requires little in-domain parallel-aligned corpus\nfor training. NOOV integrates a bilingual lexicon automatically learned from\nparallel-aligned corpora and a phrase look-up table extracted from a large\nbiomedical knowledge resource, to alleviate both the unknown word problem and\nthe word-repeat challenge in NMT, enhancing better phrase generation of NMT\nsystems. Evaluation shows that NOOV is able to generate better translation of\nEHR with improvement in both accuracy and fluency."}
{"id": "2508.18406", "pdf": "https://arxiv.org/pdf/2508.18406.pdf", "abs": "https://arxiv.org/abs/2508.18406", "title": "Toward Generalized Autonomous Agents: A Neuro-Symbolic AI Framework for Integrating Social and Technical Support in Education", "authors": ["Ryan Hare", "Ying Tang"], "categories": ["cs.MA", "cs.AI", "cs.HC"], "comment": "Preprint. This work has been submitted to the IEEE for possible\n  publication. In review for IEEE's Systems, Man, and Cybernetics Magazine. 8\n  pages, 3 figures. arxiv abstract has been shortened as the magazine format\n  uses a long-form abstract", "summary": "One of the enduring challenges in education is how to empower students to\ntake ownership of their learning by setting meaningful goals, tracking their\nprogress, and adapting their strategies when faced with setbacks. Research has\nshown that this form of leaner-centered learning is best cultivated through\nstructured, supportive environments that promote guided practice, scaffolded\ninquiry, and collaborative dialogue. In response, educational efforts have\nincreasingly embraced artificial-intelligence (AI)-powered digital learning\nenvironments, ranging from educational apps and virtual labs to serious games.\nRecent advances in large language models (LLMs) and neuro-symbolic systems,\nmeanwhile, offer a transformative opportunity to reimagine how support is\ndelivered in digital learning environments. LLMs are enabling socially\ninteractive learning experiences and scalable, cross-domain learning support\nthat can adapt instructional strategies across varied subjects and contexts. In\nparallel, neuro-symbolic AI provides new avenues for designing these agents\nthat are not only adaptive but also scalable across domains. Based on these\nremarks, this paper presents a multi-agent, neuro-symbolic framework designed\nto resolve the aforementioned challenges. The framework assigns distinct\npedagogical roles to specialized agents: an RL-based 'tutor' agent provides\nauthoritative, non-verbal scaffolding, while a proactive, LLM-powered 'peer'\nagent facilitates the social dimensions of learning. While prior work has\nexplored such agents in isolation, our framework's novelty lies in unifying\nthem through a central educational ontology. Through case studies in both\ncollege-level and middle school settings, we demonstrate the framework's\nadaptability across domains. We conclude by outlining key insights and future\ndirections for advancing AI-driven learning environments."}
{"id": "2508.18609", "pdf": "https://arxiv.org/pdf/2508.18609.pdf", "abs": "https://arxiv.org/abs/2508.18609", "title": "Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models", "authors": ["Chenxi Zhou", "Pengfei Cao", "Jiang Li", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) present significant deployment challenges due to\ntheir scale, with post-training quantization (PTQ) emerging as a practical\ncompression solution. However, a comprehensive understanding of how PTQ\nprecisely impacts diverse LLM knowledge capabilities remains elusive, and\nexisting scaling laws for quantized models often overlook crucial PTQ-specific\nparameters and task-specific sensitivities. This paper addresses these gaps by\nconducting an extensive empirical investigation to establish task-stratified\nscaling laws. We disentangle LLM knowledge into memorization and utilization\ncapabilities and develop a unified quantitative framework that incorporates\nmodel size, effective bit-width, calibration set size, and group size. Our\ncentral finding reveals that knowledge memorization exhibits markedly greater\nsensitivity to variations in effective bit-width, calibration set size, and\nmodel size compared to the more robust knowledge utilization. These findings\noffer a fine-grained understanding of PTQ's impact and provide guidance for\ndeveloping knowledge-aware quantization strategies that can better preserve\ntargeted cognitive functions."}
{"id": "2508.18431", "pdf": "https://arxiv.org/pdf/2508.18431.pdf", "abs": "https://arxiv.org/abs/2508.18431", "title": "DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting", "authors": ["Kérian Fiter", "Louis Malassigné-Onfroy", "Bentley Oakes"], "categories": ["cs.SE", "cs.ET", "cs.HC", "cs.SY", "eess.SY"], "comment": null, "summary": "With Digital Twin (DT) construction and evolution occurring over time,\nstakeholders require tools to understand the current characteristics and\nconceptual architecture of the system at any time. We introduce DTInsight, a\nsystematic and automated tool and methodology for producing continuous\nreporting for DTs. DTInsight offers three key features: (a) an interactive\nconceptual architecture visualization of DTs; (b) generation of summaries of DT\ncharacteristics based on ontological data; and (c) integration of these outputs\ninto a reporting page within a continuous integration and continuous deployment\n(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT\nDescription Framework (DTDF), DTInsight enables up-to-date and detailed reports\nfor enhanced stakeholder understanding."}
{"id": "2508.18648", "pdf": "https://arxiv.org/pdf/2508.18648.pdf", "abs": "https://arxiv.org/abs/2508.18648", "title": "Thinking Before You Speak: A Proactive Test-time Scaling Approach", "authors": ["Cong Li", "Wenchang Chai", "Hejun Wu", "Yan Pan", "Pengxu Wei", "Liang Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit deficiencies with complex\nreasoning tasks, such as maths, which we attribute to the discrepancy between\nhuman reasoning patterns and those presented in the LLMs' training data. When\ndealing with complex problems, humans tend to think carefully before expressing\nsolutions. However, they often do not articulate their inner thoughts,\nincluding their intentions and chosen methodologies. Consequently, critical\ninsights essential for bridging reasoning steps may be absent in training data\ncollected from human sources. To bridge this gap, we proposes inserting\n\\emph{insight}s between consecutive reasoning steps, which review the status\nand initiate the next reasoning steps. Unlike prior prompting strategies that\nrely on a single or a workflow of static prompts to facilitate reasoning,\n\\emph{insight}s are \\emph{proactively} generated to guide reasoning processes.\nWe implement our idea as a reasoning framework, named \\emph{Thinking Before You\nSpeak} (TBYS), and design a pipeline for automatically collecting and filtering\nin-context examples for the generation of \\emph{insight}s, which alleviates\nhuman labeling efforts and fine-tuning overheads. Experiments on challenging\nmathematical datasets verify the effectiveness of TBYS. Project website:\nhttps://gitee.com/jswrt/TBYS"}
{"id": "2508.18492", "pdf": "https://arxiv.org/pdf/2508.18492.pdf", "abs": "https://arxiv.org/abs/2508.18492", "title": "The Accessibility Paradox: How Blind and Low Vision Employees Experience and Negotiate Accessibility in the Technology Industry", "authors": ["Aparajita Marathe", "Anne Marie Piper"], "categories": ["cs.CY", "cs.HC"], "comment": "Article to be published in CSCW 2025 November edition", "summary": "Many technology companies aim to improve access and inclusion not only by\nmaking their products accessible but also by bringing people with disabilities\ninto the tech workforce. We know less about how accessibility is experienced\nand negotiated by disabled workers within these organizations. Through\ninterviews with 20 BLV workers across various tech companies, we uncover a\npersistent misalignment between organizational attempts at accessibility and\nthe current realities of these employees. We introduce the concept of the\naccessibility paradox, which we define as the inherent tension between the\nproductivity- and profit-driven nature of tech companies and their desire to\nhire and retain disabled workers. Focusing on the experiences of BLV workers,\nwe show how the accessibility paradox manifests in their everyday workplace\ninteractions, including digital infrastructure, accommodations processes and\npolicies, ability assumptions, and competing priorities. We offer\nrecommendations for future research and practice to understand and improve\nworkplace accessibility and inclusion."}
{"id": "2508.18651", "pdf": "https://arxiv.org/pdf/2508.18651.pdf", "abs": "https://arxiv.org/abs/2508.18651", "title": "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models", "authors": ["Chenxu Yang", "Qingyi Si", "Zheng Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Grounding responses in external knowledge represents an effective strategy\nfor mitigating hallucinations in Large Language Models (LLMs). However, current\nLLMs struggle to seamlessly integrate knowledge while simultaneously\nmaintaining faithfulness (or fidelity) and expressiveness, capabilities that\nhumans naturally possess. This limitation results in outputs that either lack\nsupport from external knowledge, thereby compromising faithfulness, or appear\noverly verbose and unnatural, thus sacrificing expressiveness. In this work, to\nbreak the trade-off between faithfulness and expressiveness, we propose\nCollaborative Decoding (CoDe), a novel approach that dynamically integrates\noutput probabilities generated with and without external knowledge. This\nintegration is guided by distribution divergence and model confidence, enabling\nthe selective activation of relevant and reliable expressions from the model's\ninternal parameters. Furthermore, we introduce a knowledge-aware reranking\nmechanism that prevents over-reliance on prior parametric knowledge while\nensuring proper utilization of provided external information. Through\ncomprehensive experiments, our plug-and-play CoDe framework demonstrates\nsuperior performance in enhancing faithfulness without compromising\nexpressiveness across diverse LLMs and evaluation metrics, validating both its\neffectiveness and generalizability."}
{"id": "2508.19163", "pdf": "https://arxiv.org/pdf/2508.19163.pdf", "abs": "https://arxiv.org/abs/2508.19163", "title": "MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation", "authors": ["Ernest Lim", "Yajie Vera He", "Jared Joselowitz", "Kate Preston", "Mohita Chowdhury", "Louis Williams", "Aisling Higham", "Katrina Mason", "Mariane Melo", "Tom Lawton", "Yan Jia", "Ibrahim Habli"], "categories": ["cs.AI", "cs.HC", "cs.MA", "68T50, 68T42, 92C50, 68Q60", "I.2.0; J.3"], "comment": "36 pages, 16 figures", "summary": "Despite the growing use of large language models (LLMs) in clinical dialogue\nsystems, existing evaluations focus on task completion or fluency, offering\nlittle insight into the behavioral and risk management requirements essential\nfor safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion\nfRamework for safe Interactions and conteXtual clinical conversational\nevaluation), a structured, extensible framework for safety-oriented evaluation\nof clinical dialogue agents.\n  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical\nscenarios, expected system behaviors and failure modes derived through\nstructured safety engineering methods; (2) BehvJudge, an LLM-based evaluator\nfor detecting safety-relevant dialogue failures, validated against expert\nclinician annotations; and (3) PatBot, a simulated patient agent capable of\nproducing diverse, scenario-conditioned responses, evaluated for realism and\nbehavioral fidelity with human factors expertise, and a patient-preference\nstudy.\n  Across three experiments, we show that MATRIX enables systematic, scalable\nsafety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard\ndetection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded\nassessment of 240 dialogues. We also conducted one of the first realism\nanalyses of LLM-based patient simulation, showing that PatBot reliably\nsimulates realistic patient behavior in quantitative and qualitative\nevaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking\nfive LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios\nand 10 clinical domains.\n  MATRIX is the first framework to unify structured safety engineering with\nscalable, validated conversational AI evaluation, enabling regulator-aligned\nsafety auditing. We release all evaluation tools, prompts, structured\nscenarios, and datasets."}
{"id": "2508.18655", "pdf": "https://arxiv.org/pdf/2508.18655.pdf", "abs": "https://arxiv.org/abs/2508.18655", "title": "Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models", "authors": ["Haoyu Wang", "Guangyan Zhang", "Jiale Chen", "Jingyu Li", "Yuehai Wang", "Yiwen Guo"], "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "comment": "5 pages, 1 figure, submitted to ICASSP 2026", "summary": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nsimply convert the response content into speech without fully understanding the\nrich emotional and paralinguistic cues embedded in the user's query. In many\ncases, the same sentence can have different meanings depending on the emotional\nexpression. Furthermore, emotional understanding is essential for improving\nuser experience in human-machine interaction. Currently, most speech LLMs with\nempathetic capabilities are trained on massive datasets. This approach requires\nvast amounts of data and significant computational resources. Therefore, a key\nchallenge lies in how to develop a speech LLM capable of generating empathetic\nresponses with limited data and without the need for large-scale training. To\naddress this challenge, we propose Emotion Omni, a novel model architecture\ndesigned to understand the emotional content of user speech input and generate\nempathetic speech responses. Additionally, we developed a data generation\npipeline based on an open-source TTS framework to construct a 200k emotional\ndialogue dataset, which supports the construction of an empathetic speech\nassistant. The demos are available at https://w311411.github.io/omni_demo/"}
{"id": "2508.19227", "pdf": "https://arxiv.org/pdf/2508.19227.pdf", "abs": "https://arxiv.org/abs/2508.19227", "title": "Generative Interfaces for Language Models", "authors": ["Jiaqi Chen", "Yanzhe Zhang", "Yutong Zhang", "Yijia Shao", "Diyi Yang"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction."}
{"id": "2508.18673", "pdf": "https://arxiv.org/pdf/2508.18673.pdf", "abs": "https://arxiv.org/abs/2508.18673", "title": "Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum", "authors": ["Xinglong Yang", "Quan Feng", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Wentong Li", "Shuofei Qiao", "Yuxia Geng", "Xingyu Zhao", "Sheng-Jun Huang"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often\nlimited by the use of randomly or manually selected examples. These examples\nfail to account for both model-specific knowledge distributions and the\nintrinsic complexity of the tasks, resulting in suboptimal and unstable model\nperformance. To address this, we propose a novel framework inspired by the\npedagogical principle of \"tailored teaching with balanced difficulty\". We\nreframe prompt selection as a prompt curriculum design problem: constructing a\nwell ordered set of training examples that align with the model's current\ncapabilities. Our approach integrates two complementary signals: (1)\nmodel-perceived difficulty, quantified through prediction disagreement in an\nactive learning setup, capturing what the model itself finds challenging; and\n(2) intrinsic sample complexity, which measures the inherent difficulty of each\nquestion-image pair independently of any model. By jointly analyzing these\nsignals, we develop a difficulty-balanced sampling strategy that ensures the\nselected prompt examples are diverse across both dimensions. Extensive\nexperiments conducted on five challenging benchmarks and multiple popular\nMultimodal Large Language Models (MLLMs) demonstrate that our method yields\nsubstantial and consistent improvements and greatly reduces performance\ndiscrepancies caused by random sampling, providing a principled and robust\napproach for enhancing multimodal reasoning."}
{"id": "2308.03651", "pdf": "https://arxiv.org/pdf/2308.03651.pdf", "abs": "https://arxiv.org/abs/2308.03651", "title": "Cluster-Aware Grid Layout", "authors": ["Yuxing Zhou", "Weikai Yang", "Jiashu Chen", "Changjian Chen", "Zhiyang Shen", "Xiaonan Luo", "Lingyun Yu", "Shixia Liu"], "categories": ["cs.HC"], "comment": "Accecpted in IEEE VIS 2023. 11 pages. 10 figures", "summary": "Grid visualizations are widely used in many applications to visually explain\na set of data and their proximity relationships. However, existing layout\nmethods face difficulties when dealing with the inherent cluster structures\nwithin the data. To address this issue, we propose a cluster-aware grid layout\nmethod that aims to better preserve cluster structures by simultaneously\nconsidering proximity, compactness, and convexity in the optimization process.\nOur method utilizes a hybrid optimization strategy that consists of two phases.\nThe global phase aims to balance proximity and compactness within each cluster,\nwhile the local phase ensures the convexity of cluster shapes. We evaluate the\nproposed grid layout method through a series of quantitative experiments and\ntwo use cases, demonstrating its effectiveness in preserving cluster structures\nand facilitating analysis tasks."}
{"id": "2508.18687", "pdf": "https://arxiv.org/pdf/2508.18687.pdf", "abs": "https://arxiv.org/abs/2508.18687", "title": "Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning", "authors": ["Songtao Jiang", "Yuxi Chen", "Sibo Song", "Yan Zhang", "Yeying Jin", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": null, "summary": "In high-stakes medical applications, consistent answering across diverse\nquestion phrasings is essential for reliable diagnosis. However, we reveal that\ncurrent Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility\nin Medical Visual Question Answering, as their answers fluctuate significantly\nwhen faced with semantically equivalent rephrasings of medical questions. We\nattribute this to two limitations: (1) insufficient alignment of medical\nconcepts, leading to divergent reasoning patterns, and (2) hidden biases in\ntraining data that prioritize syntactic shortcuts over semantic understanding.\nTo address these challenges, we construct RoMed, a dataset built upon original\nVQA datasets containing 144k questions with variations spanning word-level,\nsentence-level, and semantic-level perturbations. When evaluating\nstate-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming\nperformance drops (e.g., a 40\\% decline in Recall) compared to original VQA\nbenchmarks, exposing critical robustness gaps. To bridge this gap, we propose\nConsistency and Contrastive Learning (CCL), which integrates two key\ncomponents: (1) knowledge-anchored consistency learning, aligning Med-VLMs with\nmedical knowledge rather than shallow feature patterns, and (2) bias-aware\ncontrastive learning, mitigating data-specific priors through discriminative\nrepresentation refinement. CCL achieves SOTA performance on three popular VQA\nbenchmarks and notably improves answer consistency by 50\\% on the challenging\nRoMed test set, demonstrating significantly enhanced robustness. Code will be\nreleased."}
{"id": "2504.09955", "pdf": "https://arxiv.org/pdf/2504.09955.pdf", "abs": "https://arxiv.org/abs/2504.09955", "title": "VR MRI Training for Adolescents: A Comparative Study of Gamified VR, Passive VR, 360° Video, and Traditional Educational Video", "authors": ["Yue Yang", "Mengyao Guo", "Yuxuan Wu", "Wally Niu", "Emmanuel A Corona", "Bruce Daniel", "Christoph Leuze", "Fred Baik"], "categories": ["cs.HC"], "comment": "Download our application at\n  https://www.meta.com/experiences/stanford-mri-simulator/8205539289482347/", "summary": "Meta Quest Store:\nhttps://www.meta.com/experiences/stanford-mri-simulator/8205539289482347/\nMagnetic Resonance Imaging (MRI) can be a stressful experience for pediatric\npatients due to the loud acoustic environment, enclosed scanner bore, and a\nprolonged requirement to remain still. While sedation is commonly used to\nmanage anxiety and motion, it carries clinical risks and logistical burdens.\nTraditional preparatory approaches, such as instructional videos and mock\nscans, often lack engagement for older children and adolescents. In this study,\nwe present a comparative evaluation of four MRI preparation modalities: (1) a\ngamified virtual reality (VR) simulation that trains stillness through\nreal-time feedback; (2) a passive VR experience replicating the MRI environment\nwithout interactivity; (3) a 360{\\deg} first-person video of a real MRI\nprocedure; and (4) a standard 2D educational video. Using a within-subjects\ndesign (N = 11, ages 10-16), we assess each method's impact on head motion\ndata, anxiety reduction, procedural preparedness, usability, cognitive\nworkload, and subjective preference. Results show that the gamified VR\ncondition has significantly lower head motion (p < 0.001) and yielded the\nhighest preparedness scores (p < 0.05). Head motion data were significantly\ncorrelated with learning outcomes (p < 0.01), suggesting that behavioral\nperformance in VR strongly indicates procedural readiness. While all modalities\nreduced anxiety and were rated usable, interactive VR was preferred by most\nparticipants and demonstrated unique advantages in promoting engagement and\nbehavioral rehearsal. We conclude with design recommendations for designing\nimmersive simulations and integrating VR training into pediatric imaging\nworkflows."}
{"id": "2508.18701", "pdf": "https://arxiv.org/pdf/2508.18701.pdf", "abs": "https://arxiv.org/abs/2508.18701", "title": "Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System", "authors": ["Yanfan Du", "Jun Zhang", "Bin Wang", "Jin Qiu", "Lu Huang", "Yuan Ge", "Xiaoqian Liu", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "9 pages, 4 figures, 5 tables", "summary": "Recent advances in speech large language models (SLMs) have improved speech\nrecognition and translation in general domains, but accurately generating\ndomain-specific terms or neologisms remains challenging. To address this, we\npropose Attention2Probability: attention-driven terminology probability\nestimation for robust speech-to-text system, which is lightweight, flexible,\nand accurate. Attention2Probability converts cross-attention weights between\nspeech and terminology into presence probabilities, and it further employs\ncurriculum learning to enhance retrieval accuracy. Furthermore, to tackle the\nlack of data for speech-to-text tasks with terminology intervention, we create\nand release a new speech dataset with terminology to support future research in\nthis area. Experimental results show that Attention2Probability significantly\noutperforms the VectorDB method on our test set. Specifically, its maximum\nrecall rates reach 92.57% for Chinese and 86.83% for English. This high recall\nis achieved with a latency of only 8.71ms per query. Intervening in SLMs'\nrecognition and translation tasks using Attention2Probability-retrieved terms\nimproves terminology accuracy by 6-17%, while revealing that the current\nutilization of terminology by SLMs has limitations."}
{"id": "2504.12830", "pdf": "https://arxiv.org/pdf/2504.12830.pdf", "abs": "https://arxiv.org/abs/2504.12830", "title": "A Taxonomy of Questions for Critical Reflection in Machine-Assisted Decision-Making", "authors": ["Simon W. S. Fischer", "Hanna Schraffenberger", "Serge Thill", "Pim Haselager"], "categories": ["cs.HC", "cs.CY"], "comment": "Accepted at Eight AAAI/ACM Conference on AI, Ethics, and Society\n  (AIES 2025), 20-22 October 2025 in Madrid, Spain", "summary": "Decision-makers run the risk of relying too much on machine recommendations,\nwhich is associated with lower cognitive engagement. Reflection has been shown\nto increase cognitive engagement and improve critical thinking and therefore\ndecision-making. Questions are a means to stimulate reflection, but there is a\nresearch gap regarding the systematic creation and use of relevant questions\nfor machine-assisted decision-making. We therefore present a taxonomy of\nquestions aimed at promoting reflection and cognitive engagement in order to\nstimulate a deliberate decision-making process. Our taxonomy builds on the\nSocratic questioning method and a question bank for explainable AI. As a\nstarting point, we focus on clinical decision-making. Brief discussions with\ntwo medical and three educational researchers provide feedback on the relevance\nand expected benefits of our taxonomy. Our work contributes to research on\nmitigating overreliance in human-AI interactions and aims to support effective\nhuman oversight as required by the European AI Act."}
{"id": "2508.18709", "pdf": "https://arxiv.org/pdf/2508.18709.pdf", "abs": "https://arxiv.org/abs/2508.18709", "title": "Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs", "authors": ["Duy Le", "Kent Ziti", "Evan Girard-Sun", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual riddle generation challenges large language models (LLMs) to\nbalance cultural fluency with creative abstraction. Standard prompting\nstrategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized\nriddles or perform shallow paraphrasing. We introduce Adaptive Originality\nFiltering (AOF), a prompting framework that filters redundant generations using\ncosine-based similarity rejection, while enforcing lexical novelty and\ncross-lingual fidelity. Evaluated across three LLMs and four language pairs,\nAOF-enhanced GPT-4o achieves \\texttt{0.177} Self-BLEU and \\texttt{0.915}\nDistinct-2 in Japanese, signaling improved lexical diversity and reduced\nredundancy compared to other prompting methods and language pairs. Our findings\nshow that semantic rejection can guide culturally grounded, creative generation\nwithout task-specific fine-tuning."}
{"id": "2507.22900", "pdf": "https://arxiv.org/pdf/2507.22900.pdf", "abs": "https://arxiv.org/abs/2507.22900", "title": "New Kid in the Classroom: Exploring Student Perceptions of AI Coding Assistants", "authors": ["Sergio Rojas-Galeano"], "categories": ["cs.HC", "cs.AI"], "comment": "A shorter version of the manuscript (16 pages) has been accepted to\n  be published in Proceedings of 19th Colombian Conference on Computing, CCC\n  2025", "summary": "The arrival of AI coding assistants in educational settings presents a\nparadigm shift, introducing a \"new kid in the classroom\" for both students and\ninstructors. This exploratory study addresses how these tools are shaping the\nexperiences of novice programmers in an introductory programming course.\nThrough a two-part exam, we investigated student perceptions by first providing\naccess to AI support for a programming task and then requiring an extension of\nthe solution without it. We collected Likert-scale and open-ended responses\nfrom 20 students to understand their perceptions on the challenges they faced.\nOur findings reveal that students perceived AI tools as helpful for grasping\ncode concepts and boosting their confidence during the initial development\nphase. However, a noticeable difficulty emerged when students were asked to\nwork unaided, pointing to potential overreliance and gaps in foundational\nknowledge transfer. These insights highlight a critical need for new\npedagogical approaches that integrate AI effectively while effectively\nenhancing core programming skills, rather than impersonating them."}
{"id": "2508.18715", "pdf": "https://arxiv.org/pdf/2508.18715.pdf", "abs": "https://arxiv.org/abs/2508.18715", "title": "EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues", "authors": ["Angela Yifei Yuan", "Haoyi Li", "Soyeon Caren Han", "Christopher Leckie"], "categories": ["cs.CL"], "comment": "15 pages", "summary": "The rapid adoption of large language models (LLMs) in customer service\nintroduces new risks, as malicious actors can exploit them to conduct\nlarge-scale user impersonation through machine-generated text (MGT). Current\nMGT detection methods often struggle in online conversational settings,\nreducing the reliability and interpretability essential for trustworthy AI\ndeployment. In customer service scenarios where operators are typically\nnon-expert users, explanation become crucial for trustworthy MGT detection. In\nthis paper, we propose EMMM, an explanation-then-detection framework that\nbalances latency, accuracy, and non-expert-oriented interpretability.\nExperimental results demonstrate that EMMM provides explanations accessible to\nnon-expert users, with 70\\% of human evaluators preferring its outputs, while\nachieving competitive accuracy compared to state-of-the-art models and\nmaintaining low latency, generating outputs within 1 second. Our code and\ndataset are open-sourced at\nhttps://github.com/AngieYYF/EMMM-explainable-chatbot-detection."}
{"id": "2508.16076", "pdf": "https://arxiv.org/pdf/2508.16076.pdf", "abs": "https://arxiv.org/abs/2508.16076", "title": "Prompting with Sign Parameters for Low-resource Sign Language Instruction Generation", "authors": ["Md Tariquzzaman", "Md Farhan Ishmam", "Saiyma Sittul Muna", "Md Kamrul Hasan", "Hasan Mahmud"], "categories": ["cs.HC", "cs.CV"], "comment": "CV4A11y@ICCV 2025", "summary": "Sign Language (SL) enables two-way communication for the deaf and\nhard-of-hearing community, yet many sign languages remain under-resourced in\nthe AI space. Sign Language Instruction Generation (SLIG) produces step-by-step\ntextual instructions that enable non-SL users to imitate and learn SL gestures,\npromoting two-way interaction. We introduce BdSLIG, the first Bengali SLIG\ndataset, used to evaluate Vision Language Models (VLMs) (i) on under-resourced\nSLIG tasks, and (ii) on long-tail visual concepts, as Bengali SL is unlikely to\nappear in the VLM pre-training data. To enhance zero-shot performance, we\nintroduce Sign Parameter-Infused (SPI) prompting, which integrates standard SL\nparameters, like hand shape, motion, and orientation, directly into the textual\nprompts. Subsuming standard sign parameters into the prompt makes the\ninstructions more structured and reproducible than free-form natural text from\nvanilla prompting. We envision that our work would promote inclusivity and\nadvancement in SL learning systems for the under-resourced communities."}
{"id": "2508.18739", "pdf": "https://arxiv.org/pdf/2508.18739.pdf", "abs": "https://arxiv.org/abs/2508.18739", "title": "Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models", "authors": ["Chang Wang", "Siyu Yan", "Depeng Yuan", "Yuqi Chen", "Yanhua Huang", "Yuanhang Zheng", "Shuhao Li", "Yinqi Zhang", "Kedi Chen", "Mingrui Zhu", "Ruiwen Xu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The generation of ad headlines plays a vital role in modern advertising,\nwhere both quality and diversity are essential to engage a broad range of\naudience segments. Current approaches primarily optimize language models for\nheadline quality or click-through rates (CTR), often overlooking the need for\ndiversity and resulting in homogeneous outputs. To address this limitation, we\npropose DIVER, a novel framework based on large language models (LLMs) that are\njointly optimized for both diversity and quality. We first design a semantic-\nand stylistic-aware data generation pipeline that automatically produces\nhigh-quality training pairs with ad content and multiple diverse headlines. To\nachieve the goal of generating high-quality and diversified ad headlines within\na single forward pass, we propose a multi-stage multi-objective optimization\nframework with supervised fine-tuning (SFT) and reinforcement learning (RL).\nExperiments on real-world industrial datasets demonstrate that DIVER\neffectively balances quality and diversity. Deployed on a large-scale\ncontent-sharing platform serving hundreds of millions of users, our framework\nimproves advertiser value (ADVV) and CTR by 4.0% and 1.4%."}
{"id": "2411.02470", "pdf": "https://arxiv.org/pdf/2411.02470.pdf", "abs": "https://arxiv.org/abs/2411.02470", "title": "Benchmarking XAI Explanations with Human-Aligned Evaluations", "authors": ["Rémi Kazmierczak", "Steve Azzolin", "Eloïse Berthier", "Anna Hedström", "Patricia Delhomme", "David Filliat", "Nicolas Bousquet", "Goran Frehse", "Massimiliano Mancini", "Baptiste Caramiaux", "Andrea Passerini", "Gianni Franchi"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "https://github.com/ENSTA-U2IS-AI/Dataset_XAI", "summary": "We introduce PASTA (Perceptual Assessment System for explanaTion of\nArtificial Intelligence), a novel human-centric framework for evaluating\neXplainable AI (XAI) techniques in computer vision. Our first contribution is\nthe creation of the PASTA-dataset, the first large-scale benchmark that spans a\ndiverse set of models and both saliency-based and concept-based explanation\nmethods. This dataset enables robust, comparative analysis of XAI techniques\nbased on human judgment. Our second contribution is an automated, data-driven\nbenchmark that predicts human preferences using the PASTA-dataset. This scoring\ncalled PASTA-score method offers scalable, reliable, and consistent evaluation\naligned with human perception. Additionally, our benchmark allows for\ncomparisons between explanations across different modalities, an aspect\npreviously unaddressed. We then propose to apply our scoring method to probe\nthe interpretability of existing models and to build more human interpretable\nXAI methods."}
{"id": "2508.18740", "pdf": "https://arxiv.org/pdf/2508.18740.pdf", "abs": "https://arxiv.org/abs/2508.18740", "title": "M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations", "authors": ["Qiao Liang", "Ying Shen", "Tiantian Chen", "Lin Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 8 figures. Accepted to Findings of ACL 2025", "summary": "Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has\nrecently gained significant attention in social media analysis, aiming to\nextract emotion utterances, cause utterances, and emotion categories\nsimultaneously. However, the scarcity of related datasets, with only one\npublished dataset featuring highly uniform dialogue scenarios, hinders model\ndevelopment in this field. To address this, we introduce MECAD, the first\nmultimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56\nTV series spanning a wide range of dialogue contexts. In addition, existing\nMECTEC methods fail to explicitly model emotional and causal contexts and\nneglect the fusion of semantic information at different levels, leading to\nperformance degradation. In this paper, we propose M3HG, a novel model that\nexplicitly captures emotional and causal contexts and effectively fuses\ncontextual information at both inter- and intra-utterance levels via a\nmultimodal heterogeneous graph. Extensive experiments demonstrate the\neffectiveness of M3HG compared with existing state-of-the-art methods. The\ncodes and dataset are available at https://github.com/redifinition/M3HG."}
{"id": "2412.01459", "pdf": "https://arxiv.org/pdf/2412.01459.pdf", "abs": "https://arxiv.org/abs/2412.01459", "title": "Perception Gaps in Risk, Benefit, and Value Between Experts and Public Challenge Socially Accepted AI", "authors": ["Philipp Brauner", "Felix Glawe", "Gian Luca Liehner", "Luisa Vervier", "Martina Ziefle"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Artificial Intelligence (AI) is reshaping many societal domains, raising\ncritical questions about its risks, benefits, and the potential misalignment\nbetween public and academic perspectives. This study examines how the general\npublic (N=1110) -- individuals who interact with or are impacted by AI\ntechnologies -- and academic AI experts (N=119) -- those elites shaping AI\ndevelopment -- perceive AI's capabilities and impact across 71 scenarios. These\nscenarios span domains such as sustainability, healthcare, job performance,\nsocietal inequality, art, and warfare. Participants evaluated these scenarios\nacross four dimensions using the psychometric model: likelihood, perceived risk\nand benefit, and overall value (or sentiment). The results suggest significant\ndifferences: experts consistently anticipate higher probabilities, perceive\nlower risks, report greater benefits, and express more positive sentiment\ntoward AI compared to the non-experts. Moreover, both groups apply different\nweighting schemes: experts discount risk more heavily relative to benefit than\nnon-experts. Visual mappings of these evaluations uncover areas convergent\nevaluations (e.g., AI performing medical diagnoses or criminal use) as well as\ntension points (e.g., decision of legal cases, political decision making),\nhighlighting areas where communication and policy interventions may be needed.\nThese findings underscore a critical translational challenge: if AI research\nand deployment are to align with societal priorities, the perception gap\nbetween developers and the public must be better understood and addressed. Our\nresults provide an empirical foundation for value-sensitive AI governance and\ntrust-building strategies across stakeholder groups."}
{"id": "2508.18748", "pdf": "https://arxiv.org/pdf/2508.18748.pdf", "abs": "https://arxiv.org/abs/2508.18748", "title": "Chronological Passage Assembling in RAG framework for Temporal Question Answering", "authors": ["Byeongjeong Kim", "Jeonghyun Park", "Joonho Yang", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "7 pages, 3 figures", "summary": "Long-context question answering over narrative tasks is challenging because\ncorrect answers often hinge on reconstructing a coherent timeline of events\nwhile preserving contextual flow in a limited context window.\nRetrieval-augmented generation (RAG) indexing methods aim to address this\nchallenge by selectively retrieving only necessary document segments. However,\nnarrative texts possess unique characteristics that limit the effectiveness of\nthese existing approaches. Specifically, understanding narrative texts requires\nmore than isolated segments, as the broader context and sequential\nrelationships between segments are crucial for comprehension. To address these\nlimitations, we propose ChronoRAG, a novel RAG framework specialized for\nnarrative texts. This approach focuses on two essential aspects: refining\ndispersed document information into coherent and structured passages, and\npreserving narrative flow by explicitly capturing and maintaining the temporal\norder among retrieved passages. We empirically demonstrate the effectiveness of\nChronoRAG through experiments on the NarrativeQA dataset, showing substantial\nimprovements in tasks requiring both factual identification and comprehension\nof complex sequential relationships, underscoring that reasoning over temporal\norder is crucial in resolving narrative QA."}
{"id": "2412.13841", "pdf": "https://arxiv.org/pdf/2412.13841.pdf", "abs": "https://arxiv.org/abs/2412.13841", "title": "Cultural Dimensions of AI Perception: Charting Expectations, Risks, Benefits, Tradeoffs, and Value in Germany and China", "authors": ["Philipp Brauner", "Felix Glawe", "Gian Luca Liehner", "Luisa Vervier", "Martina Ziefle"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "As artificial intelligence (AI) continues to advance, understanding public\nperceptions -- including biases, risks, and benefits -- is essential for\nguiding research priorities and AI alignment, shaping public discourse, and\ninforming policy. This exploratory study investigates cultural differences in\nmental models of AI using 71 imaginaries of AI's potential futures. Drawing on\ncross-cultural convenience samples from Germany (N=52) and China (N=60), we\nidentify significant differences in expectations, evaluations, and risk-benefit\ntradeoffs. Participants from Germany generally provided more cautious\nassessments, whereas participants from China expressed greater optimism\nregarding AI's societal benefits. Chinese participants exhibited relatively\nbalanced risk-benefit tradeoffs ($\\beta=-0.463$ for risk and $\\beta=+0.484$ for\nbenefit, $r^2=.630$). In contrast, German participants placed greater emphasis\non AI's benefits and comparatively less on risks ($\\beta=-0.337$ for risk and\n$\\beta=+0.715$ for benefit, $r^2=.839$). Visual cognitive maps illustrate these\ncontrasts, offering new perspectives on how cultural contexts shape AI\nacceptance. Our findings highlight key factors influencing public perception\nand provide insights for aligning AI with societal values and promoting\nequitable and culturally sensitive integration of AI technologies."}
{"id": "2508.18773", "pdf": "https://arxiv.org/pdf/2508.18773.pdf", "abs": "https://arxiv.org/abs/2508.18773", "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models", "authors": ["Qianyu He", "Siyu Yuan", "Xuefeng Li", "Mingxuan Wang", "Jiangjie Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks."}
{"id": "2502.09787", "pdf": "https://arxiv.org/pdf/2502.09787.pdf", "abs": "https://arxiv.org/abs/2502.09787", "title": "TableTalk: Scaffolding Spreadsheet Development with a Language Agent", "authors": ["Jenny T. Liang", "Aayush Kumar", "Yasharth Bajpai", "Sumit Gulwani", "Vu Le", "Chris Parnin", "Arjun Radhakrishna", "Ashish Tiwari", "Emerson Murphy-Hill", "Guastavo Soares"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Spreadsheet programming is challenging. Programmers use spreadsheet\nprogramming knowledge (e.g., formulas) and problem-solving skills to combine\nactions into complex tasks. Advancements in large language models have\nintroduced language agents that observe, plan, and perform tasks, showing\npromise for spreadsheet creation. We present TableTalk, a spreadsheet\nprogramming agent embodying three design principles -- scaffolding,\nflexibility, and incrementality -- derived from studies with seven spreadsheet\nprogrammers and 85 Excel templates. TableTalk guides programmers through\nstructured plans based on professional workflows, generating three potential\nnext steps to adapt plans to programmer needs. It uses pre-defined tools to\ngenerate spreadsheet components and incrementally build spreadsheets. In a\nstudy with 20 programmers, TableTalk produced higher-quality spreadsheets 2.3\ntimes more likely to be preferred than the baseline. It reduced cognitive load\nand thinking time by 12.6%. From this, we derive design guidelines for agentic\nspreadsheet programming tools and discuss implications on spreadsheet\nprogramming, end-user programming, AI-assisted programming, and human-agent\ncollaboration."}
{"id": "2508.18780", "pdf": "https://arxiv.org/pdf/2508.18780.pdf", "abs": "https://arxiv.org/abs/2508.18780", "title": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction", "authors": ["Yilin Li", "Xunjian Yin", "Yilin Chen", "Xiaojun Wan"], "categories": ["cs.CL", "cs.AI"], "comment": "Code will be released upon publication", "summary": "Grammatical error correction is a significant task in NLP. Traditional\nmethods based on encoder-decoder models have achieved certain success, but the\napplication of LLMs in this field is still underexplored. Current research\npredominantly relies on supervised fine-tuning to train LLMs to directly\ngenerate the corrected sentence, which limits the model's powerful reasoning\nability. To address this limitation, we propose a novel framework based on\nRule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL\nframework achieves \\textbf{state-of-the-art }performance, with a notable\nincrease in \\textbf{recall}. This result clearly highlights the advantages of\nusing RL to steer LLMs, offering a more controllable and reliable paradigm for\nfuture development in GEC."}
{"id": "2503.04945", "pdf": "https://arxiv.org/pdf/2503.04945.pdf", "abs": "https://arxiv.org/abs/2503.04945", "title": "Collaborative Evaluation of Deepfake Text with Deliberation-Enhancing Dialogue Systems", "authors": ["Jooyoung Lee", "Xiaochen Zhu", "Georgi Karadzhov", "Tom Stafford", "Andreas Vlachos", "Dongwon Lee"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "15; To appear in ICWSM 2026 (https://www.icwsm.org/2026/)", "summary": "The proliferation of generative models has presented significant challenges\nin distinguishing authentic human-authored content from deepfake content.\nCollaborative human efforts, augmented by AI tools, present a promising\nsolution. In this study, we explore the potential of DeepFakeDeLiBot, a\ndeliberation-enhancing chatbot, to support groups in detecting deepfake text.\nOur findings reveal that group-based problem-solving significantly improves the\naccuracy of identifying machine-generated paragraphs compared to individual\nefforts. While engagement with DeepFakeDeLiBot does not yield substantial\nperformance gains overall, it enhances group dynamics by fostering greater\nparticipant engagement, consensus building, and the frequency and diversity of\nreasoning-based utterances. Additionally, participants with higher perceived\neffectiveness of group collaboration exhibited performance benefits from\nDeepFakeDeLiBot. These findings underscore the potential of deliberative\nchatbots in fostering interactive and productive group dynamics while ensuring\naccuracy in collaborative deepfake text detection. \\textit{Dataset and source\ncode used in this study will be made publicly available upon acceptance of the\nmanuscript."}
{"id": "2508.18783", "pdf": "https://arxiv.org/pdf/2508.18783.pdf", "abs": "https://arxiv.org/abs/2508.18783", "title": "Controllable Conversational Theme Detection Track at DSTC 12", "authors": ["Igor Shalyminov", "Hang Su", "Jake Vincent", "Siffi Singh", "Jason Cai", "James Gung", "Raphael Shu", "Saab Mansour"], "categories": ["cs.CL"], "comment": "DSTC12@SigDial2025; data and code available at\n  https://github.com/amazon-science/dstc12-controllable-conversational-theme-detection", "summary": "Conversational analytics has been on the forefront of transformation driven\nby the advances in Speech and Natural Language Processing techniques. Rapid\nadoption of Large Language Models (LLMs) in the analytics field has taken the\nproblems that can be automated to a new level of complexity and scale. In this\npaper, we introduce Theme Detection as a critical task in conversational\nanalytics, aimed at automatically identifying and categorizing topics within\nconversations. This process can significantly reduce the manual effort involved\nin analyzing expansive dialogs, particularly in domains like customer support\nor sales. Unlike traditional dialog intent detection, which often relies on a\nfixed set of intents for downstream system logic, themes are intended as a\ndirect, user-facing summary of the conversation's core inquiry. This\ndistinction allows for greater flexibility in theme surface forms and\nuser-specific customizations. We pose Controllable Conversational Theme\nDetection problem as a public competition track at Dialog System Technology\nChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of\ndialog utterances, with the distinctive aspect being controllability of the\nresulting theme clusters' granularity achieved via the provided user preference\ndata. We give an overview of the problem, the associated dataset and the\nevaluation metrics, both automatic and human. Finally, we discuss the\nparticipant teams' submissions and provide insights from those. The track\nmaterials (data and code) are openly available in the GitHub repository."}
{"id": "2508.13051", "pdf": "https://arxiv.org/pdf/2508.13051.pdf", "abs": "https://arxiv.org/abs/2508.13051", "title": "Investigating VR Accessibility Reviews for Users with Disabilities: A Qualitative Analysis", "authors": ["Yi Wang", "Chetan Arora", "Xiao Liu", "Thuong Hoang", "ZHengxin Zhang", "Henry Been Lirn Duh", "John Grundy"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Accessibility reviews provide valuable insights into both the limitations and\nbenefits experienced by users with disabilities when using virtual reality (VR)\napplications. However, a comprehensive investigation into VR accessibility for\nusers with disabilities is still lacking. To fill this gap, this study analyzes\nuser reviews from the Meta and Steam stores of VR apps, focusing on the\nreported issues affecting users with disabilities. We applied selection\ncriteria to 1,367,419 reviews from the top 40, the 20 most popular, and the 40\nlowest-rated VR applications on both platforms. In total, 1,076 (0.078%) VR\naccessibility reviews referenced various disabilities across 100 VR\napplications. These applications were categorized into Action, Sports, Social,\nPuzzle, Horror, and Simulation, with Action receiving the highest number of\naccessibility related-reviews. We identified 16 different types of disabilities\nacross six categories. Furthermore, we examined the causes of accessibility\nissues as reported by users with disabilities. Overall, VR accessibility\nreviews were predominantly under-supported."}
{"id": "2508.18791", "pdf": "https://arxiv.org/pdf/2508.18791.pdf", "abs": "https://arxiv.org/abs/2508.18791", "title": "LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination", "authors": ["Ziming Zhu", "Chenglong Wang", "Shunjie Xing", "Yifu Huo", "Fengning Tian", "Quan Du", "Di Yang", "Chunliang Zhang", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Despite the remarkable progress of modern machine translation (MT) systems on\ngeneral-domain texts, translating structured LaTeX-formatted documents remains\na significant challenge. These documents typically interleave natural language\nwith domain-specific syntax, such as mathematical equations, tables, figures,\nand cross-references, all of which must be accurately preserved to maintain\nsemantic integrity and compilability. In this paper, we introduce LaTeXTrans, a\ncollaborative multi-agent system designed to address this challenge. LaTeXTrans\nensures format preservation, structural fidelity, and terminology consistency\nthrough six specialized agents: 1) a Parser that decomposes LaTeX into\ntranslation-friendly units via placeholder substitution and syntax filtering;\n2) a Translator, Validator, Summarizer, and Terminology Extractor that work\ncollaboratively to ensure context-aware, self-correcting, and\nterminology-consistent translations; 3) a Generator that reconstructs the\ntranslated content into well-structured LaTeX documents. Experimental results\ndemonstrate that LaTeXTrans can outperform mainstream MT systems in both\ntranslation accuracy and structural fidelity, offering an effective and\npractical solution for translating LaTeX-formatted documents."}
{"id": "2508.14996", "pdf": "https://arxiv.org/pdf/2508.14996.pdf", "abs": "https://arxiv.org/abs/2508.14996", "title": "adder-viz: Real-Time Visualization Software for Transcoding Event Video", "authors": ["Andrew C. Freeman", "Luke Reinkensmeyer"], "categories": ["cs.MM", "cs.CV", "cs.HC", "eess.IV"], "comment": "Accepted to the Open-Source Track at ACM Multimedia 2025", "summary": "Recent years have brought about a surge in neuromorphic ``event'' video\nresearch, primarily targeting computer vision applications. Event video eschews\nvideo frames in favor of asynchronous, per-pixel intensity samples. While much\nwork has focused on a handful of representations for specific event cameras,\nthese representations have shown limitations in flexibility, speed, and\ncompressibility. We previously proposed the unified ADDER representation to\naddress these concerns. This paper introduces numerous improvements to the\nadder-viz software for visualizing real-time event transcode processes and\napplications in-the-loop. The MIT-licensed software is available from a\ncentralized repository at https://github.com/ac-freeman/adder-codec-rs."}
{"id": "2508.18819", "pdf": "https://arxiv.org/pdf/2508.18819.pdf", "abs": "https://arxiv.org/abs/2508.18819", "title": "LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection", "authors": ["Shubham Gupta", "Shraban Kumar Chatterjee", "Suman Kundu"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "The proliferation of misinformation in the digital age has led to significant\nsocietal challenges. Existing approaches often struggle with capturing\nlong-range dependencies, complex semantic relations, and the social dynamics\ninfluencing news dissemination. Furthermore, these methods require extensive\nlabelled datasets, making their deployment resource-intensive. In this study,\nwe propose a novel self-supervised misinformation detection framework that\nintegrates both complex semantic relations using Abstract Meaning\nRepresentation (AMR) and news propagation dynamics. We introduce an LLM-based\ngraph contrastive loss (LGCL) that utilizes negative anchor points generated by\na Large Language Model (LLM) to enhance feature separability in a zero-shot\nmanner. To incorporate social context, we employ a multi view graph masked\nautoencoder, which learns news propagation features from social context graph.\nBy combining these semantic and propagation-based features, our approach\neffectively differentiates between fake and real news in a self-supervised\nmanner. Extensive experiments demonstrate that our self-supervised framework\nachieves superior performance compared to other state-of-the-art methodologies,\neven with limited labelled datasets while improving generalizability."}
{"id": "2508.18824", "pdf": "https://arxiv.org/pdf/2508.18824.pdf", "abs": "https://arxiv.org/abs/2508.18824", "title": "Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness", "authors": ["Sirui Chen", "Changxin Tian", "Binbin Hu", "Kunlong Chen", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the mathematical reasoning of large language models (LLMs) demands\nhigh-quality training data, yet conventional methods face critical challenges\nin scalability, cost, and data reliability. To address these limitations, we\npropose a novel program-assisted synthesis framework that systematically\ngenerates a high-quality mathematical corpus with guaranteed diversity,\ncomplexity, and correctness. This framework integrates mathematical knowledge\nsystems and domain-specific tools to create executable programs. These programs\nare then translated into natural language problem-solution pairs and vetted by\na bilateral validation mechanism that verifies solution correctness against\nprogram outputs and ensures program-problem consistency. We have generated 12.3\nmillion such problem-solving triples. Experiments demonstrate that models\nfine-tuned on our data significantly improve their inference capabilities,\nachieving state-of-the-art performance on several benchmark datasets and\nshowcasing the effectiveness of our synthesis approach."}
{"id": "2508.18847", "pdf": "https://arxiv.org/pdf/2508.18847.pdf", "abs": "https://arxiv.org/abs/2508.18847", "title": "ConfTuner: Training Large Language Models to Express Their Confidence Verbally", "authors": ["Yibo Li", "Miao Xiong", "Jiaying Wu", "Bryan Hooi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes domains\nsuch as science, law, and healthcare, where accurate expressions of uncertainty\nare essential for reliability and trust. However, current LLMs are often\nobserved to generate incorrect answers with high confidence, a phenomenon known\nas \"overconfidence\". Recent efforts have focused on calibrating LLMs'\nverbalized confidence: i.e., their expressions of confidence in text form, such\nas \"I am 80% confident that...\". Existing approaches either rely on prompt\nengineering or fine-tuning with heuristically generated uncertainty estimates,\nboth of which have limited effectiveness and generalizability. Motivated by the\nnotion of proper scoring rules for calibration in classical machine learning\nmodels, we introduce ConfTuner, a simple and efficient fine-tuning method that\nintroduces minimal overhead and does not require ground-truth confidence scores\nor proxy confidence estimates. ConfTuner relies on a new loss function,\ntokenized Brier score, which we theoretically prove to be a proper scoring\nrule, intuitively meaning that it \"correctly incentivizes the model to report\nits true probability of being correct\". ConfTuner improves calibration across\ndiverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our\nresults further show that better-calibrated confidence enables downstream gains\nin self-correction and model cascade, advancing the development of trustworthy\nLLM systems. The code is available at\nhttps://github.com/liushiliushi/ConfTuner."}
{"id": "2508.18870", "pdf": "https://arxiv.org/pdf/2508.18870.pdf", "abs": "https://arxiv.org/abs/2508.18870", "title": "ReflectivePrompt: Reflective evolution in autoprompting algorithms", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting."}
{"id": "2508.18872", "pdf": "https://arxiv.org/pdf/2508.18872.pdf", "abs": "https://arxiv.org/abs/2508.18872", "title": "Empowering Computing Education Researchers Through LLM-Assisted Content Analysis", "authors": ["Laurie Gale", "Sebastian Mateos Nicolajsen"], "categories": ["cs.CL"], "comment": "7 pages, 2 figures", "summary": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline."}
{"id": "2508.18916", "pdf": "https://arxiv.org/pdf/2508.18916.pdf", "abs": "https://arxiv.org/abs/2508.18916", "title": "Affective Polarization across European Parliaments", "authors": ["Bojan Evkoski", "Igor Mozetič", "Nikola Ljubešić", "Petra Kralj Novak"], "categories": ["cs.CL", "cs.SI"], "comment": "6 pages, 4 figures", "summary": "Affective polarization, characterized by increased negativity and hostility\ntowards opposing groups, has become a prominent feature of political discourse\nworldwide. Our study examines the presence of this type of polarization in a\nselection of European parliaments in a fully automated manner. Utilizing a\ncomprehensive corpus of parliamentary speeches from the parliaments of six\nEuropean countries, we employ natural language processing techniques to\nestimate parliamentarian sentiment. By comparing the levels of negativity\nconveyed in references to individuals from opposing groups versus one's own, we\ndiscover patterns of affectively polarized interactions. The findings\ndemonstrate the existence of consistent affective polarization across all six\nEuropean parliaments. Although activity correlates with negativity, there is no\nobserved difference in affective polarization between less active and more\nactive members of parliament. Finally, we show that reciprocity is a\ncontributing mechanism in affective polarization between parliamentarians\nacross all six parliaments."}
{"id": "2508.18929", "pdf": "https://arxiv.org/pdf/2508.18929.pdf", "abs": "https://arxiv.org/abs/2508.18929", "title": "Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework", "authors": ["Ilias Driouich", "Hongliu Cao", "Eoin Thomas"], "categories": ["cs.CL", "cs.AI"], "comment": "ECAI 2025 TRUST AI workshop", "summary": "Retrieval-augmented generation (RAG) systems improve large language model\noutputs by incorporating external knowledge, enabling more informed and\ncontext-aware responses. However, the effectiveness and trustworthiness of\nthese systems critically depends on how they are evaluated, particularly on\nwhether the evaluation process captures real-world constraints like protecting\nsensitive information. While current evaluation efforts for RAG systems have\nprimarily focused on the development of performance metrics, far less attention\nhas been given to the design and quality of the underlying evaluation datasets,\ndespite their pivotal role in enabling meaningful, reliable assessments. In\nthis work, we introduce a novel multi-agent framework for generating synthetic\nQA datasets for RAG evaluation that prioritize semantic diversity and privacy\npreservation. Our approach involves: (1) a Diversity agent leveraging\nclustering techniques to maximize topical coverage and semantic variability,\n(2) a Privacy Agent that detects and mask sensitive information across multiple\ndomains and (3) a QA curation agent that synthesizes private and diverse QA\npairs suitable as ground truth for RAG evaluation. Extensive experiments\ndemonstrate that our evaluation sets outperform baseline methods in diversity\nand achieve robust privacy masking on domain-specific datasets. This work\noffers a practical and ethically aligned pathway toward safer, more\ncomprehensive RAG system evaluation, laying the foundation for future\nenhancements aligned with evolving AI regulations and compliance standards."}
{"id": "2508.18988", "pdf": "https://arxiv.org/pdf/2508.18988.pdf", "abs": "https://arxiv.org/abs/2508.18988", "title": "Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models", "authors": ["Hung Ming Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 9 figures. The AI Intuition Explorer dashboard is available\n  at: https://cyrilliu1974.github.io/github.io/vi.html", "summary": "We present a framework where neural models develop an AI Mother Tongue, a\nnative symbolic language that simultaneously supports intuitive reasoning,\ncompositional symbol chains, and inherent interpretability. Unlike post-hoc\nexplanation methods, our approach embeds reasoning directly into the model's\nrepresentations: symbols capture meaningful semantic patterns, chains trace\ndecision paths, and gated induction mechanisms guide selective focus, yielding\ntransparent yet flexible reasoning. We introduce complementary training\nobjectives to enhance symbol purity and decision sparsity, and employ a\nsequential specialization strategy to first build broad symbolic competence and\nthen refine intuitive judgments. Experiments on AI tasks demonstrate\ncompetitive accuracy alongside verifiable reasoning traces, showing that AI\nMother Tongue can serve as a unified mechanism for interpretability, intuition,\nand symbolic reasoning in neural models."}
{"id": "2508.18992", "pdf": "https://arxiv.org/pdf/2508.18992.pdf", "abs": "https://arxiv.org/abs/2508.18992", "title": "Automatic Prompt Optimization with Prompt Distillation", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting."}
{"id": "2508.19026", "pdf": "https://arxiv.org/pdf/2508.19026.pdf", "abs": "https://arxiv.org/abs/2508.19026", "title": "MovieCORE: COgnitive REasoning in Movies", "authors": ["Gueter Josmy Faure", "Min-Hung Chen", "Jia-Fong Yeh", "Ying Cheng", "Hung-Ting Su", "Yung-Hao Tang", "Shang-Hong Lai", "Winston H. Hsu"], "categories": ["cs.CL"], "comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html", "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html."}
{"id": "2508.19076", "pdf": "https://arxiv.org/pdf/2508.19076.pdf", "abs": "https://arxiv.org/abs/2508.19076", "title": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance", "authors": ["Ziyue Li", "Yuan Chang", "Gaihong Yu", "Xiaoqiu Le"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components."}
{"id": "2508.19077", "pdf": "https://arxiv.org/pdf/2508.19077.pdf", "abs": "https://arxiv.org/abs/2508.19077", "title": "\"Where does it hurt?\" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues", "authors": ["Tom Röhr", "Soumyadeep Roy", "Fares Al Mohamad", "Jens-Michalis Papaioannou", "Wolfgang Nejdl", "Felix Gers", "Alexander Löser"], "categories": ["cs.CL"], "comment": "Accepted at ECAI 2025", "summary": "In a doctor-patient dialogue, the primary objective of physicians is to\ndiagnose patients and propose a treatment plan. Medical doctors guide these\nconversations through targeted questioning to efficiently gather the\ninformation required to provide the best possible outcomes for patients. To the\nbest of our knowledge, this is the first work that studies physician intent\ntrajectories in doctor-patient dialogues. We use the `Ambient Clinical\nIntelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with\nmedical professionals to develop a fine-grained taxonomy of physician intents\nbased on the SOAP framework (Subjective, Objective, Assessment, and Plan). We\nthen conduct a large-scale annotation effort to label over 5000 doctor-patient\nturns with the help of a large number of medical experts recruited using\nProlific, a popular crowd-sourcing platform. This large labeled dataset is an\nimportant resource contribution that we use for benchmarking the\nstate-of-the-art generative and encoder models for medical intent\nclassification tasks. Our findings show that our models understand the general\nstructure of medical dialogues with high accuracy, but often fail to identify\ntransitions between SOAP categories. We also report for the first time common\ntrajectories in medical dialogue structures that provide valuable insights for\ndesigning `differential diagnosis' systems. Finally, we extensively study the\nimpact of intent filtering for medical dialogue summarization and observe a\nsignificant boost in performance. We make the codes and data, including\nannotation guidelines, publicly available at\nhttps://github.com/DATEXIS/medical-intent-classification."}
{"id": "2508.19089", "pdf": "https://arxiv.org/pdf/2508.19089.pdf", "abs": "https://arxiv.org/abs/2508.19089", "title": "It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs", "authors": ["Yue Li", "Zhixue Zhao", "Carolina Scarton"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025", "summary": "Extremely low-resource languages, especially those written in rare scripts,\nas shown in Figure 1, remain largely unsupported by large language models\n(LLMs). This is due in part to compounding factors such as the lack of training\ndata. This paper delivers the first comprehensive analysis of whether LLMs can\nacquire such languages purely via in-context learning (ICL), with or without\nauxiliary alignment signals, and how these methods compare to\nparameter-efficient fine-tuning (PEFT). We systematically evaluate 20\nunder-represented languages across three state-of-the-art multilingual LLMs.\nOur findings highlight the limitation of PEFT when both language and its script\nare extremely under-represented by the LLM. In contrast, zero-shot ICL with\nlanguage alignment is impressively effective on extremely low-resource\nlanguages, while few-shot ICL or PEFT is more beneficial for languages\nrelatively better represented by LLMs. For LLM practitioners working on\nextremely low-resource languages, we summarise guidelines grounded by our\nresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning\na multilingual model on languages of unseen scripts."}
{"id": "2508.19093", "pdf": "https://arxiv.org/pdf/2508.19093.pdf", "abs": "https://arxiv.org/abs/2508.19093", "title": "Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index", "authors": ["Mathew Henrickson"], "categories": ["cs.CL"], "comment": null, "summary": "This research presents a Retrieval-Augmented Generation (RAG) framework for\nart provenance studies, focusing on the Getty Provenance Index. Provenance\nresearch establishes the ownership history of artworks, which is essential for\nverifying authenticity, supporting restitution and legal claims, and\nunderstanding the cultural and historical context of art objects. The process\nis complicated by fragmented, multilingual archival data that hinders efficient\nretrieval. Current search portals require precise metadata, limiting\nexploratory searches. Our method enables natural-language and multilingual\nsearches through semantic retrieval and contextual summarization, reducing\ndependence on metadata structures. We assess RAG's capability to retrieve and\nsummarize auction records using a 10,000-record sample from the Getty\nProvenance Index - German Sales. The results show this approach provides a\nscalable solution for navigating art market archives, offering a practical tool\nfor historians and cultural heritage professionals conducting historically\nsensitive research."}
{"id": "2508.19099", "pdf": "https://arxiv.org/pdf/2508.19099.pdf", "abs": "https://arxiv.org/abs/2508.19099", "title": "Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic", "authors": ["Thomas Compton"], "categories": ["cs.CL"], "comment": "5 pages conference paper, 4 tables", "summary": "Quantitative Discourse Analysis has seen growing adoption with the rise of\nLarge Language Models and computational tools. However, reliance on black box\nsoftware such as MAXQDA and NVivo risks undermining methodological transparency\nand alignment with research goals. This paper presents a hybrid, transparent\nframework for QDA that combines lexical and semantic methods to enable\ntriangulation, reproducibility, and interpretability. Drawing from a case study\nin historical political discourse, we demonstrate how custom Python pipelines\nusing NLTK, spaCy, and Sentence Transformers allow fine-grained control over\npreprocessing, lemmatisation, and embedding generation. We further detail our\niterative BERTopic modelling process, incorporating UMAP dimensionality\nreduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised\nthrough parameter tuning and multiple runs to enhance topic coherence and\ncoverage. By juxtaposing precise lexical searches with context-aware semantic\nclustering, we argue for a multi-layered approach that mitigates the\nlimitations of either method in isolation. Our workflow underscores the\nimportance of code-level transparency, researcher agency, and methodological\ntriangulation in computational discourse studies. Code and supplementary\nmaterials are available via GitHub."}
{"id": "2508.19111", "pdf": "https://arxiv.org/pdf/2508.19111.pdf", "abs": "https://arxiv.org/abs/2508.19111", "title": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs", "authors": ["Zhikai Ding", "Shiyu Ni", "Keping Bi"], "categories": ["cs.CL"], "comment": "EMNLP2025 Findings", "summary": "Large vision-language models (LVLMs) demonstrate strong visual question\nanswering (VQA) capabilities but are shown to hallucinate. A reliable model\nshould perceive its knowledge boundaries-knowing what it knows and what it does\nnot. This paper investigates LVLMs' perception of their knowledge boundaries by\nevaluating three types of confidence signals: probabilistic confidence, answer\nconsistency-based confidence, and verbalized confidence. Experiments on three\nLVLMs across three VQA datasets show that, although LVLMs possess a reasonable\nperception level, there is substantial room for improvement. Among the three\nconfidences, probabilistic and consistency-based signals are more reliable\nindicators, while verbalized confidence often leads to overconfidence. To\nenhance LVLMs' perception, we adapt several established confidence calibration\nmethods from Large Language Models (LLMs) and propose three effective methods.\nAdditionally, we compare LVLMs with their LLM counterparts, finding that\njointly processing visual and textual inputs decreases question-answering\nperformance but reduces confidence, resulting in an improved perception level\ncompared to LLMs."}
{"id": "2508.19202", "pdf": "https://arxiv.org/pdf/2508.19202.pdf", "abs": "https://arxiv.org/abs/2508.19202", "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning", "authors": ["Alan Li", "Yixin Liu", "Arpan Sarkar", "Doug Downey", "Arman Cohan"], "categories": ["cs.CL"], "comment": "28 pages, 16 figures", "summary": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning."}
{"id": "2508.19205", "pdf": "https://arxiv.org/pdf/2508.19205.pdf", "abs": "https://arxiv.org/abs/2508.19205", "title": "VibeVoice Technical Report", "authors": ["Zhiliang Peng", "Jianwei Yu", "Wenhui Wang", "Yaoyao Chang", "Yutao Sun", "Li Dong", "Yi Zhu", "Weijiang Xu", "Hangbo Bao", "Zehua Wang", "Shaohan Huang", "Yan Xia", "Furu Wei"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "This report presents VibeVoice, a novel model designed to synthesize\nlong-form speech with multiple speakers by employing next-token diffusion,\nwhich is a unified method for modeling continuous data by autoregressively\ngenerating latent vectors via diffusion. To enable this, we introduce a novel\ncontinuous speech tokenizer that, when compared to the popular Encodec model,\nimproves data compression by 80 times while maintaining comparable performance.\nThe tokenizer effectively preserves audio fidelity while significantly boosting\ncomputational efficiency for processing long sequences. Thus, VibeVoice can\nsynthesize long-form speech for up to 90 minutes (in a 64K context window\nlength) with a maximum of 4 speakers, capturing the authentic conversational\n``vibe'' and surpassing open-source and proprietary dialogue models."}
{"id": "2508.19221", "pdf": "https://arxiv.org/pdf/2508.19221.pdf", "abs": "https://arxiv.org/abs/2508.19221", "title": "Evaluating the Evaluators: Are readability metrics good measures of readability?", "authors": ["Isabel Cachola", "Daniel Khashabi", "Mark Dredze"], "categories": ["cs.CL"], "comment": null, "summary": "Plain Language Summarization (PLS) aims to distill complex documents into\naccessible summaries for non-expert audiences. In this paper, we conduct a\nthorough survey of PLS literature, and identify that the current standard\npractice for readability evaluation is to use traditional readability metrics,\nsuch as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in\nother fields, these metrics have not been compared to human readability\njudgments in PLS. We evaluate 8 readability metrics and show that most\ncorrelate poorly with human judgments, including the most popular metric, FKGL.\nWe then show that Language Models (LMs) are better judges of readability, with\nthe best-performing model achieving a Pearson correlation of 0.56 with human\njudgments. Extending our analysis to PLS datasets, which contain summaries\naimed at non-expert audiences, we find that LMs better capture deeper measures\nof readability, such as required background knowledge, and lead to different\nconclusions than the traditional metrics. Based on these findings, we offer\nrecommendations for best practices in the evaluation of plain language\nsummaries. We release our analysis code and survey data."}
{"id": "2508.19227", "pdf": "https://arxiv.org/pdf/2508.19227.pdf", "abs": "https://arxiv.org/abs/2508.19227", "title": "Generative Interfaces for Language Models", "authors": ["Jiaqi Chen", "Yanzhe Zhang", "Yutong Zhang", "Yijia Shao", "Diyi Yang"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction."}
{"id": "2508.18288", "pdf": "https://arxiv.org/pdf/2508.18288.pdf", "abs": "https://arxiv.org/abs/2508.18288", "title": "Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology", "authors": ["Jay L. Cunningham", "Adinawa Adjagbodjou", "Jeffrey Basoah", "Jainaba Jawara", "Kowe Kadoma", "Aaleyah Lewis"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "10 pages, 9 Pages (References and Appendices). The archival version\n  has been accepted to AAAI (AIES 2025) without the extended Appendices. This\n  extended version includes Appendices", "summary": "This scoping literature review examines how fairness, bias, and equity are\nconceptualized and operationalized in Automatic Speech Recognition (ASR) and\nadjacent speech and language technologies (SLT) for African American English\n(AAE) speakers and other linguistically diverse communities. Drawing from 44\npeer-reviewed publications across Human-Computer Interaction (HCI), Machine\nLearning/Natural Language Processing (ML/NLP), and Sociolinguistics, we\nidentify four major areas of inquiry: (1) how researchers understand\nASR-related harms; (2) inclusive data practices spanning collection, curation,\nannotation, and model training; (3) methodological and theoretical approaches\nto linguistic inclusion; and (4) emerging practices and design recommendations\nfor more equitable systems. While technical fairness interventions are growing,\nour review highlights a critical gap in governance-centered approaches that\nforeground community agency, linguistic justice, and participatory\naccountability. We propose a governance-centered ASR lifecycle as an emergent\ninterdisciplinary framework for responsible ASR development and offer\nimplications for researchers, practitioners, and policymakers seeking to\naddress language marginalization in speech AI systems."}
{"id": "2508.18295", "pdf": "https://arxiv.org/pdf/2508.18295.pdf", "abs": "https://arxiv.org/abs/2508.18295", "title": "H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems", "authors": ["Huangyu Dai", "Lingtao Mao", "Ben Chen", "Zihan Wang", "Zihan Liang", "Ying Han", "Chenyi Lei", "Han Li"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Hotword customization is crucial in ASR to enhance the accuracy of\ndomain-specific terms. It has been primarily driven by the advancements in\ntraditional models and Audio large language models (LLMs). However, existing\nmodels often struggle with large-scale hotwords, as the recognition rate drops\ndramatically with the number of hotwords increasing. In this paper, we\nintroduce a novel hotword customization system that utilizes a hotword\npre-retrieval module (H-PRM) to identify the most relevant hotword candidate by\nmeasuring the acoustic similarity between the hotwords and the speech segment.\nThis plug-and-play solution can be easily integrated into traditional models\nsuch as SeACo-Paraformer, significantly enhancing hotwords post-recall rate\n(PRR). Additionally, we incorporate H-PRM into Audio LLMs through a\nprompt-based approach, enabling seamless customization of hotwords. Extensive\ntesting validates that H-PRM can outperform existing methods, showing a new\ndirection for hotword customization in ASR."}
{"id": "2508.18297", "pdf": "https://arxiv.org/pdf/2508.18297.pdf", "abs": "https://arxiv.org/abs/2508.18297", "title": "Can VLMs Recall Factual Associations From Visual References?", "authors": ["Dhananjay Ashok", "Ashutosh Chaubey", "Hirona J. Arai", "Jonathan May", "Jesse Thomason"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "To appear at EMNLP 2025 (Findings)", "summary": "Through a controlled study, we identify a systematic deficiency in the\nmultimodal grounding of Vision Language Models (VLMs). While VLMs can recall\nfactual associations when provided a textual reference to an entity; their\nability to do so is significantly diminished when the reference is visual\ninstead. Forcing VLMs to rely on image representations of an entity halves\ntheir ability to recall factual knowledge, suggesting that VLMs struggle to\nlink their internal knowledge of an entity with its image representation. We\nshow that such linking failures are correlated with the expression of distinct\npatterns in model internal states, and that probes on these internal states\nachieve over 92% accuracy at flagging cases where the VLM response is\nunreliable. These probes can be applied, without retraining, to identify when a\nVLM will fail to correctly answer a question that requires an understanding of\nmultimodal input. When used to facilitate selective prediction on a visual\nquestion answering task, the probes increase coverage by 7.87% (absolute) while\nalso reducing the risk of error by 0.9% (absolute). Addressing the systematic,\ndetectable deficiency is an important avenue in language grounding, and we\nprovide informed recommendations for future directions."}
{"id": "2508.18306", "pdf": "https://arxiv.org/pdf/2508.18306.pdf", "abs": "https://arxiv.org/abs/2508.18306", "title": "SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds", "authors": ["Wuxinlin Cheng", "Yupeng Cao", "Jinwen Wu", "Koduvayur Subbalakshmi", "Tian Han", "Zhuo Feng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent strides in pretrained transformer-based language models have propelled\nstate-of-the-art performance in numerous NLP tasks. Yet, as these models grow\nin size and deployment, their robustness under input perturbations becomes an\nincreasingly urgent question. Existing robustness methods often diverge between\nsmall-parameter and large-scale models (LLMs), and they typically rely on\nlabor-intensive, sample-specific adversarial designs. In this paper, we propose\na unified, local (sample-level) robustness framework (SALMAN) that evaluates\nmodel stability without modifying internal parameters or resorting to complex\nperturbation heuristics. Central to our approach is a novel Distance Mapping\nDistortion (DMD) measure, which ranks each sample's susceptibility by comparing\ninput-to-output distance mappings in a near-linear complexity manner. By\ndemonstrating significant gains in attack efficiency and robust training, we\nposition our framework as a practical, model-agnostic tool for advancing the\nreliability of transformer-based NLP systems."}
{"id": "2508.18370", "pdf": "https://arxiv.org/pdf/2508.18370.pdf", "abs": "https://arxiv.org/abs/2508.18370", "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "categories": ["cs.SE", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional capabilities when\ntrained within executable runtime environments, notably excelling at software\nengineering tasks through verified feedback loops. Yet, scalable and\ngeneralizable execution-grounded environments remain scarce, limiting progress\nin training more capable ML agents. We introduce CTF-Dojo, the first\nlarge-scale executable runtime tailored for training LLMs with verifiable\nfeedback, featuring 658 fully functional Capture-The-Flag (CTF)-style\nchallenges containerized in Docker with guaranteed reproducibility. To enable\nrapid scaling without manual intervention, we develop CTF-Forge, an automated\npipeline that transforms publicly available artifacts into ready-to-use\nexecution environments in minutes, eliminating weeks of expert configuration\ntraditionally required. We trained LLM-based agents on just 486 high-quality,\nexecution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute\ngains over strong baselines across three competitive benchmarks: InterCode-CTF,\nNYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,\nestablishing a new open-weight state-of-the-art that rivals frontier models\nlike DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a\nbenchmark for executable-agent learning, CTF-Dojo demonstrates that\nexecution-grounded training signals are not only effective but pivotal in\nadvancing high-performance ML agents without dependence on costly proprietary\nsystems."}
{"id": "2508.18439", "pdf": "https://arxiv.org/pdf/2508.18439.pdf", "abs": "https://arxiv.org/abs/2508.18439", "title": "A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs", "authors": ["Anders Mølmen Høst", "Pierre Lison", "Leon Moonen"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.SE"], "comment": null, "summary": "Vulnerability databases, such as the National Vulnerability Database (NVD),\noffer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but\noften lack information on their real-world impact, such as the tactics,\ntechniques, and procedures (TTPs) that adversaries may use to exploit the\nvulnerability. However, manually linking CVEs to their corresponding TTPs is a\nchallenging and time-consuming task, and the high volume of new vulnerabilities\npublished annually makes automated support desirable.\n  This paper introduces TRIAGE, a two-pronged automated approach that uses\nLarge Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK\nknowledge base. We first prompt an LLM with instructions based on MITRE's CVE\nMapping Methodology to predict an initial list of techniques. This list is then\ncombined with the results from a second LLM-based module that uses in-context\nlearning to map a CVE to relevant techniques. This hybrid approach\nstrategically combines rule-based reasoning with data-driven inference. Our\nevaluation reveals that in-context learning outperforms the individual mapping\nmethods, and the hybrid approach improves recall of exploitation techniques. We\nalso find that GPT-4o-mini performs better than Llama3.3-70B on this task.\nOverall, our results show that LLMs can be used to automatically predict the\nimpact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping\nCVEs to ATT&CK more efficient.\n  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language\nmodels, automated mapping."}
{"id": "2508.18512", "pdf": "https://arxiv.org/pdf/2508.18512.pdf", "abs": "https://arxiv.org/abs/2508.18512", "title": "Designing across domains with declarative thinking: Insights from the 96-Eyes ptychographic imager project", "authors": ["Antony C Chan"], "categories": ["physics.optics", "cs.CL"], "comment": null, "summary": "This article presents a practitioner's reflection on applying declarative,\n5th generation, problem formulation language (5GL) to de novo imaging system\ndesign, informed by experiences across the interdisciplinary research in\nacademia and cross-functional product development within the private sector.\nUsing the 96-Eyes project: 96-camera parallel multi-modal imager for\nhigh-throughput drug discovery as a representative case, I illustrate how\nproject requirements, ranging from hardware constraints to life sciences needs,\ncan be formalized into machine-readable problem statements to preserve\nmission-critical input from diverse domain stakeholders. This declarative\napproach enhances transparency, ensures design traceability, and minimizes\ncostly misalignment across optical, algorithmic, hardware-accelerated compute,\nand life sciences teams.\n  Alongside the technical discussion of 5GL with real-world code examples, I\nreflect on the practical barriers to adopting 5GL in environments where\nimperative, 3rd-generation languages (3GL) remain the default medium for\ninter-team collaboration. Rather than offering an one-size-fits-all solution,\nthese learned lessons highlight how programming paradigms implicitly shapes\nresearch workflows through existing domain hierarchies. The discussion aims to\ninvite further explorations into how declarative problem formulations can\nfacilitate innovation in settings where concurrent R\\&{}D workflows are gaining\ntraction, as opposed to environments where sequential, phase-driven workflows\nremain the norm."}
{"id": "2508.18642", "pdf": "https://arxiv.org/pdf/2508.18642.pdf", "abs": "https://arxiv.org/abs/2508.18642", "title": "RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing", "authors": ["Jianxing Liao", "Tian Zhang", "Xiao Feng", "Yusong Zhang", "Rui Yang", "Haorui Wang", "Bosi Wen", "Ziying Wang", "Runzhi Shi"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models are extensively utilized in creative writing\napplications. Creative writing requires a balance between subjective writing\nquality (e.g., literariness and emotional expression) and objective constraint\nfollowing (e.g., format requirements and word limits). Existing reinforcement\nlearning methods struggle to balance these two aspects: single reward\nstrategies fail to improve both abilities simultaneously, while fixed-weight\nmixed-reward methods lack the ability to adapt to different writing scenarios.\nTo address this problem, we propose Reinforcement Learning with Mixed Rewards\n(RLMR), utilizing a dynamically mixed reward system from a writing reward model\nevaluating subjective writing quality and a constraint verification model\nassessing objective constraint following. The constraint following reward\nweight is adjusted dynamically according to the writing quality within sampled\ngroups, ensuring that samples violating constraints get negative advantage in\nGRPO and thus penalized during training, which is the key innovation of this\nproposed method. We conduct automated and manual evaluations across diverse\nmodel families from 8B to 72B parameters. Additionally, we construct a\nreal-world writing benchmark named WriteEval for comprehensive evaluation.\nResults illustrate that our method achieves consistent improvements in both\ninstruction following (IFEval from 83.36\\% to 86.65\\%) and writing quality\n(72.75\\% win rate in manual expert pairwise evaluations on WriteEval). To the\nbest of our knowledge, RLMR is the first work to combine subjective preferences\nwith objective verification in online RL training, providing an effective\nsolution for multi-dimensional creative writing optimization."}
{"id": "2508.18646", "pdf": "https://arxiv.org/pdf/2508.18646.pdf", "abs": "https://arxiv.org/abs/2508.18646", "title": "Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap", "authors": ["Jun Wang", "Ninglun Gu", "Kailai Zhang", "Zijiao Zhang", "Yelun Bao", "Jin Yang", "Xu Yin", "Liwei Liu", "Yihuan Liu", "Pengyong Li", "Gary G. Yen", "Junchi Yan"], "categories": ["cs.AI", "cs.CL"], "comment": "Preprint. Under review", "summary": "For Large Language Models (LLMs), a disconnect persists between benchmark\nperformance and real-world utility. Current evaluation frameworks remain\nfragmented, prioritizing technical metrics while neglecting holistic assessment\nfor deployment. This survey introduces an anthropomorphic evaluation paradigm\nthrough the lens of human intelligence, proposing a novel three-dimensional\ntaxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational\ncapacity, Emotional Quotient (EQ)-Alignment Ability for value-based\ninteractions, and Professional Quotient (PQ)-Professional Expertise for\nspecialized proficiency. For practical value, we pioneer a Value-oriented\nEvaluation (VQ) framework assessing economic viability, social impact, ethical\nalignment, and environmental sustainability. Our modular architecture\nintegrates six components with an implementation roadmap. Through analysis of\n200+ benchmarks, we identify key challenges including dynamic assessment needs\nand interpretability gaps. It provides actionable guidance for developing LLMs\nthat are technically proficient, contextually relevant, and ethically sound. We\nmaintain a curated repository of open-source evaluation resources at:\nhttps://github.com/onejune2018/Awesome-LLM-Eval."}
{"id": "2508.18652", "pdf": "https://arxiv.org/pdf/2508.18652.pdf", "abs": "https://arxiv.org/abs/2508.18652", "title": "UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation", "authors": ["Runpeng Geng", "Yanting Wang", "Ying Chen", "Jinyuan Jia"], "categories": ["cs.CR", "cs.CL", "I.2.7"], "comment": "21 pages, 4 figures", "summary": "Retrieval-augmented generation (RAG) systems are widely deployed in\nreal-world applications in diverse domains such as finance, healthcare, and\ncybersecurity. However, many studies showed that they are vulnerable to\nknowledge corruption attacks, where an attacker can inject adversarial texts\ninto the knowledge database of a RAG system to induce the LLM to generate\nattacker-desired outputs. Existing studies mainly focus on attacking specific\nqueries or queries with similar topics (or keywords). In this work, we propose\nUniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike\nprior work, UniC-RAG jointly optimizes a small number of adversarial texts that\ncan simultaneously attack a large number of user queries with diverse topics\nand domains, enabling an attacker to achieve various malicious objectives, such\nas directing users to malicious websites, triggering harmful command execution,\nor launching denial-of-service attacks. We formulate UniC-RAG as an\noptimization problem and further design an effective solution to solve it,\nincluding a balanced similarity-based clustering method to enhance the attack's\neffectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly\neffective and significantly outperforms baselines. For instance, UniC-RAG could\nachieve over 90% attack success rate by injecting 100 adversarial texts into a\nknowledge database with millions of texts to simultaneously attack a large set\nof user queries (e.g., 2,000). Additionally, we evaluate existing defenses and\nshow that they are insufficient to defend against UniC-RAG, highlighting the\nneed for new defense mechanisms in RAG systems."}
{"id": "2508.18665", "pdf": "https://arxiv.org/pdf/2508.18665.pdf", "abs": "https://arxiv.org/abs/2508.18665", "title": "Membership Inference Attacks on LLM-based Recommender Systems", "authors": ["Jiajie He", "Yuechun Gu", "Min-Chun Chen", "Keke Chen"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) based Recommender Systems (RecSys) can flexibly\nadapt recommendation systems to different domains. It utilizes in-context\nlearning (ICL), i.e., the prompts, to customize the recommendation functions,\nwhich include sensitive historical user-specific item interactions, e.g.,\nimplicit feedback like clicked items or explicit product reviews. Such private\ninformation may be exposed to novel privacy attack. However, no study has been\ndone on this important issue. We design four membership inference attacks\n(MIAs), aiming to reveal whether victims' historical interactions have been\nused by system prompts. They are \\emph{direct inquiry, hallucination,\nsimilarity, and poisoning attacks}, each of which utilizes the unique features\nof LLMs or RecSys. We have carefully evaluated them on three LLMs that have\nbeen used to develop ICL-LLM RecSys and two well-known RecSys benchmark\ndatasets. The results confirm that the MIA threat on LLM RecSys is realistic:\ndirect inquiry and poisoning attacks showing significantly high attack\nadvantages. We have also analyzed the factors affecting these attacks, such as\nthe number of shots in system prompts and the position of the victim in the\nshots."}
{"id": "2508.18672", "pdf": "https://arxiv.org/pdf/2508.18672.pdf", "abs": "https://arxiv.org/abs/2508.18672", "title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks", "authors": ["Taishi Nakamura", "Satoki Ishikawa", "Masaki Kawamura", "Takumi Okamoto", "Daisuke Nohara", "Jun Suzuki", "Rio Yokota"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Presented at the Second AI for Math Workshop at ICML", "summary": "Empirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes. Mixture-of-Experts (MoE) models, now standard in\nstate-of-the-art systems, introduce a new sparsity dimension that current\ndense-model frontiers overlook. We investigate how MoE sparsity influences two\ndistinct capability regimes: memorization and reasoning. We train families of\nMoE Transformers that systematically vary total parameters, active parameters,\nand top-$k$ routing while holding the compute budget fixed. For every model we\nrecord pre-training loss, downstream task loss, and task accuracy, allowing us\nto separate the train-test generalization gap from the loss-accuracy gap.\nMemorization benchmarks improve monotonically with total parameters, mirroring\ntraining loss. By contrast, reasoning performance saturates and can even\nregress despite continued gains in both total parameters and training loss.\nAltering top-$k$ alone has little effect when active parameters are constant,\nand classic hyperparameters such as learning rate and initialization modulate\nthe generalization gap in the same direction as sparsity. Neither post-training\nreinforcement learning (GRPO) nor extra test-time compute rescues the reasoning\ndeficit of overly sparse models. Our model checkpoints, code and logs are\nopen-source at https://github.com/rioyokotalab/optimal-sparsity."}
{"id": "2508.18684", "pdf": "https://arxiv.org/pdf/2508.18684.pdf", "abs": "https://arxiv.org/abs/2508.18684", "title": "FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation", "authors": ["Shaswata Mitra", "Azim Bazarov", "Martin Duclos", "Sudip Mittal", "Aritran Piplai", "Md Rayhanur Rahman", "Edward Zieglar", "Shahram Rahimi"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "comment": "11 pages, 5 figures, 4 tables", "summary": "Signature-based Intrusion Detection Systems (IDS) detect malicious activities\nby matching network or host activity against predefined rules. These rules are\nderived from extensive Cyber Threat Intelligence (CTI), which includes attack\nsignatures and behavioral patterns obtained through automated tools and manual\nthreat analysis, such as sandboxing. The CTI is then transformed into\nactionable rules for the IDS engine, enabling real-time detection and\nprevention. However, the constant evolution of cyber threats necessitates\nfrequent rule updates, which delay deployment time and weaken overall security\nreadiness. Recent advancements in agentic systems powered by Large Language\nModels (LLMs) offer the potential for autonomous IDS rule generation with\ninternal evaluation. We introduce FALCON, an autonomous agentic framework that\ngenerates deployable IDS rules from CTI data in real-time and evaluates them\nusing built-in multi-phased validators. To demonstrate versatility, we target\nboth network (Snort) and host-based (YARA) mediums and construct a\ncomprehensive dataset of IDS rules with their corresponding CTIs. Our\nevaluations indicate FALCON excels in automatic rule generation, with an\naverage of 95% accuracy validated by qualitative evaluation with 84%\ninter-rater agreement among multiple cybersecurity analysts across all metrics.\nThese results underscore the feasibility and effectiveness of LLM-driven data\nmining for real-time cyber threat mitigation."}
{"id": "2508.18724", "pdf": "https://arxiv.org/pdf/2508.18724.pdf", "abs": "https://arxiv.org/abs/2508.18724", "title": "Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval", "authors": ["Karanbir Singh", "Deepak Muppiri", "William Ngu"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at KDD'2025 Agent4IR workshop", "summary": "Large Language Models (LLMs) have transformed the field of artificial\nintelligence by unlocking the era of generative applications. Built on top of\ngenerative AI capabilities, Agentic AI represents a major shift toward\nautonomous, goal-driven systems that can reason, retrieve, and act. However,\nthey also inherit the bias present in both internal and external information\nsources. This significantly affects the fairness and balance of retrieved\ninformation, and hence reduces user trust. To address this critical challenge,\nwe introduce a novel Bias Mitigation Agent, a multi-agent system designed to\norchestrate the workflow of bias mitigation through specialized agents that\noptimize the selection of sources to ensure that the retrieved content is both\nhighly relevant and minimally biased to promote fair and balanced knowledge\ndissemination. The experimental results demonstrate an 81.82\\% reduction in\nbias compared to a baseline naive retrieval strategy."}
{"id": "2508.18743", "pdf": "https://arxiv.org/pdf/2508.18743.pdf", "abs": "https://arxiv.org/abs/2508.18743", "title": "CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks", "authors": ["Sunguk Choi", "Yonghoon Kwon", "Heondeuk Lee"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at EMNLP 2025 findings", "summary": "Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)\nsolve difficult problems, but very long traces often slow or even degrade\nperformance on fast, intuitive \"System-1\" tasks. We introduce Connector-Aware\nCompact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a\nsmall, fixed set of connector phrases, steering the model toward concise and\nwell -- structured explanations. Despite its simplicity, our synthetic method\nwith Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves\napproximately 85% on GSM8K and approximately 40% on GPQA (System-2) while\nretaining approximately 90% on S1-Bench (System-1). Its reasoning traces\naverage approximately 300 tokens(ART), about one-third the length of baseline\ntraces, delivering higher efficiency without loss of accuracy."}
{"id": "2508.18758", "pdf": "https://arxiv.org/pdf/2508.18758.pdf", "abs": "https://arxiv.org/abs/2508.18758", "title": "Text to Query Plans for Question Answering on Large Tables", "authors": ["Yipeng Zhang", "Chen Wang", "Yuzhe Zhang", "Jacky Jiang"], "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "Efficient querying and analysis of large tabular datasets remain significant\nchallenges, especially for users without expertise in programming languages\nlike SQL. Text-to-SQL approaches have shown promising performance on benchmark\ndata; however, they inherit SQL's drawbacks, including inefficiency with large\ndatasets and limited support for complex data analyses beyond basic querying.\nWe propose a novel framework that transforms natural language queries into\nquery plans. Our solution is implemented outside traditional databases,\nallowing us to support classical SQL commands while avoiding SQL's inherent\nlimitations. Additionally, we enable complex analytical functions, such as\nprincipal component analysis and anomaly detection, providing greater\nflexibility and extensibility than traditional SQL capabilities. We leverage\nLLMs to iteratively interpret queries and construct operation sequences,\naddressing computational complexity by incrementally building solutions. By\nexecuting operations directly on the data, we overcome context length\nlimitations without requiring the entire dataset to be processed by the model.\nWe validate our framework through experiments on both standard databases and\nlarge scientific tables, demonstrating its effectiveness in handling extensive\ndatasets and performing sophisticated data analyses."}
{"id": "2508.18760", "pdf": "https://arxiv.org/pdf/2508.18760.pdf", "abs": "https://arxiv.org/abs/2508.18760", "title": "Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models", "authors": ["Yi Liu", "Xiangyu Liu", "Zequn Sun", "Wei Hu"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have shown remarkable progress on complex\nreasoning tasks. However, some questions posed to LRMs are inherently\nunanswerable, such as math problems lacking sufficient conditions. We find that\nLRMs continually fail to provide appropriate abstentions when confronted with\nthese unanswerable questions. In this paper, we systematically analyze,\ninvestigate, and resolve this issue for trustworthy AI. We first conduct a\ndetailed analysis of the distinct response behaviors of LRMs when facing\nunanswerable questions. Then, we show that LRMs possess sufficient cognitive\ncapabilities to recognize the flaws in these questions. However, they fail to\nexhibit appropriate abstention behavior, revealing a misalignment between their\ninternal cognition and external response. Finally, to resolve this issue, we\npropose a lightweight, two-stage method that combines cognitive monitoring with\ninference-time intervention. Experimental results demonstrate that our method\nsignificantly improves the abstention rate while maintaining the overall\nreasoning performance."}
{"id": "2508.18772", "pdf": "https://arxiv.org/pdf/2508.18772.pdf", "abs": "https://arxiv.org/abs/2508.18772", "title": "Beyond the Textual: Generating Coherent Visual Options for MCQs", "authors": ["Wanqiang Wang", "Longzhu He", "Wei Zheng"], "categories": ["cs.CV", "cs.CL"], "comment": "EMNLP 2025", "summary": "Multiple-choice questions (MCQs) play a crucial role in fostering deep\nthinking and knowledge integration in education. However, previous research has\nprimarily focused on generating MCQs with textual options, but it largely\noverlooks the visual options. Moreover, generating high-quality distractors\nremains a major challenge due to the high cost and limited scalability of\nmanual authoring. To tackle these problems, we propose a Cross-modal Options\nSynthesis (CmOS), a novel framework for generating educational MCQs with visual\noptions. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning\nprocess and Retrieval-Augmented Generation (RAG) to produce semantically\nplausible and visually similar answer and distractors. It also includes a\ndiscrimination module to identify content suitable for visual options.\nExperimental results on test tasks demonstrate the superiority of CmOS in\ncontent discrimination, question generation and visual option generation over\nexisting methods across various subjects and educational levels."}
{"id": "2508.18976", "pdf": "https://arxiv.org/pdf/2508.18976.pdf", "abs": "https://arxiv.org/abs/2508.18976", "title": "The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization", "authors": ["Stephen Meisenbacher", "Alexandra Klymenko", "Andreea-Elena Bodea", "Florian Matthes"], "categories": ["cs.CR", "cs.CL"], "comment": "15 pages, 4 figures, 8 tables. Accepted to WPES @ CCS 2025", "summary": "Differentially private text sanitization refers to the process of privatizing\ntexts under the framework of Differential Privacy (DP), providing provable\nprivacy guarantees while also empirically defending against adversaries seeking\nto harm privacy. Despite their simplicity, DP text sanitization methods\noperating at the word level exhibit a number of shortcomings, among them the\ntendency to leave contextual clues from the original texts due to randomization\nduring sanitization $\\unicode{x2013}$ this we refer to as $\\textit{contextual\nvulnerability}$. Given the powerful contextual understanding and inference\ncapabilities of Large Language Models (LLMs), we explore to what extent LLMs\ncan be leveraged to exploit the contextual vulnerability of DP-sanitized texts.\nWe expand on previous work not only in the use of advanced LLMs, but also in\ntesting a broader range of sanitization mechanisms at various privacy levels.\nOur experiments uncover a double-edged sword effect of LLM-based data\nreconstruction attacks on privacy and utility: while LLMs can indeed infer\noriginal semantics and sometimes degrade empirical privacy protections, they\ncan also be used for good, to improve the quality and privacy of DP-sanitized\ntexts. Based on our findings, we propose recommendations for using LLM data\nreconstruction as a post-processing step, serving to increase privacy\nprotection by thinking adversarially."}
{"id": "2508.19005", "pdf": "https://arxiv.org/pdf/2508.19005.pdf", "abs": "https://arxiv.org/abs/2508.19005", "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark", "authors": ["Yuxuan Cai", "Yipeng Hao", "Jie Zhou", "Hang Yan", "Zhikai Lei", "Rui Zhen", "Zhenhua Han", "Yutao Yang", "Junsong Li", "Qianjun Pan", "Tianyu Huai", "Qin Chen", "Xin Li", "Kai Chen", "Bo Zhang", "Xipeng Qiu", "Liang He"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm shifts: From Passive to Proactive, From\nContext to Memory, and From Imitation to Learning. In this dynamic environment,\nagents must acquire and distill practical skills and maintain persistent memory\nto make decisions based on evolving state variables. StuLife provides a\ncomprehensive platform for evaluating lifelong learning capabilities, including\nmemory retention, skill transfer, and self-motivated behavior. Beyond\nevaluating SOTA LLMs on the StuLife benchmark, we also explore the role of\ncontext engineering in advancing AGI."}
{"id": "2508.19200", "pdf": "https://arxiv.org/pdf/2508.19200.pdf", "abs": "https://arxiv.org/abs/2508.19200", "title": "The Ramon Llull's Thinking Machine for Automated Ideation", "authors": ["Xinran Zhao", "Boyuan Zheng", "Chenglei Si", "Haofei Yu", "Ken Liu", "Runlong Zhou", "Ruochen Li", "Tong Chen", "Xiang Li", "Yiming Zhang", "Tongshuang Wu"], "categories": ["cs.AI", "cs.CL"], "comment": "21 pages, 3 figures", "summary": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI."}
{"id": "2508.19229", "pdf": "https://arxiv.org/pdf/2508.19229.pdf", "abs": "https://arxiv.org/abs/2508.19229", "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning", "authors": ["Wei Xiong", "Wenting Zhao", "Weizhe Yuan", "Olga Golovneva", "Tong Zhang", "Jason Weston", "Sainbayar Sukhbaatar"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search."}
{"id": "2402.05123", "pdf": "https://arxiv.org/pdf/2402.05123.pdf", "abs": "https://arxiv.org/abs/2402.05123", "title": "A Survey on Data Selection for LLM Instruction Tuning", "authors": ["Bolin Zhang", "Jiahao Wang", "Qianlong Du", "Jiajun Zhang", "Zhiying Tu", "Dianhui Chu"], "categories": ["cs.CL"], "comment": "Published in JAIR (Vol. 83, Article 32, 2025)", "summary": "Instruction tuning is a vital step of training large language models (LLMs),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLMs. Therefore,\nrecently a lot of studies focus on exploring the methods of selecting\nhigh-quality subset from instruction datasets, aiming to reduce training costs\nand enhance the instruction-following capabilities of LLMs. This paper presents\na comprehensive survey on data selection for LLM instruction tuning. Firstly,\nwe introduce the wildly used instruction datasets. Then, we propose a new\ntaxonomy of the data selection methods and provide a detailed introduction of\nrecent advances, and the evaluation strategies and results of data selection\nmethods are also elaborated in detail. Finally, we emphasize the open\nchallenges and present new frontiers of this task."}
{"id": "2406.04876", "pdf": "https://arxiv.org/pdf/2406.04876.pdf", "abs": "https://arxiv.org/abs/2406.04876", "title": "HateDebias: On the Diversity and Variability of Hate Speech Debiasing", "authors": ["Hongyan Wu", "Zhengming Chen", "Zijian Li", "Nankai Lin", "Lianxi Wang", "Shengyi Jiang", "Aimin Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Hate speech frequently appears on social media platforms and urgently needs\nto be effectively controlled. Alleviating the bias caused by hate speech can\nhelp resolve various ethical issues. Although existing research has constructed\nseveral datasets for hate speech detection, these datasets seldom consider the\ndiversity and variability of bias, making them far from real-world scenarios.\nTo fill this gap, we propose a benchmark HateDebias to analyze the fairness of\nmodels under dynamically evolving environments. Specifically, to meet the\ndiversity of biases, we collect hate speech data with different types of biases\nfrom real-world scenarios. To further simulate the variability in the\nreal-world scenarios(i.e., the changing of bias attributes in datasets), we\nconstruct a dataset to follow the continuous learning setting and evaluate the\ndetection accuracy of models on the HateDebias, where performance degradation\nindicates a significant bias toward a specific attribute. To provide a\npotential direction, we further propose a continual debiasing framework\ntailored to dynamic bias in real-world scenarios, integrating memory replay and\nbias information regularization to ensure the fairness of the model. Experiment\nresults on the HateDebias benchmark reveal that our methods achieve improved\nperformance in mitigating dynamic biases in real-world scenarios, highlighting\nthe practicality in real-world applications."}
{"id": "2406.12719", "pdf": "https://arxiv.org/pdf/2406.12719.pdf", "abs": "https://arxiv.org/abs/2406.12719", "title": "Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis", "authors": ["Kushal Raj Bhandari", "Sixue Xing", "Soham Dan", "Jianxi Gao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted TMLR 2025", "summary": "Large Language Models (LLMs), already shown to ace various unstructured text\ncomprehension tasks, have also remarkably been shown to tackle table\n(structured) comprehension tasks without specific training. Building on earlier\nstudies of LLMs for tabular tasks, we probe how in-context learning (ICL),\nmodel scale, instruction tuning, and domain bias affect Tabular QA (TQA)\nrobustness by testing LLMs, under diverse augmentations and perturbations, on\ndiverse domains: Wikipedia-based $\\textbf{WTQ}$, financial $\\textbf{TAT-QA}$,\nand scientific $\\textbf{SCITAB}$. Although instruction tuning and larger, newer\nLLMs deliver stronger, more robust TQA performance, data contamination and\nreliability issues, especially on $\\textbf{WTQ}$, remain unresolved. Through an\nin-depth attention analysis, we reveal a strong correlation between\nperturbation-induced shifts in attention dispersion and the drops in\nperformance, with sensitivity peaking in the model's middle layers. We\nhighlight the need for improved interpretable methodologies to develop more\nreliable LLMs for table comprehension. Through an in-depth attention analysis,\nwe reveal a strong correlation between perturbation-induced shifts in attention\ndispersion and performance drops, with sensitivity peaking in the model's\nmiddle layers. Based on these findings, we argue for the development of\nstructure-aware self-attention mechanisms and domain-adaptive processing\ntechniques to improve the transparency, generalization, and real-world\nreliability of LLMs on tabular data."}
{"id": "2407.06866", "pdf": "https://arxiv.org/pdf/2407.06866.pdf", "abs": "https://arxiv.org/abs/2407.06866", "title": "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "authors": ["Victoria R. Li", "Yida Chen", "Naomi Saphra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While the biases of language models in production are extensively documented,\nthe biases of their guardrails have been neglected. This paper studies how\ncontextual information about the user influences the likelihood of an LLM to\nrefuse to execute a request. By generating user biographies that offer\nideological and demographic information, we find a number of biases in\nguardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas\nare more likely to trigger a refusal guardrail when requesting censored or\nillegal information. Guardrails are also sycophantic, refusing to comply with\nrequests for a political position the user is likely to disagree with. We find\nthat certain identity groups and seemingly innocuous information, e.g., sports\nfandom, can elicit changes in guardrail sensitivity similar to direct\nstatements of political ideology. For each demographic category and even for\nAmerican football team fandom, we find that ChatGPT appears to infer a likely\npolitical ideology and modify guardrail behavior accordingly."}
{"id": "2408.05873", "pdf": "https://arxiv.org/pdf/2408.05873.pdf", "abs": "https://arxiv.org/abs/2408.05873", "title": "Recognizing Limits: Investigating Infeasibility in Large Language Models", "authors": ["Wenbo Zhang", "Zihang Xu", "Hengrui Cai"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Large language models (LLMs) have shown remarkable performance in various\ntasks but often fail to handle queries that exceed their knowledge and\ncapabilities, leading to incorrect or fabricated responses. This paper\naddresses the need for LLMs to recognize and refuse infeasible tasks due to the\nrequests surpassing their capabilities. We conceptualize four main categories\nof infeasible tasks for LLMs, which cover a broad spectrum of\nhallucination-related challenges identified in prior literature. We develop and\nbenchmark a new dataset comprising diverse infeasible and feasible tasks to\nevaluate multiple LLMs' abilities to decline infeasible tasks. Furthermore, we\nexplore the potential of increasing LLMs' refusal capabilities with\nfine-tuning. Our experiments validate the effectiveness of the trained models,\nsuggesting promising directions for improving the performance of LLMs in\nreal-world applications."}
{"id": "2410.19195", "pdf": "https://arxiv.org/pdf/2410.19195.pdf", "abs": "https://arxiv.org/abs/2410.19195", "title": "Label Set Optimization via Activation Distribution Kurtosis for Zero-shot Classification with Generative Models", "authors": ["Yue Li", "Zhixue Zhao", "Carolina Scarton"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025", "summary": "In-context learning (ICL) performance is highly sensitive to prompt design,\nyet the impact of class label options (e.g. lexicon or order) in zero-shot\nclassification remains underexplored. This study proposes LOADS (Label set\nOptimization via Activation Distribution kurtosiS), a post-hoc method for\nselecting optimal label sets in zero-shot ICL with large language models\n(LLMs). LOADS is built upon the observations in our empirical analysis, the\nfirst to systematically examine how label option design (i.e., lexical choice,\norder, and elaboration) impacts classification performance. This analysis shows\nthat the lexical choice of the labels in the prompt (such as agree vs. support\nin stance classification) plays an important role in both model performance and\nmodel's sensitivity to the label order. A further investigation demonstrates\nthat optimal label words tend to activate fewer outlier neurons in LLMs'\nfeed-forward networks. LOADS then leverages kurtosis to measure the neuron\nactivation distribution for label selection, requiring only a single forward\npass without gradient propagation or labelled data. The LOADS-selected label\nwords consistently demonstrate effectiveness for zero-shot ICL across\nclassification tasks, datasets, models and languages, achieving maximum\nperformance gain from 0.54 to 0.76 compared to the conventional approach of\nusing original dataset label words."}
{"id": "2411.14252", "pdf": "https://arxiv.org/pdf/2411.14252.pdf", "abs": "https://arxiv.org/abs/2411.14252", "title": "From Intents to Conversations: Generating Intent-Driven Dialogues with Contrastive Learning for Multi-Turn Classification", "authors": ["Junhua Liu", "Yong Keat Tan", "Bin Fu", "Kwan Hui Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Proceedings of CIKM 2025", "summary": "In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We further propose MINT-CL, a multi-task contrastive learning\nframework for multi-turn intent classification, which improves performance\nwhile reducing dependence on large-scale annotated datasets. Empirical results\ndemonstrate that our approach outperforms competitive baselines in both\ndialogue generation quality and classification accuracy, particularly in\nmultilingual settings. To facilitate future research, we release MINT-E, a\ncomprehensive, multilingual, intent-aware multi-turn dialogue corpus derived\nfrom the e-commerce domain. The reproduced source code and dataset are\navailable at https://github.com/junhua/chain-of-intent."}
{"id": "2412.01377", "pdf": "https://arxiv.org/pdf/2412.01377.pdf", "abs": "https://arxiv.org/abs/2412.01377", "title": "Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge", "authors": ["Yuhe Ji", "Yilun Liu", "Feiyu Yao", "Minggui He", "Shimin Tao", "Xiaofeng Zhao", "Su Chang", "Xinhua Yang", "Weibin Meng", "Yuming Xie", "Boxing Chen", "Shenglin Zhang", "Yongqian Sun"], "categories": ["cs.CL", "cs.SE"], "comment": "Accepted by CIKM 2025", "summary": "Log analysis represents a critical sub-domain within AI applications that\nfacilitates automatic approaches to fault and error management of large-scaled\nsoftware systems, saving labors of traditional manual methods. While existing\nsolutions using large language models (LLMs) show promise, they are limited by\na significant domain gap between natural and log languages (the latter contains\nrich domain-specific tokens such as status codes, IP addresses, resource\npathes), which restricts their effectiveness in real-world applications.\nHowever, directly adapting general-purpose LLMs to log analysis using raw logs\nmay degrade their performance due to inconsistent token distribution. In this\npaper, we present a domain adaptation approach that addresses these limitations\nby integrating interpretable domain knowledge into open-source LLMs through\ncontinual pre-training (CPT), which bridges this domain gap by adapting LLMs on\ninterpretable natural texts with log knowledge (instead of raw logs) to reduce\ndistribution discrepancy. To achieve this, we developed NLPLog, a comprehensive\ndataset containing over 250,000 question-answer pairs on log-related knowledge.\nOur resulting model, SuperLog, achieves the best performance across four log\nanalysis tasks, with an average accuracy improvement of 12.01% over the\nsecond-best model. Ablation study also suggests advantages of domain adaption\nusing interpretable log knowledge over using raw logs."}
{"id": "2412.15495", "pdf": "https://arxiv.org/pdf/2412.15495.pdf", "abs": "https://arxiv.org/abs/2412.15495", "title": "TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use", "authors": ["Junjie Ye", "Yilong Wu", "Sixian Li", "Yuming Yang", "Zhiheng Xi", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Peng Wang", "Zhongchao Shi", "Jianping Fan", "Zhengyin Du"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) achieve remarkable advancements by leveraging\ntools to interact with environments, a critical step toward generalized AI.\nHowever, the standard supervised fine-tuning (SFT) approach, which relies on\nlarge-scale datasets, often overlooks task-specific characteristics in tool\nuse, leading to performance bottlenecks. To address this issue, we analyze\nthree existing LLMs and uncover key insights: training data can inadvertently\nimpede tool-use behavior, token importance is distributed unevenly, and errors\nin tool calls fall into a small set of categories. Building on these findings,\nwe propose~\\emph{TL-Training}, a task-feature-based framework that mitigates\nthe effects of suboptimal training data, dynamically adjusts token weights to\nprioritize key tokens during SFT, and incorporates a robust reward mechanism\ntailored to error categories, optimized through proximal policy optimization.\nWe validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four\nopen-source test sets. Our results demonstrate that the LLM trained by our\nmethod matches or surpasses both open- and closed-source LLMs in tool-use\nperformance using only 1,217 training data points. Additionally, our method\nenhances robustness in noisy environments and improves general task\nperformance, offering a scalable and efficient paradigm for tool-use training\nin LLMs. Code and data are available at\nhttps://github.com/Junjie-Ye/TL-Training."}
{"id": "2502.12459", "pdf": "https://arxiv.org/pdf/2502.12459.pdf", "abs": "https://arxiv.org/abs/2502.12459", "title": "Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements", "authors": ["Guangxiang Zhao", "Saier Hu", "Xiaoqi Jian", "Jinzhu Wu", "Yuhan Wu", "Change Jia", "Lin Sun", "Xiangzheng Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 Main Conference", "summary": "In this paper, we propose a ``Generalization Stress Test\" to assess Large\nLanguage Models' (LLMs) generalization ability under slight and controlled\nperturbations, including option length, problem types, and irrelevant noun\nreplacements. We achieve novel and significant findings that, despite high\nbenchmark scores, LLMs exhibit severe accuracy drops and unexpected biases\n(e.g., preference for longer distractors) when faced with these minor but\ncontent-preserving modifications. For example, Qwen 2.5 1.5B's MMLU score rises\nfrom 60 to 89 and drops from 89 to 36 when option lengths are changed without\naltering the question. Even GPT4o experiences a 25-point accuracy loss when\nproblem types are changed, with a 6-point drop across all three modification\ncategories. These analyses suggest that LLMs rely heavily on superficial cues\nrather than forming robust, abstract representations that generalize across\nformats, lexical variations, and irrelevant content shifts."}
{"id": "2502.13358", "pdf": "https://arxiv.org/pdf/2502.13358.pdf", "abs": "https://arxiv.org/abs/2502.13358", "title": "Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications", "authors": ["Yiming Zeng", "Wanhao Yu", "Zexin Li", "Tao Ren", "Yu Ma", "Jinghan Cao", "Xiyan Chen", "Tingting Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing, demonstrating strong capabilities in tasks such as text generation,\nsummarization, and reasoning. Recently, their potential for automating precise\ntext editing tasks across specialized domains, such as programming code, LaTeX,\nand structured database languages, has gained attention. However, current\nstate-of-the-art LLMs still struggle with executing precise, instruction-driven\nedits, particularly when structural accuracy and strict adherence to domain\nconventions are required. To address these challenges, we introduce\nInstrEditBench, an automated benchmark dataset comprising over 30,000\nstructured editing tasks spanning diverse domains, including Wikipedia\narticles, LaTeX documents, source code, and database languages. Using this\nbenchmark, we develop FineEdit, a specialized editing model explicitly trained\nfor accurate, context-aware text modifications. Experimental evaluations\ndemonstrate that FineEdit outperforms state-of-the-art models, achieving\nimprovements of approximately 10\\% over Gemini models on single-turn edits, up\nto 30\\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance by\nover 40\\% on direct editing tasks. FineEdit also effectively generalizes to\nrealistic multi-turn editing scenarios, highlighting its practical\napplicability. To facilitate further research and reproducibility, we release\nFineEdit at https://github.com/StuRinDQB/FineEdit} and\nhttps://huggingface.co/datasets/YimingZeng/FineEdit_bench."}
{"id": "2503.04945", "pdf": "https://arxiv.org/pdf/2503.04945.pdf", "abs": "https://arxiv.org/abs/2503.04945", "title": "Collaborative Evaluation of Deepfake Text with Deliberation-Enhancing Dialogue Systems", "authors": ["Jooyoung Lee", "Xiaochen Zhu", "Georgi Karadzhov", "Tom Stafford", "Andreas Vlachos", "Dongwon Lee"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "15; To appear in ICWSM 2026 (https://www.icwsm.org/2026/)", "summary": "The proliferation of generative models has presented significant challenges\nin distinguishing authentic human-authored content from deepfake content.\nCollaborative human efforts, augmented by AI tools, present a promising\nsolution. In this study, we explore the potential of DeepFakeDeLiBot, a\ndeliberation-enhancing chatbot, to support groups in detecting deepfake text.\nOur findings reveal that group-based problem-solving significantly improves the\naccuracy of identifying machine-generated paragraphs compared to individual\nefforts. While engagement with DeepFakeDeLiBot does not yield substantial\nperformance gains overall, it enhances group dynamics by fostering greater\nparticipant engagement, consensus building, and the frequency and diversity of\nreasoning-based utterances. Additionally, participants with higher perceived\neffectiveness of group collaboration exhibited performance benefits from\nDeepFakeDeLiBot. These findings underscore the potential of deliberative\nchatbots in fostering interactive and productive group dynamics while ensuring\naccuracy in collaborative deepfake text detection. \\textit{Dataset and source\ncode used in this study will be made publicly available upon acceptance of the\nmanuscript."}
{"id": "2503.06029", "pdf": "https://arxiv.org/pdf/2503.06029.pdf", "abs": "https://arxiv.org/abs/2503.06029", "title": "SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?", "authors": ["Xudong Lu", "Haohao Gao", "Renshou Wu", "Shuai Ren", "Xiaoxin Chen", "Hongsheng Li", "Fangyuan Li"], "categories": ["cs.CL", "cs.LG"], "comment": "26 pages", "summary": "Large Language Models (LLMs) have become integral to daily life, especially\nadvancing as intelligent assistants through on-device deployment on\nsmartphones. However, existing LLM evaluation benchmarks predominantly focus on\nobjective tasks like mathematics and coding in English, which do not\nnecessarily reflect the practical use cases of on-device LLMs in real-world\nmobile scenarios, especially for Chinese users. To address these gaps, we\nintroduce SmartBench, the first benchmark designed to evaluate the capabilities\nof on-device LLMs in Chinese mobile contexts. We analyze functionalities\nprovided by representative smartphone manufacturers and divide them into five\ncategories: text summarization, text Q&A, information extraction, content\ncreation, and notification management, further detailed into 20 specific tasks.\nFor each task, we construct high-quality datasets comprising 50 to 200\nquestion-answer pairs that reflect everyday mobile interactions, and we develop\nautomated evaluation criteria tailored for these tasks. We conduct\ncomprehensive evaluations of on-device LLMs and MLLMs using SmartBench and also\nassess their performance after quantized deployment on real smartphone NPUs.\nOur contributions provide a standardized framework for evaluating on-device\nLLMs in Chinese, promoting further development and optimization in this\ncritical area. Code and data will be available at\nhttps://github.com/vivo-ai-lab/SmartBench."}
{"id": "2503.20533", "pdf": "https://arxiv.org/pdf/2503.20533.pdf", "abs": "https://arxiv.org/abs/2503.20533", "title": "Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence", "authors": ["Yijiong Yu"], "categories": ["cs.CL"], "comment": "Our code is available in\n  https://github.com/yuyijiong/parallel-decoding-in-one-sequence", "summary": "Recent advances in reasoning models have demonstrated significant\nimprovements in accuracy by employing detailed and comprehensive reasoning\nprocesses. However, generating these lengthy reasoning sequences is\ncomputationally expensive and time-consuming. To address this inefficiency, we\nleverage the inherent parallelizability of certain tasks to accelerate the\nreasoning process. Specifically, when multiple parallel reasoning steps exist,\nwe decode multiple tokens per forward pass via a tree-like attention mask\nwithin a single sequence, avoiding additional memory usage. Experimental\nresults show that our method achieves up to nearly 100\\% speedup in decoding\nwhile basically maintaining the answer quality."}
{"id": "2505.00039", "pdf": "https://arxiv.org/pdf/2505.00039.pdf", "abs": "https://arxiv.org/abs/2505.00039", "title": "An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal, and Deterministic Approach", "authors": ["Hudson de Martim"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "This is a major revision that significantly expands and deepens the\n  original manuscript. While the core ontological model remains the same, this\n  version provides a substantially more rigorous and detailed account of how\n  the framework is applied in practice, particularly within a\n  Retrieval-Augmented Generation (RAG) context", "summary": "Retrieval-Augmented Generation (RAG) systems in the legal domain face a\ncritical challenge: standard, flat-text retrieval is blind to the hierarchical,\ndiachronic, and causal structure of law, leading to anachronistic and\nunreliable answers. This paper introduces an ontology-driven Graph RAG\nframework designed to overcome these limitations. We ground our knowledge graph\nin a formal, LRMoo-inspired model that distinguishes abstract legal Works from\ntheir versioned Expressions. We model temporal states as efficient aggregations\nthat reuse the versioned expressions (CTVs) of unchanged components, and we\nreify legislative events as first-class Action nodes to make causality explicit\nand queryable. This structured backbone enables a unified, planner-guided query\nstrategy that applies explicit policies to deterministically resolve complex\nrequests for (i) point-in-time retrieval, (ii) hierarchical impact analysis,\nand (iii) auditable provenance reconstruction. Through a case study on the\nBrazilian Constitution, we demonstrate how this approach provides a verifiable,\ntemporally-correct substrate for LLMs, enabling higher-order analytical\ncapabilities while drastically reducing the risk of factual errors. The result\nis a practical framework for building more trustworthy and explainable legal AI\nsystems."}
{"id": "2505.12584", "pdf": "https://arxiv.org/pdf/2505.12584.pdf", "abs": "https://arxiv.org/abs/2505.12584", "title": "Improving Multilingual Language Models by Aligning Representations through Steering", "authors": ["Omar Mahmoud", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "categories": ["cs.CL"], "comment": null, "summary": "This paper investigates how Large Language Models (LLMs) represent\nnon-English tokens -- a question that remains underexplored despite recent\nprogress. We propose a lightweight intervention method using representation\nsteering, where a learned vector is added to the residual stream at a single\nmodel layer to enhance multilingual performance. Through extensive experiments\nacross seven competitive baselines -- including prompt optimization, supervised\nfine-tuning (SFT), in-context learning, cross-lingual transfer, and\ntranslation-based methods-we show that our approach consistently outperforms\nmost alternatives. In particular, it achieves performance on par with\nproduction-grade translation systems while requiring far fewer resources. We\nfurther explore the complementarity between our method and SFT, demonstrating\nthat steering offers a direct, efficient way to realign internal\nrepresentations. These findings underscore the potential of activation-level\ninterventions as a powerful tool for improving the multilingual capabilities of\nLLMs."}
{"id": "2505.13972", "pdf": "https://arxiv.org/pdf/2505.13972.pdf", "abs": "https://arxiv.org/abs/2505.13972", "title": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals", "authors": ["Qianli Wang", "Van Bach Nguyen", "Nils Feldhus", "Luis Felipe Villa-Arenas", "Christin Seifert", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL"], "comment": "Accepted at INLG 2025, camera-ready version", "summary": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models: being the same model, belonging to the same model family, being\nindependent models, and having an distillation relationship. Through extensive\nexperiments involving two state-of-the-art LLM-based methods, three datasets,\nfour generator models, and 15 judge models, complemented by a user study (n =\n90), we demonstrate that judge models with an independent, non-fine-tuned\nrelationship to the generator model provide the most reliable label flipping\nevaluations. Relationships between the generator and judge models, which are\nclosely aligned with the user study for CDA, result in better model performance\nand robustness. Nevertheless, we find that the gap between the most effective\njudge models and the results obtained from the user study remains considerably\nlarge. This suggests that a fully automated pipeline for CDA may be inadequate\nand requires human intervention."}
{"id": "2505.14582", "pdf": "https://arxiv.org/pdf/2505.14582.pdf", "abs": "https://arxiv.org/abs/2505.14582", "title": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning", "authors": ["Shangziqi Zhao", "Jiahao Yuan", "Guisong Yang", "Usman Naseem"], "categories": ["cs.CL"], "comment": "19 pages,6 figures", "summary": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies - targeting entire chains, core reasoning, and verification - we\nfind that verification pruning consistently improves accuracy while reducing\ntoken usage, whereas reasoning or indiscriminate pruning degrades performance.\nOur study reveals that effective pruning aligns supervision with model capacity\nrather than merely shortening inputs. Gains hold across tasks, model scales,\nand CoT capability, with larger models benefiting more from pruning due to\nricher but more redundant reasoning. Our empirical findings highlight pruning\nas a structural optimization strategy for aligning CoT reasoning with SLM\ncapacity."}
{"id": "2505.14607", "pdf": "https://arxiv.org/pdf/2505.14607.pdf", "abs": "https://arxiv.org/abs/2505.14607", "title": "sudoLLM: On Multi-role Alignment of Language Models", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Joy Mahapatra", "Utpal Garain"], "categories": ["cs.CL", "cs.CR", "I.2.7"], "comment": "Accepted to EMNLP 2025 (findings)", "summary": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have not been extensively studied in the large\nlanguage model (LLM) realm. In this work, drawing inspiration from such access\ncontrol systems, we introduce sudoLLM, a novel framework that results in\nmulti-role aligned LLMs, i.e., LLMs that account for, and behave in accordance\nwith, user access rights. sudoLLM injects subtle user-based biases into queries\nand trains an LLM to utilize this bias signal in order to produce sensitive\ninformation if and only if the user is authorized. We present empirical results\ndemonstrating that this approach shows substantially improved alignment,\ngeneralization, resistance to prefix-based jailbreaking attacks, and\n``fails-closed''. The persistent tension between the language modeling\nobjective and safety alignment, which is often exploited to jailbreak LLMs, is\nsomewhat resolved with the aid of the injected bias signal. Our framework is\nmeant as an additional security layer, and complements existing guardrail\nmechanisms for enhanced end-to-end safety with LLMs."}
{"id": "2505.15386", "pdf": "https://arxiv.org/pdf/2505.15386.pdf", "abs": "https://arxiv.org/abs/2505.15386", "title": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection", "authors": ["Yiming Huang", "Junyan Zhang", "Zihao Wang", "Biquan Bie", "Yunzhong Qiu", "Yi R. Fung", "Xinlei He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have become powerful, but hallucinations remain\na vital obstacle to their trustworthy use. While previous works improved the\ncapability of hallucination detection by measuring uncertainty, they all lack\nthe ability to explain the provenance behind why hallucinations occur, i.e.,\nwhich part of the inputs tends to trigger hallucinations. Recent works on the\nprompt attack indicate that uncertainty exists in semantic propagation, where\nattention mechanisms gradually fuse local token information into high-level\nsemantics across layers. Meanwhile, uncertainty also emerges in language\ngeneration, due to its probability-based selection of high-level semantics for\nsampled generations. Based on that, we propose RePPL to recalibrate uncertainty\nmeasurement by these two aspects, which dispatches explainable uncertainty\nscores to each token and aggregates in Perplexity-style Log-Average form as\ntotal score. Experiments show that our method achieves the best comprehensive\ndetection performance across various QA datasets on advanced models (average\nAUC of 0.833), and our method is capable of producing token-level uncertainty\nscores as explanations for the hallucination. Leveraging these scores, we\npreliminarily find the chaotic pattern of hallucination and showcase its\npromising usage."}
{"id": "2505.17691", "pdf": "https://arxiv.org/pdf/2505.17691.pdf", "abs": "https://arxiv.org/abs/2505.17691", "title": "ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction", "authors": ["Yan Yu", "Yilun Liu", "Minggui He", "Shimin Tao", "Weibin Meng", "Xinhua Yang", "Li Zhang", "Hongxia Ma", "Dengye Li", "Daimeng Wei", "Boxing Chen", "Fuliang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Pairwise evaluation of large language models (LLMs) has become the dominant\nparadigm for benchmarking open-ended tasks, yet non-transitive preferences,\nwhere evaluators prefer A over B, B over C, but C over A, fundamentally\nundermine ranking reliability. We show that this critical issue stems largely\nfrom low-quality data that contains inherently ambiguous preference pairs. To\naddress this challenge, we propose ELSPR, a principled graph-theoretic\nframework that models pairwise preferences as tournament graphs and\nsystematically identifies problematic training data. ELSPR quantifies\nnon-transitivity through strongly connected components (SCCs) analysis and\nmeasures overall preference clarity using a novel normalized directed graph\nstructural entropy metric. Our filtering methodology selectively removes\npreference data that induce non-transitivity while preserving transitive\npreferences. Extensive experiments on the AlpacaEval benchmark demonstrate that\nmodels fine-tuned on ELSPR-filtered data achieve substantial improvements: a\n13.8% reduction in non-transitivity, a 0.088 decrease in structural entropy,\nand significantly enhanced discriminative power in real-world evaluation\nsystems. Human validation confirms that discarded data exhibit dramatically\nlower inter-annotator agreement (34.4% vs. 52.6%) and model-human consistency\n(51.2% vs. 80.6%) compared to cleaned data. These findings establish ELSPR as\nan effective data self-purification approach for developing more robust,\nconsistent, and human-aligned LLM evaluation systems."}
{"id": "2505.18596", "pdf": "https://arxiv.org/pdf/2505.18596.pdf", "abs": "https://arxiv.org/abs/2505.18596", "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models", "authors": ["Chen Han", "Wenzhen Zheng", "Xijin Tang"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper has been accepted to EMNLP 2025 (Main Conference)", "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards\ninterpretable misinformation detection. The code will be released publicly\nafter the official publication."}
{"id": "2505.23840", "pdf": "https://arxiv.org/pdf/2505.23840.pdf", "abs": "https://arxiv.org/abs/2505.23840", "title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues", "authors": ["Jiseung Hong", "Grace Byun", "Seungone Kim", "Kai Shu", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": "Accepted to Findings of EMNLP 2025", "summary": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench."}
{"id": "2506.04616", "pdf": "https://arxiv.org/pdf/2506.04616.pdf", "abs": "https://arxiv.org/abs/2506.04616", "title": "Subjective Perspectives within Learned Representations Predict High-Impact Innovation", "authors": ["Likun Cao", "Rui Pan", "James Evans"], "categories": ["cs.CL", "stat.AP", "stat.ML"], "comment": "123 pages, 23 figures", "summary": "Existing studies of innovation emphasize the power of social structures to\nshape innovation capacity. Emerging machine learning approaches, however,\nenable us to model innovators' personal perspectives and interpersonal\ninnovation opportunities as a function of their prior experience. We theorize\nand then quantify subjective perspectives and their interaction based on\ninnovator positions within the geometric space of concepts inscribed by dynamic\nmachine-learned language representations. Using data on millions of scientists,\ninventors, screenplay writers, entrepreneurs, and Wikipedia contributors across\ntheir respective creative domains, here we show that measured subjective\nperspectives predict which ideas individuals and groups will creatively attend\nto and successfully combine in the future. Across all cases and time periods we\nexamine, when perspective diversity is decomposed as the difference between\ncollaborators' perspectives on their creation, and background diversity as the\ndifference between their experiences, the former consistently anticipates\ncreative achievement while the latter portends its opposite. We analyze a\nnatural experiment and simulate creative collaborations between AI agents\ndesigned with various perspective and background diversity, which support our\nobservational findings. We explore mechanisms underlying these findings and\nidentify how successful collaborators leverage common language to weave\ntogether diverse experiences obtained through trajectories of prior work. These\nperspectives converge and provoke one another to innovate. We examine the\nsignificance of these findings for team formation and research policy."}
{"id": "2506.08592", "pdf": "https://arxiv.org/pdf/2506.08592.pdf", "abs": "https://arxiv.org/abs/2506.08592", "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings", "authors": ["Liyan Xu", "Zhenlin Su", "Mo Yu", "Jiangnan Li", "Fandong Meng", "Jie Zhou"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "This work stems from an observed limitation of text encoders: embeddings may\nnot be able to recognize fine-grained entities or events within encoded\nsemantics, resulting in failed retrieval even in simple cases. To examine such\nbehaviors, we first introduce a new evaluation dataset, CapRetrieval, in which\npassages are image captions and queries are phrases targeting entity or event\nconcepts in diverse forms. Zero-shot evaluation suggests that encoders often\nstruggle with these fine-grained matching, regardless of training sources or\nmodel size. Aiming for enhancement, we proceed to finetune encoders with our\nproposed data generation strategies, enabling a small 0.1B encoder to\noutperform the state-of-the-art 7B model. Within this process, we further\nuncover the granularity dilemma, a challenge for embeddings to capture\nfine-grained salience while aligning with overall semantics. Our dataset, code\nand models in this work are publicly released at\nhttps://github.com/lxucs/CapRetrieval."}
{"id": "2506.20430", "pdf": "https://arxiv.org/pdf/2506.20430.pdf", "abs": "https://arxiv.org/abs/2506.20430", "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning", "authors": ["Weike Zhao", "Chaoyi Wu", "Yanjie Fan", "Xiaoman Zhang", "Pengcheng Qiu", "Yuze Sun", "Xiao Zhou", "Yanfeng Wang", "Xin Sun", "Ya Zhang", "Yongguo Yu", "Kun Sun", "Weidi Xie"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "comment": null, "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor."}
{"id": "2506.22316", "pdf": "https://arxiv.org/pdf/2506.22316.pdf", "abs": "https://arxiv.org/abs/2506.22316", "title": "Evaluating Scoring Bias in LLM-as-a-Judge", "authors": ["Qingquan Li", "Shaoyu Dou", "Kailai Shao", "Chao Chen", "Haixiang Hu"], "categories": ["cs.CL"], "comment": null, "summary": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection."}
{"id": "2507.08045", "pdf": "https://arxiv.org/pdf/2507.08045.pdf", "abs": "https://arxiv.org/abs/2507.08045", "title": "Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing", "authors": ["Junyi Wen", "Junyuan Liang", "Zicong Hong", "Wuhui Chen", "Ting Cai", "Zibin Zheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."}
{"id": "2507.17178", "pdf": "https://arxiv.org/pdf/2507.17178.pdf", "abs": "https://arxiv.org/abs/2507.17178", "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs", "authors": ["Zhiqiang Liu", "Enpei Niu", "Yin Hua", "Mengshu Sun", "Lei Liang", "Huajun Chen", "Wen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench."}
{"id": "2508.14090", "pdf": "https://arxiv.org/pdf/2508.14090.pdf", "abs": "https://arxiv.org/abs/2508.14090", "title": "DLLMQuant: Quantizing Diffusion-based Large Language Models", "authors": ["Chen Xu", "Dawei Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 6 figures", "summary": "Diffusion-based large language models (DLLMs) have shown promise for\nnon-autoregressive text generation, but their deployment is constrained by\nlarge model sizes and heavy computational costs. Post-training quantization\n(PTQ), a widely used method for compressing and accelerating Large Language\nModels (LLMs), suffers from severe accuracy degradation and reduced\ngeneralization performance when directly applied to DLLMs (e.g., AWQ suffers a\n16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key\nmechanisms - dynamic masking, iterative generation, bidirectional attention -\nclash with quantization. We identify three core issues: 1) Iterative generation\nand dynamic masking ratios lead to distinct token distributions across decoding\nsteps, which are not adequately captured by existing PTQ calibration methods;\n2) Quantization errors are accumulated and amplified progressively during\niteration in DLLMs, causing quantized models to perform worse as decoding steps\nprogress; 3) Unmasked tokens stabilize while masked remain probabilistic,\nmaking overall feature distribution incompatible with existing PTQ methods. To\naddress these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs,\nwhich incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling\n(TMAS), a calibration method that accounts for both time and mask factors, with\nthe capacity to capture distributions across timesteps. 2) Interaction-Aware\nActivation Quantization (IA-AQ), which utilizes bidirectional attention's\ninteraction signals to dynamically allocate quantization resources. 3)\nCertainty-Guided Quantization (CGQ), which integrates mask status and token\nscores as key weighting criteria into error compensation, making weight\nquantization more suitable for DLLMs. Experiments show that DLLMQuant achieves\nsignificant performance gains while enhancing efficiency."}
{"id": "2508.14444", "pdf": "https://arxiv.org/pdf/2508.14444.pdf", "abs": "https://arxiv.org/abs/2508.14444", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "authors": ["NVIDIA", ":", "Aarti Basant", "Abhijit Khairnar", "Abhijit Paithankar", "Abhinav Khattar", "Adithya Renduchintala", "Aditya Malte", "Akhiad Bercovich", "Akshay Hazare", "Alejandra Rico", "Aleksander Ficek", "Alex Kondratenko", "Alex Shaposhnikov", "Alexander Bukharin", "Ali Taghibakhshi", "Amelia Barton", "Ameya Sunil Mahabaleshwarkar", "Amy Shen", "Andrew Tao", "Ann Guan", "Anna Shors", "Anubhav Mandarwal", "Arham Mehta", "Arun Venkatesan", "Ashton Sharabiani", "Ashwath Aithal", "Ashwin Poojary", "Ayush Dattagupta", "Balaram Buddharaju", "Banghua Zhu", "Barnaby Simkin", "Bilal Kartal", "Bita Darvish Rouhani", "Bobby Chen", "Boris Ginsburg", "Brandon Norick", "Brian Yu", "Bryan Catanzaro", "Charles Wang", "Charlie Truong", "Chetan Mungekar", "Chintan Patel", "Chris Alexiuk", "Christian Munley", "Christopher Parisien", "Dan Su", "Daniel Afrimi", "Daniel Korzekwa", "Daniel Rohrer", "Daria Gitman", "David Mosallanezhad", "Deepak Narayanan", "Dima Rekesh", "Dina Yared", "Dmytro Pykhtar", "Dong Ahn", "Duncan Riach", "Eileen Long", "Elliott Ning", "Eric Chung", "Erick Galinkin", "Evelina Bakhturina", "Gargi Prasad", "Gerald Shen", "Haifeng Qian", "Haim Elisha", "Harsh Sharma", "Hayley Ross", "Helen Ngo", "Herman Sahota", "Hexin Wang", "Hoo Chang Shin", "Hua Huang", "Iain Cunningham", "Igor Gitman", "Ivan Moshkov", "Jaehun Jung", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jian Zhang", "Jiaqi Zeng", "Jimmy Zhang", "Jinze Xue", "Jocelyn Huang", "Joey Conway", "John Kamalu", "Jonathan Cohen", "Joseph Jennings", "Julien Veron Vialard", "Junkeun Yi", "Jupinder Parmar", "Kari Briski", "Katherine Cheung", "Katherine Luna", "Keith Wyss", "Keshav Santhanam", "Kezhi Kong", "Krzysztof Pawelec", "Kumar Anik", "Kunlun Li", "Kushan Ahmadian", "Lawrence McAfee", "Laya Sleiman", "Leon Derczynski", "Luis Vega", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Marcin Chochowski", "Mark Cai", "Markus Kliegl", "Marta Stepniewska-Dziubinska", "Matvei Novikov", "Mehrzad Samadi", "Meredith Price", "Meriem Boubdir", "Michael Boone", "Michael Evans", "Michal Bien", "Michal Zawalski", "Miguel Martinez", "Mike Chrzanowski", "Mohammad Shoeybi", "Mostofa Patwary", "Namit Dhameja", "Nave Assaf", "Negar Habibi", "Nidhi Bhatia", "Nikki Pope", "Nima Tajbakhsh", "Nirmal Kumar Juluru", "Oleg Rybakov", "Oleksii Hrinchuk", "Oleksii Kuchaiev", "Oluwatobi Olabiyi", "Pablo Ribalta", "Padmavathy Subramanian", "Parth Chadha", "Pavlo Molchanov", "Peter Dykas", "Peter Jin", "Piotr Bialecki", "Piotr Januszewski", "Pradeep Thalasta", "Prashant Gaikwad", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Rabeeh Karimi Mahabadi", "Rajen Patel", "Ran El-Yaniv", "Ranjit Rajan", "Ria Cheruvu", "Rima Shahbazyan", "Ritika Borkar", "Ritu Gala", "Roger Waleffe", "Ruoxi Zhang", "Russell J. Hewett", "Ryan Prenger", "Sahil Jain", "Samuel Kriman", "Sanjeev Satheesh", "Saori Kaji", "Sarah Yurick", "Saurav Muralidharan", "Sean Narenthiran", "Seonmyeong Bak", "Sepehr Sameni", "Seungju Han", "Shanmugam Ramasamy", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Shelby Thomas", "Shizhe Diao", "Shreya Gopal", "Shrimai Prabhumoye", "Shubham Toshniwal", "Shuoyang Ding", "Siddharth Singh", "Siddhartha Jain", "Somshubra Majumdar", "Soumye Singhal", "Stefania Alborghetti", "Syeda Nahida Akter", "Terry Kong", "Tim Moon", "Tomasz Hliwiak", "Tomer Asida", "Tony Wang", "Tugrul Konuk", "Twinkle Vashishth", "Tyler Poon", "Udi Karpas", "Vahid Noroozi", "Venkat Srinivasan", "Vijay Korthikanti", "Vikram Fugro", "Vineeth Kalluru", "Vitaly Kurin", "Vitaly Lavrukhin", "Wasi Uddin Ahmad", "Wei Du", "Wonmin Byeon", "Ximing Lu", "Xin Dong", "Yashaswi Karnati", "Yejin Choi", "Yian Zhang", "Ying Lin", "Yonggan Fu", "Yoshi Suhara", "Zhen Dong", "Zhiyu Li", "Zhongbo Zhu", "Zijia Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving\nstate-of-the-art accuracy compared to similarly-sized models.\nNemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the\nmajority of the self-attention layers in the common Transformer architecture\nare replaced with Mamba-2 layers, to achieve improved inference speed when\ngenerating the long thinking traces needed for reasoning. We create\nNemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model\n(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.\nAfter aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to\ncompress and distill the model with the goal of enabling inference on up to\n128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).\nCompared to existing similarly-sized models (e.g., Qwen3-8B), we show that\nNemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks\nwhile achieving up to 6x higher inference throughput in reasoning settings like\n8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,\nNemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with\nthe majority of our pre- and post-training datasets on Hugging Face."}
{"id": "2508.15648", "pdf": "https://arxiv.org/pdf/2508.15648.pdf", "abs": "https://arxiv.org/abs/2508.15648", "title": "SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models", "authors": ["Peng Ding", "Wen Sun", "Dailin Li", "Wei Zou", "Jiaming Wang", "Jiajun Chen", "Shujian Huang"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 (Main Conference), 15 pages, 4 figures, 6\n  tables", "summary": "Large Language Models (LLMs) excel at various natural language processing\ntasks but remain vulnerable to jailbreaking attacks that induce harmful content\ngeneration. In this paper, we reveal a critical safety inconsistency: LLMs can\nmore effectively identify harmful requests as discriminators than defend\nagainst them as generators. This insight inspires us to explore aligning the\nmodel's inherent discrimination and generation capabilities. To this end, we\npropose SDGO (Self-Discrimination-Guided Optimization), a reinforcement\nlearning framework that leverages the model's own discrimination capabilities\nas a reward signal to enhance generation safety through iterative\nself-improvement. Our method does not require any additional annotated data or\nexternal models during the training phase. Extensive experiments demonstrate\nthat SDGO significantly improves model safety compared to both prompt-based and\ntraining-based baselines while maintaining helpfulness on general benchmarks.\nBy aligning LLMs' discrimination and generation capabilities, SDGO brings\nrobust performance against out-of-distribution (OOD) jailbreaking attacks. This\nalignment achieves tighter coupling between these two capabilities, enabling\nthe model's generation capability to be further enhanced with only a small\namount of discriminative samples. Our code and datasets are available at\nhttps://github.com/NJUNLP/SDGO."}
{"id": "2508.16267", "pdf": "https://arxiv.org/pdf/2508.16267.pdf", "abs": "https://arxiv.org/abs/2508.16267", "title": "From Confidence to Collapse in LLM Factual Robustness", "authors": ["Alina Fastowski", "Bardh Prenkaj", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models."}
{"id": "2508.16876", "pdf": "https://arxiv.org/pdf/2508.16876.pdf", "abs": "https://arxiv.org/abs/2508.16876", "title": "Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling", "authors": ["Yue Zhao", "Xiaoyu Wang", "Dan Wang", "Zhonglin Jiang", "Qingqing Gu", "Teng Chen", "Ningyuan Xi", "Jinxian Qu", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "World models have been widely utilized in robotics, gaming, and auto-driving.\nHowever, their applications on natural language tasks are relatively limited.\nIn this paper, we construct the dialogue world model, which could predict the\nuser's emotion, sentiment, and intention, and future utterances. By defining a\nPOMDP, we argue emotion, sentiment and intention can be modeled as the user\nbelief and solved by maximizing the information bottleneck. By this user belief\nmodeling, we apply the model-based reinforcement learning framework to the\ndialogue system, and propose a framework called DreamCUB. Experiments show that\nthe pretrained dialogue world model can achieve state-of-the-art performances\non emotion classification and sentiment identification, while dialogue quality\nis also enhanced by joint training of the policy, critic and dialogue world\nmodel. Further analysis shows that this manner holds a reasonable\nexploration-exploitation balance and also transfers well to out-of-domain\nscenarios such as empathetic dialogues."}
{"id": "2508.17576", "pdf": "https://arxiv.org/pdf/2508.17576.pdf", "abs": "https://arxiv.org/abs/2508.17576", "title": "CausalSent: Interpretable Sentiment Classification with RieszNet", "authors": ["Daniel Frees", "Martin Pollack"], "categories": ["cs.CL", "cs.LG", "68T50"], "comment": null, "summary": "Despite the overwhelming performance improvements offered by recent natural\nlanguage processing (NLP) models, the decisions made by these models are\nlargely a black box. Towards closing this gap, the field of causal NLP combines\ncausal inference literature with modern NLP models to elucidate causal effects\nof text features. We replicate and extend Bansal et al's work on regularizing\ntext classifiers to adhere to estimated effects, focusing instead on model\ninterpretability. Specifically, we focus on developing a two-headed\nRieszNet-based neural network architecture which achieves better treatment\neffect estimation accuracy. Our framework, CausalSent, accurately predicts\ntreatment effects in semi-synthetic IMDB movie reviews, reducing MAE of effect\nestimates by 2-3x compared to Bansal et al's MAE on synthetic Civil Comments\ndata. With an ensemble of validated models, we perform an observational case\nstudy on the causal effect of the word \"love\" in IMDB movie reviews, finding\nthat the presence of the word \"love\" causes a +2.9% increase in the probability\nof a positive sentiment."}
{"id": "2508.17610", "pdf": "https://arxiv.org/pdf/2508.17610.pdf", "abs": "https://arxiv.org/abs/2508.17610", "title": "Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions", "authors": ["Nannan Huang", "Haytham Fayek", "Xiuzhen Zhang"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Model compression through post-training pruning offers a way to reduce model\nsize and computational requirements without significantly impacting model\nperformance. However, the effect of pruning on the fairness of LLM-generated\nsummaries remains unexplored, particularly for opinion summarisation where\nbiased outputs could influence public views.In this paper, we present a\ncomprehensive empirical analysis of opinion summarisation, examining three\nstate-of-the-art pruning methods and various calibration sets across three\nopen-source LLMs using four fairness metrics. Our systematic analysis reveals\nthat pruning methods have a greater impact on fairness than calibration sets.\nBuilding on these insights, we propose High Gradient Low Activation (HGLA)\npruning, which identifies and removes parameters that are redundant for input\nprocessing but influential in output generation. Our experiments demonstrate\nthat HGLA can better maintain or even improve fairness compared to existing\nmethods, showing promise across models and tasks where traditional methods have\nlimitations. Our human evaluation shows HGLA-generated outputs are fairer than\nexisting state-of-the-art pruning methods. Code is available at:\nhttps://github.com/amberhuang01/HGLA."}
{"id": "2508.17623", "pdf": "https://arxiv.org/pdf/2508.17623.pdf", "abs": "https://arxiv.org/abs/2508.17623", "title": "EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in Spoken Dialogue Systems", "authors": ["Jingwen Liu", "Kan Jen Cheng", "Jiachen Lian", "Akshay Anand", "Rishi Jain", "Faith Qiao", "Robin Netzorg", "Huang-Cheng Chou", "Tingle Li", "Guan-Ting Lin", "Gopala Anumanchipalli"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at (ASRU 2025) 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "Speech emotions play a crucial role in human-computer interaction, shaping\nengagement and context-aware communication. Despite recent advances in spoken\ndialogue systems, a holistic system for evaluating emotional reasoning is still\nlacking. To address this, we introduce EMO-Reasoning, a benchmark for assessing\nemotional coherence in dialogue systems. It leverages a curated dataset\ngenerated via text-to-speech to simulate diverse emotional states, overcoming\nthe scarcity of emotional speech data. We further propose the Cross-turn\nEmotion Reasoning Score to assess the emotion transitions in multi-turn\ndialogues. Evaluating seven dialogue systems through continuous, categorical,\nand perceptual metrics, we show that our framework effectively detects\nemotional inconsistencies, providing insights for improving current dialogue\nsystems. By releasing a systematic evaluation benchmark, we aim to advance\nemotion-aware spoken dialogue modeling toward more natural and adaptive\ninteractions."}
{"id": "2312.09625", "pdf": "https://arxiv.org/pdf/2312.09625.pdf", "abs": "https://arxiv.org/abs/2312.09625", "title": "Weakly-Supervised 3D Visual Grounding based on Visual Language Alignment", "authors": ["Xiaoxu Xu", "Yitian Yuan", "Qiudan Zhang", "Wenhui Wu", "Zequn Jie", "Lin Ma", "Xu Wang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly\nsupervised approach for 3D visual grounding based on Visual Linguistic\nAlignment. Our 3D-VLA exploits the superior ability of current large-scale\nvision-language models (VLMs) on aligning the semantics between texts and 2D\nimages, as well as the naturally existing correspondences between 2D images and\n3D point clouds, and thus implicitly constructs correspondences between texts\nand 3D point clouds with no need for fine-grained box annotations in the\ntraining procedure. During the inference stage, the learned text-3D\ncorrespondence will help us ground the text queries to the 3D target objects\neven without 2D images. To the best of our knowledge, this is the first work to\ninvestigate 3D visual grounding in a weakly supervised manner by involving\nlarge scale vision-language models, and extensive experiments on ReferIt3D and\nScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even\nsuperior results over the fully supervised methods."}
{"id": "2409.08846", "pdf": "https://arxiv.org/pdf/2409.08846.pdf", "abs": "https://arxiv.org/abs/2409.08846", "title": "Fingerprint Vector: Enabling Scalable and Efficient Model Fingerprint Transfer via Vector Addition", "authors": ["Zhenhua Xu", "Qichen Liu", "Zhebo Wang", "Wenpeng Xing", "Dezhang Kong", "Mohan Li", "Meng Han"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "Backdoor-based fingerprinting has emerged as an effective technique for\ntracing the ownership of large language models. However, in real-world\ndeployment scenarios, developers often instantiate multiple downstream models\nfrom a shared base model, and applying fingerprinting to each variant\nindividually incurs prohibitive computational overhead. While inheritance-based\napproaches -- where fingerprints are embedded into the base model and expected\nto persist through fine-tuning -- appear attractive, they suffer from three key\nlimitations: late-stage fingerprinting, fingerprint instability, and\ninterference with downstream adaptation. To address these challenges, we\npropose a novel mechanism called the Fingerprint Vector. Our method first\nembeds a fingerprint into the base model via backdoor-based fine-tuning, then\nextracts a task-specific parameter delta as a fingerprint vector by computing\nthe difference between the fingerprinted and clean models. This vector can be\ndirectly added to any structurally compatible downstream model, allowing the\nfingerprint to be transferred post hoc without additional fine-tuning.\nExtensive experiments show that Fingerprint Vector achieves comparable or\nsuperior performance to direct injection across key desiderata. It maintains\nstrong effectiveness across diverse model architectures as well as mainstream\ndownstream variants within the same family. It also preserves harmlessness and\nrobustness in most cases. Even when slight robustness degradation is observed,\nthe impact remains within acceptable bounds and is outweighed by the\nscalability benefits of our approach."}
{"id": "2411.14137", "pdf": "https://arxiv.org/pdf/2411.14137.pdf", "abs": "https://arxiv.org/abs/2411.14137", "title": "VAGUE: Visual Contexts Clarify Ambiguous Expressions", "authors": ["Heejeong Nam", "Jinwoo Ahn", "Keummin Ka", "Jiwan Chung", "Youngjae Yu"], "categories": ["cs.CV", "cs.CL"], "comment": "ICCV 2025, 32 pages", "summary": "Human communication often relies on visual cues to resolve ambiguity. While\nhumans can intuitively integrate these cues, AI systems often find it\nchallenging to engage in sophisticated multimodal reasoning. We introduce\nVAGUE, a benchmark evaluating multimodal AI systems' ability to integrate\nvisual context for intent disambiguation. VAGUE consists of 1.6K ambiguous\ntextual expressions, each paired with an image and multiple-choice\ninterpretations, where the correct answer is only apparent with visual context.\nThe dataset spans both staged, complex (Visual Commonsense Reasoning) and\nnatural, personal (Ego4D) scenes, ensuring diversity. Our experiments reveal\nthat existing multimodal AI models struggle to infer the speaker's true intent.\nWhile performance consistently improves from the introduction of more visual\ncues, the overall accuracy remains far below human performance, highlighting a\ncritical gap in multimodal reasoning. Analysis of failure cases demonstrates\nthat current models fail to distinguish true intent from superficial\ncorrelations in the visual scene, indicating that they perceive images but do\nnot effectively reason with them. We release our code and data at\nhttps://hazel-heejeong-nam.github.io/vague/."}
{"id": "2411.15008", "pdf": "https://arxiv.org/pdf/2411.15008.pdf", "abs": "https://arxiv.org/abs/2411.15008", "title": "Evolutionary Automata and Deep Evolutionary Computation", "authors": ["Eugene Eberbach"], "categories": ["cs.NE", "cs.CL", "68"], "comment": null, "summary": "Evolution by natural selection, which is one of the most compelling themes of\nmodern science, brought forth evolutionary algorithms and evolutionary\ncomputation, applying mechanisms of evolution in nature to various problems\nsolved by computers. In this paper we concentrate on evolutionary automata that\nconstitute an analogous model of evolutionary computation compared to\nwell-known evolutionary algorithms. Evolutionary automata provide a more\ncomplete dual model of evolutionary computation, similar like abstract automata\n(e.g., Turing machines) form a more formal and precise model compared to\nrecursive algorithms and their subset - evolutionary algorithms. An\nevolutionary automaton is an automaton that evolves performing evolutionary\ncomputation perhaps using an infinite number of generations. This model allows\nfor a direct modeling evolution of evolution, and leads to tremendous\nexpressiveness of evolutionary automata and evolutionary computation. This also\ngives the hint to the power of natural evolution that is self-evolving by\ninteractive feedback with the environment."}
{"id": "2501.06826", "pdf": "https://arxiv.org/pdf/2501.06826.pdf", "abs": "https://arxiv.org/abs/2501.06826", "title": "Aligning NLP Models with Target Population Perspectives using PAIR: Population-Aligned Instance Replication", "authors": ["Stephanie Eckman", "Bolei Ma", "Christoph Kern", "Rob Chew", "Barbara Plank", "Frauke Kreuter"], "categories": ["stat.ME", "cs.CL"], "comment": "EMNLP 2025 NLPerspectives Workshop", "summary": "Models trained on crowdsourced annotations may not reflect population views,\nif those who work as annotators do not represent the broader population. In\nthis paper, we propose PAIR: Population-Aligned Instance Replication, a\npost-processing method that adjusts training data to better reflect target\npopulation characteristics without collecting additional annotations. Using\nsimulation studies on offensive language and hate speech detection with varying\nannotator compositions, we show that non-representative pools degrade model\ncalibration while leaving accuracy largely unchanged. PAIR corrects these\ncalibration problems by replicating annotations from underrepresented annotator\ngroups to match population proportions. We conclude with recommendations for\nimproving the representativity of training data and model performance."}
{"id": "2505.24073", "pdf": "https://arxiv.org/pdf/2505.24073.pdf", "abs": "https://arxiv.org/abs/2505.24073", "title": "mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation", "authors": ["Chan-Wei Hu", "Yueqi Wang", "Shuo Xing", "Chia-Ju Chen", "Suofei Feng", "Ryan Rossi", "Zhengzhong Tu"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "16 pages", "summary": "Large Vision-Language Models (LVLMs) have made remarkable strides in\nmultimodal tasks such as visual question answering, visual grounding, and\ncomplex reasoning. However, they remain limited by static training data,\nsusceptibility to hallucinations, and inability to verify claims against\nup-to-date, external evidence, compromising their performance in dynamic\nreal-world applications. Retrieval-Augmented Generation (RAG) offers a\npractical solution to mitigate these challenges by allowing the LVLMs to access\nlarge-scale knowledge databases via retrieval mechanisms, thereby grounding\nmodel outputs in factual, contextually relevant information. Here in this\npaper, we conduct the first systematic dissection of the multimodal RAG\npipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the\nmodality configurations and retrieval strategies, (2) the re-ranking stage: on\nstrategies to mitigate positional biases and improve the relevance of retrieved\nevidence, and (3) the generation phase: we further investigate how to best\nintegrate retrieved candidates into the final generation process. Finally, we\nextend to explore a unified agentic framework that integrates re-ranking and\ngeneration through self-reflection, enabling LVLMs to select relevant evidence\nand suppress irrelevant context dynamically. Our full-stack exploration of RAG\nfor LVLMs yields substantial insights, resulting in an average performance\nboost of 5% without any fine-tuning."}
{"id": "2508.00910", "pdf": "https://arxiv.org/pdf/2508.00910.pdf", "abs": "https://arxiv.org/abs/2508.00910", "title": "Cyber-Zero: Training Cybersecurity Agents without Runtime", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "Public Link: https://github.com/amazon-science/cyber-zero", "summary": "Large Language Models (LLMs) have achieved remarkable success in software\nengineering tasks when trained with executable runtime environments,\nparticularly in resolving GitHub issues. However, such runtime environments are\noften unavailable in other domains, especially cybersecurity, where challenge\nconfigurations and execution contexts are ephemeral or restricted. We present\nCyber-Zero, the first runtime-free framework for synthesizing high-quality\nagent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly\navailable CTF writeups and employs persona-driven LLM simulation to\nreverse-engineer runtime behaviors and generate realistic, long-horizon\ninteraction sequences without actual environments. Using trajectories\nsynthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1%\nabsolute performance gains over baseline models on three prominent CTF\nbenchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model,\nCyber-Zero-32B, establishes new state-of-the-art performance among open-weight\nmodels, matching the capabilities of proprietary systems like DeepSeek-V3-0324\nand Claude-3.5-Sonnet while offering superior cost-effectiveness, and\ndemonstrating that runtime-free trajectory synthesis can effectively\ndemocratize the development of state-of-the-art cybersecurity agents."}
{"id": "2508.13500", "pdf": "https://arxiv.org/pdf/2508.13500.pdf", "abs": "https://arxiv.org/abs/2508.13500", "title": "LLM-Enhanced Linear Autoencoders for Recommendation", "authors": ["Jaewan Moon", "Seongmin Park", "Jongwuk Lee"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by CIKM 2025", "summary": "Large language models (LLMs) have been widely adopted to enrich the semantic\nrepresentation of textual item information in recommender systems. However,\nexisting linear autoencoders (LAEs) that incorporate textual information rely\non sparse word co-occurrence patterns, limiting their ability to capture rich\ntextual semantics. To address this, we propose L3AE, the first integration of\nLLMs into the LAE framework. L3AE effectively integrates the heterogeneous\nknowledge of textual semantics and user-item interactions through a two-phase\noptimization strategy. (i) L3AE first constructs a semantic item-to-item\ncorrelation matrix from LLM-derived item representations. (ii) It then learns\nan item-to-item weight matrix from collaborative signals while distilling\nsemantic item correlations as regularization. Notably, each phase of L3AE is\noptimized through closed-form solutions, ensuring global optimality and\ncomputational efficiency. Extensive experiments demonstrate that L3AE\nconsistently outperforms state-of-the-art LLM-enhanced models on three\nbenchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20.\nThe source code is available at https://github.com/jaewan7599/L3AE_CIKM2025."}
{"id": "2508.16313", "pdf": "https://arxiv.org/pdf/2508.16313.pdf", "abs": "https://arxiv.org/abs/2508.16313", "title": "Retrieval Enhanced Feedback via In-context Neural Error-book", "authors": ["Jongyeop Hyun", "Bumsoo Kim"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at EMNLP 2025 main conference", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved reasoning capabilities, with in-context learning (ICL) emerging as a\nkey technique for adaptation without retraining. While previous works have\nfocused on leveraging correct examples, recent research highlights the\nimportance of learning from errors to enhance performance. However, existing\nmethods lack a structured framework for analyzing and mitigating errors,\nparticularly in Multimodal Large Language Models (MLLMs), where integrating\nvisual and textual inputs adds complexity. To address this issue, we propose\nREFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a\nteacher-student framework that systematically structures errors and provides\ntargeted feedback. REFINE introduces three systematic queries to construct\nstructured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance\nmultimodal reasoning by prioritizing relevant visual information, diagnosing\ncritical failure points, and formulating corrective actions. Unlike prior\napproaches that rely on redundant retrievals, REFINE optimizes structured\nfeedback retrieval, improving inference efficiency, token usage, and\nscalability. Our results demonstrate substantial speedup, reduced computational\ncosts, and successful generalization, highlighting REFINE's potential for\nenhancing multimodal reasoning."}
{"id": "2508.17334", "pdf": "https://arxiv.org/pdf/2508.17334.pdf", "abs": "https://arxiv.org/abs/2508.17334", "title": "Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs", "authors": ["Somraj Gautam", "Abhirama Subramanyam Penamakuri", "Abhishek Bhandari", "Gaurav Harit"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA)\non cricket scorecards, designed to evaluate large vision-language models\n(LVLMs) on complex numerical and cross-lingual reasoning over semi-structured\ntabular images. MMCRICBENCH-3K comprises 1,463 synthetically generated\nscorecard images from ODI, T20, and Test formats, accompanied by 1,500 English\nQA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring English\nscorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindi\nscorecards, with all questions and answers kept in English to enable controlled\ncross-script evaluation. The task demands reasoning over structured numerical\ndata, multi-image context, and implicit domain knowledge. Empirical results\nshow that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggle\non the English subset despite it being their primary training language and\nexhibit a further drop in performance on the Hindi subset. This reveals key\nlimitations in structure-aware visual text understanding, numerical reasoning,\nand cross-lingual generalization. The dataset is publicly available via Hugging\nFace at https://huggingface.co/datasets/DIALab/MMCricBench, to promote LVLM\nresearch in this direction."}
